{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['show_config']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"7b760d00-546a-4124-8558-394a6cd5cbd3\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.0.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#7b760d00-546a-4124-8558-394a6cd5cbd3\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from graph_tool.all import *\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import *\n",
    "import random\n",
    "from csv import QUOTE_ALL\n",
    "import datetime\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "from bokeh.charts import Scatter\n",
    "from bokeh.models import HoverTool, ColumnDataSource \n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "from textstat.textstat import textstat\n",
    "from termcolor import colored\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D, MaxPooling1D\n",
    "from keras.engine import Input\n",
    "from keras.layers import Merge\n",
    "from keras.layers import merge\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.convolutional import Convolution2D, Convolution1D\n",
    "from keras.regularizers import WeightRegularizer\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.regularizers import l2, activity_l2, activity_l1, l1l2, l1\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, mean_squared_error, label_ranking_average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, StratifiedKFold, LabelKFold, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from pprint import pprint\n",
    "import string\n",
    "allowed_chars = set(string.ascii_lowercase) | {' '}\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_text_inside_brackets(text):\n",
    "    return re.sub(r'\\[[^\\]]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleanup(text, filter_w=None, min_len=None):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = text.replace('videolectures net', ' ').replace('videolectures.net', ' ')\n",
    "    text = text.replace(' nan ', ' ')\n",
    "    #text = ' '.join((stemmer.stem(i.decode('utf-8')) for i in text.split()))\n",
    "    if text == 'nan':\n",
    "        text = ''\n",
    "    text = remove_text_inside_brackets(text)\n",
    "    text = ''.join([i if i in allowed_chars else ' ' for i in text])\n",
    "    if filter_w is not None:\n",
    "        text = [i for i in text.split() if i in filter_w]\n",
    "        text = ' '.join(text)\n",
    "    if min_len is not None:\n",
    "        text = ' '.join(i for i in text.split() if len(i) >= min_len)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights(y):\n",
    "    w = np.zeros(len(y))\n",
    "    w_pos = w > 0\n",
    "    w_neg = w < 1\n",
    "    w[w_pos] = len(y)/2./len(w_pos)\n",
    "    w[w_neg] = len(y)/2./len(w_neg)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nn_hist(hist, secondary_y=False):\n",
    "    data = zip(hist.history['loss'], hist.history['val_loss'])\n",
    "    hist_df = pd.DataFrame(columns=['train', 'val'], data=data)\n",
    "    hist_df.plot(y=['train', 'val'], secondary_y=['val'] if secondary_y else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction_violins(y_true, y_pred):\n",
    "    pred = pd.DataFrame(columns=['prediction', 'true'], data=zip(y_pred, y_true))\n",
    "    sns.violinplot(x='true', y='prediction', data=pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Digitizer:\n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: len(self.word_dict))\n",
    "        self.word_dict[0] = None\n",
    "    def series_digitizer(self, text_series, max_len=-1):\n",
    "        digitzed_ser = text_series.apply(self.digitize, args=(max_len,)).astype('object')\n",
    "        max_len = digitzed_ser.apply(len).max()\n",
    "        digitzed_ser = digitzed_ser.apply(lambda x: x if len(x) == max_len else x + [0] * (max_len - len(x)))\n",
    "        return digitzed_ser\n",
    "        \n",
    "    def digitize(self, text, max_len=-1):\n",
    "        if max_len < 0:\n",
    "            return [self.word_dict[i] for i in text.split()]\n",
    "        else:\n",
    "            return [self.word_dict[i] for i in text.split()[:max_len]]\n",
    "    def num_words(self):\n",
    "        return len(self.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred, classes, score_f):\n",
    "    scores = list()\n",
    "    for i in classes:\n",
    "        t = y_true == i\n",
    "        p = y_pred == i\n",
    "        scores.append(score_f(t, p))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    data = list()\n",
    "    y_pred_int = np.round(y_pred).astype('int')\n",
    "    classes = sorted(set(y_true))\n",
    "    data.extend([('f1(' + str(i) + ')', s) for i,s in zip(classes, score(y_true, y_pred_int, classes, f1_score))])\n",
    "    data.extend([('precision(' + str(i) + ')', s) for i,s in zip(classes, score(y_true, y_pred_int, classes, precision_score))])\n",
    "    data.extend([('recall(' + str(i) + ')', s) for i,s in zip(classes, score(y_true, y_pred_int, classes, recall_score))])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viz_res(data, title=''):\n",
    "    df = pd.DataFrame(columns=['type', 'value'], data=data)\n",
    "    sns.barplot(x='type', y='value', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tfidf_vals(vectorized_text):\n",
    "    tfidf_vals = pd.DataFrame(data=vectorized_text.copy().ravel())\n",
    "    tfidf_vals = tfidf_vals[tfidf_vals[0] > 0]\n",
    "    tfidf_vals.plot(kind='hist', bins=200, logy=True)\n",
    "    plt.show()\n",
    "    tfidf_vals = pd.DataFrame(data=vectorized_text.max(axis=1).ravel())\n",
    "    sns.distplot(tfidf_vals)\n",
    "    plt.title('max tfidf vals per doc')\n",
    "    plt.show()\n",
    "    tfidf_vals = pd.DataFrame(data=vectorized_text.max(axis=0).ravel())\n",
    "    sns.distplot(tfidf_vals)\n",
    "    plt.title('max tfidf vals per term')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_tfidf_max(tfidf, words, min_th=None, max_th=None):\n",
    "    tfidf = tfidf.copy()\n",
    "    words = np.array(words)\n",
    "    if min_th is not None:\n",
    "        keep = tfidf.max(axis=0) >= min_th\n",
    "        try:\n",
    "            keep = keep.flatten()\n",
    "        except:\n",
    "            keep = np.array(keep.todense()).flatten()\n",
    "        words = words[keep]\n",
    "        tfidf = tfidf[:, keep]\n",
    "    if max_th is not None:\n",
    "        keep = tfidf.max(axis=0) <= max_th\n",
    "        try:\n",
    "            keep = keep.flatten()\n",
    "        except:\n",
    "            keep = np.array(keep.todense()).flatten()\n",
    "        words = words[keep]\n",
    "        tfidf = tfidf[:, keep]\n",
    "    assert len(words) == tfidf.shape[1]\n",
    "    return tfidf, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_word_rep(tfidf, words):\n",
    "    words = np.array(words)\n",
    "    bidx = np.array([True if len(w) == 1 or len(set(w)) == len(w) else False for w in map(lambda x: x.split(), words)], \n",
    "                   dtype=bool)\n",
    "    words = words[bidx]\n",
    "    tfidf = tfidf[:, bidx]\n",
    "    return tfidf, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordcloud(text_freq, ax=None, **kwargs):\n",
    "    wordcloud = WordCloud(**kwargs).generate_from_frequencies(text_freq)\n",
    "    if ax is None:\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        ax.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prereq_graph(title_topic, topic_prereq, draw=True):\n",
    "    g = Graph(directed=True)\n",
    "    v_dict = defaultdict(g.add_vertex)\n",
    "    \n",
    "    topic_to_titles = defaultdict(list)\n",
    "    for title, topic in title_topic:\n",
    "        topic_to_titles[topic].append(title)\n",
    "\n",
    "    topic_prereq_dict = defaultdict(list)\n",
    "    for topic, prereq in topic_prereq:\n",
    "        topic_prereq_dict[topic].append(prereq)\n",
    "    v_text = g.new_vertex_property('string')\n",
    "    v_topic = g.new_vertex_property('int')\n",
    "    e_c = g.new_edge_property('float')\n",
    "    for title, topic in title_topic:\n",
    "        v = v_dict[title]\n",
    "        v_text[v] = title\n",
    "        v_topic[v] = topic\n",
    "        prereq = topic_prereq_dict[topic]\n",
    "        for p in prereq:\n",
    "            p_titles = topic_to_titles[p]\n",
    "            for p_t in p_titles:\n",
    "                p_v = v_dict[p_t]\n",
    "                e = g.add_edge(p_v, v)\n",
    "                e_c[e] = p\n",
    "    print(g)\n",
    "    if draw:\n",
    "        #pos = arf_layout(g)\n",
    "        pos = sfdp_layout(g, groups=v_topic, C=4, p=3, mu_p=.9)\n",
    "        g.vp['pos'] = pos\n",
    "        deg = g.degree_property_map('out')\n",
    "        deg.a = 4 * (np.sqrt(deg.a) * 0.5 + 0.4)\n",
    "        g.set_reversed(True)\n",
    "        pr = pagerank(g)\n",
    "        g.set_reversed(False)\n",
    "\n",
    "        ebet = betweenness(g)[1]\n",
    "        ebet.a /= ebet.a.max()\n",
    "        ebet.a *= 10.\n",
    "        e_c.a /= e_c.a.max()\n",
    "        v_c = g.new_vertex_property('float')\n",
    "        v_c.a = v_topic.a.astype('float')\n",
    "        v_c.a /= v_c.a.max()\n",
    "        graph_draw(g, pos, output_size=(15000, 15000), \n",
    "                   vertex_text=v_text, inline=True, \n",
    "                   edge_color=e_c,\n",
    "                   edge_pen_width=ebet,\n",
    "                   output='prereq_graph.png', vorder=deg,\n",
    "                   vertex_fill_color=v_c, marker_size=50, bg_color=[1.,1., 1., 1.])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_topic_coocurrence(transformed):\n",
    "    cooc = transformed.T.dot(transformed)\n",
    "    np.fill_diagonal(cooc, 0)\n",
    "    sns.heatmap(cooc)\n",
    "    #plt.grid('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Digitizer:\n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: len(self.word_dict))\n",
    "        self.word_dict[0] = None\n",
    "    def series_digitizer(self, text_series, max_len=-1):\n",
    "        digitzed_ser = text_series.apply(self.digitize, args=(max_len, )).astype('object')\n",
    "        max_len = digitzed_ser.apply(len).max()\n",
    "        digitzed_ser = digitzed_ser.apply(lambda x: x if len(x) == max_len else x + [0] * (max_len - len(x)))\n",
    "        return digitzed_ser\n",
    "        \n",
    "    def digitize(self, text, max_len=-1):\n",
    "        if max_len < 0:\n",
    "            return [self.word_dict[i] for i in text.split()]\n",
    "        else:\n",
    "            return [self.word_dict[i] for i in text.split()[:max_len]]\n",
    "    def num_words(self):\n",
    "        return len(self.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot(X, df, fit=True):\n",
    "    if fit:\n",
    "        tsne_plot.tsne = TSNE(n_components=2, learning_rate=500, n_iter=1000000, random_state=2016)\n",
    "        tsne_text = tsne_plot.tsne.fit_transform(X) # [n_samples, n_features]\n",
    "    else:\n",
    "        tsne = tsne_plot.tsne.transform(X)\n",
    "        \n",
    "    plot_df = pd.DataFrame(columns=['x', 'y'], data=tsne_text, index=df.index)\n",
    "    for c in df.columns:\n",
    "        plot_df[c] = df[c]\n",
    "\n",
    "    print(len(plot_df))\n",
    "    hover_tips = [(i[0].upper(), '@' + i) for i in df.columns]\n",
    "    hover = HoverTool(\n",
    "        tooltips=hover_tips\n",
    "    )\n",
    "\n",
    "    source = ColumnDataSource(plot_df)\n",
    "    p = figure(background_fill='#DFDFE5', plot_width=800, \n",
    "                              plot_height=600)\n",
    "    # Add the hover tool\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    # Populate glyphs\n",
    "    p.circle(x='x', y='y', size=7, alpha=0.3, source=source)\n",
    "    #p = Scatter(plot_df, title=\"scat\", width=800, tools=[hover, \"pan\",\"box_zoom\",\"wheel_zoom\",\"reset\",\"resize\",\"save\"])\n",
    "    show(p)\n",
    "    return plot_df\n",
    "tsne_plot.tsne = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viz_nmf_output(nmf):\n",
    "    sns.distplot(nmf.argmax(axis=1).flatten(), bins=nmf.shape[1])\n",
    "    plt.title('documents over topics distribution')\n",
    "    plt.show()\n",
    "    \n",
    "    data = list()\n",
    "    for r in nmf:\n",
    "        for idx, i in enumerate(sorted(r, reverse=True)):\n",
    "            data.append((idx, i))\n",
    "    dist_df = pd.DataFrame(columns=['pos', 'val'], data=data)\n",
    "    sns.barplot(x='pos', y='val', data=dist_df)\n",
    "    plt.title('certainty of topic assignment')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_entropy_prereq(mat, th=0.1, entropy_ge=True, cross_entropy_diff=True):\n",
    "    prereq = list()\n",
    "    eps = np.finfo(float).eps\n",
    "    log = np.log\n",
    "    save_log = lambda x: log(x + eps)\n",
    "    entropy = lambda x: - np.sum(x_i * save_log(x_i) for x_i in x)\n",
    "    cross_entropy = lambda x, y: - np.sum(x_i * save_log(y_i) for x_i, y_i in \n",
    "                                          filter(lambda (x_i, y_i): x_i > th and y_i > th, zip(x, y)))\n",
    "    entropies = list(map(entropy, mat))\n",
    "    for idx, i in tqdm(enumerate(mat), total=mat.shape[0]):\n",
    "        i_H = entropies[idx]\n",
    "        for jdx, j in enumerate(mat):\n",
    "            if idx != jdx:\n",
    "                j_H = entropies[jdx]\n",
    "                if not entropy_ge or i_H >= j_H: # i more general than j\n",
    "                    c_H_i_j = cross_entropy(i, j)\n",
    "                    c_H_j_i = cross_entropy(j, i)\n",
    "                    if c_H_j_i >= 0. and c_H_i_j >= c_H_j_i:\n",
    "                        if cross_entropy_diff:\n",
    "                            dep = c_H_i_j - c_H_j_i\n",
    "                        else:\n",
    "                            dep = c_H_i_j\n",
    "                        if dep > 0:\n",
    "                            prereq.append((jdx, idx, dep))\n",
    "    return pd.DataFrame(columns=['t', 'prereq', 'val'], data=prereq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classification_report_df(t, p, *args, **kwargs):\n",
    "    d = list()\n",
    "    for i in classification_report(t, p, *args, **kwargs).split('\\n')[1:]:\n",
    "        i = i.strip()\n",
    "        if i != '':\n",
    "            #print(i.split())\n",
    "            try:\n",
    "                class_name, precision, recall, f1_score, support = i.split()\n",
    "                d.append((class_name, float(precision), float(recall), float(f1_score), int(support)))\n",
    "            except:\n",
    "                pass\n",
    "    return pd.DataFrame(columns=['class_name', 'precision', 'recall', 'f1-score', 'support'], data=d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
