{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1a522c89-18df-4b18-ae0d-af4e60ca1bf5\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.0.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#1a522c89-18df-4b18-ae0d-af4e60ca1bf5\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from graph_tool.all import *\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import *\n",
    "import random\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "from bokeh.charts import Scatter\n",
    "from bokeh.models import HoverTool, ColumnDataSource \n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "from textstat.textstat import textstat\n",
    "from termcolor import colored\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.engine import Input\n",
    "from keras.layers import Merge\n",
    "from keras.layers import merge\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.regularizers import WeightRegularizer\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.regularizers import l2, activity_l2, activity_l1, l1l2, l1\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "import string\n",
    "allowed_chars = set(string.ascii_lowercase) | {' '}\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_text_inside_brackets(text):\n",
    "    return re.sub(r'\\[[^\\]]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleanup(text, filter_w=None, min_len=None):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = text.replace('videolectures net', ' ')\n",
    "    text = text.replace(' nan ', ' ')\n",
    "    #text = ' '.join((stemmer.stem(i.decode('utf-8')) for i in text.split()))\n",
    "    if text == 'nan':\n",
    "        text = ''\n",
    "    text = remove_text_inside_brackets(text)\n",
    "    text = ''.join([i if i in allowed_chars else ' ' for i in text])\n",
    "    if filter_w is not None:\n",
    "        text = [i for i in text.split() if i in filter_w]\n",
    "        text = ' '.join(text)\n",
    "    if min_len is not None:\n",
    "        text = ' '.join(i for i in text.split() if len(i) >= min_len)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights(y):\n",
    "    w = np.zeros(len(y))\n",
    "    w_pos = w > 0\n",
    "    w_neg = w < 1\n",
    "    w[w_pos] = len(y)/2./len(w_pos)\n",
    "    w[w_neg] = len(y)/2./len(w_neg)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_nn_hist(hist, secondary_y=False):\n",
    "    data = zip(hist.history['loss'], hist.history['val_loss'])\n",
    "    hist_df = pd.DataFrame(columns=['train', 'val'], data=data)\n",
    "    hist_df.plot(y=['train', 'val'], secondary_y=['val'] if secondary_y else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction_violins(y_true, y_pred):\n",
    "    pred = pd.DataFrame(columns=['prediction', 'true'], data=zip(y_pred, y_true))\n",
    "    sns.violinplot(x='true', y='prediction', data=pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Digitizer:\n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: len(self.word_dict))\n",
    "        self.word_dict[0] = None\n",
    "    def series_digitizer(self, text_series, max_len=-1):\n",
    "        digitzed_ser = text_series.apply(self.digitize, args=(max_len,)).astype('object')\n",
    "        max_len = digitzed_ser.apply(len).max()\n",
    "        digitzed_ser = digitzed_ser.apply(lambda x: x if len(x) == max_len else x + [0] * (max_len - len(x)))\n",
    "        return digitzed_ser\n",
    "        \n",
    "    def digitize(self, text, max_len=-1):\n",
    "        if max_len < 0:\n",
    "            return [self.word_dict[i] for i in text.split()]\n",
    "        else:\n",
    "            return [self.word_dict[i] for i in text.split()[:max_len]]\n",
    "    def num_words(self):\n",
    "        return len(self.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(y_true, y_pred, classes, score_f):\n",
    "    scores = list()\n",
    "    for i in classes:\n",
    "        t = y_true == i\n",
    "        p = y_pred == i\n",
    "        scores.append(score_f(t, p))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    data = list()\n",
    "    y_pred_int = np.round(y_pred).astype('int')\n",
    "    classes = sorted(set(y_true))\n",
    "    data.extend([('f1(' + str(i) + ')', s) for i,s in zip(classes, score(y_true, y_pred_int, classes, f1_score))])\n",
    "    data.extend([('precision(' + str(i) + ')', s) for i,s in zip(classes, score(y_true, y_pred_int, classes, precision_score))])\n",
    "    data.extend([('recall(' + str(i) + ')', s) for i,s in zip(classes, score(y_true, y_pred_int, classes, recall_score))])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viz_res(data, title=''):\n",
    "    df = pd.DataFrame(columns=['type', 'value'], data=data)\n",
    "    sns.barplot(x='type', y='value', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tfidf_vals(vectorized_text):\n",
    "    tfidf_vals = pd.DataFrame(data=vectorized_text.copy().ravel())\n",
    "    tfidf_vals = tfidf_vals[tfidf_vals[0] > 0]\n",
    "    tfidf_vals.plot(kind='hist', bins=200, logy=True)\n",
    "    plt.show()\n",
    "    tfidf_vals = pd.DataFrame(data=vectorized_text.max(axis=1).ravel())\n",
    "    sns.distplot(tfidf_vals)\n",
    "    plt.title('max tfidf vals per doc')\n",
    "    plt.show()\n",
    "    tfidf_vals = pd.DataFrame(data=vectorized_text.max(axis=0).ravel())\n",
    "    sns.distplot(tfidf_vals)\n",
    "    plt.title('max tfidf vals per term')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_tfidf_max(tfidf, words, min_th=None, max_th=None):\n",
    "    tfidf = tfidf.copy()\n",
    "    words = np.array(words)\n",
    "    if min_th is not None:\n",
    "        keep = tfidf.max(axis=0) >= min_th\n",
    "        try:\n",
    "            keep = keep.flatten()\n",
    "        except:\n",
    "            keep = np.array(keep.todense()).flatten()\n",
    "        words = words[keep]\n",
    "        tfidf = tfidf[:, keep]\n",
    "    if max_th is not None:\n",
    "        keep = tfidf.max(axis=0) <= max_th\n",
    "        try:\n",
    "            keep = keep.flatten()\n",
    "        except:\n",
    "            keep = np.array(keep.todense()).flatten()\n",
    "        words = words[keep]\n",
    "        tfidf = tfidf[:, keep]\n",
    "    assert len(words) == tfidf.shape[1]\n",
    "    return tfidf, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_word_rep(tfidf, words):\n",
    "    words = np.array(words)\n",
    "    bidx = np.array([True if len(w) == 1 or len(set(w)) == len(w) else False for w in map(lambda x: x.split(), words)], \n",
    "                   dtype=bool)\n",
    "    words = words[bidx]\n",
    "    tfidf = tfidf[:, bidx]\n",
    "    return tfidf, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordcloud(text_freq, **kwargs):\n",
    "    wordcloud = WordCloud(**kwargs).generate_from_frequencies(text_freq)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prereq_graph(title_topic, topic_prereq, draw=True):\n",
    "    g = Graph(directed=True)\n",
    "    v_dict = defaultdict(g.add_vertex)\n",
    "    \n",
    "    topic_to_titles = defaultdict(list)\n",
    "    for title, topic in title_topic:\n",
    "        topic_to_titles[topic].append(title)\n",
    "\n",
    "    topic_prereq_dict = defaultdict(list)\n",
    "    for topic, prereq in topic_prereq:\n",
    "        topic_prereq_dict[topic].append(prereq)\n",
    "    v_text = g.new_vertex_property('string')\n",
    "    v_topic = g.new_vertex_property('int')\n",
    "    e_c = g.new_edge_property('float')\n",
    "    for title, topic in title_topic:\n",
    "        v = v_dict[title]\n",
    "        v_text[v] = title\n",
    "        v_topic[v] = topic\n",
    "        prereq = topic_prereq_dict[topic]\n",
    "        for p in prereq:\n",
    "            p_titles = topic_to_titles[p]\n",
    "            for p_t in p_titles:\n",
    "                p_v = v_dict[p_t]\n",
    "                e = g.add_edge(p_v, v)\n",
    "                e_c[e] = p\n",
    "    print(g)\n",
    "    if draw:\n",
    "        #pos = arf_layout(g)\n",
    "        pos = sfdp_layout(g, groups=v_topic, C=4, p=3, mu_p=.9)\n",
    "        g.vp['pos'] = pos\n",
    "        deg = g.degree_property_map('out')\n",
    "        deg.a = 4 * (np.sqrt(deg.a) * 0.5 + 0.4)\n",
    "        g.set_reversed(True)\n",
    "        pr = pagerank(g)\n",
    "        g.set_reversed(False)\n",
    "\n",
    "        ebet = betweenness(g)[1]\n",
    "        ebet.a /= ebet.a.max()\n",
    "        ebet.a *= 10.\n",
    "        e_c.a /= e_c.a.max()\n",
    "        v_c = g.new_vertex_property('float')\n",
    "        v_c.a = v_topic.a.astype('float')\n",
    "        v_c.a /= v_c.a.max()\n",
    "        graph_draw(g, pos, output_size=(15000, 15000), \n",
    "                   vertex_text=v_text, inline=True, \n",
    "                   edge_color=e_c,\n",
    "                   edge_pen_width=ebet,\n",
    "                   output='prereq_graph.png', vorder=deg,\n",
    "                   vertex_fill_color=v_c, marker_size=50, bg_color=[1.,1., 1., 1.])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_topic_coocurrence(transformed):\n",
    "    cooc = transformed.T.dot(transformed)\n",
    "    np.fill_diagonal(cooc, 0)\n",
    "    sns.heatmap(cooc)\n",
    "    #plt.grid('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
