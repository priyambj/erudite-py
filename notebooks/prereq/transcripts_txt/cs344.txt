Ben wants to implement a smoothing operation on a 4096 element one-dimensional array This array is indexed by i and the value at each index is V [ i ]. The smoothing operation on element i = 0.25 x V [i - 1] + 0.5 x V [ i ] + 0.25 x V [ i + 1]. And a quick note. At the boundary, assume that all values beyond the boundary like V [-1] or V [4096] are equal to the value at that boundary, V [0] or V [4095]. The new values, Vnew [ i ] at each step can be computed in parallel. Then Vnew becomes V, and we continue. Ben wants to implement this in CUDA. He wants to launch 4096 threads to solve this problem, and he decides to launch 16 blocks of 256 threads each. And here is Ben's code. Ben says his code works but is running much slower than he would like. So rewrite Ben's code to increase the speed. In particular, can you reduce the number of global memory transfers? What is the ratio between the amount of traffic incurred by Ben's implementation, versus your optimized implementation? Please fill in the blank here. Your answer does not have to be exact, but it should be within 10% of the actual ratio. Line C, in the following graph, represents how performance scales on a particular GPU on a CUDA application, as a function of the number of blocks launched. Assume this application is strongly compute bound, whereas computation is the performance bottleneck for this problem. If we run this application on a different GPU with twice as many SMs or in other words, cores, which curve best represents the performance you might see? Please fill in the box with either A, B, C, D or E. Assume this application is strongly compute bound, whereas computation is the performance bottleneck for this problem. If we run the same application on a different GPU with twice the memory bandwidth, which curve best represents the performance that you might see? Assume this application is strongly memory bandwidth bound, whereas memory bandwidth is the performance bottleneck for this problem. If we run the same application on a different GPU with half as many SMs, which curve best represents the performance that you might see? Assume this application is strongly memory bandwidth bound whereas memory bandwidth is the performance bottleneck for this problem. If we run the same application on a different GPU with half as much memory bandwidth and twice as many SMs, which curve best represents the performance that you might see? We are using CUDA to run a blurring kernel on a gray scale image where each pixel is a 32-bit single precision floating point number. Let's consider that the kernel is a 5 by 5 kernel, and we're not going to worry about where it's stored. Then our operation will be 25 multiplies and 24 adds to compute each pixel output. Naturally, we want to use shared memory to store the image so that we save memory bandwidth by reading each pixel as few times as possible. Our code will look something like this. You can choose how to launch this kernel in terms of threads per block configuration. You have 2 choices, 1024 by 1 or 32 by 32. Both choices will feature 1,024 threads per block. So the first question is, which 1 of these 2 configurations will require less global memory bandwidth? Please choose here. And the second question is, how much less was the ratio of global memory bandwidth between the kernel that uses more memory bandwidth and the kernel with less? In your calculation, please ignore the bandwidth required for the blurKernel and the bandwidth required for right back of v new, and please enter your answer here. You choose to implement this with a 32 x 32 block, and your boss comes running into your office excitedly one day, saying NVIDIA is shipping a new GPU, and it has 4 times the storage per SM, 4 times the registers, and 4 times the shared memory. You know that your blur kernel is completely bound by global memory bandwidth, but now you know that you can run a kernel that's twice as big in both directions. So instead of 32 x 32, now it's 64 x 64. Express as a decimal what is the speed up of your program on this new GPU. Speed up reflects my program will run n times faster. Again ignore the bandwidth required for the blur kernel in the right back of v_new. On a CUDA compute capability 3.5 machine, if I want to run the maximum number of threads possible that are all resident on an SM at the same time, each thread must use no more than how many registers? Please fill in the number here. If I want to run the maximum number of threads that are all resident on the SM at the same time, each thread must use no more than how many bytes of shared memory? Please fill in the blank here. If we want to maximize the number of blocks resident on the same SM, what is the maximum number of threads we can have per block? Please fill in that number here. Then we'll actually implement a fast compact primitive that operates on a single block of data. Recall that a compact inputs a true or false flag for every thread and a value for every thread, and outputs only those values which the input flags as true. So, in this problem, the strategy that we will do this scan is as follows. For step 1, within each warp, we will sum up the number of true flags. For step 2, we will do an exclusive sum scan of the per warp results. For step 3, we will do a exclusive sum scan of each warp, with the starting value of each warp equal to the corresponding output in step 2. For step 4, we will use the resulting address as scatter address into the output array for each thread for which the flag is true and we will scatter the value into the output array. As an example, consider the following, where we used a warp of size 4 to make the example easy to understand. And here's our input of 4 warps of 4 flags each. Step 1, I'll put 2241. Step 2, I'll put 0248. Step 3, I'll put 0112, 2233, 4567, 8999. And step 4 scatters the underlined elements to the following flags. And that's that, and I will follow them up with programming questions. In Problem Set number 6, you will be implementing a fast parallel algorithm for seamless image cloning. Seamless image cloning is a modern algorithm for pasting one image into another in a very subtle way. Here is an example of seamless image cloning. For example, you have a bear, a swimming pool. With seamless image cloning, you now have a polar bear in a swimming pool. Our algorithm for seamless image cloning uses a technique called "double buffering" to iteratively compute a solution. The main idea behind double buffering in the context of this homework is that we store an approximate solution to a problem in one buffer. Then we use that approximate solution to compute a better solution which we will store in another buffer. Then we go in the other direction, computing an even better solution, and store that solution in the first buffer. After going back and forth many times, we will arrive at an ideal solution to our problem. This style of computation arises in many parallel algorithms, which is why we think it will make a really interesting homework assignment. Before we implement seamless image cloning, let's take a closer look at our inputs. We are given as inputs a source image, a target image, and a mask. We want to copy the masked value from the source image into the target image in a subtle way. To do this, we hard code the values on the inside border of the masked region to be the same as the target image, then what we do is to solve for the unknown pixel values in the strict interior of the masked region. How do we find these unknown pixel values in the strict interior portion of the masked image? We can do that as follows. First step, we will make a guess for the unknown pixel values, and that's called our initial guess I0. I0 doesn't have to be a very good guess, so we can set I0 to be all zeros in the strict interior portion of our masked image. But note that 0 will be a very bad guess, because it would not be very subtle and it also would not conform to the source image very well. Now suppose we are given a guess solution IK to our problem, we can compute a better guess IK + 1 as follows. For some pixel i(k + 1) in I(k + 1) in the strict interior region of the mask, we can compute the value of i of K + 1 as A + B + C divided by D. And let's go through each of A and B and C and D one at a time. A is the sum of iK's neighbors. We don't include iK in the sum, and we only include iK's neighbors if they are in the strict interior portion of the masked region. B is the sum of t's neighbors where t is the corresponding pixel value in the target image, and the target image in our case is the swimming pool. And we only include t's neighbors if they are in the inside border of the masked region. And C is the sum of the difference between S and S's neighbors where S in this case is the corresponding pixel value in the source image and the source image in our case is the polar bear. Finally, D is the number of iK's neighbors, and iK's neighbors can be less than 4 if iK is on the border of the image. I'm here with Dr. Bill Dally, who is Chief Scientist and Senior Vice president of Research NVIDIA. Welcome, Bill. Well, thanks John. It's a pleasure to be here. I'm glad to have an opportunity to chat with you about important trends in computing. Super, tell us about your job in NVIDIA. What do you do? I really have two jobs. As Chief Scientist, I get to sort of look into a lot of neat technologies that we're developing and try to set technical directions in certain strategic areas work with key customers and partners on technology initiatives. And then as Senior Vice President of Research, I build and lead NVIDIA Research, which is a world class research lab doing research in a lot of interesting areas from graphics to parallel computing to circuits and architecture programming systems relevant to the future of NVIDIA. Tell us what you did before you came to NVIDIA. For 26 years I was a professor, first at MIT for 11 years, then at Stanford for 15 years. And I taught courses on computer architecture and parallel computing and led a research group that built a lot of interesting parallel computers and developed many interesting programming systems for them, including some early object-oriented parallel programming systems, concurrent small talk, concurrent aggregates. some stream programming systems, Dream C, Kernel C, Brooks, and Sequoia. That sequence of programming systems ultimately led to CUDA, which ultimately led to CUDA which was done by the same person who had done Brooks, a stream programming language we did at Stanford. During that period of time, I also was a consultant for industry working for numerous companies, particularly for Cray. I worked with Cray since the late 1980s into the middle of the 2000s, helping them develop interesting parallel computer systems. One of the cool recent announcements we've seen is the announcement of the Titan Supercomputer, which is now the fastest supercomputer in the world, and this has NVIDIA processors in its core. Can you talk a little bit about that process and how NVIDIA got to be involved and why that's such an exciting thing for GPU computing? Well, first of all, Titan is an awesome machine. It's 18,688 Kepler K20 GPUs and is the fastest computer in the world at running high performance winPAC. There's an interesting story there. The story of Titan actually starts with a meeting that I had with Steve Scott, who at the time was CTO of Cray at the Salishan Conference up on the Oregon Coast in 2009. I was talking to Steve and trying to see how can we work together. We really should get NVIDIA GPUs into Cray supercomputers because we have the best compute per dollar, compute per watt, which are the two things that matter in high performance computing, of anybody in the world, and they were actually going through a problem because they had bet on a different vendor who cancelled a project on them, and it left a hole in what they wanted to bid for this solicitation that was out from Oakridge to build what they call their leadership class computing facility-- ultimately what turned in into Titan. It was just a nice sort of juxtaposition in time that I was having this conversation with him right at the point in time where there was a hole to be filled. It turns out Kepler filled that hole wonderfully. There was a lot of challenges along the way that, I think, really had to do with getting the people in the National Labs to embrace the model of parallelism that Cuda presents. I think that once they embraced it, they found it was actually easier to write their programs that way, and they actually ran better across the board once they were reorganized into that style of parallelism of watching CTAs and organizing things in that style. But they had a very large chunk of legacy code mostly in Fortran. It was coded sort of Fortran running on a single node with MPI used to communicate between the nodes, and it was a nontrivial exercise to really bring that software over and get it to run well on a GPU accelerated system. And I think beyond the LINPAC number, which is extra relatively easy number to get because it's one program, what really is a success of Titan is the very large number of basic energy science and defense codes that have been ported over very successfully and get just tremendous performance on the K20s. NVIDIA is not only building high end, more expensive processors, like the couplers and like the GPUs you might buy for gaming, but it also really works very hard in the mobile arena. So how is GPU computing going to influence that? Well it's not in our current shipping products but the strategy will be available in the relatively near future is to have the same GPU from the top to the bottom of the line. So eventually you will have a Kepler GPU in the Titan supercomputer and you'll have a Kepler GPU in your cell phone, and you can apply the same programming models across that line and do GPU computing in your cell phone to the extent that it makes sense to do so. So where do you see the most interesting applications going for things that you carry in your pocket? I think that the really compelling applications for mobile devices are in computational photography and computer vision. I think that photography is at this cusp of being completely transformed from something that you do primarily with lenses and optics to something you do primarily with computing, and a GPU is just a perfectly matched tool to basically do the type of image processing and signal processing you need, basically after you've collected a bunch of photons, to process them in a way to produce really compelling images. So that sounds fairly abstract, and I think this is a fascinating field. So can you give an example of something that maybe a GPU would help you do in this computational photography realm? Little low level things or like if you want to do high dynamic range images, you'll actually acquire a set of images. Then there is a bunch of problems in composing them together. The camera may have moved slightly between the images, so they have to be registered. There may be objects moving in the images, so even if you registered the images, you have to back that object motion out to get the equivalent of a still photograph that was taken at one point in time. You may have what's called a rolling shutter, where because of the way the CCDs work you expose one line of the image at a time, and so that if there's any motion, that will produce sort of a wavy action in the photo. You, again, have to remove that. Then you want to do some processing of those images to remove noise and to enhance the image. And then finally to decide both when you acquired, what exposure times you wanted to get an optimal high dynamic range image and then how to combine those images into one final image that basically you're given a limited gamut that you'll display on your display device. Gives the human who's viewing it the appearance of actually many orders of magnitude dynamic range, from the original collected image. So bright areas of the image look bright but not that bright. Not much, though. We want you to have that much dynamic range structures display that. Let's turn to the technology side a little bit. Ten years ago if I wanted a faster computer, then what I would do is wait 6 months and then the clock speed of the CPUs I could buy would be 30% higher than it was, and I'd go down to the local store and buy a new faster processor. And that was largely determined by just a faster clock that I'd go from 1 to 2 to 3 GHz. That's not happening anymore. How come? Yeah, so it's interesting. A lot of people refer to that scaling in CPU performance as Moore's Law, which is a little bit of a misnomer. Moore's Law really predicts a growth rate in the number of devices you can fabricate on a chip. And Moore's Law is actually alive and well. We can still, every generation of technology, fabricate more devices and assuming extra chip, that's still growing exponentially. What stopped around 2005 was what's called the Nard scaling, which was scaling the voltages that we operate our chips at as we scaled the dimensions that we make the transistors at. And this stopped because of weakened current in the devices. And without going too technically into it, what that meant is that now when we get a new generation of chip, let's say we produced a line width by from say unit to 0.7 of whatever that unit was. A recent jump is from 28 nanometers to 20 nanometers. When we do that, we now get twice as many devices in the same amount of area. In the old days we would scale the voltage by 0.7 and since the power goes as CV squared, C would go at 0.7 the voltage will go by 0.7 squared. We'd wind up with the, I should say energy goes to CV squared. We would wind up with energy of switching the device being about a third of what it was before √8, so we'd get this 3x improvement in performace per watt of a basic unit, and then you can take that and use it in various ways. One of the ways we used it was to crank clock rate up. Now that's not happening anymore. Now that we're holding voltage constant, we're getting a little bit of energy gain from the 0.7 in capacity. But even that, we're not really getting the whole 0.7, we're getting maybe half of that. We get perhaps in a generation of technology a 15% improvement in the basic underlying technology. So that's why you can't take the same old serial processor and just have it go faster anymore. You're not going to get any of the frequency improvement. Frequencies are largely flattened out. You get more parallelism but it's no longer that factor of 3 each generation that you used to get. It's now you know perhaps 15% or so before you start applying architecture and circuit innovation. If you just relied on process, that's all you would get. Even though each one of those transistors is a little smaller and sucks up a little less power, that's not nearly enough to change the fact that you're making these bigger and more parallel every generation. That's right. The better way to think about it is suppose you got the whole 0.7 on capacitance in that scaling. That means you now sort of are 30% less energy. So you can now put 30% more units on. You would only grow 30% more parallelism in a power constrained environment on that same dime. You could take that 30% either in clock rate or in more units. Although it's harder to take in clock rate because that scaling is less linear. Now, you don't get the whole 30%. You may get half of that for various reasons. But that's what you're getting to play with each generation until you start innovating. That's why it's actually kind of fun to be a computer architect these days, because that's where the value is. It used be largely in process, and so companies that had a proprietary process kind of an advantage , and they still have a little bit of an advantage but that advantage has really shrunk since process matters less these days. And what you do with the process, the architecture and circuits and programming system matters a lot more. When you meet with your architects and you talk about, oh, let's design the next generation processor, what are really the design constraints that you think are at the top of the list? Certainly, power's going to be number one for sure. Energy efficiency is always the number one design constraint. Very closely behind it, though, is programability and range of applications. Again, sports re-divergence is a good example of that. We want to have both a control structure and a memory system that will run diverge codes well because that'll give us a broader range of applications. Then we'll look at programability, and it's not just being able to write the program and get it to function. But to get a large fraction of the absolute performance you can get out of the machines without undue effort. And a lot of this is removing performance surprises from the architecture. So that if you write the code in the very straightforward way, you'll actually get pretty good performance. It won't be awful and then you'll have to sit there, trying to deeply understand some obscure fact in the memory system to figure out why you've fallen off a performance cliff. And that's something that's changed really an enormous amount in the last 5 years. The cliffs for GPUs have historically been steeper than for CPUs, but they're getting much flatter as a result of the work you guys are doing. One of the questions I ask the students in the classes that I teach is, "What are you going to do with 100 times more compute?" And sometimes that's a really hard question for them. There's a lot of head scratching both in terms of what can we do with a super computer that's 100 times more powerful and what can you do with something on your desk or in your pocket? -Where do you see us going in this direction? -Yeah, well I I have an insatiable appetite for FLOPs. I would have no trouble using 100 or even 1,000 or even 10,000 times more compute. A lot of what I do is designing computers, and a lot of that involves prototyping and simulation new computer designs. I'm always frustrated about how those simulations run. So of I could run RTL simulations of a new computer 100 times faster, it would enable me to be much more productive in trying out new ideas for computer design. Same for circuit simulations. I spend a lot of time waiting for circuit simulation to converge. If I could run it 100 times faster, I could not just run one simulation but run whole parameter sweeps at once and do optimizations the same time I'm simulating. Another thing is also you look at the computers in your car. I mean our Tegra processors are actually designed into lots of different automobiles, including the Tesla Model S, the Motor Trend Car of the Year, but also Audis and BMWs and all sorts of Fords have Tegras in them. And the applications people are starting to use for these mobile processors in cars involve having lots of computer vision to look at what people inside the car are doing, look at what people outside of the car are doing. And in many ways it makes your cars much safer by having the car aware of what's going on around it. It can in many ways compensate for the driver not being completely alert or perhaps texting or doing something they shouldn't be doing. And in mobile devices I think there are a lot of compelling applications in both computational photography and augmented reality. If your mobile device is constantly aware of what's around you, it can be informing you. Oh, I think you're hungry. Here's a place that has gyros that I know you like because I have your profile of your likes and dislikes. Maybe you should stop for lunch. Or a block away is this guy who you really don't like. Maybe you should turn right at this corner and avoid running into him. In many ways, I think it sort of evolved to having your computing devices becoming your personal assistant. I always liked Jeeves in the Iron Man movies. I would like to have a device I can kind of talk to that is aware of the environment around me and can be basically a brain amplifier for me. It can sort of remember things that I forget and tell me about things in my environment and basically assist me in going through my day, both on professional and personal bases. So one of the goals of the supercomputer industry is to get up to—the term they use is exascale that they'd like to do 10 ^ 18 FLOPs per second. Certainly, Nvidia is going to be interested in being in those computers. What are we going to use that for? Well, I think first of all, there's nothing magical about an exascale. It's like, you know, when we first made petascale machines, which is just a few years ago, it wasn't like breaking the sound barrier or anything really qualitatively changed, but enabled better science and there's always— You look at sort of the fidelity of simulations we're able to do today to, say, simulate a more efficient engine for automobiles to improve gas mileage, and we're making lots of approximations to fit them on the supercomputers we have today. As we can get to higher fidelity by resolving grids finer and modeling a bunch of effects like turbulence more directly rather than using macro models to model them, we'll get more accurate simulations. And that will enable a better understanding of combustion in some of the you biotech applications of how proteins fold, various other climate— -Climate modeling. -Sure. Climate evolves. Basically as we get better computing capacity— and it's not you're reaching magic exascale and wonderful things happen, but at every step along the way, we get better science, we are able to design better products. And computing is a big driver of both scientific understanding and economic progress across across the board. And I think it's very important that we maintain that steady march forward, and exascale is just one milestone along that march. And my understanding is that power is really an enormously crucial thing for them to get right to be able to enable the exascale that we don't want machines that are going to cost $2 million a month just to plug in. -Right. It's really an economic argument. I mean if you really wanted an exascale machine today, you could build one. You just have to write a really big check and locate it right next to the nuclear power plant, the entire output of which it will consume. But I think if there was some application that was so compelling they were willing to really write the multi-billion dollar check required to do that, you would do it. I think that the real question of exascale is an economical exascale, and because on total cost of ownership the power bill is a tremendous fraction. So it's not actually an economical exascale machine unless you can do it for reasonable power level, and the number that's been thrown out is 20 megawatts. So that's $20 million a year. Yeah, $20 million a year power bill if you're paying roughly $10 a kilowatt hour. In fact, the bill actually winds up usually being a little bit higher than that because the cost of provisioning energy amortized over, say, a 30-year lifetime of the facility, usually is about equal to the annual bill for the energy. There is also something called the PUE, which is basically the efficiency of providing the energy. Even for a very good installation today maybe on the order of 1.1 to 1.2. So you pay another, say, 20% to run the air conditioners and fans and things like that in the facility, and basically energy you're consuming isn't being consumed by the computer. But it's a big challenge for us to get from, say, Sandy Bridge today— that's 1.5 nanojoules per instruction—to if you wanted to do exa instructions per second, to do an exa FLOP you might have to do more than an exa instruction per second. But even if you take that as a thing at 20 megawatts, that's 20 picojoules per instruction. And that's not just the processor; that's everything. That's the memory system, that's the network, that's the io storage system. It's the whole ball of wax that to do it. So you mainly get 10 picojoules per instruction to actually use in the processor. And so even in Nvidia it's not quite close enough to that. Yeah, well compared to Sandy Bridge, that's a factor of 150 down, and process isn't going to help you much. So that's why conventional CPUs are not going to get there. It's going to require a hybrid multi-core approach with most of the work being done in a GPU like throughput processor to get there. But even we have a ways to go. We're probably close to over-magnitude, and we might get a factor of three from process. We need to be very clever to come up with the other factor of 3 or 4 that we need. -Titan does have CPUs in it, yes? -That's correct. So is there a vision where that won't even be the case? No I think there are always pieces of the code where you have a critical path, you have a piece of single thread code that you need to run very quickly. And so you always need a latency optimized processor around to do that, but most of the work it's one of these things, it's kind of like a cache memory, where most of your acesses are to this little memory that runs really fast but you still need the capacity of the big memory sitting behind it, right? And so it's, it's the same thing on throughput versus latency. Most of your work is done in the throughput processors, but when you do have a latency critical thing, you run it in on the latency optimized processors. And so you wind up getting the critical path performance of the CPU with the bulk of the energy consumption of the GPU. -And the bulk of the FLOPs and Titan is certainly going to the GPU's. -Right. The bulk of the FLOPs will be in the GPU's. So to close, do you have any thoughts for students who are taking this class? I mean, it's very exciting to get to teach thousands of students about parallel computing. What should they be thinking about going forward? I think my big advice for students is to get your feet wet. Go out there and write a bunch of parallel programs. The future of the world is parallel. Serial programs, it's going to be like COBOL programs. I mean, and people used to do those, but they don't do them anymore. Don't just write the program, but be a critic of your own code, and go and profile it, understand why it performs the way it does, and understand that memory is not flat. If you want to get good performance out of a program, you have to understand the hierarchy of the memory system and find a compelling application. I always find that if you really want to make progress on improving your programming skills, especially parallel programming skills, it's motivating to say, okay, I've got this really cool computational photography application. I'm going to make cool pictures. That's actually one reason why graphics is so fun is that you can see your results. Super. Thanks so much for coming, Bill. I really appreciate your time. Oh, well thank you, John. It's really been a pleasure. I'm John Owens. I'm an Associate Professor of Electrical and Computer Engineering at UC Davis, where I lead a group of amazing graduate students in exploring the frontiers of GPU computing. And I'm Dave Luebke. I'm a Senior Director of Research at Nvidia where I study 3D graphics, and I've been working with John and others for many years on expanding the reach of GPUs. This class focuses on parallel computing on the GPU. You'll learn about both the hardware and the software on the modern GPU, and get to work with high end GPUs as you develop a set of image processing tools during the assignments, such as those that you might find in cool applications such as Photoshop or Instagram. We think that projects will be both interesting and fun. But most important, you'll learn how to think about computing in a parallel way. Thinking in parallel is a crucial skill, maybe even the crucial skill as you approach challenging problems in all areas of computing. All right. So let's get started. Welcome to Unit 1! We're really excited to have you join us in this course. I'll be teaching Unit 1, and Dave will follow with Unit 2. Now what we're going to cover today is the fundamentals of the GPU programming model and how to write your first CUDA program, and in the rest of the course, we're going to build from this foundation to cover the most interesting and exciting topics in all of parallel computing. So let's get started. Good morning, afternoon, or evening wherever you happen to be. This is John, and I'll be presenting Unit 1. So let's get started. Like a lot of American kids, when I was probably 6 years old I tried to dig a hole to China. I know I'm not the only kid who tried this. In general, you can probably conclude that American kids are a, ambitious, and b, not particularly good at geography, at least when they're six years old. Needless to say I was not successful. Why? Besides having the attention span of a gnat, I just couldn't dig a hole fast enough. The question I'd like to pose today is how could you dig a hole faster? I also meant there's 3 ways I could have gone faster. One, dig faster. Instead of, say, removing one shovelful of dirt every 2 seconds maybe with a lot of effort I could have removed 1 shovelful every second. This certainly would help, but I think we can all agree that there's a limit to this speeding up. It's not likely I could go ten times as fast no matter how hard I work. My shovel would probably explode. Two: buy a more productive shovel. Let's say a shovel with two or even three blades instead of one. An interesting approach. Perhaps a better shovel would make me more productive. But again, I think we can agree there's diminishing returns in making shovels better. It's not likely I could use a 10- or 20-bladed shovel. And number 3, hire more diggers. I have a younger sister and a brother. Perhaps I could have persuaded them to help me dig faster. This is probably the approach that would have been most productive overall toward my whole digging project. Of course, adding more diggers comes with it's own problems. How do I manage them? Will they get in each other's way? Will more diggers help me dig my hole deeper instead of just wider? Well, as you probably figured out I am not just talking about digging holes. Each of these three methods for digging holes faster has a parallel in terms of building a faster processor. When I talk about digging faster I'm actually talking about asking our processor should run with the faster clock to spend a shorter amount of time on each step of a computation. However, in a modern processor turning up clock speed also increases power consumption and we're the limit as far as power consumption on a chip. When I talk about buying a more productive shovel, I'm actually talking about asking our processor to do more work on each step, on each clock cycle. But like the super shovel, a single processor has also reached diminishing returns on how much work it can do per cycle. To be a little more technical, the most advanced CPUs are at a limit, as far as how much instruction-level parallelism they can extract per clock cycle. And when I talk about hire more diggers, I'm referring to parallel computing in a way that we hope to teach you in this class. Instead of having one fast digger with an awesome shovel, we're going to have many diggers with many shovels. Instead of having one or just a few very powerful processors, we're going to have many, many weaker, less powerful processors. Our first quiz. What are three traditional ways that hardware designers make computers run faster? Please check the three that are true. Our choices are faster clocks, longer clock period, more work per clock cycle, a larger hard disk, adding more processors, and reducing the amount of memory. And the three correct answers are faster clocks. This corresponds to our more productive digger. More work per clock cycle—this corresponds to our super shovel. And more processors—this corresponds to hiring more diggers. And this is the approach that we'll be taking in this class. You might have watched the intro video for this class, where I quoted the American super computer designer, Seymour Cray. "If you were plowing a field, which would you rather use? Two strong oxen or 1024 chickens?" Well, I love the chickens, and Dave loves the chickens, and I hope by the end of this class you'll love the chickens too. Now, the biggest idea in this course is parallelism. We can solve large problems by breaking them up into smaller pieces. Then we can run these smaller pieces at the same time. Parallel computing used to be a niche technology used by exotic supercomputers. Today, the world has gone parallel. Modern computing products are like the chickens. They have hundreds of processors that can each run a piece of your problem in parallel. A high end GPU contains over 3,000 arithmetic units, ALUs, that can simultaneously run 3,000 arithmetic operations. GPUs can have tens of thousands of parallel pieces of work all active at the same time. We call each of those pieces of work a thread, and a modern GPU may be running up to 65,000 concurrent threads. Together, all this computing power has the potential to help you solve your problems faster than you ever could before. But harnessing all that computing power at the same time, in parallel, requires a different way of thinking than programming a single scalar processor, where you're only doing one thing at a time. In this class we'll teach you how to program the GPU, allowing you to practice what we call GPU computing or GPGPU, standing for general purpose programmability on the graphics processing unit. More importantly, we'll teach you how to think about programming through a parallel lens. The homework assignments in this class will center around image processing applications. Frankly, they're pretty cool assignments, and with them you'll be able to do some interesting visual effects at blazing fast speeds. To understand why the GPU is such an interesting processor today, we'll start with technology trends. Why the world has gone parallel. And first, we'll start with some good news. Modern processors are made from transistors. Each year those transistors get smaller and smaller. This graph is from Stanford's CPUDB project. Thanks, guys. What it shows is the feature size of processors over time, where the feature size is the minimum size of a transistor or wire on a chip. So what we're seeing is this is time going in this direction. That's a long time ago and that's today. This is the feature size, so how big transistors are. Notice that it's getting smaller and smaller every generation. When you hear talk about 45 nanometer process or a 22 nanometer that's what we're referring to, the feature size. We see that it's consistently going down over time. As the feature size decreases, transistors get smaller, run faster, use less power, and we can put more of them on a chip. The consequence is that we have more and more resources for computation every single year. However, we've also got some bad news. Historically, as transistors improved, processor designers would then increase the clock rates of processors, running them faster and faster every year. Let's take a look at this diagram of clock speeds over the years. So again, we have time going on this axis, okay? So, a long time ago, today. Here we have clock frequency—how fast we're clocking these transistors. Historically, one of the primary drivers of clock performance has been clock speed increases. We see over many years, we see clock speeds continue to go up. However, over the last decade we see that clock speeds have essentially remained constant. A quick quiz to make sure we're all on the same page. At this point in time, our processor is getting faster every generation because we're running our transistors faster, or instead because we have more transistors available for computation? The answer is we have more transistors available for computation. We're not clocking things any faster. Clock rates are remaining constant. The reason we're getting faster is that we get more and more transistors available on a processor die with every generation. The reason that we're not increasing clock speed is not that the transistors have stopped getting smaller and faster. Even though transistors are continuing to get smaller and faster and consume less energy per transistor, the problem is running a billion transistors generates an awful lot of heat, and we can't keep all these processors cool. Power has emerged as a primary driving factor, possibly the most important factor in modern processor design at all scales, from a mobile phone that you keep in your pocket all the way to the very largest supercomputers. The consequence is we can't keep building processors in the way we always have by making a single processor faster and faster. When we do that, we end up with processors that we simply can't keep cool. Instead, processor designers have turned to building smaller, more efficient processors in terms of power, and they then spend their extra resources to build a larger number of efficient processors rather than faster less-efficient processors. So what kind of processors are we going to build? Let's assume that the major design constraint is power. Why are traditional CPU-like processors not the most energy efficient processors? Well, traditional CPUs have very complicated control hardware. This allows flexibility in performance, but as control hardware gets more complicated, it's increasingly expensive in terms of power and design complexity. So if we want the most bang for the buck in terms of computation for a fixed amount of power, we might instead choose to build simpler control structures and instead take those transistors and devote them to supporting more computation in the data path. If we want the most bang for the buck in terms of computation for a fixed amount of power, we might choose to build simpler control structures and instead devote those transistors to supporting more computation to the data path. The way that we're going to build that data path in the GPU is by building a large number of parallel compute units. Individually, those compute units are small, simple, and power efficient. These are the chickens. But we can put a very large number of them on a single chip, and the challenge to us in this class, and more generally in the computing industry, is to program them in such a way that they can all work together to solve complex problems. So let's wrap up with a quiz. What techniques are computer designers today using to build more power-efficient chips? Having fewer more complex processors. Having more or less complex processors. Maximizing the speed of the processor clock, or increasing the complexity of the control hardware. Check the ones you think are correct. And the answer is primarily, more simpler processors. And that's the philosophy of the GPU. Now the next question. When we build a high performance processor, which of course is going to be power-efficient, what are we optimizing for? One choice is minimizing latency. Latency is the amount of time to complete a task. We measure latency in units of time, like seconds. The other choice is throughput. Throughput is tasks completed per unit time. And we measure throughput in units as stuff per time, like jobs completed per hour. Unfortunately, these two goals are not necessarily aligned. In America, if you have a driver's license or a car, you've had the unfortunate opportunity to visit a government office called The Department of Motor Vehicles. If you're not from America, you've probably visited something like it. So in your head when when I say "DMV," substitute your favorite government office. When you visit the DMV, it's a very frustrating experience. You wait in lines a lot. This is not necessarily the fault of the DMC, though. The reason this happens is because your goals are not aligned with the DMV's goals. Your goal is to optimize for latency. You want to spend as little time in the DMV as possible. Instead, however, the DMV optimizes for throughput, specifically, the number of customers they serve per day. Consequently, these two people sitting behind the desk right here that work for DMV want long lines. Long lines mean their hard working employees are always busy because there's never a time they don't have a customer waiting. Traditional CPU's optimize for latency. They try to minimize the time elapsed of one particular task. GPUs instead chose to optimize for throughput. This is a fundamentally different approach and one that is aligned with technical trends in the computer industry. I'll refer you to a 2004 article by David Patterson called Latency Lags Bandwith. There are many, many applications where optimizing for throughput is the right approach. In computer graphics, for instance, we care more about pixels per second than the latency of any particular pixe l. We're willing to make the processing time of one pixel take twice as long if it means we get more pixel throughput. This class's homework focuses on image processing applications. Here, we also care more about throughput, which is more about pixels produced per second than the time for one individual pixel. Imaging processing applications are a perfect match for the GPU, which is why we're so excited about using them as a driving example in this course. To understand latency versus bandwidth with a real-world example, let's say my research group and I were going to drive from California to Virginia to visit Dave. This is 4,500 kilometers. Now we could either take a sports car, which holds 2 of us, and speed across the country at 200 kilometers an hour. Or we could take a bus that holds 40 of us but only travels at 50 kilometers per hour. What is the latency of each, measured in hours, and the throughput measured in people transported per hour? If we take the car, we travel 4,500 km at 200 km per hour. That's going to take 22.5 hours. The throughput is 2 people divided by 22.5 hours, which equals about 0.089 people per hour. For the bus, we're traveling 4,500 km at 50 km per hour, and that's going to take us 90 hours to get there. However, the throughput is 40 people divided by 90 hours, and that's going to give us a much better throughput of 0.45 people per hour. These two trends aren't necessarily opposed. Improved latency often leads to improved throughput, and sometimes improved throughput also leads to improved latency. But the GP designers are really prioritizing throughput. That's really the focus of their efforts. Given these technical trends, let's characterize the decisions that GPU designers have made. One, GPUs have lots of simple compute units that together can perform a large amount of computation. In general, the GPU is willing to trade off control for compute and choose simpler control complexity and more compute power. The consequence of this decision is that the GPU programming model, the programmer's view of the machine, is perhaps more restrictive than the CPU's. Two, GPUs have an explicitly parallel programming model. When we write programs for this machine, we know that we have lots of processors, and we have to program it in that way. We don't pretend that it has one processor and rely on a magic compiler chain to figure out how to map work onto many processors. This programming model is a main focus of the class. We'll talk a lot more about this. And three, GPUs optimize for throughput, not latency. They are willing to accept increased latency of any single individual computation in exchange for more computation being performed per second. Consequently, they're well suited for application domains where throughput is the most important metric. Now that we have some background on why GPUs look the way they do, we'll now discuss what a GPU looks like from the point of view of a software developer. One of the themes of this class is the importance of programming in parallel. This is a crucial skill, not just for GPUs, but also for CPUs even. If you buy an 8 core Intel Ivy Bridge processor, we see that it has 8 cores. Each core has 8-wide AVX vector operations. Each core supports two simultaneously running threads. Multiply those together and you get 128-way parallelism. On this processor, if you run a completely serial, C program with no parallelism at all, you're going to use less than 1% of the capabilities of this machine. There's no doubt that parallel programming is harder than serial programming, no matter what your target. But I hope by the end of the class, we'll convince you that this additional complexity is worthwhile for the potential performance gains. And throughout the course of the class, we'll try to lay out the benefits and the limitations of this programming model. One of the really exciting things about GPU computing is that it moves quickly, and some of today's limitations may disappear in the next generation of GPUs. The computers we're using in this class are termed heterogeneous. They have two different processors in them, the CPU and the GPU. Now, if you write a plain C program, your code will only allow you to use the CPU to run your program. So how do we write code that will run on the GPU? That's where CUDA comes in. The CUDA programming model allows us to program both processors with one program so that we can use the power of the GPU in our programs. CUDA supports numerous languages, but in this class we're using C. Now, part of your CUDA program is plain C and it will run on your CPU. CUDA calls this the host. The other part of your problem will run on the GPU in parallel. It's also written in C but with some extensions that we use to express parallelism. The CUDA term for your GPU is the device. Then, the CUDA compiler will compile your program, split it into pieces that will run on the CPU and the GPU, and generate code for each. CUDA assumes that the device, the GPU, is a co-processor to the host, the CPU. It also assumes that both the host and the device have their own separate memories where they store data. In the systems we use in this class, both the CPU and the GPU have their own physical dedicated memory in the form of DRAM, with the GPU's memory typically being a very high performance block of memory. Now, in this relationship between CPU and GPU, the CPU is in charge. It runs the main program, and it sends directions to the GPU to tell it what to do. It's the part of the system that's responsible for the following. One, moving data from the CPU's memory to the GPU's memory. Two, moving data from the GPU back to the CPU. Now, in the C programming language, moving data from one place to another is called Memcpy. So, it makes sense that in CUDA, this command, either moving data from the CPU to the GPU or moving data from the GPU to the CPU, is called cudaMemcpy. Three, allocating memory on the GPU, and in C this command is Malloc, so in CUDA, it's cudaMalloc. And four, invoking programs on the GPU that compute things in parallel. These programs are called kernels. And, here's a lot of jargon in one phrase. We say that the host launches kernels on the device. Okay, so let's recap with a little quiz. GPU can do the following, so mark each one that's true. One, initiate a send data from the GPU to the CPU. Two, respond to a CPU request to send data from GPU to CPU. Three, initiate or receive of data sent from CPU to GPU. Four, respond to a CPU request to receive data from CPU to GPU. Five, compute a kernel launched by the CPU. Six, and there's no box here so don't check and don't not check. Compute a kernel launched by the GPU. The typical program looks like this. First, the CPU allocates storage on the GPU. Then, the CPU copies some input data from the CPU to the GPU. Next, the CPU calls some kernels watching these kernels on the GPU that process this data. And finally, the CPU copies the results back to the CPU from the GPU. Now, two of these steps require moving data back and forth between the CPU and the GPU. Is this expensive? Well, in general, you'd like to minimize data transfer between the CPU and the GPU as much as you can. If you're going to move a lot of data and do only a little bit of computation on that data, Cuda or GPU computing probably isn't a great fit for your problem. Generally, we've found that the most successful GPU computing applications do a lot of computation and have a high ratio of computation to communication. They send their data to the GPU. They do a lot of work, and only then, they bring it back. Now allocating and transferring memory is pretty straight forward. The interesting part of GPU computing is defining the computation that actually happens on the GPU. We structure that computation as a series of one or more kernels. Now as we said earlier, the GPU has lots of parallel computation units. When you write kernels, those kernels need to take advantage of that hardware parallelism. So how do they do that? And here is the big idea. It is one of the very core concepts of CUDA. As a programmer, when you write a kernel, you write what looks like a serial program. You write this program as if it runs on a single thread. Then when you call the kernel from the CPU, you tell it how many threads to launch, and each of those threads will run this kernel. It is perfectly okay to write a kernel and then tell the GPU. Okay, when you start running this kernel, launch 100,000. Launch a million. Launch 10 million threads, each of which will run this kernel code. You can and will kick off an enormous amount of computation each time you launch a kernel. Now, we haven't covered this yet, but you might be able to make a good guess. Given this programming model, what is the GPU good at? Check all that apply. Launching a small number of threads efficiently. Launching a large number of threads efficiently. Running one thread very quickly. Running one thread that does lots of work in parallel. Or running a large number of threads in parallel. Well, the two answers here are launching a large number of threads efficiently and running those threads in parallel. So let's go into a little more detail. We've presented at a high level what the programming model looks like. Now what you need to know is what is the GPU good at? How is it going to be good at running programs I write in this programming model? So what is the GPU good at? For now, let me just tell you two things. Keep these in mind as you're planning your program. Thing number one: it is really good at efficiently launching lots of threads. You may be used to other programming environments where launching threads is an expensive process. That is not the case here. In fact, as we'll discuss later, if you're not launching lots of threads, you're not using the GPU effectively. Dave, my co-instructor, likes to say that the GPU doesn't even get out of bed in the morning for fewer than a 1000 threads. The second thing that a GPU is good at is actually running lots of those threads in parallel all at the same time. Now we're going to consider a simple example. We're going to take an input array of 64 floating point numbers, numbered 0 to 63, and we're going to square each number in the array. So the output will be 0, 1, 4, 9, and so on. We're going to do this in three steps. We're going to start by looking at how we'd run this code on the CPU. Then we'll talk about how we'd run this on the GPU without looking at code, instead just discussing what our code would do. Then we'll dive into what the code actually looks like. For the CPU code we'll actually just look at three lines of C source code without worrying about details like allocating memory or initializing the array. Here's our code. The first line is setting up the loop. We're going to loop from i = 0 until i = 63, incrementing i each time we walk through the loop. What are we going to do on each iteration of the loop? We're going to fetch the input value at array location i, multiply it times itself and store it into the output array. There's two interesting things to note about this code. One, we have only one thread of execution, and that thread explicitly loops over all of its input. We define thread here as one independent path of execution through the code. This definition is also going to apply to our GPU code. Two, note this code has no explicit parallelism. This is serial code. There's only one thread and it loops 64 times, doing one computation per iteration. Quick quiz to make sure we're all on the same page. Two quick questions. One, how many multiplications will this CPU code perform? And two, assuming that each multiplication operation takes 2 nanoseconds and everything else is free, how long will it take to compute the entire computation? Well, for the first question, we know that we're going to loop through this code 64 times, and we're going to do one multiplication for each iteration, so that's going to take 64 multiplications. And for the second question, we know that each one of these iterations is going to proceed serially, one after another after another. So for each of these 64 iterations, it's going to take us 2 nanoseconds. If we multiply those together, we'll see that the answer is 128 nanoseconds. So the GPU code will have two parts, one of which runs on the GPU, one of which runs on the CPU. So we're going to start by saying what do we have to express in the GPU part of this program. And we only need to express something very simple, the idea that out = in X in. Now this kernel that we write for the GPU doesn't say anything about the level of parallelism. If you remember, kernels look like serial programs. So the idea of the fact we're going to do this 64 times, not expressed in the GPU program at all. What does the CPU do? Well, it has to do the allocation of the memory, copying the data to and from the GPU, but the important part in terms of the computation is that the CPU launches the kernel. This is where the parallelism of threads is expressed. The CPU program will look something like, okay, let's launch a square kernel, we're going to launch 64 threads, and the arguments to those threads are an output array and an input array. Just to get our terminology straight, what we're doing here is launching a kernel called square kernel on 64 threads and each of those 64 instances of the kernel will perform one of the 64 square operations that we need to do. Now, one question that may be puzzling you is what good is it to launch 64 instances of the exact same program? So here's how we answer that question. Each thread that we launch knows which thread it is. We're going to call that the thread index. So if you launched 64 threads, one of them knows it's thread 0, one of them knows it's thread 1, and so on. Then you can assign thread number n to work on the nth element of the array. So let's summarize here. You write your kernel so that it will run on one thread at a time. Then you will launch many threads, each of which will run that kernel independently. In the last quiz, we talked about the performance characteristics of the serial program. Now we're going to look at the performance characteristics of the parallel program. The same two questions. How many multiplications will this code perform? And assuming that each multiplication operation takes 10 nanoseconds and we have enough GPU hardware resources to run all the multiplications in parallel and everything else is free, how long will it take to compute the entire computation? Well, just like in the serial code we're going to have 64 multiplications to perform, but now we can perform all of these multiplications in parallel. It's going to take 10 nanoseconds. This is a good example of the throughput versus latency debate. The latency of each operation is certainly longer than what we saw in the CPU case. But because we can do them in parallel, we both complete faster and have a higher throughput. Now, we're going to look at actual code. We're showing this in the class IDE. We're going to walk through it very quickly and then run it in a shell window. And then we'll come back and look at specific bits of the code, so you're sure you know what all of these calls do. Then when we're done with that, we're going to ask you to make some changes and run it yourself. So this is the actual class IDE. I'm scrolling up and down. Here is our main routine. That's what we're going to run. Let me briefly go through what we're looking at and then I'll switch over to a shell window and show you how it actually runs. We're going to start off by running code here that's going to generate the input array on the host. Then we will declare the GPU memory pointers, allocate the memory for those pointers, copy our host array over to the device. Here's where the magic happens. This is where we actually launch the kernel. We then take the results here and copy them back to the CPU and then we print out the resulting array, free the memory, and return 0. And if we go way back up to the top here, here's the actual kernel that we're going to run. Notice it's very simple. Recall that we write a serial program here, and we know that all that program is going to do is square its input and return it into output. Now, let's actually run this code. What we see here is me connected to a Cuda capable computer, actually in our lab at UC Davis. We're going to first look at the code itself just so you know I'm compiling the real thing. Okay, here's our program. Now we're going to compile it, and here is the command to compile it. You don't have to worry about this in the class IDE, but I'm just showing you what this looks like if you're running things on your own computer. What we see here is instead of running the regular C compiler we're running NVCC, the Nvidia C Compiler. The output is going to go an executable called square and our input file is square.cu. Dot cu is the convention for how we name Cuda files when we're saving them in a file system, okay? So, it completes pretty quickly. Then I'm going to run the program square. What's square done? It's taken our 64 element input array of the numbers 0 to 63, and it squared each one of those and printed out the output. We see 0, 1, 4, 9, 16, and so on, all the way up to 63 squared. Now, again, you'll do this in the class IDE and some of the later examples that we do in this class will show how you do it in the class IDE. But just for now I'm showing you the results of this program done in a shell window. Let's go back to our code. So, let's take a closer look at the code, line by line, so we can all be sure we know what each call does. We're going to walk through the CPU code first. The first thing we're going to do is declare the size of the array and determine how many bytes it uses. We then fill it up in this loop with floating point numbers, where array element i is simply set to i. All of this is standard C, nothing GPU-specific so far. One thing to note, though, is a common Cuda convention. Data on the CPU, the host, starts with h underscore. Data on the GPU, the device, starts with d underscore. This is just a convention. You can name your variables anything you want. But naming variables in this way helps you avoid the single most common beginner error in Cuda, where you try to access a piece of data on the CPU from the GPU, or vice versa. If you're accessing data through a pointer on the CPU, your pointer better point to something in CPU memory, or you're going to have a bad time. Same thing for the GPU. You'll find lots of Cuda code that you see uses this convention. So, let's scroll up just a little bit. And the first interesting thing that you see is how to declare a pointer on the GPU. It looks just like a pointer declared on the CPU. It's just a float star. Now to tell Cuda that your data is actually on the GPU, not the CPU, look at the next 2 lines. We're using cudaMalloc with 2 arguments, the pointer and the number of bytes to allocate. CudaMalloc means allocate the data on the GPU, whereas a plain Malloc would mean allocate the data on the CPU. The next thing we do is actually copy the data from the CPU, the array h underscore in on to the GPU, the array d underscore in. This call is cudamMemcpy--it's just like a regular Memcpy, but it takes 4 arguments instead of 3. The first 3 arguments are the same as regular C Memcpy, the destination, the source, and the number of bytes. The fourth argument says the direction of the transfer. The 3 choices are Cuda memory host to device, Cuda memory device to host, and Cuda memory device to device. Here's a code snippet that specifically shows the data movement calls, these cudaMemcpy calls. What I'd like you to do is fill in the question marks here. Your choices are going to be cudaMemcpy host to device, cudaMemcpy device to host, or cudaMemcpy device to device. Number one here is moving the host array, h_in, to the GPU array, d_in. So we need to go host to device. This argument is going to be cudaMemcpy host to device. This transfer goes the other way, from an array on the GPU, d_out, to an array on the CPU, h_out. So we're going to go device to host. CudaMemcpy device to device simply moves a piece of memory on the GPU from one place to another. Let's scroll up a little more to look at the second half of the problem. Now, that we've got all the preliminaries out of the way, how do we actually launch a kernel on the GPU? Here's a new piece of syntax in Cuda, the Cudalaunch operator. the Cuda launch operator is indicated by these three less than signs and these three greater than signs with some parameters in the middle. This line says launch the kernel name square on one block of 64 elements. Then the arguments to the kernel are two pointers, d out and d in. This code tells the CPU to launch on the GPU 64 copies of the kernel on 64 threads. Note that we can only call the kernel on GPU data, not CPU data. Since we named our GPU data to start with d_, we can visually see that we've done the right thing. Then when we're done with the kernel, the results are in d_out on the GPU, and this cudaMemcpy call will move memory from device to host and place it in h_out. The next thing we do is print it out, okay? We're just walking through the h_out array. We're printing four things per line. We're putting tabs in and then a new line after 4, and then we free the memory that we allocated on the GPU and return 0. That's all the CPU code. There's a fair amount in boilerplate in here. It looks fairly similar for most programs. Most programs are going to have you create some data on the CPU, allocate some data on the GPU, copy memory from CPU to GPU, launch some kernels that will run on the GPU, copy the results back to the CPU, and then continue to process them, print them, and so on. Now let's look at the kernel itself. Recall that this will look like a serial program that will run on one thread, and the CPU is responsible for launching that program on many parallel threads. This kernel indeed looks exactly like a serial program. So here's our kernel right here, this global void square. Here's the interesting things about this short kernel program. First global. This is underscore, underscore, global, underscore, underscore. That's a C language construct called a "declaration specifier"--for short, deckle speck. Don't worry about the name. Just know that this is the way that cuda knows this code is a kernel as opposed to CPU code. Next we have void. Void just means the kernel doesn't return a value. Instead it writes the output into the pointer specified in its argument list. This kernel takes two arguments. These are pointers to the output and the input arrays. Recall that both these pointers need to be allocated on the GPU, or else your program will crash spectacularly. Now note that I name them with d out and d in. That's certainly not foolproof, but if I called it d_ and I'm consistent about the way I allocate my variables, I know that d_ variables are allocated on the GPU. Let's walk through the body of the kernel. So the first line of the body here. Remember how I said that each thread know its own index? Here's how we get that index. CUDA has a built in variable called thread index, threadIDX, and that's going to tell each thread its index within a block. ThreadIDX is actually a c struct with 3 members. .x, .y, and .z. The c struct is called a dim3. We're just using .x in this code, but we'll explain the others a little bit later. Now, we'll launch 64 threads. For the first instance of those threads, threadIDX.x will return zero, for the second instance 1, and so on, up to 63 for the last element. Everything else in this kernel just looks like straightforward C. It looks just like a serial program. What are we actually doing in this kernel? Well, for each thread we're going to first read the array element corresponding to this thread index from global memory. We're going to store it in this float variable f. We're then going to square f, and we're going to write that value back to global memory in the output array element that corresponds to our thread index. So now lets try it. The first thing you should do is copy this code into your own IDE and run it and confirm that it works. And then as a quiz what I'd like you to do is change this program in two ways. First, instead of processing an array of 64 elements, lets process an array of 96 elements. Don't forget to change the allocation if necessary. Second, instead of computing the square of each element in the array let's compute its cube. Okay, so let's see ho we'd solve this problem. The first thing we're going to do is to change the square routine into a cubed routine and change the math accordingly. So now we're going to change square to cube. And instead of computing f x f, now we simply compute f x f x f. Now, since our new name is cube here, we are going to have to scroll all the way back down here. And instead of launching square, we've now launched q. The next thing we have to change is the allocation. Now, instead of running this on 64 elements, we want to run this on 96 elements. We've conveniently written this code such that everything is done in terms of array size, including the allocations. That should be all that we need to do. Now, let's go over to my shell window and demonstrate that this works, okay? Now we're going to do it in our own terminal window. First, I'll make sure to show you that I actually made the changes in this file. Notice that we now have a cube routine, f x f x f and now 96. Now, we're going to compile it into a cube executable. Then we're going to run it. Notice that instead of printing out squares now it prints 0, 1, 8, 27 and so on. Ninety-six cubes altogether. Okay, good work. Let's move on. Let me come back to one piece of this code that needs a little more explanation, and that's this kernel call. Here's the name of the kernel call, square, and we call it with these launch parameters with these arguments. There's a lot going on in this call so I need to explain details that we didn't need in our example that are necessary as we move forward. What I told you is that we were launching one block of 64 threads, and that's absolutely true. But let me explain what's happening under the hood in a little more detail. When you launch a kernel, you specify both the number of blocks and the number of threads per block. In our example, we only had one block of 64 threads. But we want to run bigger problems than this. What you need to know about the hardware right now is two things. One, it is capable of running many blocks at the same time. Two, each block has a maximum number of threads that it can support. Newer GPUs can support 1024 threads per block; older GPUs can only support 512. When you have lots of work to do, you'll divide that work into any number of blocks each of which had no more than 512 or possibly 1,024 threads. If we wanted to launch 128 threads and square the values in each of them instead of 64 threads, we could change this call to square of 1,128. If we wanted to launch 1280 threads instead we could call square of 10,128, launching ten blocks of 128 threads each. Or square 5,256, we're launching five blocks of 256 threads each. But we can't call square 1,1280 because that's too many threads per block. You should pick the breakdown of threads and blocks that makes the most sense for your problem. As we saw before, each thread that we launch knows its index within that block. And you won't be surprised to hear that each thread also knows the index of its block as well. We'll see how we access this information in a moment. Now how do these kernels actually map into threads and blocks? Well, when we square 10,128, we're going to launch 10 thread blocks of 128 threads each. When we square 5,256, we'll launch 5 consecutive thread blocks of 256 threads each. These diagrams are one dimensional, they only progress in one dimension, the x dimension. That's fine if your problem is one dimensional. But many problems are 2 or 3 dimensional. You'll be doing image processing in the homeworks, for instance, and that's very definitely a two dimensional problem. It makes sense that CUDA would natively support not only one dimensional layouts of blocks and threads, like we're showing here, but also 2 and 3 dimension as well. For instance, perhaps we like to process this 128 x 128 pixel image. We'd like to launch one thread per pixel. We might choose, for instance, to launch these 128 x 128 threads as 128 blocks in the y-dimension where each one of those blocks is a 128 by 1 block of threads in the x-dimension. Or we might instead choose to launch an 8 x 8 grid of blocks where each block is 16 threads by 16 threads. So CUDA supports 1, 2, or 3 dimensional thread blocks. We can also arrange thread blocks into 1, 2, or 3 dimensional grids. Now, let's return to how we launch kernels. We put two parameters in the triple chevrons. The first is the dimensionality of the grid of blocks, and the second is the dimensionality of the threads within a block. We can specify up to three dimensions for each. But if we don't specify a dimension, it defaults to one. More generally, you can specify a 3 dimensional configuration for grid of blocks or block of threads with this dim3 struct which you initialize as dim3 x,y,z. Recall that, again, if we don't specify y or z, they default to 1. When we say dim3 of w, that means the same thing as dim3 of w,1,1, and you can actually abbreviate this with simply the integer w. When we specified square of 1,64, that was equivalent to square of dim3 1,1,1, dim3 64,1,1. The most general kernel launch we can do looks like thi:, square of 3 parameters. The first is the dimensionality of the grid of blocks that has bx X by X bz blocks. Each one of those blocks is specified by this parameter: the block of threads that has tx X ty X tz threads in it, and recall that this has a maximum size. Finally, there's a third argument that defaults to zero if you don't use it, and we're not going to cover it specifically today. It's the amount of shared memory in bytes allocated per thread block. With this one kernel call, you can launch an enormous number of threads. And let's all remember, with great power comes great responsibility, so launch your kernels wisely. One more important thing about blocks and threads. Recall from our square kernel, that each thread knows its thread ID within a block. It actually knows many things. First is threaded x, as we've seen, which thread it is within the block. Here we have a block. Each thread, say this thread here, knows its index in each of the x, y, and z dimensions, and we can access those as thread idx.x, thread idx.y, and dot z. We also know block Dim, the size of a block. How many threads are there in this block along the x dimension, the y dimension, and potentially the z dimension? So we know those two things for a block. We know the analogous things for a grid. Block index for instance is which block am I in within the grid. Again dot x, dot y, and dot z. And grid Dim will tell us the size of the grid, how many blocks there are in the x dimension, the y dimension, and the z dimension. What I want you to take home from this little discussion is only the following. It's convenient to have multi-dimensional grids and blocks when your problem has multiple dimensions. CUDA implements this natively and efficiently. When you call thread at idx.x, or block dim.y, that's a very efficient thing within CUDA. Since we're doing image processing in this course, you should be counting on finding a lot of two dimensional grids and blocks. So, let's wrap up with a little quiz. Let's say I launch the following kernel. Kernel with 2 parameters dim 3 (8, 4, 2, 2) and dim 3 (16, 16). How many blocks will this call launch, how many threads per block, and how many total threads? Well, we can determine the number of blocks simply by looking at the dimensionality of the grid. We are launching this kernel with a grid of blocks and the dimensionality of the grid is 8 in the x-direction, 4 in the y-direction, and 2 in the z-direction. So we have 8 X 4 X 2 blocks, which is 64 blocks. How many threads per block? Well, we can look only at the configuration per block of threads. Here, we have a 16 in the x-dimension, 16 in the y-dimension of each block of threads. So, that's 256 threads per block. The total is 64 blocks of 256 threads per block. That's 16,324 threads that we launched with this kernel call. So we're starting to get to the wrap-up phase of this lecture. And so I want to sum up what we've learned about the programming model so far. So I want to try to sum this up in three points. First, we know that when we write a kernel program that program looks like it's only going to run on one threads. However, when we launch that program and we launch the kernel from CPU code, we specify the launch bounds of that such that that program can be launched to run on any number of threads that we define. Finally, within the kernel program, each one of those threads knows its own index and its own thread block and then the grid of thread blocks, so that it can properly pick up the right piece of work for it to compute. Now, what we're going to do is take a slightly broader view of the kind of operations that we just did in our sample program. And what we're going to have is two ingredients that we've learned that are going to lead to an interesting abstraction that we're going to call map. Only one of these can be solved using map, and that's adding one to each element in the input array. Why can we do that? Each individual element in this input array, is processed in parallel. We can't sort an input array using map because the output at any position is dependent on all the input data. Similarly, to sum up all the elements in an input array, requires that we look at all of the separate elements to compute a single output. The individual elements are not parallel. Finally to compute the average of an input array, also requires this all the one style of communication. We don't have in parallel things that happen independently without any communication. Now, all these other operations are very, very interesting in GPU computing, and we will absolutely be covering them in the next few units. To wrap up, today we learned about not only that the world is becoming parallel but why. The GPU is a processor that takes advantage of some of the most important technical trends in computing. We looked at our first CUDA program, learned how it worked, and changed the program to do something new. Now for our first project, we're going to use CUDA to input a color image, and output that image in grayscale. This will involve a map operation over every pixel in the image, where the operation on each pixel is converting the input color in red, green, and blue to a single intensity value. When you're done, you'll have the chance to show your grandparents what your new photos would've looked like back in the day, or at least that's what I'm going to do. Enjoy the assignment, and I hope you enjoy an excellent presentation from Dave for Unit 2. Congratulations and good job! You've made it to the end of Unit 1. You've learned a lot about parallel computing on the GPU already, both the fundamentals of the GPU programming model, and how to write your first GPU program. Now we're going to talk to NVIDIA Chief Scientist and Senior Vice President of Research, Bill Dally. Bill is one of the pioneers of parallel computing, so I hope you enjoy his perspective as much as I do. After the interview, we can dive into problem set 1, where you'll write a program that will transform a color image into a gray scale image, and we'll provide a lot of the surrounding code for this assignment, but you're going to write the most important part; the part that actually does the parallel processing on the GPU. Welcome to Problem Set #1. I'm Chin Hun and I'm your TA for the class. Today I will be walking you through Problem Set #1. And for Problem Set #1, our goal is convert an image from color to black and white. To convert an image from color to black and white, we first have to understand how colored digital images are represented. The most common format is to specify how much red, green, and blue is presented at each pixel. Each color is called a channel. Zero means that a color is fully absent, whereas 255 means that color is fully saturated. Therefore, if at a pixel all three channels—red, green, and blue— are zero means that pixel is black, like such. Or if all three channels—red, green, and blue—are 255 that pixel is fully white, just like the background of this slide. So how are pixels represented in CUDA? In CUDA code each pixel is represented as an unsigned char4 struct. This structure has four unsigned char components named x, y, z, and w. X is your red channel, Y is your green channel, Z is your blue channel, and W is your alpha channel. The fourth component, char W here, is reserved for the Alpha channel, which carries the transparency information. We will ignore this component throughout our homework, as this does not pertain to transforming the image from color to greyscale. How do we convert a color image to a black and white image? One possibility was simply to take the average of all of the three color channels. For example, add R to G to B— the channels R stands for red, G stands for green, and B stands for blue— and simply divide it by three. But it turns out that our eyes don't respond to all colors equally. We're more sensitive to the color green than red and more sensitive to the color red than to the color blue. Taking into account our varying sensitivities to the different color channels, we are going to use this following formula instead. We are going to multiply the color channel red by 0.299, we are going to multiply the color channel green by 0.587, and we are going to multiply the color channel blue by 0.114. So let's look at Problem Set #1, and let's look at the code that you will actually have to write for this assignment. For Problem Set #1, you have two jobs that you have to do. For the first part, your job is to fill in this RGBA to greyscale function. We will pass you in an array of uchar4 which represents the pixel in the color image, and your job is to convert each pixel in this 2D array into an intensity value that you will write back to this 1D array called greyImage. You will follow the formula that we talked about in the previous slide, as seen here. In part two, the kernel's already filled in for you but with incorrect dimensions and spot is empty. So your job for part two is to fill in the body of the kernel and to correct the dimension of block size and grid size. We have included reference code that performs the same calculation of serial on the CPU for your reference, and if you have any questions or if you run into any problems, feel free to post on the forums, and I'll be there to help you. Lastly, I would like to thank Eric Elson of Royal Caliber and Mike Roberts for their fantastic job with writing the homeworks. They have also written the script for this video today as well, so thank you Eric and Mike. Welcome to unit 2. It's good to see you again. In the last unit you learned about the fundamentals of the GPU programming model and the basics of writing a simple program using CUDA. In this unit we're going to build off of that. We'll learn about important parallel communication patterns like scatter and gather and stencil. And we'll dive a little deeper into the GPU hardware and learn about things like global memory and shared memory, and we'll put these together to learn how to write efficient GPU programs. Let's recap what we've learned so far. Parallel computing is all about many threads solving a problem by working together. The key is this working together. Any books on business practices or teamwork will tell you that working together is really all about communication. In CUDA, this communication takes place through memory. For example, threads may need to read from the same input location. Threads may need to write to the same output location. Sometimes threads may need to exchange partial results. Let's talk about the different kinds of communication, the different patterns of communication you'll see in parallel computing. And as you'll see, this is really all about how to map tasks and memory together-- how to map tasks, which are threads in CUDA, and the memory that they're communicating through. The communication pattern you've already seen is called map. Now with map, you've got many data elements such as elements of an array, or entries in a matrix, or pixels in an image, And you're going to do the same function or computational on each piece of data. This means each task is going to read from and write to a specific place in memory. There's a 1 to 1 correspondence between input and output. So map is very efficient on GPUs, and it's easily expressed in an efficient way in CUDA by simply having 1 thread do each task, but this isn't a very flexible framework. There's many things you can't do with a simple map operation. Now suppose that you want each thread to compute and store the average across a range of data elements. Say maybe we want to average each set of 3 elements together. In this case, each thread is going to read the values from 3 locations in memory and write them into a single place and so on. Or suppose you want to blur an image by setting each pixel to the average of its neighboring pixels, so that this pixel would average together the values of all 5 of these pixels, and then let's take this pixel next to it, would average together the values of all these pixels and so on. We'll do exactly this kind of blurring operation in the homework assignment that's coming up at the end of this lecture. This operation is called a gather, because each calculation gathers input data elements together from different places to compute an output result. Rather than having each thread read 3 neighboring elements, average their value, and write a single output result, we can have each thread read a single input result and add 1/3 of its element's value to the 3 neighboring elements. So each of these writes would really be an increment operation. You can imagine the same thing on our 2D image blurring example, where each thread takes 1 input element or pixel and writes a fraction of its value to the neighboring pixels. So when each parallel task needs to write its result in a different place or in multiple places, we call this scatter because the threads are scattering the results over memory. You can see already a problem that we're going to have with scatter. You've got several threads attempting to write to the same place at more or less the same time. This is something we'll have to talk about later. Let's have a quick quiz on this. Suppose you have a list of basketball players. So you've got a bunch of records and each one has the name of the player and the height of the player, and the rank in the height, okay? So in the league or on the team, whether this is the first tallest, the second tallest, the third tallest, the last tallest, or so on, okay? So you've got the rank and height. And say that your goal now is to write each player's record into its location in a sorted list. So if we implement this in CUDA by having each thread read a record and look at the rank and use that rank to determine where to write into the array, is this a map operation or a gather operation or a scatter operation? And the correct answer is that it's a scatter operation. Each thread is computing where to write its result. So the image blurring example that we've been using actually illustrates another important kind of communication pattern called Stencil. Stencil codes update each element in an array using neighboring array elements in a fixed pattern called the stencil. This is the stencil pattern we saw before. It's technically known as a 2D von Neumann stencil. Let me reiterate how this worked and, this time use color coding a little differently to show you what's going on. So here, I've color coded the threads to show you which one is going to be working on which output element. So I'll choose the blue one to be writing into this value. Here's where the red one's going to write its output value. Here's where the green thread will write it's output value. And if you look at what's going to happen, each of these threads is going to read from several locations in memory surrounding the corresponding input value, and those locations are defined by the stencil. So the blue thread will do a gather from these threads, and then the red-colored thread will do a gather from the overlapping neighborhood. And then the green thread will do a gather from this neighborhood, and so on. Eventually there'll be some other thread whose responsible for, say, writing to this value, and that thread is going to go and access these values. So something you're going to notice right away is that there is a lot of data reuse going on. Many threads are accessing and computing from the same data. And exploiting that data reuse is something we're going to use later on, when you're working on your homework assignment. We're going to try to exploit that reuse to speed up your homework assignment. Now there are other common stencil patterns. For example, you might read from all of the neighboring elements, including the diagonal elements, and that would be called a 2D Moore pattern. And there are also 3D analogs of these, so for my next trick, I'm going to attempt to draw a 3D von Neumann stencil, which is sort of kind of a cross shape extending in all 3 dimensions. Hopefully you can see that from my drawing. So speaking of data reuse, here's a quick quiz. Can you figure out how many times a given input element in the array will be read when applying each of these stencils? The answer, of course, is simply the number of elements in the stencil,right? So every element in that array is going to be read 5 times by the 2D von Neumann stencil because there are 5 entries in the neighborhood. So, all elements will be read 5 times by the 2D von Neumann stencil, 9 times by the 2D Moore stencil, and seven times by the 3D von Neumann stencil. Another parallel communication pattern worth mentioning is called transpose. For example, you might have a 2D array, such as an image laid out in row-major order. This means that the elements of the array, or the pixels of the image, are laid out one row at a time. And I've color-coded the rows here just to show you more clearly what I'm doing. But you might want to do some processing on the columns of this image. And so you'd want a lay out like this. This means you need to do an operation to reorder the elements. As you can see, I've drawn this as a scatter operation. So each thread is reading from an adjacent element in the array, but is writing to someplace scattered in memory, according to the stride of this row column transpose. I could also have expressed this as a gather operation, like so. So you can see where a transpose might come up when you're doing array operations, matrix operations, image operations. But the concept is generally applicable to all kinds of data structures. Let me give an example. Here's some sort of structure you might have, right? It's a perfectly reasonably structure foo. It's got a float field and an integer field. And say that you have an array of a thousand of these. Well what does that look like in memory? You're going to have the floats and the integers interspersed throughout memory. And as we will talk about later, it can be more efficient to access-- if you're going to do a lot of processing on the floats, it can be more efficient to access all of the floats contiguously. You're going to want some operation that lets you take your-- what's called an array of structures representation, and turn it into a structure of arrays. And that operation is, again, a transpose. By the way, these two terms are so common that array of structures is often abbreviated AOS. And structure of arrays is often abbreviated SOA. You'll see these terms come up frequently in parallel computing. So, to summarize, the transpose operation is where tasks reorder data elements in memory. Okay, let's have a quiz on communication patterns. I'm going to give you a bunch of code snippets and I'm going to ask you to label them according to the parallel communication pattern that they embody. For each code snippet, you should indicate whether it is a map operation, a gather operation, a scatter operation, a stencil operation, or a transpose operation. Here's the code, and this is really sort of pseudo code. I'm not explaining where these variables came from, or showing you that how many threads I'm watching, or anything like that. But this is kernel code, and as you can see, I have two variables, out and in. These are arrays, the floating point numbers. And just for berevity, I've created two variables, i and j, to represent thread Idx.x, and thread Idx.y. Just to have something to do, I'm going to multiply a bunch of numbers by pi. So I define pi here, and here are our code snippets. Out i equals pi times in. Out i plus j times 128 equals in j times, j plus i times 128. And then, you see these 2 I have guarded with an if statement, but only the odd threads get executed. Out i minus 1 plus equals pi times in i, out i plus 1 plus equals pi times in i. Finally, out i equals in i plus in i minus 1 plus in i plus 1 times pi divided by 3. So for each of these statements, each of these little code snippets indicate whether it's a map, a gather, a scatter, a stencil, or a transpose. So this first one is pretty easy, right? There is a 1-to-1correspondence between the output and the input, so that's clearly an app operation. And this next one is also is 1-to-1 operation. One value gets written in the output array corresponding to every value that gets read from the input array. And you can see that we're writing into an array which is represented in I major order here in the output, and in j major order in the input. So this is a transpose operation. Now this next code, as I said, I put a guard around. Only odd the numbered threads are going to execute this. So that rules out a map, it's not 1- to-1. And that also rules out a transpose operation, which is also 1-to-1. And you really couldn't call it a stencil operation either because a stencil operation should generate a result for every element in the output array, and this doesn't do that. Now if you look the first one, the thread is taking the input at a given location, and multiplying it by pi, and placing that into a couple of different places in the output array. In fact it's incrementing a couple different places in the output array. So this would be a scatter operation. The thread is computing for itself where it needs to write its result. And this final line would be a gather. You can see that every thread is writing a single location in the output array and it's reading from multiple places in the input array, locations that it computes. So this would be a gather. And again, this looks very much like a stencil operation since it's reading from a local neighborhood, and doing some averaging, and writing the result. But I wouldn't call it a stencil because it's not writing into every location because of this guard here. So that's why I refer to this as a gather rather than a stencil. Conceptually map and transpose are 1-to-1. Each input maps to a single unique output. You can think of a gather operation as many-to-1. Many possible inputs can be chosen to compute an output. In this terminology, scatter is 1-to-many, so each thread chooses from many possible output destinations. Stencil can be seen as a specialized gather that pulls output from a select few inputs in a given neighborhood of the output. So you might turn that to several-to-1. In the next lecture John will tell you about 2 more really fundamental patterns. Reduce could be turned all-to-one. For example if you're adding up all the numbers in an array. Finally scan and sort can be considered all-to-all because all of the input can affect the destination of the resulting output. You'll learn more about these in the next lectures. Okay, now we've talked about parallel communications patterns. Check that off. Now, that we've discussed the general forms of communication that we'll see among the code of threads in our parallel programs, let's talk about how those threads can most efficiently access memory in concert. An important subtopic of this is going to be how to exploit the data reuse that we saw during all those gathers and stencils codes. There were a lot of threads often accessing the same data at the same time and how can we exploit that to reduce the total amount of time spent on memory? The next big topic is how can threads communicate partial results by sharing memory? A real problem then becomes once threads are sharing memory and reading and writing from the same memory how do you keep them from stomping on each others' memory accesses? How can you do this safely? In order to better understand all of these topics, we need to dive a bit deeper and learn more about the underlying GPU hardware. In this lecture we're going to talk about GPU hardware and how that affects the programming model. Let's summarize what we've learned so far about the GPU programming model and start adding to it. In our last lecture, we learned about the programmers view of the GPU. I'm going to summarize that and then start adding some more details to it. Your job as the programmer is to divide up your program into smaller computations called kernels. A kernel in our case is a C or C++ function. The key idea of a kernel is that it's going to be performed by many threads. A thread is a path of execution, a thread of execution through the program code. I've drawn them as wiggly lines here because they're not necessarily all going to take the same path through that code. There might be branches like if statements or switch statements. There might be loops--for loops, to loops. You don't know in advance what path you're going to take. This is not straight-line code. In fact, different threads might take different paths. There might be a thread which takes a completely different path. The key thing about threads is that they come in thread blocks. A thread block is a group of threads that cooperate to solve a sub-problem. A GPU program launches many threads to run one kernel, and then they all run to completion and exit. The program launches many threads to run the next kernel like bar. Another thing to notice about the way I drew this is that, you notice I have a different number of thread blocks with a different number of threads. That's actually something you can pick for each kernel. And we'll talk later why you would choose different numbers of threads, different number of thread blocks, how you would organize these. That's the programmer's view of the GPU. Let's talk about thread blocks and why they're there. Hopefully, you have some questions about how the GPU works, since we've just sort of been telling you about threads and thread blocks without giving you much motivation. You might be wondering why do we divide the problem into blocks. When do blocks get run? And if you have a whole bunch of thread blocks, in what order do they run? And I've told you that thread blocks were about letting groups of threads cooperate. But I haven't told you how those threads cooperate or with what limitations. To answer this we're going to have to dive in to learn a little bit more about the GPU hardware. At a high level, a Cuda GPU is a bunch of these. We call them streaming multiprocessors or SMs for short. Now, different GPUs have a different number of SMs. A small GPU might only have one SM whereas a really big GPU might have 16 SMs, for example. An SM in turn has many simple processors that can run a bunch of parallel threads. It also has some other things like some memory that we'll talk more about in a moment. When you've got a Cuda program with a bunch of threads organized into thread blocks, the important thing to understand is that the GPU is responsible for allocating blocks to SMs. Let me say that again because it's really important, the GPU is responsible for allocating the blocks to the SMs. As a programmer, all you have to worry about is giving the GPU a big pile of thread blocks, and the GPU will take care of assigning them to run on the hardware SMs. All the SMs run in parallel and independently. Check all the true statements. A thread block contains many threads. An SM might run more than one thread block. A thread block may run on more than one SM. All the threads in a thread block might cooperate to solve a subproblem. All the threads that run on a given SM may cooperate to solve a subproblem. Yes, a thread block contains many threads, and it's also true that an SM might run more than one block. On the other hand, a thread block may not run on more than one SM. By definition, a thread block is run on a single SM. Now, all the threads in a thread block may cooperate to solve a subproblem. That's why we have thread blocks. This last one is false, and it's a bit subtle so let me talk it through. The key, as I said, all the threads that run on a given SM may cooperate to solve a subproblem, but in fact you might have multiple thread blocks on an SM, and by definition threads and different thread blocks should not attempt to cooperate with each other. And that's why this statement is not the same as the one before it. As a review, the programmer or the GPU is responsible for defining thread blocks in software. And the programmer or the GPU is responsible for allocating those thread blocks to hardware streaming multiprocessors or SMs. And of course the correct answer is that the programmer is the one writing software. The programmer's job is to define those thread blocks. The GPU is running the hardware. And the GPU is completely responsible for allocating those thread blocks to run on hardware SMs. One more quiz to drive this home. If we have a single kernel that's launched on many thread blocks, including x and y, the programmer can specify that block x will run at the same time as block y, that block x will run after block y, that block x will run on sm z. The answer of course is that all of these are false. There are no such guarantees. So in short, CUDA makes few guarantees about when and where thread blocks will run, and this is actually a huge advantage for CUDA. This is one of the big reasons why GPU programs can go so fast. Why is that? So among the advantages that the GPU gains from this programing model is that the hardware can run things really efficiently, because it has so much flexibility. For example, if 1 thread block completes quickly, the SM can immediately schedule another thread block without waiting for any others to complete. But the biggest advantage is scalability. Because you've made no guarantees about where the thread blocks will run or how many thread blocks might be running at a time, that means that you can scale all the way down to a GPU that would be running with a single SM, something that you might find in a tablet or a cell phone, all the way up to the massive GPUs that are used in supercomputers. Importantly, the scalability also applies to future GPUs, so you can be sure that GPUs will get more and more SMs as Moore's Law gives us more and more transistors on a chip, and by writing the code in such a way that it can run on an arbitrary number of SMs and doesn't depend on a certain number of SM's, a certain number of thread blocks being resident at a time, you can be sure that your CUDA code will scale forward to larger and larger GPUs. So this scalability applies from cell phones to supercomputers, from current to future GPUs, and that's a really huge advantage. And there are also consequences to this programming model, which are the things we've been talking about. You can make no assumptions about what blocks will run on what SM, and you can't have any explicit communication between blocks. For example, if block x is waiting for block y to give it some result before it can proceed, but block y has already run to completion and exited, then you're going to be in a bad place. That, by the way, is an example of something called dead lock in parallel computing. This really means that threads and blocks must complete, okay? You can't simply have a thread that hangs around forever, processing input or doing something, because that thread must complete in order for the block that it's in to complete so that other threads and blocks can get scheduled onto that SM. Okay, here's a simple CUDA program that'll illustrate these ideas. So we're going to launch 16 blocks, and each one's going to have a single thread running, and it's going to run a trivial kernel that just prints, "Hello world! I'm a thread in block." And then in the main, we're going to do nothing more than launch that kernel. You need this call cudaDeviceSynchronize to make sure that these print f's flush and then we'll print, "That's all." So before we run this, here's a quick quiz. How many different outputs do you think different runs of this program can produce? Is it 1? 16 possible different outputs? 2 to the 16th possible different outputs? Or 21 trillion possible outputs? Well as we'll see, the correct answer is actually 21 trillion, which is 16 factorial. Let's see how that works. So here I'm just going to run this program in my terminal, and as you can see, the blocks complete in some basically random order: block 7 completes, then block 6, then 1, then 0, and so forth. If I run it again, I get a different order and a different order and so forth. You'll find that the blocks executed in a different order every time, which is where that number 16 factorial comes from. So we spent so long talking about what CUDA doesn't guarantee, you may be wondering what CUDA does guarantee. There are 2 main things. Again, recall that all the threads in the block are guaranteed to run on the same SM at the same time. Second, all blocks in the kernel finish before any blocks from the next kernel are launched. This goes back to our programming model diagram where you'll remember, we had some threads running one kernel, say foo, and then some threads running another kernel, let's say bar. So what CUDA is promising is any threads running foo will finish in their entirety before any threads running bar are launched. Now we're going to talk about the memory model. Let's go back to our diagram and draw the threads and blocks and kernels. So every thread has access to something called local memory, and this is memory that's private to that thread, things like its local variables. So the thread can read and write from local memory, and the threads in the thread block also have access to something called shared memory. So all the threads in this thread block can read and write to the per block shared memory. It's important to understand that shared memory is shared among the threads in a block. This is a small amount of memory that sits on the SM directly. And finally, there's global memory; every thread in the entire system at any time can read and write to global memory. So threads in 1 kernel can read and write from it. Threads in a later kernel can also read and write from it. So every thread, to recap, has access to its own local memory, has access to shared memory that's also accessible to threads specifically in its thread block, and to global memory, which is accessible to all of the threads everywhere. Of course, everything we've been talking about is on the GPU, but there's also a CPU. The CPU thread launches the work on the GPU, and of course the CPU has access to its own memory which in CUDA we call host memory. Usually data is copied to and from this host memory into the GPU's fast global memory before launching a kernel to work on that data. We'll talk later about how CUDA threads can access host memory directly. Let's have a quiz. Go ahead and check all the statements that are true. All threads from a block can access the same variable in that block's shared memory. Threads from 2 different blocks can access the same variable in global memory. Threads from different blocks have their own copy of local variables in local memory. Threads from the same block have their own copy of local variables in local memory. So it turns out these are all true. All threads from a block can access the same variable in that block's shared memory. That's what the shared memory is. It's a chunk of memory that stores variables that are accessible from all threads in a given block, and threads from 2 different blocks can access the same variable in global memory. Well all threads from any blocks at any time can access a variable or piece of data that's sitting in global memory, so this is also true. Threads from different blocks have their own copy of local variables in local memory; yes, this is true. So every thread has its own copy of whatever local variables-- private variables to that thread are sitting in local memory, so this is true. And threads from the same block have their own copy of local variables in local memory. Right, so just because they're from the same block doesn't mean that they share local variables. They share shared memory--per block shared memory, but they don't share local memory. So all 4 of these are true. So now we know that threads can access each others' results through shared and global memory. This means they can work together on a computation, but there's a problem. What if a thread tries to read a result before another thread has had a chance to compute or write it? This means we need synchronization. Threads need to synchronize with each other to avoid this situation. This need for synchronization is really one of the most fundamental problems in parallel computing. Now the simplest form of synchronization is called a barrier. A barrier is a point in the program where all the threads stop and wait. When all the threads have reached the barrier, they can proceed on to the rest of the code. Let's illustrate this. Here's some threads and they're all proceeding along through to code. I'll draw them in different colors, and I'm also drawing them different lengths so that you get the idea that they're at different places in the code. They're at different points in their execution of the program. The idea is that when they reach the barrier, they're going to stop and wait for all the others to catch up. So in my drawing, the red one reaches the barrier first and stops. In the meantime, the blue one is proceeding along, and the green one is proceeding along, and eventually the blue one arrives at the barrier and stops, and the green one is the last one to arrive at the barrier and stops. And now all 3 threads--in my example, say I only have 3 threads. Now that all of the threads have arrived at the barrier, then they're all free to go again. And so they'll all proceed again, and we don't actually know which one's going to go first. It might be that the blue one is the first out of the gate, maybe green is next, maybe red is last. So let's look at some code to illustrate this. Here's an example of the need for barriers. Say we've got an array in memory with a bunch of elements: 1, 2, 3, 4, 5, 6, and we want to shift each of these left by 1. In other words, we want this element go here, this element go here, this element go here and so on. So here's a little code snippet that should do this. We first initialize the elements of array to the thread index, and you'll see that I've hardcoded of this to 128 just to be lazy. So every thread is going to set its corresponding array element to its own index. So this should initialize things to 1, 2, 3, 4, 5, 6 and so on. And then avoiding stepping off the end of the array with this if statement, every thread will set its corresponding array element at its index equal to the value of the array element at the thread's index plus 1. So thread 1 will set its value to whatever is written in array sub 2. Thread 2 will set array sub 2 equal to whatever is in array sub 3 and so forth. So here's a quick quiz: How many barriers does this code need? So, the answer is 3. Why is that? Well, here's the code. Let's take it step by step. First of all, every thread is setting a local variable equal to it's own thread index. Then in this line, we're declaring a shared variable, which is going to be shared by all of the threads in the thread block. I just hardcoded in 128 elements here. And now we're actually going to do something. We're going to write every thread is going to write into a single element of the array corresponding to it's thread index. And the thing that it's going to write into that element of the array happens to be its own thread index, okay? So, this is going to load up that array with one, two, three, four, and so on. And that's just to put something in the array that we can play with. This operation is right. So, we need a barrier after this write completes in order to make sure that nobody tries to read from the array until after all of the threads have finished the write operation. So, let's insert a barrier here. And now, all of the threads that pass this if statement are going to do this shift operation, right? They're going to set the array at their index to the array at their index plus one. But if you think about it, this is really a read and a write operation. Here was our first write operation. That was number 1. And now, we're doing a read operation. That's number 2. Okay, we're going to read from the array at index plus 1. And then, finally we're going to write to the array at index. And that's operation three. And the tricky thing about this is that all of these reads should complete before we allow any of these writes to finish. So, really we need to restructure this code. We'll make this read operation explicit, put in a local temporary variable. We'll call the same threads. We'll perform the write operation explicitly, and then just to be absolutely safe before anybody can use these written values. We needed another synchthreads to ensure that all of these write operations have completed before anybody attempts to do anything with this array value afterwards. So let's recreate our programming model diagram. We've got threads, and we've got thread blocks. And really what is happening with these barriers is that the sync threads call is creating a barrier within a thread block. So all the threads within this thread block are going to get to this sync threads call, stop, and wait until they've all arrived and then proceed. And these thread blocks are organized in kernels; every kernel has a bunch of thread blocks. We haven't talked about this really, but there's an implicit barrier between kernels. So if I launch 2 kernels, one after another, then by default kernel a will finish before kernel b is allowed to start. So all of these threads will complete before any of these threads get launched. So when you add in our memory model that we've talked about, where every thread has access to its own local memory, to its block shared memory, and to the global memory shared by all threads and all kernels in the system, then what you've got is CUDA. At its heart, CUDA is a hierarchy of computation. That would the threads, thread blocks, and kernels with the corresponding hierarchy of memory spaces: local, shared, and global, and synchronization primitives: sync threads, barriers, and the implicit barrier in between 2 synchronous kernels. So let's have a quiz. Are the following code snippets correct? Here's some function, some kernel, and it's going to declare an array of ints and shared memory--1,024 ints, and then for convenience we'll define i to be equal to thread index, and then we've got a bunch of functions. And I want you to check the ones that are going to run correctly without additional sync threads. And let's go ahead and put a sync thread before and after each of these code snippets, because it's really the lines here themselves that I'm asking you about, and we can assume there is a sync thread ahead of this and a sync thread after. This first example is pretty much the same thing as we already saw, right? Every thread is going to be reading from s of i -1 and then writing the result in the s of i, for it's own value of i. And there's nothing in this statement to guarantee that all of these reads complete before any of these rights complete. And since they can be one thread's s of i sub 1 is another thread's s of i, then there's absolutely the potential for a collision here. The correct way to write this, as before, would be something like this. Where we go ahead and insert a syncthreads and a temporary variable to separate the read and the write. So what about this second example? Here, what this if statement does is it basically insures that only the odd threads are going to try to read from s of i sub 1 and write to s sub i. You think about this, thread 1 is going to be reading from location 0 and then writing to location 1. Thread 2 does nothing. Thread 3 reads from location 2 and writes to location 3. Thread 4 does nothing, thread 5 reads from location 4 and writes to location 5 and so on. So in this case there are no conflicts so this code is actually correct even without any additional internal sync threads. Our final example is, again, not correct. It's similar to the first one. Every thread is going to read from locations I - 1, I, and I + 1. It's going to do a little math, and then it's going to store the result in location i. Clearly to make sure that all of these reads happen before any of these writes, we're going to need a sync threads. So to make this code correct, we'd have to do something like this. We'd have to use a temporary variable, do all of the reads, sync, do the write, sync, and then do the printf. The writes have to complete before this printf is done to make sure that the printf prints the correct value. In the next homework, we're going to implement some pretty cool image blurring techniques. You now know enough that you can go and implement that program on a massively parallel GPU, and you'd get a correct answer, and it would be pretty fast. But we can do better. Now we have all the ingredients to start talking about writing efficient parallel programs in CUDA. For now I'm only going to talk about high level strategies. We're going to have a whole unit later on about detailed optimization approaches to help you really squeeze the maximum performance out of the GPU. So think of this as a preview that covers some of the really important things high level things that you have to keep in mind when you are writing a GPU program. So the first thing to keep in mind is that GPUs have really incredible computational horsepower. A high-end GPU can do over 3 trillion math operations per second. You'll sometimes see this written down as T FLOPS, okay? A FLOPS stands for a Floating Point Operation Per Second. T FLOPS is Tera-FLOPS, and minor GPU's can do over 3 trillion of these every second at the high end. But all that power is wasted if the arithmetic units that are doing the math need to spend their time waiting while the system fetches the operands from memory. So the first strategy we need to keep in mind is to maximize arithmetic intensity. Arithmetic intensity is basically the amount of math we do per amount of memory that we access. So we can increase arithmetic intensity by making the numerator bigger or by making the denominator smaller. So this corresponds to maximizing the work we do per thread or minimizing the memory we do per thread. And let's be more exact about what we mean here. Really what we're talking about is maximizing the number of useful compute operations per thread. Really what we care about here is minimizing the time spent on memory per thread. And I phrased this carefully because it is not the total number of memory operations that we care about and it's not the total amount of memory that comes and goes in the course of the thread executing its program. It's how long it takes us to do that, so there are a lot of ways to spend less time on memory accesses. And that's what we're going to talk about now. Let's talk about ways to minimize time spent on memory accesses. The first strategy to think about is move frequently accessed data to fast memory. We've talked about the memory spaces available to the GPU. There's local memory, which represents a given thread's private variables, local variables, parameters, things like that. There's shared memory, shared by a thread block. There's global memory shared by all the threads. And more or less, it's true to say that local memory is faster than shared memory, which in turn is faster than global memory. In fact, shared memory and local memory are usually much faster than global memory. I should mention that there are subtleties here. For those of you who know something about computer organization, the reason why I'm labeling local memory as so fast is that it tends to live either in registers or in l1 cache, and those are both quite fast. This isn't a hard and fast rule. There's some subtleties here. But in general data that is kept local to a thread is going to be about as fast as possible. Data that is shared in a thread's thread block shared memory is going to be very fast, and data that is way out in global memory is going to be a lot slower, although this is still much faster then CPU memory, also known as host memory. Let's see an example of using local, shared, and global memory. Here's a kernel. I know that it's a kernel because it starts with either device or global. It's called use local memory GPU, it has one parameter called in, and it's got one local variable called f. Because this is a local variable it's in local memory it's private to this thread, every thread will have its own copy of a variable named f. And parameters are also local memory. Every thread will have it's own copy of a parameter called in. You know real code would presumably do something with these variables. But since this is just an example of how to use local memory, I don't need to do that. How would you call a kernel that shows using local memory? Well, you would call a kernel, you would have to launch it, meaning tell the GPU how many thread blocks to run with how many threads. You pass in any parameters. So in this case, 2.0. Pretty simple. Once again we've got a kernel. We know it's a kernel because it's been tagged with global, so it's going to run on the GPU but can be called from the host. Once again we're going to pass in a local variable, a parameter called array. And the trick is that this parameter is a pointer, and so this is actually pointing to some global memory that we've allocated elsewhere. I'll show you how to do that in a moment. Once you've got a pointer to global memory, you can manipulate the contents of that memory just as you would manipulate any other chunk of memory. In this case, I'm going to take my array, and I'm going to set one of its elements which happens to be equal to the index of this thread to some number which happens to be 2.0 times the index to this thread. Again not a very useful function but it illustrates what's happening. So the point really is that since all the parameters to a function are local variables, are private to that thread, if you want to manipulate global memory you're going to have to pass in a pointer to that memory. Of course that means you're going to have to allocate a pointer, so let's look at how that works. Here's the code to show off how we use global memory. The first thing I'm going to do is to clear some host memory. Once again I'm using a convention that's starting a variable with a prefix H_ indicates that this is memory that lives on the host. Hhere's an array of 128 floats. I'm also going to declare a pointer that I'm going to use to point to the global memory that I allocate on the device. And once again the d_ convention indicates that this variable is on the device. Now I want to allocate some global memory on the device. I'm going to use the function cudaMalloc. What's happening here is that I'm passing it a pointer to this variable which is itself a pointer. Right? And cudaMalloc is going to allocate some memory in this case room for 128 floats, and stuff a pointer to that memory into the pointer d array. If you're allocating memory you'll probably initialize to something. We use cudaMemcpy for that operation. In this case we pass in a pointer to the destination memory, which is this d array that we've allocated, and a pointer to the source memory, which is this h array variable. Then the number of bytes allocate, and then we indicate whether we're copying from the hosted device, or vice versa. Oops, this is a bug. Now we've got a chunk of global memory, we've put something in it, and now we're ready to launch the kernel that's going to operate on that global memory. So here's the kernel that we saw earlier. Again, we're going to launch a single thread block, consisting of 128 threads. I'm going to pass in this pointer where I've allocated and initialized memory. After this runs, presumably this code will do something to that memory that I pass in, and now I'll need to copy it back onto the host. If I want to use the results of this kernel back on the host, then I need to copy the memory back into host memory. Here's that operation, once again cudaMemcpy. This time the destination is h array. The source is d array, this same number of bytes. Now we're copying from device to host. That's how you use global memory. The trick is that, since you can only pass in local variables in a kernel, you have to allocate and initialize global memory outside the kernel and then pass in a pointer. Finally, let's look at how you would use shared memory. This example's a little more complicated. For clarity, remember that I'm just hard coding the idea that there's 128 threads and therefore we're going to operate on 128 elements of the array, all right? And I'm going to skip all this sort of out-of-bounds check and assertions that you would normally use to make sure that you're not trying to access a piece of memory that's not there. So once again, we have a function, use shared memory GPU. This function is a kernel, and we're going to pass in a local variable, which is a pointer to a bunch of floats. And, this local variable is a pointer to global memory that's been allocated in advance. I wanted to come up with some code that actually did something using shared memory, and that's why function is a little more complicated than the examples you've seen. So, we're going to declare a few local variables that are private to each thread, a couple of indices we'll declare this variable index, just to be shorthand for thread index dot x. That's just to save some typing. And we're going to declare a floating point variable called average, and another one called sum. And we'll initialize sum to 0.0. Here's the big example. Now we're going to declare a variable that lives in shared memory. In this case, it's an array. Again, I've hard coded that array to contain 128 elements. It's an array of floats. And I tag it as being a shared memory with this shared_declspec. And remember the whole point of shared memory is that this variable is visible to all the threads in the thread block. And it only has the lifetime of the thread block. So it doesn't exist outside of this thread block. Every thread block is going to have its own copy, it's own single copy of this array that all of the threads in that thread block can see. I call it sh_array. The first thing we're going to do is put some data in this array. Remember I passed in this array that's in global memory called array, and I'm basically going to initialize the shared memory array to contain exactly what's in the global memory array. Notice how I do it. I'm copying data from this array in global memory to this array in shared memory. And I'm doing it with a single operation. How does this work? If you look at this code, every thread is figuring out what its own index is, okay? Which thread it is, and it's copying the element at array sub index into the element at shared array sub index. Okay, so since all of the threads in the thread block will be doing exactly this operation and since they will all have different values for index, when this single line has completed in all the threads then we'll have copied all of the information in this global memory array into our shared memory array. Okay, and the trick is that operation is going to take some time. Multiple threads are running. They're not running all at the same time, so it won't happen instantaneously. We have to make sure all of these writes to shared memory have completed before we go on and do anything with that array in shared memory. And that's what the syncthreads operation is about. You've seen this before. We call our barrier to make sure that all the writes to shared memory have completed. Now after that barrier we're guaranteed that this shared memory array is fully populated. Every element has been initialized now. Just to be doing something, I said let's just find the average of all the elements previous to this thread's index. What we're going to do is we're going to do is with this for loop we're going to set i = 0 and go up and up to index, which is the number of this thread. And we're going to accumulate all the variables in the shared memory array up to this thread's index. And after we're done we'll compute the average, which is simply equal to the sum divided by the number of elements that we added up. And then, once again, now we need to do something with that average. What I decided to do is to have every thread look at the average that it just computed of all the elements in the array to its left if you will. If the value in the array at this thread's index is greater than the average that it just computed of all of the elements in the array to the left of this thread's index, then we're going to set array for this thread's index equal to that average. In other words, we're going to replace any threads that are greater than the average of all the threads to the left with that average. Notice that I'm operating on array and not shared array. I used this shared memory array to do my averaging. And that's a good idea, because remember that shared memory is fast. It's much faster to access than global memory. Here every thread is accessing a bunch of elements in the array. It make sense to move this array to first shared memory so that it moves faster. We'll talk about this later. But now I'm making this change back in the global memory array. And that means that when this kernel completes this change is going to be seen by the host and would also potentially be seen by other threads in other thread blocks if there were any. And then just to sort of make a point, here's a piece of code afterwards that has no effect at all, because I'm going to set an element of the shared memory array to 3.14, but then the kernel completes. Nothing ever gets done with that value that's sitting in shared memory. And since the shared memory has the lifetime of the thread block, this memory evaporates as soon as this thread block completes. This code has no effect and in fact can probably be removed by the compiler. Calling a kernel that uses a shared memory is no different than calling a kernel that uses global memory right. Since all you can do is pass in a local variable that points to global memory if you've so allocated it. Then you know what that kernel does with its shared memory is completely up to it and not visible to the host code at all. This code showing how to use shared memory is identical to the code we saw up here. Our question for the quiz is which of these operations would we expect to be fastest and which would we expect to be slowest? We would expect the assignment of local variables to be the very fastest. Shared memory is also very fast. A and b are both shared memory variables, so you would expect that to be fast. Global variables are all the way out in global memory, so they're going to be the slowest. We would expect that this assignment of the contents of y to the contents of z is probably the slowest operation. And this one, which is moving a piece of data from global memory into shared memory, is probably the second slowest. By the way, if you know anything about compilers, you realize that this is an oversimplification. Right? It's quite possible that many of these values will be promoted into registers. An optimizing compiler might rearrange accesses and so forth. But the point is simply to get across the relative speeds of memories. The other major thing that you can do to minimize the time that your program spends in its memory accesses is what's called coalescing. We want to coalesce your accesses to global memory. Let me explain what that means. Whenever a thread on the GPU reads or writes global memory, it always accesses a large chunk of memory at once. Even if that thread only needs to read or write a small subset of the data in that large chunk. But if other threads are making similar axises at the same time then the GPU can exploit that and reuse this larger chunk for all of the threads that're trying to access that memory. This means the GPU is at its most efficient when threads read or write contiguous global memory locations. We say such an access pattern is coalesced. In this example every thread is reading or writing from a chunk of memory that's basically given by the index of the thread plus some offset. This is a coalesced access. This is good. You'll get very high performance on a memory read or memory write in this setting. In this example every adjacent thread is accessing every other memory location, and so this is not coalesced. We would call this strided because there is a stride between every threads access and this pattern is not so good. If you think about it, the way that I drew this dotted line here sort of implies that the GPU in this case was accessing memory in chunks of five memory locations. If I were to just draw out the next five memory locations, you could see that here, to service the needs of four threads making a request each to an adjacent memory location, I was able to service that with a single memory transaction. This dotted line. Whereas in this case the same four threads are striding across memory, and I actually need to pull in 2 memory transactions to these chunks of memory in order to service that. I'm going to get half of the speed of out of my global memory here. You can probably see that the larger the stride between my threads, the more total memory transactions I'm going to have to do and the lower my performance will get. At the limit you can get to a place where each thread is accessing spots so far in memory or so unrelated to each other in memory that every single thread gets its own memory transaction. This, as you can imagine, will lead to pretty bad memory performance from the memory system. We'll talk more about memory optimizations later. For now, just know that global memory is going to be fastest when successive threads read or write adjacent locations in a continuous stretch of memory. Okay, let's have a quick quiz. So, which of these statements has a coalesced access pattern? Here's a simple kernel foo--it takes a pointer to global memory g. And as a shortcut, I'm going to define a as 3.14 and i as thread index dot x. So now, each of these statements either reads or writes g or both. And I'd like you to tell me, in each case, whether the accesses to g follow a coalesced access pattern. Okay, let's look at these. So here I've drawn a chunk of memory. And we're going to say that g points into this memory. So this is g sub 0, g sub 1, g sub 2, and so forth. And here's a bunch of threads, and we're just going to reason out where each of these threads is accessing in memory. So in the first case, g sub i equals a. Well every thread is simply accessing allocation and memory defined by its thread index. This is exactly the example we've been talking about. A given set of threads will be accessing a bunch of adjacent contiguous locations in memory, so this case is clearly coalesced. In this case, every thread is accessing a location in memory defined by its index times 2. So there's going to be a strided access here, right? Threads are going to end up reading every other location in memory. So, this is the access pattern we've called strided, it's not coalesced. This next access pattern is exactly like the first time, except that we're doing reads instead of writes. So again, every thread is simply reading a location defined by it's own index in memory. Therefore, adjacent threads will access adjacent locations in memory, and just like the first example, we're going to have a nice coalesced access pattern. This next example is coalesced because every thread is reading from a location defined by g plus some offset block width over 2 plus the thread's index. So if this is block width number 2, then every thread will be accessing adjacent locations starting at that offset. So this is coalesced. And this example is simply the same pattern. We're going to read from this location which is defined by an offset plus the thread index. We're going to multiply it times a constant. And we're going to store the result back into a contiguous, chunk of memory. So this is simply a coalesced read followed by a coalesced write. Therefore, the statement is coalesced. And finally, this example is a little different. Here we're going to be accessing a location, offset of memory minus the thread index. So thread 0 will access block width over 2, thread 1 will access block width over 2 minus 1, thread 2 will access block width over 2 minus 2, and so forth. And as you can see, even though we're doing the subtraction here instead of an addition, we're still accessing a contiguous region in memory. Every thread is accessing adjacent locations and contiguous chunk of memory, so this is still coalesced. Okay. Let's talk about a related problem. What happens when you have lots of threads both reading and writing the same memory locations? If lots of threads are all trying to read and write the same memory locations, then you're going to start to get conflicts. Let's look at an example. Suppose you had 10,000 threads that were all trying to increment 10 array elements. What's going to happen? Let's look at some code. This goes a little more complicated than we've seen in these examples before, so let me walk through it. What I'm going to do is I'm going to try writing with many threads to a small number of array elements. In this case, I'm going to write with 1,000,000 threads into 10 array elements, and I'm going to do so in blocks of a thousand. That's the #defines. It got a little helper function that just prints out an array. We'll use that for debugging. And then here's the kernel we're going to use. It's a kernel because it's labeled global. It is called increment naive, and it takes a pointer to global memory an array of integers. Each thread simply figures out what thread it is by looking at the block index, which block it is times how many threads there are in a block plus which thread it is inside that block. Now, what we're going to do is we're going to have every thread increment one of the elements in the array. To do that, we're just going to mod the thread index by the array size. Every thread is going, every consecutive threads are going to increment consecutive elements in the array, wrapping around at the array size. The fact that we have a million threads writing into only ten elements means that after each thread has added one to its corresponding element in the array, we're going to end up with 10 elements each containing the number 100,000. And then the code itself is simple. We have a timer class. Again, I've sort of hidden that away so you don't have to deal with it right now. We're going to declare some host memory. We're going to declare some GPU memory. And we're going to zero out that memory. You haven't seen cudaMemset before, but it's exactly what you'd think. We're going to set all of the bytes of this device array to zero. Now, we're going to launch the kernel. And I've put a timer around this, because one of the things I want to show you is that atomic operations can slow things down. So here's the kernel that we called increment_naive. We're going to launch it with a number of blocks equal to the total number of threads divided by the block width and the number of threads per block equal to the block width. Remember, these numbers initially are 1,000,000 and 1,000, okay? We're going to end up launching a thousand blocks, and we're going to pass in the device array, and then each thread is going to do its incrementing. And when it's all done we're going to stop the timer and copy back the array using cudaMemcpy. Now, we'll take that array that we just incremented and copy it back to the host and then I hid away a little print array helper function. It just prints out the contents of the array. Then I'm going to print out the amount of time taken milliseconds by this kernel that I measured with a timer. Okay. That's the whole Cuda program. Let's compile and run it. Okay, so let's see what we've got. So a million threads and 1,000 blocks wrote into 10 array elements, right? So we should have-- in every array element we've done an increment operation-- so we should have 100,000 in each, the number 100,000 in each of the array elements. And when we print it out, instead what we see is we've got 476. That is clearly completely wrong and we need to figure out what happened. Let's notice by the way that it took about 3 milliseconds to run this code. I can run it again a few times. Hm, this time I got a different number. Okay, that's odd--slight different amount of time took slightly less time, about the same, though, about 3 milliseconds. I can run it again and again, and what you're seeing is that every time I'm getting different, seemingly random values written into each of these array elements. And it's consistently taking around 3 milliseconds, 3 to 3.2 milliseconds. So let's see what's going on. The problem is that each of these threads is writing directly into the array and ignoring the fact that there might be many other threads doing the same thing. So if you look at this actual increment operation, every thread is reading the value at g sub i. It's adding 1 to that value, and then it's storing the result back in G sub I again. So it's read, a modify, and a write. It's called a read modify write operation for that reason. The difficulty is that there's 100,000 other threads that are also trying to read this value, compute a different value, and write the result into the same location. So in the amount of time it takes each thread to read this value, and increment, it and store it back, many, many other threads have gotten to run at the same time, and they read the old value, the un-incremented value. Since so many threads are reading the old value, the un-incremented value and adding a number, incrementing it, and storing it back, you're going to get the same result written over and over and over again. You also might have older threads overriding the results from later threads. It's no surprise, really, when we look at this code that, in fact, the numbers that we're getting in those array elements are essentially random. So, what do we do about it? So, by just naively having 10,000 threads incrementing 10 array elements, we got the wrong answer. One solution might be to sprinkle barriers throughout the code that I just showed you. But another solution is something called atomic memory operations. Atomic memory operations, or atomics for short, are special instructions that the GPU implements specifically for this problem. And, there's a bunch of atomics. You can get the list in the programing guide, but some of the examples that you might use are atomic add, atomic min, atomic xor, and so forth. A particularly interesting one is called atomic CAS, which stands for compare and swap. Let's look at how these work. Okay, so here's our code again. And this time, instead of calling it increment naive, let's call a new function increment atomic. This kernel is almost exactly the same. Every thread computes what thread it is, and then it mods that by the size of the array to figure out where it's going to write. But the difference is that, whereas before we were just doing a standard c operation-- g sub i equals g sub i plus 1, which has 1 read, a modify, and a write, here we're going to use the atomic add operation. This atomic ad function is a CUDA built in that's going to take a pointer-- which should be to somewhere in device memory, shared or global memory-- and an amount to add, a value to add, okay? So, this has the same effect. Every thread is going to go and add 1 to the value at g sub i, but the difference is that this is using special hardware that the GPU has built in to perform atomic operations. So now we're guaranteed that only one thread at a time can do this read modify write operation. So now we'll scroll down, change our code, call increment atomic-- that's the only change we'll make--compile the code again, and run it. So this looks more like it. Now we're writing a million total threads and a thousand blocks writing into 10 array elements. And this time, as we expect, every array element contains the number 100,000 when all is done. So the atomic operation prevented these collisions where multiple threads were trying to read and write from the same memory location at the same time, and instead serialized them, making sure that only one thread at a time has access during the course of that read modify write-- that add operation--and we get the right result. Notice that took longer. Before it took about 3 milliseconds. Now we're taking about 3.7 milliseconds. So atomic operations come with a cost, and that's something I'm going to go into. Let's run this a few more times to make sure that we get same result. That time it took about 4 milliseconds, closer to 4 again, down around 3, and so forth. Atomic memory operation have a number of limitations that you need to know about. First of all, only certain operations and data types are supported. So there's things like add and subtract, and min and x or, and so forth. But you can't do every operation. There's no mod, for example. There's no exponentiate. You can't do every operation. And only certain data types are supported, in particular, mostly integer types. So, atomic ad and atomic exchange are the only types that actually support floating point operations. There's a workaround for this. It turns out that you can implement any atomic operation by using the atomic compare and swap operation. And I'm not going to go into the details. This gets into mutexes and critical sections. There's a short example in the programming guide that gives you an example of how to implement, for example, 64 bit operations using atomic CAS. You need to be aware that there's still is no ordering constraints, right? So the different threads in a thread block and the different thread blocks themselves will be running in some unspecified order. It will be different every time. So the operations that you're performing on memory using a atomics are still going to happen in an unspecified order. This is important, because floating-point arithmetic is actually non-associative. In other words, the quantity of "a plus b plus c" is not the same as "a plus the quantity of b plus c." It's real easy to convince yourself if you look at numbers like say, a equals 1, b equals 10 to the 99th, and c equals 10 to the 99th. You can just plug this into your c decompiler or your calculator, for that matter, and you'll discover that you don't get the same number, depending on what order you do these operations in. And the final thing to be really aware of with atomics is that there's no magic happening under the hood. The GPU is still forcing each thread to take turns giving access to that memory. It's serializing the access to memory among the different threads. And this can make atomic operations very slow, if you're not careful. Let's look into that. So let's have a quiz and a programming exercise on this. Here, we've given you the code that you just saw. Now, what I want you do to is modify the code to time several different scenarios, okay? So try running a million threads, each incrementing one of a million elements, so every thread is uniquely incrementing a single element, a million threads atomically incrementing a million elements, a million threads incrementing a hundred elements, or a million threads atomically incrementing a hundred elements, or finally, 10 million threads atomically incrementing a hundred elements. And for each of these choices, I'm going to want you to tell me 2 things. First of all, does it give the correct answer? Put a check mark by those that give the correct answer. And second of all, rank them from fastest to slowest, so the fastest will be 1, the slowest will be 5. So taking these 1 at a time--a million threads incrementing a million elements does give the correct answer because, in this case, there's a unique element for every thread, so there's no conflict. So even though we didn't make these atomic increments we're still safe. A million threads atomically incrementing a million elements, is, of course, also safe. So you'll also get the correct answer. A million threads incrementing a hundred elements is the same example we saw before. And as we saw, that will give the wrong answer, unless we use atomics. So the next one is not correct. The fourth one is correct. And finally, ten million threads atomically incrementing a hundred elements will still be the correct answer, So all but one of these give you the correct answer. Okay, so the more interesting question is how long do each of these options take? The fastest, perhaps counter-intuitively, is going to be option 3-- a million threads writing into a hundred elements. And the next fastest would be option 1-- a million threads writing into a million elements. On my laptop, these 2 operations take around 3.2 milliseconds, and 3.4 milliseconds, respectively. Of course, it's not a very useful option since it doesn't give the correct answer. But it's still interesting to look at what's going on. So the reason why this is slightly faster is that you have your million threads all trying to write to the same 100 elements. Well, those 100 elements occupy a very small fraction of memory, and they're all going to fit into a cash line or 2 on this system, whereas a million elements is large enough that it's not going to fit in cache. You're actually going to have to touch more of the DRAM, where global memory is stored. For the same reason, a million threads writing into 100 elements atomically is going to be slightly faster than a million threads writing into a million elements. So the next fastest is option 4. The next fastest after that is option 2. And in my system, again, these took 3.6 and 3.8 milliseconds, which means the slowest of all options is the one where 10 million threads write into a hundred elements. This is actually 36 milliseconds, so it takes approximately 10 times as long to complete. Not surprisingly, there's about 10 times as much going on. So you might play around with this code for a little bit. For example, see what happens to the time as you go to even more threads writing into even fewer elements. The big lesson here is that atomic memory operations come with a cost. Okay let's recap where we are. So we're talking strategies to do efficient CUDA programming. And the first thing that we've talked about is using high arithmetic intensity-- trying to get your ratio of math operations to memory time spent accessing memory as high as possible. And so far we've been talking about the denominator. The goal has been to minimize the time spent on memory. Part of that is simply moving data to faster memory, if you're going to access it a lot. Keeping in mind that, you know, the fastest memory of all is local-- local variables followed by shared memory, followed by global memory. Another thing you can do is use coalesced global memory accesses. So when you are accessing global memory, try to do it quickly. And the trick there, is, adjacent threads are accessing a contiguous chunk of memory. Usually, they're accessing adjacent memory locations. Well what else do I need to think about when we're writing efficient CUDA programs? In addition to striving for high arithmetic intensity, we also want to avoid thread divergence. Let me explain what that means. When parallel threads, like threads in our trusty thread block, do something different, we say they diverge. So we had some code that looked like this in the kernel. You're doing some stuff and you reach an if statement--if condition is true then execute some code. Else executes another code, then you proceed. If you think about a whole bunch of threads in a kernel executing this code, all of these threads are going to get down here. So these threads are going. They're going to hit this condition. They are all going to execute this condition. And then some of those threads are going to take the if branch, some of them are to take the else branch. So this thread proceeds, hits the condition, and then we'll see it takes the if branch. Maybe this thread proceeds when it hits the condition, maybe it takes the else branch. Perhaps this thread takes the else branch as well. And maybe this thread takes the if branch. Okay, so these threads have diverged. Of course, in this particular case, afterwards they're going to all proceed together again. So this thread's going to keep on going, finish the if code. This thread will keep on going, finish the if code. These two threads will each keep proceeding and do the else code. And it'll all converge again. And you might notice that I tried to draw them so that they kind of reassemble into the same order they were. In fact, their thread IDs have never changed. Okay? This is still thread 0, this is still thread 3, this is still thread 2-- sorry thread 1 and thread 2. So the thread indexes don't change, it's just the path that they're executing through the program is different. So that's what divergence means. This is thread divergence, threads that do different things. You can also encounter thread divergence due to loops. Here's a somewhat contrived example. We have some pre-loop code in our kernel. All the threads are going to do this code, and then they're going to reach this for loop. And the way I've constructed it is, they're going to go through this loop a number of times equal to their thread index. So thread 0 will execute this code once. Thread 1 will execute it twice. Thread 2 will execute it 3 times and so on. And then eventually they're all going to exit the loop and proceed and do some post loop stuff. So what does this look like? Here's a bunch of threads and they're all in the same thread block. I've just color coded them so you can see what they do more easily. And they're all going to be executing this pre-loop code, and then they're going to reach the loop. So thread 0 is going to proceed into this loop code. And they just keep going. Thread 1 is going to execute the loop code, and then execute again, and keep going. Thread 2 will execute the loop code again and again, and keep going. And thread 3 will execute the loop code 4 times. So if we think about these threads a little differently in terms of what they're doing over time, the first order is executing the pre-loop code, then goes ahead and executes the loop code. And then it really just kind of sits around. Okay, it doesn't have anything to do for a while because, in the mean time, thread 1 has executed the pre-loop code and then the loop code and then executes the loop code again. The 3rd thread executes the pre-loop code, the loop code, the loop code, then executes the loop code again. And the final thread executes pre-loop code, and then executes the loop code 4 times. And finally, all the threads can go ahead and proceed with post-loop code. This diagram, when you draw it like this, kind of gives you a sense of why loop divergence is a bad thing, why it slows you down. Because it turns out that the hardware likes to run these threads together, and as long as they're doing the same thing, as long as they're executing the same code, then it has the ability to do that. But in this case, the blue thread proceeds for a while, and then, because it's not going to do the loop again, it just ends up waiting around while the other threads do so. And then the red thread waits for a little while. The green thread waits a little bit. And only the purple thread was executing, at full efficiency the whole time. And so you can imagine that if the hardware gets some efficiency out of running all 4 of these threads at the same time, then that efficiency has been lost during this portion of the loop. So let's wrap up and, and summarize some of the things that we've learned. We've learned about parallel communication patterns going beyond the simple map operation that you saw in Unit 1 to encompass other important communication patterns, like gather, scatter, stencil, and transpose. We've learned more about the GPU programming model and the underlying hardware such as how thread blocks run on streaming multi-processors, or SMs, and what assumptions you can make about ordering, and how threads and thread blocks can synchronize to safely share data and memory. We've learned about that GPU memory model, topics like local, and global, and shared memory, and how atomic operations can simplify memory writes by concurrent threads. Finally we got a quick preview of strategies for efficient GPU programming. The first principle is to minimize the time spent on memory accesses by doing things like coalescing global memory access. We saw that the extremely fast global memory on the GPU operates best when adjacent threads access contiguous chunks of global memory, and this is called a coalesced memory access. We also learned to move frequently accessed data to faster memory. So, for example, promoting data to shared memory. And we learned that atomic memory operations have a cost, but they're great and they're useful and you shouldn't necessarily freak out about the cost. And often the cost is negligible, but it's something to be aware of. Along the same lines, we learned about avoiding thread divergence that comes with branches and loops and, once again, thread divergence comes at a cost. You should be aware of that, but it isn't necessarily something that you should be freaked out about. We're going to revisit these topics and talk much more about optimizing GPU programs in Unit 5. So that's it. Okay, congratulations on finishing unit 2. Let's recap what we've learned. So you learned about how threads communicate with each other through memory and how they can access that memory efficiently when operating in concert. Along the way, we've learned a few things about the GPU hardware, like its memory model, and what assumptions we can and cannot make about when threads will run. Now you'll have a chance to put those concepts into practice. You'll implement a simple image blurring operation. Jen Hahn will tell you more. In Problem Set #2 you will be implementing a parallel algorithm for blurring images. Here is an example of the effect we're talking about. Here's your original image and here's the image after we apply a blur effect to that original image. Blurring an image involves averaging a local neighborhood of pixels, and it is expressed naturally using a parallel stencil operation. Stencil operations come up all the time in all types of application domains. This is why we are going to focus in on stencil in this homework. Let's take a closer look at a simple example demonstrating the kind of local averaging that we are talking about here. Suppose we have the following pixel representation of an image, and we want to calculate the average intensity value for this pixel right here. What do we do? First we take the value of this pixel, and then we add this value to the value of all its neighbors. So 10 4 6 2 1 2 3 and 6, and once when we add up all these values then we take the average. Since we have 9 elements or 9 pixels here, then we multiply the sum by 1/9, and that is how you would calculate the average intensity value for a pixel in an image. If we do this operation for every pixel in the image, we will arrive at a blurred version of input image. However it turns out that performing an unweighted average of pixels can sometimes look really ugly, and we can achieve a better looking blur by computing a weighted average of these pixels. What I mean by weighted average is the following. Rather than multiplying 1/9 to each pixel value here, we will multiply each pixel value by a different weight. So w1 is different than w2. And w2 may be different than w3. And w3 may be different than w4. And that is the approach that we will take in Problem Set #2. Here is an image produced by weighted blur, and here is an image produced by unweighted blur, and as you can see that the weighted blur is much smoother than the unweighted blur counterpart. In this problem set we will give you a small 2D array that contains weight values between 0 and 1 as follows. But this is just an example. The actual weight values that we will use will look like this: the smooth shape of the weights, as you can see here, will produce the nice looking blur effect that we saw earlier. And also, here's a note. We will blur color images by blurring each color channel independently, and we will include a more detailed mathematical formula on blurring computations in the instructor comments. This is what you need to do for Problem Set #2. First, you will need to write the actual blur kernel. Second, you will need to actually write the kernel that separates the color image to its R, G, B channels. And third we will give you the opportunity to allocate memory on the device for the filter, so you will have an opportunity to code CUDA mem copies. And fourth, you will have to set the correct or the optimal grid and block size for this problem set. And, as you remember in Problem Set #1, the grid and the block size has a huge impact on your program's execution time. Set the size correctly and be careful. Lastly your submission will be evaluated based on correctness and speed. But we recommend that you focus on correctness first. Then after your blurring kernal is run correctly then we recommend that you try to make it run faster. And lastly we have supplied serial code that you can reference and compare your solution against. Good luck on writing Problem Set #2. If you have any questions, feel free to ask in the class forums. Hi! Welcome back to Unit 3. Today you're going to learn some new techniques for your GPU toolbox. First, how to analyze the speed and efficiency of GPU algorithms, and second, 3 new fundamental algorithms: Reduce, scan, and histogram. Really excited about this material so let's get started. All right, today we're going to cover Lecture 3. This lecture is going to be on fundamental GPU algorithms. We're going to cover 3 different algorithms today: reduce, scan, and histogram. GPU computing is based on exploiting concurrency in a workload. Expressing that concurrency in a language like CUDA allows parallel implementations on the GPU that can take advantage of hundreds of simultaneous computations. The GPU is well suited to communication patterns that are completely independent, like the map operation. So in the map operation, we're going to have a number of computational elements, and each one of them will compute its output in parallel and completely separately without any communication whatsoever. It's also great at stencil or, more generally, gather operations like we saw in the last lecture. So we might have an item we want to compute here, and we're going to go out and fetch several items from memory and do our computation. But the computation of the next item is again completely separate from the computation of the first one. However, not all communications fall into these categories. Some of them have more complex computation patterns, such as all to one or all to many communication patterns. So, for instance, this element here depends on these 3 elements here, and the computation of this element here will depend on these 3 elements. And notice that there's some overlap here. So it's a little bit more complicated. So we'll cover these more complex computation patterns and 3 primitives that can implement them in today's lecture. These primitives are reduce, scan and histogram. And we'll also use them in this week's homework. So let's go back to our dig a hole example. If we're going to dig a hole, we're really interested in 2 things. One is how long it's going to take to dig that hole, and 2, how much work, how many total hours it will take to dig. If you're the only one digging this hole, these 2 things are the same. So we have 1 worker, and perhaps it's going to take you 8 hours to finish, and so that means overall you're going to do 8 hours of work. But if you bring all your friends over to dig the hole with you, the amount of time to finish and the total amount of work might be different. So now we're going to have 4 workers, and perhaps they'll work together for 8 hours total and thus be able to finish in 2 hours. Thus they've finished digging this hole 4 times as fast. We'd actually call this ideal scaling since adding 4 times as many workers allowed us to finish 4 times as fast. Alternatively, we might have 4 diggers, each of which gets in each other's way, so we're not going to be quite as efficient. So, for instance, we might have 4 workers, and together it's going to take them 4 hours to finish, thus meaning that together they've done 16 hours of work. Now you might reasonably ask yourself why your instructor spends so much time talking about digging holes, and that's a fair point. But there's a couple of important points of terminology that I want to make here. The first is the concept of a step. It's how long it takes to complete a particular computation--Operation Hole Digging. The second is the total amount of work that it took to perform that computation over all the active workers. And so we're going to go into a little more depth in terms of the way that we define these. As we talk about algorithms, we're going to talk about 2 costs. The first is step complexity, and the second is work complexity. So as an example here we have 8 elements that we'd like to combine using this tree style structure. And so we're going to try to characterize the number of steps that it's going to take us to do this computation as well as the total amount of work. So first we're going to look at the number of steps. We see that it's going to take us 3 steps to finish. This first step here we'll do 4 operations, the second step can be done in parallel with 2 operations, and then the third step is a final operation to get a final result. But we can also count the total amount of work that we've done here. We've done 1, 2, 3, 4, 5, 6, 7 operations. So we'd say the step complexity is 3 and the work complexity is 7. So we'll compare the step and work complexity of the parallel implementations that we develop against the step and work complexity for a serial implementation. And 1 more piece of terminology. We will say that a parallel algorithm is work-efficient. If it's work complexity, it's asymptotically the same, so within a constant factor as the work complexity of the sequential algorithm. Now if we can reduce the step complexity in our parallel implementation compared to the serial implementation while still having a reasonable work complexity that isn't too expensive, we expect that this will lead to faster runtime overall. So the real key here is to formulate the most efficient parallel algorithm, and that's the focus of this lecture and the next lecture. Okay, so a quick quiz just to test a little understanding about number of steps and total amount of work. We have a more complicated tree right here, and so you can think of every circle here as 1 operation. Some operations take 2 inputs, some operations take 1. So what are the total number of steps in going from the inputs up here to the output down here? And what is the total amount of work measured in number of operations to get from the 8 inputs up here to the 1 output down here? So to compute the number of steps, what we're going to do is start at the top and count how many steps it gets from the top to the bottom. So here's the first step. It has 8 operations. Second step, third step, fourth step, fifth step, sixth step, and then we've computed the final results. So we'd say there are 6 steps here. What's the total amount of work? Well, to do that, we count the number of operations. That's 8 + 4 + 4 + 2 + 2 + 1. That gives us 21 operations to compute the final answer here. When we discuss algorithms, we'll often discuss these metrics not so much in terms of numbers--the number of steps or the total amount of work-- but instead as functions of the size of the input. So we might have a totally different problem, and we might say that the amount of work in that problem is proportional to the size of the input squared or the number of steps in this particular problem is proportional to the size of the input. So the first algorithm that we're going to cover today is called reduce. So an example of reducing a set of numbers, for example, is adding up all the numbers in this set to get a total sum. Reduce is an interesting algorithm because it requires cooperating between processors. So here we have a tree that we can use to compute the sum of a large number of integers. And so it's interesting because we now have dependencies between the operations. First, we add 1 and 2, and we have to take the result and then add it to 3. Then when we have that sum, we can take the result and add it to 4 and so on. So we now have dependencies between the operations that we need to do. And we haven't looked at any algorithms that have this behavior before. Reduce is also interesting because a parallel implementation, like the one that we're about to describe, will have a smaller and a better number of steps than a serial implementation. So we're going to start with the mathematical definition of reduce. Reduce has 2 inputs. The 1st input is a set of elements, and we're going to assume they're in an array. The 2nd input is a reduction operator that will operate on the elements in this array. For example, we have a list of numbers here, and we have a reduction operator of plus, and the reduction operator will sum them all up. Let's be a little more precise in terms of what operators we're going to support with our parallel implementation. Our operators need to have the following 2 characteristics. The 1st is that the operators are binary. The operator must operate on 2 inputs and create 1 output. Plus is a binary operator, for instance. Two, associative. Associativity means that if we have 2 or more operations in a row, a op b op c, the order of operations doesn't matter as long as the order of the operands aren't changed. To put it simply, a op b, take the result and op it with c, needs to give you the same answer as b op c, and then you op the result with a. You should convince yourself that plus is an associative operator but minus is not. Now, it's not immediately clear why we need this property but we're going to see why in a few minutes. Now it's time for a quick quiz. In the list below, check which operators are both binary and associative. And the list is, multiply, minimum, factorial, logical or, bitwise and, exponentiation, and division. Four of these operators are both binary and associative-- multiply, minimum, logical or, bitwise and. Factorial is not binary. It only has 1 argument and it needs 2 arguments to be binary. And both exponentiation and division are not associative. So if you want to prove that to yourself, note that, say, (4^3)^2 is not equal to 4(3^2) or that 8/(4/2) is not equal to (8 /4)/2. So today we're going to discuss how you implement reduce in a serial way, sort of the traditional way that we all know and love. And so the structure of this looks a little bit like map. In both reduce and map, we loop through all of our elements. But it's different in an important way. In map each loop iteration is independent. We can run these simultaneously and in any order we choose. In reduce, on the other hand, each iteration is dependent on the previous iteration. And here's the serial code to sum a series of elements, and this is relatively straightforward. We have a serial variable named sum, we initialize it to 0. We then loop through our set of elements and on each iteration add the current element to the previous sum. And so when we're done we can return this sum variable and we're done. So on the 1st iteration we do this 1st add here, and we take the result on the 2nd iteration, do a 2nd add here. On the 3rd iteration we do a 3rd add and so on. So what's different about this than the map example is that this add operation is dependent on the previous one, whereas in map, all these things can happen in parallel and at the same time. So now let's take a little bit of a closer look at a serial reduction. We're going to take 5 pieces of data here--a, b, c, d, e-- and reduce them using the plus operator. First, how many operations does it take to perform this serial reduction? So we can just count the operations--1, 2, 3, 4. So 4 operations That's our amount of work. The next thing we're going to do is count the number of steps it takes. So that would be 1, 2, 3, 4. So it's also 4 steps. So a quiz. Which of these 4 statements are true about a serial reduction code running on an input of size n, arbitrary size? First, it takes n operations, then it takes n - 1 operations, or its work complexity is O(n), proportional to the size of the input, or its step complexity is O(1), independent of the size of the input. So please check which ones are true. So the first 2 we're talking about the amount of work. And the correct answer is it takes n - 1 operations. To add up n elements takes n - 1 adds. Next, is its work complexity order of n? Yes, it is. So the amount of work that we do is linear with respect to the size of the input. If we double the size of the input, we're going to double the number of additions that we do. Is its step complexity O(1), meaning independent of the size of input? No, its step complexity is also linear in the size of the input. As we double the size of the input, we'll double the number of steps to take to reduce all these items. So to sum up, our serial reduction algorithm has a work complexity of O(n). What that means is that the amount of work is linear in the size of the input. If we double the size of the input, we double the number of operations that we're doing. Our step complexity is also O(n), meaning again that if we double the size of the input we double the number of steps it takes to compute the output. So now we're going to turn to the parallel reduce. So how do we formulate an algorithm, a procedure to be able to speed this up by running in parallel? Let's take a look at this reduce picture here. At first glance this seems very difficult to parallelize. Note that every operation is dependent on the result of the previous operation. So let's write these operations explicitly. When we formulate the reduction in this way, we have no parallelism. So can you figure out how to rewrite this to expose more parallelism so that we can do more than 1 thing at the same time? And the hint is, use the associative property of the operator. How do you rewrite a + b, take the result, add it to c, take the result. add it to d, to allow parallel execution? In your answer please use parentheses to show grouping. So what we're going to try to do is expose a little bit of parallelism, and we're going to do that by first computing a + b and perhaps at the same time computing c + d and then add the results together. Now what's that going to look like in terms of our tree structure? So now we've got serial reduce and parallel reduce side by side. Here's serial reduce and the equation. Here's parallel reduce and the equation. Now what we've done to do parallel reduce is regrouping these operations in a different order, and this exposes more concurrency. We now have the ability to run multiple operations in parallel at the same time. Now you see why associativity was a necessary quality of the reduction operator. We can reassociate operations to expose more concurrency, then we can run concurrent operations in parallel to reduce the time to solution. Let's see how this works with our 4 element reduction. Let's compare our serial reduction and our parallel reduction. Both of these have 3 additions. Both of them have 3 units of work. But whereas the serial reduction has 1, 2, 3 steps to complete, the parallel reduction only has 1, 2 steps to complete. So potentially, we can run this with parallel hardware and it will complete faster. If we extend this problem to reduce a set of elements of size n, we're very interested in the work and the step complexity as a function of n. So what's the complexity of these 4 metrics? So 3 of them are pretty straightforward. The serial implementation here has both O(n) linear work and linear steps. And the parallel reduction also has linear work, so it's also O(n). But this one's a little bit more complicated to compute. So let's dive in and take a look at this. So let's look at the trend of the step complexity of parallel reduction. First we're going to look at what happens when we add 1, 2 elements to create 1 output. So if we have 2 elements in our input sequence, it's going to take 1 step to finish. If we have 4 elements in our input sequence, it's now going to take 1, 2 steps to finish. And if we have 8 elements in our input sequence-- 1, 2, 3, 4, 5, 6, 7, 8--we know that it's now going to take 1, 2, 3 steps to finish. So the quiz is, what is this relationship? Is the number of steps as a function of n, √n, log2(n), n, or n log2 (n)? And the answer is log2 (n). Note that 2^ the number of steps equals the number of elements. We would say that the step complexity of this parallel algorithm is O(log n). So if we reduced 1024 elements, we would see that it would take 10 steps, compared to 1023 in the serial case. This is 2 orders of magnitude fewer steps. And now you're starting to get an idea of why parallel implementation might get significant speedups. However, note that it can only get these speedups if it has enough processors to handle all these operations at the same time. If we're adding 1024 elements in parallel, our 1st step requires performing 512 additions at the same time. Now, a modern GPU can actually handle this. But if we're adding a million items in parallel, our math would tell us we could finish in 20 steps but would also tell us we'd need to perform 500,000 additions at the 1st step at the 1st time, which is not too likely given the desktop hardware we have at our disposal today. That's okay, though. Even if we're only keeping 500 processors doing additions throughout the computation rather than 500,000, that's still an enormous speedup over having only 1 processor doing additions. It's a good exercise to work out how many steps it takes to run this algorithm for an input of size n if you have only p processors. This is called Brent's Theorem, which the interested student will want to look up and perhaps discuss on the discussion forum. So now we've done all the preliminaries, so let's turn to some actual code. We're going to implement this twice with a similar strategy each time. In both we're going to implement a sum of a million--actually 2^20 elements, and we're going to do this in 2 stages. In the 1st stage we're going to launch 1024 blocks, each one of which will use 1024 threads to reduce 1024 elements. Each of those will produce 1 single item. So we're going to have 1024 items left when we're done. And we're going to launch 1 block to reduce the final 1024 elements into 1 single element. So I'll post all the code, of course. But the CPU side code is straightforward. Instead we're just going to take a look at the kernel. Let's see how that works. So each block is going to be responsible for a 1024 element chunk of floats, and we're going to run this loop within the kernel. On each iteration of this loop we're going to divide the active region in half. So on the 1st iteration, where we start with 1024 elements, we're going to have two 512-element regions. Then each of 512 threads will add its element in the 2nd half to its element in the 1st half, writing back to the 1st half. Now we're going to synchronize all threads, this syncthreads call right here, to make sure every one is done. We've got 512 elements remaining, and so we're going to loop again on this resulting region of 512 elements. Now we'll divide it into two 256-element chunks using 256 threads to sum these 256 items to these 256 items. And we're going to continue this loop, cutting it in half every time, until we have 1 element remaining at the very end of 10 iterations. And then we'll write that back out to global memory. So this works. We can run it on a computer in our lab. So we're doing that now, and we notice that it finishes in 0.65 milliseconds. Less than a millisecond. That's pretty great, but it's not as efficient as we might like. Specifically, if we take a look at the code again, we're going to global memory more often than we'd like. On each iteration of the loop, we read n items from global memory and we write back n/2 items. Then we read those n/2 items back from global memory and so on. In an ideal world, we'd do an original read where we read all of the 1024 items into the thread block, do all the reduction internally, and then write back the final value. And this should be faster because we would incur less memory traffic overall. The CUDA feature we use to do this is called shared memory and will store all intermediate values in shared memory where all threads can access them. Shared memory is considerably faster than global memory. So let's take a look at the kernel. It's going to look very similar. And in this kernel we're going to have the exact same loop structure. What's going to be different, though, is this little part right here. We have to 1st copy all the values from global memory into shared memory and that's done with this little block. And then all the further accesses here are from shared memory-- this s data--as opposed to from global memory, which we did last time. And when we're done, we have to write this final value back to global memory again. The only other interesting part of this code is how we declare the amount of shared memory we need. And we do that here. We're declaring that we're going to have an externally defined amount of shared data. Now, we haven't actually said how much we do, so to do that, we're going to have to go down to where we actually call the kernel. So when we're calling the reduce kernel using the shared memory, we call it with now 3 arguments inside the triple chevrons, the normal blocks and the threads, but then we say how many bytes we need allocated in shared memory. In this case, every thread is going to ask for 1 float stored in shared memory. So the advantage of the shared memory version is that it saves global memory bandwidth. It's a good exercise to figure out how much memory bandwidth you'll save. So I'll ask that as a quiz. The global memory version uses how many times as much memory bandwidth as the shared memory version? Round to the nearest integer. So the answer is 3. And so here I'm showing the memory traffic required for the global version. Here I'm showing the memory traffic required from the shared version. I'm showing all the reads you need to do in red and all the writes you need to do in blue. And if you sum this series, you'll find for reducing n values in shared memory you'll do n reads and 1 write but for global memory you do 2 n reads and n writes. So you would expect that it would run faster, which it will, and let's show that now. So now I'm going to run the shared memory version. And we see that it does in fact run faster. Now we're down to 464 microseconds as opposed to 651. But it doesn't run 3 times faster. How come? Well, the detailed reason is that we're not saturating the memory system. And there's numerous advanced techniques you would need to do to totally max out the performance of reduce. We're not doing that today, but if you're really interested in micro-optimization of this kind, this application is a really great place to start. You'll want to look in particular at processing multiple items per thread instead of just 1. You'll also want to perform the 1st step of the reduction right when you read the items from global memory into shared memory. And you'll want to take advantage of the fact that warps are synchronous when you're doing the last steps of the reduction. But these are all advanced techniques. We can talk about them on the forums if you all are interested. We're going to introduce one of the most important parallel primitives--scan. Let me give you a very short example of a scan operation. The input to scan is a list of numbers, such as 1, 2, 3, 4, and an operation, such as add, and the output is the running sum of those numbers. So each output is the sum of all the numbers in the input up to that given point. 6 is the sum of 1, 2, and 3. Scan is important because it allows us to address a set of problems that seem difficult to parallelize. At first glance it might seem difficult to compute this output from this input in parallel because each element in the output depends on the previous element. So 1st we compute this, add 2 and get 3, add 3 and get 6, add 4 and get 10. That doesn't seem like a very parallel operation. But this style of operation turns out to be incredibly useful. It's also interesting because scan is just not a very useful operation in the serial world. It's really only useful when you're doing parallel computation. But once you know it and use it, you'll wonder what you ever did without it. There's lots of uses for scan, with compaction and allocation being 2 of the most popular. Later in this lecture we'll discuss histogram, which uses scan. And our research group has used scan for quick sort, for sparse matrix computation, and for data compression, among others. It's a very useful parallel primitive. But for this part of the lecture I'm only going to concentrate on what scan does and how it works rather than how it's useful. We'll learn about some more general scan applications in the next unit. Because scan isn't an interesting serial primitive, it's a little bit harder to find a real-life example. The best example I know is balancing your checkbook, which was useful back in the Paleolithic days when people still wrote checks to each other. So let's take a look at an example. When you manually balanced your checkbook, you had a list of transactions and you usually had a couple columns in your checkbook. The 1st column has your transactions--your deposits and withdrawals. The 2nd column has your account balance--the cumulative sum of all the transactions so far. So let's do this as an example. We start off with nothing in our bank, and our first deposit is $20. So our bank balance is now $20. We add another $5, then we make 3 withdrawals. Withdraw $11, withdraw 9 more dollars, withdraw 3 more dollars leaving us with $2 in our bank account. Add in another $15 and so on. Now, the input to scan is this column of transactions, and the output of scan is like your bank balance. What you can see is that at any given time your balance is the cumulative sum of all the transactions that you've seen so far. Scan here is calculating a running sum of its input. That's a perfectly cromulent way to think about scan. So now let's turn to the mathematical description. Like reduce, scan takes an input array and a binary associative operator as inputs. You may recall that associativity was useful in reduce to allow us to reorder operations to expose concurrency. And you'd be right in thinking that we're going to do the same thing here. For completeness, I should note that all the operators we use in this class are also commutative. X op y gives you the same result as y op x. Implementations turn out to be a little bit more complex if you don't use that assumption, but we're not going to cover that here. Now, scan also requires a 3rd input-- an identity value associated with the binary operator. So for any operator, op, I op a, where I is the identity element and a is any value, always gives you back a. For the binary associative operator addition then, the identity element is 0 because if you add 0 to any value, you get back that value. For the binary associative operator minimum, the identity element is the maximum representative value. Consider an unsigned char data type, for instance. The identity element there is 0xFF, because min of unsigned chars between 0xFF and any value a will always give you back a. So a little quiz to make sure we understand binary associative operators and their identity elements. What are the identity elements for multiplication, for the logical or operator, and for the logical and operator? The identity element for multiply is 1 because a times 1 always gives you back a. The identity element for the logical or operator is false because a or false always gives you back a. And the identity element for logical and is true because a and true will always give you back a. The identity element for unsigned int is the element that will give us back the same output for any given input, and for us that's going to be the smallest representable value, and that's going to be a big, fat 0. Any element max 0 is going to give us back that element. Now we're going to start computing the output here. So remember, we always start with the identity element here and then for each output element we take the max of all the elements that precede it. So here, that's the max of 3, so that's simply 3, next the max of 3 and 1, that's going to give us 3, next max of 3, 1, and 4, max of 3, 1, 4, 1, max of 3, 1, 4, 1, 5, so our output array here is 0, 3, 3, 4, 4, 5. This is the max scan on unsigned ints of this 6-element input vector. You might note that the input element 9 is not used in this calculation, and that's correct. That's simply the nature of this formulation of scan. Let's analyze this algorithm. What we want to know is how much work is it going to take for us to complete this algorithm, how many operations are we going to do, and how many steps is that going to take? So the analysis is straightforward. For either flavor, inclusive or exclusive, for each of the n iterations of the loop, we take 1 step and we do 1 operation. Thus we require n operations to complete the algorithm, and we also require n steps. But the parallelization of this algorithm is more complex than with reduce. With reduce it was really clear that the different pieces of the computation tree could be computed in parallel. With scan this is much less clear. So the questions you should be asking yourself now are a) how do we compute scan and parallel at all, and b) how can we reduce the work and step complexity of a parallel implementation as much as possible? But even before we get to those 2 questions, let's answer a larger question. Why do I care about the parallelization of scan in the 1st place? So here's a high level, intuitive explanation. Many computations have the following form. We want to process a list of inputs, which I'm showing here with blue circles, and create a list of outputs, which I'm showing here with red circles. And we begin by computing this 1st output from this 1st input. Then we use that 1st output and the 2nd input to compute the 2nd output, use that 2nd output and the 3rd input to create the 3rd output, and so on. For instance, in our checkbook example we used the previous balance and the next check amount to compute the current balance. This creates a dependency structure that looks like what I've drawn in green. When we structure our computation like this, we see that it has no apparent concurrency. We're only doing 1 operation at a time. First we compute the 1st output, and then we use the 1st output as an input to get the 2nd output, we use this output to get the 3rd output, and so on. Now, this is a very serial structure, and if we implement it in this way it would be a poor fit for a GPU indeed. Computations like this can often be expressed as a scan. And if we can characterize our computation in terms of scan, that's great, because--let me tell you a little secret--we can parallelize scan and make it run fast on a parallel processor like a GPU. Now, not all computations with this sort of dependency structure will fit into the scan model, but many will. And for those computations, leveraging an efficient parallel scan implementation turns the problem from something that would be a poor fit for a GPU into something that is now a much better fit. So how do we parallelize a scan computation? Now, there's actually 2 different flavors of scan. The one I just presented is exclusive scan. The output at each location is a function of all the elements that came before, not including the current element. So let's see what the output would be if we do a sum scan on this 8-element input array. So this is straightforward to compute, and let's just keep in mind the output here is the sum of all elements that come before it but not including it. Inclusive scan is defined slightly differently. Whereas exclusive scan summed up all the elements that came before and didn't include the current element, inclusive scan instead sums up all the elements before and including the current element. So let's see what the answer is here. And so in inclusive scan, this element is the sum of all the elements up to and including the current element. Now, we're not really covering applications of the scan primitives in this lecture. We'll do these in the next lecture. So it's not immediately obvious why you might need both flavors. But what you'll find is that some algorithms are best expressed using an inclusive scan. Others prefer using an exclusive scan. It's likely that if you use a library that contains scan primitives, you'll have access to both flavors. Fortunately, it's fairly straightforward to convert between them if you need to. So let's turn to what implementations of scan look like, and we're going to start off with the straightforward serial implementation. So here's a serial implementation of inclusive scan. So let me tell you how this works. We start off by initializing an accumulator variable that's going to sum our partial results to the identity element. And then we're going to loop through all the elements in our input array 1 at a time. So at each step we're going to do 2 things. The 1st thing we're going to do is we're going to take the current value of the accumulator and we're going to apply the operation to it with the current element. So this could be any binary associative operator--plus, times, max, min, and so on. And we're going to store that value back in the accumulator. Then we're going to set the output element at this particular position equal to the accumulator, then move on to the next element. Now, what we've just defined is an inclusive scan. What we'd like you to do as a quiz is to convert this code to exclusive scan. And this is fairly straightforward, so why don't you give it a shot. So it turns out doing this conversion is actually very straightforward. All we're going to do is flip these 2 lines around, okay? If we interchange these 2 lines, then we've turned this scan from inclusive to exclusive. And why is this the case? So if we remember, an inclusive scan sums up all the elements, including the current element, and then writes its output. Instead, we want to convert this to an exclusive scan, which is going to sum up all of the elements except for the current element, and then set the output. So what we do in the inclusive code that we see here is first we apply the current element and then output the result. But for an exclusive scan, we will output the result and then apply the current element. So it's a very simple transformation to go from one to the next. So let's revisit this inclusive scan example. We have again a 6-element input sequence, and we're going to reduce it with the plus operator to generate this output sequence. Let's think about it this way. What is this output element as a function of all these input elements? Well, really what it is is just a reduction of the 1st element. The 2nd output element is simply a reduction of the 1st 2 input elements, the 3rd output element is a reduction of the first 3 inputs, and so on. In general, the nth output element is the reduction of the 1st n input elements. So if we want to go ahead and compute this output as a function of the input, we can do that with the tools that we already have at our disposal. To compute all n outputs, we simply run n separate reductions, and then we can run all n of these reductions in parallel. So let's take a quiz on this. As a function of n, how many steps is it going to take to scan in this way, with n reductions, and how much work is it going to take overall? And our choices are constant amount of steps or work, amount of steps or work proportional to the logarithm of the size of the input, a linear relationship between the size of the input and the number of steps or the amount of work, and a quadratic relationship between the size of the input and the number of steps and the number of work. So what I'd like you to do is check the right box in the steps row and check the right box in the work row that corresponds to the relationship between the size of the input and the number of steps or the amount of work. First we're going to look at the number of steps, the step complexity. And that should be the same as reduction since all are running as a whole bunch of parallel reductions. The reductions all have different sizes. So to get the overall step complexity, we need to look at the largest reduction, which is a reduction at the end here of all n elements. So the number of steps to reduce n elements and thus the number of steps to perform this entire computation is O(log n). To compute the total amount of work, we notice that we're going to have n reductions to perform. The 1st reduction requires 0 additions, the 2nd reduction requires a single addition, the 3rd reduction requires 2 additions and, in general, the nth output will require n - 1 additions. And so what we're going to have to do is sum up that whole series, 0 + 1 + 2 on to n - 1. And if we do add up that series, we're going to find that the result is roughly n^2 /2 additions. So the overall amount of work is proportional to the square of the number of elements. We would say that the number of additions overall is O(n^). So while we like having a smaller number of steps than the serial version-- remember that took O(n) steps-- this quadratic work complexity makes this formulation of scan ridiculously inefficient. So what we're going to do next is look at 2 other algorithms that have a more reasonable cost. So the 2 algorithms that we're going to look at are by Hillis and Steele and by Blelloch. Both of them are going to be more efficient than the implementation we just saw, but they have different performance characteristics. What we'll learn is that Hillis and Steele's formulation of scan is more step-efficient than Blelloch's, but Blelloch's formulation is more work-efficient than Hillis and Steele's. They're both quite relevant today in parallel computing, and I hope you find their implementations as interesting as I do. Our next implementation is an inclusive scan that was popularized in a 1986 paper by Danny Hillis and Guy Steele, who are 2 really interesting people. Danny Hillis founded a company that built a massively parallel computer called the Thinking Machine, which had scan as one of its core primitives and awesome-looking blinky lights. He designed it for AI applications and said that his goal was to build a computer that was proud of him. At Thinking Machines he worked with Guy Steele, who developed the Scheme programming language at MIT and later was one of the core developers of Java. Anyway, back to scan and this particular algorithm, the easiest way to show it is simply to draw it. So we're going to start with an input array that's simply the numbers 1 to 8. So the 1st stage of this algorithm involves adding yourself to your neighbor 1 away. So for instance, 1 + 2 is going to give you 3, 2 + 3 is going to give you 5, and so on. Then we come back to the beginning, and anything that doesn't have a left neighbor, we just copy its value down. Stage 2 is going to involve adding yourself to your neighbor 2 to the left, so let's see how that's going to work. 1 + 5 gives you 6, 3 + 7 gives you 10, and so on. And again, if you don't have a neighbor 2 to the left, then you just copy your value down. Final stage, now you're going to add yourself to your neighbor 4 to the left, so we add 1 and 14, giving us 15, we add 3 and 18 and give us 21, and so on. And again, if you don't have a neighbor 4 to the left, then you'll just copy your own value down. And now, take a look at what we got. This bottom row here is the inclusive scan of the top row here. So what's the algorithm that we used here? Starting with step 0, here's the steps here. On step i, your job at each location is to add yourself to your neighbor to the left, 2 to the i hops away, so here 1 hop away, here 2 hops away, and here 4 hops away. And if you don't have a neighbor that far to the left, you just copy yourself from the previous iteration. So as a quiz, let's analyze the complexity of this computation. So as a function of n, the input size, how much work are we going to do in this algorithm, and how many steps is it going to take? So we'll start with the steps. Intuitively, we're doubling the size of the communication on every step until we reach the size of the input. This goes 1 hop, this goes 2 hops, this goes 4 hops over, and so on. So if that sounds to you like some sort of log, you're right. There are exactly log n steps to compute this scan. So our step complexity is log n. So now we turn to computing the amount of work that we're going to do. And the way we're going to do that is we're going to think about this matrix that we drew as a big rectangle and we're going to say, well, at every point in this rectangle we're doing a computation. Some of those computations are adds. Some of those computations are copies. It turns out that if you just consider the adds, the analysis ends up being the exact same. But we're going to say the total amount of work we do, the total number of computations, is simply the area of this rectangle. So how big is this rectangle? Well, in the x direction, the rectangle is n items. We're scanning an array of n items. We already computed what the y size of this rectangle is. That's log(n), the number of steps that we have. So the product of those 2 is going to give us the work complexity. And so the work complexity here is O(n log n). So Hillis and Steele scan is fairly efficient. It's actually the first scan to be implemented on GPUs. Guy Blelloch popularized another formulation of scan in 1990, however, that is even better at work efficiency. It's a little bit more complex in terms of the algorithm though, so let's take a look at another example. This algorithm has 2 stages--first to reduce, then to downsweep. And let's note that this is an exclusive scan. So we're going to do a sum scan that is exclusive, on the input array 1 through 8. And so it's going to, again, have 2 stages. The first is going to be a reduction, and it looks similar to the reductions that we've seen already. And then the second stage is going to be new. It's going to be a downsweep, and so that's going to take a different operator than we've seen before. Let's start with the reduction. The reduce step actually looks like a fairly standard reduction. The first step adds neighbors 1 hop away. The next step adds neighbors 2 hops away. And the third step is going to add neighbors 4 hops away. However, unlike in reduce, we're going to keep the intermediate results around. By intermediate results I mean this 1, this 3, this 3, this 10, 5, 11 and 7 because we're going to use them during the downsweep step. Now step 2, the downsweep. We're going to start by resetting the rightmost piece of data to the identify operator. For sum, that identify operator is 0 and now we're going to use a communication pattern that is exactly the same as what we saw in the reduction step except in mirror image. So we see that we have this triangle kind of structure here. We're going to do the same thing upside down when we do downsweep. But what's going to be different about downsweep are 2 things. The first one is that we're going to use a different operator. So each operator is going to take 2 inputs, just as we did in reduction, a left input and a right input, but it's going to produce 2 outputs, not 1. So the left-most output is simply the right output copied to the left. The right output is equal to the sum because we're doing sum scan of the 2 inputs. The second thing that's a little bit different is that we're going to be using these intermediate results. As we need to do a downsweep between 2 elements, some of the downsweep operators might be written down here because we've derived them during the down-sweep. But if we're missing a piece of data, then we just simply drag down the piece of data that we need from the intermediate results that we stored up here during the reduce. So let's get started. We're going to being the downsweep by operating on this element and this element. That's, again, the mirror image of the communication pattern that we had before. So since we don't have an element written here, we'll just copy it down from the element that we have, right? So now we'll apply this down sweep operator to this pair of elements. The first we're going to do is we're going to take this 0, copy it over here. And then we'll take this 10, add it to this 0, and get 10. Now, we're going to operate on elements 2 to our left. So, the 10 is going to be paired with this 11 here, that we've dragged down. The 0 is going to be paired with this 3. So 0 will be copied over to the left. Three and 0 will be added to get a 3 here. Ten will be copied to the left, and 11 plus 10 is 21. Now for the final step, each of these elements will be paired with the item to its left. So, again, we're going to have to drag down intermediate values we kept around. We're going to drag down the 7, drag down this 5, drag down this 3, drag down this 1. And then for the final time apply your downsweep operator-- 0, 0 plus 1 is 1, copy the 3, 3 and 3 make 6. Copy the 10, 10 and 5 makes 15. Copy the 21, 7 and 21 make 28. So now our output sequence is 0, 1, 3, 6, 10, 15, 21, 28, which is the exclusive sum scan of the input of the vector from 1 to 8. What we see, for instance, is that every output such as this 21 here-- 21 is the sum of all the elements that came before. So 21 is the sum of 1 to 6, and so on. As a quiz I'd like you to compute the max scan of this input sequence-- 2 1 4 3, using this reduced downsweep method. So you're going to fill in these values from reduction and then fill in each of these values from a downsweep. And when you finish you should have the max scan, the max exclusive scan of these elements given these inputs. So give it a shot. Okay, let's see how we do this. The operator that we're going to do during the reduce is max. So 2 max 1. Which one's bigger? Two. Four max 3. Which one's bigger? Four. Two max 4. Which one's bigger? Four. We're done with the reduce. So now we begin the downsweep. Remember, we start with the identity element here. And then we're going to apply the downsweep operator. In this case, we're doing a downsweep with a max. So, we will copy to the left, just as in the sum example. And we will apply max to each, each of the pairs of elements to create the output to its right. So right now we're running our downsweep operator on this 2 and this 0, okay? We copy the 0 over. And now we run max on 2 and 0 and get 2, okay? Now, we'll drag down these intermediate values here. So, we'll copy this to the left, and then 2 max 0, gives us 2. We'll copy to the left, and then 2 max 4 gives us 4, and then we'll complete. Note that the output on any particular element is equal to the maximum value we've seen up to, but not including this element. The max value we see here is 4, which is the max value of each element we've seen up to this point. &gt;&gt; So now, let's analyze this algorithm. We know the complexity of the first phase, this reduced phase, already. We know that it has log n steps, as a function of the input count n, and order of n operations. So actually, if we have n elements, we're going to have n minus 1 additions. And the analysis for the downsweep phase is exactly the same as the analysis for the reduce phase because the communication pattern here is exactly the same, except mirror imaged as the communication pattern here. So we know the downsweep also has log n steps and a linear amount of work. This is great. We've now reached the same work efficiency as the serial implementation. Note, however, that it has more steps than the Hillis and Steele formulation. Recall that the Hillis and Steele formulation had log n steps and the Blelloch formulation, this reduced downsweep method, has two log n steps. But the advantage of this Blelloch implementation is less work over all. Recall again that the Hillis and Steele scan had o of n log n work, whereas the Blelloch scan has linear amount of work with respect to the input. Now which is faster? That's a good question. To some extent, it's dependent on the particular GPU, the problem size, the particular optimizations that you chose during the implementation. But to give you a little bit of intuition as to how you might think about the problem of choosing between two different formulations of the same algorithm, one of which has superior work efficiency like Blelloch scan, one of which has superior step efficiency like Hillis and Steele scan. Consider the case where you have way more data, way more work than processors. In this case, your implementation speed is limited by the number of processors because your processors are busy all the time. So you'd prefer the implementation that is more work efficient. Firstly, let's say you have more processors than work. In this case, you have plenty of processors, so you're limited instead by the number of steps in the algorithm, and you'd be willing to do more work to get fewer steps. You've got more than enough processors to handle the extra work, so you'd pick the implementation that is more step efficient. There are a number of interesting parallel algorithms that have a pattern that looks like this. You start off with a lot of work. You narrow down to not very much work. And then, you widen back out to a lot of work. An example of this is the Blelloch scan that we just looked at. You start reducing with a large number of items. You get down to a small number of items. And then during down-sweep, you widen out to a large number of items again. In this case, an advanced but good strategy is to start off with a work efficient algorithm when there's lots of work. Once you get down to not very much work, you switch to a step efficient algorithm, when you have more processors than work. And then, as you start to widen back out, you switch back to a work efficient algorithm, when you have enough work to fill the machine again. So now, as a quiz, we're going to look at 3 different scenarios of things that we want to scan and given a particular hardware configuration. Your task is to figure out which best suited--the serial algorithm, the step efficient Hillis and Steele algorithm, or the work efficient Blelloch algorithm. And the scenarios are, you have a 512 element vector and a machine like a GPU that has 512 processors, you have a 1 million element input vector in 512 processors, or a 128K element vector with one processor. Okay. So, let's look at these scenarios and figure out what the right choice would be. So for the first, we're looking at a small input vector, and you've got plenty of processors. So, you're not really worried so much about work efficiency. You have more than enough processors to do the work that you need to do. Thus, you're probably going to be concerned with the step efficiency of the algorithm that you pick. And the one with the greatest step efficiency is Hillis and Steele. Now conversely, when you have an enormous amount of work to do, and not nearly enough processors to do it, you're going to be looking for the algorithm that is going to have the best work complexity. And so, for this, if you have parallel processors, you're absolutely going to want to run the work-efficient algorithm, Blelloch's algorithm. Now, if you only have 1 processor to work with, you're stuck with a serial algorithm no matter what. The third algorithm we'll cover today is called a histogram. Now, what's a histogram? It's a means of categorizing and usually displaying data that varies with a particular measurement. In practice, to create a histogram, we take a measurement for each item in a population and then place those measurements into a set of bins that we define, where each bin covers a certain range of measurements. This is much easier to explain with an example. I'm recording this lecture at Udacity headquarters in Palo Alto, California. So let's say I walk out on the street in Palo Alto, and record the height of each of the next 100 adults that happen to walk by. What that gets me is a list of 100 measurements. Here's what that list might look like. But it's difficult to draw out conclusions from such a list. Instead, what I'll do is create a histogram. I'm going to specify a number of bins, for instance, 4 bins, shorter than 150 centimeters, between 150 and 165 centimeters, between 165 and 180 centimeters, and taller than 180 centimeters. Then I'll place each measurement into its bin. So every time someone walks by, I place another tick mark into the bin. Here, for instance, we might end up with the measurements 12, 34, 38, 60 out of 100 people. Often in statistics you might want to plot this histogram, which might give you a curve that tells you something interesting about the data. Let me introduce you to one specific operation you might want to do on a histogram of data because you'll be doing this in your assignment. For the histogram we just showed here, we might want to compute the answer to the question "if I'm this tall, how many people are shorter than me?" for all bins in the histogram. This computation is called the cumulative distribution function on a histogram. It's also known as a definite integral. We want to compute the cumulative distribution function on this histogram. The input of the histogram is an array of values, for instance, 12, 34, 38, 16. In 1 word, or 2 words, to be a little more precise, what is the operation we need to do to this input to get the cumulative distribution function? Enter, scan, or more precisely, exclusive scan. Exclusive scan of this input is 0, 12, 46, 84, which is simply the exclusive sum scan of this input. With this, we can answer the question-- If I'm 170 cm tall, how many people are definitely shorter than me? And just look at the answer here. We know that 46 people are definitely shorter than me, out of the 100 people that we interviewed in front of Udacity headquarters. Our focus is instead to compute the histogram itself. Look forward in this week's assignment. You'll compute the histogram of pixel intensity values in an image. Histograms are not a difficult problem if you think about them serially, but they get more interesting and parallel. So let's first take a look at the serial histogram algorithm. Here's our code here. We're going to have 2 for loops where we loop over all the histogram bins. So in the first phase, we initialize all the Bins to 0, the counts for each of the bins to zero. This is so simple, we're not going to mention it again. Much more interesting is the second phase. We're going to loop through all the data measurements. For each data measurement, we first determine to which bin it belongs. That's the compute bin call. And I'll show an example in a second. Then we fetch the current value of that bin, increment it, and store the new value back into the bin. For instance, in the previous example, let's say the input to compute bin is 175 centimeters. So we take a measurement. It's 175 centimeters. Compute bin then decides which of these bins is associated with 175 centimeters. In this case, compute bin would return this bin because it stores all of the measurements between 165 and 180. So, we take 175. It's then associated with this bin, and now we increment the value of this bin. So now let's consider these 4 bins, and we're going to trace a program with the 4 measurements 155, 150, 175, and 170 centimeters. So the first loop here will initialize each of these bins to 0 items, and then we'll walk through these 4 measurements using compute bin to decide which bin they're in and incrementing the values in those bins. So, first we'll consider 155. Compute bin will tell us it's in this bin right here, so I'll increment its value to 1. Then we look at 150. Compute bin will tell us its value is in this bin again, and we'll increment its value to 2. Next we'll consider 175, which is associated with this bin. And now we've incremented its value. And finally 170 will allow us to store 2. So a couple of quick questions to make sure you understand the histogram. Let's say we have n measurements and b bins. As a function of n and b, what is the maximum number of measurements we would possibly ever see per bin, and what is the average number of measurement you would see per bin? Well the maximum value that you'd ever see per bin is if all n values went in the same bin, so the answer is n. In any event, you have n items spread over b bins, so on average you'd have n over b items per bin. So now we're going to look at 3 different formulations of a workable parallel histogram implementation. Now you might be looking at this serial code and thinking, "Why can't we just unroll the serial loop n times and launch a thread for each iteration of the loop?" Note that if there's n measurements taken, we can consider launching n threads, each of which increments 1 bin count. It turns out this doesn't work, and it's important to understand why. Dave already covered this topic in the last unit, so you can feel free to zip through this part if you totally understand it already. Anyway, we're going to take a look at this particular kernel, this naive histo kernel. In this kernel, each thread will be responsible for classifying 1 element and incrementing the histogram bin corresponding to that element. So let's look at this very simple kernel. In this code, the first thing we're going to do is compute our global ID. Then we're going to fetch our item from global memory. Then we're going to calculate which bin our item is associated with, and in this case, we're just using a very simple mod operator to do it. And finally, we're going to increment the bin with which our item is associated. Though if we scroll down to the main routine here, we see that we have 65,000 elements that we're going to classify into 16 bins, so we would expect 4,096 items per bin. So what happens when we run this? I'm going to run this kernel on a work station that I'm connected to in our lab. So, let's run this histo executable. And what we're going to see are the bin counts that we see for each bin. We expected 4096 items per bin, and we're not getting anywhere close to 4,096 items per bin. In fact we'll run it again, and if you note, these bin counts are even changing from iteration to iteration. So what's going on here? Let's go back and look at the most important instruction in the kernel, the one where we increment the value in the bin. Let's look at what's actually going on here. What does each thread program actually do if we implement the simple serial algorithm in parallel? It does 3 separate operations. The first one is doing a global memory read to fetch the bin value into a register. The second thing is it increments the bin value within the register. And the third thing is storing the incremented value, back to global memory Let's illustrate how this could go wrong. Consider 2 threads running in parallel--1 the black thread, 2--the red thread. Both want to increment in the same bin--this blue bin here. It starts off with the value 5. Both threads happen to be running at the same time. So the first reads the value of the bin into its local register, then the second reads the value of the bin into its local register. Both increment the value in their own local registers, and the first writes its value back to global memory, storing a 6, and the second does the same, also a 6. And now we've got a problem since we'd really like the answer 7. The fundamental issue here is called a race condition. The problem is that incrementing the value in global memory takes multiple steps, and it's possible, as we've seen here, for 2 processors running simultaneously to interleave these steps. Note that this is not going to happen in the serial code because in the serial code each iteration of the loop runs separately, and there's no danger of one thread's code running at the same time as another thread's code. So the simple solution doesn't work. So now let's look at 3 different ways that we might implement this in parallel that will work. All 3 of these methods are good parallel methods. None are obviously better or worse, and we'll talk about their pros and cons as we describe them. &gt;&gt; First we'll use what's probably the simplest method. Recall that the reason that a trivial parallelization didn't work was that the read, modify, write operation that we needed to increment a bin was actually 3 separate operations-- a global memory read, a local increment in a register and a global memory write. What if we could make this read, modify, write operation here, actually be 1 operation instead of 3? If it was only 1 operation, it could not be interrupted by another thread. Now, we call such an operation atomic, and as we learned last unit, modern GPU's support atomic operations to memory. What the GPU does, in essence, is locks down a particular memory location during the read, modify, write process, so that no other thread can access it. This is a simple change in the code. So we're only going to change 1 line in the code. Here's our naive histogram, here's this new histogram that we're doing, and you notice we're only going to change the last line. We're changing this simple increment here to an atomic increment. It's a simple change, but let's see the effect. So now we're going to run the histogram program using simple histo instead of naive histo. And we see when we do that we get the expected 4096 elements per bin, great news. Now the disadvantage of this technique, is that it serializes access to any particular bin during the atomic operation. Many threads might want to update that bin, but only one thread at a time actually gets to do so, and so the others have to wait. This contention for the atomic will likely be the performance limiter in any histogram code. So let's say you have a million measurements, with which you'd like to calculate a histogram. You can choose to have 10 bins, 100 bins, or 1,000 bins in your histogram if you use the atomic method, which will most likely run fastest. The answer--1,000 bins will probably be fastest. Since you're likely going to be limited by atomic performance, the histogram that offers the most bins is probably going to have the highest throughput--with a 1,000 bin histogram, 1000 threads can be updating the histogram at any given time, whereas with a 10 bin or a 100 bin histogram, only 10 or 100 bins can actually update the histogram at any time. In general, algorithms that rely on atomics have limited scalability because the atomics limit the amount of parallelism. If we have 100 bins and use atomics to access them, then no matter how awesome a GPU we might have, we're still only doing 100 useful things at a time. Thus, there's interest in algorithms that might be a little bit more complex, but might scale better. So we'll look at a reduce based method next. Let's say we have a large number of items to place into our histogram, but relatively few bins. Now, we're going to consider a pretty small example here, for the sake of discussion, just so we can draw everything that we need to. And for the sake of discussion, we're going to say we have 128 items to place into bins. We have 8 threads that we've launched to take care of these items, and we only have 3 bins. So we're going to divide those 128 items among the 8 threads, so each thread will be responsible for 16 items. Now, rather than have only one single set of 3 bins in memory, let's launch 8 threads and give every thread its own set of bins, which we'll call its local histogram. Since we don't have very many bins, we can store each thread's bins in that thread's registers, which are fast. Now we have 16 items per thread, and each thread can independently accumulate its own local histogram and its own registers. So, we get 8 separate local histograms, each of which has accumulated 16 items, which isn't particularly useful. What we actually need is one global histogram rather than 8 local histograms. Quick quiz--do we need to use atomics to manage access to these local per thread histograms, yes or no? Answer, no. Each local histogram is only accessed by one thread, serially. So there's no shared access. And thus, we don't need any automics. We can use reduction to combine these 8 local histograms into a single global histogram. So how do we do this? We simply reduce the first bin of each of the 8 local histograms. That's an 8-item reduction, and we can do that with a reduction trait. Then we reduce the second bin. And then the third bin. Recall that reduction requires a binary associative operator. Note that adding 2 histograms together, 2 entire histograms of 3 bins each, is both binary and associative. So it might be more efficient to figure out how to add 2 histograms together during the reduction implementation, rather than adding each bin separately. The final algorithm we'll discuss, which we're going to call sort, then reduce by key, is one that I'll just sketch out, primarily because we haven't discussed the underlying algorithms yet. We'll get to them in the next lecture. However, it'll be a great mental exercise for you to think about how you might implement this approach. Let's think about our entries. Here we have 8 entries, as key-value pairs. The key is the bin number, and in this example we're going to have 3 bins--0, 1 and 2. And the value is 1, which we're going to add to each bin. This algorithm has 2 steps. The first is a sort, the second is a reduce. So first, let's look at the sort. We're going to sort the key value pairs by key. So here's our keys, and notice it's the same key value pairs that we have up here, but now they're in ascending order by key. And the second step is to reduce the result by key. Naturally enough, this algorithm is called reduce by key. What I mean here is to add all the elements that have the key together. So, we'd like to add these four 1's up to create the reduction value of 4, or, more generally, to reduce these values using reduction operator. Note that all elements with the same key are contiguous, which is the key to making an efficient implementation. Now again, this is likely not something you can do with the algorithms that you know so far. But libraries of parallel primitives may have implementations for both of these operations. For instance, the thrust library that accompanies acuda distribution implements both sort and reduce by key methods. Thinking about how you might implement these 2 operations is worth your time over the next week. Now, with one word we learned today, how can we combine these 8 local histograms, each of which summarizes the histogram for 16 items assigned to that thread, into one global histogram? And the answer is reduce. We're going to take these 8 local histograms, and we're going to reduce them into one global histogram. Let's see how that works. Final thought on histogram--it's certainly possible, even desirable to combine these methods. For instance, the disadvantage of the atomic method is that a large number of threads are trying to get simultaneous access to bins, so most threads end up waiting around. If we could reduce the number of threads trying to simultaneously access an atomic, that would remove the bottleneck. So, we might choose to use the reduction technique within a block to combine all the individual histogram operations from each thread in that block into a single histogram, then use a single thread within that block to add that per-block histogram into the global histogram. For instance, if we have 256 threads in our block and 8 bins in our histogram, we might choose to do our implementation 1 of 2 ways. One is that each individual thread within our block atomically increments a global bin. Another is that we can take the 256 threads that we have, each one containing a single measurement, and then reduce them within the block, locally, to produce one 8-element histogram. Then, use that 8-element histogram to update the global histogram. For each of these, how many atomic adds are we going to need to do? Well for the atomic only technique, we're going to have 256 threads, each of which has to increment one bin. So we know we're going to need 256 increments. However, if we do all of our reduction within the block, with no atomics necessary, and only after we've reduced to a single 8-element histogram, only each 1 of those 8 elements must update the global histogram with an atomic to be able to complete the histogram calculation. So, this is potentially a large savings in atomics, which is in turn going to help your performance. We learned 3 algorithms today. We'll use all 3 in this week's homework, when we implement a procedure called tone mapping on an image. Generally, tone mapping is the process of converting one set of colors into another set of colors. The reason we want to do this is typically to take a raw image that accurately measures light intensity, and map it onto some physical medium, which might be a piece of paper, or a television, or your computer monitor. The problem is that a real world scene can have brightnesses that span many orders of magnitude and intensity. A piece of white paper in bright sunlight is literally a billion times brighter than on a dark night. And an output device like a CRT can only represent about two orders of magnitude between its brightest and least bright pixel. So, how do we do this mapping here? If we do a bad job, then we'll have images that are either too washed out, overly bright, or instead, overly dark. Let's take a quick look at some pictures to see why this is important. Here's a couple lovely images of old Saint Paul's in Wellington, New Zealand, licensed by the photographer Dean Pemberton, under the Creative Commons license. Thanks, Dean. What we see here are two different exposures of the interior of this building. If you look at the darker image here, you'll see most of it is quite dark and so it shows little detail. If we look at the brighter image, we see a lot more detail in some areas but bright parts of the image, like the stained glass window, are completely washed out. We can classify this in terms of histograms. In the dark image, most of the pixels are at the low end of an intensity histogram. And in the light image, they're at the high end. What tone mapping does is it remaps those colors, so they do a better job spanning the entire color range. The intensity spectrum, thus, will look closer to this. In the homework, you'll implement a classic and excellent tone mapping algorithm developed by Greg Ward. This algorithm uses all three of the algorithms you learned today, reduce, scan, and histogram to implement such a tone mapping. Good luck, and we'll see you in the next unit. Great job on unit 3! Understanding these fundamental parallel primitives opens the door to mapping many interesting tasks to the GPU, and it also gives you the foundation for the next unit, where we're going to look at sort and applications of scan. You'll use today's topics in the problem set where you will implement a high dynamic range, HDR, imaging algorithm that you can use to better represent the wide range of intensities in real scenes. In Problem Set #3, you will be implementing a parallel algorithm for tone mapping. Tone mapping is the process of mapping an image with a wide range of brightness values into an image with a narrow range of brightness values. This is an important operation because the real world has an enormous range of different brightness values. For example, think of this spectrum as the range of brightness values in the real world. On one end of the spectrum we have a bright, sunny day. On the other end of the spectrum we have a very dark, moonlit night. And it turns out that the bright, sunny day is about a million times brighter than the night. Unfortunately, our computer screen can only display a narrow range of values. And think of this spectrum as the range of brightness values that our computer screen can display. So tone mapping is the problem of taking the brightness values that we seen the real world and mapping them down into a tiny range of brightness values that we can display on our computer screen. This process is tone mapping. There's a good chance that the camera on your cell phone has a high dynamic range or HDR mode. When you take pictures in HDR mode, then your camera is performing a tone mapping step to arrive at the final image that it shows you. In this problem set, you will be implementing a simple algorithm for tone mapping called "histogram equalization," and this factors nicely into parallel map, reduce, scatter, and scan operations. You have been exposed to the map operation in previous homework, so in this homework you will focus on reduce, scatter, and scan. Recall that histogram counts the number of occurrences of something in the data set. For example, if we measure the heights of everyone taking this class and compute a histogram of the heights, it may look something like this. The height of each bar in this histogram is a count of how many peoples' height fall into a particular range. For example, this bar may represent that range of people whose height is from 170 cm to 180 cm. And this bar right here may represent people whose height is from 200 cm to 210 cm, and these people are really tall by the way. The main idea in this homework is to first compute a histogram of the brightness values in the image. And after that we will compute a scan of the histogram that we computed in step one, using the plus operator. It turns out that the array resulting from the scan, tells us exactly how to remap the brightness values in the original image. This is just one of the many really interesting applications of scan, and that is why we're going to focus on scan in this particular problem set. In order to figure out the range of brightness values that should go into each column of the histogram, you will need to, first, compute the minimum brightness in the image and, second, compute the maximum brightness in the image. We will provide you with an array of brightness values, but you will need to figure out the minimum and the maximum values using the parallel reduce operation. When computing a histogram parallel, it is very likely that different threads will try to update the same memory locations at the same time, and to correctly handle this parallel data scattering requires special care. For now we recommend that you use the atomicAdd function when computing your histogram. Although using atomicAdd may be a little slow, but it will ensure that multiple threads don't try to update the same memory location concurrently. In return, that will ensure the correctness of your histogram computation. Once you get everything working, feel free to start experimenting and see if you can avoid using atomicAdd. If you're interested, we have included a more detailed mathematical formulation of the histogram equalization in the instructor comments. Lastly, I'd like to thank Eric Ellson of Royal Caliber and Mike Roberts for writing the script and the code to this problem set. Good luck. Good to see you back for unit 4! I'm sure you asked yourself after the last unit just what do we do with this scan primitive? Well, today we're going to find out. We're going to look at applications of scan, and then we're going to learn how to sort on the GPU. So let's get started. All right, welcome to Unit 4. Today we're going to talk about fundamental GPU algorithms, part 2. We're going to start with scan, which we talked about last week, and look at specific applications of scan. And then we're going to turn to looking at how to sort on a GPU. So in the last unit we learned about fundamental GPU algorithms. And the last one, and the most important one for this lecture, was scan. Scan is an efficient way to paralyze a certain class of parallel problems that otherwise seem difficult to paralyze. We usually refer to these as recurrence relationships. One of the use cases for scan is for problems that keep some sort of running total, such as a running sum. We can also use other binary associative operators like Max, Min and Most Logical operations. So, as a quiz, let's recall 2 of the important properties of scan. So we're looking at a scan of n elements. And so, in the best GPU scan implementations, what is the amount of work to do the scan, and what is the number of steps to do the scan? Your choices are proportional to log n, proportional to n, proportional to n log n, and proportional to n squared. So we'd like you to check the box that corresponds to what the work complexity of the algorithm is and what the step complexity of the algorithm is. So what we learned last week was that scan can be computed efficiently on n elements with run time proportion to n. And we also learned that it can be completed with a number of steps proportional to log n. This is something we can implement very efficiently on the GPU. And because we can implement it efficiently, what we'll find today is that it's the core of a significant number of interesting parallel parameters. And we're going to start with one called compact. We're going to start with what is compact. And so let's consider a common problem in computing. We have a set of things like cards, and we want to filter that set to get a subset that we care about. This comes up all the time. We might decide that we only want to do computation on the diamonds from this set of cards, and take all the other cards and throw them away. We might select students from the roster of this class who have perfect scores on the programming assignments. My background is in computer graphics, for instance. So an example there, we might have a lot of objects to draw in a scene, but we only want to keep those that actually intersect the screen. We call this operation compact because we're compacting the large input set of potentially many items, into something smaller. Another word for this is filter. Now, why do we want to do this? Well, if we only care about a subset of the input, and we want to run some computation on that input, it makes more sense to throw away the items that we don't care about, and only compute on the objects that we do care about. We can rightfully assume this is both cheaper to compute and takes less space. We have a set of objects S 0, S 1, S 2, S 3, S 4 and so on. And we have what we call a predicate, and that's this line here. Predicate is a function that inputs an S object and returns true or false for that object. For instance, this predicate we're looking at here is, "is my index even?" So for S 0, S 2 and S 4, the predicate returns true. For S 1 and S 3, the predicate returns false and so on. We want to calculate S filtered by this predicate. So we only keep those objects in S for which the predicate is true. So what form do we want the output of this filter of this compact operation to be? We have a choice of how to do it. We can either have a sparse output, or a dense output. For the sparse output, each element tracks to its same position in the output, and if an input has a false predicate we just put some sort of nole element in there. Alternatively, we could have a dense output, where all the elements for which the predicate is true are then packed together into the output so they're all sitting right next to each other. There aren't any gaps in the output. In general, we want the output of a compact operation to be dense. And let me tell you why. Let's go back to one of the examples I cited earlier. Selecting the 13 diamonds from a deck of 52 playing cards and running a procedure called compute card on each of the diamonds, we can structure this code in 1 of 2 ways. We could either wrap our computation in a big if clause-- so what we're doing here is on each card we check if it's a diamond, and if it is a diamond then we run computecard. Otherwise, we do nothing. This is the sparse approach. Or, we could run a compact on the deck of 52 cards to get back 13 diamonds, by running compact on the cards, using the is diamond predicate, and then run a map on the compacted cards, calling compute card only on the diamonds. So note that the sparse computation, on the left side, is going to launch 52 threads, 1 per card, and 39 of those threads will be idle. The dense computation, on the other hand, incurs the cost of the compact, but then in the map step, it only launches 13 threads. If the compute card routine is at all expensive, then the sparce approach loses because we have to launch 4 times as many threads. Three quarters of those threads are going to be idle, while the other quarter of the threads are actually doing useful work. But because both idle and non-idle threads are running in log-step, we're still paying the run time costs of having 4 times as many threads Therefore, generally, you prefer the dense approach especially when this computation is expensive. So let's take a quick quiz on when we want to be using compact. In general, compact is going to be most useful in scenarios where we compact away, where we delete a small or large number of elements, and the computation we need to run on each surviving element is cheap or expensive. So the answers are large and expensive because we are saving the most computation by culling away a large number of elements, each of which would otherwise incur an expensive computation. So now we know how to compact in parallel, but only if we figure out how to compute the scatter addresses efficiently and in parallel. So let's take a closer look at the computation that we need to do here. We're going to have a set predicates here-- true, false, false, true, true, false, true, false. And what we need to do is compute the scatter addresses that will result from this string of predicates. Since we don't care about the scatter addresses for the false values, we'll just put a dash for don't care. Our algorithm is going to generate something there, it just doesn't matter what it generates. Again, our goal is to input this predicate string and output this string of addresses. And now we'll do a simple substitution. Whenever we see a true here, we'll substitute the number 1. Whenever we see a false here, we'll substitute the number 0. And rather than "don't cares" I'm actually going to fill in some values here. So we're going to have the strings 0, 1, 1, 1, 2, 3, 3, 4. So here's our input, a bunch of trues and falses. We're translating it into 1s and 0s, and this is our output, our set of addresses here. And now perhaps the shortest quiz of the course. If this is the input, and this is the output, what is the operation we need to do to get from this input to this output? Hint. Your answer is 1 word, of 4 letters. And the answer is scan. Actually, if you want to be a little bit more precise, it is exclusive sum scan. And we already know how to do a scan efficiently. This predicate is our input. This black line here is the output of our scan. And note that it corresponds precisely to the addresses we need, to scatter the input to the output. So let's summarize how to compact. Conceptually, there are 4 steps, though an efficient implementation might combine these into fewer. First, we're going to run a predicate on each element of the input. Then we'll create a scan-in array equal in size to the input. For each element of the input, if the predicate is true, put a 1 into the scan-in array. If it's false, put a 0. Then we exclusive sum scan the scan in array. The output of this is scatter addresses for compacted array. Now, for the final step--for each element of the input, if the predicate is true, then scatter the input element into the output array, at the address in scan out. Let's try a little quiz to get some intuition about the run time of compact. We're going to compare 2 compact operations, and both are going to compact the list of numbers from 1 to 1 million. Compact operation a--let's say is divisible by 17--is only going to keep a very few of the input items. Whereas compact operation b--is not divisible by 31--is going to keep most of the input items. So for each of the predicate, scan, and scatter faces of the compact operation, will a run faster, b run faster or will they take the same amount of time? For the predicate operation, both will take the same amount of time. Both will run their predicate on each of 1 million items. Scan will also take the identical amount of time. Scan is going to run on an array of a million items. Where they're going to differ is scatter. A will run faster than B for scatter because there's fewer memory operations, since fewer elements need to be copied into the output array. We can generalize compaction in the following way--compact is an operation that allocates exactly 1 item in the output for each true input and 0 items in the output for each false input. This is useful, of course, in compact as a really common operation. But more generally we can do a compact-like operation where the number of output items can be computed dynamically for each input item. Let me offer a graphics example to show why this might be useful. The input to most real time computer graphics pipelines is a set of triangles. Some of these triangles might appear on the screen and some might not. And so we generally test to see if each triangle is visible or not, before we pass it down the pipeline for later processing. This is a compact operation--we compact an input stream of triangles, some of which are visible and some are not, into a smaller output stream where all triangles in the output stream are visible. And we know how to do this with compact. Now, here's the more complicated problem. What if a triangle intersects the boundary of the screen or window, for example this triangle here, or this triangle here? In this case we apply an operation called clipping, where we cut the triangle with the boundary, and then triangulate the resulting shape. So for this triangle, for instance, we're going to convert it to this triangle right here. This triangle has left behind a trapezoid, and so we only want to deal with triangles. So instead we triangulate this trapezoid and send 2 triangles down the pipe. Now we have a situation where we input a set of triangles, and each triangle can produce 0, 1, or possibly many output triangles. And we want to output the resulting triangles in a dense array. Here's a good geometry quiz question for you that has really nothing to do with the course material, but it's a pretty cool question nevertheless. What is the maximum number of triangles that can result from clipping a triangle with a rectangle? So here's the worst case. We have a green triangle clipped by a blue rectangle. How many sides is the resulting polygon? So 1, 2, 3, 4, 5, 6, 7. And so how many triangles will this result in? 1, 2, 3, 4, and 5. So our answer here is 5. So one way we could do this operation is to allocate the maximum space per element. In this case we would create an output array with 5 output triangles per input triangle. And we're going to put these in an intermediate array and then compact the result. However, this has 2 significant disadvantages. The first is that it's really wasteful in space. We've now allocated an intermediate array that has the maximum amount of space per element, and that's going to lead to a very large array overall. The second disadvantage is that now we have to scan this large intermediate array, and that's not going to be as efficient as we would like either. So how can we do this more efficiently? So let's go back to what we'd really like to do. What we'd like to do is have a list of allocation requests, 1 per input element, and then get back a location to write your requests. As an example, let's say we have 8 input triangles here, and those 8 triangles want to generate, in order, 1, 0, 1, 2, 1, 0, 3, and 0 triangles. So what's that going to look like? The first input element is going to want to write its 1 triangle here into the output array. The second triangle isn't going to have any output elements to write. The third input triangle is wanting to write into this element here. The next triangle has 2 outputs. It will write them into the next 2 slots of the array. The next triangle wants to write its output here. The next triangle wants to write nothing. The next triangle wants to write its output into these 3 output slots. So what we want to generate here is the output addresses for these. He wants to write his output into address 0. He wants to write his output into address 1. He wants to write his output into address 2 and 3. And this guy wants to write his output into addresses 5, 6, and 7, and so on. And look. This is exclusive scan as well. It generalizes what we did with compact. In compact we allocated 1 slot in the output array for each true element and 0 for each false element. Instead, now we've generalized this so that we simply indicate in the input array the exact number that we need. This is simpler and it's more efficient. Instead of scanning an array that has the maximum possible allocation for all elements-- in the case of clipping, 5 times the size of the input array-- now we just have to scan something the size of the input array. So it's more efficient to be able to do it this way. So that's a couple of interesting applications of scan. If you're doing any sort of GPU programming with irregular workloads, you'll find that scan comes up all the time. Recent work has used scan in diverse application domains, such as graft reversal, data compression, and collision detection, among many others. And we'll see later in this lecture that scan is the core of the fastest GPU sort as well. Now in some applications you might have the need for many small scans instead of 1 large scan. We've said before that when you launch a kernel on a GPU, you want to have lots of work to do in that kernel. So it doesn't make a lot of sense to launch a scan kernel separately on each one of these small scans. Instead, we're going to pack these scans together as segments inside 1 big array and then have a special scan operator that scans each of the segments independently. Typically to indicate where segments begin in an array we use a second array that has 1s for segment heads and 0s for non-segment heads. Let's do an example. So if you'll recall, exclusive sum scan on an array simply takes the running sum of all elements that come before the current element. So the sum of all elements before 8, for instance, is 28. Now what's different about a segmented scan is that we have a normal array, but what we do is we're marking with these large lines here boundaries between these segments. So when we call a segmented exclusive sum scan on this array, what it's going to do is take separate scans of each of these 3 segments. So the result is the scan of 1, 2, the scan of 3, 4, 5, and the scan of 6, 7, 8. And so the way that we're going to represent these segments is with segment heads so that we have a second array the same length as the input array that marks where segments begin. So a segment begins here with the number 1, a segment begins with the number 3, and a segment begins with the number 6. So to make sure we're on the same page, we're going to take the same array we did last time where we computed an exclusive segmented sum scan and instead you're going to fill in the result of an inclusive segmented sum scan on this input array. If you'll recall, an inclusive sum scan is going to be the sum of all elements that came before in the segment as well as the current element. So why don't you fill in these 8 boxes? Okay, so this is pretty straightforward. The output at any particular element is the sum of that element and all elements that come before it in the current segment. But we're starting it over at every segment boundary. So 12, for instance, is the sum of 3, 4, and 5 but nothing from the previous segment. We're not going to go through how this is implemented, but it's a terrific exercise to work out on your own, whether you do it with Hillis and Steele scan or the Blelloch scan. It has the same complexity as unsegmented scan, but it runs a little slower because you have to keep track of the segments along the way. So it requires more memory traffic as well as more complex computation. We'll have to put a little more information about the implementation of segmented scan in the supplementary materials. But again, this is a really great problem for you to work out on your own. You might be wondering why this might be useful. Never fear. We'll have 2 good examples in this unit and cover the first one here and the second a little later. So let's consider the problem of sparse matrix/dense vector multiplication, which we abbreviate SpMv, sparse matrix vector. Many interesting matrices have lots of zeroes, so we call those matrices sparse. And we want to find a data structure to represent them that squeezes out the zeroes as well as to find a more efficient way to multiply them by vectors. After all, if there's lots of zeroes, we'll do lots of multiplications that have no effect. So sparse matrices are incredibly common in computational problems in many domains. For instance, PageRank is the world's largest matrix computation. PageRank is based on a giant sparse matrix that represents every web page in the world. How big is this matrix? Well if there is n web pages in the world, this matrix is n by n and each entry at row R in column C is non-zero only if web page R Links to web page C. So the PageRank computation on this sparse matrix is how Google computes the importance of web pages. Let's briefly refresh how we multiply a matrix by a vector. Students who already know this are welcome to skip to the next segment. So we're going to multiply this 3x3 matrix here by this 3x1 matrix here. So how do we do this? For each output in the output vector, we multiply the row of the input matrix by the column of the input vector. So we do this in a pairwise way-- a times x + b times y + c times z, and that will create the entry here ax + by + cz. For each additional row in the output matrix we simply do another one of these dot products. So for instance, to compute this value here we dot product this vector with, again, our input vector to get g times x + h times y + i times z. So let's do a matrix times a vector as a quiz. We're going to have this sample 3 x3 matrix times this sample 3x3 vector. And I'd like you to write the vector answer in these 3 blanks here. Okay, how do we go about doing this? Well, first we're going to dot this row times this column, so that's 1 x 1 + 0 x 2 + 2 x 3. Okay? Then we're going to dot this row with this column, so that's 2 x 1 + 1 x 2 + 0 x 3, and then this row with this vector, 0 x 1 + 1 x 2 + 3 x 3. So let's sum these up. 1 + 6 gives us 7, 2 + 2 gives us 4, 2 + 9 gives us 11. So our answers are 7, 4, 11. So the traditional way to represent a sparse matrix is what we call compressed sparse row. So here's a small matrix of 9 elements. Three of them are zeroes, and so we want some sort of representation that's going to squeeze out those zeroes and only represent the values that are non-zero. And it seems a little silly on a small matrix like this, but, trust me, as you get to very large matrices with lots and lots of zeroes, this representation is going to save you a lot of space and save you a lot of computation. So in CSR format we require 3 vectors that together are going to represent this sparse matrix. So the first one is what we call the value vector, and it is simply going to represent all the non-zero data. So here we're simply going to list all the data that are not zero as 1 long array. The second array that we need is recording which column each of these data came from. So for instance, a is in column 0, b is in column 2, c is in column 0, and so on. And finally we have to indicate at which element each one of these 3 rows begin. So the 3 rows begin with value a and value c and value f. So what we're going to write in the row pointer is that value a is at index 0, value c is at index 2, and value f is at index 5. And now we can reconstruct this sparse matrix with these 3 arrays. So let's take the matrix that we saw earlier, and I'd like you to generate the CSR representation of this particular matrix. Please fill in your answers in the value, index, and row pointer arrays. So if you recall the value array, it is simply the non-zero elements of this input array, which we'll just fill in here. The index array here is which column each of these array elements are in. So the 1 is in column 0, the 2 is in column 2, the 2 here is in column 0, the 1 here is in column 1, this 1 is also in column 1, and this final 3 is in column 2. Finally, what are the indices of each one of these elements that actually begins a row? So we should look for the index of this 1, the index of this 2, and the index of this 1 because they are what begin each one of these rows. That's this element, this element, and this element. This element is the zeroth element, this element is the second element, and this element here is the fourth element. So this is the CSR representation of this sparse matrix here. So now we're going to start actually doing the matrix multiplication, and so we're going to show the first 3 steps of this. The first thing we're going to do is take the value vector and the row pointer vector and together we're going to create a segmented representation of the value vector. Note that each segment in this resulting vector represents 1 row of the sparse matrix and the row pointer shows where each segment head begins. The next thing that we're going to do is gather these vector values using these column indices to create a corresponding list from which we can multiply each of these individual matrix entries. So we see that value a is located in column 0, which means it needs to be multiplied by the vector element in row 0. So what we're going to do is use these column indices to gather from this vector, and that's going to give us the following array. Okay, now we actually need to do the pairwise multiplications. So we simply use a map operation to multiply this vector-- not caring about the segmentation--by this vector here, and it's going to give us yet another vector of the same size. And the final step is that we need to add up the partial products within each segment to be able to get the final output vector, and that's where we can use segmented scan. But now we have to add up the partial products on each row to get the output vector. Specifically, we need to add the partial products within each segment, and that's where our segmented scan comes in. We need to do an exclusive segmented sum scan to sum up each segment of partial products. It's actually a little bit more convenient to do a backwards exclusive segmented sum scan so all the sums instead end up at the head of each segment since then the row pointer array can be used to gather those per row sums into a dense vector. Now here we're actually using a segmented scan to perform what's really a segmented reduce. And if you want a good challenge, think about how you might want to build a segmented reduce that's a little bit more efficient than a segmented scan just to go back to our original matrix to make sure we're actually going to get the same answers. So here's our original matrix here and our original vector. And we note that we dot product this vector with this vector to get this answer, this vector dotted with this vector to get this answer, and then this vector dotted with this vector to get this answer. And you'll see if you look at each one of these rows, they're going to correspond to the sums within each one of these segments. But the nice part about multiplying using the sparse matrix representation is that we're never multiplying by any zeroes at all. So here we've actually had to multiply by zeroes, and then we just throw away the answers because they're going to be 0. On the other hand, when we're actually implementing this using a true sparse matrix representation, we're not going to have any zeroes present at all, so this is going to be substantially more efficient for any real matrix that has a significant amount of zeroes. Okay, so the second half of this unit concentrates on sort. Sorting is a fundamental operation, and understanding how we might approach this problem in the parallel world gives a lot of insight as to what works well and what doesn't work well on GPUs. There's a lot of neat algorithms in sorting, and I hope the rest of the unit gives you some new ideas about how to approach the design of efficient parallel algorithms. Now, sort is a challenging problem on GPUs for several reasons. The first is that most sort algorithms are serial algorithms, or at least, usually expressed in a serial fashion, particularly those you might have learned in an algorithms class. So all those nice algorithms that you learned in school are not necessarily applicable here, and we'll see this in a bit. The second is that for performance reasons, we prefer to look at algorithms, parallel algorithms that coalesce memory axises and have little branch divergents among threads, and particularly that are able to keep the hardware busy by keeping lots of threads busy at the same time. So the sort of algorithms that you might have learned in an algorithm course tend to be moving around little bits of memory at a time and they have very branchy code, and they're not often very parallel, so we'd like to take a look at algorithms that have the other characteristics, that can keep the hardware busy, that do limit branch divergence, that do prefer coalesced memory accesses. What we're going to do is look at some classic sorting algorithms and discuss how they might map onto a parallel world. We'll start with one of the simplest algorithms and one that maps nicely to a parallel implementation. Odd-even sort, also known as brick sort. If you're familiar with the serial algorithm called bubble sort, this is the parallel version of bubble sort. So, we're going to start by putting all of our elements to sort in a row, And then we're going to mark all the even elements as black and all the odd elements as red. In an odd-even sort, in the first phase, each red element looks down the line to the left toward the left end and pairs up with the black elements it's facing. Now if that pair is out of order, they swap places and colors as well. Otherwise, they stay in the same places. Now, every red element turns around, faces to its right, and pairs up with the black element in the other direction. Again, they swap if they're out of order. So we continue until the sequence is sorted. So let's try an example. So just to show this with some real numbers, we'll try a sample with these five numbers. We start by pairing them up, starting at the left, and we swap each pair if they're out of order. Then we pair them with the opposite polarity and continue the process. We return to pairing them the way we did in the first step and continue to pair them one way then the other way, then one way then the other way, until we finally conclude with a sorted sequence. So it's very important that we understand how to measure the step and work complexity of these algorithms, because that's often the dominant factor in their run time. So, for this algorithm, what is the step complexity of this algorithm? Order of 1, log n, n, n log n, or n squared, the number of steps. And what is the total amount of work that we need to do with the same choices? Please check your choice. We'll look at step complexity first. The worst case is that an element has to move all the way from one side of the array to the other side of the array. So the example here is the number 5, which starts at the far left and then has to travel all the way to the right. So how quickly does one item move? Its maximum speed is moving 1 position per step since the best they can do is swap with its neighbor and move only 1 position away. So it takes on the order of n steps to get from 1 side to the other. And how much work does it do per step? Well, on every step if we have n items then we're going to do n over 2 comparisons, so in total we do order n steps and order n per step, totaling order of n squared. Overall, this is not a particularly efficient sort. We'd like to be able to do better than order of n squared steps. That being said, it's kind of a neat parallel algorithm because we can see that within a step each one of these comparisons can proceed completely in parallel. So at least, even though this isn't the most efficient algorithm, it's at least one that exploits a lot of parallelism in it's underlying structure. More interesting from the GPU context is a merge sort. And it's really interesting to discuss how to map this efficiently to a GPU because it's a great instance of a divide-and-conquer approach to the problem. So here's a tree that represents our merge sort, and what's particularly interesting about mapping this to a GPU is that the problem starts off with tons of little tiny parallel problems, and then as the algorithm proceeds, we eventually end up with only 1 very large problem to solve, and that's the final merge. This is more challenging many of the algorithms we've discussed, where we have the luxury of being able to solve lots of little parallel problems independently throughout the whole computation. So what we do at each stage of this tree is the same. The only operation that we need is merging 2 sorted lists together into 1 sorted list. We begin with n items, which we treat as n sorted 1-element lists, then we run n over 2 merges to create n over 2, sorted 2-element lists. Then we run n over 4 merges to create n over 4, sorted 4-element lists, and so on. Overall this is log n stages, and in each stage we merge a total of n elements, so the overall number of operations that were complexity is order of n log n. This algorithmic exposes a lot of parallelism within each step, each individual merge: this merge, this merge, this merge, this merge, and so on can proceed in parallel. Now the hard part about mapping this to the GPU is that the number and size of merge as we do on each step differs greatly between the first step and the last. So I'm going to talk about a GPU implementation that has 3 stages. The first stage, this blue stage here, is merging a very large number of very short sequences. Because we have many, many tasks and each is very small, we might choose to assign 1 merge to 1 thread, which can perform each individual merge using a serial algorithm on that thread. We might get better memory coalescing performance if we use shared memories as staging area to read many input elements or store many output elements at the same time. I'd say it's more common, though, at the start of a large merge sort, to just sort a block of elements, say, 1024 within shared memory and then start the merge sort with sorted chunks of size 1024. In other words, if we're actually doing this in practice, we're probably going to use a different algorithm to do this blue stage here with lots of little tiny tasks and then use merge sort to do the last 2 stages here, and we're going to see a good algorithm for an in block sort after we finish the discussion on merge sort. Now we move on to stage 2. Now we have lots of small sorted blocks, and we need to merge these small sorted blocks together. On the GPU for these intermediate merges, we would usually assign 1 merge to 1 thread block. Now, the obvious way to merge 2 sorted sequences is a serial algorithm. So let's take a little bit of a closer look at the algorithm that we choose to use here, and we'll come back to this diagram a little bit later. The obvious way to merge 2 sorted sequences is a serial algorithm, and here's our little serial processor here. The input to this algorithm is 2 sorted sequences, and the output is 1 sorted sequence. And so the algorithm is that the serial processor looks at the head of each one of the sorted sequences, chooses whichever element is smaller, outputs it on to the tail of the output sequence, and then advances the input sequence from which he took the previous element. However, this would be a poor match for the GPU because this is a serial algorithm and it wouldn't keep all of our hardware busy. So it's instructive to look another way, a better way, a more parallel way to merge 2 sorted sequences. If you recall from earlier in this unit, the way that we compacted an array of input elements was to compute the address for each of the input elements to be written into the output array, and then we would scatter these input elements using these scatter addresses into the output array. We've computed, for instance, that element 21 would be scattered by address 5 into output location 5. We're going to have the same goal here with merge, but we're going to use a different algorithm, and our algorithm is going to rely on both of the input arrays being sorted. So let's take a little bit of a closer look. So, mentally, we're going to think about assigning 1 thread per input element in each of the 2 sorted lists. So in this example, we're going to merge 2 sorted sequences of 4 elements each, so we would launch 8 threads, each of which would be responsible for 1 input element. So the goal of each thread is, just like the compact example, to calculate the position of its element in the final list and then scatter that element there. So, to start with, let's figure out what we are trying to do. What we're going to ask you to do is fill in the scatter addresses for these 2 sorts. This is a very useful and common thing that you need to be able to do as you're building a parallel algorithm, understanding the scatter pattern that will let you get to the final answer. Since you're going to generate a dense sorted list of 8 elements, the scatter addresses in these 8 boxes are going to be the numbers from 0 to 7. So how do we do this? Well, we find the smallest element, and we know that it must move to the smallest output location. So, in that case, that's going to be a zero here. This is the smallest element out of all of these. It's going to be scattered to output location zero. The next smallest element is here, and then here, and then here, and then here, here, here, and here. So the key here is, how do we get these numbers? That's the important part of the merge algorithm here. So we're going to put ourselves in the mental position of one of these elements in the first list. So we're going to pick this particular element right here, and we're going to say, "Hey, what position am I in my own list? Well, I'm a position 2 because there are 2 elements in front of me in my list." So, he would be at position zero, he would be at position 1, and I'm at position 2. Now here's the cool part. This guy has to ask, "Where would I be in the other list?" Well, if I look at this list I would need to be right here. So if I was in that list I would also be at position number 2 because there would be 2 elements in front of me. This guy would be at position 0, this guy would be at position 1, and I would be at position 2, so in the overall list I know I'm behind 2 elements here, and I know I'm behind 2 elements here, so I can add those things together and discover that in the final sorted list I would be at position number 4, and that is the scatter address that I need to put him in the right position in the output list. So how do we know our position in our own list? Well, that's very simple. We've launched these threads as a contiguous block with 1 element per thread. So I'm going to be exactly at my thread ID. He's thread ID 0, he's thread ID 1, and he's thread ID 2. So how do we know our position in the other list? That's a little bit more complex. What we need to do to make this work is to do a binary search in the other list. So, this element will look in the other list, move down as a binary search until it finds out where it's going to belong in the other list. Every thread does an independent binary search in parallel in the other list. So for a sorted list of length n, that will take log n steps per element, and all of those elements will do the search in parallel, and that's a very fast operation if we're doing it out of shared memory. Now, for sorting more elements that can fit in shared memory, then what we generally do is read in only a chunk of each sorted list, compute the first part of the output, write it back into main memory, refill the input arrays with more elements, and repeat until done. So in the intermediate stages of the merge, we have all the SMs working on independent merges, one SM per merge. At some point though, when we get to the very top of the merge tree, we only have a very small number of merges left or eventually, just one. We have a single task, but it's a very big task. Now, why is this bad? Why is it problematic for efficiency that we only have one merging task remaining? So why is this bad? Why is it problematic for efficiency that we have only one merging task remaining? Well, could be a number of things. We could have lots of idle threads per SM, We could just have lots of SMs that are idle, or we might suffer from very high branch divergence. So please check the one you think is the best answer. The best answer is that we have lots of SMs that are idle. We know how to keep threads busy within one SM as we did in our immediate merge, but we don't know how to keep lots of SMs busy if we have only one task to do, and it's not at all efficient to have most of our SMs sitting idle. So now we need to turn to an algorithm that allows us to have many SMs working on a single merge. Our strategy here is going to be to try to break up this one huge merge task that we need to do and do a bunch of smaller merges, each of which can then be processed independently and in parallel by a different SM. So the algorithm for doing this is pretty cool. The goal is that when we're breaking up these two long lists, this blue list and this green list, into shorter sub-lists and then merging the results, that we don't have any sub-tasks that are too big to do on one SM. It's okay for us to have some smaller and some bigger sub-tasks, but we don't want a sub-task that's too big for one SM. So here's what we're going to do. We have a big, long list with tons of elements in it, and that's too big for us to be able to deal with, so what we're going to do instead is take a subset of each of these lists. In fact, what we're going to do is take every nth element of each of these lists, let's say every 256th element, so we're going to create much shorter lists now. And so, we're going to call these elements that we select splitters, and they are 256 elements apart. And our list here is going to be A, B, C, D, and so on. And our list here is going to be E, F, G, H, and so on. So then what we're going to do is we're going to merge these splitters to get a single sorted list, say E, A, B, F, C, G, D, H. Now, just as with the merge algorithm we defined a few minutes ago, we calculate the position of each splitter in the other list, so it's very straightforward for us to use the merge algorithm we already know, to merge these two small lists of splitters. Note that the sorted splitter list already tells us in which sub-list our splitter will appear, so just as before we can do binary search for the exact position. Now, here's the cool bit. What I will submit to you is that we now have a sorted list of splitters and the elements that fall between any two splitters in this list are an independent set that we can send to an SM and independently sort. So, the elements between E and A can go to one SM, the elements between A and B can go to another SM, and so on. So let's look at this in a little bit more detail with the elements between F and C. We can calculate the position of F in C's list, we can calculate the position of C in F's list. And so the work that we need to do for the pair defined by F and C is to merge this list with this list, and one of our goals was to make sure that none of these sub lists would be too big, and we can guarantee that because we know that there are no more than 256 elements between F and G, and we know there are no more than 256 elements between B and C, so we can guarantee that there are no more than 512 elements between F and C, so by choosing the spacing between the splitters, we can guarantee a maximum size on any of the independent chunks that we need to independently merge in the last stage of this algorithm. So the challenge we have addressed here, we take one big problem and divide it into a large number of small problems, each of which can be processed in parallel. So to sum up what we just did, we looked at 3 different phases of the merge sort, each of which we attack with a different strategy. First we used one thread block to solve many merging problems in parallel. Here the number of problems was much, much greater than the number of SMs. Then we used one thread block to solve one larger merging problem. Here the number of problems was on par with the number of SMs. Finally, we cooperated between all thread blocks to solve a single problem. Now we have fewer problems than SMs, so to keep all the hardware busy, we needed to find a way to divide the single problem into multiple independent problems, so that we could keep all the SMs busy. And that concludes the merge sort. Now we're going to take a completely different approach to sorting. Generally, most sorting algorithms are data dependent. Based on the values of the data, we choose to do different things. And if we sort 2 different sequences, we'd probably take a different path through the code. Instead, now we're going to consider a set of sorting algorithms that we call oblivious. No matter what input we choose, the sorting algorithm proceeds the exact same way. In fact, the only data dependence we will have in the whole algorithm is a swap operation that inputs 2 elements and outputs them in the correct order. Let me take a brief digression. Why is an oblivious algorithm a good algorithm for a parallel processor like a GPU? So when I talk about an oblivious algorithm, what I mean is that its behavior is independent of some particular aspect of the problem. In this case we're talking about a sorting algorithm that always does the exact same steps no matter what the input is. Good CPU sorting algorithms are generally more clever. They have more complex control flow and a lot of data-dependent decisions to run fast. GPUs aren't so great at complex control flow. Instead, they're great at simple control flow and massive parallelism, and oblivious algorithms are generally a good match for massively parallel approaches to problems. Okay. Let's return to sorting. Clearly, an example will help show what I mean. This structure that I'm showing here is called a sorting network. The input is placed on the lines at the left. So this will input 4 elements--1, 2, 3, 4. Each time 2 values are on either side of a shared vertical line, these values are swapped if they are in the wrong order. So let's put some numbers on there and give it a shot. So we're going to start with the input sequence 4, 2, 1, 3. And so each time 2 elements are connected by 1 green line, we will swap them if they are out of order. So first we'll swap 2 and 4. But we won't swap 1 and 3 because they're in the right order. Now we will look at 2 and 3, and we don't have to swap them but we do have to swap 1 and 4. And finally we have 2 more swaps to do. 1 and 2 are in the wrong order, so we'll swap them. 4 and 3 are also in the wrong order, so we will swap them. And, voila, now we've moved from an unsorted sequence to a sorted sequence. Fortunately, this bitonic sorting network scales in a straightforward and easily programmable way. So we had a little sorting network that would sort 4 items. It's fairly straightforward to expand it so that now it can sort 8. So let me try to give you a little intuition about how that works. So a bitonic sequence is a sequence that only changes direction a maximum of once. So if we look at this sequence here, we're going up for a little while and then down for a little while, but we only change direction right here. So this is bitonic. How about this sequence here? Well, we're going down and then up. 753 goes down, then we change direction and go back up. So sort of the trace of the sequence looks like that and that is also bitonic. But we might have a sequence that looks like this--1324-- where we go up and then down and then up again. This, however, is not bitonic. Now why do we care? It turns out that it's particularly easy to sort a bitonic sequence, and let me tell you how. So let's say we have this bitonic sequence or, alternatively, 2 monotonic sequences that we can turn into 1 bitonic sequence. Here is what we are going to do. We're going to take the first half of this bitonic sequence and we're going to lay it on top of the second half of this bitonic sequence. Then what we're going to do is do a pairwise comparison of each element here. And we're going to take the larger element and we're going to put it in this set here, and we're going to take the smaller element and we're going to put it in this size here. So what we've done is we've partitioned this bitonic sequence into 2 bitonic sequences, one of which is bigger and one of which is smaller. And so then we can recurse and do the bitonic sort on each one of these subsequences and continue until we have a completely sorted sequence. The overall algorithm is generally to sort 2 sequences, we reverse 1, we append it to the other to create a bitonic sequence, we split that bitonic sequence into the larger and smaller halves, and then we sort each half. So if we look at this big picture here, these 2 boxes here sort to input sequences of size 4, this segment here splits 2 sorted sequences into a larger half and a smaller half, and then these 2 boxes here will sort 2 bitonic sequences that result for the smaller half and the bigger half. So if we actually did the analysis here, we would find that for an input set of size n it requires something proportional to log n stages overall. If we actually looked here, we would find that's 1, 2, 3, 4, 5, 6. And the first stage compares and swaps all elements once, the next stage does 2 swaps, and so on. And so the total complexity here is that we have n(log^n) swaps overall. Well, that's all nice theory, but here's the best part. What should immediately draw your eye is that within any set of swaps every swap is completely in parallel. So if you look at this stage here, for instance, we have 4 swaps but each of them can proceed in parallel. We have 4 swaps in this particular stage. Each of those can proceed in parallel. Here's a different permutation of swaps, and again these can all be pursued in parallel. And this obviously makes a GPU programmer very, very happy. Now that we know how this sorting network works, let's think about its performance when given different inputs. For each of the following possibilities with the same number of elements, what are the comparative runtimes in units of time? So a completely sorted sequence, how long does it take, an almost sorted sequence--just about sorted, maybe a couple elements off-- a completely reversed sequence, or a completely random sequence? So here this would indicate that A would be the fastest, then B, then D, then C. So our choices are A, B, D, C; C is fastest, then D, B, A; A is fastest, then B, C, D; or they'll all take the same amount of time. The answer? And this is pretty particular to sorting networks, but with a sorting network all will take the same amount of time. It's an important consequence of an oblivious sorting algorithm. Any input will require the same amount of time to sort. So how do we implement this on the GPU? Well, it's very simple. We're going to assign 1 thread per input element--in this case, 8 input elements. For each comparison, each thread simply needs to know if it's keeping the smaller or larger value. So we're actually doing each comparison twice--once on either side of the comparison-- and the only difference is that 1 side of the comparison will keep the smaller element and 1 side will keep the larger element. Now, we do need to synchronize after each comparison. So after we're done with this particular set of comparisons, we synchronize and then begin the next stage and then synchronize and then begin the next stage and so on. So just a couple notes on the computation itself. One, if the input set is small enough to fit into shared memory, then a sorting network is actually a very efficient way to sort that input set. So in fact, many sorting algorithms that start by sorting small blocks of data, like merge sort, use a sorting network to do their sorts within a block. And it helps that a bitonic sort is probably the simplest sort to actually implement. Two, bitonic sort is not the only kind of sorting network. The odd-even merge sort is a different kind of sorting network that generally runs a little faster, but it's a little more complicated to explain or program. But the basic ideas are the same. So as a quiz, what I'd like you to do is fill in each of the black boxes with the correct numbers to make sure that these 4 input elements are sorted by an odd-even merge sort. So we're going to go about this the same way that we did on the other sort. Every time we see a pair of lines that are connected by a green line, we will compare the 2 elements and put the smaller one at the top and the larger one at the bottom. So we'll compare 4 and 2 and switch their order. We'll compare 1 and 3, and they're already in the right order. Now we're comparing 2 and 1. So 2 will go here and 1 will go here. Now we'll compare 4 and 3 and we say they're in the wrong order as well, and note that 2 and 3 have one more comparison to go. But note that we come out again with a sorted sequence. So we've seen some really interesting algorithms so far, but the GPU performance leader is none of the ones that we've discussed to date. Instead, typically on the GPU for the highest performing sorts we use a different sorting algorithm called radix sort. Now, all of the previous sorting algorithms were comparison sorts, meaning the only operation we did on an item was compare it to another one. Radix sort relies on a number representation that uses positional notation. In this case, bits are more significant as we move further left in the word, and it's most easily explained using integers. So the algorithm for radix sort is as follows. Start with the least significant bit of the integer, split the input into 2 sets, those that have a 0 with this particular bit location and those that have a 1. Otherwise, maintain the order. Then proceed to the next least significant bit and repeat until we run out of bits. So as usual, we're going to do an example that will make more sense, and we're going to use unsigned integers. So what we're going to sort is this column of numbers to the left, and here is the binary representation of those numbers. And so we're going to start here with the least significant bit. So the way that we're going to do this is take all the elements that have a 0 as the least significant bit, and we're going to otherwise maintain their order, but we're going to put them up top. Then we're going to take all the rest of the elements, those that have a 1 as the least significant bit, and we're going to again keep them in order and append them to the list that we've just created. So what this creates is a list where all the least significant bits are 0 and then a list where all the least significant bits is 1. Now we move to the next least significant bit, so the bit in the middle, and we're going to do the same thing. We're going to take all the 0s and put them up top, and then we're going to take all the 1s and put them below. And here the dotted lines are just showing the data movement that we're looking at. The green lines are the ones where the middle bits are 0, and the blue line is the one where the middle bits are 1. Now we move on to the next most significant bit-- in this case, the very most significant bit--and we do the same operation again. Zeroes in the most significant bit move up top, 1s move to the bottom, otherwise, we maintain the order. And now we have a sorted sequence. Pretty cool, huh? Now, there's 2 big reasons this code runs great on GPUs. The first is its work complexity. The best comparison base sorts are O(n log n). This algorithm, on the other hand, is O(kn), meaning the runtime is linear in 2 different things. First, it's linear in the number of bits in the representation. So this particular integer has 3 bits in its representation, and it took 3 stages for us to be able to sort the input. Second, it's linear in the number of items to sort. So we have 8 items in the representation here, and so the amount of work is proportional to 8. Generally k is constant, say a 32-bit word or a 64-bit word for any reasonable applications. And so in general, the work complexity of this is mostly proportional to the number of items that we need to sort. And so that's a superior work complexity to any of the sorts that we've talked about to date, and so that's 1 reason why this looks so good. The second is that the underlying operations that we need to do this split of the input at each step are ones that are actually very efficient. And in fact, they're efficient operations that you already know. Let's take a closer look at what we're doing. We're only going to look at the first stage of the radix sort algorithm, where we're only considering the value of the least significant bit, and we're only going to look at the output for which the least significant bit is 0. Now what are we actually doing here? We've already learned an algorithm that does this operation today. So what is the name of the algorithm that takes this input and creates that as the output? That algorithm is compact. What we're doing is compacting the items that have a 0 as the least significant bit. So if you recall, when we compact what we need to do is have a predicate that are going to tell us which of these input elements to keep. So if we call each of these elements an unsigned integer and that unsigned integer is named i, what is the predicate on each one of these integers that is going to allow us to return true only for this set of integers? So write this in this box in the C programming language, please. So the answer here is that what we're going to want to do is look at the least significant bit. That means we'll take the bit representation of i and end it with the value 1, which is only going to leave us with the value of the least significant bit. And then we're going to test whether that bit is 0 or 1. And we only want to keep the ones that are 0. We only want this to return true if that least significant bit is 0. So what we're really doing here is simply running a scan over the input. So the input to the scan is a 1 for each 0 bit and a 0 for each 1 bit, and that will give us the scatter addresses for the 0 half of the split. So we're going to scatter this element to an output 0, this element to 1, this element to 2, and this element to 3. Notice that the last element of the scan, with a little bit of extra math because it ends with a 0 element here, tells us how many 0 bits there are total in the input. In this case there are four--1, 2, 3, 4. Then we can begin with that value to do a second scan to compute the scatter addresses for the other half of the split, for the 1 bits. There are a number of interesting ways to make this faster, and the most common one is to reduce the total number of passes by taking multiple bits per pass. Four bits per pass and a resulting 16-way split, instead of our 2-way split here, appears to be fairly common. Overall, radix sort is a fairly brute force algorithm, but it's both simple and fast. Recent GPUs can run radix sort on 32-bit keys at a rate of over a billion keys sorted per second. The final sort algorithm we'll discuss is one of the most efficient ones for serial processors; this is quick sort. It is a more complex algorithm on GPUs because of the control complexity of the algorithm, so let's recap what the quick sort algorithm does. First, it chooses a pivot element, one particular element from within its input, then it compares all of the elements in its input to the pivot, and it uses that comparison to divide the input into 3 sub-arrays. Those that are less than the pivot, those that are equal to the pivot, and those that are greater than the pivot, and then it calls quick sort on each of these sub-arrays and continues until the entire input is sorted. As an example, let's look at a particular array and choose that the pivot is equal to 3. So what we're going to do here is compare each one of these elements to the pivot, and we're going to decide if they're equal to, greater than, or less than the pivot. Then we'll divide it into 3 arrays, those that are less than the pivot, those that are equal to the pivot, and those that are greater than the pivot, and we'll call quick sort on each of these arrays and do the same thing. So when we have 2 and 1, let's say we choose 2 as the pivot, then we'll divide this into smaller than the pivot and equal to the pivot. This doesn't need to be recursed because it only has a single element. And let's say we choose 4 as the pivot here, this is greater than the pivot, this is equal to the pivot, and now we have a completely sorted array. So this is a very challenging algorithm to implement on the GPU. The other algorithms that we've looked at are pretty simple to describe, and they don't recurse. This one is more complex, and until very recently GPUs didn't support recursion at all. Indeed, the GPUs we use in this class don't support recursion currently. So how can we take this seemingly recursive-only algorithm and map it to the GPU using the primitives that we've learned? So I'm bringing up this example for two reasons. The first is that you have already learned all the pieces that you need to implement quick sort on the GPU, and the second is to motivate the benefits of new GPU capabilities that do natively support recursion. So we can implement quick sort without recursion by using the idea of segments. Recall that segmented operations, like scans, only operate within a single segment; operations on one segment don't affect other segments. That sounds a little bit like recursion, and in fact it maps in a similar way. For quick sort, when we begin to process the initial array, we're going to use distributes, maps, and compacts to eventually divide it into 3 segments. We can use segmented scans to do all the necessary operations that we need to make this work, including distributing a pivot across a segment for comparisons and splitting a segment, which is similar to the way that we split on a particular bit in radix sort. Quick sort is interesting because it shows you how useful segments can be, that they can let you mirror the same approach you use in recursion, without actually using recursion. But, and I gotta be perfectly honest here, it's really a challenge to implement and equally challenging to make it fast. So it's very exciting that the newest in video GPUs support a more flexible programming model where kernels can actually call, can launch other kernels, which makes quick sort's recursive calls much more straightforward. We're not using this new capability in the class. The Amazon instances where our code is running don't have these new GPUs that support this capability at this time, but it's really exciting, and so when we get to unit 7, we'll see exactly what it looks like and how it applies to Quick sort. Final note on sort. All the algorithms we discussed were what we call key sorts, where we only sort a set of keys. Many interesting applications, however, require that you sort not just a key, but a value associated with that key. For instance, in unit two Dave described sorting a set of NBA players by height. In this case, the key might be the height, and the value would be the player's name, the player's team, the player's history and so on. Dealing with values as well as keys is pretty straightforward. First, if you have a value that's large in terms of bytes, it's usually smarter to make the value instead, a pointer to the actual data, and then just sort the value and its pointer. For instance, if you had a large NBA player data structure, don't sort the whole data structure as the value. Instead, just use a pointer to the data structure as the value. Second, our sort algorithms generally move around data keys, whether that be a sorting network, or a radix sort, or a merge. Instead of just moving the key, if you have to deal with a key value sort, move the key and the value together as one unit. Finally, since today's GPUs can often natively handle 64-bit data, it might make sense to use a 64-bit data type to store both a 32-bit key and a 32-bit value and just change any comparison functions you have to input 64-bit values instead. So if you could sort and scan, and more importantly understand the kinds of algorithms that we study in using sort and scan and why they're good and bad for the GPU, you are in fine shape. The ideas you've learned will carry over to many other algorithms. You'll be using these techniques in the assignment this week where you'll do automatic red-eye removal using template matching. This algorithm relies on sorting a vector by key which indirectly relies on scan, so go apply your new found knowledge to the assignment. Congratulations on finishing unit 4! We've done a lot in this unit, so well done for getting through this. Now we're going to talk to Ian Buck from NVIDIA about CUDA's past, present, and future, and then in the assignment, what we're going to do is learn how to remove red eyes from photographs, so that your photographs can look just a little bit better. In problem set number 4, you will be implementing a parallel algorithm for removing the red eye effect that commonly occurs in pictures of human faces. Here is an example of the effect that we are talking about. You will be implementing a simple algorithm for red eye removal that factors nicely into 3 different parallel operations. The first operation is a stencil computation over the image, the second is a sort, and the third is map. You have been exposed to map and stencil operations in the previous homework, so in this homework, you will focus on sorting. We start our red eye removal algorithm by computing a score for each pixel that estimates the likelihood of that pixel belonging to a red eye, and here's what the score looks like. This score is known as normalized cross-correlation, and it is expressed naturally as a stencil operation. We have computed this normalized cross-correlation square for you, but if you're interested we will provide extra details about normalized cross-correlation in the Instructor Comments. Our next step is to sort all of the pixels in the image according to their score. Note that the pixels with the highest scores are the pixels that most likely belong to a red eye, as you can see here. This sorting step is what you will be implementing in problem set number 4. If you are interested, we will discuss several different types of optimization strategies for sorting in the Instructor Comments. Our final step is to reduce the redness of the high scoring pixels. This computation is expressed naturally as a map operation. Once again we've performed the step for you so you can concentrate your efforts on sorting. That is all, good luck and I hope you will enjoy problem set number 4, and like the previous problem sets I want to thank Eric and Mike for writing the code and the script to this problem set, so thank you. All right, I'm happy to be here today with Ian Buck from NVIDIA. Ian, welcome! &gt;&gt; Hey John, how are you? &gt;&gt; Doing great. Can you tell us what your job is at NVIDIA? &gt;&gt; Sure. I'm the General Manager of Computing Software at NVIDIA. Basically what that means is I'm in charge of all the things for using GPU's that are not really graphics or video related. Things like simulation and problem solving that you don't really do in a game but recycling those cycles to use them more kind of supercomputing kind of applications. &gt;&gt; And you've had a lot to do with the development of CUDA. Yeah, I started the CUDA team many years ago, going on 6, 7 years now, finishing my work at Stanford to do it for real at NVIDIA. So tell us how you got involved in GPU's with the first place. &gt;&gt; It goes way back; I was actually an undergrad. I was working on some of the original SGI Octanes with OpenGL and discovering that actually some of those OpenGL Octane machines for doing graphics-- you know, the kinds of image processing you do in OpenGL could actually be used for simple computational things like fluid flow. So 1 of my undergraduate theses was to study fluid flow and simulation in the OpenGL pipeline on an old SGI Octane, and then went on to Stanford, did a few other things but came back to it eventually and that built a career out of it. &gt;&gt; So the project that you're best known for when you were a graduate student is this Brooke Project. So can you tell us a little bit about that? And this was a very important step in sort of the progression of GPU computing to something that we can do commercially today. &gt;&gt; Yeah, back in the Direct X 9 days of computer graphics, GPUs started becoming truly programmable. People could express real programs, fairly short ones, maybe only 4 to 8 instructions long, that can actually solve a shading problem like face, skin, shade, fairly complicated stuff. A bunch of us in academia noticed that the performance of these GPUs was huge compared to CPUs of the day. The number of floating point operations you could do was an order of magnitude larger. So we studied and looked at how you could use GPUs for more general purpose things by hacking graphics APIs like Direct X. The research--a lot of people showed that was doable. But it was really hard. It was really hard to program. In fact, you needed a PhD in computer graphics to do even basic mathematical kind of stuff. In fact, there were folks in finance that were hiring game programmers to try to hack GPUs to solve financial problems. The research I did was basically study the more fundamental programming model around GPUs. What was it about GPUs that made them programmable and parallel but also present a more cohesive program model to general programmer to let them think about GPUs in a more general way. In the end, the project was called Brook. It was an actual software programming language for how to program in a general way GPUs on top of its existing graphic APIs so that users could think about more general data concepts instead of triangles and pixels and textures. &gt;&gt; What sort of things did people write in Brook? &gt;&gt; We wrote Raytracer to sort of study could a GPU actually implement a Raytracer, which is a fun thing, because lot of people said it couldn't be done. We did triangle meshing. We did a lot of simulation and grid kinds of stuff, thermodynamic problems, heat transfer problems, fluid flow kinds of problems. For the most part, it worked. It demonstrated the performance benefit. It was still early. They weren't fully general, so you did have to work around some of the hiccups, but from a research perspective, it achieved its goal of showing the potential of GPU computing. NVIDIA called and said, "We'd like you to come here and start looking at how we can apply this in a more product-orientated sense." Yeah, NVIDIA was a great partner in the research. They were giving us access to the hardware and some of the early specs so we could think about where they were going and how it influenced our research. When we were all said and done, I got the opportunity to come to NVIDIA and do it for real. I started the CUDA group with one other guy.--it was just the two of us-- and we hired in a compiler and library and QA guys, and we slowly built our way up, this time really taking the learnings from Brook and starting over a little bit, also influencing some of the hardware design, the hardware architecture. What could we do the generalize the architecture just a little bit to make it a little easier, to eliminate those roadblocks that we discovered with Brook, and then eventually come up with our sort of CUDA 1.0? So how long did you take from the day you got there and you started on this project to the time it launched? It was probably a good 2 years of development work that we were pushing to do this work. A lot of it was spec writing, kind of coming up with paper specs, writing some sample code examples, seeing if it was usable, easy to use, picking between a variety of different approaches for how to express the parallelism. Eventually we latched on to a very basic idea of threading because we knew that most C and C++ programmers understood the concept of a thread, at least at the simplest level, and if we could latch on to that we could actually present a program model that was familiar. Very early on in the CUDA project we did a road show. We went out and we talked to every customer who was willing to talk to us, saying, "Tell us your problems around computing. How can we help you?" And one of the big sentiments was they didn't want to learn a new language. And while it was tempting as a researcher to come up with the new parallel programming language of the world, they just wanted an easy way to get access to the performance, and they had a bunch of C or C++ or Fortran programmers. So when we set out to start CUDA, we made sure it's not a new language. It's just a couple of simple extensions to C and C++ so that anyone who knows those languages plus has a basic understanding of threading could get access to the GPU's performance. What are some of the most interesting projects that you've seen done with CUDA from the beginnings, all the way until today? Yeah, you know, one of the great parts about this job is that I continuously learn about fields of science and computing that I had no idea existed. Whether it be, you know, defense, or oil and gas, or finance. One of my more favorite projects is the work being done in CT scanners. A CT scanner is actually--I don't know if you know how this works but it's a series of X-rays they take at different angles, and you're taking 2D X-rays and the machine actually composits the X-rays together and were actually, recreates the 3D model of what's going on inside your lungs or heart or body or whatever. While it's really cool to get that 3D vision, the bad part actually is the radiation is kind of harmful. In fact, you want to kind of use the least amount of radiation, but the more x rays you get the better the view of the problem you have. So what this one group is doing is actually, they're using GPUs and they've been able to reduce the amount of x-ray radiation by something like 20 X, really making it much safe as a procedure. And applying the power of the GPU to take many more lower-dosage radiation shots, and combine them together to not only get a clearer picture with less radiation, but you get it really quickly, in one sitting. You don't have to home and come back to figure out if you have x, y, z problem or issue. So it's really cool to see that kind of stuff truly affecting lives. So if you fast forward a few years, there're been a lot of new ideas that have come into CUDA between the time you launched 5 or 6 years ago and today. So what are maybe the 2 or 3 coolest ideas that you think have come in since the time you launched? &gt;&gt; Well certainly we've added a lot of support for managing concurrency. There's no shortage of parallelism in these codes if people want to move to the GPU. So we've added better support for concurrency, for streams. We've even moved some of that from what started out in software support-- actually is now baked into the hardware with the Kepler and the hyper queue. Those are pieces of silk and they're actually inspired by some of the stuff we were doing in software, originally with CUDA 2.0. Other stuff we've been doing is integrating closer with the memory hierarchy. In CUDA 4.0 we did the UVA work, which is unifying the virtual address base of the entire machine such that--this was work we did so tightly integrating with the operating system such that a bi-pointer value, the system can know if memory is living on the CPU or living on the GPU. So there's no separate memory spaces anymore. You can simply dereference a pointer, it goes to the right machine-- a right CPU or GPU or your peer's GPU. That made multi-GPU programming way easier than we had before CUDA 4.0. &gt;&gt; Yeah, I mean as a professor, I think usually what I say is the most common error is accessing memory on the wrong side of the machine. Yeah. &gt;&gt; You think you're dereferencing a GPU pointer, and it's really on the CPU. That's certainly very helpful in going out and doing those. So the world is becoming more and more mobile, and so the expectation is that the sorts of things we do with big GPU's today-- maybe one day we'll see being done with something you carry in your pocket, so where do you see GPU computing having an impact on sort of the mobile world? &gt;&gt; Well like I said, computing is just sort of everywhere, and it's certainly coming to the mobile space. I think cell phones today are replacing our laptop PCs, and there are opportunities, and there are new mobile devices. It's something that is always on, we're carrying it with us, we're interacting with people and friends, as well as ourselves with the device, and that's another opportunity for computing to make a difference. Camera is one area, and one thing you do see is that cell phones are starting to take over the digital camera market, and there are things that you can do with a digital camera, with a processor, with an interactive test screen-- things like computational photography and computer vision with the camera that just wants to be done on the device and really changes it from a point-and-shoot thing to a more dynamic interface to something interactive, and I hope to see some really cool stuff happen there. So I mean--that's an area we can probably use a lot more computation and use it profitably and actually see the difference, so one of the questions I always ask my students when I'm teaching is, "What are you going to do with 100 times more compute power?" So how about in the more high performance computing area? What are people going to do with 100 times more flops? &gt;&gt; Well there are some problems which have limitless amounts of computational need-- things like understanding disease. The work being done by folks like Vijay Pande at Stanford, where you're trying to simulate the actual progression of a cell as it gets attacked in the disease. He's got a project working on Alzheimer's where he's trying to actually witness a nerve cell and watch a nerve cell die at the atomic level, simulating every gene and protein and cell membrane as it gets infected and eventually deteriorates. Just think how many atoms are in a cell and simulating that in real time or at least in a timeline where you can actually understand disease down to the actual chemical level. It's really going to provide a cool breakthrough for how we treat those things and understand the very complicated pathways and then design the right kind of drug that can get in there and stop it at the right point and keep people healthy. I think that's one of the cooler things we are going to be doing in computers. So what's really surprised you over the last five years in terms of how CUDA has developed and how people have used it? And I'll throw one thing out there. I think the day that I was disclosed on CUDA, right before it launched, if you had asked me at the time I would have said, "In 5 years, maybe people aren't going to be programming so much in CUDA." "Instead people are going to build higher level languages on top of this, and that's what we're going to be using." But CUDA's really proven to have legs and is a very efficient way to keep these machines busy with lots of work to do. So I certainly would have lost a bet I made then. What have been some of the things that have surprised you? I think one of the things that surprised me the most is just the diversity of fields, and really, you can't predict it in some regards but you can. There are applications for computing in tons of spaces, and we have a vision of if computation is free, what would that mean, and what would it mean for everyone? And we can inspire ourselves. But there are all these short-term things that we can go achieve with GPUs that were simply just not--no one thought of. There may be a small group of people out there that were researching them, but by giving them this processor where computation is effectively free and memory bandwidth is 10 to 100x faster than what you're used to, they're just unleashed. ASTRA GPU. There's an actual conference now on doing astronomy simulations with GPUs. And they were one of the first people to jump on the bandwagon because they have a really big problem. They're trying to simulate galaxies and black holes and star formation, and they were just stuck. The simulations were so hard and they'd maybe get to run it on a super computer for a brief few cycles at a time. But now all these guys are able to do their computing right at their desktop and actually prove theory about how stars form, how a galaxy forms. I mean, I think no one ever knew this kind of problem that existed because this little community was off to themselves. But we discovered these things, and we're so excited to see that we make a difference. &gt;&gt; What advice to you have for students that are taking this course that are starting to learn about parallel computing? So what can you say to them that they should be thinking about as they grow and mature in this area? &gt;&gt; Well first off, they've picked a good course, because this is definitely an enabling technology. By having experience and expertise in GPU computing can let them be a vehicle for extracting, you know, that kind of flops are free, computation and unlock some of the things that researchers or industry or even commercial space haven't been able to do yet, because they just don't have the bandwidth, they don't have the compute power, horsepower. Cuda NGB/g computing, in some cases, is meant to inspire. I mean, just think about if computing was free. If you didn't have to worry about, you know, the flops or the bits per second. What could you do with the kinds of problems in your space or in your field? Being part of that revolution is totally fine, and I see it everyday when I work customers and selling GPUs or talking to developers. I think they should be, not only just learning the technology but thinking about what they could do with it and where they could apply it. Thanks so much for spending some time with us today. I appreciate it. Sure. Thanks John. Today we're going to talk about optimizing GPU programs. Now the whole reason we want to use a parallel platform like the GPU is to solve problems faster, and in turn the reason we might want to solve problems faster could be simply because we want to a solve problems faster. Or more often it's because we want to solve bigger problems, or solve more problems. So the good news is that it's often the case that the first initial port of a problem gets a speed up, assuming that you got a parallel problem to begin with, and in my experience, this is actually when a lot of GPU programmers get hooked. You know, over the weekend they go home and out of curiosity they try porting some piece of their existing CPU code to the GPU, and they get a nice speed up, a 5x or 8x speed up. And that's, that's what sort of gets them hooked and makes them realize they could put some more effort into this and get a bigger speed up. So if you have a naturally parallel problem, it's often the case that that first initial Cuda port will get you good speed up, and that's cool. But, you know, by definition GPU programmers care about performance, that's why they're using the GPU. That means they often want to spend additional effort to maximize the speed up beyond that first initial try. So in this unit we're going to talk about how to optimize GPU programs. So cast your mind back to unit 2, when we talked for a little bit about some basic principles of efficient GPU programming. So check which of these principles accurately correspond to things we talked about in unit 2 about efficient GPU programming. Do we want to decrease arithmetic intensity? Do we want to decrease the time spent on memory operations per thread? Do we want to coalesce global memory accesses? Do we want to do fewer memory operations per thread? Do we want to avoid thread divergence? Do we want to move all data to shared memory? Well, arithmetic intensity, basically defined as the amount of math per memory, or to be a little more precise, the amount of useful work, which is essentially math operations that we do for the amount of time that we spend doing memory accesses. So, we want to maximize arithmetic intensity, not minimize it. Spending less time on memory operations clearly helps, and the single most important way that you can do that is to coalesce your global memory accesses. Now, doing fewer memory operations per thread, well this may or may not help. Okay, for example, we might want to move data into the fast shared memory, do some operations on it there, and then move it back, and that would be more total memory operations, but the overall time spent on accessing memory would go down, because we're doing a lot of frequent accesses within shared memory. So this one's not correct necessarily. By the same token, we don't necessarily want to move all data to shared memory, right? Because that could be an unnecessary memory operation. What we really care about is moving frequently accessed data to shared memory. So these 3 are not correct. Finally, the GPU does run most efficiently when adjacent threads take the same path through the code. That means it's good to avoid thread divergence. In this unit we'll give a lot more explanation, examples, and specifics for all 3 of these general guidelines. Now, just as on a CPU, there are different levels of optimization for the GPU. There's picking good algorithms in the first place. There's some basic principles for writing efficient code. There's architecture-specific detailed optimizations, and there's sort of bit twiddling micro-optimization at the instructions level. So, let's use a quiz to explore a few CPU examples. There's the use of vector registers such as SSE and AVX. There's the use of something like mergesort which runs an order n log n time versus insertion sort which runs an order n squared time. Next is writing cache-aware code, meaning write code that is likely to make efficient use of the cache. So an example would be it's generally faster to traverse across the rows of a large 2D array than down the columns, assuming that array is laid out in row order. You'll get better cache performance in that case. Or you can do explicit cache blocking, for example, for the L1 cache and a CPU core. Or finally, you could approximate the inverse square root of a floating point number by shifting it right one bit and subtracting it from the integer 0x5f3759df. Alright, let's get through these. So picking good algorithms, obviously this is the most important level of optimization, and when sorting a large random list, an order N log N algorithm like mergesort is just intrinsically going to be faster than an order N squared algorithm like insertion sort, and clearly this is a case of choosing the right algorithm. You know much of performance CPU programming relies on making efficient use of the cache. This is sort of the number one thing to keep in mind when you're trying to write efficient CPU code, is that there is this big cache hierarchy and you want to write code that's going to make good use of it. And so, I would consider this an example of optimization level 2, right, it's a basic principle of efficiency on a CPU. On the other hand, blocking for the L1 cache, which means carefully sizing your working set to fit exactly in the per-core cache on the CPU. This is a detailed optimization. It's going to depend on the exact CPU model you're using because every CPU core has different size L1 cache. I would also put the use of vector registers like SSE or AVX intrinsics into this category. Not every CPU has SSE; some have AVX. Some don't have either. So, the use of vector registers, the choice of blocking for the L1 cache, cache blocking, I would consider these architecture-specific detailed optimizations. So these kind of architectural-specific optimizations tend to be the domain of what I call ninja programmers. And we'll also touch on a few ninja-style GPU topics like shared-memory bank conflicts. I'll try to highlight these ninja topics with this little icon here. The idea is that these are really not needed or necessarily accessible to every programmer. They tend to be when you're sort of squabbling over the last few percent of optimizations, the last few percent of speedup. I think this is one of the big differences between CPU and GPU programming. On a CPU, these sort of ninja-level optimizations can make a really big difference. For example, if you ignore the existence of SSE registers or AVX registers on really modern processors, then you're only getting a fourth or an eighth of the power of each core on your CPU, so you're taking a huge hit in potential performance. On the GPU, for the most part as a rule of thumb, the speedups to be gained by these sort of ninja optimizations are usually comparatively minor. Okay, so we're talking more like a 30% or 80% speedup by using some of the techniques that we'll talk about with this little ninja icon versus the speedup that you might hope to get from just picking the right algorithm in the first place or just obeying the basic principles of efficiency on a GPU such as coalescing your global memory accesses. These can also often make a difference of 3 times, 10 times, sometimes more. So these, just doing a good job, the sort of higher level principles of optimization matter more on the GPU, and the ninja level optimizations definitely help, sometimes you can get a speedup here, but it's not a sort of vital step to extract maximum performance the way that it is on a CPU where if you don't start doing something to make sure you're using the vector registers, you're going to take a huge hit in terms of the efficiency that you get out of your modern CPU. Finally, this last one is, I really just threw that in there for fun. Clearly, shifting a number, casting it, shifting a floating point number, casting it to a float and subtracting it from this magic constant, is firmly in the category of micro optimization at this sort of bit-twiddling instruction level. This is an infamous hack that's been used in many places, most famously in the video game Quake 3, and if you're curious go to Wikipedia and look up fast inverse square root. Of course these are firmly in the ninja category, and we won't be talking about them at all today or in rest of the course. Let me give a few examples on the GPU. So, on the GPU, picking good algorithms really means picking algorithms that are fundamentally parallel. My favorite example of this is related to the example I gave for the CPU where I compared mergesort, which is order n log n with insertion sort, which is order n squared. Now, in fact, most CPU algorithm texts would probably tell you that heapsort, which is also order n log in, is a better choice than either of these. It tends to run a little better on the CPU, but it turns out that heapsort is all about updating this shared data structure called the heap, and that's a very hard thing to do in parallel. And so, in practice, a mergesort is a much better choice on a parallel machine like the GPU. In terms of basic principles for efficiency, we've already discussed some of these. A big one is coalescing global memory accesses, and another is using shared memory, which is faster for frequently accessed data within a thread block. We'll just touch on a few GPU ninja topics like optimizing bank conflicts and shared memory and optimizing the use of registers, and I've certainly seen my share of GPU bit twiddling micro optimization. One example that comes to mind is the use of floating-point denorms in order to abuse the floating-point arithmetic units for additional integer math performance, and this kind of stuff is just firmly in the ninja category. You know, it's almost never worth the time or the code obfuscation that results, and we won't be talking about them at all today. So you can find entire books, entire courses on how to go about analyzing and optimizing code. The real point is simply that you should have a systematic optimization process. So I'm going to introduce one systematic optimization process called APOD. APOD stands for Analyze, Parallelize, Optimize, and Deploy. And the real point to this process is that it's a cycle. There's nothing really new here. This is just good software engineering discipline. But the point is that we want to break down the process of optimization into several steps and to remember not to short-circuit the really important one, which is to actually deploy the optimizations that you've made into the real world, get some feedback, see how they're performing on real-world data sets. It's too easy to spend all your time circling around these 2 steps and optimize in a vacuum, forgetting that actually you might be able to get a bigger speedup by doing a better job of thinking about what you're trying to accomplish or by going out and taking a partial optimization into the real world and seeing how it performs and getting guidance from that. So this is really nothing more than good software engineering discipline. Let's take these steps 1 at a time. So the analyze stage is really all about profiling the whole application, looking not just at the kernels that you intend to parallelize but looking at the whole thing and trying to figure out, where can this application benefit from parallelization, and how much can you expect to benefit? So once you've decided that there's a region of the code that you need to parallelize, of course there's different approaches to doing this. Certainly no code is better than the code you don't have to write at all. So if you can find that there's a parallel library for the GPU that already does exactly what you want, then great, you're done. You can simply call out to that library. Sometimes you have a lot of code and you want to be able to instrument it, you want to do a minimal amount of work to get a little bit of parallelization, and there's an approach for this called Directives. The most well known one on the CPU is called OpenMP. There's another cross-platform standard called OpenACC which has emerged, which ACC stands for accelerator. This is sort of an extension of OpenMP to encompass the ideas of accelerators like the GPU. And so if you're looking at GPU programming, OpenACC is a really lightweight way to experiment with it. But of course sometimes what you want to do is actually go in and write a parallel routine, and naturally that's what we've been focusing on throughout this course using CUDA C++. So of course we're going to focus here. So assuming that you're going to be coding something up from scratch for this purpose, the next choice is how to pick an algorithm. And this is a huge deal. This is your real chance to make a huge improvement. Pick the right algorithm. So all the bit twiddling optimization in the world won't gain you nearly as much as choosing the right sort of fundamentally parallel-friendly algorithm. I'll give a couple of simple examples in this unit, and you'll see more examples in Unit 6. There's no recipe for picking the right algorithm. What you need to do is sort of think deeply about what is the parallelism in your problem. We'll try to give you a few examples of that so that you have a little practice. So once you've decided how to parallelize your algorithm, then you want to optimize the code, and there will be sort of a cycle between these. As you'll see in the example, you try a parallelization, study how well it does, suggest some changes that might suggest a way that you approach the parallelization differently. So we're really talking about profile-driven optimization, by which I mean measure it. Measure, measure, measure, measure how fast things are going and use that to base your decisions. Don't just sort of take a guess at what's going to work well and what doesn't. And finally, the deploy step. So look, in this class we do these little homeworks, and they're fairly self-contained so that you can get them done in a reasonable amount of time. So the process of actually deploying a GPU-accelerated code into real use is not going to come up a lot in this class, so consider this just sort of free software engineering advice. When you're working on a real code, it's a really, really bad idea to optimize in a vacuum. And what I mean by this is that you can easily spend tons of time adding tons of unnecessary complexity to your code, speeding up a kernel way past the point where it's no longer a bottleneck. And it's just fundamentally a good idea to push any improvements through to the end code. You're making it real by doing that. And making it real as soon as possible ensures that you're running and profiling and parallelizing and optimizing real workloads. And the other thing to remember is that even small improvements are useful. If you make your code 50% faster or 2 times faster or 4 times faster, that's useful and you can push it out to the users of your code; you can start employing it and making it real, even if you think that there's a factor of 20 times speedup in the wings. The advice here again is be disciplined, analyze what you're doing, decide how you're going to parallelize it, decide how you're going to optimize it by studying that code and measuring it using profile-driven optimization, and finally, be sure to deploy frequently. So deploy early and often. Remember that this whole thing is intended to be a cycle. It's also important to understand what it is you're trying to accomplish by parallelizing the code. Say you have a serial program that solves a certain problem size P in time T. Maybe you're simulating how a certain protein molecule folds, and that simulation takes an hour to run. Weak scaling is what we refer to when we're talking about using parallel computing to run a larger problem or perhaps to run more problems. So if you wanted to run a bigger protein, if you wanted to fold a bigger protein in your simulation and still take an hour, or if you wanted to fold many small proteins in the same hour, then that would be an example of weak scaling. Strong scaling means using parallel computing to run a problem faster, a single problem size. So if you wanted to fold that same protein that you were folding in an hour and you wanted to fold it in a minute, then that would be an example of strong scaling. More formally, we could say that weak scaling describes how the solution size varies with a fixed problem size per processor as you add processors. We would say that strong scaling describes how the solution size varies as you add processors to tackle a fixed total problem size. Now there are really 2 components to the analyze stage-- understanding where your application spends its time and understanding what you want to do with the additional parallelism. So understanding hotspots is pretty straightforward. Often the programmer will have some intuition for this. They'll have some idea where the time is being spent in their code. But intuition can be wrong and there's no substitute for data, so don't rely on intuition. So run your favorite profiler, whether that's GProf or VTune or the brilliantly named Very Sleepy profiler, and look at how much time the various functions or lines of code take. You're going to get some output back that tells you how much time each function takes. So this function might take 10% of the time, this function might take 20% of the total time of the program, and in this case, the program is spending 50% of its time in this 1 function, so there's a clear hotspot here. Maybe your intuition would have told you that this is where the time is going. Often you do know that first hotspot, but sometimes you're surprised surprised by that second and third hotspot. Sometimes they're not what you think. And this gives you a starting point of where to begin parallelizing the code and an indication as well of how much speedup you can expect to do so. Let's try that at home with a quiz. So in this example that we've shown here, the question is what is the maximum speedup possible that you could hope for if you parallelized this top function, the function which takes the most time? Okay, that's this one. And then I'll say what if you parallelized the top 3 functions? So what if you parallelized all of these? The top function takes 50% of the runtime, and that means that even if you made it go to 0 time, the program would only go twice as fast. So you'd get a 2x speedup. The top 3 functions between them take 50%, 20%, and 10%, or 80% of the total time. So if you were to make those all drop to 0, the program would be taking 20% of the time that it used to take and you'd have gotten a 5x speed up. So this is an example of a really key principle in parallel programming and, indeed, in all of computer science. We call that principle Amdahl's law. The total speedup you get is limited by the portion of the time you spend doing something that can be parallelized. In the limit of enough parallelism, doing a good enough job with parallelism, having a parallel enough machine, your maximum speedup goes to 1/1/P, where P is defined as the percentage or portion of parallelizable time-- time spent on this parallelizable activity. So it's really important that you know these limits. So, for example, if your application spends 50% of its time on I/O, reading and writing to the disk, that implies that you can't possibly do better than a 2x speedup, unless, that is, you can come up with a way to refactor the computation. For example, maybe you can read and write larger batches of data at once so that the GPU has more work to do on a single batch. And I want to say this situation comes up pretty often in practice, right? GPUs are so fast that often porting the hotspot to CUDA makes that hotspot so much faster that it's no longer the bottleneck. And that's why I made the point earlier that if you have a bunch of functions, you might need to do more than port a single hotspot to the GPU. You might find that after you've ported that first hotspot to the GPU, it's gotten so much faster that now you need to focus somewhere else. And remember, once you've crossed this point, there's little point in continuing to optimize this. The total time on your application is not going to go down very much if you continue to work on this hotspot. You need to shift your attention to the new bottlenecks. Okay, enough about analyze. Let's talk about parallelize. We're going to use a running example to illustrate the concepts that we'll go through over the rest of this unit. And that example is matrix transpose. It's a very simple algorithm that still has enough meat in it to illustrate a lot of the points we want to bring up. Matrix transpose just takes some matrix and exchanges the rows and columns. So a particular element in this row, for example, would have coordinate a,b. So its i coordinate is a, its j coordinate is b, and its new i coordinate in the transpose matrix will be b, and its new j coordinate will be a. Pretty simple. To make our code even simpler, we're actually going to restrict ourselves to square matrices. You'll remember that this came up all the way back in Unit 2 as a really common parallel communication pattern. So transposing a matrix or doing some equivalent operation on an array or an image or a structure of arrays, this comes up literally all the time. So even though it's a simple example, as you'll see, there's a lot of opportunity to optimize it and it's a really important one. Okay. Let's work through some code. Now in this series of coding exercises I'm going to show you, I'm going to be transposing a matrix, and the coordinates of this matrix are going to be giving us I and J. I refers to the column of an element, J refers to the row of an element. Now, this matrix is going to be laid out in row major order, meaning that in memory all the elements of a given row will be adjacent and in the next row, and then in the next row, and so on. And remember that our goal is to take for every element IJ, we want to switch it with element JI. So that's what the code is going to do. Let me slide this over here as a reference, and I'll bring up the code window. So here's how you would code transpose on a serial machine like a CPU, really simple code. I'm going to pass in 2 matrices in and out, and I'm going to walk through a 4 loop that loops over the rows, J, and within each row loops over the column, and then this is where you can tell that the matrix is laid out in row major order. I, J in the input matrix maps to J, I in the output matrix. Okay, really simple, and here's how you would call that code. So, I'm going to allocate a few matrices, an input matrix and an output matrix. And I call this little utility routine fill_matrix, which is just going to put a bunch of consecutive numbers inside the matrix. I call the transpose routine, and I'm going to put the result in the matrix I call gold, meaning that this is our golden reference. We know that this code is correct, and we're going to compare the various GPU versions that we code to this golden reference and then print it. Okay. So let's compile and run this code. We compile it and run it. I made a really small matrix to begin with so we can just see that it's acting correctly. So as you see our input matrix goes from 0 to 63, and the transposed version does the same thing down the columns instead of the rows. So we've written correct code, as we expected, and we're just going to store this routine for generating this transpose in the gold matrix, and we'll use that to compare so we don't have to keep printing out these matrices. For the rest of the lecture, I'll leave a smaller window so that we can see the results without actually printing out the matrices. Ok, so here's how you code transpose on a serial machine like a CPU, and here's how you call it. So let's do our first CUDA version. We'll copy this, paste it, make it a kernel, rename it. So now we've got a kernel that will operate on the GPU and call it. We need to do a few things. We need to allocate space for the input and output matrix on the device, so allocate those, and we need to copy the input matrix onto the device, call our kernel, so what you see here is I've taken that same kernel that we wrote, a serial kernel, I've called it on a single thread, running at a single block, passed in d_in and d_out and then I copy my result and we're done. See, that was quick. GPU programming is easy. I'm actually making an important point here. Okay, this code is going to run correctly, and it was really easy to write. And the problem, of course, is that it won't perform very well, right? It's using a single thread, which is a really small fraction of the horsepower of a GPU, especially a high-end GPU which can run tens of thousands of threads concurrently. So let's go ahead and time this. Okay, so I've added some timer code. Okay, we're going to start the timer, measure it, and I also added--I'm using a helper function here called compare matrices, which just compares the output to the golden matrix that we calculated to make sure that we did the transpose correctly. Lets compile again, run again. Great, so, yes, we performed the transpose successfully, and it took some very small fraction of a millisecond, but this was a really small problem. Lets make this problem bigger again, say 1024 by 1024 matrix. So now we've got roughly a million elements to touch, and we're going to do this in a single thread. Okay, now this is taking almost half a second; that's a long time. So let's keep track of what we're doing. Version 1 is serial in a single thread, and it took about 466 milliseconds. So 466 milliseconds is pretty slow, but sometimes that's okay. For code that's only going to be executed once, code that's not performance critical at all, or code that's going to run on a really small data set, like that 8 by 8 matrix that we started with, it's just not worthwhile to optimize the heck out of this. So even though this simple serial kernel may seem very naive, that's really sometimes the right thing to do, so keep this in mind when you're optimizing. Think about what you need to optimize, whether it's important. Now let's assume that in fact, performance is critical on this section, and that's why we're optimizing it, and let's go back to the code and see what we can do. Now one easy step would be to launch 1 thread for each row of the input, okay? So now here's the code that does that. In this code, we're going to launch 1 thread per row of the output matrix. So the value of i is going to be fixed by the thread ID, and every thread is going to execute just the outer loop of this code we saw before, and the inner loop we're essentially handing off to be run across many different threads. So these 2 codes are almost identical, and the only difference is that we're launching threads instead of looping over values of i. Let's time this. Okay so here's the code for calling our new function. We're going to launch the function transpose parallel per row as a kernel. We're going to launch a single thread block consisting of n threads. Remember, n is the size of our matrix, 1,024, currently. We're going to pass in the input matrix, pull out the output matrix, copy it, and then we're going to print out the timing and verify it. Let's compile and run this code. Okay. Transpose serial ran 484 milliseconds again, roughly what we saw before. Transpose parallel per row is running in 4.7 milliseconds. So obviously we're making a huge improvement by parallelizing this just across the threads of a single thread block. So let's note that down: 4.7 milliseconds, roughly a 100x improvement. So for our next version, let's parallelize this per element, right? So we essentially replaced the inner loop with a thread launch. Let's replace the outer and inner loops with a thread launch. And I'm going to leave this as a programming exercise for you to do. We'll give you the same code that we've been using here with a little bit more instrumentation that should be pretty obvious. Go ahead and run this code, verify the timings. They might be different on whatever GPU and system you're running on. And then add a new kernel, which performs the transpose on a per element basis and see how it times out. Now one thing to be aware of when you're doing this programming exercise is that you can only launch up to a 1,024 threads in a thread block, so I suggest you organize your code to use K by K thread blocks, multiple thread blocks, each of dimension of K by K, and use a value of, like, 16 for K for now. We're going to play around with this value a little bit later. Okay, so here's my solution to that. I started by adding a constant in; K equals 16, like we talked about. So now if you think about this, we're going to have thread blocks, each of which is responsible for a tile in the matrix. Let's draw that. We'll be processing this matrix in blocks called tiles, each tile corresponding to a single in the thread block. Here's the new kernel I wrote. Very similar to the previous one. The difference is that now I need to compute the INJ values, not just the I values, and that computation involves using the block index as well as the thread index, so the block index times K plus the thread index plus x.x gives me my I value. Similarly, I get my J value from the Y, and so now the threads in my grid have an overall index, X and Y, as well as an index within their block, which is the thread index. So I am going to compute that X and Y, and that's going to correspond to the INJ of the output matrix, and that's the calculation that's going on here. And finally I do the same final calculation that I've done all along. I go and grab an element from the input matrix, equal to the coordinates IJ, and I write that into the output matrix, the location of the output matrix corresponding to element JI, and here's how I'm going to call it. Now I need to define the number of blocks and the number of threads, which is simply N over K, N over K. I'm being a little lazy here; you can tell. I'm assuming that N is a multiple of K. There's K by K threads in a block, and now when I launch, instead of launching a single thread, I launch blocks threads. N over K by N over K, and instead of launching 1,024 elements or N elements, I'm going to launch K by K elements per block. Let's go and compile and run this. Okay, now we're talking. So now we're down to about 0.67 milliseconds. Let's make a note. And thinking about what we've done here is we've now extracted the maximum amount of parallelism from the application, right? We started with a single serial code, 1 thread doing all the work. We switched to a code that used 1 thread per row of the input matrix, and now we're down to something that uses 1 thread per element. There's really no more parallelism that we can extract here, And you can see that increasing the parallelism has really helped us. I want to make a point here that--it's a bit of a ninja topic, but it turns out that exploding every last bit of parallelism isn't always the very best performing code. Sometimes it helps to do more work per thread, and this leads to an advanced optimization technique called granularity coarsening that we'll talk about later. With that said, the first problem is almost always to find enough parallelism. So keeping that in mind, are we done? Is this 0.7 milliseconds the fastest that we can transpose this matrix on this GPU? Let's reason that out. So 2 things can limit your performance on any code: Time spent fetching and storing data from and to memory, or time spent performing compute operations on that data. Now the transpose code has almost no computation at all; it's entirely about moving data around. So let's ignore compute for the moment and focus on memory. My question is, are we moving that data around efficiently? How can we tell? There's a handy utility called Device Query that's included in the CUDA SDK. Let's run it. Device Query spits out an enormous amount of information, and most of which you really don't need to know right now, but buried in here are a few things that I want to point out. The GPU clock rate is how fast the actual processors in the GPU are going. The memory clock rate shows you how fast the memory in the GPU is operating. And the memory bus width describes how many bits of memory are actually being transferred for each of these clock cycles. So from this we can actually figure out the maximum speed of the memory, the maximum bandwidth, the maximum amount of data that we can transfer in a second. So here's the 2 pertinent things from that whole device query. Memory clock: 2,508 megahertz. Memory bus: 128 bits wide. As a quick quiz, let's ask, what's the theoretical peak bandwidth? And I'd like the answer in gigabytes per second, because that's how we'll usually hear it described. So 2,508 megahertz is 2,508 times 10 to the 6 clocks per second. Memory bus of 128 bits is equal to 16 bytes per clock. So if you multiply this out, the maximum theoretical peak bandwidth of the memory system is going to be just over 40 gigabytes per second. And as a rough rule of thumb, anytime we achieve something like 40 to 60% of the memory bandwidth, we'd say, "Well, that's doing okay. It's not great. There's probably room for improvement." If you can get into the 60 to 75% range, that's doing pretty well. You might not be able to improve on that. And any time you get over 75%, we would consider that excellent, okay? You'll never achieve this theoretical peak bandwidth on any real substantial code, okay? This is literally just what you get from multiplying out the clock rate and the memory bus, and a real code is going to have an additional overhead. So if you can get over 75% of that, you're doing really well, and you probably don't need to optimize the memory further. So how well are we doing in this code? Let's make that another quiz. So what bandwidth does our kernel achieve? And so remember the last version I showed you ran with a value of N equals 1024, and I told you it took 0.67 milliseconds. And once again, I'd like the answer in gigabytes per second. Our transpose code is reading 1024 by 1024 elements. Each element has 4 bytes, and we are both reading and writing them, so we're going to have to transfer those values across the bus twice. That's the total amount of memory. It's taking 0.67 milliseconds, which is about 1.25 times 10 to the 10 bytes per second, or scaled to gigabytes, about 12.5 gigabytes per second. So we're not really coming that close to the theoretical peak bandwidth. We can probably do better. So, if we go through this analysis for all of these kernels, we'll see that our parallel per-element version of the code achieves 12.5 gigabytes per second, our parallel per-row version of the code gets about 1.8 gigabytes per second, and our serial version of the code gets an abysmal 0.018 gigabytes per second. This is roughly the speed of a carrier pigeon. And a better way to think about this, perhaps, is not in absolute numbers but as a percentage of what the particular GPU we're using can achieve. So, if we were to work out the percentages we're achieving, it's something like 31% of theoretical peak bandwidth with our highest performing kernel, 4.5% peak bandwidth with our per-row kernel, and less than a 10th of a percent with our serial kernel. So, back to the question. Why is this number so low? Well, we can take a pretty shrewd guess that whenever you see really low DRAM utilization, really low percentage bandwidth, your first guess is always coalescing. A way to think about coalescing is that the GPU is always accessing global memory, accessing the DRAM in pretty large chunks, 32 or 128 bytes at a time. And this means that we are going to need the fewest total memory transactions when the threads in a warp access contiguous adjacent memory locations. So, this is an example of good coalescing. Every thread is either reading or writing an adjacent memory location. And clearly, if the threads in a warp are reading and writing completely random locations and memory, then you're going to get poor coalescing, right? So, if these accesses are spread out all over the memory, then the total number of chunks of memory that we have to read could be as large as the number of threads in the warp. So, a random access pattern clearly leads to bad coalescing. So, a much more common access pattern is what's called strided, and this is where threads access a location memory that's a function of their thread ID times some stride. So, for example, thread 0 might access location 0, thread 1 location 2, thread 2 location 4, thread 3 location 6, and so on. In that case, that would be a stride of 2 because there's two elements between thread accesses, and strided accesses range from, okay, like in this case where with the stride of 2 elements, I'm really only doubling the number of memory transactions. So, I'm sort of halving the quality of my coalescing all the way to really, really bad, right? So, you can imagine that if the stride between elements is large enough, then every thread in the warp is accessing a completely different 32- or 128-byte chunk of memory, and then you're guaranteed to get bad behavior. Guaranteed to be maximizing the number of memory transactions that you have to do. So, let's look at the code for our kernels. Here's where we're reading from the input matrix, and this actually works out pretty well. Every thread is reading a value in memory which is equal to some large offset; J times N plus I. And if you look at I, I is really the thread index plus some offset. So, adjacent threads, you know, threads with adjacent thread into season X are reading adjacent values of the input matrix. That's exactly what we want, so this is good coalescing. On the other hand, when we write the output matrix, adjacent threads, threads with adjacent values of I, are riding to places separated in memory by N, right? And N was like 1024. So, adjacent threads are running memory locations that are 1024 elements away from each other. This is clearly bad, this is bad coalescing. Bad, bad, bad, bad. This is, in fact, the root of our problem. This is a really important optimization pattern, so let me emphasize it. In practice, most well-tuned GPU codes are memory-limited. I'll repeat that. Most, not all, but most GPU codes are memory-limited. So, always start by measuring your achieved bandwidth to see if you're using memory efficiently. And if not, ask yourself, why not? It's important to be able to reason about this the way that I just described to you, right? So we sort of walked our way through. We figured out what kind of bandwidth we were getting and what percentage of theoretical peak that was. We saw that it was really quite low and we said, why would we be getting low bandwidth-to-global memory? Well, the first thing you always look at there is coalescing. And then we inspected the code and convinced ourselves that, yes, there's bad coalescing happening when we write to the output matrix. But, I also want to make the point that you don't have to do this from scratch every time. Right? Doing all these calculations is a little bit like rubbing two sticks together to start a fire; it's good to know how, but there are tools to help you do this. The tool that we're going to be using is called nSight. This is an Nvidia product, there's also third-party products. Maybe I'll give some links to those in supplementary material. And if you're using Linux or a Mac like I'm using, then you'll be using the nSight Eclipse edition. If you were using Windows, you'd by using nSight Visual Studio edition. These are integrated debuggers and profilers, they're full-blown development environments. The part that we're going to use is called the Nvidia Visual Profiler, or NVPP. Let's fire that up now. So, this is NVVP. It's a very powerful tool. It has a lot of functionality. We're only going to touch on it today. My real point is simply to point out to you that these tools exist and that even though in this class we're going to focus on understanding the concepts and being able to reason about these things from scratch, I do want you to understand that you don't have to always do this from scratch, that you should use the tools that are available. So the first thing we're going to do is pick an executable to run on. And so this is where we tell it what executable to run, we're going to run our transpose code. Before we do that, I'm actually going to go into the transpose code and comment out the section of it that is running the transpose serial kernel, that first kernel that we figured out. That one is so slow that I don't really want to spend any more time on that. And as you'll see, the profiler takes a little time to run the code several times and acquire statistics about it. So I comment that out and save it. I go back and I recompile my code, and now I'm ready to go back to NVVP, tell it what executable we're going to run, and here we go. It runs the program, it measures all of the calls to the CUDA API, things like malloc and memcpy, and it measures the time taken for every kernel. Here's the transpose parallel per row kernel. Let me zoom In on that. You can see that the parallel per row kernel started a 110 milliseconds into the executional programs, finished at 119 milliseconds into the program, ran a single block at a grid size of 111 so it launched a single block, that block had 1024 threads. You can get a bunch of other statistics from this. If we scroll to the right, we can see the much shorter time taken by our transpose per element kernel. I'll zoom In even further. And this one, as you see, ran in a larger grid of 64 by 64 blocks, each of which had 16 by 16 threads. Now, I'll make the point that we measured a shorter time than that. And that the times that you see in the profile are not going to match up exactly to the timings that we measured a little bit ago, when we were outside the profiler, and that's to be expected. But the interesting thing that we can do in something like NVVP is we can go in and we can analyze the program. So this is going to run the program many times. It's collecting a bunch of statistics about the program, it's averaging them together, and now that we've done this analysis we have a lot more information about these kernels. So if I click on this you see I have more statistics over here, and the one that I want to highlight for you is the global load efficiency, which tells us how efficient our global loads were. In other words, of all of the bytes that we fetched with each memory transaction, how many of them are actually useful? 100%, looks pretty good. That's what we would expect from fully coalesced accesses. The global store efficiency, our stores to global memory, which in our case is writing the output matrix, achieved 12.5%, and that's pretty wretched, right, and that's again what we would expect from having inspected the code. Our total DRAM utilization is down at 7.6%, again remember this is not going to exactly match what we calculated outside the profiler, but we can tell that there's a problem. That was the parallel per-row kernel but only had a single thread block. We can see that the parallel per-element kernel doesn't do much better. It has slightly higher global store efficiency. Very slightly higher DRAM utilization. So, we've still got this problem. Our problem is clearly our ability to write to the output matrix is hampered. We're not achieving the bandwidth that we ought to. As I said before, I'm not intending to go through all the many, many things that you can analyze in NVVP. I'll pull it up once or twice again in the course of this lecture to just illustrate that there are tools to help you figure out what's going on. You can see, in fact, that down here it's actually analyzing the program for you and giving you some suggestions. It's saying look the multiprocessors in your program are mostly idle, you're not getting a lot of work done for the total amount of time this program runs, and your total compute to mem copy efficiency is low; in other words, you're not doing a lot of computation given the amount of time that you spend doing mem copies. Here's the mem copies in our timeline. You can see that we spent, you know, 2 point, let's see, 2.6 milliseconds copying information in and then 8 milliseconds processing it and then another 2.6 milliseconds copying it out. So it warns us, hey, the total compute to mem copy efficiency is low. So these are really useful tools, you should use them. You should know about them, but we're not going to rely on them too much in this class because our point is to teach you how to reason about things from from first principles. So, our problem is bad coalescing on the write to the output matrix. What can we do about that? So our problem is that we're achieving coalesced reads but scattered writes. And our goal is to achieve coalesced reads and coalesced writes. Clearly, swapping the order of the reads and writes wouldn't help because then we'd simply have scattered reads and coalesced writes. So, how do we achieve both of these? The solution is going to be a really important general strategy, so I'll spend a little time on it. The idea is that we're going to take a tile of the input matrix at a time, and we're going to transpose it and copy it into its transposed location in the output matrix. And this is going to be the job of a single thread block, so the threads in this thread block are going to work cooperatively together to perform this copy and transpose of a tile of elements at a time. The threads in the thread block are going to copy the tile into shared memory-- a shared memory belonging to that thread block--they're going to perform the transpose. So now the transpose of these elements, the elements in this tile is happening in shared memory, where you don't pay that tremendous cost that you see in global memory to do a scattered write or read. And finally, the threads in the thread block will work together to copy the elements out. And the key is this. If our tile is large enough--say it's a K by K tile--say the K is 32. In that case, each warp will copy out a chunk of 32 elements at a time into shared memory. And because all 32 threads in that warp are reading and writing in adjacent locations in memory, you'll get good coalescing. And then, when you copy the transposed matrix to its new location in global memory, you can once again do it in a coalesced fashion. So, let's try this as a programming exercise. We'll give you the start of the tile transpose kernel and the code that calls it, and you should modify the kernel code to declare it an array in shared memory and copy appropriate elements in and out of the shared memory so that the final elements are written out in transpose fashion. And don't forget to add any syncthread barriers that you need to make sure that you get the correct result. So here is my code for this. I'll start by setting k equal to 32 and here is the actual code. I begin by figuring out the locations of the tile corners. This is going to tell me where I need to start writing in the output and start reading from the input-- so just a little book keeping and giving things variable names that mean something to me. But as you can see the, the place where I start reading the i value is a function of which block we're in times the width of the tile because each tile is responsible for 1 block and that's the case. And the j value is the same but in y, in y instead of x, and the output simply inverts y and x. Okay, so now that I know where I need to read to write my tile, I'm going to want to know which element of the tile to read and write from, and just a shorthand to make the code a little more readable. I'm going to set x to thread index .x, and y to threat index y. So now, the code itself is really simple. I declare my floating point array in shared memory k by k array of tiles, and I read from global memory, and write the result into shared memory. So here's my read from global memory and its function of where the tile starts plus which thread I'm responsible, or which element this thread is responsible for in the tile. To avoid an extra sync threads, I'm going to go ahead and write this into shared memory in transposed fashion. So it's not tile x y, it's tile y x. Okay, that saves one of these sync thread's barriers. Now I've got the transpose tile sitting in shared memory. It's already been transposed, and I want to write it out to global memory. And I want to write it out in coalesced fashion, so adjacent threads write adjacent locations and memory. In other words, adjacent threads are varying by x the way I've set this out. So here's my write to global memory after my read to from a shared memory. You could have done this in 2 sync threads. You could have read this in the shared memory, performed the transpose, and written it out to global memory. And you have needed a sync threads after reading it in to shared memory and again after performing the transpose. So, if you did that I encourage you to go back and convert it to this single version, and see how much faster it goes. Let's go ahead and run this on my laptop. Okay, there's 2 interesting things to note here. One is that the amount of time taken by the parallel per element code-- the kernel that we had before--actually went up. It's almost twice as slow now as it was before. And if you think about it this code didn't change at all, except that we changed the value of k. We changed the size of the thread block that's being used in this code. We're going to come back to that. That's going to give us a hint as to a further optimization. In the meantime, transpose parallel per element tiled, our new version, is a little bit faster--not a lot faster and that's kind of disturbing. We should have gone to perfectly coalesced loads and stores which should have made a difference. Let's go ahead and fire up NVVP again and see what happened. Okay, so here we've run NVVP on the function again. I'll zoom in on these second functions. Here's our transpose per element tile. This is the kernel we just wrote. And as you can see, it's running with 32 by 32 thread blocks, each of which has 32 by 32 threads. And sure enough we're achieving a 100% global load efficiency and 100% global store efficiency. And yet, our DRAM utilization has actually gone down slightly. So whats going on? Why is our achieved bandwidth still so low? The answer is going to come down to this statistic here--Shared Memory Replay Overhead. But before we get into the details of Shared Memory Replay Overhead, what that means and what to do about it, I want to back up for a little bit and talk about a general principle of how do we make the GPU fast. Let's step back for a bit to remind us why we're focusing on memory the way we are. Our overarching goal, of course, is just to make code fast. So we say great, GPUs are fast, let's use those. But why are they fast? Gpus are fast, first, because they are massively parallel, with hundreds or thousands of processors on a single chip, working for you to solve your problem, but also because they have an extremely high bandwidth memory system to feed those massively parallel processors, okay? So if the memory system can't deliver data to all of these processors and store results from all those processors, then we're not going to get the full speed out of our GPU. And that's why, on a memory limited kernel like transpose, our subgoal is really to utilize all the available memory bandwidth. Hence our focus on global memory coalescing, DRAM utilization, and so on. Now I really want to ask a question a little bit more rigorously. What do we mean by utilizing all the available memory bandwidth? And this is going to bring us to a very important, very simple principle called Little's Law. Let's have the talented Kim Dilla illustrate this for us. Now John Little is a MIT professor who studies Marketing. He formulated his Eponymous Law, when writing about queuing theory in business processes. And Little's Law is usually used to reason about things like optimizing the number of customers in a line at Starbucks, or maybe the size of queues in a factory. But Little's Law is really very general and can be applied to many things including memory systems in computers. In that context, Little's Law states that the number of bytes delivered equals the average latency of each memory transaction times the bandwidth. Let's be a little more precise and emphasize that we care about the useful bytes delivered, and the problem with uncoalesced global memory accesses is that not all of the bytes in every memory transaction are actually being used. That's why coalescing global memory accesses helps ensure that every byte delivered in a memory transaction will be used. So given this definition, what can we do to improve our bandwidth? Let's check all that apply. We can increase the number of bytes delivered, we can increase the latency-- meaning the time between memory transactions--we can decrease the number of bytes delivered, or we can decrease the latency or time between transactions. That's right, we can increase the number of bytes delivered per transaction or we can decrease the latency, the time between transactions. To increase the number of bytes delivered, the Starbucks analogy might be to have many servers simultaneously serving up multiple cups of coffee. Whereas, decreasing the latency means having your servers work faster, taking less time to take an order, create a coffee, and deliver it back. So, let's look at Litte's Law for GPUs. To recap, Litte's Law states that the number of useful bytes delivered is equal to the average latency of memory transaction times the bandwidth. Now, what are some implications of this? First of all, there's a minimum latency to take a signal or piece of data all the way from an SM to somewhere on the DRAM, or to take information from the DRAM and pull it into an SM. Okay, you can find the details for your particular GPU online, but in general any DRAM transaction is going to take hundreds of clock cycles. And by the way, this isn't a GPU thing. This is true of all modern processors. A clock cycle on a modern chip takes half a nanosecond, for example, on a 2 gigahertz chip. And even the speed of light--you know, light doesn't go very far in half a nanosecond. And electricity is even slower, especially on the tiny wires that you find in computer chips. So to go from somewhere inside the GPU off the chip, over a wire somewhere on the board into the DRAM, get a result, go all the way back, hundreds and hundreds of clock cycles, many, many nanoseconds. So this means that a thread that's trying to read or write global memory is going to have to wait 100s of clocks, and time that it could otherwise be spending by doing actual computation. And this, in turn, is why we have so many threads in flight. We deal with this high latency hundreds of clocks between memory accesses by having many, many threads that are able to run at any one time, so after one thread requests a piece of data from global memory or initiates a store to global memory, another thread can step in and do some computation. I find it helpful to think of the memory system as a pipe. Threads issuing requests stuff memory transactions into that pipe. For example, these could be load instructions to read in a certain address in memory. And the result of that transaction eventually falls out the bottom of the pipe, to head back to those threads. Now, the pipe is really deep. It takes 200 or 300 clock cycles for a transaction to move through this pipe. And the pipe is also really thick and wide. Okay, it's designed to be filled with many transactions at the same time from lots of SM's running lots of threads. So when you only have a few threads issuing transactions, the pipe is mostly empty. This could happen, for example, if you don't have all of your SM's, actively filled with threads that are issuing transactions, or if the latency between the transactions coming from each thread is too high. So if we only have a few threads issuing transactions, the pipe is mostly empty and not many bytes are being delivered. And Little's law tells us that if not many bytes are being delivered, then our total bandwidth is going to suffer. So you can make this better by having more threads issuing memory transactions, or by having more memory in flight per thread. So here's one of those strategies. This is measured data from taking a GPU and copying a bunch of data from one matrix to another, just like we're doing. And the important thing to notice is that we're doing it in 3 different styles. We're doing a 4 byte word--so this is the equivalent of a single precision floating point, which is what our code is doing-- an 8 byte word--for example, if we were moving double precision floating point around, that's what we would be getting--or a 16 byte word. And there exist in CUDA data types in the 4, 8, and 16 byte, variety. For example, float, which is what we've been using so far, float 2, which is 2 adjacent floating point numbers, float 4, which is 4 adjacent floating point numbers. And so one option that we could do is we could try to restructure our code around the idea of having a single thread pull in 4 floating point numbers at a time and do its transpose on that. That would move us up from this line. In terms which is a percent of achieved bandwidth, up to this line, that's a healthy increase. On the other hand, the sort of torquing the code around to do something that's less natural-- like manipulate 4 single floating point values at a time-- in this case, it's not a particularly natural thing to do. What we have coming in--an array of floating point numbers or a matrix of floating point numbers. And to read them 4 at a time, in little bitty rows, and transpose those into little bitty columns, we can certainly do it, but I think it borders on a Ninja topic. While this would help, this is the kind of Ninja optimization that will prove useful. I don't think that it's vital. Let's talk about other things that we can do. So if we can't make all of our memory transactions wider very easily, then we can try to have more transactions. Let's have a quiz. Which of our transpose kernels, so far, probably suffer from too few transactions? Our first version had a single thread for the entire matrix. Our next version had a single thread for every row of the matrix. Our third version had a single thread for every element, and our fourth version was tiled. Check those which probably suffered from too few transactions. Well having only 1 thread issuing memory transactions certainly limits the number of bytes delivered and that's our total achieved bandwidth. Version 2 with a single thread per a row still will only have a single thread block running on a single SM issuing transactions, and again this is not going to be enough to fill this big fat pipe. Version 3 had plenty of parallelism and plenty of transactions and flights. The problem there was the uncoalesced right ack operations limited our useful bytes delivered. And our current version of the code, which uses shared memory to achieve better coalescing among the threads and get higher numbers of useful bytes delivered for every transaction, still uses 1 thread per element, and therefore is achieving plenty of transactions to fill the pipeline. So where does that leave us? We're still achieving lower bandwidth than we would have expected. And we know that we're delivering plenty of useful bytes per transaction. So, the problem really must be the average latency. There must be something that's keeping the time between transactions higher than it should be to fill this pipeline. Let's look at the code again. This is our tile per element kernel. And this syncthreads is the problem. Remember we have a thread block of K by K threads, and at the moment, K is set to 32. So, we have a thread block of 1024 threads, and if you think about it, each thread is doing very little work. It's reading a single word into shared memory, and then it's waiting around at the barrier until all the other threads get done with their job, and then it's writing a single word to memory. So most of the threads in this block are spending most of their time waiting on the syncthreads call, waiting on this barrier for everybody else to get there. And the more threads in the block, the more time on average they spend waiting. So for a simple kernel like this, most threads spend a large chunk of their lifetime simply waiting at this barrier until the last few threads complete their read operations. So, what can we do to reduce the average wait time per thread? Should we eliminate the syncthreads call? Should we reduce the number of threads per thread block? Or should we increase the number of threads per thread block, or perhaps increase the number of thread blocks per SM? Well we need the SyncThreads call for correctness. This is what ensures that all the data in the tile has been placed in the shared memory, before we attempt to copy it to its transposed location in global memory. So we can't really eliminate the SyncThreads call. Now the more threads in the block, the more time on average they spend waiting. So reducing the number of threads that are actually waiting in the barrier by reducing the number of threads in the block will work, whereas increasing the number of threads per block will make the problem worse. And this final one is a little more subtle. And this, again, verges on a ninja level optimization. Every thread block runs on a single SM but every SM can potentially hold more than one thread block. In this case, the threads in one thread block can still be making progress while those at the other thread block are gathering at a SyncThreads barrier. So this final strategy of increasing the number of blocks per SM can actually be a good one as well. So what limits the number of thread blocks than an SM can run? Let me digress a bit to answer this question, which leads into the related topic of occupancy. You'll hear this term a lot if you pay attention to the CUDA forums, or if you watch presentations on optimizing CUDA code. Each SM has a limited number of resources. There's a maximum number of thread blocks allowed on an SM--turns out to be 8 on current GPUs. There's a maximum number of threads that a single SM can run across all of the thread blocks on it. This number ranges on modern GPUs from about 1500 threads on, for example, the Fermi-based GPU's that you use on Amazon, or up to 2048 threads on the Kepler GPU in my laptop. Every thread running a given kernel takes a certain number of registers. And there's a total number of registers for all of the threads on the SM equal to 64 K on most GPUs. And finally there is limited number of bytes of shared memory. This is going to be either 16K or 48K on modern GPUs. As at maximum number of 8 thread blocks usually one of these other things is going to get in the way first-- the total number of threads that you want to run across all the thread blocks, the total number registers that each thread is going to take, and the total number of bytes of shared memory that each thread block wants to use. So for example, if I am running on a GPU with 48 kilobytes of shared memory and a single thread block in my kernel requires 16 kilobytes of shared memory, then I can run at most 3 blocks on that SM. Now if that same GPU has a maximum number of threads of 1536, but my kernel takes 1024 threads then I can only run 1 block per SM. So in this case, even though my kernel to little enough shared memory that I'd be able to get 3 blocks per SM, the share number of threads in my kernel is preventing me from running more 1 block per SM. And if I can only run 1 block on that SM, then I'm getting a maximum of 1024 threads on that SM. So my occupancy is not as high as the SM could handle. You can get these vital statistics for the particular GPU you're programming to by looking it up in a CUDA reference manual or by making various CUDA calls on the GPU. And the CUDA SDK ships with a sample called deviceQuery—we saw it earlier— that conveniently collects all of this information into a single useful utility. Earlier we did a print out of the device query, the result of deviceQuery on my GPU. Let's look at that again. It prints out a lot of information. Earlier we were using this to estimate the total peak bandwidth by looking at the memory clock rate and the memory bus width. Now I want to draw your attention to the total amount of shared memory per block, the total number of registers per block, the maximum number of threads per multiprocessor or SM, the maximum number of threads per block. And here's the register and shared memory usage for our tiled transpose kernel. Going back into NVPP, we can see that we're launching a grid size of 32 x 32, a block size of 32 x 32, we're using 7 registers per thread, and we're requesting 4 kilobytes of shared memory per block. Given the register and shared memory usage for the that kernel we just saw and given the GPU statistics from deviceQuery running on my laptop that we just saw, how many thread blocks per SM can we run? And how many threads per SM can we run? And which resources are preventing us from running more? Are we limited by the maximum number of threads per SM or the maximum number of registers per SM, or the maximum amount of shared memory per SM, or by the maximum number of thread blocks per SM? Well the kernel, as we've written it, uses 32 by 32 threads, or 1,024 threads, and the limit on my machine is 2,048 threads. That's keeping us down to no more than 2 thread blocks per SM. The kernel requires 7 registers for each of those threads, so that's a total of 7,168 registers. The limit is 65,536 registers per SM. So this is limiting us to no more than 9 thread blocks per SM. This is not the limit. The kernel is requesting 4 kilobytes of shared memory to store the tile. The limit is 48k. So, this is restricting us to 12 blocks per SM, That's not our limiting factor. And finally, there's a limit of 8 thread blocks per SM total, and that's clearly not the limiting factor. So in the end, the thing that's keeping us from running more thread blocks per SM, and therefore keeping us from running more threads per SM is this first limit, the maximum number of threads per SM. We're limited to running 2 thread blocks per SM, and that means that we're going to be running a total of 2,048 threads on each SM. Let's go through this exercise one more time, this time using the Fermi GPUs that are installed in the Amazon clusters that Udacity is running on. I'll put the device query information in the supplementary material, and you should look through that and answer the questions again. Well, this time you're going to get similar answers. The big difference is that the maximum number of threads per SM is now 1536. All these other numbers are the same. The maximum number of registers per SM is still 64K. The maximum amount of shared memory per SM is still 48K. The maximum number of thread blocks per SM is 8. So we have the same limiting factor, which is the maximum of threads per SM, but now the answer is a little different. Now we can only run a single thread block per SM, and that means our total number of threads per SM will be 1024. So let's look at those results. On my laptop we had a maximum number of threads running of 2048 out of a maximum possible of 2048. This means that as long as we have enough thread blocks to fill the machine, enough thread blocks to run on all the SMs, then we should get 100% occupancy. On the other hand, the Fermi GPUs that Amazon uses and that Udacity runs on were limited to a maximum number of threads running at 1024 out of a maximum possible of 1536, and this means that we're going to achieve at best 66% occupancy, okay, so occupancy refers to the percentage of threads that are actually running versus the number of threads that actually could be running. There's a useful tool that's a spreadsheet in the CUDA tool kit installation called the CUDA Occupancy Calendar that lets you just plug in your numbers and see how your kernels are going to perform in terms of occupancy on all the various GPU's that have existed over time. So the examples that we're looking at get pretty good occupancy. In general, how do affect the occupancy of a kernel? Well, it's usually fairly easy to control the amount of shared memory needed by a thread block. For example, in our transpose kernel that would be the tile size, and, of course, you can change the number of threads and thread blocks that you launch when you launch your kernel. You can also work with a compiler to control the number of registers a kernel uses, though this qualifies as a ninja-level optimization and isn't usually worth the trouble. So increasing occupancy is usually, but not always, a good thing, and it only helps up to a point. It exposes more parallelism to the GPU and allows more memory transactions in flight, but it may force the GPU to run less efficiently when taken too far. This is always a tradeoff. For example, decreasing the tile size and the transpose code will let more thread blocks run. It decreases the amount of time spent waiting at barriers, but if we get to too small a tile, we'll lose the whole point of tiling, which was to coalesce the global memory accesses. So let's go back to that transpose code. So here's our kernel, and you'll recall that we concluded that the syncthreads was really the problem. We need it there, but we've got a large thread block, 32 by 32 threads, 1024 total threads, and most of the time most of those threads are simply sitting at the barrier waiting for the rest of the threads to reach the barrier before they can proceed. So we conclude that maybe we can make this faster by simply going up and changing to 16 by 16 blocks, a very small change that should decrease the amount of time that we spend waiting at a barrier and therefore decrease the latency between memory operations, which, by Little's Law, we would expect to let us increase the total bandwidth. So now we'll compile and run, and sure enough, this time we took just over 0.53 milliseconds. So let's update our running tally of results. The tiled version of our code, we managed to get about half a millisecond by adjusting the tile size. So here's a programming exercise. Let's go back to the transpose code and try a few different tile sizes and see which ones work best. Try 8 by 8, 16 by 16, 32 by 32, and 64 by 64 tiles. This last one will be trickier because, of course, this is more elements than you can put threads in a thread block, so you'll have to think about how to pack this into a single thread block, and I should point out we are looking for the relative ranking of these on the Udacity servers, so using the Fermi-based GPUs as opposed to, for example, your own machine that you might be developing on at home. Okay, so let's quickly recap. We had a serial version of the code that did everything in a single thread-- trivial to write that code, 0 parallelism, and pretty terrible performance. Then we had a parallel per row version of the code that was also pretty simple to write and assigned 1 thread per row and a single thread block-- again did not get great performance, although a huge improvement factor of about a hundred. Then in the third version, we extracted pretty much all the parallelism that the problem has to offer and assigned one thread per element to the matrix. So we did this transposed operation now and sort of massive parallel. And now we got quite a bit better performance. Remember that all this is happening in the context of APOD, Analyze, Parallelize, Optimize and Deploy. And this actually might be fast enough that you're ready to deploy it. So we started by looking at analyzing the code and deciding that this function of transposing matrix need to be sped up. We started exploring ways to parallelize it. And this might be fast enough. This is already a place where you ought to look at this and say is this the bottleneck anymore? Is speeding this up going to make a big difference to my application? If not, then deploy. If so, then we can start thinking about optimizing it. And that was the next thing we did. We looked at the performance here we saw that we were getting pretty poor DRAM utilization. And we diagnosed that the problem must be that our global stores were getting bad coalescing. In other words, when we were writing the output matrix we were getting bad coalescing in those accesses to global memory. So to fix that we came up with the tiled version of our code, where each thread block is responsible for reading a tile of the input matrix, transposing it and writing that transposed matrix to its location in the output matrix. And it can perform these reads and writes in a coalesced fashion. We did this in blocks of 32 by 32 tiles and got excellent coalescing. This improved our speed a little bit. But we're still not getting great DRAM utilization. So, we looked into why. We thought a little bit more about why we wouldn't have taken great improvement in bandwidth. We concluded we probably had too many threads waiting at barriers. So we improved that by shrinking the tile size. Experimenting with different tile sizes led us to conclude that a 16 by 16 thread block writing into a 16 by 16 tile ensured memory, struck a good balance between achieving coalesced writes, reads and writes to memory, and not spending too much time waiting at barriers. Okay, so before we proceed I want to fill in these numbers again and rerun this program to get values that correspond to what you'll see on the Udacity IDE. Until now, as I've mentioned, I've been using my laptop but I want to-- we're going to let you play around with this transpose code, and I want you to see on the screen numbers that are a little bit more comparable to what you'll get on the much bigger GPUs that Udacity is using versus the one that's in my laptop. Okay, so here's the code we've looked at before. This time it's running in the Udacity IDE. And what I want to do now is do a test run. And here's the results we get. So you can see that now transpose serial took 82.6 milliseconds, all the way down to transpose parallel per element tiled. which took about 0.13 milliseconds. So here I've written those results down again, and now we can go through and fill in the DRAM utilization. So you can see that we've got the same pattern. The serial version of the code, which runs in a single thread, takes 82.7 milliseconds. As we get more parallel, we start moving into single milliseconds, and then sub-millisecond. And by the time we get here, we're running quite well at 0.35 milliseconds. And sure enough, by tiling that code we improve on this a little bit, but now we're still spending a lot of time waiting at barriers. And by going to a smaller tile size we can get that all the way down to 0.13 milliseconds. Now at this point we're geting about5 43.5% of the theoretical peak limit of our bandwidth, and that's actually doing pretty good. There's one last optimization effect that we're not going to talk about at this lecture, but you're welcome, in fact encouraged, to go look it up in the CUDA C Best Practices Guide, or on many presentations that you can find online if you're motivated and curious. I decided that it's a fairly subtle effect and not too important. And that's shared memory bank conflicts. Essentially what this says is that shared memory is organized into banks, and depending on how you line up your thread accesses that the array of threads is making to the memory that's actually stored on the tile, you can end up spending a lot of time sort of replaying over and over. You saw that shared memory replays statistic much earlier when we were looking at the NVVP output, and that's what this is about. The fix is relatively simple, and as I said, we're not going to go into it here, but there's a version of this code that you can download and play with on Wiki. The fix is actually simply to reserve a slightly larger chunk of shared memory than you're actually going to use, and this has the effect of padding the shared memory and striping it across the banks, which can actually improve our run time even slightly more. So remember that our goal is to maximize the amount of useful computation done per second, and we've been talking a lot about optimizing memory access because that's usually the bottleneck for GPU codes, but some codes, or some portions of codes, are limited by the actual computation, so let's talk about optimizing compute performance. This topic boils down to two simple guidelines: minimize time spent waiting at barriers and minimize thread divergence. Now, we've already seen an example of the first optimization. Too much time spent waiting at barriers prevented us, in our tile transpose code, from making memory accesses fast enough to saturate global memory bandwidth. By shrinking the number of threads in the thread block, we're able to minimize the number of them that were actually waiting on the barrier and get closer to fully utilizing the global memory. In CUDA this discussion focuses on the concept of a warp. Now remember that a warp is a set of threads that operate in lock step, all executing the same instruction at the same time on whatever data they happen to be processing. And it's general technique is called SIMD, that stands for Single Instruction, Multiple Data. This is a term you'll hear a lot in parallel computing. Computer architects have been building SIMD processors for decades. It saves a lot of transistors and a lot of power if you can amortize the work used to decode and fetch and perform a single instruction against multiple pieces of data at once. All modern CPUs use vector instructions, and the most typical ones on Intel CPUs are called SSE or AVX instruction sets, and these are ways that the CPU can execute the same instruction on multiple pieces of data at once. Going back to a point I made at the very beginning of this lecture, on a CPU, if you're performing an SSE instruction in a single clock cycle, you're affecting 4 pieces of data. On AVX you could be affecting 8 pieces of data. These vector registers represent a huge amount of the computational horsepower, and if you're not using them on the CPU, then you're missing most of the power of your processor. Another term you'll see a lot is SIMT. This is a term coined by Nvidia to stand for Single Instruction, Multiple Threads, and this is a subtle distinction that has to do with what happens when there is thread divergence. Let me explain what that means. It all comes down to what happens at a branch in the code. Here's some code with an if L statement, and I'll color the instruction stream black for instructions that all the threads are going to execute, red for instructions that only threads that take this branch in the if statement will execute, blue for instructions that only threads that take the else branch will execute, and black again. So, here's a bunch of threads executing instructions over time. Now these threads all belong to the same warp, and in all Nvidia GPU's today, a warp has 32 threads. Here's another warp where all the threads take the else clause, so there's no problems with this. All of these threads execute all of their instructions in lock step, but here's a warp where some threads take the if the branch and some take the else branch. And because the threads in a warp can only execute a single instruction at a time, the hardware automatically deactivates some of the threads, executes the red instructions, then flips which threads are activated and executes the blue instructions. And as you can see, if this is time, the code is going to take longer to execute overall. Some of our threads are wasting time sitting idle while their workmates execute. We say that the threads have diverged during this section of code and would refer to this as 2-way branch divergence. If I'd written a nested if-else statement, I could have 4-way branch divergence and could take even longer to execute. So, as a quiz, what is the maximum branch divergence penalty that a CUDA thread block with 1024 threads could experience when executing a kernel? Is it a 2x slow down, a 4x slowdown, or so forth? The answer is a 32x slow down, okay, so since warps only have 32 threads, you can't have worse than 32-way branch divergence. So, at worst, you'll have to deactivate each thread one at a time, execute some code, and then activate another thread and deactivate the one you just executed, and at most you can do this 31 times. So, even if you have a 1024 threads in a thread block, you won't experience worse than 32x slow down. It's actually easy to construct such a kernel. For example, we could have a switch statement with 32 or more cases. If each thread in a warp switches to a different case, and each case took the same amount of time to execute, this segment of the kernel would run 32 times slower than if all the threads in a warp switch to the same case. Okay, so let's explore this with a quiz. So I'm going to give you a bunch of switch statements, and we're going to pretend that they're in a kernel and that all of the cases of those switch statements take an equal amount of time. Okay, so I'll do the first example to explain the format that I'm looking for. Here, we've got a switch statement, this is in kernel code. It's switching on the thread index, .x, mod 32, and that means that you're going to get a number from 0 to 31. And I've just sort of used this short-cut notation to indicate that I've got cases 0 through 31. I want you to assume that all these cases take the same amount of time, and then there's some more code, and later on I'm actually going to launch this kernel, and I'm going to launch it in a configuration like this. It's going to have a single thread block with 1024 threads. And so the question that I'm asking you is, what is the slowdown? Is it 1x, meaning no slowdown at all? Is it 32x, meaning a factor of 30-- you know, assuming all these cases take the same amount of time, it's going to run 32 times slower through this section of the code due to branch divergence? What's going on? And so I'll give you a series of these, using this sort of short-hand notation, and I want you to think about what is the slowdown that you're going to get due to the thread divergence, the different threads in a warp. So the answer to this first one, this is sort of the example we've been discussing. Okay, every thread is going to go a different path. Every thread in the warp will have a different value for this thread index, those values will be 0 through 31, so every single thread will take a different path, and that means that the warp is going to have to activate each thread in turn, run through that particular case, deactivate that thread, activate another thread, run through that case and so forth. So this will lead to the maximum slowdown of 32 times. Here is our next example, and what I am going to try to do for each of these examples is highlight in black the parts of the code that are different from the first example that I gave. So in this case we are similarly launching a kernel with 1024 threads, and now we have got cases 0 through 63, and we're taking the thread index and we're moding it by 64 instead of by 32, Your job is to figure out the slowdown. What about a two-dimensional kernel? Here, I'm using some shorthand to indicate that I'm launching a thread block which has 64 threads in the X direction and 16 threads in the Y direction, again a single thread block, and I'm going to switch on the Y index of the thread. And next, another example where I'm going to switch on the Y index of the thread as before, except this time I'm launching a thread block with 16 by 16 threads. So the thing to understand about this first example is that the maximum slowdown is 32. There are only 32 threads in a warp, so no matter how many cases there are, no matter how many ways you can branch, you're not going to branch more than 32 ways in a single warp because there's only 32 threads in that warp. So again, the maximum here is going to be 32x slowdown for just this segment of code, right, for this switch statement in particular. What about this case? We're going to switch on the thread index of Y, so the thing you need to understand is that CUDA assigns thread ideas to warps in such as a way that the X ID varies fastest, the Y ID varies slower, the Z ID varies slowest, and this is much more clear if I use a concrete example. So in my quiz question I have a 64 by 16 thread block. So let's just draw that out. Here's a thread with X ID 0, X ID 1, X ID 2 and so forth, and these are the Y IDs. This first row has Y ID 1, the second row has Y ID 2, all the way down to Y ID 16, and when CUDA assigns these to warps, it will launch a warp with 32 threads comprising X IDs 0 through 31, then another warp, again with 32 threads containing thread IDs 32 through 63, and yet another warp with Y ID equal to 1 and X ID varying from 0 to 31, and so forth. So now we know the answer to our question. All of the threads in a given warp, in this case, belong to the same Y ID, they all have the same Y ID, and that means that we'll get no slowdown at all because all of the threads will be branching to the same place. What about this next case where we've got a smaller thread block that has 16 by 16 threads? So, in this case, we only have 16 X IDs that it can assign to a warp before it has to change to the next Y ID and start assigning the next set of X IDs, so all of these threads will be placed into a single warp. The first 16 in the first half of the warp, the next 16 in the next half of the warp, next all of these threads will be placed in a warp, and so on. And so now, as you can see, there are 2 different Y IDs for the threads in a single warp. We'll end up having to execute this code twice for this warp, once with 1 Y ID, once with a second Y ID, and we'll end up with a total of a 2x slowdown. Lets do 3 more examples. In this example we're switching on the threadIdx.x % 2 and we're launching a single thread block with 1024 threads, 1-dimensional. In this example we're switching on threadIdx.x/32, integer divide. Otherwise the same, 1024 threads, 1-dimensional. And finally, we'll switch on threadIdx.x /8. Now cases range from 0 to 63 and again we're going to launch 1024 threads single, 1-dimensional block. So what is the slowdown for these 3 situations? Okay. Well, in this case we're really just picking whether or not the thread is even or odd. Each warp will have some even threads and some odd threads, so it's a total slowdown of 2. Here we're doing an integer divide, okay? So threads 0 through 31, when divided by 32, will return case 0, and threads 32 through 63 will return this expression, will evaluate to 1. They'll evaluate case 1 and so forth. So, here we're getting no slowdown at all. All the threads in a 32-thread warp will still end up taking the same case. And finally, threadIdx.x/8. So now there will be 4 different distinct values that this expression evaluates to for the threads in a warp. And so we'll end up evaluating cases 0, 1, 2, and 3 in the 1st warp, 4, 5, 6, 7 in the 2nd warp, and so forth. So this time we'll take a 4x slowdown. So we've talked about if-then-else and switch statements as 1 kind of branch. Loops are another kind of branch. So here is a snippet of CUDA kernel code with 2 loops. So for both of these loops, assume that this function bar is what's going to take the most time, that all these calls to bar are going to dominate the total time spent in foo. And the only difference between these loops is that in this case we repeat a number of times which is threadIdx.x%32, and here we repeat a number of times which is threadIdx.x/32, integer divide. So as a quiz, see if you can figure out which of these is faster--loop 1 or loop 2. In other words, where do we spend total, across the whole kernel, more of our time? And remember, we're assuming that bar is a really expensive function that will dominate the overhead of actually calling a loop. And roughly how much faster is it? So give me just an integer answer. So to answer this, we want to look at these for loops and decide how many times each warp is going to have to execute the for loop. And that means how many times at least 1 thread in the warp is going to have to execute it. So looking at this expression here, for a single warp these values will vary from 0 to 31. So there will be at least 1 thread in that warp for whom this modulo expression evaluates to 31. And that means that the entire warp is going to go through the motions of executing this bar function 31 times. Now, some of those times some of the threads will be deactivated. So the very first time, thread 0 will not execute the bar function. It will be deactivated because i will not be less than 0. And the next time thread 0 and 1 will be deactivated and so forth. Ultimately, the total amount of time that the warp has to spend in this loop depends on the total number of time that any 1 thread has to spend on it. Each warp will executive this loop 31 times. This next loop, though, is different. In this case the integer divide means that threads 0 through 31 are going to evaluate to 0. This expression will evaluate to 0, and therefore they're not going to execute the bar function at all. And in threads 32 through 63 we'll evaluate this expression to 1, and they'll execute the loop 1s and so forth. So there will be 1 warp which evaluates at 0 times, 1 warp which evaluates at 1 time, 1 that evaluates at 2 times and so forth, all the way up to a single warp, which evaluates at 31 times. So the average number of times that all of the warps will execute this loop is 15.5. So now we know what we need to know to answer the question. Clearly, the second loop will execute faster, and it will be twice as fast because, on average, the number of times that the expensive bar function gets evaluated is half the number of times that that bar function gets evaluated during the first loop. Let's look at a real example of branch divergence, the kind of thing that might come up in the real world. Suppose that I'm doing some sort of operation on a 1024x1024 image but I need to do some special operation to deal with the pixels that are on the boundary. For example, if I'm doing a blur, I don't want look off of the edge of the image. So here's a simple kernel code that illustrates this idea. I'm going to call per pixel kernel. I'm going to check to see if the thread index is 0 or 1024 in either x or y. If so, I'm going to call some code that deals with the boundary conditions. Otherwise, I will run some code that does whatever it is I'm doing to the pixel. Okay. You can totally imagine that this is a common kind of operation. You've probably written code like this already in this class. So as a quiz, what is the maximum branch divergence of any warp in this code? Is it 1-way, 2-way, 3-way, 32-way, and so forth? Well, if this is my image and these are the pixels on the boundary, I've assigned 1 thread per element or 1 thread per pixel. A warp somewhere in the center of the image is not going to have any divergence. And similarly, a warp along the top or bottom rows is also not going to have any divergence because in this case all of the threads responsible for the pixels along this row are going to call that special boundary handling code. So it's really only warps like these that are at the beginning or end of a row, of an interior row, that have any divergence. In this case 1 thread will execute the boundary code. That's the one responsible for the pixel in the boundary. The rest of the threads will execute the mainline code, the interior code. So in all these cases you have 2-way divergence, and that's the answer to our question. So how many total threads are in diverged warps--warps with at least 2-way divergence? Well, there's 1 row of pixels which is not diverged down here, 1 row of pixels which are not diverged down here. So there's 1022 rows of pixels that do have diverged warps, 1 here and 1 here, 1 on each side. So the answer is 1022 times 2 warps, which is 2044 warps or 1920 threads. And we launched a million threads to deal with a million pixels in this image, so that means that just over 6% of the threads in this entire kernel are running at perhaps half efficiency for part of the kernel, which is probably not worth trying to optimize. This drives home an important point. You need to be aware of branch divergence. It's one of the fundamental things about GPU architectures or any massively parallel architecture that uses a SIMD approach. But don't freak out just because your code has an if statement or for loop. Reason it out, profile the code, figure out whether this is a real problem worth optimizing. There's no real recipe for reducing branch divergence. What to do depends on the algorithm, and sometimes the right answer is an entirely new algorithm. There are a couple of general principles, though. Try to avoid branchy code. If you have a lot of if or switch statements in your code, consider whether or not adjacent threads are likely to take different branches and if so, try to restructure. The second general principle is to beware large imbalance in thread workloads. Sometimes 1 thread can take much, much longer than the average thread to complete a task, and when this happens, the long-running thread can hold up the rest of the warp or even the rest of the thread block. We saw this in the example earlier where some threads performed a loop much more often than other threads in the same warp. So look carefully at loops and recursive calls to decide if this kind of work imbalance might be hurting you. In the next unit you'll see another example, breadth-first search in a graph, where each thread is assigned a work item of random size. And if 1 thread gets unlucky and picks up a work item that takes a thousand times longer than the rest of the threads, you're wasting a lot of computational horsepower as all the rest of those threads sit idle waiting for that one long-running thread to finish. So there's a lot of simple strategies that could mitigate this imbalance. You can restructure your code entirely. Or you might, for example, coarsely sort the work items before size. And John will dive deeper into parallel strategies for breadth-first search and a lot of other interesting problems in the next unit. It's also worth noting that different math instructions take different amounts of time. And this topic gets maybe half a ninja. You can go really deep understanding the latencies involved in different math optimizations, but there's a few general principles that probably everybody should keep in mind. So the first to keep in mind is use double precision only when you really mean it. 64-bit math is slower than 32-bit math, but it's easy to forget that floating point literals like 2.5 here are interpreted as fp64 unless you add the f suffix. Therefore, this statement on the left will take longer to execute than this one on the right. It's a subtle distinction, and clearly sometimes you need to use double precision, but if you're concerned about performance and you're trying to squeeze the last few percent out of your kernels, only use it when you're absolutely intending to use it. A second math-oriented optimization is to use intrinsics whenever possible for common operations. CUDA supports special versions of many common math operations, like sine and cosine and exponent, that are called intrinsics. These built-in functions achieve 2 to 3 bits less precision than their counterparts in math.h, but they are much faster. There are also compiler flags for fast square root, fast division, 0D norms and so forth. And you can see the programming guide for more detail. Finally, let's talk about optimizations at the whole system level, including the interactions that happen between the host and the GPU. So on most systems the CPU and the GPU are on different chips, and they communicate through something called a PCI Express bus, or PCIe for short. Today, for example, most systems are PCI Express Gen2, which can, in practice, get about 6 gigabytes per second maximum in either direction. PCI can transfer memory that has been page locked, or pinned, and it keeps a special chunk of pinned host memory set aside for this purpose. So when you have a chunk of memory that you want to copy over to the device memory on the GPU, CUDA first has to copy it into the staging area before it can then transfer it over the PCI Express bus onto the GPU. This extra copy here increases the total time taken by the memory transfer. Instead, you can use the cudaHostMalloc function to allocate pinned host memory in the first place, such that the memory you've got sitting on the host is already ready to copy directly over without this additional staging step. Or you can use the cudaHostRegister command to take some data you've already allocated and pin it, again avoiding the extra copy to take it into GPU memory. So that's why you care. Memory transfers from the host to device can go faster if you pin the memory first. And if you're transferring pinned memory, you can use the asynchronous memory transfer, cudaMemcpyAsync, which let's the CPU keep working while the memory transfer completes. How does that work? Well with cudaMemcpy, which you have been using until now, you make a call to cudaMemcpy, you pass it some information, like the pointers and the size and bytes and whether this is a transfer from device to host or host to device. And then there's a semicolon at the end of that statement. And with cudaMemcpy the CPU blocks until the transfer completes. With cudaMemcpyAsync, when it hits the semicolon it just keeps going. This kicks off an asynchronous memory transfer that continues to go on while control returns to the CPU and the CUDA program continues executing. In other words, the CPU can keep getting worked on while this memory transfers. So to control that asynchronous interaction, we have to introduce the next topic in host GPU interaction, and that's streams. So in CUDA a stream is a sequence of operations that will execute in order on the GPU. And the particular operations we care about are memory transfers and kernel launches. If I perform a cudaMemcpyAsync, launch kernel A, then perform a second cudaMemcpyAsync and then launch kernel B, then each operation completes before the next starts. So the first Memcpy happens, then kernel A runs to completion. Then the second Memcpy happens and then the second kernel runs. But if I do those same operations and this time I put them each in a different stream by adding a parameter which is the stream, then now these operations can potentially run concurrently, therefore completing in much less time. So this operation is in stream 1, this operation is in stream 2, this operation is in stream 3, this operation is in stream 4. Since we didn't specify a stream for any of these, we would say that they are in the default stream, which happens to be stream 0. A couple of notes. A CUDA stream object is of type cudaStream_t. For example, here we declare a stream called s1. You create streams using the cudaStreamCreate call, passing in a pointer to the stream you want to create, and you destroy them by calling cudaStreamDestroy. And remember, to get asynchronous memory transfers you need to be using cudaMemcopyAsync, which in turn must be called on pinned host memory. Let's have a quiz on this. If all memcpys and kernel launch operations in the below code snippets take exactly 1 second to complete, what is the minimum time taken before the results are available on the host? So here's the code snippets. We'll start by declaring a couple of cudaStreams, s1 and s2, and by using the cudaStreamCreate call to create streams with those stream objects. And then there's are a bunch of code snippets, and I want you to tell me how long each one is going to take, in seconds. I've taken a couple of shortcuts here. I haven't showed you d array, h array, d array 1, h array 1, 2. I haven't showed you these things getting allocated. You can assume that d_arr has been allocated with cudaMalloc and it is a pointer to some device memory and that h_arr has been allocated in pinned host memory using cudaHostMalloc. And for a shorthand, I just abbreviated CUDA host to device as H2D and CUDA device to host as D2H. Notice that I'm launching a single block with a fairly small number of threads in each of these thread launches. So if all of these operations take 1 second, what's the minimum time that each of them can take to complete? What's the minimum time that it will be before the final results are ready for the host? Okay. Well, this first code snippet doesn't assign streams at all. So none of these operations declare a stream, and therefore they're all in the default stream. And since they are in the same stream, each one must complete before the next one in that stream is allowed to run. So cudaMemcpy will run for 1 second, kernel A will launch and run for 1 second, and cudaMemcpy will run from another second, will copy the memory down, do some operation on it, copy the memory back up. And after 3 seconds the results will be ready for the host. This next code snippet does declare a stream. It puts them all in s1. And so since all 3 operations are in the same stream, the previous operation must complete before the next one can start. So once again, this will take a second, this will take a second, this will take a second, and the final result will be ready in 3 seconds. This one's a bit more complicated. Here I'm doing 2 operations on different chunks of memory, so I'm copying h array 1 into d array 1, calling kernel A on d array 1, and copying d array 1 back to h array 1. And then I'm copying h array 2 to d array 2, doing some operation on d array 2 by launching kernel B, and copying d array 2 back to h array 2. So the first 3 operations are in stream 1. So cudaMemcpyAsync will be called, and it'll start a memory copy to happen on the GPU and then CPU execution will continue; the next statement will happen. We don't stop and wait for Memcpy to finish before we execute the next statement, which is the launch of kernel A. Kernel launches are always asynchronous. So, in effect, this queues up the launch of kernel A, which will be run on the GPU after this Memcpy has finished, and it proceeds to the next statement, which is another cudaMemcpyAsync, to copy the results of kernel A back onto the host. All 3 of these are in stream 1, so they will all wait for each other on the GPU, but the host just issues a command, issues a command, issues a command. It doesn't stop. It proceeds to cudaMemcpyAsync in s2. And so now it starts another asynchronous memory transfer. This one, because it's in s2, doesn't have to wait for any of these operations to complete, so it gets started right away. It'll complete in 1 second. It calls kernel B in 1 block of 192 threads, also in s2, and then calls cudaMemcpyAsync, again in s2, to copy the results back. So the first memory copy will happen, these next 2 operations in s1 will get queued up, and the next memory copy will happen essentially immediately. So in the first second this Memcpy and this Memcpy will be running, in the next second kernel A will be running and kernel B will be running, and in the final second the return Memcpy will be running in s1 and the return Memcpy will be running in s2. And so the minimum amount of time this can take, if all of the operations that can run concurrently do run concurrently, will again be 3 seconds. But in this 3 seconds we'll have actually gotten twice as much work done. We've overlapped the execution here. And I phrased this very carefully when I talked about the minimum time this is going to take. It's worth noting that there's a lot of reasons why it might not be able to overlap all of these operations. Some older GPUs can't do this many asynchronous Memcpys at the same time. And I was pretty careful to launch a single block, so this probably will not fill the GPU. On the other hand, if kernel A had run tens of thousands of blocks, it might well fill up the GPU with those blocks before kernel B got a chance to start filling the GPU with any of its blocks. And so kernel B might end up waiting on kernel A simply because resources are not available to run it. The final example is the same operations just reorganized. So in this case we issue both Memcpys in stream 1 and 2, we launch both kernels in stream 1 and 2, and we issue both Memcpys back in stream 1 and 2. So rearranging these things shouldn't make a difference. The minimum time that this could take is still 3 seconds. Here's 2 more examples. What's the minimum amount of time that these code segments take before their results are complete? Well, in this operation, we're copying some memory in to d_arr1. We're operating in stream1 and then the kernel is launched in stream2, so it can proceed immediately and operate on d_arr2. Both of these operations will complete in 1 second. Everything's done in 1 second. And this one is a bit of a trick question. Once again, we have 2 operations. They're in different streams. So it's correct to say that the final result will be completed in 1 second. But hopefully you noticed that we're both copying some information into d_arr1 and at the same time, because we didn't wait to launch the kernel, we're operating on d_arr1. We're doing something with contents of d_arr1, even as it's being overwritten by the cudaMemcpyAsync. And so while it's true that we'll complete in 1 second, we're going to get the wrong answer. Now you know what this is really useful for? Sometimes you need to perform a computation on a really big hunk of data, maybe so big that it won't even fit into the GPU memory. No problem, you say. I'll break up the data into chunks that will fit on GPU memory. I'll copy each chunk over, do my processing, and then copy each chunk back. Then I'll do the same thing with the next chunk and so on. And this will work fine, and it will get the job done. But the problem is if this is a high-end GPU, it can easily have several gigabytes of memory. And so these copy operations where you're copying several gigabytes of data, enough to fill the GPU memory, do your processing, and then copying several gigabytes of data back, these copy operations can end up taking a significant amount of the time of your total computation. You see our problem. If this is the timeline, we spend some time copying, then we spend some time processing, then we spend some time copying back, and the process repeats. So here's where asynchronous copies can come to the rescue. Instead, we can break up our data into chunks that are half the size and we can do an asynchronous copy, filling half the GPU. Then we can launch some kernels on the GPU to start processing that data and immediately do another asynchronous copy, filling the rest of the GPU data, and immediately start processing that data. When the first chunk of data completes, we'll copy it back and start copying the next half size chunk of data, all while this one is still completing. So you get the idea. We're going to copy over half the data at a time and immediately start processing it while we copy the next half of the data. Now the timeline looks like this. And the important point is that we're managing to overlap data transfers and computation. So this is the advantage of streams. You can overlap data transferring computation. And there's a subtler issue. The ability to launch multiple kernels asynchronously at the same time on the GPU can help you fill the GPU with smaller kernels. Sometimes you have many problems with limited parallelism or sometimes you're doing a computation, such as a reduction, where there's sort of a narrow phase, the amount of work gets less and less. And in the narrow phases, you just don't have enough work to fill up the GPU. Well, maybe you have something else that you can start running on the GPU at the same time to help fill up all of those SMs. Now, there are some details to be aware of. So check out streams and events in the CUDA Programming Guide if you're interested in learning more. All right, it's time to wrap up. Here's what I hope you've taken away from this unit. Remember APOD--Analyze, Parallelize, Optimize, and Deploy. The important points here are do profile-guided optimization at every step and deploy early and often rather than optimizing forever in a vacuum. I can't emphasize this enough. Optimization takes effort and often complicates code, so optimize only where and when you need it and go around this cycle multiple times. Now, most codes are limited by memory bandwidth. So compare your performance to the theoretical peak bandwidth and if it's lacking, see what you can do about it. Things that will help improve in order from most to least important are assure sufficient occupancy, make sure you have enough threads to keep the machine busy. This doesn't mean having as many threads as you can possibly fit on the machine, but you do need enough that the machine is basically busy. Coalesce global memory accesses. Really strive to see if you can find a way to cast your algorithm so that you achieve perfect coalescing. And if you can't, consider whether you can do a transpose operation or something that will get poor coalescing once but then put the data into a memory form where all your subsequent accesses will get good coalescing. Remember Little's law. In order to achieve the maximum bandwidth, you may need to reduce the latency between your memory accesses. And so for example, we saw that in 1 case we spent too much time waiting at barriers. By reducing the number of threads in a block, we are able to reduce the average time spent waiting at a barrier and help saturate that global memory bandwidth. We've talked about minimizing the branch divergence experience by threads. Remember that this really applies to threads that diverge within a warp. If the warps themselves diverge--in other words, if all the threads within a warp take the same branch, go in the same code path--then that comes for free. There's no additional penalty for threads in different warps diverging. It's only when threads within a warp diverge that you have to execute both sides of the branch. As a rule, you should generally try to avoid branchy code-- code with of lots of if statements, switch statements, and so on. And you should generally be thinking about avoiding thread workload imbalance. In other words, if you have loops in your kernels that might execute a very different number of times between threads, then that 1 thread that's taking much longer than average thread can end holding the rest of the threads hostage. All that said, don't let a little bit of thread divergence freak you out. Remember we analyzed a real-world example of dealing with boundary conditions at the edge of an image and figured out that in fact the if statements to guard the edge of the images weren't really costing us very much. Only a few warps ended up being divergent. If you're limited by the actual computational performance of your kernel rather than the time it takes to get the data to and from your kernel, then consider using fast math operations. This includes things like the intrinsics for sine and cosine and so forth. They go quite a bit faster than their math.h counterparts, at the cost of a few bits of precision. And remember that when you use double precision it should be on purpose. So just typing the literal 3.14, well, that's a 64-bit double precision number, and the compiler will treat it as such, whereas typing 3.14f tells the compiler, hey, this is a single precision operation, you don't have to promote everything I multiply this by or add this to to be a double precision number. Finally, if you're limited by host device memory transfer time, consider using streams and asynchronous memcpys to overlap computation and memory transfers. And that's it. Now go forth and optimize your codes. In Problem Set Number 5, you will be designing and implementing a fast parallel algorithm to compute histograms. You already implemented a kernel for computing histograms in Problem Set Number 3 that used atomic operations. However, it turns out that we can get much better performance if we avoid atomic operations all together. And that is our goal in Problem Set Number 5. There are a lot of different ways to efficiently compute histograms. We strongly encourage you to experiment and test out different ideas, but one basic strategy that works well is as follows. First, sort the input data into coarse bins. Two, use each thread block to compute a local histogram for the data that falls into a single coarse bin. Three, concatenate each of the coarse histograms together. This strategy is advantageous, because each local histogram will have a small number of bins, so it will be able to fit into shared memory. This is true regardless of how many bins the final histogram has. Let's look at a simple example to see how this strategy will work. Suppose our input data is in the range of 0 to 999. And suppose we want to bin our input data into 100 different equally sized histogram bins. And here's our data. As you can see, it ranges from 0 to 999. And now, let's compute the bin ID for each input element as well as the coarse bin ID for each input element. So let's suppose we have 10 equally sized coarse bins. And this is the result of calculating the bin ID as well as the coarse bin ID for each of the input element. And now, let's sort the final bin ID by the coarse bin ID, and let's see what we have. And this is the final result after we sort the bin ID by its coarse bin ID. Now, let's assign a thread block to each coarse bin. So thread block 0 is responsible for computing the histogram of coarse bin ID 0. Thread block 1 won't have anything to do, because in the sorted coarse bin ID, there is not a bin with an ID of 1. So it will compute an empty histogram. Thread block 2 will compute the histogram of coarse bin ID of 2, and so on. And to get our final histogram, we can simply concatenate all those histograms together end to end. So when we would combine the histogram that we calculated for coarse bin ID of 1 with the histogram that we computed for coarse bin ID of 2 and so on and so forth. And that's it. Good luck. We're getting pretty close to the end of this class, so congratulations on making it this far. Now one of the most challenging skills in all of GPU computing. is trying to parallelize a problem that you've never seen before, so what we're going to do today is look at several examples of interesting applications where we've done that. Hopefully this is going to give you a few more tools in your GPU computing toolbox. So let's get started. In previous units, you've learned about the building blocks of GPU computing, the fundamentals of parallelism, mechanics of writing programs, and techniques for making them fast. However, one of the challenging things about learning a new programming model is what happens when you run into a kind of problem you've never seen before. So what we're going to do in this unit is look at a variety of interesting problems to solve, and work through how we might map these problems efficiently onto the GPU. There's no magic pill that will allow you to instantly think in parallel, but what I've found helpful for me is to see a variety of problems and how they map to parallel algorithms and data structures. It helps me have a little broader tool box of techniques and lessons that I can use in attacking other problems I may not have seen before, and I hope the same will be true for you. Let's take a look at a commonly used kernel in scientific computation. This is called N-body computation. In an N-body computation you have N objects, each of which exerts a force on each other object. You'll use N-body computations to, say, model the gravitational interaction between stars and a galaxy or the electrostatic forces between a number of charged particles. It's the way we compute molecular dynamics, which in turn can model protein folding and misfolding and is an important component of our understanding of disease and drug discovery. N-body techniques are used in folding at home, for instance. The specific formulation of N-body computation that we'll discuss today is called all-pairs N-body, and I'm basing this discussion on an excellent article by Lars Nyland, Mark Harris, and Jan Prins in 2007 in GPU Gems 3. All-pairs N-Body is the simplest N-body computation. It's known as a brute-force method, and it involves separately calculating the force between each pair of elements and then adding up the resulting forces on each element. A simulation using N-body will usually have two steps. First, compute the forces on each element, then move each element a little bit based on its force, and then repeat. So we first want to understand the complexity of this approach. So we have N objects, each of which computes the forces on it from each other object. For N objects, what is the work complexity of computing all-pairs N-body? Is the amount of work proportional to N, N log N, N squared, or N cubed? All-pairs N-body is an N squared algorithm. Each object must compute its interaction with each other object, So each N object has to compute N minus 1 forces. N objects times N minus 1 force computations per object yields work proportional to N squared. Now, there's a way you can approximate N-body computations with high accuracy with superior work efficiency. There are tree-based algorithms, for example. One's called Barnes Hut, that has N log N complexity and a method called fast multi-pole that's actually linear order of N complexity. They basically work by lumping distant bodies together and treating them as a single body during the calculation. These are neat algorithms; fast multi-pole was actually named one of the top 10 algorithms of the 20th century, but they're also considerably more complex. All-pairs, on the other hand, is very simple and often the way that production N-Body algorithms compute forces among close together objects, and with it, we're going to show how to use the memory hierarchy in an efficient way to make this computation as fast as possible. So let's start by looking at how we might implement this as simply as possible, and we're going to begin by thinking about this computation as working on an N by N matrix. The N bodies on the X-axis here are the source of each force, and the N bodies on the Y-axis here are the destination of each force. The source and destination objects are the same objects, of course. This is just how we're organizing the computation. So the matrix element at the position X,Y represents the force that source element X exerts on destination element Y, and what we want to do is calculate the total amount of force from all source objects on each destination object. This formulation of the problem exhibits a lot of parallelism. Each of the N-squared forces can be computed in parallel. We just load the parameters of the source and destination thread to do one particular computation, and compute the force, say, using gravitational attraction. This requires specifying parameters for each object. For gravity, this would be, say, the mass and the position, X, Y, Z. Then each object, say object Y, must add up all N forces that act on it. Then, we have to calculate N reductions of N forces also in parallel. Since we want to run this computation on tens to hundreds of thousands, maybe even millions of elements, we can see that we have a lot of parallelism to exploit. In this case, the computation would be very straightforward. We will now see that if we run it in this way, we would be making poor use of our memory bandwidth, so if we assume that all of our N-squared force computations running in parallel must go to global memory, how many times must we fetch each element as a function of N? We know that there are going to be N source elements, and we're going to have the same N destination elements. So for any particular source element, say X here, we must fetch that source element for each individual computation for every destination element, and for a particular destination element, we know that we have to fetch the parameters for each individual source element. So overall we have to fetch each element N times as the source and N times as the destination, or if we assume that an object does not exert any force on itself, then we'd fetch each element N minus 1 for each of the source or destination. The answer should be either 2N or 2N minus 2 times; This is quite expensive. We'd really like to organize the computation in such a way that we consume less global memory bandwidth, and we're going to try to do this by storing and reusing some of these memory fetches, and in particular, we'd like to be able to take advantage of the GPU's ability to share data between neighboring threads. The answer is 2P. We'd like to fetch these P items once each, we'd like to fetch these P items once each and never have to fetch anything again. Let's start by thinking about how we'll put tiles together after we compute them. Each individual tile will compute partial results for these P destination threads. The partial results will reflect the contributions of the forces produced by these P objects, then, for each of these tiles, we'll add the partial results from all the tiles in a row to get the complete set of forces that act on the P threads in this band right here. So let's say we run this tile with P squared threads. That's going to allow us to compute all P squared individual forces at the same time. This is good, very parallel, but it's got a couple of not too major, but still significant, downsides. First is that we would have to share both source and destination parameters for each object across multiple threads. Remember, we're only fetching each item once. How many threads must get the information of parameters for each source object and for each destination object? So remember that we only fetch each source object once, but we have to distribute that information across all P threads, and for each destination object, again we're going to fetch that information once, but then we have to distribute that information across all P source threads. It would be nicer if we didn't have to do this because we can store that information in shared memory once, but then we have to read out of shared memory, which is slower, or we have to replicate that information in each thread's local storage, which wastes space and limits the number of threads we can have resident in a single thread block. The second downside is that we have to communicate between threads to sum up the forces. So we've individually computed each separate force on separate threads, and then we have to communicate between all these threads to sum up the force for this particular tile. We'd prefer not to have to do that either, so instead we'll look at another way to compute a tile. Instead of having P-squared threads to compute P-squared force interactions and then summing them up between threads, we'll only use P threads total. Each of these P threads will then be responsible for P interactions. Another way to think about it is that 1 thread, this horizontal thread here, is responsible for applying the force of all P source objects to 1 destination object. So the code for this is really simple, and we're showing it here. Our kernel code simply loops N over P times over this particular routine. It accumulates P by P interactions onto P threads in each iteration. So what's happening in this code? Well the input to the kernel is my parameters, those associated with my destination object. Those would be say, your position and your mass or your charge and the accumulated force that has acted on my object so far. And the output will be the accumulated force after I add in P more interactions, 1 for each of the source objects with which I'm interacting. So we assume that we store parameters for the P source objects in shared memory, sourceParams here, and then we just loop P times over those P objects. We compute the interaction, and this might be computation of gravitational force, it might be electrostatic force, and the params will differ depending on what kind of force it is. And then we'll compute the interaction and add resulting force into the force accumulator variable and return it. Why is this transformation a good idea? We still have to share the information for a source object up here across all these threads. That doesn't change from our previous implementation. But now we don't have to share any destination object information at all, because 1 thread is completely responsible for 1 destination object, so we can load that object's information directly into that thread's local storage. And we don't have to communicate between threads to sum up individual forces. So it used to be we had P different threads here, and we had to communicate between those P threads to add up all these forces. Now because we only have 1 thread that's responsible for all these forces, we don't have to do any communication between threads at all. That thread can just accumulate all of the partial results in its local storage. So the result is a faster implementation overall. Dave discussed this technique generically in the last unit, Unit 5, and he used the term privatization. So in making this transformation, is the amount of parallelism now increased, decreased, or kept constant from the previous implementation? This transformation has actually reduced the amount of parallelism. Previously, we had P-squared threads per tile and P-squared way parallelism within a tile. Now we only have P threads per tile, so we have less parallelism than we did before. Well does that matter? Well, if we have a big, big N body problem to solve, it probably doesn't matter. For a big, big problem there's probably so much parallel work that will easily fill the machine, even with our new version, but if we have a more modest-sized problem, the lesser amount of parallelism might mean this method isn't the best, because we might reduce the amount of parallelism to the point where we can't keep the whole GPU busy. The big picture by what we've done here is that we've chosen an implementation that has fewer threads and does more work per thread. You'll often see this technique when you can convert communication between threads, which is correspondingly more expensive to communication within a thread, because communication between thread local storage is faster than communication between threads within shared memory. Finally, how big should we make a tile? Let's think about this. If we choose a larger P, does that mean more or less total main memory bandwidth? The answer here is less. We load each source object's parameters once per tile. Larger tiles means fewer tiles and thus fewer loads of each source object's parameters. Now if P is too big or small, then we can't use shared memory to share a source objects parameter across all destination object threads. The answer, too big: If P has more threads than a thread block is allowed to have, then we can't use shared memory to share data among all P threads, because we have to distribute that tile across multiple thread blocks. Another consideration is making sure that we have at least as many thread blocks as SMs or else SMs will sit idle. In this problem you need to divide your work up into chunks; in this case, tiles. We have a continuum between tiny tiles--lots of them, and fewer tiles where each tile is sized to the maximum that can fit in a single thread block. And in this particular problem, bigger tiles means less memory bandwidth; this is good. Generally then you want to make your tiles as big as can fit into a single thread block, because that minimizes overall memory bandwidth. But note the following 2 caveats. One, you need to have at least as many thread blocks as you have SMs in your GPU, because otherwise you'll have SMs sitting idle. Definitely you want to make sure fill the machine with enough work to keep all the SMs busy, even if you have to move a little bit this way on the continuum and size your tiles just a little bit smaller. Two, if you're sitting at the right end of this continuum, it's best for overall memory bandwidth, but often it turns out that you would actually prefer to just maybe 1 tick to the left. This allows a small number, say, 2 blocks to both B-resident at a time, And that potentially gives better latency-hiding characteristics, because you have more warps that may be in flight at the same time from slightly different pieces of the program. It's certainly something that you would want to tune carefully if you needed the fastest possible implementation. So let's summarize the big ideas of all-pairs N-body include. This might not be the most efficient algorithm. It's not. But it's both simple and high performance. So if you implement this, you'll find that you're actually able to sustain a significant fraction of the peak theoretical value of the GPU. The real advantage here is that we have a very large amount of parallelism in this problem that we're able to exploit well in our implementation. The interesting part of this implementation is how to structure the ample parallelism that is available in this problem. And the big idea we discussed is the tradeoff between exploiting the maximum amount of parallelism and doing more work per thread. And there's many problems where increasing the amount of work per thread and reducing the amount of parallelism is really the right thing to do because of the speed of communicating results within a single thread is faster than communicating between threads. In Unit 4, we looked at Sparse Matrix Vector Multiply, SpMv, noting that this is one of the most important operations in all of computing. What we're going to look at today is how to implement this efficiently. So first a quick recap on what SpMv is. We wish to multiply a matrix by a vector. If this matrix is dense where every entry in the entire matrix is represented in the matrix data structure, GPUs can get excellent performance, but many interesting matrices are what we call sparse. Many, if not most, of the entries in these matrices are 0. So we prefer to represent only the non-zeros in this matrix, which gives us 2 advantages. One, less storage since we don't have to represent a bunch of zeros, and two, less computation, since we're not doing a bunch of multiplications and additions of zeros. If you need a refresher on how to multiply a sparse matrix by a dense factor, please take a short trip back to Unit 4, because what we're going to talk about today is a strategy to build a more efficient SpMv. The broad question that we want to answer in thinking about how to make this problem efficiently is what is the role of a thread? Let's look at this sparse matrix to answer this question. Here we're using X's to represent non-zeros and blanks to represent zeros. We will consider 2 alternatives. The first is that we will assign 1 thread per row of the sparse matrix. Here a thread is responsible for computing 1 entry in the output vector, and we do this by taking the dot product of this row with this column. Second, we will assign 1 thread per element in the sparse matrix; for instance, this element here. Here a thread is responsible for doing 1 multiplication; in this case, times its corresponding element in the input vector-- what we call a partial product, and then cooperating with other other threads-- this thread, for instance, in summing up the partial products. So which is better? The answer is, it depends, and we'll look at how to do both of them. First though, a little quiz to make sure we think about the implications of the 2 approaches, which we're calling again, thread per row and thread per element. It's not immediately clear which is better. So for the same matrix, which approach will answer these 3 questions for-- please choose either thread per row or thread per element. And the 3 questions are, which approach will launch more threads? Which approach will require communication between threads? And which approach will do more work per thread? So which approach will launch more threads: Thread per row, thread per element? The answer is thread per element. Thread per element will launch a thread for every X in this matrix, but thread per row, on the other hand, will only launch a thread for every row in this matrix. So thread per element will certainly have at least as many, if not many more threads than thread per row. Which approach will require communication between threads? Well, that's also going to be thread per element. What's going to happen there is we compute a partial product for each element in a particular row and then we need to add up those partial products across those elements to create the final value. And if you remember we use the segmented scan operation to be able to do that in Unit 4. The final question, which approach will do more work per thread? Well that's going to be thread per row. This row has to do all the partial products and add them up, whereas thread per element only does 1 partial product per element and then combines the work across multiple threads to be able to get a final result. So we'll start with the thread per row approach. Let's start with the data structure. We're going to use the CSR, the compressed sparse row format here, just as we did in Unit 4. Recall that value contains the non-zero elements in the matrix, index gives the column of each entry, and row pointer contains the index of the beginning of each row. So each blue dot here corresponds to the element that begins each row, which is element 0, 2, 3, and 5. So let's just walk through some code. Note this code, like many spmv routines, calculates y+=mx. So it multiplies m by vector x and then adds it to the element y and resets the result as y. It adds the matrix vector product to the destination vector y. We're going to start by this line here computing the global index for each thread. The thread with this index i will calculate the result for row i. Next we're going to have an if statement, if row less than the number of rows. Why do we have this if statement? We're going to launch many blocks of many threads, and it might be that the number of rows is not a perfect multiple of blocks and threads. This if statement is a common one in CUDA programs. Inside the if is the meat of the routine. Recall that row pointer contains the indices of the starts of each row. So, for instance, the value 3 here says that the third element D here is the beginning of a particular row that then contains D and E. So we're going to start with D. We're going to start at the beginning of a row and we're going to go up to, but not including, the first element of the next row, so that's this loop right here. And at every iteration of that loop we will multiply 2 things. One is the value of that element, so in this case D, and the second is we check which column D is in. In this case D is in column 0 so we're going to look up the vector element at position 0 and multiply D by that vector element. So that's this value times that vector element, and then add that to dot. And when we're finally done, we take our destination value y, add it to dot, and put it back into y. Now let's do a little performance analysis here. Let's say that we have 32 adjacent threads in a warp that are each processing a separate row in the matrix. These rows may be different lengths. Will the overall run time of this set of threads be dependent on the shortest row, the average row, or the longest row? And the answer is longest, and that's really important. So, let's think about how these threads would run. We'll start with all the threads doing useful work. They'll all process their first element at the same time, then their second element, and so on. But eventually, the threads assigned to short rows will run out of work to do, so they'll be idle while the longer rows continue to do work. This code is really efficient if all the rows have the same length. But it's a lot less efficient if the lengths of the rows differ, because then the threads with shorter rows are idle, while threads with longer rows are still working. So let's try to think about a method that might perform better for matrices with varying numbers of non-zero elements per row. If you'll recall, the approach that we pursued in Unit 4 was to assign a thread to each element, do 1 multiplication per element, then use a backwards inclusive segmented sum scan to add up the partial products and generate a result per row. Let's look at this approach step by step so that we can understand it. So we're going to start with a CSR representation of the matrix. So step 1, we are going to use the row pointer data structure to generate an array with 1s for segment heads and 0s for non-segment heads. Then were going to launch a thread for each element in the matrix. We're going to multiply it by the corresponding vector element, which we fetch using the index data structure. For thread n, that code will look like value(n) times x(index(n)). Then were going to perform our backwards inclusive segmented sum scan, which will sum up all of the partial products in the matrix row. And then finally, the output of that sum scan will be a sparse vector, which we might want to pack back into a dense vector. And we can use the row pointer addresses to gather that sparse vector back into the dense vector. Now let's go answer the same question we asked about the thread per row approach. Now, let's go answer the same question we asked about the thread per row approach. Let's say we have 32 adjacent threads in a warp that are each processing a separate element in the matrix. The rows in this matrix may be of different lengths. Will the overall run time of these threads be proportional to the longest row of any of any of those elements, proportional to the shortest row of any of those elements, or completely insensitive to the distribution of rows and elements in the matrix? Well the answer--this an important answer--is insensitive. It doesn't matter, the structure of the matrix. It's insensitive to the structure of the matrix. It doesn't matter if we have short rows, long rows, all rows the same length, rows of wildly different lengths. The performance of the multiplications and the performance of the segmented scan don't depend on the distribution of elements under rows at all. Instead the run time is dependent only on the number of elements. So we have 2 approaches here--thread per row and thread per element. Which is better? So we might have different performance on matrices that have a similar number of elements per row, and we might have differing performance if we have a varying or even a wildly varying number of elements per row. So which of these is comparatively better on each of these kinds of matrices? So I'd like you to put a couple checkboxes in. So if we start by looking at a varying number of elements per row, what we've just seen is that the thread per element approach is insensitive to the structure of the matrix, whereas if we have a varying number of elements per row, thread per row suffers from load imbalance and it has a runtime dependent on the longest row. So of these 2 methods, thread per element is probably a better choice. It's completely insensitive to the structure of the matrix. But on the other hand, if our matrices have roughly the same number of elements in each row, which is better? Well, now there's really no load imbalance issues, and that's the primary disadvantage with the thread per row approach. And in this case, where we have a similar number of elements per row, it turns out that thread per row is roughly 3 times faster than thread per element. How come? Well, in thread per row, all of the partial products are accumulated within a single thread, so they're going to communicate through registers and there's no communication between threads at all. In thread per element, on the other hand, the segmented scan operation requires communicating between threads to sum up the partial products. That communication is more expensive. So, because we have different performance characteristics here, it's kind of a head-scratcher problem. Which data structure do we choose, given that we probably don't know what the structure of the matrix is before we need to make this call? So 1 elegant answer to this problem was proposed by Nathan Bell and Michael Garland at the Supercomputing Conference in 2009, and their answer was, use both approaches, and here's how we're going to do that. We're going to divide the matrix into 2 pieces. One piece, the more regular piece, is going to use the thread per row approach. This right side here, the more irregular approach, is instead going to use the thread per element approach. And then we compute each of these separately and add the results back together at the very end. So the final question is, where do we draw this line? What happens if we draw it too far to the left or draw it too far to the right? So let's turn this into a quiz. If we draw this line too far to the left, then we'll suffer from poor peak performance or load imbalance. And if we draw it too far to the right, will we again get poor peak performance, or load imbalance? Well if we go too far to the left, it means that we're going to have poor peak performance because we'll be using the slower thread per element approach on data that would instead be a better fit for the faster thread per row approach. But if we go too far to the right, then we'll be using thread per row on irregular row lengths, and that's going to lead to poor load balance. Bell and Garland noted that thread per row is about 3 times faster than thread per element for matrices with balanced rows. So their rule of thumb for drawing the line was when 1/3 of the rows have non-zero elements--simple and sensible. You'll find their approach called hybrid, an NVIDIA cuSPARSE, sparse matrix library. So what's the big picture here? I'll highlight 2 lessons here from this discussion. The first is the importance of keeping all your threads busy. Fine grain load imbalance is a performance killer because your thread hardware is sitting idle. Choosing algorithms that reduce imbalance is crucial. Second, managing communication is equally important. Communicating through registers is faster than communicating through shared memory, as we see here, which is in turn faster than communicating with the host. Now we'll turn to another interesting problem where the choice of approach is crucial for good performance. This is the problem of graph traversal, specifically, a breadth-first traversal of a graph. So let's define the terms that we're using here. A graph consists of a set of vertices and edges that connect these vertices. So in this picture vertices are blue circles, edges are green lines. Some graphs might be sparse with few edges per vertex, some might be dense and have lots of edges per vertex. Large graphs, very large graphs, are an interesting recent topic of study. The World Wide Web, for instance, can be considered a graph, with pages as vertices and links between pages as edges. Or the social network of Facebook, Google, or Twitter is a graph where people are vertices and friendships are links. These companies would like to use this social graph to, say, suggest friends you might know based on the friends you already have. And making these suggestions depends on analyzing the graph. A traversal involves visiting all the nodes in the graph. Web crawlers must traverse the Web to catalog it for searching. How do they make sure they visit each web page once and only once? A traversal of the web graph will allow them to do that. Now, there's 2 approaches to traversing a graph, called depth-first traversal and breadth-first traversal. And so this picture is going to help us show the differences. In a depth-first traversal we begin at a particular node. So we're going to pick this one in the middle and we're going to label it 0. We're going to pick a neighbor we haven't visited yet, let's say this one here, and then we're going to do a depth-first traversal from that node. So we're going to continue down the chain, label this one, and so on. If we run out of unvisited neighbors, such as here, we pop back to a previously visited node and continue. Well, we don't have any neighbors here either, so we might pop back here. And now we're going to continue with our other unvisited neighbors. So we might come here next. We don't have any more. We pop back up. We might come here next, 4, 5, 6, pop all the way back up here. Now we might come down to this one here, move here, pop back up, and then go 9 and 10, eventually come back up to our original route node. We decide that there are no more unvisited neighbors and then we complete. In a breadth-first traversal we're going to begin with a node just as we did as in a DFS, and we're going to pick the same starting node, but now we're going to have a different algorithm. So once we're at a particular node and we're running a breadth-first traversal, what we're going to do is immediately visit all the neighbors that are 1 hop away before we do any descent at all. So first we're going to visit all the neighbors that are 1 hop away. Only then will we start to descend farther past any of these neighbors. So now we'll take the first one of these and we will visit all of its unvisited numbers that are 1 hop away. Well, now we're complete here, so now we're going to go to number 2. We're going to do a traversal here, we're complete there. We come back out to number 3. We continue to traverse here. So we go 1 hop away and then pop back up. Now we go to number 4. What are the ones that are 1 hop away from number 4? Now we're at 8 and so on. One thing you can notice here is that we have a structure that we call a frontier that forms the boundary between all the nodes that we've already visited and all the nodes that we haven't visited. And we see that as we continue our traversal that's just going to expand the frontier out and out and out again. So we'll complete this by going to number 9 and then eventually to number 10. Now, which one of these is better? Well, it depends on the problem, the structure of the graph, what you're looking for, if you're able to make good decisions about which way to go next. Generally, people will say that DFS requires less state, less temporary storage to run. But BFS, on the other hand, exposes more parallelism during the traversal, and specifically, because of this parallelism quality, today we're going to look at how to do BFS on the GPU. So let's state the specific problem that we want to solve today, and then we'll look at how we might solve it. We want to start at a specific node, and we'll call it s. And then visit every node in the graph and record its distance from s in terms of graph hops. We call this quantity its depth. So any neighbor of s has depth 1. Any neighbor of those neighbors that we haven't visited yet has depth 2. And in general, a neighbor of a node with depth d that hasn't already been visited has depth d plus 1. I'm not sure how familiar our international audience is with the actor Kevin Bacon, but the Bacon number of a particular actor is the number of degrees of separation from Kevin Bacon. So if I had acted in a movie a movie with say, John Belushi, my Bacon number would be 2, because John Belushi's Bacon number is 1, because he appeared with Kevin Bacon in the outstanding film, Animal House. We could generate the Bacon numbers for all movie actors from a graph that records who appeared in movies together, and we could do this with a breadth-first traversal that begins at Kevin Bacon and calculates depth for each vertex for actor in the graph. So,let's make sure we understand BFS and the problem that we're going to solve. If we have a graph, an arbitrary graph with n nodes, what is the minimum possible maximum depth of that graph, and what is the maximum possible maximum depth of that graph? And by that I mean we can draw any graph we want, okay? And we would like to draw it to minimize the biggest number that we write in here. Or we'd like to draw a different graph with N nodes and we'd like to maximize the biggest number that we write in here. So, I'd like you to express these as functions of N. The minimum possible maximum depth is 1. So if we start with a node here that we're going to number 0, and then we're going to have a graph where every single node is attached to that root node, well the depth of each one of these is going to be 1. On the other hand, the maximum possible max depth is going to be N minus 1. So what's our graph look like there? We're going to start with the root node and then we're going to have a linear chain of graph nodes all the way out to N minus 1. Now it's important for you to realize these are 2 very different graphs from the point of view of parallelization. This linear chain here--very hard to parallelize if we're doing a BFS. It's going to take us order of n steps to get from the start node all the way to the end node. Not going to get a lot of parallelization out of that. And clever students are going to realize, this is the worst case step complexity of the BFS. But if we have a graph that looks like this, a star shaped graph, then parallelization is a lot more straightforward. We can process each one of those leaves independently, in parallel. So let's think of a good parallel algorithm to solve this problem. So here's some of the characteristics that we want out of our algorithm. We want lots of parallelism, coalesced memory access, minimal execution divergence, and something that's easy to implement. And I want to show you an algorithm that gives you all of those characteristics, and then I'm going to tell you why it's a crummy algorithm. So as we're going along, I want you to think about why this algorithm isn't going to be good enough. So here's what we're going to do. First I'm going to describe the algorithm at a high level, then we're going to do an example, then we're going look at code. So at a high level, we're going to operate over our list of edges. Then we're going to iterate n times, where n is the maximum depth of the graph. So on each iteration, n parallel, we can look at each edge independently in n parallel. And if one end of that edge has a depth d but we haven't visited the other end of the edge yet, the vertex on the other side, then we set that vertex to depth d plus 1. So the first iteration will set all the vertices that have depth 1. The next iteration will set all the vertices with depth 2 and so on. So let's do an example. So here's a simple graph. We're going to start here with node number 2, and we're going to set the depth there equal to 0. And our goal is to find these depths for each of these vertices. Originally, none of these are set. We're just going to say that's the value for hasn't visited yet. And we're going to begin by setting vertex number 2 to the value 0. So we note that the furthest vertex away from node 2 is two hops away. So the maximum depth here is 2, and we should expect that we should iterate twice. So we're going to begin the algorithm by looking at all of these edges in parallel. And what we're looking for is a pattern where one of the vertices of these edges is going to have a depth value set, and the other one does not. And we're going to find that 3 edges have this particular quality. So one of them is going to be the edge between 1 and 2. We'll check and see. The depth at vertex 2 is 0 and vertex 1 has not been visited yet. So we'll set it equal to 0 plus 1, which is 1. We'll do the same thing for this edge here between 2 and 3, okay? We visited 2; its value was 0. We haven't visited 3, so we set its value to be 1. And this edge here, 2 and 6. Now we complete the first iteration. We visited every single edge and run 6 threads in parallel to make this particular determination. Now we begin our second iteration. So now, again, what we're looking for is an edge where the vertex value of one end of the edge has been set and the other one has not yet been visited. So we're going to find that's true for the other 3 edges. So the edge between 0 and 1, we find that the depth at vertex number 1 is 1. We find that vertex 0 has not yet been visited. So we set its depth to 1 plus 1 equals 2. And the same is going to be true for the edges between 3 and 4 and the edge between 5 and 6, okay? Now we filled in a value for all of our vertices here, and we're thus complete. So let's analyze the complexity here. So the word complexity is possibly a function of both the number of vertices, V, and the number of edges, E. So the worst case, what is the portion of the word complexity that is a function of V, and what is a portion that's a function of E? So your choices are as a function of V, constant with respect to V, log V, linear, order of V, V log V, or V-squared. Similarly for E, constant order of 1, order of log E, order of E, order of E log E, and quadratic and E, order of E-squared. So let's think about the maximum number of iterations that we might have. What graph is going to give us the most iterations? And that graph is going to be this awful looking linear chain. How many iterations is that going to have? Well it's going to have on the order of V iterations, because we are going to have V vertices here. And how much work are we going to do in each one of those iterations? Well each one of these iterations, we know that we will visit all E edges, so if we take the amount of work here as a product between V and E, we know that our final answer is going to be on the order of V times E, so we know that it's at least going to be on the order of V, and we know that it's at least going to be in the order of E. Probably in any reasonable graph, there's going to be more edges than vertices; otherwise, it's not a very interesting graph, and if that's the case, we know that the overall work complexity is going to be at least order of V squared. It's quadratic in the number of vertices, and that's bad. That is a very bad property. And so I told you in the beginning of the discussion here that this was a crummy algorithm and this is why. We really do not want an algorithm that is quadratic in the number of vertices. Okay, now let's look at some code. It's going to take us 2 kernels to be able to run this. The first one is going to be an initialization kernel that we run once, and then we're going to show our iteration kernel that we're going to run on every iteration of the loop. Now the initialization kernel is extremely simple. It's just a map over all the vertices, and it's simply going to initialize the vertex array. So we get the vertex array as an argument, we know how long it is, and we know which vertex is the starting vertex. All we're going to do is calculate a global index for our particular thread, and then if our thread is the starting vertex, then we're going to set its depth to 0. Otherwise, we're going to set it to negative 1, and here we're going to designate negative 1 as our not visited value. Okay. This is very simple. Easy map operation. The more interesting one is the one we'll show next, which is the kernel that we're going to run on every iteration. Okay, so here's the more interesting kernel. This is a kernel that we're going to run on our iterations, each iteration is going to launch this kernel over E threads. Each of those threads is associated with 1 edge and can process that edge completely in parallel. So again, this is a map operation. It's something that maps very nicely to the GPU. So the first thing we're going to do is we're going to calculate a global index for our particular edge so that our thread is associated with 1 and only 1 edge. Then we're going to compute a few quantities, so the first thing is, what is the vertex at one end of my edge, and what is the depth of that vertex? What is the vertex at the second end of my edge, and what is the depth of that second vertex? Then we're actually going to do the logic that we care about. So if we have visited the first vertex in our edge, but we haven't visited the second vertex in our edge, okay? So we check the current depth against the first depth and make sure that we haven't visited the second vertex yet. If that's the case, then we set the depth of the second vertex equal to the depth of the first vertex plus 1. So the converse case is fairly straight forward. Now instead of checking the first against the current depth, we're checking the second, and we know the first has not been visited yet. If that's the case, then we're going to set the depth of the first to the depth of the second plus 1. Now, there's 2 other cases, and 1 of those cases is if we visited both vertices, and the other case is if we visited neither vertex. So you should convince yourself that we could ignore these cases, that we have no operations to do if either of these cases is true. Just a couple of implementation notes, and first a quick mental exercise. What happens if 2 different paths reach the same vertex at the same time? So let's consider this graph here. We're going to start at node S here, and we're going to set its depth to zero on the first iteration of our algorithm, then the depth of A will be set to 1 and the depth of B will be set to 1, but we know in the next iteration of the algorithm this edge and this edge will both try to set Cs depth. Is this a race condition? Does the result depend on which thread happens to run first? Well, fortunately no, because the algorithm is structured to always write only a singe depth value on any iteration, so if 2 threads are both trying to write to the same vertex within a single iteration, they'll always both be writing the same depth. In this case, they'll both be trying to write the value 2, so it doesn't really matter who goes first or second, because the end result, the depth of C, is going to be the same. Second, how do we know when we're done? So typically the way we're going to write this code is as follows. We're going to check a variable to see if we're done, and we're going to check that variable on the host. That variable's going to be set on the device. The kernel needs to set that variable. Then we'll copy it at the end of every iteration to the host. The host will check if we're done. If we're not done, we're going to iterate again. If we are done, then we'll fall through the loop and move on to the next code. So how do we actually set this variable? So, in this case, what we're going to do is we're going to first initialize a device variable called done to true. The way we're going to initialize it is we're going to have a variable on the host we know is true and just copy it over to the device so it's set as true at the beginning, then we're going to run our iteration, run this particular kernel, and if the kernel knows that it's not done yet, it will take this done variable and set it to false. Then, whatever the state of the done variable is, will copy it back to the host, and so the host will then know if we're complete or not and make a decision accordingly. So how are we going to actually do this on a GPU? What's inside the kernel? So this is an enhancement to the code that we just saw on the previous slide. What we're going to do is every time we set any vertex depth on the GPU, we also, alongside, set the global variable done to false so it's initialized to true if we make no changes at all in this BFS code, then it will stay true and we'll be done. But if we make any changes at all, then we'll set that global variable to false, then we will copy that false variable back to the host, and it will give us another iteration. Note that more than one thread can set done to false, but that's fine. It doesn't matter in what order these writes occur since they're all going to be the same value. So let's summarize our breadth first search try number 1. So this algorithm has some great properties. It's completely parallel, and there's no direct communication or synchronization between threads at all. There's very little thread divergence. The memory behavior isn't ideal since it involves gathers and scatters, but it's not bad. Unfortunately, the host has to control the iterations, which isn't super attractive, though we'll look at a new feature in CUDA 5 in the next unit called Dynamic Parallelism that allows the GPU to control the iterations directly, but the real problem is the quadratic complexity. We're going to have perfectly data parallel work, but we're going to do way too much of it. This approach will scale poorly as we process larger and larger graphs, and we are not cool with that at all. In Problem Set number 6, you will be implementing a fast parallel algorithm for seamless image cloning. Seamless image cloning is a modern algorithm for pasting one image into another in a very subtle way. Here is an example of seamless image cloning. For example, you have a bear, a swimming pool. With seamless image cloning, you now have a polar bear in a swimming pool. Our algorithm for seamless image cloning uses a technique called "double buffering" to iteratively compute a solution. The main idea behind double buffering in the context of this homework is that we store an approximate solution to a problem in one buffer. Then we use that approximate solution to compute a better solution which we will store in another buffer. Then we go in the other direction, computing an even better solution, and store that solution in the first buffer. After going back and forth many times, we will arrive at an ideal solution to our problem. This style of computation arises in many parallel algorithms, which is why we think it will make a really interesting homework assignment. Before we implement seamless image cloning, let's take a closer look at our inputs. We are given as inputs a source image, a target image, and a mask. We want to copy the masked value from the source image into the target image in a subtle way. To do this, we hard code the values on the inside border of the masked region to be the same as the target image, then what we do is to solve for the unknown pixel values in the strict interior of the masked region. How do we find these unknown pixel values in the strict interior portion of the masked image? We can do that as follows. First step, we will make a guess for the unknown pixel values, and that's called our initial guess I0. I0 doesn't have to be a very good guess, so we can set I0 to be all zeros in the strict interior portion of our masked image. But note that 0 will be a very bad guess, because it would not be very subtle and it also would not conform to the source image very well. Now suppose we are given a guess solution IK to our problem, we can compute a better guess IK + 1 as follows. For some pixel i(k + 1) in I(k + 1) in the strict interior region of the mask, we can compute the value of i of K + 1 as A + B + C divided by D. And let's go through each of A and B and C and D one at a time. A is the sum of iK's neighbors. We don't include iK in the sum, and we only include iK's neighbors if they are in the strict interior portion of the masked region. B is the sum of t's neighbors where t is the corresponding pixel value in the target image, and the target image in our case is the swimming pool. And we only include t's neighbors if they are in the inside border of the masked region. And C is the sum of the difference between S and S's neighbors where S in this case is the corresponding pixel value in the source image and the source image in our case is the polar bear. Finally, D is the number of iK's neighbors, and iK's neighbors can be less than 4 if iK is on the border of the image. So instead, we're going to look at another algorithm, one which is far better in terms of work complexity. This implementation was written by Duane Merrill and his colleagues and published in 2012. What's inefficient about the previous algorithm is that we visit the same edge over and over again on each iteration, but we only set its depth once. If we could, instead, only visit an edge when it's ready to be processed and never visit it again, we'd be much more efficient. So let's go back to the serial algorithm and think about why it's an efficient algorithm. It's efficient because it tries to minimize the number of visits to nodes, and it does so by maintaining a frontier that marks the boundary between visited nodes and unvisited nodes. At any given time, only the nodes that border the frontier are subject to computation. This is different than our brute force N squared algorithm, where every node is touched on every iteration. How do we implement something like the frontier on the GPU? So I'm just going to sketch out at a high level how Merle chooses to do this. So first we're going to look at the data structure for a graph. We're going to store the graph with a CSR-like structure, similar to how we'd store a sparse matrix. So let's look at this graph here; notice it has 9 nodes, and it has links that are either unidirectional or bidirectional, and we're going to consider beginning our breadth first traversal with this particular node right here, node 0. We're going to store 2 arrays to represent the graph. The first, called C, tells us the neighbors for each node. Node 0 has neighbors 1 and 3, so we see 1 and 3. Node 1 has neighbors 0, 2, and 4; 0, 2 and 4 and so on. The second array, called R, tells us the starting location for each vertex's neighbors. Node 0's neighbors started off set 0. Node 1's neighbors started off set 2. Node 2s neighbors started off set 5 and so on. As a function of V, the number of vertices, and E, the number of edges in the graph, how long is C and how long is R, in terms of the number of elements in the array? C has 1 entry per edge, so it is E long. R, on the other hand, has 1 entry per vertex, so it's V long. Although we're going to put 1 additional element at the end, and we'll see why in a minute. Either V or V+1 is accurate. We're also going to store a depth array, D initialized to -1, that indicates the depth for each node in the graph. To show how this algorithm works, let's assume that we have a list of nodes that are on the current frontier. What we want to find are all nodes that are 1 hop away from the frontier, so let's use this example here, and we're going to assume that we're 1 step into the breadth-first search, so the frontier is nodes 1 and 3. So now we're going to walk through the algorithm. Step 1, In parallel: For each node in the frontier, find the starting point of its neighbors. This is pretty simple; for vertex V, that's just R of V. In our example, node 1's neighbors start at offset 2, and node 3's neighbors start at offset 5. 2. Now for each node in the frontier, how many neighbors does it have? This is slightly more complex but still straightforward. You only need the array R. How do you compute the number of neighbors per vertex using array R and vertex number V? So we simply take the difference between the vertex's entry in the R array and its neighbors entry in the R array, and that's going to show how many neighbors this particular entry has. So it's just R of V plus 1 minus R of V. Earlier, we said the R array was 1 longer than we needed, and this is the reason why. So what's the starting point of the next vertex? Subtract my starting point. So in our example, node 1 has 3 neighbors, 5 minus 2, and node 3 has 1 neighbor, 6 minus 5. So the new frontier then has a maximum number of 3 plus 1 nodes in it. Step 3, allocate space to store the new frontier. So we have an empty array where we can copy the new frontier, but each node needs to know where in that array it might copy its edge list. We covered this operation in unit 4; it's called... Allocate, and remember, this is based on scan; we just scan the number of neighbors. In our example, we begin with the input array of 3 and 1, because vertex 1 has 3 neighbors and vertex 3 has 1 neighbor, and we scan this array with an exclusive sum scan. So we get the resulting array 0, 3. Node 1 knows to start writing its edge list at offset 0. Node 3 knows to start writing its edge list at offset 3. Step 4, copy each act of nodes edge list to this array. So vertex 1 copies its 3 element neighbor list into the output array, starting at offset 0, and vertex 3 copies its only neighbor to offset 3. So the potential new frontier here are nodes 0, 2, 4, and 4. 5. Cull the list, removing elements that'll already been visited. How do we know if they've been visited? Well we're going to look in the D array, the depth array for each element, and we can mark the vertex 0 as already visited, because it has a depth of 0. All the rest we're going to keep, because they have depth minus 1. So what we need is an operation where we take a vector of elements and a set of trues and falses and omit only the elements who are associated with trues. What do we call this operation? And that's compact. Now the sharp viewer might note that we're going to see vertex 4 twice in the next frontier. Now the algorithm's going to work okay with duplicates. It's going to work correctly; it's just wasteful. And this turns out to be a tricky problem to solve in parallel and it's beyond the scope of this talk, but you might consult Merrill's paper for a couple of interesting solutions. Finally, we initialize the algorithm by setting the starting nodes depth to 0 and the initial frontier to that node's neighbor list. So let's recap the big idea here. The basic step is a sparse copy. We start with a frontier, we look up the adjacency lists for the vertices in that frontier, and then we copy them into a contiguous block to make the next frontier, and then repeat. The result is a really fast breadth first search, one that runs in linear work and achieves on the order of 3.3 billion vertices per second on a GPU. This is conservatively 4 times faster than a optimized CPU implementation. So in the discussion on graphs, I made the following statement; let me quote myself. "If we had a graph that was just a linear chain, the last node would be node N minus 1. "The linear chain is very hard to paralyze. If we're doing a BFS here, it's going to take us order of N steps to get to the end of the graph." Let me state this problem another way. I have N nodes in a linear chain, and each node knows the ID of the next node in the chain. This is just a simple link list. What is the algorithm for each node, every node finding the end of the list? And so of course we can solve this in N steps. Let's say that each node has a next pointer, and I've shown those in blue, that points to the next node in the chain, and the last node has next equals null. Now we don't want to change the next pointers at all, or else we'll lose the structure of our lists. So we're going to assume they're read only. So we're also going to store a second pointer per node that we can change. For historical purposes, we'll call this pointer chum, and we're going to designate it in red. And at the end of the algorithm, we want each node's chum pointer to point to the last node in the chain. So the straightforward algorithm is on each iteration on each node, set chum to chum to next until we reach a node where next is null. So we'll start off by making all chum pointers point to their own nodes. That's how we are going to initialize them, and again, on each iteration, we will set chum to the next pointer. So on the first iteration it's going to look like this, so now we're going to do another iteration, so for any particular node, we look for chum and then next. So on the second iteration, it's going to look like this, and so on, and so on. So on each iteration, the length of the chum pointer is going to be 1 more than it was on the iteration before. So the question is--the important question is, can we do better? And it turns out the answer is yes, and this algorithm described by Danny Hillis and Guy Steele in 1986, not discovered by them but described very nicely, is so cool that it's one of the reasons that I decided to do parallel computing in the first place. So let's analyze the complexity of the algorithm that we just described. Clearly a serial processor can do this computation in N steps in order of N work. How about the parallel processor? We know that it takes N steps; how much work? Your choices are order of N, N log N, N square root of N, or N-squared. And the answer is, order of n squared work. We have all n processors working and it's going to take n iterations to complete, so that's n squared work overall. Way back in Unit 1, when we talked about step in work complexity, we said that we would love to find algorithms that have the same work complexity as the serial algorithm but have a smaller step complexity. Reduce would be a good example here, or scan. But if we said we can't do that, sometimes we would be willing to accept more work if it gets us fewer steps. And that's what we are going to do here. But how? This is not intuitive, or more at least it isn't for most people, and it certainly wasn't for me when I learned this material. Hills and Steele first expressed skepticism they could improve on quadratic work, but then concluded--and I am quoting from their paper--essentially we overlooked the power of having many processors working on the problem at once. So at a high level, here's what we are going to do. On every node, we start by knowing the node that's 1 hop away. That's the next pointer, in blue. So on the next iteration, we can visit our next pointer's next pointer, and get 2 hops away. And then, on the next iteration, we can get to 3 hops away, and so on. So that's what I am showing here when I say straight forward approach. As we increase the number of iterations, we are also increasing the number of hops away. But that's the wrong way to think about it. If we just did it that way, we'd be repeating a lot of work that the nodes down the chain are doing, and we would have quadratic complexity work. Instead, after the first iteration, we have each node knowing the node that is 2 hops away. So let's say we're interested in this node and we know that we are pointing to node x here. Well normally what we'd do is we'd take the red pointer here, and then change our red pointer to be red pointer plus blue pointer. So, we'd be moving from knowing the node 2 hops away to the node knowing 3 hops away. But look, x also knows the node that is 2 hops away. So, I know the node that's 2 hops away, x knows the node that's 2 hops away. So, on the second iteration we can leverage the work that x already did on it's first iteration to get a pointer that is now 4 hops away. So we can go red, then red to set our new red pointer here, to be the chum of the chum. Now we are going to have a pointer to a node that is 4 hops away, which also has a pointer to a node that is 4 hops away. So the next iteration will have a pointer that is 8 hops away and so on. So now, instead of going 1, 2, 3, 4, we're now going 1, 2, 4, 8. So for N iterations, as a function of N, how many steps is this going to take? And the answer is log n. How cool is that? We're now doing more work in operations on each of the log n steps. So O of n log n overall. But we'll finish in log n steps instead of n steps, and I find this pretty amazing. What a beautiful algorithm. So now let's write the code. You've gotta put up with my coding handwriting here. What we're going to do is initialize k to our global thread index. We're going to initialize chum to our next pointer, and then we're going to loop. We're going to run this routine until we run into the end. And the question is, what do we do at each stage? What do we set the new chum of k to? And the answer is, you set it to the chum of chum of K. So let's try a slightly harder algorithm called list ranking. In list ranking, each node has the index of its successor in the list, and we know the first element in the output. What we want to be able to do is put the nodes in order. This has a number of uses, and in our lab, we've used it to decompress data that's been compressed with B sub 2, so let's take a look at an example. As an example we've got 10 nodes here. We know that the output is going to begin with node 0 and the successor to node 0, so one farther in the chain is node 5. The successor to node 5 is node 2. The successor to node 2 is node 7 and so on. So this is our input--this array of 5, 6, 7, 9, 2, 3, 0, 1. What's our output going to be? It's going to be the chain 0, 5, 2, 7, 4, 9, 1, 6, 3, 8. So here's our input, and here's the output. Now, you can note that the array is actually circular, so it's necessary to actually designate the starting point, and in this case we're saying that node number 0 is the starting point. Of course, a serial processor could do this N steps. The question is, how can we make it work with parallel hardware with a smaller number of steps? We're going to sketch out a solution that uses a similar logarithmic structure to the previous problem. I'm going to write it in a different way, though, and I'm going to use a table here. So recall in the previous algorithm, we began with each node knowing its neighbor 1 hop away. In the first step, each node learned the node that was 2 hops away. Then in the next step, 4 hops away and so on. So we're going to try to use a similar approach here, and the algorithm's going to proceed in 2 phases. The first phase is filling in this table, so that every node knows the nodes that are 1, 2, 4, and 8 hops away, and this if functionally equivalent to find the last element in the link list problem that we just saw, except that now we're treating a list of circulars so we wrap around. So at each step of this phase, we're going to begin by knowing the node that's K hops away; for instance, K equals 1. And then we're going to compute per node, the node that is 2 K hops away. So let's begin. Every node is going to start by knowing the idea of a node that's exactly 1 hop away. So let's see how to compute the node that's 2 hops away. We simply look at our node that's 1 hop away and take its node that's 1 hop away, and we'll continue as long as the number of hops away isn't greater than number of nodes. So let's do an example here; we know that the successor--the node that's 1 hop away from node 0 is node 5, so if we want to compute the node that's 2 hops away from 0, we'll look to see that the successor to 0 is 5, and the successor to 5 is 2, so we'll fill in a 2 here. The successor to 1 is 6, and the successor to 6 is node 3, so we'll fill in a 3 here. The successor to node 2 is 7, the successor to node 7 is node 4, so we'll fill in a 4 here. And notice this is a perfectly data parallel operation. Each one of these vertices can do this computation completely in parallel. Okay, so the next step of the algorithm is to the compute the nodes that are 4 hops away, and we're going to be able to do this in a similar way. Say we are starting with node 0 and we want the node that is 4 hops away from node 0. So we know the node that's 2 hops away and then the node that is 2 hops away from node 2 is node 4. We'll fill in a 4 here. To find the node that is 4 hops away from node 1, we know the node that's 2 hops away, and the node that's 2 hops away from it is node 0 and so on. And then we'll do the same thing for the last line here and find the nodes that are 8 hops away from each of these starting nodes. And we'll continue this progress as long as the number of hops away isn't greater than the number of nodes. In our example here, for instance, we have 10 nodes, so we will compute the nodes that are 1, 2, 4, and 8 hops away, but if we computed more, we would be going all the way around the list and beyond. How much work does it take to compute this entire table for N nodes proportional to N, N log N, N-squared, or N-cubed? Well let's take a look at the dimensions of this table. Every node participates in every step, so each step takes order of N work, and we're doubling the hop count on each step, thus there's log of N steps, so it takes N log N work to construct the entire table. Note this is more expensive than the serial algorithm which takes linear work, order of N work. However, the serial algorithm also takes N steps, whereas, we're finishing N log N steps here. Now the second phase of this algorithm uses this table to construct the output order. What we're looking to generate is the list of nodes in order, and we computed this before. We see it over here: 0, 5, 2, 7, 4, 9, 1, 6, 3, 8. Remember we can get that by using a serial algorithm just to convince ourselves that it's correct. We start at node 0, its successor is 5, the successor to 5 is 2, the successor to 2 is 7 and so on. To generate this answer, each node is going to compute its position in the output list, and we're going to call that position outpos, and then when we're done, we can use that position to scatter the node into its proper location in that output list. So just to understand what outpos is going to look like, we see that node 0 ends up at location 0 in the output list, so outpos better be 0. Node 5 ends up at the first location in the output list, so it's going to get outpos equals 1. Node 2 was next and then node 7, and so on. So this is what we're trying to compute here. We're trying to compute outpos. So when we start this, all we know is that node 0 is in position 0, so we can fill in that outpos right away. Now that node 0 is calculated where it will end up in the output, we're going to consider it awake, but all the other nodes are asleep. A node is awake if it has filled in its value for outpos, otherwise it's asleep. So now we're going to do several iterations of an algorithm that will eventually wake up all the nodes and calculate their output positions. So on each iteration we're going to launch N threads, 1 per element here, but if a thread is asleep we immediately return from that thread. Only awake threads do any work at all. So for the first iteration, node 0 wants to wake up its successor, so how does it find the successor? Well it can look in this plus 1 array to find its immediate successor, and so we can immediately see that's node 5. So where will node number 5 write its output? That's what we need to fill in here for outpos. It knows that it's coming after node number 0, right here, and it knows that it's coming 1 hop past node number 0. So we can add these 2 values together to see that node number 5 will write its output to position number 1 in the output array, and so now we have 2 nodes that are awake: Node 0, node 5. So now both awake nodes are going to wake up another node and help those nodes calculate their output position. So now instead of looking at the immediate successors plus 1, we're going to look at the next line, plus 2, and we're going to use this to find the nodes 2 away from each of our awake nodes, so node 0 is going to wake up node number 2. So node 0 is going to wake up node number 2, and so where does node 2 write its output? Well it'll be the sum of where node 0 is at output location 0, and we know we're 2 hops away, so when we add those 2 up we know that node 2 will be written to position number 2. Similarly node 5 is going to wake up the node that's 2 hops away; in this case, node 7. Where will node 7 write its output? It knows that it'll start from node number 5 at position number 1, and then we'll add 2 hops to that and get position number 3. On the next iteration, we're going to use the next line of the array, the plus 4 array, and each of the 4 awake nodes from output position 0, 1, 2, and 3 will wake up the node that's 4 hops away. This will allow us to fill in 4 more values into the outpos array. Node 4 will get position number 4, node 9 gets position 5, node 1 gets position 6, and node 6 gets position 7, and the final iteration using the plus 8 array lets us fill in node 3 as outpos 8 and node 8 as outpos 9. So the final step is to use outpos to scatter the nodes to their destinations, so let's see how that works. We're going to start by looking at the first node, and we know that node 0 is going to end up in output position 0. So we will scatter this 0 to output location 0 and write the 0. The next node, node number 1, is going to end up in output position 6. The next node, node 2, will end up directly in output position 2, and so after we do all of these scatter operations, we come up with the nodes in the correct order. So as a function of N how many steps does this take? Square root of N, log N, N or N squared? Well the answer again is log N. Just like in the previous step, we're doubling the hop count on each step so it will take log N steps to fill in all output positions, and if you count both awake and asleep threads as doing 1 unit of work each iteration, we do N log N work overall. The big idea here is that this entire algorithm is a good example of trading off work for steps. We do more work than the serial version, N log N versus N, but we finish in fewer steps, log N versus N. The most common way to construct a hash table on a CPU works as follows. So we have a bunch of buckets here, and we have a hash function h. And this hash function takes a key and maps that key into one of those buckets. So if the the hash function of the key k returns 0, h of k is 0, then that key is associated with bucket 0. If h of k is 1, then that key is associated with bucket 1 and so on. Within a bucket, we store a bunch of items as a linked list, and this is called chaining. So we might have multiple items in this bucket, multiple keys. So key 12, key 29, key 123 all have a hash function that's equal to 1, so they're placed in bucket 1, and we store them as this chained, linked list. Then when we want to look up a key, we take that key. We run it through the hash function to get a particular value out of the hash function that's going to refer us to a particular bucket, so we have this key. It's going to return; oh, he's in bucket 1. Then we will look through all these chained items to find the key that we're looking for. So let's say we have n items to hash n keys, and we have b buckets. So what I'd like you to do as a function of n and b, what's the ideal number of keys per bucket? So the ideal number of keys per bucket is simply n over b. If every bucket has n over b items, then the items are all evenly spread between buckets. So this is largely a function of this hash function. Did we do a good job choosing a hash function that will evenly distribute all the keys among the buckets? If we pick a bad hash function, maybe we end up with all the items in 1 bucket, and this is bad because any lookups into that bucket might have to look at all n items, or we could end up with no items in a bucket, and that's a waste of a bucket. Ideally, a hash function distributes all input keys evenly across buckets, so every bucket ends up with roughly the same number of items. So let's think about how chaining would behave in a parallel setting. For construction, that means we have many items to put into the hash table, 1 per thread. Per look up, that means we have many keys to look up in the hash table; again, 1 per thread. And chaining has 2 main disadvantages. Number 1, let's say we're looking up many items, 1 per thread, and we know to look up 1 item, we calculate the hash function for that item. That paralyzes nicely. It's just a map operation. More problematic, however, is searching the link list in the bucket. So we have a number of threads here. Each thread ends up looking in a different bucket. This particular bucket has 3 items. The bucket for thread 2 has 2 items, but the bucket for thread 1 has many, many, many items. Some threads, like thread 2, might find their item right away. Some threads, like thread 1, for instance, might have to visit many or even all the items in a lengthy link list before finding its item. Because threads within a warp run in lock step, the run time of a warp is completely dependent on the slowest lookup within the warp. The other threads in the warp have to wait until the slowest item is found, and this behavior, as you might imagine, is bad. So let's say we have 32 threads, each of which has a different item to look up in a hash table. Let's say all those threads map to the same bucket, and there's 32 items in that bucket, each of which will be the target of one of the lookups. So all 32 threads will loop through the chain until all 32 threads have found their item. If we consider the fundamental unit of work here, the thread iterations, what fraction of thread iterations here actually do useful work? The answer: Roughly half of them are going to do useful work. The actual fraction ends up being 0.516, and I'll say how I got that. So we've got 32 threads here. I'm putting them on X-axis, and then on the Y-axis is the number of iterations. We know that 1 thread will find its answer on the first iteration, the next thread will find its answer on the second iteration and so on. So why don't we say the first thread finds it's answer on the first iteration. The second thread will find it's answer on the second iteration and so on. So eventually the last thread will take 32 iterations to find its answer, and what that means is that it has spent 32 iterations walking through this link list. We have 32 threads times 32 iterations, so the amount of work we could have that's useful is sort of the product of the number of threads and the number of iterations. So we see all of this area in that square is being useful. These threads are doing useful work; they're walking through the link list until they find their item right here and stop, but all the blue area here, on the other hand, is wasted work. We have threads that have gone to sleep, because they've already found their item, and now they're not doing anything useful; they're just waiting for the last thread to finish. So if you actually do the math in detail, what you'll find is the red area compared to the entire square area is slightly more than half, but basically what we're looking at here is that roughly half the time if we pick any random thread, it's going to be asleep and waiting for some other thread to finish so it can progress to the next computation. The second disadvantage is in construction, particularly contention in the construction process. What we might have is 2 different items, each of which wants to place 1 item into the hash table. Let's say both of these items decide that they have the same hash function for their particular item, and so they both want to add an item to hash table bucket number 12, for instance. These 2 items want to simultaneously manipulate the link list in the same bucket. To do that, they're going to have to serialize and synchronize. Only 1 of them can update the bucket at any given time, and the other must wait its turn. So any serialization like this within a parallel algorithm is definitely undesirable, as well. So the conclusion here is that chaining is a sub-optimal strategy if were dealing with parallel hash tables. So we're going to turn to a different method. The approach we're instead going to take is based on a different hashing algorithm, non-chaining, and this hashing algorithm is called cuckoo hashing. This is a cuckoo bird; thanks, Wikipedia. And it's termed a brood parasite, but let's put it into more understandable English. This bird is one of the biggest jerks in the animal kingdom. Rather than taking care of its own eggs and chicks, it instead lays its eggs in another bird's nest, throwing out the other bird's eggs to make room and lets the other bird raise its chicks. See if you can detect why the algorithm I'm about to describe is called cuckoo hashing, as I describe it. The key to this method is having multiple hash tables instead of just 1. Multiple hash functions, 1 per hash table. And those hash tables only allow 1 item in each one of their buckets. There is no chaining in this algorithm at all. In the example we'll show, we're going to have 2 hash tables and 2 hash functions, but the method generalizes to more than 2. So here's where we're going to do at a high level. First, all the items that we want to hash, we use the first hash function and try to hash into the first hash table. Some of them will collide and fail, and by that, I mean, we're going to try write multiple items into the same bucket, but we're only allowing 1 item per bucket. That's okay; if n items try to write into 1 hash bucket, we only require that 1 of them succeeds, and it doesn't really matter which one. So then those that fail--we're going to take all the ones that are leftover that failed, and we're going to use the second hash function to try to hash into the second hash table and so on. Now the cool part is what happens next, but we're going to go and take an example here and I'm going to show you how it works, and then we'll see exactly how this cuckoo hashing works. We're going to have 4 items that we're going to try to hash into this giant cuckoo hash table, and the cuckoo hash table has 2 sub tables, t1 and t2, and so each one of those 2 sub tables has 2 slots corresponding to h1 equals 0, h1 equals 1, h2 equals 0, h2 equals 1. Now we have 4 items that we are going to try to hash into this big cuckoo hash table. And so, I've already computed their hash functions. So item A, for instance--it's h sub 1 of its particular key is 1, and h sub 2 is equal to 1, and so on. So you can see all these hash functions here. So let's do round 1. What do we do on round 1? What we try to do is place all of the alive items, and we start off with all the items being alive, and we're trying to place them all into t1 using this hash function. So now we see that items A and C both would like to map into this bucket, and items B and D, because of their values of h1, would like to map into this bucket. Now we can only fit 1 item per bucket, so we're going to arbitrarily pick a couple of items to win. Okay, so I'll arbitrarily pick that C and D happen to win here. D is written into hash bucket number 0, and C is written into hash bucket number 1. Cool, and so A and B are still alive. And now, we'll move to the next iteration of the algorithm, and we'll use h sub 2 to try to map A and B into table 2, and we see, well, both of these guys would like to map into this same bucket. So again, they're both going to try; only 1 of them is going to succeed. For the purposes of argument, I'm going to say that's going to B. Okay, so now we still have the element A left after both rounds, so here's the neat part. We now go back to table 1, and we know that if we hash item A, it's going to collide with an item already stored in the hash table. We know that because it didn't succeed the first time. So we're going to try to hash it again, but now when it collides with an item that's already in the hash table, here's the different part. We're going to take A and place it into the hash table, and we're going to kick out the item that's already there, and continue. And now we continue on the next iteration of the algorithm again. We see that the only item that's left alive is now C. We see that its value of h2 is 0, so we can safely place C into the hash table here, and we're done. So the big picture here is that by kicking out things that are already in the hash table, we have a new set of items to try to continue to place in the hash table, and maybe they'll fit better than the old set, and we continue to apply this procedure. On each iteration, we always try to place our outstanding items into the next sub table of the hash table, and if they succeed, they kick out the items already there until we have no more items left. Now, will this always succeed? Yes or no? No, it definitely will not always succeed. There are some nice probabilistic guarantees about how often it will succeed depending on the size and number of the hash tables, but the easy counterexample is to say that, well, here we have 2 hash tables. If we had 3 items, each of which had the same H1 and H2-- so for instance if we had 3 items where H1 and H2 were both 0, there's no possible way that we can fit them into the hash table, because we only have 2 slots where any hash function is equal to 0. So in practice we choose a certain number of iterations, and we continue to iterate, trying to fill up this hash table until we decide that we've done too many iterations. And so if that's the case then we just stop, we choose new hash functions, and we start over. And again, there's very nice probabilistic guarantees about how often this is going to finish. So in the research that inspired this work, the guarantee that we tried to use was that we could guarantee that it was going to fail less than 1 out of every million times. So once we construct the hash table, the lookup procedure is really simple. We're going to calculate all the hash functions for the item that we want to look up. So for instance, if I want to look up item B, I calculate item B's hash functions. Here hash function 1 is equal to 0, and hash function 2 is equal to 1. So what I'm going to do is I'm going to look in all tables using the particular hash functions until I find what I'm looking for. So first, I'm going to look in table 1, and I know that I'm going to look in slot 0. Here I look in slot 0, and I say, "Wait a second, that's not B." So then I have to go to table 2, look and see that its hash value is equal to 1, so I'll look in slot 1 in table 2, and I'll say, "Ah, there's B!" I've now found the value that I'm looking for. Now if we don't find it in any of these locations, it's just not in the has table. Now the nice part here is that this is a constant time lookup. It just requires T lookups, and T is a constant. It might be 2, it might be 3, and so on. This is different than chaining. Chaining has a variable time lookup. It depends on how many items are in the bucket, and if we have many items in the bucket, and we have to look all the way to the end, it can potentially take a very long time, whereas we can guarantee exactly how much work, and it's a constant amount of work to look up any item in these hash tables. So a few notes on implementation. The real benefits of this particular formulation of hashing are that our look-ups are again constant time, and that's terrific for parallelism for a machine like the GPU, because we keep all of our threads busy. There's no thread divergence at all, and the construction algorithm is actually pretty simple and pretty fast. It's actually a very efficient hash table construction when it's done in shared memory with a small of item to hash, but it turns out that it actually works fairly well with large data sets when the hash table needs to be constructed in main memory. The really important thing to keep in mind there is that the operation of write my item into the hash table and kick the other item out needs to be an atomic operation-- in our implementation we used atomic exchange to make sure the 2 competing threads don't have any possibility of stomping on top of each other. And a final note, there's more than one way we might solve this general problem of check if an element is in a set. Instead of using hash tables, we could choose a fairly brute force way to do this. We could choose to sort all the elements in the set as a construction step, and then, for the look-up step, we could do binary into the set to see if that particular element is present. So for instance, we might have 5 keys here in a set. To construct this particular data structure, we sort these in order of the key, and to do look-up we just do binary search within this set. So for instance, if we're looking up value k6 with one thread, we'd go see k5--my keys bigger than that. I'd then pop this way and so on until I get to k6. I might have another thread that's looking up k3. It would start in the middle at k5 and then do binary search until it finds k3. Now sorting is quite fast on GPUs, and even though hash tables are faster for this particular computation, they're not so much faster that should never consider a sort to be a good idea. Often on the GPU, sometimes a brute force approach, like sort and binary search, might be your best option. So let's recap some of the important lessons from this unit. If you've made it this far, good for you. There's a lot of material in this unit, advanced material that's pushing the frontiers of field, but that's why you took this class, right? So here's some of the lessons we learned along the way. Dense N-body. We learn 2 main lessons here. One is the importance of minimizing global memory bandwidth, and the second, reducing parallelism by increasing the amount of work per thread might reduce your overall communication costs and hence, your overall run time. Sparse Matrix Vector Multiply. The right data structure can make a big difference, and the keys to high performance with our implementation was reducing load imbalance between threads, keeping all of our threads busy and optimizing to use the most efficient communication possible. Breadth-first Reversal Traversal. Choosing the most efficient parallel algorithm is perhaps the most important thing you can do. For large problems, a superior problem in terms of asymptotic word complexity will nearly always be even an optimized, more expensive algorithm. And the real challenge in our optimized algorithm was handling the irregular expansion and compaction at each step. Scan is the natural primitive to handle these operations. List ranking. This is a good example of a problem that is not inherently parallel. To solve it well on a GPU, we use the important technique of trading more work for fewer steps. And once again, we see the power of scan to address a problem that at first glance seems difficult to parallelize. And the hash table. The key insight in the hash table implementation was recognizing that a serial data structure was the wrong fit for this problem. Instead, we used cuckoo hashing, which was much more parallel friendly. Great job on getting through this unit. You've learned a lot, and there's only 1 more unit left in this course. Pat yourselves on the back for making it this far. Now the next homework assignment is your last homework assignment, and it's the most challenging one since you are going to be writing it--most of it yourself. But don't worry, we're there to help; if you've got any issues, please post on the forums. Hi, everybody. Welcome to the final unit of the course. We're delighted that you stuck with us and hopefully learned a whole lot about parallel programming with Cuda. Now, this unit is going to be a little bit different. Instead of discussing one topic in depth, we're going to go wide and discuss many topics briefly. The goal is to give you some pointers to things that you can follow up, according to your own interest and needs. Let's get started. So today we're going to cover the following broad topics. We'll start by returning to the topic of optimizing GPU programs. Now in unit 5, we gave some pretty specific, detailed advice. And unit 6 explored some examples of how to think parallel. Here we're going to back off and talk more generally about strategies that parallel programmers use to optimize their programs. Now the best kind of programming, as we mentioned briefly in unit 5, is the kind of programming that you don't do because somebody else has already programmed it for you and packaged into a library you can use. So we'll talk about some important and useful CUDA libraries that are out there. Some of these libraries are less about packaging up code to solve a particular problem and more about providing what I call programming power tools to help code up your own solutions. So examples familiar to C++ programmers would be the standard template library, or STL, or the boost library. We'll discuss a few such power tools for CUDA C++ programmers. Now to write this class we focused on the CUDA C++ language, but there are many other platforms for parallel programming on GPUs. We'll talk about CUDA platforms that support other languages from Fortran to Python to Matlab. And we'll also discuss cross platform accelerator solutions like Open CL, Open ACC and Open GL compute. Now GPU computing is a young field and part of what makes it exciting is that the hardware and software are improving each year, not just getting incrementally faster but also adding fundamentally new capabilities. So we're going to finish the unit and the course with a fantastic guest lecture, inviting Dr. Stephen Jones from NVIDIA to come and teach us about the latest advance in CUDA called Dynamic Parllelism. So let's dive in and talk about Parallel Optimization Patterns. Now in the supplentary material, you'll find pointers to a fantastic couple of papers by John Stratton and colleagues at the University of Illinois. One is titled, "Optimization and Architecture Effects on GPU Computing and Workload Performance," and it was published in the Innovative Parallel Computing Conference in 2012. A reworked version of that same basic paper was published later that year in IEEE Computer. The Computer paper is a little shorter and more readable. Now, what they did is survey a whole bunch of highly optimized parallel codes, paid attention to the patterns that emerged, and extracted 7 fundamental techniques that come up over and over again. And as you'll see, several of these techniques encapsulate or formalize lessons that we've already learned. So this will also be a good review of material that we've covered in Unit 2 and Unit 5, and along the way in the homeworks. Let's walk through the taxonomy of seven techniques that that they've come up with. They call their first technique data layout transformation. So, by now, you're very familiar with the concept of global memory coalescing, where we achieve the best bandwidth to memory when adjacent threads access adjacent locations in memory. As a quick review, global memory coalescing is important because modern DRAM systems transfer large chunks of data per transaction, because modern DRAM systems transfer data in many small chunks, one per thread request, or because modern GPUs have no on-chip caches in which to prefetch data, meaning to hold data likely to be used by a thread but not yet actually requested by it? And the answer, of course, is that modern DRAM systems such as the ones used by high-performance GPUs transfer large chunks of data per transaction, and therefore, if you can coalesce your memory request by many threads that are performing a transaction at the same time, then you'll get many more useful bytes out of that transaction. So the second statement, that DRAM systems transfer data in many small chunks, is simply false, and the third statement, that GPUs have no on-chip caches to prefetch data, that's also false. Modern GPUs do have on-chip caches, although it is the case that prefetching data is not a particularly effective strategy for GPUs. The caches are quite small considering the number of threads that are trying to use them, and so you can't really afford to speculatively load data that some thread might use. Okay, you have tens of thousands of threads that are actually trying to use those caches to hold data, so prefetching is not an effective strategy, and this is also not a correct answer. So this burst utilitzation, as Stratton and Company call it, is usually the most critical factor to optimize for global memory performance and that's where the examples of a ray of structure, the structure of a ray comes in. But there are also subtler issues like partition camping that can occasionally rear their heads. So as before, I'll use the ninja icon to indicate topics that the average GPU programmer probably won't need to concern themselves with. Now a more sophisticated example of the data layout transformation technique is something called the array of structures of tiled arrays, or ASTA. This technique can address issues like partition camping and can potentially achieve even better bandwidth utilization on a wider range of architectures. So you'll find a reference in the supplementary materials. So the idea of the data layout transformation is to reorganize the way data is laid out in memory to facilitate better memory performance. So a really common example of this data layout transformation is something we've seen before where we take data, which is laid out as an array of structures, like so. An array of structures. And transform it into a structure of arrays. Here's what those layouts would look like in memory. In the first case, the different fields in the structure are laid out one after another. Each structure is adjacent. In the next case, they're all laid out adjacently. So as a quick quiz, which of these 2 layouts: the array of structures or AOS or the structure of arrays, SOA, will perform better on the little code snippets I've given here? So you set i equal to the thread index. And then we access field a for a given thread. And then we access field b, c, and d. And then over here I've done the same code, but I've transformed it to use this data layout. So the question is to check off, which of these 2 data layouts you think will perform better? The answer of course is the second one, structure of arrays, because you're going to have many threads executing this code snippet at the same time. In the first case, each thread will be accessing element A from a different place in memory followed element B and element C and so forth. Whereas in the second case, all these threads running at the same time will access adjacent elements. And remember why this matters? Because the actual memory, the DRAM that represents the global memory, accesses data in large chunks or bursts. So you'll get much better utilization of one of these bursts, a fixed sized memory transaction, when you have many threads that are simultaneously trying to access memory that will be contained within that transaction. So the next technique in Stratton's Optimization Taxonomy is called the scatter-to-gather transformation. Now most interesting computations involve a step where many input elements contribute to the computation of an output element. So in this example the elements labeled blue in the input array are contributing to the blue element in the output array, and the elements labeled green in the input array are contributing to the green in the output array and so forth. And examples from you homework included blurring the image or computing a histogram. And sometimes, of course, the locations of the input elements that will contribute to particular output element aren't know ahead of time like in a histogram example. And we've talked about the difference between scatter and gather. In scatter the threads are assigned to the input elements, and each one is deciding where it needs to write. In gather the threads were signed to the output elements, and each one decides where he needs to go read from. So here's a quick quiz which should be pure review. Which code snippet represents a gather, and which one will run more efficiently? Here's code snippet 1, and here's code snippet 2. In each of these cases you can assume that out and in are arrays of floating point numbers and that i represents the thread index. And of course code snippet 2 represents a gather since each thread is going to access 3 elements of the N array and then do some math and store the results into a single place in the output array, and that gather operation is going to run much faster, right? You probably noticed that in fact the first operation is accessing more memory since it has to read the 3 output locations in order to increment them, and it won't even run correctly unless we insert some sink threads barrier in here or use atomic operations to guarantee that 1 thread doesn't read a value while another one is in process of updating it. Either of those would further reduce performance, and this is typical, A gather pattern results in many overlapping reads that the hardware can typically handle more efficiently than it can handle many potentially conflicting writes, which is what you tend to get from this scatter pattern. So now if the pattern of reads can't be determined ahead of time, you may need to combine this with Stratten's technique number 5, binning. We'll come back to that. Stratton calls the third technique tiling. Often multiple threads or, more generally, multiple computational tasks will need to access overlapping parts of a data structure. Stratton uses the term tiling to refer to buffering data onto fast on-chip storage where it will be repeatedly accessed. Now, this is an old, old idea. On the CPU it generally means sizing the data accessed by a program so that it will fit naturally into the CPU core's on-chip cache. There's an implicit copy going on between the locations in DRAM and the fast on-chip storage--in this case, the cache. Another way to do it is to explicitly allocate some of this fast memory to be used as a scratchpad. And on the GPU in CUDA, this is what we call shared memory. And then we can have a set of threads that do the explicit copy, pulling memory in from DRAM, explicitly laying it out in the scratchpad memory, in the shared memory, just the way we want it, where then we'll operate on it. It's worth emphasizing GPUs run so many threads that the amount of cache per thread is limited. So this style of cache blocking which is what you call this technique on the CPU, isn't so great for the GPU. A better approach is to explicitly copy data that will be reused by the threads in a thread block into the fast shared memory that's available and set aside for that block. Let's have a quiz. Which of the following 2 code snippets would benefit from tiling? The first snippet averages values from 5 arrays; the second averages 5 values from 1 array. These kernels look similar. Both of them are reading 5 values for memory, performing the same arithmetic operations, and writing a single value to memory. And you'll get good coalescing in both cases. But the second kernel has a great deal of reuse. Two adjacent threads will be reading 4 of the same 5 values. So tiling the n array by copying it into shared memory before performing the averaging operation will capture that reuse and make the code run faster. In the first kernel, each value from the A, B, C, D and E arrays gets used only once. So tiling will just add an extra step without allowing any reuse. So the second code snippet will benefit from tiling and the first won't. In fact, the first will probably slow down. So as a programming exercise, I want you to take this code and tile both foo and bar using thread blocks that have 128 threads and tiling, a tile size of 128 elements. Okay? And as you'll see if you look at the code, we've structured it so that it'll be safe for you to ignore the boundary effects at the beginning and end of the arrays, the input and output arrays. We're only calling this on a portion of the code in the middle of the array. But of course you can't ignore the boundary effects that happen at the boundaries between thread blocks. So I'm looking for the percentage of time of the non-tiled version of the code that your tiled version takes. So for example, if your code ran exactly as fast as the non-tiled version, then it would be 100% of the time, and if your code ran twice as fast as the non-tiled version, then it would take 50% of the time. Give me the percentage of time for both foo and bar after you've done the tiling. The next optimization technique in Stratton's taxonomy is called privatization. So where tiling exploited the fact that multiple threads are reading from the same memory locations, thread sharing input is good, privatization helps avoid having multiple threads write to the same output locations. So privatization takes some output data shared by multiple threads and duplicates it, so that different parallel threads can operate in their own private copy. The private copies are eventually merged together to recreate the result that would of been produced in the share data used by all the threads before privatization. So histogram makes a good example. As you've learned from our previous discussions and problem sets on histogram, there's a bunch of ways that you can approach the problem. In particular, you can program the kernel to have each thread keep its own local histogram as it looked at a bunch of pixels, and then use atomics or a global reduce operation to combine the buckets from the histograms from all the threads into one set of shared results. You can also apply this idea hierarchically. For example, you can have each thread block compute a combined histogram in shared memory, and then have other thread blocks do the same, and then accumulate the results into a single global histogram. So in this example we would have per-thread privatized histograms, and per-block privatized histograms. Now in practice, the GPU has so many threads that you don't want each one to have a huge private data structure. So this approach might work well for a small histogram-- say 5 or 10 buckets-- but you would probably skip the per-thread privatized copies for a histogram with hundreds of buckets, and just do the local version per block. Okay, let's turn to Stratton's fifth technique, which he refers to as binning or spatial data structures. So say you've got an algorithm where certain input elements will contribute to certain output elements. In technique 2, we talked about transforming this from a scatter operation, where you assign threads to the input elements to perform the computation, and then they figure out where to write, to a gather operation, where you assign threads the output elements, and they figure out where to go read from. Now sometimes it can be hard to orchestrate this gather operation, right? It can be hard to figure out which inputs matter for a given output. One thing you can always do is you could have every thread assigned to an output element, check every possible input element to see if this input element will contribute to this output element. But obviously, that's going to be ruinously expensive if you take it too far. So what we want to do is optimize away most of the redundant work that you would be doing if all of these threads were checking all of these inputs to see if they contribute. And we're going to call this binning. So binning is when you build a data structure that maps output locations to the relevant input data. And a point that I want to make is that this will work best when you're mapping output locations to a relatively small subset of the input data. Stratton and company called a creation of this data structure binning, because it often reflects a sorting of the input in to bins that contain often a region of space, representing a region of space containing those input elements that are relevant. So let's look at an example. Let's suppose that you're trying to figure out the best places to open a chain of stores in the United States. So let's say you have a list of cities in the United States. And here I went and traced a map from the United States. I can't draw this well. I traced a map onto the tablet, and I dotted in, I think, the 50 largest cities in the U.S. And let's say that you're trying to open a chain of stores across the United States, and you want to know the accessible population of each city, meaning how many people could easily get to a store in this city? So let's define accessible population as all the people that live in cities within 300 kilometers of this city. So these are people who could go for a long drive and get to your store. How are we going to find the accessible population of each city? You could launch a thread for each city and have it look at every other city. You get the idea. And then it could compute the distance to all those other cities, and if it's under 300 kilometers, then add the population of that other city to a running total. So in practice, most of these would be more than 300 kilometers away, and you'd just discard them. That would be really inefficient, right? You can imagine that--having every city check every other city. I've only drawn 50 cities here--if I'd drawn the 2,000, or 3,000, or 10,000 cities that actually exist in the United States, this would quickly become way too much work. So instead we could create a spatial data structure, such as overlaying let say a 300 kilometer grid onto the map and then creating a list of cities that fall in each grid cell. So this cell will be empty. This one would have 3 cities. This one would have 1, 2, 3, 4 cities. I think that one has 3, this one is 1 and so on. Right, so we could make a list of all the cities that fall into each grid cell. And now if you want to find all cities within 300 kilometers of a given city, say Denver here, you only need to look at the grid cell that that city falls in and the neighboring cells. So I need to check these 3 here. There are two cities so close together, you can not even tell that is Denver and Boulder. I need to check these cities. So I still have several checks to make but this is still vastly fewer checks than I would have to make if I were checking all of the cities against all of the other cities. So by creating these grid cells, these bins that contain the cities and therefore being able to restrict which sections, which bins I need to look in, I've drastically reduced the amount of work that I need to do to define this accessible population that I'm trying to figure out. So this is a great example of binning. Let's have a quiz. Which of the following operations would probably benefit from the geographic binning stage described above? Suppose that for all U.S. cities in the list we wanted to compute a histogram of city distances from a central warehouse in Denver, Colorado. What if you had a list of roads, each of which was represented as a list of line segments, and you wanted to compute the population of cities within 20 kilometers of that road. Would it help to use the geographic binning that we described? Most U.S. cities are in geographic administrative divisions called counties. To give you a sense of what a county is if you're not from the U.S., I've pulled out the Wikipedia map of U.S. counties and I've zoomed in so you can see some of the counties in our states. And as you can see there's thousands of counties. There's about 3,000 counties across the U.S. So given a list of counties and a list of cities, each of which contains an index to the containing county, the county that it's inside, what if you needed to compute the city dwelling population of all counties? Would this geographic binning operation help? Compute a histogram of city distances from Denver. For every city you would compute its distance from Denver. And you would build a histogram using the techniques that you've already studied. And so, a histogram is essentially a binning operation, right? In this case it's binning on distance, the 1-dimensional quantity of distance. And so although you would use a binning step in the process of creating that histogram, it really wouldn't be using the geographic binning that we discussed, the sort of 2-dimensional geographic binning that we used to accelerate our computation of which cities were within 300 kilometers of each other. And so, I would answer this no--I would say that it's not useful to use that geographic binning to compute a histogram of city distances, even though this is still a geographic problem and a binning problem. On the other hand, if you have a list of roads and each road is a list of line segments, you could easily imagine launching a thread per road or a thread per line segment and using that geographic binning operation to compute which bins, which tiles, which of the grid cells, that line segment intersects, and computing the population of all cities within that grid cell or nearby grid cell. So yes, a geographic binning operation like we described would accelerate this kind of operation. And, finally, here's a step where the geographic binning is simply unnecessary work. If we have a list of counties, and we have a list of cities that have already got a pointer to containing county that the city is in, then to compute the city-dwelling population of all the counties is simply a matter of having every city update its county. So one of the points I wanted to make here is that, in fact, every city is in, at most, one county. And so you already sort of have a bin, a binning in some sense. The counties sort of provide a binning. So even though this isn't a geographic or spatial data structure, this index of counties and pointers from cities to counties kind of gives you a binning And so It would be redundant to apply an additional binning step in this case. So once again, this one is not really going to benefit from the geographic binning operation that we described. So the Stratton Taxonomy's 6th optimization technique is called compaction. And John has described compaction a couple of times. He described it in unit 4 and revisited it in unit 6, where it comes up in sparse matrix vector multiply and in-graph traversal. So I'm not going to go into great length describing what compaction is. Just as a quick review, compaction is useful when you have a computation to perform on a set of elements but the exact elements that require computation are not known in advance. If most of the elements require computation, then its simplest just to assign threads to every element and just have every thread check to see whether they should process or not. The threads that are active will do whatever they are suppose to do. The threads that will not will simply return. In that case you will end up storing something valid-- some valid output in most cells and the cells in the output are the-- the elements in the output where the thread wasn't active, because it didn't need to be processed you will end up with leaving blank. Now if relatively few of the elements require computation, then you're wasting a lot of storage on the output, and you're doing less efficient computation, because a lot of your threads are sitting idol, instead of doing some sort of useful work. Compacting the elements means creating a dense array containing only active elements. So that then you can do your processing on the stents array, producing a dense output array directly. You're not wasting storage on all these invalid, null output elements, and you're not wasting computation on all these threads that are just sitting around not doing anything while the other threads in their thread block are-- the relatively few threads in this thread block that have active elements have have anything useful to do. So as a quiz, suppose that only every 8th element will be processed. So here I've drawn the elements that will be processed and all the ones I've left blank will not be processed. And the idea is that every 8th element--it requires processing. And assume that we're compute limited; in other words, that whatever operation is we're going to do on these every 8 elements is expensive, and that's the limiting factor rather than the memory bandwidth to fetch elements. So assume that we are limited by computation and not memory bandwidth. So the question is about how much less time will the computation take if a thread is launched to run on every element of the compacted array rather than if the thread is launched to run for every element of the original array? In other words, how much faster is it going to be to run on a compacted array? Will it be 8x faster, will it be 4x faster, will it be the same speed, 1x faster? And then I want to ask, what if only every 32nd element will be processed, and what if only 128th element must be processed? So for the answer I'm looking for an integer like 4 or 8 or 16 or 32 or 1, and you're telling me whether its 4 times faster, 8 times faster, 1 times faster and so on. In unit five, remember that we learned the, the GPU hardware processes groups of threads called warps. And at every thread in a warp, performs the same instruction at the same time. And this means, that is some threads are not actively doing computation, they will, they're still going to have to wait while the active threads in warp do there thing. Now todays GPUs, in fact all GPU that Nvidia has ever made, have 32 threads per warp. In this first case, only four out of every 32 threads are actually doing something. And, the rest of the threads are just along for the ride. In a compacted case, if we can compact this down to a dense array. Where all four of these elements, are next to each other and then there's, the remaining elements from elsewhere in the array all kind of compacted in into one dense array. In that case, all 32 threads in the warp are going to be doing active work and so you'll finish the total work eight times faster. So that's our answer. We're going to go eight times faster, if every eighth element is active in the original array, and we compact it and operate and still in the compacted array. And by the same reasoning, if only one of these threads in a warp operating on the original array, has anything useful to do, then you'll go 32 times faster, which is a big deal. You can see why compaction can lead to big speed ups. But the other subtlety to remember about warps, is that if all the threads in the warp take the same branch, so in the code snippet I showed before. If all 32 threads in the warp check to see if they were active, decided they weren't and then exited. Then you pay almost no penalty for that. And so that, that warp is going to fire up, all threads are going to see they have nothing to do, it's going to disappear. And so, actually the warps that are primarily empty or that are entirely empty we'll exit immediately. And so, the case where only one in 128 threads is doing something useful, actually still goes about 32 times faster. because there's one warp that's going to have some work to do. And 31 of the threads in that warp, are going to be sitting around waiting for the single thread that has some useful work to do. But, all of the, all of the threads and the entirely empty warps, that have nothing to do, they exit quite quickly. So you'd pay a very slight penalty for having launched those warps and having them immediately exist. But, unbalance is still are going to be close to 32 times as fast to operate on the compacted array than it would have been to operate on the original array. Stratton and colleagues call their final technique regularization, and regularization is all about load balancing. Load balancing is a problem that's plagued parallel programming since its inception. Now to illustrate the idea of load imbalance, let's go back to our example for technique 5 where we were binning the cities of the United States. If you look at this map you'll notice that the cities aren't spread out evenly. Some grid cells are going to have a lot of cities, like up in this region, and some grid cells are going to have relatively few cities. So any threads that are adding up the city population to nearby cities in these cells-- they've got a lot of work to do, right? They've got lots of cities to look at in the neighboring cells so that they can look at those cities and compute distances. So any threads that are adding up the city population and cells up here have a lot of work to do. They've got a lot of cities to compare to, while other threads that are in the same warp or the same block might be responsible for a city in 1 of these grid cells and have a lot less work to do, and so it will complete quickly and sit around waiting for these long running threads. This is an example--a classic example of load imbalance. So regularization is all about reorganizing the input data to reduce load imbalance. In other words you're taking irregular parallelism and turning it into regular parallelism. For example, in the US cities' example we've been looking at some bins might contain more cities than the average bin, so you can imagine provisioning each bin to have a fixed number of cities, say 5, and then have a special way of handling those relatively few overflow cities. For example, you might handle the cities or grid cells that overflow on the CPU, or you might use a different kernel that implements a different algorithm such as sorting the cities by longitude or latitude. There are many ways you could do this computation that we were trying to do with figuring out which cities were within 50 or 300 kilometers of each other. And now the storage in the main kernel code get's a lot simpler, right? Every bin has 5 slots; each thread loops over exactly 5 cities. And the fact that you'll occasionally waste a bit of compute while 1 thread sits idle for a slot, because it only had 3 cities or 4 cities in its bin is more than made up for by the fact that you never have an entire warp or an entire thread block or an entire kernel waiting on a single thread that has 127 cities to check or 1,000 cities to check. You've regularized the problem. Let's have a quiz on regularization. So check the statements that are true. Regularization will likely have the most impact when: The load is relatively evenly distributed among tasks but with some significant outliers. So for example, most U.S. counties contain at most 1 city, but there are a few that have dozens of cities. Or is it the case that regularization will have most impact when the load varies wildly from task to task? And by wildly I really mean that there's no real average case. An example of this would be popularity. Some people have claimed that celebrity popularity, like if you measure something like number of followers somebody has on Twitter, follows a power law graph distribution, and power law looks something like this, and if true, then this would mean that at any level of popularity, a roughly fixed fraction of people will be twice as popular. Okay so in this case you might say that, "Oh, 80% of the people-- 20% of the people have twice as many followers as the remaining 80% of the people below the line," and that becomes true, no matter where you draw the line, so that would be a power law. And in a case like this, there's no real average case. A few very popular people have as many followers as the rest of the world combined. There's an interesting discussion of this that I found online; I'll put it in the Instructor Notes. Finally, is it true to say that regularization will likely have the most impact when the application can predict load imbalance at run time? In other words can predict where and how load imbalance will occur. So an example here is that it might be a lot easier to add up the number of cities in a grid cell and notice that there's going to be load imbalance than to actually perform some heavyweight computation on each city. So this first example is a good case for regularization. The load is mostly evenly distributed, so there is some regular sort of average case that you can shoot for that'll be relatively easy to predict, And then occasionally you'll have outliers, and you can do something about them such as dealing with them separately in another algorithm or on another processor as we discussed before. So I would consider this a good case for regularization; it's likely that we can come up with a regularization strategy here that'll work. On the other hand, when the load varies wildly, and it's not really easy to predict what is the average case, you have no idea when you start a process how long it's going to take or how many items you'll have to process, then I would say this is not an easy case for regularization, which is really sort of just restated in the final bullet. Obviously for regularization, the whole goal is to extract regular parallelism from irregular parallelism. So for that point you kind of like you be able to know what is regular, so if you can predict where and how load and balance is going to take place at run time, then you can structure the application to deal with the load balancing on the spot. So unload is relatively evenly distributed. It's almost regular already and dealing separately with occasional irregularities is likely to work. If load varies wildly, then it's harder to know what to regularize to, and it's much easier to regularize the workload if you can predict what that workload is going to be. Okay, and that's Stratton's Taxonomy. Data layout transformation, transforming scatter patterns to gather patterns. Tiling, so that we can share input upon multiple threads. Privatization, so that we can avoid sharing output, as long as possible. Binning and spatial data structures, so that we can reduce the amount of input that's necessary to look at for given output. Compaction, so that we can avoid processing inactive inputs and process more efficiently on a sparse set of inputs. And regularization, where we can extract regular parallelism from irregular parallelism, often in the face of strategies like binning and spatial data structures or the scatter to gather transformation. Now you've encountered most of these ideas already in the class. But I do hope this overview helps you think consciously about the strategies you can employ to optimize parallel programs. I definitely recommend that you read the original papers if you're interested in learning more. They give examples from much different bench marks and describe case studies for some of these techniques. I find them useful to help me be principled in my thinking about Parallel Optimization. I'll include, again, links to those papers in the instructor's notes and on the wiki and so forth. Let's shift gears now and talk about ways to avoid programming in the first place. There are many libraries available for CUDA, more come out every day. There are some libraries that are developed by NVIDIA, some by third parties, there's open source libraries, and there's commercial products, and of course there's no way I can cover all of the available libraries out there, but I do want to hit a few highlights here and talk about some of the more popular and useful libraries out there. The libraries I'm going to talk about are, for the most part, fairly mature. They're designed and optimized for performance by experts, and they get tuned up and re-released every time a new GPU architecture comes out. So if you can use these libraries, you should. So the first one I want to highlight is called cuBLAS, and cuBLAS is an implementation of the BLAS, or Basic Linear Algebra Subroutines. This is a venerable library that's been around for a long time. It's used in Fortran and C, and scientists and engineers everywhere use this as one of their go-to workhorses for dealing with dense linear algebra. Next is cuFFT. FFT stands for Fast Fourier Transform, and so not surprisingly, this is the CUDA Fast Fourier Transform. This includes various batched transforms, as well as support for various real to complex, complex to complex FFTs and so forth. It has an interface similar in ways to the FFTW, the popular Fastest Fourier Transform in the West routine, so it's a familiar interface to anybody who uses FFTs as part of their bread and butter toolbox. cuSPARSE is BLAS-like routines for doing linear algebra on sparse matrix formats. Sparse matrices, as you know, are matrices that are mostly 0 and therefore stored in some sort of compressed format, and cuSPARSE supports a variety of formats and includes higher level routines like incomplete LU factorization. cuRAND is a bunch of pseudo and quasi random number generation routines for making random numbers, and this includes device side functions as well as host interfaces for quickly filling arrays with numbers drawn from particular distributions, various high quality random number generations basically. NPP stands for NVIDIA Performance Primitives, and this is basically, for the most part, low-level image processing primitives, so highly optimized low-level primitives for image processing. Magma is developed by the same group that wrote the original LAPACK library, and LAPACK is another one of these tools in the toolbox of many scientists and engineers who simply use these tools for linear algebra all the time. So Magma provides GPU and multicore CPU implementations of many LAPACK routines, so it's sort of a modern parallel rethinking of our implementation of LAPACK style linear algebra. Another linear algebra tool is CULA, which is implementations of Eigensolvers and matrix factorizations and matrix solvers, and for dense matrices similar to the LAPACK API, and there's also a sparse CULA package as well. Array Fire is a framework for data parallel manipulation of different types of array data, including a wide variety of built-in operations from numerical linear algebra to signal processing to financial, so this is somewhere in between a domain-specific library like these others and programming power tool, which is sort of the next category we'll talk about. Assuming that you're already using a library--say, the equivalent of one of these libraries on the CPU, then porting to use one of the GPU libraries is a simple 3-step process. First, you need to substitute library calls with their equivalent, CUDA library calls. So if you're using blas, and you call the function SAXPY-- stands for Single Precision AX plus Y, then you will simply shift that to be a call to cublas SAXPY. Next, you need to manage data locality, so if you are going to be running on the GPU, you want the data to already be on the GPU. So for example you might need to call CUDA Malloc and CUDA Memcpy to move the data on the GPU where the library can use to it. Cublas as CUDA cublas Alloc, cublas SetVector, cublas get vector, for getting the result. And some of the libraries manage this entirely transparently for you, and others you do it explicitly. And finally, step 3, you want to rebuild and link with the CUDA accelerated library; for example, calling the compiler with the -L cublas line. Here's an example of some simple BLAS code. First we initialize N to 2 to the 20th, or about a million, and then we perform SAXPY. Remember, SAXPY--the S single precision, and that stands for AX plus Y. So we're going to set Y equal to AX plus Y by calling SAXPY, and we're going to do this on a million elements, or more exactly, M elements. So here we're going to say Y is equal to A, which is 2.0, times X plus Y. This is the original BLAS code. If we want to run this on the GPU, then the first thing we're going to do is change the comment at the top. So the first thing we're going to do after we change the comment at the top is add a CUBLAS Prefix and change these to be device variables. Next we need to initialize CUBLAS and shut it down again when we're done. And now we need to allocate those device vectors and free them when we're done. Finally, we need to set the vectors, so copying the data over to the GPU, and get the result back. Okay, so by the addition of just a few lines, we've accelerated this using CUBLAS, and I'll point out that of course, if you were doing more than just this one simple SAXPY operation, you'd probably leave the intermediate results. You wouldn't keep setting and getting your vectors back and forth to the CPU. You would do some operation, then you'd use the results to do the next operation and so forth. So a lot of this overhead really only appears once at the beginning and end of your calculation. Now, some libraries are less about solving a particular domain of problems and more about enabling the programmers to design their own solutions. And I call these programming power tools, and the first example I'm going to highlight is called Thrust. Now, C++ programmers will know about the Standard Template Library, or STL. STL provides a set of incredibly useful abstractions that are organized into containers and iterators. Thrust provides a data parallel analog to the STL for CUDA and also borrows niceties from the popular Boost library. Thrust allows host code that runs on the CPU to very easily create and manipulate data on the GPU. The principal container abstraction in Thrust is device vector, and this is analogous to the vector container in STL, but as the name implies, it lives on the device, on the GPU. Like the STL vector container, Thrust device vectors are generic containers, which means they're able to hold any data type and they can be resized dynamically. So once your data is in a device vector, you can do a bunch of things trivially. You can do things like sort the device vector or perform a scan operation on it, you can do a reduction operation or a reduce-by-key operation where you actually take a vector and reduce it according to a key that's in another vector. So, for example, if you have a device_vector X-- in this case it's got 3 elements and it's of type float-- then I can do a reduction on those 3 elements by just saying thrust::reduce, pass in x.begin and x.end to indicate that I'm going to reduce over the entire range of the vector, and it will give me the result. Or you can do a reduction of a different sort. So rather than simply doing a sum reduce, we can pass in an operator to use or a functor. In this case we'll do thrust::maximum. So in this case, this reduction will give us the maximum number in this vector. You can also do more general transformations. You can transform 1 or more input vectors into an output vector. For instance, you could do a vector addition operation, taking 2 vectors and adding them into a third vector using the built-in Thrust plus operator. Or you can apply a user-defined transformation, using a functor, and this is a chunk of code that you give Thrust and tell it, run this on every element or on the collection of input and output elements that I'm interested in. And you can interoperate with CUDA codes, so you can mix up Thrust code which is running on the host and doing things on the device, but if you then need to do something that is not built in to Thrust or that's difficult to do with Thrust and you've got some raw CUDA kernels that you want to run, then you can simply get the pointer to the data in a device vector or hand a pointer to Thrust to wrap up as a device vector. In the end, using Thrust is a good idea because it helps you avoid writing a lot of error-prone sort of cut and paste boilerplate code, and you can exploit high-performance implementations of these data parallel primitives like sort and scan and reduce and reduce-by-key. These have all been implemented by CUDA Ninjas for you so that you can just reuse that code. So Thrust can be very handy. We're going to give you Thrust code that sorts 10^6 floats, and we're going to ask you to time it, and then we're going to ask you to also time sorting 10^5 and 10^7 floats and then enter here how long it took to sort 10^5 floats, 10^6 floats, 10^7 floats, and 10^5, 10^6, and 10^7 bytes. And enter your answer in milliseconds, and we'll just check to within a significant digit or so. So Thrust gives a host-side interface that can replace writing CUDA kernels in many cases. But for those times when you are writing CUDA kernels you'd still like to achieve software reuse. You'd like to identify the building blocks in your kernel and use somebody else's highly optimized implementation of those building blocks and then stitch them together with any custom code of your own. But software reuse in CUDA kernel code faces some kind of special challenges. So suppose, for example, that you are calling a library implementation of, say, radix sort, and this is somebody else's implementation; you're going to call it from inside your kernel. And so the question is, how does that implementation know how many threads it is running in? How much shared memory is it allowed to use? Can it use all of the shared memory, or have you reserved some for other purposes? Should it use a work-efficient or a step-efficient algorithm for this piece of the problem? The answer probably depends on what you're sorting, how much you're sorting, and exactly what's important to you right now. The CUB library, by Duane Merrill, provides a neat solution to these challenges of software reuse for CUDA kernel code at high performance. CUB stands for CUDA Unbound, and the reason it's called that is because the implementation in CUB leaves these decisions, like the number of threads and the amount of shared memory, unbound. You the programmer bind the necessary parameters when you write your program using templates and a novel binding approach that's embedded in the C++ type system. The building blocks that CUB provides are really high-performance implementations. So by stitching them together you can create a high-performance CUDA kernel. And finally, because CUB doesn't dictate things like thread block size or shared memory usage, it allows you to experiment with these values as you optimize or auto-tune your code. And one thing you'll find if you get into optimizing CUDA code is that this kind of auto-tuning where you experiment with or even automatically sweep the parameters of your code, things like the block size and the shared memory size, that's a really powerful tool for extracting the maximum amount of performance from your code without investing a whole lot of personal effort. Now let me focus on a specific aspect of writing high performance kernel code. At a high level, GPU programming looks like this. There's a bunch of data sitting in global memory, and you have an algorithm that you want to run on that data. That algorithm will get executed by threads running on SMs, and for various reasons, you might want to stage the data through shared memory. And you've seen for yourself that loading or storing global memory into shared memory or into local variables of the threads can be complicated if you are striving for really high performance. Think about problem set 2 where you loaded an image tile into shared memory and also had to load a halo of extra cells around the image tile in order to account for the width of the blur, and this can be tricky because the number of threads that want to perform a computation on the pixels in that tile is naturally a different number than the number of threads you would launch if your goal was simply to load the pixels in the tile as well as the pixels outside of the tile. Or think about the tile transpose example in Unit 5 where we staged through shared memory in order to pay careful attention to coalescent global memory. Think about our discussion of Little's Law and the trade offs that we went over between latency, bandwidth, occupancy, the number of threads, and the transaction size per thread. Remember that we saw a graph that looked sort of like this where the bandwidth that we achieved as we increased the number of threads was higher if we were able to access 4 floating point values in a single load versus 2 floating point values in a single load versus a single floating point value in a load. Finally, there are ninja level optimizations we haven't even talked about in this class, like using the Kepler LDG intrinsic. In short, CUDA's explicit use of user managed shared memory enables predictable high performance. In contrast, the hardware manage caches that you find on CPUs can result in unpredictable performance, especially when you have many multiple threads sharing the same cache, and they're interfering with each other. So that's an advantage of explicit shared memory, but that advantage comes at the cost of an additional burden on the programmer who has to explicitly manage the movement of data in and out from global memory. And this is a big part of where CUB comes in. CUB puts an abstraction around the algorithm and its memory access pattern and deals opaquely with the movement of data from global memory, possibly through shared memory, into the actual local variables of the threads. I want to mention another programming power tool called Cuda DMA that focuses specifically around the movement of data from global into shared memory. So CUDA DMA is a template library designed to make it easier to use shared memory while achieving high performance. Now to use CUDA DMA, programmers declare CUDA DMA objects for each shared memory buffer that needs to be loaded or stored, and the cool thing is that CUDA DMA let's you explicitly describe the transfer pattern for that data. So, for example, you might be transferring one long sequential trunk of memory, you might be transferring strided trunks of memory, or you might be doing sort of indirect access to memory, such as you would find in a sparse matrix representation. As with CUB, decoupling the transfer patterns from the actual processing that we're going to do on each thread to achieve that transfer pattern has several benefits and improves programmability because the code is now simpler. You packaged away all of that logic for doing the transfer separately from the actual compute in your kernel. It improves portability because you can have the CUDA DMA ninjas or the CUB ninjas develop the very best implementations of these various transfer patterns for your situation and package that all up in a library for you, and because those CUDA ninjas are good at what they do, you get high performance. You're going to achieve high DRAM memory bandwidth, you're going to hide the global memory latency for kernels that don't have a lot of occupancy, and hopefully this will lead to better compiler-generated code. And as I said, these benefits really accrue to both CUB, which tackles the whole circle from bringing data in from global memory and doing the computation on it, as well as CUDA DMA, which is just tackling the top part of that cycle where you're bringing memory into shared memory, and then you're going to do your own operations on it, so these benefits really accrue to both approaches. So let's have a programming exercise with the CUB library. Here's a simple example of the CUB blockscan primitive, okay, and this is doing a prefix sum in a single block. And you've seen scan a lot by now; you know a lot of the intricacies of how it can implemented, and so here's an example of how easy this is to implement with high performance in the IDE. And there's a couple different things about this from some of the examples you've seen; again, we're focusing on a single block, because the whole point of CUB is to give you intra-block primitives: Code for writing your own thread block level code within CUDA kernels. So we're just going to time a single block, and we're going to measure something we haven't quite seen before. We're actually going to measure the scan throughput, and we're measuring that in clocks per element scanned. These are SM clocks: How many actual clock cycles the SM took averaged across the total number of elements that were scanned by this single thread block. You'll see the CUDA built-in clock in this kernel, and the way it's used is obvious. And what I want you to do is fill out this matrix. This is a performance matrix where we have the number of threads per block which is represented in the code with block threads, and the number of items processed per thread, which is represented in the code with items per thread. And I want you to go ahead and look at the matrix here and experiment with different values of block threads and items per thread, and try to figure out how these 2 items affect the scan throughput. It would be too much work to fill out this whole matrix, so let me just guide you to a few sweet spots. So much of the interesting action is going to happen along this diagonal. So for example, 1,024 threads each representing a single item, each in charge of scanning a single item per thread is the kind of thing you might code up naturally. It might be the first thing you try; it's the simplest to code up. And then try to analyze what happens when you do 512 by 2 threads, 256 by 4 threads, 128 by 8 threads. and then let's go ahead and complete the diagonal--won't take long at all. All the way down to 32 threads each responsible for 32 items and 16 threads each responsible for 64 items. And so what I'm going to have you do is fill in this diagonal and from the choices along this diagonal, check off the option that gives you the best performance; in other words, the highest scan throughput which corresponds to the fewest clocks per elements scanned. So when I run all of these, this one is slightly the fastest of all of these on the diagonal-- 64 threads per block operating on 16 items each. And so the thing to notice is that throughput is generally increasing as the granularity per thread increases, right? So having threads do more and more serial work is generally a good thing. On the other hand, if you've got a fixed tile size-- in other words, the tile here is the total number of items that we're scanning over-- for a fixed tile size there's diminishing returns. The improvements get smaller and smaller as you approach this sweet spot because you're trading off increased granularity per thread for decreased parallelilsm. And then it starts to go up again, and in my measurements on Fermi, which is the same GPU that you'll be using on the Udacity IDE, I get 32x32 being slightly slower than 64 threads with 16 items each. And at some point, you get to the point where you can no longer fit a problem size in a single thread's registers, and then performance falls of a cliff again. And for me on Fermi, 64 items per thread running at only 16 threads really starts to get pretty bad performance again. I encourage you to play around, experiment a little bit more to see what the rest of this matrix looks like. And if you're interested, go check out the CUB homepage and see what else you can do. There's different varieties of block scan, for example. There's work-efficient and step-efficient varieties of block scan, and so experiment a little bit and see how fast you can get this scan throughput, how few clocks you can spend per element scanned. And what I've found is that with the right balance, you can have a computational overhead of less than 1 clock cycle per item scanned. That is really pretty remarkable. Think about running this code on a CPU. You couldn't really achieve less than 1 clock per item scanned. It's important to remember that CUDA isn't just a specific language or a set of extensions to the CC++ language. CUDA is the whole GPU computing architecture. So let's talk about some other platforms that support CUDA. First we'll talk about other languages that support CUDA, then we'll talk about cross-platform solutions. So the lightweight version of targeting CUDA is simply to wrap CUDA C++ into the other language, so that you can transfer data to the GPU and call a CUDA C++ kernel from within the other language. And a great example of this is PyCUDA, which allows Python programs to call CUDA C++ kernels, and even to construct them programmatically from within Python. At a deeper level, an increasing number of languages are directly targeting the CUDA architecture. So fans of Python will also want to check out Copperhead. Copperhead is a data parallel subset of Python, that uses a library of data parallel functions such as map, produce, filter, scan and sort. When a Copperhead function is called, the runtime generates thrust code and calls it from the Python interpreter. Memory is managed by the Python garbage collector and lazily transferred to and from the GPU. So, a cool thing about Copperhead programs is that they interoperate with Python programs, using packages like numpy or matplotlib. This makes it easier to write entire applicatons and not just kernels. Cuda Fortran is a product form the Portland group That does exactly what it sounds like. It integrates CUDA constructs such as thread blocks and shared memory directly into the FORTRAN language. More on the research front, Halide is a brand new language specifically designed for image processing. This makes it a DSL or Domain Specific Language. And it's a really cool example of this. Halide targets GPUs as well as multi-core CPUs, on both desktops and on mobile devices. And many of the parallel optimization patterns we've discussed, such as tiling, are particularly easy to express and optimize in Halide. And that makes it possible to get very high-performance image processing code. Finally, the widely used Matlab platform supports CUDA both for wrapping CUDA kernels as well as directly targeting CUDA with Matlab code in core functions. So let's talk about Matlab a little further. So one of the GPU computing platforms I'm most excited about is Matlab. Many of you probably already know about Matlab, especially if you're a scientist or an engineer. Matlab is a high level programming language and development environment that's designed for scientific, numeric and mathematical computations. Scientists and engineers use Matlab all the time for algorithm development, data analysis, visualization, and mathematical modeling. Matlab supports GPU computing in a couple of ways by allowing Matlab code to be executed on a GPU, as well as by providing a way to interface with custom CUDA code. We're going to explore these features using an application that fits the theme of this class, an imaging processing application called white balancing. The white balance operation is used to adjust the tints of a photograph. So you can see here, if this is the before image after a white balance operation, it might look like this. And you can see that we've removed the sort of reddish tint from the image. So this is a simple example of the kind of operation that you might implement in Matlab. It's easy to implement, as you'll see, and as you'll see, it's also easy to use the GPU as well. So let's look at a demo. This is what the example white balance routine looks like in Matlab. As you can see it's quite short. Matlab is a high level programming language. It's got a simple syntax and a whole lot of built in math and engineering functions. So this whole routine can be written in just 6 lines of code. Now the way this is written out, currently this function runs entirely on a CPU. And so next what we're going to do is use the Matlab profiler to identify sections of this code that might be candidates to run on GPU to improve performance. So we run the profiler and it indicates that this last line is by far the most time consuming. This line of code is applying an operation to each color channel across every pixel on the image. Such an operation could be sped up when executed on a GPU. This is clearly a good candidate for parallel computing. So before using the GPU, let's first measure how long it takes to execute the white balance function on the CPU, using the tick and tock commands. Notice that it takes approximately 0.124 seconds to execute the code entirely on the CPU. Now to utilize the GPU for executing that last line of code, there's a couple of options. One thing that we could do is simply transfer the image data to the GPU memory using the GPU array command. And that's what we're doing here on line 14 of this slightly modified white balance routine. Whenever a GPU is passed as input to the most common core Matlab functions, as in this last line of code, then those computations will automatically be executed on the GPU. So, here by adding just that single line of code, notice that the time it takes to execute the white balance function went down from 0.124 seconds to 0.026 seconds. So that's a big speed up, especially for a single line of code. So the second approach to using the GPU for computing within Matlab is to use the Matlab interface to invoke custom Cuda kernels. Here's a Cuda kernel that performs the same operation that was causing that original bottleneck in the white balance routine. And in this Matlab function, that Cuda kernel is being used to speed up the execution time instead of using GPU array. So the first 12 lines of code are the same as in the original white balance routine. Then, however, Matlab loads the kernel and sets up the threading parameters, Creates data on the GPU, and finally invokes the kernel using f eval. And now notice that the white balance routine executes in .022 seconds using the Cuda kernel. So the application executes faster but we should still verify that our Cuda does what its supposed to do. So we need ask the question, do the results from the GPU match the results that we were getting on the CPU? And you can check that the outputs match, using this simple script which computes the norm of the difference between outputs. So a norm equal to 0, signifies that the 2 outputs are the same. After running verified script, we can see that the 2 outputs are the same, and there we go. So, in summary, Matlab is a nice high-level language. It is used heavily by scientists and engineers all over the world for all kinds of applications, and its a great way to quickly design and test algorithms that perform computations on a GPU, either by using the gpuArray command, or by using an interface directly to CUDA kernels. Furthermore, existing CUDA kernels can be quickly and easily tested inside Matlab. It makes a great harness for developing an exploring and testing CUDA kernels. And that's Matlab. Now we focused on the NVIDIA CUDA architecture for this course, but there are also some Cross Platform solutions that compile not only to CUDA but to GPUs and multicore CPUs from AMD, Intel, or ARM. Some of the tools and libraries we've already mentioned, like Halite and Copperhead, specifically target CPU back ends. Three other cross platform solutions worth mentioning are openCL, openGL Compute, and openACC. You'll find that these first 2 are really similar to CUDA in the overall shape. They share concepts such as thread blocks and shared memory, even though the names and syntax vary. For example, openCL refers to work groups rather than thread blocks. But the basic ideas are isomorphic to CUDA, and that's one reason why we focused on CUDA is that what you learn there is really applicable in many places. OpenGL compute, as the name implies, is tightly integrative with the extremely popular openGL graphics library, and this makes it a good choice for developers that are doing graphics or image processing work who need to support multiple GPU vendors. Now the third option, openACC, is a little different. This is a directives based approach. Directives are annotations that the programmer puts into his or her serial code that help the compiler figure out how to automatically parallelize some of the loops. For example, with the appropriate directives, an openACC compiler can transform a nested for loop into a thread launch on CUDA or a multi-threaded SIMD routine on a multicore CPU, so often adding just a few lines to an existing code base can get dramatic speed ups. And this is what makes openACC a great choice for programmers that have a large legacy code that they want to parallelize. OpenACC makes it easy and incremental to add parallelism to an existing code, and it'll be very familiar to programmers that are used to using openMP. The ACC stands for accelerators, and Open ACC is essentially updating of OpenMP to reflect the evolution of accelerators like the GPU. Open ACC compilers exist for C, C++, and Fortran. We're just about ready to wrap up the course. We've covered Stratton's taxonomy of parallel optimization patterns, and we've tied those back to the lessons and assignments that you've had throughout the course. We've discussed the wide array of libraries that let you easily exploit the computational horsepower of the GPU. We've reviewed a few programming power tools to help you write high-performance applications with less work. And finally, we've talked about other platforms, including CUDA support for other languages like Python and MATLAB and Fortran as well as cross-platform solutions for CUDA-style massively parallel programming. So to close the lecture and the course, we're going to hear about one of the more exciting new features in the latest CUDA GPUs, dynamic parallelism. We've invited the real expert on this topic, Dr. Stephen Jones from NVIDIA, to give us a guest lecture on dynamic parallelism. We've also recorded an interview with Stephen for those of you interested in diving a bit deeper afterwards. So let's turn it over to Stephen, then we'll wrap up the course. Congratulations, you finished the final unit. We hope you are as excited about GPUs as we are, and are ready to put your learning in to practice. It's been great having you in the class. Hi, my name is Steven Jones, and I'm one of the senior engineers in the CUDA group at Nvidia. My job is to work on what we think about the CUDA programming model. I have to figure out what we should and shouldn't do to add to it, how it should look once we actually do decide to add something, and of course getting the actual engineering work done under the covers to support the new stuff that we add. And one of the things I've worked on most recently is what I'm here to tell you about right now. It's something we call dynamic parallelism. So, in essence, it's really simple. Everything you've learned so far about running programs on the GPU has followed sort of a code processor model where the GPU is fully under control of the CPU. If you think about it, the CPU can create threads for itself. It cannot work on the GPU like this. Or it can synchronize to wait for the GPU work to finish. It sounds pretty simple, but this extra little arrow allows all sorts of new types of problems to be solved on the GPU, and I'm going to tell you about some of those. So you're used to the idea of bulk parallelism. This is where you break down a problem, an array for example, like this, into a series of threads each operating independently and simultaneously on the data. So imagine I've got a series of numbers, and I want to scale everything by a factor of a quarter. So then I would take each of these numbers, each thread would operate on them independently, and I would end up with my result. This is bulk parallelism because everybody is operating at the same time without any dependencies. Here's a list of some algorithms, and you need to identify which of these is a bulk parallel algorithm. So finding the largest value in a set as if I have a collection of numbers, and I want to find which one is biggest. Summing elements of an array as if I have an array of values, adding them all together to get a final total sum. Adding 2 strings together as if I have a string, for example, hello everybody. Another string, how are you today? Combining them to make a single string of hello everybody, how are you today? So identify in these boxes which of these algorithms would be a bulk parallel algorithm. Okay, so, finding the largest value in a set is something which uses what's called a reduction. It's a parallel algorithm, and it is in fact a bulk parallel algorithm, so that is yes. In this case, you're summing repeatedly between threads to find who has the greatest element and reducing that down to a single element, which is the largest of them all. This is actually very similar to summing elements of an array. This is called a parallel reduction, and it's something you've probably seen before that is also a bulk parallel algorithm that works on the same lines. Adding two strings together, however, is not a bulk parallel algorithm because you have to work your way to the end of the first string and then add character by character the elements of the second string. This is a serial operation, and so it would not be something that you could do with a large number of threads together, so it is not bulk parallel algorithm. So with dynamic parallelism, programs can use other power patterns as well, and that's where it gets really interesting. There's nested parallelism, where I can call a parallel function from inside my parallel program. That's taking a parallel program, creating other parallel programs, so if I have kernel sequence of kernels A, B, and C inside a CUDA stream, and kernel B wants to do its own sequence of steps X, Y and Z, I can just launch that work in line from B exactly where I need it. Without nested powers I might have to work X, Y, and Z into Bs code and make a huge program or do some gymnastics with the CPU to launch the work. Like for example, I would split B in two. I would come out to X, Y, and Z, and I'd come back to the second half of B. That's all very complicated to write and very complicated to manage. It's much, much simpler if I can simply launch my work directly out of B. So here's an example. Suppose you want to adjust the volume of audio stream. We've got an incoming sound wave, this green wave right here. A lot of sound processing requires us to have a certain maximum volume, which means constraining the wave to fit within these two dashed lines right here. We could write a parallel program to deal with this by creating a kernel to process this wave, which would then perform sequences of parallel operations to cause the rescaling of this waveform. So I'd feed this wave into a kernel, and the kernel would break it down into a series of blocks like a normal CUDA kernel would do. The first step would be to find the maximum peak value in parallel, say 1.8 volts. So, having found the maximum peak voltage, my kernel would then launch a second kernel, which would rescale everything by the 1.8 volts to end up with a normalized 1 volt peak-to-peak audio. We're combining several bulk power algorithms into a single program operating on the whole wave at once. So now that I can create a complex composition of parallel nested with parallelism, taking in my audio stream, putting it through a sequence of operations, coming out with a re-normalized stream like this, it opens the door to something called task parallelism. Now, task parallelism is where you run multiple independent tasks, more or less like separate programs each working on their own data. So you've probably noticed that the nested problem that we looked at before, the volume normalizer, could be launched from the CPU, but the GPU's really good at running lots of things at once, so we can take our volume normalization algorithm, and we can run it on dozens of audio streams all at the same time. We've got one program with multiple instances handling multiple audio streams all at the same time on a single GPU without any complicated CPU to GPU communication. This is task parallelism, and this is one way to get a lot of power out of the GPU all at once. So the final new parallel pattern is something called recursive parallelism, and this is really interesting because it lets you do stuff you literally could not easily do before. A recursive algorithm would be, for example, a divide and conquer kind of algorithm. A classic example of this is quicksort, which I think you covered in an earlier lecture, but it's much, much simpler and faster when you use recursion for this. Recursion is where you would subdivide a problem, and using the same operating kernel, you would apply that repeatedly to smaller and smaller subsets of data until you've solved your problem. We'll go into more detail on recursive algorithm later on, but first, let's have a look at how you program dynamic parallelism. Okay, so in this quiz I'm asking you to identify which type of the 4 types of parallelism we've looked at is represented by which of these algorithms that I've written down here. So, matrix multiplication, which is multiplying 2 matrices together, finding faces in a photograph album where I might have hundreds of photographs and I'm trying to find faces in all of them, binary search, finding a number in a list of numbers, and calling a parallel library from inside of a kernel. So matrix multiplication is one of the classic examples of bulk parallelism. Different threads can take different elements of the matrices, multiply them all together and combine them. Finding faces in photographs is task parallelism because I am performing a similar operation on a large number of different input data sets. Each photograph effectively amounts to a task, and I'm doing all of these tasks in parallel. Binary search is a recursive algorithm. I repeatedly look through increasingly small subsets of my data to find the value that I'm looking for, and parallel library calls from inside of a kernel amount to creating new parallel work inside my existing kernel parallel work, so that is nested parallelism. So let's go over some of how actually you program this stuff. >From the programming perspective it's pretty simple. You just use all of the CUDA you've learned so far, except inside a kernel. So the kernel launches, streams, events, synchronization, all that kind of stuff as normal from inside a kernel, so your CUDA from the GP program looks pretty much exactly like your CUDA from the CPU program as well. So what I've got here is a typical hello world in CUDA. What you can see with this version of hello world, I'm launching a kernel to print hello from the GPU, and then I'm following it up with a print of world from the CPU. Note how I need the synchronization here after the launch to ensure that the hello happens first, that it's flushed out to the screen, and then the world will print. And here's the equivalent code for dynamic parallelism. You can see it looks pretty much exactly the same. I have moved the code that was on the CPU directly to the GPU, and the same rules apply. I'm calling hello to be printed from another kernel, I'm synchronizing so that the output completes, and then I'm printing world from the GPU kernel. I would end up launching this just trivially as a single kernel launch of hello world from the CPU to make all of this work. So as you can see, the basic idea is that all the CUDA that you already know how to use from the CPU, you just use it directly from the GPU in exactly the same way you've learned. The rules of the programming model are just as simple. They are based on something called composability. Let me tell you a little bit about that. If you recall how CUDA works, kernels launched into a given stream are interdependent. That means that in this case, for example, B cannot run before A has finished, and C cannot run before B has finished. It's how CUDA streams work, which I think you probably learned about in a previous lecture. So if B creates its own work X, Y, Z, we call this child work to B where B is the parent of X, Y and Z. Now composability says that whatever B does is entirely private to B. That means C has no idea and quite honestly doesn't care what B is doing. B is not considered to have finished until all of its other children, X, Y, and Z in this case, have also finished, so A, B and C still execute in sequence as expected, and B follows the rules of composability such that whatever B does has no effect on Cs ability to execute. So, in effect, B actually looks like a single kernel, even though internally it is doing all of these other steps. So it does not just stop here. Note you can have child kernels start their own children, P, Q and R in this case for example, so you have got grandchildren of B, you can go on to generations of great, great, great grandchildren if you like, until you run out of memory, the point being that it all composes and stacks back into looking like it's a single part of B, and that's composability. So, so far so good. It looks like CUDA. It acts like you would expect, but there are some things you need to watch out for. The first of these is that every thread executes the same program. This is standard CUDA, you know, when you write your kernel code, every thread executes all the lines of that kernel code, but that means that if you have a kernel launch inside your program, every thread will make that launch, and if you don't want multiple parallel launches and you only want one, you're going to have to explicitly select just one thread. Otherwise, you get as many launches as you have threads. Okay, so here's a quiz. Like I said, if you have every thread in a CUDA program executing the same program, you can end up with a lot of parallel launches unless you do something about it. So what do we need to add to this program in order to make sure only the first thread launches this kernel? I've given you a hint at the bottom; see if you can figure it out. So the first thread is thread Id 0 in CUDA. Inside the if statement, you want to check if my thread Id is 0 which is comparing thread Idx.x to 0. In this case, only thread 0 will launch that kernel, and it will only get 1 kernel launch no matter how many threads I've launched in my block. So the second and most important thing to understand about the programming model is that each CUDA block is considered to be its own independent program. That means CUDA objects are private to a block, so streams and events that you create inside a block can be used by any thread in that block, but they can't be used by or exchanged with any other block, including your children. So that means you can synchronize on common launch within your block, but you cannot synchronize on work launched by another block. You can't pass streams or events to your children and have them synchronize on it or you risk a deadlock. This is all part of composability that we looked at a minute ago. And finally, data that's private to a block, like shared memory for example, is private to that block. That means you cannot pass shared memory to your child kernels. So if I have a person data that might block as accumulated in shared memory, which is a pretty common thing to do in CUDA, I would have to write this out to global memory in order for my child kernel to see it. Remember your child kernel is another grid, it's not sets of blocks, and the first, most important, rule is all the blocks are considered independent programs, so no passing around of private data. Okay. So now time for a quiz. So I've written a program here where I'm launching a kernel, just with launch down here, and I'm passing it parameters X, Y, and Z. Select X, Y, or Z to select which of the parameters is not allowed to be passed to the child kernel. Okay, so in this case, the answer is Y is not allowed to be passed to the child kernel because Y is in shared memory, and shared memory is private to a block, which means that the launched kernel possessing different blocks will not be permitted to have access to that shared memory. Note, in this case, when I created Z, I allocated it from the heap. A heap allocation goes in global memory and is therefore permitted to be passed in. So, X and Z are both in global memory, X being declared at global scope and Z being allocated, Y is in shared memory and would need to be copied before it's passed in. Alright. So, let's pull all this together and look at an example of something that was really hard to do before and is now shockingly easy with dynamic parallelism. So, if you remember from one of your earlier lectures, quicksort is what's called a divide and conquer algorithm. It works my partitioning an array of data into two pieces. Partitioning based on what's less than or greater than a pivot value. You then call quicksort on each sub-partition, which is why it's a recursive algorithm. These sub-partitions then get re-partitioned over and over again, recursively, and so you end up at the end with single values, and you're done. Two important things to notice at first that the sub-partitions aren't usually the same size, and second that some branches go deeper than others. This means that the number of elements to be sorted is different each time and so the decision on whether I need to sub-sort can only be made after I've done the partitioning. That's what makes it hard to implement on a GPU because after each step, I need to communicate back to the host CPU all the information about what I want to launch next. That's a lot of communication at each level of the algorithm and it gets pretty complicated to manage. But even worse, I end up launching each stage of the sort as a wave of kernels. As these things partition down further and further, the number of kernels that I have to launch gets larger and larger, and the only way to manage them is to launch them wave after wave. What that means is that I end up waiting as long as the longest operation in a wave, even if my shorter ones have already finished, before I can move on to the next stage. Okay so here's where dynamic parallelism steps scene. It solves both of these problems really neatly. What I've got here is an example of how quicksort would sort a series of numbers. At the top level, a single kernel partitions the numbers into 2 groups then launches 2 quicksort kernels, 1 on each group, and so on down. It would already have all the information it needs to decide whether to launch and how many threads to use because it's just done the partitioning, so it doesn't have to communicate this back to the CPU. That's the first problem taken care of. It solves a second problem, because as these kernels run, they each launch as soon as they finish partitioning. That means my plot progresses asynchronously. For example, this one on the left will launch its children, while the one on the right is still working, because there's more work to do on the right hand side here. Each sort will be running independently of any others. I'm not waiting around for the slowest sort anymore, and my GPU is kept busy. So here's what the code looks like. I'm not going to get into detail about the partitioning function, because we're looking at dynamic launches here and you already covered partitioning in a previous lecture. The important thing to notice is there is no host side management of data or launchers needed. Everything happens inside the code itself. Here are my launches inside my kernel of my next quicksort kernels. So my code's nice and simple, and the kernel launches its 2 children into separate streams. Remember that CUDA streams run simultaneously, which means my 2 sub sorts will execute in parallel. Without these streams, everything would run sequentially, because both of these launches would end up in the null stream, and that would defeat the purpose of the parallel sort to have my program running sequentially. So let's have a quick look back at the sort diagram to show you what I mean. If each launch runs sequentially, then I would do the left side first, then its children, left side first here, then the right-hand side, then I would come back and do the right-hand side next, and then so on, and so what we would see is that each step gets run sequentially instead of in parallel, which defeats the purpose of the parallel sort. So by launching the sort into different streams, we run both the left and right sorts in parallel. So I would run both of these together, and then each of their children and so on, and I'd get the parallel performance I'm looking for. I've combined dynamic parallism with CUDA streams to get the parallel execution that I just would not be able to do any other way. Okay, final quiz. What I've got here is a list of potential reasons why the dynamic parallel version of quicksort ends up more efficient than the CPU-controlled one. We've got more efficient partitioning of the data inside the sort. We've got launching on the fly from the GPU rather than returning to the CPU to do the launch. Simpler code, because the code is a lot shorter and easier to write, and greater GPU utilization. Remember, that GPU utilization is making sure that as much of GPU is busy doing useful work at any given time. So as long as you've got enough threads going on at one time to keep the GPU busy, you have high utilization. So check all the boxes that apply. Okay, so, for more efficient partitioning, that is actually not true. We have not been touching the partition function, the partition function does not have anything to do with the dynamic launches that I can do the recursive parallelism. Launching on the fly, however, yes. That does substantially contribute because I don't have to keep returning back to the CPU to do my launch forming. That means I'm communicating less data and it means that my launch occurs immediately when I need it, instead of waiting around until that particular wave of launched is finished. Simple code, while convenient and I can probably maintain it faster, is not the reason why it actually runs any faster. And finally, greater GPU utilization is probably the cause for the greatest of speedups. By launching on the fly, I'm making sure my GPU is always busy, so when one partial sort finishes, it creates 2 more immediately, keeping my GPU fully stacked up and busy with work. It streams more work for my GPU at one time, and my sort ends up faster end-to-end. In fact, when I've written this program in dynamic parallel form and then host launched form, I see a pretty much exactly factor of 2 speed up between the two. Alright, and that is it. Thank you everybody for taking this class. Following on from this, we have an interview between myself and John Owens, where I discuss something about dynamic parallelism and how we put it all together and where we're going from there. I'm here with Stephen Jones, who's a senior software engineer at Nvidia. Stephen, tell us about your job. &gt;&gt; I am the CUDA lead at Nvidia, which means that I'm responsible for the programming model and for the engineering which goes behind that. So, when we look at the set of things we could do for CUDA, for example, the list is long--50, 60 things that we could do. We've got time for maybe 5 or 10 of them. And so I sit and evaluate what we can do, what's useful to do, what's the right direction. I'm not the only person who makes these decisions. But I'm the engineering end of that. So I, I sit there and figure out what the resources are going to need to be, and what different groups we have to pull together to make these things happen. And so you've been at NVIDIA for 4 years? Yeah, just over 4 years. And you have had kind of an interesting path to get there. So tell us a little bit about your background. So my background is actually not computing. I believe I've never had a computing class in my life, which is not necessarily a claim to fame, but my background is actually fluid mechanics and plasma physics. So from engineering days it turned out that you can't do physics without computing. When I graduated in '96, the whole world was already moving towards that-- computing and science connected. But I was young and it seemed much more interesting to go and write computer games, so I went and wrote computer games for a while, went and worked for the military in high-performance computing, went to work for a bunch of start-ups, and somehow meandered my way around to NVIDIA. So one of the really exciting new things that you guys have recently put into CUDA is this idea of dynamic parallelism, and so we've had some material on that in the course. So before we talk about what it actually is, can you talk about what problem you're trying to solve? &gt;&gt; The thing we really want to do, at least, the way that I think about it, the thing that I want to be able to do with the GPU is to make it easier to program and to broaden the number of problems you can do on the GPU. so the GPU starts out being really, really good at very broad bulk parallelism. You can throw a lot of threads at a problem. It handles them very efficiently, but for less regular problems, for more sort of diverse or fine grain parallelism, it's hard to express that in CUDA today. So the dynamic parallelism effort was to try and aim at an easier way to extract more parallelism from your problem. So we wanted to solve the kinds of problems, things like recursion, things like task parallelism, those types of problems that are difficult to express in a single big grid of threads, sort of paradigm, we might want to enable new types of programming problems to be solved. So tell us about what dynamic parallelism adds to the CUDA programming model. &gt;&gt; So quite simply, it lets you launch kernels directly from inside another kernel, and that sounds relatively simple. If you like the analogy on the CPU side is when I could create threads inside of process. Instead of having to go back to the operating system to initiate a new process, I can suddenly spawn a P thread and do something asynchronously within the single process that I have, manage that all from within my own process. I can now do that on the GPU as well. I can create work and not just single threads, I create whole grids from inside of my GPU. So if my problem is working on something, and I suddenly need to invert a matrix or perform a Fourier transform or something, I can suddenly just call out into a kernel which does that for me, returns the data, and I can continue so I can embed parallel work exactly where I need it with the data that I have available inside my program. Sort of corollary to that is this idea of being able to take data that dynamically you are working on, a value that you have sort of mathematically, algorithmically generated, and use that value to make decisions on work you're going to do. If my value is 1 do this, if it's 2, do that. Or maybe I am partitioning a problem, I am building a tree, spatially partitioning something. If I have a certain number of things in one place and a larger number in another place, I can dynamically launch the correct number of threads to do that, and the ability to do this on the fly dynamically is really the power of this dynamic parallelism. So part of the motivation is going to be ease of programming. So how is it specifically easier for someone to write a program that has these irregular, complex control structures, data structures, compared to not having dynamic parallelism? &gt;&gt; In the past, before the dynamic stuff on Keppler, which is a new GPU, in the past whenever you needed to make a new batch of work, you had to return to the CPU, which was the master controller, to go and launch that work for you. So if ever in my program I reached a point where I needed that matrix inversion or that FFT to be done, I had to halt my program, return to the CPU, have the CPU then launch this work for me that would complete, return back to the CPU, and the CPU would then have to restart me. In fact, if I could have to split my program in 2 around this moment where I needed this extra parallel work to be done, and suddenly, instead of having 1 smooth program, I have 2 fragments of program, I have state that I have to save across the two, and then I have my CPU having to get involved to manage and marshal this work. Suddenly, with the dynamic parallelism, I can just do this all compactly on the fly. If you like, the system does all that for you. It will save off your old program. It will run the new FFT for you. It will return the result to you, and it will continue where you left off. So, from the programmer's perspective, I'm no longer programming in 2 places at once. I'm no longer having the GPU and the CPU both tightly bound over my execution, and I no longer have to manage the portions of my program around where I need to launch this new work. I can just in-line it effectively, and it makes for a much simpler and more straightforward program. That's fantastic. And what about the performance implications? There's always a performance overhead bouncing backwards and forwards between the CPU and GPU. You've got the latencies of the PCI bus, which was the communication link. You've got the overheads of shutting down your first portion of your program, starting up the next portion back, and resuming right where you left. So those overheads get amortized. You save, potentially, data transfer across the buses. And in a way, something I feel is actually more important than this is that with the GPU, you're always trying to get as much work on that GPU as possible. You can much more easily overlap the new work that you're doing with other stuff that's still going on in the GPU. I don't have to shut down completely and fire up an FFT. I can, in-line, do all of these things while something else useful is going on at the same time. And so, this ability to asynchronously do this work from different threads all at the same time. Remember you've got thousands of threads on the GPU, they can all be doing this, I mean modular resources, they can all be doing this at the same time, and so you can get a much easier overlap between the different pieces of work you're doing, and it's definitely much easier to keep the GPU busy, and that gives you a lot of potential for more performance. So what are the kind of problems you've looked at recently as you were designing this where dynamic parallelism really makes a difference either in terms of usability or performance? &gt;&gt; In terms of usability, obviously, it's simpler to program when you don't have to keep going backwards and forwards to the CPU. So any kind of problem which dynamically discovers the work as it goes--iteratively works it's way through something. Imagine you're constructing a tree. You're partitioning something into an Octree, which is a common 3D spatial problem. You don't necessarily know how many objects are going into which part of your tree until you reach that point in your tree. So typically the approach would be to do this level by level by level which is not the most balanced way of doing things necessarily. So as you discover the type of work that you need to do, the ability to simply launch that work is much, much simpler. We were very motivated by irregular parallelism, if you like; problems which did not have nice, well-balanced things. A similar sort of category or problem to that is task parallelism where I might be wanting not just to do one thing that fills my whole GPU, which is often difficult. Now the GPUs these days are a teraflop of performance. It might be much easier to have half a dozen or a dozen things running on my GPU at a time, and so if each of these can autonomously make forward progress, it's much easier to manage them if they're just managing themselves instead of having my CPU now juggle 12 different different things instead of just 1. And finally there's the there's the new type of algorithm that you can approach. You can approach recursive types of algorithm-- things where the category is generally the divide and conquer algorithm, where you take all of the work that you need to do, and you conquer it by subdividing and subdividing and subdividing repeatedly, and a typical example would be quicksort, for example-- a well-known problem where you take your data that you want to sort and recursively you progress through the data until you end up with a final sorted data set. &gt;&gt; So 1 of the demos you guys showed when you launched this was N body simulation interstellar, a bunch of stars moving around, being attracted to each other with gravity. And so, with dynamic parallelism, you were able to write that in a way that you hadn't written it before. So how did that come about--like, what was the cool thing you could do with it? &gt;&gt; I mentioned octree spatial partitioning just now, and that is a key component of an N body simulation. So the way that you approach an N body simulation with a very large number of bodies is instead of doing an all to all comparison where you just calculate the gravity between all the bodies-- so for N bodies that's an N squared problem. You can cut down the complexity of the problem to an N log N or an order of an N problem by partitioning things into space, doing a local interaction of gravity between your close neighbors, and doing an approximation to a center of mass at a more distant neighbor. To do this, you build an octree, partitioning your bodies up into small octans, little cubes. Everyone inside your cube, you do the N squared problem, and for all of the other cubes you then only have an order of an N expansion for. So what we did with dynamic parallelism in this N4 body problem was we optimize the tree build which is about half of the time in the whole simulation. We optimize the tree build by using this recursive property by using this irregular parallelism ability where the tree might--in the case of a galaxy of stars, for example, might be very, very dense in some regions and very, very sparse in others to much more efficiently build the tree. So instead of building the tree level by level by level so that you're wasting work building the tree for areas of where the bodies are sparse, you focus the compute performance on the area where you need it. &gt;&gt; Could you have done this without dynamic parallelism? &gt;&gt; Yes, I guess you could, but the overhead of moving backwards and forwards between the CPU and GPU would probably have negated the performance gain that we got. &gt;&gt; Steven, what kind of problems would be a good match for this dynamic parallelism capability? So as programmers are thinking about what techniques to use in designing the next-generation algorithms, what are the things where dynamic parallelism is going to really make a difference for them? &gt;&gt; I guess when I think about the features that I want to add to CUDA, I really want to make it easier to write the programs you want to write, and dynamic parallelism, I think, really gives you much more flexibility in how you write your code. So as well as things like recursive algorithms, which are extremely difficult without it, sometimes it's just easier to write a program wholly on the GPU, for example. If I know that my program is a small integrative loop launching lots of parallel work, it might be just simply be easier to write the whole thing on the GPU and have a GPU thread control the whole thing from scratch. So if I'm in a situation where my memory marshalling backwards and forwards between CPU and GPU would be complex or difficult-- if I put my whole program on the GPU, all my memory's in one place, and it's much easier to write. So the first kind of problem I would look at is one which would just be made easier by keeping all of your code in one place. You may not necessarily go any faster. Remember serial execution on the GPU is slower than serial execution on the CPU. When you're developing code, even though you might be able to get something working faster if you do a CPU and GPU combination, typically--you know--as I said, I came from a science background. You don't have 6 months to tune your code; you've got 4 weeks before your paper deadline. So the question is not, how good can I get it? It's how good can I get it in 4 weeks before my paper is due? And so if I can give something like dynamic parallelism, which will make it easier to get as good as you can in 4 weeks-- you might only get to 75% of performance in 4 weeks. But if without it, you'd only get to 50% of performance, you can do that much more science in the time you have available. So in spite of the new types of algorithm that it lets you do--and those are very interesting, because it lets you approach things on the GPU you simply couldn't do before. I think the first step is to say, "Well, does this just make my life easier?" If it makes your life easier, you can spend your time focusing on doing the science or the calculation you need to do, rather than wondering, how on Earth do I program this thing in the first place? So, broadly, as you and your team are thinking about what to put in the next 3, 4, 5 generations of CUDA, you know, what's foremost in your mind? What are the sort of characteristics that you're thinking about to pick design features that are going to move CUDA forward? I'm really focused right now on the heterogeneity problem, that is, I've got this system where I've got processors which are good at one thing, serial processing, a processor which is good at another thing, parallel processing. They're 2 separate processors, and they live with their own separate execution spaces and memory and hardware and all that kind of thing, and bridging that gap to make it easier for the programmer to reason about what he wants to do and to express what he wants without having to fight the system. That's really where I think that the biggest advances need to be made. So I'm putting, I'm spending a lot of time thinking about memory models, for example, making life easier for people who, you know, you might not always know what memory you need to move, or you might find it inconvenient to figure out how do I move data while I'm computing on something else. All of those types of things that you have to think about in heterogeneous system. Anything I can do, and I am really not sure what that is yet, I am still working on it though. I really see the direction such that whatever we can do to make it that easier will make the system easier to program, and I think inevitably in the future there is a place where you've got specialist processors working on the task for which they are ideally suited, because that gives you the best performance for the power that you've got, the best performance for the silicone that you've put in there, it probably will solve the problem faster. You know if you got a massively parallel processor, do your parallel work on it, but it now means you've got this space where the programmer's no longer thinking about one type of program, he has to think about 2 or 3 or 4 or however many disparate things he's got. And you know, I've not yet seen a great solution to this. But, you know, we're working towards trying to find one. &gt;&gt; And so let me conclude with another question. What advice do you have for students that are taking this course and learning parallel programming maybe for the first time? &gt;&gt; I think parallel programming requires you to think about your program in a different way. And the thing that I see most when I go out and I-- I spend a lot of my time talking to people who are trying to use not just GPUs but clustered computer systems and multi-core systems. And I spend--a lot of my job is finding out the problems people are facing. A lot of the problems boil down to not NPI is hard or clusters are hard or networking is hard. It actually boils down to parallel programming is hard. And it's hard because a lot of them are thinking in terms of, "I know how to write a serial program. I assume parallel programming must be the same." But often you need to think about your problem in a different way. You need to think about, "How do I break my problem up into independent pieces?" Because the way that you get your parallel problem solved is to have lots of independent pieces which only come together either in small subsets or infrequently. Because (inaudible) will always kill you in the end. And so you got to find a way around that, and if you start out thinking of your problem in parallel terms instead of in serial terms, you can--you will end up with a much better solution to your problem. Sort of analogous to how if you're trying to think of solve a problem in C++-- maybe this does not actually apply to the listeners--I'm not sure. If you're trying solve if you're trying to solve your problem in C++, don't start thinking about it in C, because although you can express C in C++, your solution in C++ will be a very different design to how you resolved it in C. Same goes for parallel serial programming. So it's not--this is not just Cuda advice in general. This is parallel programming advice in general. Sit down and think about your problem in terms of, "Where can I extract the independent work?" Rather than, "How do I do the work? Now how do I multiply it by 10?" &gt;&gt; Steven, thanks so much for coming in today. We appreciate your time. &gt;&gt; It's a pleasure.