Now that we have a good understanding of modern application design, I'm pretty sure our next step is to deploy our code. Now I'm thinking about application dependencies. I'm thinking about how to build packets and distribute all of these applications, and this seems like a lot of work. Kelsey, is there any way around this? One option would be figure out all your dependencies and build a native package for every Linux distro you plan to support or you can skip all of that and build a container image. What's a container image? A container image is a packaging format that includes not only your application, but all your dependencies or runtime information required to run it. Think about your mobile phone. You download a self-contained application and run it. Containers do the same thing but for your servers. So this is something that's available on servers today? Close. The container technology is built into most major operating systems, including Linux and Windows, but it's not exactly easy to use out of the box. You'll need something like Docker which provides a nice API on top of container technology and makes it easy to create, distribute, and run container images on your own servers. Okay, I get it. You know, given the available options, and my sanity, it seems like container images are the way to go. So what is Docker? Docker is an open source tool that makes it easier to create, distribute, and run applications. It does this by packaging applications as a set of container images that include all of the dependencies bundled with it. Container images can be run on any machine where Docker is installed which means containers or assist admins dream come true. Reproducable and Consistant. Containers like processes start up and shut down quickly. We're talking milliseconds here. At the same time, containers provide the benefits of a virtual machine like isolation between applications running on the same machine. This means we don't have to worry about conflicting dependencies between our apps. For this course, we use Docker to package, distribute, and run our application. So now that we're logged in to our cloud shell, let's take a look at how we manage applications without Docker. We'll kick this off by creating a Ubuntu VM to play around using the gcloud compute instances create command. Once our VM is created, we"ll log in using the gcloud compute ssh command. On our VM, let's use our native package manager to install NGINX and all of its dependencies. First we'll have to make sure everything is up to date on our system. Now, with our system up to date, we're going to install the NGINX web server. And let's check the NGINX version. Let's use systemd to start the NGINX based on what we did previously. Now I need to check that the NGINX is actually running. One way to do that is with the systemctl command. So I see that everything's actually running. I see the logs are telling me everything looks right. So now I should be able to hit NGINX locally, using curl. This is great. So modern OSs make it really easy to install applications, start, and run them. What if we want to run two versions of nginx. What happens when we install it again? Using our Package Manager, let's try to install nginx again. It says it's already installed and we have the newest version. So, that's not how you do it. Let's try starting nginx again and checking the status. I still only see one instance of nginx running. And we can confirm this using the ps command. I can see that there is only one nginx process and it's worker. So that tells me there's only one main instance of my application running. So, without going into all the details, to create two instances of nginx using native OS tools, we'll need to modify init scripts. It takes a lot more work. In particular, you have to modify this file. We'd also need to manage ports because two instances can't bind to the same port. There's a lot of complexity here and that's not even considering how to run a newer and older version of nginx at the same time. Most OSs only allow you to run one version. So we're going to have to change things up a bit and take a different approach. So let's stop nginx. Let's talk about what native operating systems are good at. When it comes to managing applications, check any of the boxes that native OS's do well. It's time to go over our answers. First let's talk about some of the problems Kelsey ran into when trying to run engine x multiple times. When we ran the sudo app get install engine x command we run into a problem, where we have to install all of engine x's underlying dependencies. So when it comes to running different versions of engine x, they might rely on different versions of the same library. And this is where we start to run into conflicts. We could try this different versions, but that will replace our old version. So without building an application from source and hoping it doesn't have problems with different library versions. It becomes almost impossible to have two versions of our application running side by side. We also saw that it was very difficult to start up a second instance in of our engine x application. So to answer our quiz question, with our native OS, it was easy to install engine x by itself, so that's a check. It was also easy to start engine x and stop it, so that's another check. But it was next to impossible to run multiple versions. And running multiple instances wasn't much easier. So those are both not checked. And this is where containers come in. What containers allow us to do is take all these dependencies and package them together. This way we can keep our applications isolated from any other running instances of an application, so containers definitely make a sys admins life easier here. Containers were developed to solve problems with installing and running software across different operating environments. You saw the problems of just installing and running the applications on a single machine. Now imagine trying to run and install EngineX on multiple machines across different OSes. It would be a nightmare. With containers, you get independent self-contained packages that avoid version conflicts. This is part of the appeal of container images. They're easy to distribute because they carry all of their dependencies with them. Now, the point of containers is that we want isolation from various processes. We've already talked about isolation of the file system level, but there are other kinds of isolation too. For instance, network isolation means that each instance gets its own IP address. They're all free to buy in port 80, which is EngineX's default port, without us having to deal with any init scripts or startup files. An important concept to note, whereas two separate virtual machines on a computer will run two whole separate OSes, multiple containers run in the same operating system, because containers are a logical construct we use within the OS. That's what makes them so lightweight, and easy to start up and shut down. Now we'll show you where Docker really shines, by showing how easy it is to get multiple versions of nginx running using it. This is where Docker adds some value. It allows us to run self-contained applications that have their own versions, packaging that includes their own dependencies. So two versions need different dependencies, Docker allows us to do that by bundling up these two containers. Plus Docker abstracts away the underlying OS package managers if they may had been to learn the RPM on Red Hat and apt-get on Debian. We can just learn the Docker CLI. Let's get started by installing Docker. Now that we have Docker installed, we are ready to work with Docker images. You can see we don't currently have any images installed on our system. So let's go ahead and get the same virgin nginx we installed through our package manager. This takes some time to download the image from a repository because we're pulling in nginx and all of its dependencies. So that the image can be self-contained. We'll talk more about repositories in the future, but for now let's focus on images. Now if we run the Docker images command, you can see that we have nginx image. And you can verify this is the same version of nginx we installed using the native OS package manager So we can install multiple instances of the engine next application now. But can we run them? Let's use the Docker Run command to run the first instance of the engine next container. Now we can use the Docker PS command to check that it's up and running. I could run another. It doesn't even have to be the same version. Notice this time, since we didn't have the image array on our system for version 1.9.3 of nginx, the docker run command pulls the image in the background first. Sometimes when I'm home alone, I'll sit in front of my computer and just start up hundreds of instances. Just because I can. Let's run one more. I'll stick to three today. We can see that we have all three instances running using the P.S. command from earlier. You can see from the output that we have three nginx master processes running. Docket makes this so easy. We don't have to touch nsscripts, setup ports, or any of that stuff to run multiple instances, or even different versions, at the same time. Now that we've used docker to start multiple instances Of our Inganest container. I want to show you how to communicate with them. What we can do is look at the container ID of a docker instance using the PS command. I'll copy the container ID and use a docker inspect command to find out more information about a specific container. >From the output I can see the IP address of the container. I can then use curl to hit the engine next instance running inside of the container. And there you go, we're talking to that instance. Now that we're done with our instances, let's clean up. When we want to stop these docker instances, we'll use the docker stop command with the container IDs of the instances we want to remove, like this. And we'll use the docker rm command to remove our docking containers from our system and any file they may have left behind. And we can verify the three inganest instances are no longer running using the docker ps command. And we'll use the docker rm command to remove our docker containers from our system and any files that they may have left behind. Until now we've only been building and running pre-made images. In this case, nginx. It's time to learn how to build our own images. As a best practice, we're not building our application with Docker. No, instead, we're taking a binary that could have come from our continuous integration pipeline. Then we use Docker to package our application as a container image, and to do that, we use a Dockerfile. Dockerfiles are text documents that contain all of the necessary steps for building an image from the command line. A Dockerfile, when used with the Docker build command, automates that process by executing these command line instructions to build the resulting image. Let's look at one of ours. Really quick aside, Dockerfiles are always named Dockerfile. That's because when the Docker build command kicks off, the command that uses our Docker file to create an image, it also creates a context of all the files in our directory and its subdirectories. The build daemon will then search for a file called Dockerfile, although, in practice it's best to just start with an empty directory as context and keep your Dockerfile there. In that directory, you only need to add the files necessary for building your Docker image. Okay, so let's pull up our Dockerfile. After running the cat command you can see the entire Dockerfile for Hello image. Each line in a Dockerfile starts with a command. This Dockerfile just uses four, the FROM, MAINTINER, ADD, and ENTRYPOINT commands. By the way, Dockerfiles can have comments, and they start with a pound sign, just like bash comments. You can use comments to make your life as the maintainer easier, and to make it easier for the people trying to decipher your Dockerfiles. Quick question. Do you think Kelsey didn't use comments in writing this Dockerfile because this is a really, really simple Dockerfile? Or because his comments are so profound that many programmers decide to retire after reading one? You decide. Moving on. The first command in a Dockerfile is the FROM command. This tells Docker which base image to build our new image on top of. These days it's always best to use Alpine Linux as your base. Because it's small, has a package manager, and it allows you to debug your containers in production. Alpine is also used by default for all the official Docker images. We could optimize and use the scratch space image, which amounts to a no op, but then you lose the ability to do basic debugging like pinging from inside of your container. Besides, if you're using the same base image, Docker only has to pull that once. So we're only pulling that 5 megabyte cost once per machine. Next we have the MAINTAINER command. That just tells us that Kelsey is the author and maintainer of this image. Following that you'll see the ADD command, which takes a file or directory is from our host machine and adds them to the file system of the container at the specified location. In this case, we're copying over the hello bin from our continuous integration or our go build earlier into our container image. Finally, ENTRYPOINT. This lets our container run as an executable. So when our container starts, it's going to run the hello app. There are more commands you can put into a Dockerfile and you'll probably see some of them in the wild. Before our course, this is everything you need to know. Until now we've been using Docker to run and install containers. But I'm going to show you that you can also use Docker as a build environment. First let's build our application. You may have already done this in a previous lesson. I'm going to make a new directory to hold our source code then I'm going to change into that directory and clone the example application. With the source code in place, we're ready to build our monolithic app from lesson 1. After that's built, we're finally ready to create a container for our app. We'll start by looking at our Dockerfile for the monolith on this line we're copying over the monolith built artifact from the go build command earlier. The next line is where we declare our entry point for the container which will be the monolith app. So with the Dockerfile and a monolith binary in place. We are ready to build the monolith Docker image. We'll use the Docker build command to upload our build context to the remote Docker daemon which will execute the Docker build command listed in the Dockerfile. This will reproduce an image for the monolith application. Now that the Docker build is complete use the Docker images command to list the monolith image. Let's make sure the monolith image is good by using the Docker run command to create a container from it. Now that we have a running monolith container use the Docker inspect command to grab its IP address. And now are we're ready to test the monolith container using the cURL command. It works! As you can see, building container images are breeze when using Dockerfiles and the Docker build command. Okay, so I've got it now. Now that we have the application packaged up, it's got to be time to copy down to the server and run it. Slow down there Cowboy, while you could do that, it kind of defeats the point the whole point of using Docker was to make it easy to distribute your applications. What you propose works for a small set of machines but won't scale to say a hundred machines. The right thing to do in this case is to push your container images to a remote repository and leverage tools like Docker to fetch those images from a central place when we need to run them. Okay, I think I've heard of those. Are you talking about Docker Hub? Yes. But if I use that can't anyone access my containers? Yes, all images you push to the Docker Hub are public by default. But don't worry, you also have the option to make those images private and limit access to them. You also have the flexibility to share some images public and others private. This often is the case when companies have open source projects they want to share with the world but keep their secret sauce to themselves. Okay, I get it but I think this is something I need to see in action. The true power of docker is the ability to not only build images, but share them with others. To do that we'll need to host our images on a remote container image registry such as the Docker Hub. Review the output of the docker images command and notice the name of our monolith container. By default all doc images go to the Docker Hub which assume that you own that repository. So what we're going to do is use our own name space which is our Docker username. So what we'll have to do is add our user name to our Docker image using the Docker tag command command. >From the help output, you can see that we take an existing image tag combination and we add a user name and a repository. The repository would be important if we were pushed into a different repository other than the in the default docker hub like GCR or QUAY.IO. But since we're not, we'll just add a username. Let's add the username using the docker tag command. Now if we use the Docker image command, you can see we have our tag. >From this output you can see that the Kelsey Hightower Monolith image name has the same image ID as a monolith image. Now we are ready to push our current tenor image to the docker hub. Used the Docker push command to push our image up to the Docker hub. So that didn't work. That's because we need to log in before we can push to the registry. We can use the docker login command to do that. Now let's try to push our Docker image again. It worked this time, and now we have a hosted image that we can share with others. Great, you've learned how to package applications in a container images. Which means you can now use tools like Docker to not only distribute them, but run them on a large range of machine. This unlocks the doors even more advance automation. Now you have the power to abstract away the gory details of packaging and running your application on specific operating systems. Now you can focus on shipping your ideas and making your users happy. But this is just the first step. Until now we've been working on a single machine. But if we're limited to a single machine, we're not going to be able to meet the demands of today's users. So how can containers be coordinated, distributed, and managed that scale? And at a even more fundamental level. Should we even be talking about running specific containers on specific machines? What's really important is that our application itself is running, delivering value and being monitored and maintained. And to do that we'll be using a platform called Kubernetes to manage all that complexity. [MUSIC] Containers, microservices, kubernetes. These are a few of the hottest buzzwords floating around. But what's driving the hype? In the last decade, user demand for always on applications have grown exponentially. If you go to your favorite site, you expect, no, demand it to be up 24/7 globally. No exceptions. Many developers have chosen application patterns such as microservices, to meet this need. But what about the infrastructure to support these always on apps? This course is going to show you, step by step, from theory to production, how to meet these demands using tools that are available today. I'm your host, Carter Morgan. And like you, I'm new to this world of microservices and the advanced infrastructures used to support that. So I reached out to my good friend, Kelsey Hightower. You may know him as Mr. Kubernetes, AKA, a sys admin who can code. Kelsey will be serving as our industry expert, giving us deep insights into the solutions we'll be covering. Additionally, we'll be hearing from the legendary former cloud architect of Netflix. The guy who helped make microservices mainstream, Adrian Cockcroft. And he will be sharing his insights on the evolution of applications in modern day infrastructure. But before we go any further, let's hear from Kelsey on who this course is for. You know Carter, this course is for anyone doing operations. Whether you're sys admin, or developer, or anyone new to the field, looking to mystify the idea of managing applications at scale. Many of you may be familiar with automation tools such as Puppet, Chef, or Ansible. We're going to build on that knowledge, and introduce you to the world of distributed systems, and take your idea of automation to the next level. This will be taught through a collection of hands on tasks, using industry standard tooling like Docker and Kubernetes. By the end of this course, you'll know the ins and outs of the cloud application Lifecycle, from packaging it into a container, to managing into production. Kelsey, I know a lot of people are excited about this, but can you give a sneak peek about what to expect. First, we're going to start with the basics of understanding modern day applications, and how a new design pattern such as microservices, has driven the need for more robust infrastructure. Once we have a solid understanding of an application, we'll shift focus towards how to package and distribute our apps, using the Docker image format and container technologies. Then we'll really up our game, to running applications using distributed platform that allows us to start small, and scale to the levels of internet giants such as Google, Facebook, and Netflix. Where do I sign up? I don't know about you, but that's exactly what I'm looking for. Let's go ahead and get started, shall we? [BLANK_AUDIO] Until now, we've given you a demo, but what we're really interested in is production. The goal of this course is to get you ready for scaling and managing containers in production. So, let's do that now. That's where deployments come in. See, deployments are a declarative way to say what goes where. The used in four state is given by the end user. Behind the scenes deployments use a Copa90's concept called replica sets to ensure that the current number of Pods equals the desired number. When it comes to managing Pods, deployment destruct away the low level details like what node is my Pod on, where is it running. Pods are tied to the lifetime of the Node. So, when the Node goes away, so does the Pod. Instead of managing that we can use deployments to make our life a little easier. In the example on screen you'll see that we have a Pod with one replica. Well, what happens when we change the number of replicas from one to three. Our deployment will go ahead and ensure that the number of replicas in production matches are desired state. This means that even if a Node goes down like Node3 just in our example, the deployment will start a new Pod and find somewhere to put it for us and that's pretty cool. It's time to combine everything we've learned so far about Pods and service and the breaking up the Monolith App in a smaller services using deployments. Now we are ready to create deployments, one for each service. Front in, auth, and hello. Then will defy internal services for the off and hello deployments. And an external service for the front in deployment. Let's examine the off deployment configuration file. The first thing I'm going to do is specify the number of replicas. This deployment can be used to scale pods by changing the replica count in our manifest. Moving on, you can see the labels that will be attached to the off pods. A little further down you'll see we're using Version 1.0 of the off container. Use the kubectl create command to create the off deployment. As with any other kubernetes object, we can use to describe command to see more information about the off deployment. Okay, it's now time to create a service for off deployment. You've already seen service manifest files, so I won't go into details here. Use the kubectl create command to create the off service. Now, I'm going to do the same thing to create and expose the hello deployment. I wanted to repeat these steps one more time for the front end deployment. We'll also need to configure engine X like we did in the previous tutorial. Now we are ready to interact with the front in by grabbing this external IP address. And using curl to hit it. Congratulations, you've deployed the multi-service application using kubernetes. The skills you've learned here will allow you to deploy complex applications on kubernetes using a collection of deployments and services. Scaling is done by updating the replicas field in our deployment manifest. This is a best practice because even though we could use imperative methods like the Cube CTS scale command. Then there is no state saved anywhere. Under the hood, deployments create a replica set to handle Pod creation, deletion and updates. Deployments own and manage the replica sets for us, so we don't have to worry about managing them. Using our deployments this way, makes scaling up and down as easy as one, two, three. Let's have show us what this looks like in practice. Behind the scenes deployments manage replica sets. Each deployment is mapped to one active replica set. Use kubectl get replicasets command to view the current set of replicas. Replicasets are scaled to the deployment for each service and can be scaled independently. As mentioned before, the real strength of kubernetes come when you work in a declarative way, instead of using imperative scale and expose commands. Let me show you how to scale the front end deployment using an existing deployment configuration file. First, let's see how many of our hello pods are running. [BLANK_AUDIO] Currently we only have one. We'll change that by updating the replicas field of our deployment manifest. Then we'll apply the change. Now we look at our replicasets, we can see information about what's happening. That desired number of replicas was updated. I can use to get pods command to watch the pods come online. We'll see the correct number of pods here. Now we can check that the deployment of that it is the correct number of replicas. Yep, looks like we now have three replicas that the deployment is managing. And we can still hit our end point like before. At this point we now have multiple copies of our Hello service running in kubernetes and we have a single front in service that's proxying traffic to all three pods. This allows us to share the load and scale our containers and kubernetes. The last question is, what happens when you want to update? Say Heartbleed comes out or you got a new version of your app, what then? Obviously, we want to update our containers to protect our data or to get new code out in front of users, but it would be risky to roll those changes out all at once. So instead we use queued CTO rollout. Let's walk through an example. In this example we're going to be updating our Pods to a new version. So we have a service with three replicated pods, after issuing the queued CTO roll out command for v2 of our app, one comes online. Then our server starts routing traffic to it. So we have both v1 and v2 getting traffic at the same time. Next we stop routing traffic to one of the old pods, and then we get rid of it entirely. At this point the cycle continues, all the way through till we get our desired three replicas back, at which point we have three versions of app v2 running. That's pretty cool. Let's have Kelsey show us this in practice. CortÃ©s makes it easy to roll out updates to your applications by modifying the deployments and managing them. It's major update time. Let's modify the off deployment configuration file to use Version 2.0 of the author container image. Now will apply or update using the coup apply command. We can track the progress of our of the plumbing using the COO CTO describe deployments [BLANK_AUDIO] Let's walk through the output of this command. Here's our only update strategy. This ensures that the right number of PAs are always available. Once the rolling update is complete we can view the running pads for the off service. Here we see the new replica said that's being used to ensure that we are running the latest version of our off container. Once the rolling update is complete we can view the running path for the off service. Notice how long they have been running. The new version of the off Pod have replaced the previous ones. We can verify that the off Pod is running the new version of the off container using the COO CTO described command. Updating deployments this way keeps us with a clean declarative approach for rolling out update Kubernetes makes it easy to roll out changes to our applications using a declarative approach. Whether you have 1 or 100 pause in a deployment, Kubernetes makes it easy to update them using a few simple commands. In this lesson, you learned about scaling and deploying clusters into the cloud. Using these features of Cooper Nettie's makes cloud applications more resilient, and now you can finally call them production ready. Looking back over this course, you learned about modern application design. You learn best practices for using decorative package and distribute containers. And you learned how to manage and scale containers Cooper Nettie's. And for many of you, this is the first container you've ever built, and it's the first cluster you've ever deployed, according to for, I expect great things from you, and if you do, I'll come to your job, and do the Cooper Nettie's dance [MUSIC] It's been a lot of fun learning and growing with you. [MUSIC] [MUSIC] Containers, microservices, kubernetes. These are a few of the hottest buzzwords floating around. But what's driving the hype? In the last decade, user demand for always on applications have grown exponentially. If you go to your favorite site, you expect, no, demand it to be up 24/7 globally. No exceptions. Many developers have chosen application patterns such as microservices, to meet this need. But what about the infrastructure to support these always on apps? This course is going to show you, step by step, from theory to production, how to meet these demands using tools that are available today. I'm your host, Carter Morgan. And like you, I'm new to this world of microservices and the advanced infrastructures used to support that. So I reached out to my good friend, Kelsey Hightower. You may know him as Mr. Kubernetes, AKA, a sys admin who can code. Kelsey will be serving as our industry expert, giving us deep insights into the solutions we'll be covering. Additionally, we'll be hearing from the legendary former cloud architect of Netflix. The guy who helped make microservices mainstream, Adrian Cockcroft. And he will be sharing his insights on the evolution of applications in modern day infrastructure. But before we go any further, let's hear from Kelsey on who this course is for. You know Carter, this course is for anyone doing operations. Whether you're sys admin, or developer, or anyone new to the field, looking to mystify the idea of managing applications at scale. Many of you may be familiar with automation tools such as Puppet, Chef, or Ansible. We're going to build on that knowledge, and introduce you to the world of distributed systems, and take your idea of automation to the next level. This will be taught through a collection of hands on tasks, using industry standard tooling like Docker and Kubernetes. By the end of this course, you'll know the ins and outs of the cloud application Lifecycle, from packaging it into a container, to managing into production. Kelsey, I know a lot of people are excited about this, but can you give a sneak peek about what to expect. First, we're going to start with the basics of understanding modern day applications, and how a new design pattern such as microservices, has driven the need for more robust infrastructure. Once we have a solid understanding of an application, we'll shift focus towards how to package and distribute our apps, using the Docker image format and container technologies. Then we'll really up our game, to running applications using distributed platform that allows us to start small, and scale to the levels of internet giants such as Google, Facebook, and Netflix. Where do I sign up? I don't know about you, but that's exactly what I'm looking for. Let's go ahead and get started, shall we? [BLANK_AUDIO] Before we get into the gory details of the sample application, let's make sure we understand why it was designed the way it was. It's always best to consult with an expert. So, let's ask Adrian Cockcroft about the evolution of the applications. Hi, Adrian. Hi, Kevin. Thanks for being here. Adrian is the former cloud architect of Netflix and the guy who Kelsey says made micro services mainstream. What have you doing since you left Netflix? So I work for a venture capital company Battery Ventures about last two years. What I mostly do is I help up a portfolio companies help with the technology consult with them and spend time looking for interesting new companies to invest in. Very cool, very cool. Well, let's get right into it. Over the last decade or so, there's been a ton happening in application space. Can we talk about some of those changes? Well if you think about the way things were put together years ago, we had huge applications. Think of 5 million lines of Java or something like that, and it would take I was to just do a build and then you'd have to go and find somewhere to run it and it would take a long time to work through all these things. So typically you would release software relatively infrequently, because it was a lot of work to do it, right? What's happened more recently is that we've broken the software into smaller and smaller chunks. So part of micro services is making the chunk smaller. So if your build time is now going from hours to seconds and then you wrap it in a container and you deploy the container it takes seconds to do a deployment. If you're doing it yourself and you're doing it, you write some two piece of code you think that should work you put it through and test it and it takes seconds to do, then you want to do that as often as makes sense. You don't want to wait a month or three months to release your code. Is that why people have been using though? So the main reason for micro services is speeding up development. If you think if you go to let's say one hundred people building a monolithic application and somebody checks in something that's broken. And then in testing they find it's broken that delays the release for a week. 99 people work didn't get out, right, or you put it at production it blows up you have to roll it back another week or so. So what happens when you have too many people contributing to a single large piece of code is that any small thing that blocks it is blocking huge amounts of work from getting out. And you've created a choke point that overly coordinates everything. So breaking it into micro services is really about freeing up everybody to run at their own speed. You can have somebody with a brand new thing they've just started building, they might do ten releases a day as they're just trying to get it into shape. Something else that's very mature might be updating once a month. Other pieces in between, right? Right. So then how do you coordinate all those pieces that are out there? You don't coordinate, right? So if you coordinate them you're going back to the monolith right. And that's what slows things down so the point really is to let everybody release on their own cycle which means you need to make sure that the interfaces are relatively stable. And you can deploy any piece of it independently of any other piece typically without sort of asking permission. But you typically let people know. Okay, that makes a lot of sense. So then what about all the infrastructure that goes into being able to roll back, deploy, monitor or test these? You need a lot of automation to do it effectively. So think of what used to be meetings with operations teams as being turned into APIs and automation and scripts. Right, so the exact way you can figure something complicated might be like a Chef script or something like that does all of the set up on something or it might be baked into a container. If it say the chunk of code that you want to deploy. So those sorts of things will all tie together and then if you want to coordinate a lot of things and release them at once, then you need some sort of container scheduler with some way to compose them together and link everything together. So like a Docker compose file, or a kubernetes configuration file. Thank you, I've learned a lot. Thank you so much Adrian. Okay, thanks very much, cheers. After talking with Adrian it seems like we need to explore this micro services pattern more. Microservices architectures, what does that term even mean? Well, microservices is just a architectural approach to designing applications in a way that's modular, easy to deploy, and scale independently. But given all the benefits, doesn't make sense for everyone to just move to them? A lot of the benefits of the microservices design pattern applies to any application, even your traditional web application. Microservices just happen to benefit more from rapid deployments and continuous delivery. Microservices also push the limits of most automation tools and infrastructure in place today to their limits. Okay, that makes sense now. Microservices are one of the reasons we need advanced tools, like Kubernetes. Throughout this course we'll be using an example application called app. In our app we have three applications. Monolith, Hello and Off Service. The Monolith service combines authentication and Hello micro services. Let's run the monolithic application now to see how it works. Our application is built and we can test this functionality using curl and we can see we got a response back. It's working. Now let's try the secure end point. The request fails because we need to get a JWT token. >From the log in end point. So let's do that now. I'm going to tie password at the prompt. Now that I have a JWT token, can I can hit the secure end point using curl and it should work. As expected everything is working now. A couple of things about the example service I should point out. While it's a monolithic application. It does leverage many principles found in 12 factor applications. For example, we are logging to standard output so logging tools can pick up our logs without touching any log files. You'll also notice the model of application is self-contained. We are but only all of our dependencies in the vendor directory. Following these practices make our application easier to maintain and deploy to the cloud, something we're striving for in this course. I'll kick it over to Carter to explain more about 12 factor and micro services in depth. Kelsey just touched on a really important concept for designing scalable applications, twelve-factor apps. You can think of the twelve factors as best-practices for designing modern applications that fit three really important criteria. Portability, deployability and scalability. Twelve-factor applications are designed to be portable, because they focus on eliminating elements that vary between execution environments, like dependencies and configuration. Not only that, but twelve-factor applications are designed to be deployed on cloud platforms, like GCP, AWS, and by focusing on keeping production in development environments close to uniform. We can start continuously deploying our applications, meaning we can set up tooling to roll out updates to code very quickly. Finally, if we follow these principles when building our applications, they'll be able to scale up to reach our users' demands, while using most of the same toolings and practices we've been employing from the start. Now, Kelsey touched on a few of the twelve-factors earlier and how our application handles them, but there's a lot more. The ones we didn't handle directly in our application, we use higher level tools for later. As you can see, when it comes to deploying apps that scale into the cloud, writing twelve-factor apps helps us reach that goal. Okay, let's go over what we know so far. For this quiz, we'll take the concept that's written down. We use the website 12factor.net to find its corresponding factor. And then we'll type that into the answer box. I'll do the first one as an example. Our application uses Git to store our code, which is an example of Factor 1 Codebase. So I go ahead and type that in. The rest is up to you. Which of the 12 factors is represented by sending our programs outputs stdout? And by the way, we use import statements to deal with dependencies. Okay, let's go over the rest. Kelsey explained that our app sends all output to stdout. This lets developers use the logs as an event stream. For example, if we fail to connect to our off database, we can search for failure error messages in the console. This output could be collated and sent to a more sophisticated service but that's all invisible to our 12 factor app. This is an example of the logs factor. Moving on, Kelsey highlighted earlier our use of go deps and import statements to manage dependencies. External packages are stored or vendored with our application, which reduces a failure point of deploying our code onto different execution environments. We know that whatever we need to run, will be there. This is an example of the dependencies factor. Now that we have a solid understanding of 12 factor applications, let's move on. Now, let's breakdown our monolith application into micro services. Ideally we want to break our monolith around functionality that's self-contained. So I create an auth and hello service. Each service has its own binary. Let's build and run both of them now and see what it's like to work with the new application deployment. I'm going to start with the hello service. In a new shell tab, I'll do the same for the auth service. Now that I have both services running, I can use the curl command in a new shell to interact with the auth and hello micro services. Now that we have multiple binaries to manage for our application, our deployment has been made twice as complex, and now our clients need to know how to talk to two separate services. The additional complexity only grows as the number of services for an application increases. This problem is exactly what's driving the adoption of application containers and management platforms, like curbernetties, to coordinate them. Earlier, we explained how our app is going to be broken down from monolith to microservices. But it's important to understand why that occasion is being broken up like it is. Let's take a break to have a quick knowledge check over the problems that microservices patterns was designed to solve. For each of the problems below, check them off if microservices helps alleviate the problem. Let's go over our answers. When it comes to slow application speeds, calling functionality across process boundaries is more expensive than just calling a function in a monolithic app. Instead of just calling a function, your service will send out a message request. Which, in turn, might lead to other requests being sent. All of this before a response is returned. This doesn't even consider issues that come up with asynchronous or blocking cause retrying for timed out requests anymore. So let's leave that one unchecked. Now a problem with traditional architecture is that making changes to one part of the code causes the recompilation of the entire binary. Micro services came about to allow independent pieces of code to be able to ship separately. Services independently deployable units of functionality that can be replaced or upgraded separately from the rest of the system. So will check yes for tightly coupled components. Finally, when it comes to maintenance micro services are easier to maintain because of their independently scaling components. Each service is also smaller and hopefully easier to reason about. So one person or team can own or be an expert on that piece of functionality. Another benefit of decoupling your application in smaller pieces and communicating between components of clearly defined APIs is that it leads to more robust and stable code base one that should be easier to maintain. Since that's the case, then maintenance will also get checked off. Now that we understand the main benefits of the micro services architecture pattern. It's time to go on. Let's talk about JSON Web Tokens. They're often abbreviated as JWT and they're pronounced like the English word jot, like, let me jot a note down about that, Sir. JWT is a useful standard because it sends information that can be verified and trusted with a digital signature. Additionally, JWTs are a compact means of storing data that's easy to decode or encode in most languages. JWTs are also ideal for authentication and information exchange because of their size and the fact that they can be signed. The most common uses for JWT is authentication, and we'll explain how that's done soon. Since JWTs can be signed, you can ensure that senders are who they say they are, and that data hasn't been tampered with. Now, because of the benefits I outlined earlier, and because of their simplicity to set up and use, we'll be using JWTs in this course to authenticate our services. For this quiz we're going to look at every part of a JSON Web Token. This way when you see them in our code base or if you see our client parsing them around on the command line, you'll understand what's going on. For this quiz copy our token into the encoded window of jwt.io, then examine the payload field of the decoded token to get its name. Let's go through this quiz together. I'm going to navigate to jwt.io and paste in the token. On jwt.io, I'm going to paste the token into the encoded window. If I look on the right, I can see that there's three parts of our JWT token. The first part is the header, the second part is the payload data, and the third part is a signature. The data we want to extract is in the payload. And we're looking for the name field. So I'll extract that data and type it into the box. Hopefully that helped to clear up any confusion about token based authentication with JSON web tokens. In our example, we have a Client and a Server. The Client wants to access some protected data on our Server, but the Server knows Clients cannot be trusted. The Server only wants to give the data to a trustworthy Client. So our Client sends a request to our Server along with data to verify who it is. In our example, this user data was accepted as valid by the Server. Now instead of saving this user data, the Server instead creates a token. This token is returned to the Client and it's up to the Client to store this data and send it along as required for any requests to the Server. The next time our Client makes a request along a secure route, it does just that. It sends along the job token. But our Server knows not to trust the Client because Clients cannot be trusted. So our Server verifies this token is who it says it's from and that it hasn't been tampered with. If everything checks out like it does in our example, the Server sends back a response with the requested data and everybody lives happily ever after. And that's a brief overview of how JSON Web Token work. Wow. There was a lot to learn in this lesson. Now that we've got an understanding of modern application design, including the micro services design pattern, we can see how Modern Apps are pushing the limits of current automation tools. Instead of being one large Binary, now these applications are composed of many parts which are smaller, easier to write and deploy. These smaller independent binaries have led to on demand updates that can happen at any time. This is led to a rethink of the tooling and underlying infrastructure we're using to support these modern apps. These requirements have led to a new set of automation tools like Docker and Kubernetes. Stay tuned for our next lesson where you learn how to use the Docker image format to containerize applications. Until now we've been working on a single machine. It's time to start thinking about scaling up and scaling out. So let's bring our expert Adrian back in. Hi there. Hey, Adrian, thanks for coming back. What role do you see containers playing in the future of application development and deployment? Well if you look at the history of having large systems in the data center. Bare metal machines, typically you'd buy them, you'd depreciate them over three years. They'd sit at the same IP address. They get installed once and used over and over again, and you can think of those machines as pets. What we really want to do is have cattle. Right. So if you lose some you know one cow out of a dairy hood you still got milk, right. You can get another cow. That's that kind of model, rather than having some machine that's very specific and if one knows its name and if they have anything goes wrong with it everyone gets very unhappy. So that transition from sort of these very specific machines to more sort of fleets or herds of machines is something that happened typically when, you know, you can do it with the VMs. But VMs take out as, you know, they basically usually you get them by the hour and they last for a few weeks and you know they have a much shorter life cycle. But when we go to containers became efficient enough that you could actually get a machine in seconds and you could run it for minutes and that was a perfectly reasonable thing to do. So, you can create an entire test environment from scratch, run it until when and run all your tests, shut it down again and you can have lots of those running in parallel. So now you've got machines, some of them you barely know they exist, they come and go very, very quickly. And finally, sort of, if you take it right to the limit, you could create a container just to run a single request and shut it down again. And that's something that's starting to be called serverless computing, and there's a few frameworks for doing that. But that's sort of the leading edge of what's happening right now. So when it comes to writing containerized apps other than make it smaller, what are problems that no one's talking about? So one of the problems is that people get into is if they keep their old organization and the old practices. So if you've got a micro service or an application you are building and the way you used to build your monolith was you had teams spread all around the world that contributed code into the system and you spent, you know, weeks of putting everything together. So when we created software it sort of followed the form. That form, this is sometimes called Conway's law which says that the structure of an organization guides the kind of structure of the code they produce. So what I'm really saying is that you can't build micro services with a waterfall organization. So people might lose some of the benefits of the microservices pattern with organizational issues. What about technical issues like monitoring, logging what are problems with those? And so you need much more automation. That's a lot more moving parts,you need systems that dynamically discover what to be monitored and track them. So, that's the kind of characteristic you're looking for. That sounds like a lot goes into it is this something developers are programming themselves? We're looking back five years when Netflix first started doing this. One of the things we had was that because we were early we had to invent a lot of stuff ourselves. And there was just, we went and read papers and we built things, right. And what the situation now is that there's really too many things to choose from and you spend more time trying to figure out which GitHub lipo has something useful in it, which open source project,s and which the SAS vendor is trying to sell you something. So, there's just too many choices right now, and those choices are changing really rapidly. So, it sounds like infrastructure is something really important, maybe not to code yourself because there's so much, so find something that works, use it for now change or readjust as you go. Yeah, there's no need to build it yourself now, and there's too many good options out there, too many things you can just get hold of and use. There're cloud based services. There's On Premise. There's things you can download for free. So something that could be an 80s has a lot of functionality you can just go use it and you don't have to go try and build all that yourself. So they're building higher and higher level services on top of lots of the detailed engineering what goes into building these frameworks and then you can build on top of them. Okay thank you so much for coming in and talking about this. Thanks very much. After talking with Adrian, one of the industry's leading experts on this subject, it turns out that infrastructure is just as important as the application itself. This course is going to cover the theory and implementation of managing that infrastructure. So in this course we're going to move past theory and show you how to do it, and we'll be using Kubernetes for that. When I first got to Google. I was trying to wrap up on all things sysadmin. It was overwhelming. There were so many tools to learn so many ways of doing the same thing. Luckily, I got to sit next to Kelsey on a two hour bus ride. I started asking him questions I said, Kelsey. I'm having information overload there are so many tools out there. What should I learn, what's the fastest way for me to become productive? And he said there's only one rule way to manage applications and that's Kubernetes. And he spent the next two hours showing me Kubernetes. After that I wasn't so overwhelmed anymore. Kubernetes made sense to me because it looked at containers at a higher level. It was an abstraction that made sense. I was blind before, but now I see. Docker really does make it easy to deploy and run our containers. The logical next step is get Docker on all of my machines, and I'm good to go. Kelsey, what is there left for you to teach me at this point? There you go again, Carter. This happens every time I tell someone about containers. They think that's the end of the story. To be honest, packaging containers is like 5% of the problem. The real issues have to do with application configuration, service discovery, managing updates and monitoring. While we can build all those things on top of Docker, we're better off leveraging a platform to manage all that complexity for us. This is where Kubernetes comes in. Yep, Kubernetes provides a new set of abstractions that go well beyond the bases of container deployments, and enable you to focus on the big picture. Previously, our focus has been deploying applications to individual machines which locks you into limited workflows. Kubernetes allows us to abstract the way in the individual machines and treat the entire cluster like a single logical machine. The application becomes a first class citizen which enables us to manage them using high level abstractions. In Kubernetes, we can describe a set of applications and how they should interact with each other and then let Kubernetes figure out how to make that happen. Okay. You're right. Maybe there's still a lot I've got to learn. Let's deep dive into Kubernetes. The easiest way to get started with Kubernetes is to use a Kubes detail run and command. Let's use a Kubes detail run and command to launch a single instance of the nginx container. As you can see, Kubernetes has created what is called a deployment. We'll explain more about deployments later, but for now, all you need to know is that deployments keep our pods up and running even when the nodes they run on fail. In Kubernetes, all containers run in what's called a pod. Use the kubectl get pods command, to view the running nginx container. Now that the nginx container is running, we can expose it outside of Kubernetes using the kubectl expose command. So what just happened? Behind the scenes, Kubernetes created an external load balancer with a public IP address attached to it. Any client who hits that public IP address will be routed to the pods behind the service. In this case, that would be the nginx pod. So if we list our services now, we'll see that we have a public IP that we can use to hit the nginx container remotely. And there you go. Kubernetes supports an easy to use workflow out of the box using the kubectl run and expose commands. Now that you've seen a quick tour of Kubernetes, it's time to dive into each of the components and abstractions. At the core of is the pod. Pods represent a logical application. Pods represent and hold a collection of one or more containers. Generally if you have multiple containers with a hard dependency on each other, they would be packaged together inside of a single pod. In our example, you can see that we have a pod that has the monolith and nginx containers. Pods also have volumes. Volumes are just data divs that live as long as the pod lives and can be used by any of the containers in that pod. This is possible because Pods provide a shared namespace for their contents. This means that the two containers inside of our example pod can communicate with each other, and they also share the attached volumes. Pods also share a network namespace. This means that a pod has one IP per pod. You know what? Let's take a deeper dive into pods now. Pods can be created using pod configuration files. Let's take a moment to explore the model of pod configuration file. There are a few things to notice here, you'll see that our pod is made up of one container, the monolith. You can also see that we're passing a few arguments to the container when it starts up. Lastly, we're open up port 80 for HTTP traffic, and port 81 for our health checks. Create the monolith pod using kubectl. Let's examine our pods. We'll use a kubectl get pods command to list all the pods running in the default namespace. It may take a few seconds before the monolith pod is up and running, as a monolith container image needs to be pulled from the doca hub before we can run it. Used the kubectl describe command to get more information about the monolith pod. You'll see a lot of information about the monolith pod, including the product IP address and the event log. This information will come in handy when troubleshooting. As you can see kubernetes makes it easy to create pods by describing them in configuration pods and viewing information about them while they're running. At this point you have the ability to create all the pods your deployment requires. PAS allocated a private IP address by default and cannot be reached outside of the cluster. I used a kubectl port-forward command to map a local port to a port inside the monolith pod. Use two terminals. One to run the kubectl port-forward command and the other to issue kernel commands. Now in our new terminal we'll start talking to our pod. Okay. Let's see what happens when we hit the secure end point. Uh-oh, let's try logging in to get it off token back from our monolith. I'll use the super secret password password as my password. We'll use the JWT token from the output to hit our secure endpoint and it works and everything is good in the world again. Here is the kubectl logs command to view the logs for the monolith pod. Let's open another terminal and use the -f flag to get a stream of logs happening in real time. And as you can see, the logs are updating in real time. We can use kubectl exec command to run an interactive show inside the model of POD. This come in handy when you want troubleshoot from within the container. For example, once we have a shell into the mile of container we can test external connectivity using the ping command. [BLANK_AUDIO] Then when you're done with the interactive show, be sure to log out. As you can see, interacting with PAS is as easy as using the kubectl command, whether you're trying to hit your containers remotely or trying to get a log in shell for troubleshooting. Cooper Nades provides everything you need to get up and going. Sometimes a container on a pod can be up and running but the application inside of the container might be malfunctioning. For instance if your code was deadlocked. Kubernetes has built in support to make sure that your application is running correctly with user implemented application health and readiness checks. Readiness probes indicate when a pod is ready to serve traffic. If a readiness check fails then the container will be marked as not ready and will be removed from any load balancers. Liveness probes indicate a container is alive. If aliveness probe fails multiple times, then the container will be restarted. Let's see a visual example of this in action. In our example, we have a pod with a container at B1 and Kubernetes daemon called a Kubelet. Since the Kubelet is responsible for making sure that a pod is healthy, it's going to perform the live check. Hey, app v1, you alive? In this example for whatever reason. Our container is dead and that's denoted by the outline in the red color. So it's going to respond, nope. Since this is the case, the Kubelet will restart our container and then it will perform the check again as necessary. Hey, app v1. You alive? And this time everything is good. App v1 will say, yeah. That was just a quick overview of pod monitoring and health checking. It's time to answer some questions about health checker. For the healthy-monolith pod, how is the readiness probe performed? How often is the readiness checked? And how often is the liveness probe checked? Have some hints. You can check the healthy-monolith.yaml file or use a kubectl describe command to learn more. Let's go over those answers. So when it comes to figuring out how the readiness probe is performed. A good place to look is in the healthy-monolith.yaml file from there we can see our readiness probes information. We see that an httpGet request is sent to the application along the readiness route on port 81. This happens after 5 seconds. Now a response between 200 and 400 must be returned and that has to happen within 1 second. So what about how often is the readiness checked? For this one I used the kubectl describe command and I grep for Readiness. The output is a little bit different so I formatted it to make it look nicer. And you can see that there's a period field here which says 10 seconds. And that's how often our readiness probe is going to be performed. So, what about liveness probe? How often is that checked? Again I used kubectl describe and I can see the period was 15 seconds. At this point we have a pretty solid understanding of pod monitoring and health checking. Good work. Many applications require configuration settings in secret such as TLS certs to run in a production environment. One problem we saw before when we used Docker was that many devs want to bake in their configs and put those on the public Docker. Don't do it. So then what should we do? Some say to put configuration management tools into the Pod. You can, you shouldn't. There's a better way. In there are Configmaps and Secrets to take care of these problems. Configmaps and Secrets are similar except Configmaps don't have to be sensitive data. They can use environment variables and they can tell downstream pods that configuration is changed along a pod or restart itself if necessary. Let's talk about secrets. It's really easy to create a secret from a file. Using the cube CTO creates secret command. In our example we've done this and the Kubernetes Master already knows about our secret. Then we can upload that secret to a pod. Let's see how this works. What happens if we start up a pod that uses a secret? Well after we run this cube CTO create Pod command, a pod is created, then our secret is mounted onto our pod as a volume. This way creators can make sure that our configs are there before our container start. Once that volume is there, we can take the contents of it and expose it on the file system to wherever our pods would go to mount it. Then our pod starts to come online. And finally our container is spun up. We're good to go. So now let's have Kelsey teach us more about creating Secrets to store sensitive application data. And creating Configmaps to store application config data. We'll create a new Pod name secure monolith. Secure monolith secures access to the monolith container using nginx which will serve as the reverse proxy serving HTTPS. The nginx container will be deployed in the same Pod as a monolith container because it's tightly coupled. Before we can use the nginx container to survey HTTP traffic, we'll need some TLS certificate. In this tutorial, we'll store a set of self-signed TLS certificates in kubernetes as secrets. Then we won't dive into all the details here, but we want to be secure. So we'll need some certs, let's look at the certs we have for this particular application. The cert.pem and the key.pem files will be used to secure traffic on a monolith server. And the ca.pem will be used by HTTP clients as a CA to trust. Since the certs being used by the monolith server were signed by the CA represented by ca.pem, HTTP clients that trust ca.pem will be able to validate the SSL connection to the Malware Server. Next we'll use QCTL to cure rate the TLS cert secret from the TLS certificates stored out of the TLS directory. kubectl will create a key for each file in the TLS directory under the TLS cert secret book. Use the kubectl describe command to verify this. The secure monolith pod requires an nginx config to handle the secure reverse proxy. Let's review the nginx configuration file that does this. The proxy.conf configuration file causes nginx to listen on port 443. For secure connections and proxy traffic to localhost on port 80 where the monolithic container is listening. This works because the nginx on monolithic containers, sharing network namespace while running in the same pod. Next, we need to create a configmap entry for the proxy.com nginx configuration file using the kubectl create config map command. Use the kubectl describe config map command to get more details about the nginx-proxy-conf config map entry. At this point we're ready to attach the nginx configuration files and tailor certificates for the secure monolith Pod. It's time to expose the ingernexs prostate cuff configmat and [INAUDIBLE] assert secrets to secure monolith pot at runtime. Examine the secure monolith pocket figuration file. This is our first multi-container pod. You'll see that we have the nginx and monolith containers in here. These containers are deployed in the same pod because they depend on each other. We've talked about the monolith before. So let's focus on nginx. There are two things interesting to note here. One, we've added a graceful shutdown to the container by adding a life cycle hook. So on shutdown, we want to run that command. This will allow ingernex to do the right thing, handle all remaining traffic and shutdown cleanly. Two, at the bottom of our pod configuration file. You'll see we added corresponding volumes. So that the internet's container Can now access our secrets and configs. Create the secure model of pod using kubectl. If we examine the secure-monolith pod, we'll notice that it has two containers running inside of it now. Now that the secure-monolith pod is up and running we can access it using kubectl port-forward command. In a second terminal, let's forward local port 10443 to 443 of the secure mine with pod. Remember, 443 is what our Internet's container is listening on. Use a curl command to test the http end point in another terminal. And now we get a response back from the server. Over the secure https connection. We can use the kuectl command to verify traffic to the Internet container on the secure monolith part. The kubernetes ttl port forwarding command is great for testing pods directly. But in production, you want to expose pod using services. We saw earlier that pods can be restarted for all kinds of reasons like fail liveliness checks, readiness checks, a command, and this leads to a problem. What happens if we want to communicate with a set of pods. When they get restarted it might have a different IP address. And this is where services come in. Instead of relying on pod IP addresses which change, provide services as a stable endpoint for pods. The pods that the service exposes are based on a set of labels. If pods have the correct labels, they are automatically picked up and exposed by our services. Now, the level of access the service provides to a set of pods depends on the services type. Currently there are three types. There is cluster IP which is internal only. There's tight node port which gives each node an external IP that's accessible. And then there's type load balancing which adds a load balancer from the cloud provider which forced traffic from the service to nodes within it. Okay, it's time for you to learn how to create a service and use label selectors to expose the limits set of those pods externally. It's time to expose the secure monolith pod externally and to do that will create a kubernetes service. Explore the monolith service configuration file. Things to note, one we've got a selector which is used to automatically find and expose any pause with the labels. App equals monolith, secure equals enabled. Two, now have to expose the no port here because this is how we will for external traffic from port 31 thousand to engine next on port four four three. Use the kubect1 create command to create the monolith service from them on the service configuration part. This output is telling us you're using a port to expose the service. You're going to have port collisions if another app tries to bind to that port on one of your servers. Kubernetes handles this port assignment for us normally. Here, I chose one so that it's easier to configure health checks later on. Use the G Cloud compute firewall rules command to allow traffic to the mildest service on the exposed no port. Now that everything is set up. We should be able to hit the secure monolith servers from outside cluster, and what value is in port 40. First let's get an IP address for one of our nodes. And then try hitting the secure Monolith service using Curl. That's not working. What's going wrong? On that note I'm going to stop here and see if Carter can help us troubleshoot. Currently the monolith service does not have any end points. One way to troubleshoot an issue like this is to use a kubecti get pods command with a label [INAUDIBLE]. We can see we have a couple of pods running with a monolith label. But what about app=monolith, and secure=enabled. Notice that this label query does not print any results. But what about our secure monolith pods? Let's see what labels they have. It seems like we need to add the secure=enabled label to them. We can use the kubecti label command to add the missing secure= enable label to secure monolith pod. Let's describe the secure-monolith pod to see if it worked.. And now that it's correctly labeled, let's see the list of imports on the monolith- service. And we have one, let's hit one of the nodes again. Bam! Houston we have contact. That was a whirlwind tour of Kubernetes. Like Docker, Kubernetes makes it really easy to run and introspect our applications. That's what I call automation. This lesson shows you many of the deployment primitives Kubernetes offers Dev's. At this point, you should be able to see the power of Kubernetes" higher level abstractions which allow you to focus on your application more so than managing an employment. But what if I told you these were just the basics and the next lesson we're going to take off the training wheels and learn to not only scale your application automatically. But how to perform rolling updates without downtime. How to work with sensitive information and so much more. See you in the next lesson.