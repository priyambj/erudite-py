Hi there. Welcome to our course on deploying a Hadoop cluster. I'm Matt Leonard, I'll be your instructor. These days companies collect massive amounts of data, hundreds and thousands of terabytes. Analyzing terabytes of data on one computer is practically impossible. One simple analysis job could take hours. A different approach is to spread the data over a cluster of computers and analyze it in parallel with Hadoop and Map Reduce. In this first lesson, you'll deploy a simple hadoop cluster. You wouldn't actually use this cluster in a real production setting, but it will help you understand the Hadoop architecture. In later lessons, you'll use automated methods to deploy larger clusters, like you'd use in production. To complete this course, you'll need a basic understanding of Hadoop and HDFS, and be able to write MapReduce programs. If you don't know about these, take our excellent Intro to Hadoop course, then return here. Also you'll need to use Shell commands such as SSH and no terminal editor such as vim or nano. You can learn about these in our command line course. Okay, let's get started. I am assuming you already know about Hadoop a bit. So, here's just a brief summary. Your basic Hadoop cluster consists of a name node. A secondary name node, resource manager and one or more data nodes. I'll go through what each of these does in a bit. The type of node is determined by the Hadoop daemon running on the machine. These dudes here. Daemons are just programs that run in the background. They handle all sorts of fun stuff like, communication across a cluster. They track jobs and resources and run MapReduce code. When they load data under the cluster, the files are split the blocks, then typically replicated across the data nodes. So if a data node dies, your data is still around somewhere. MapReduce and other tools, run in parallel on the data nodes. This increases the effective computational speed. The NameNode stores metadata about the blocks. It stores it in a file called .fsimage. The NameNode stores metadata about the blocks in a file called .fsimage. Metadata is data about the data. In this case, which block is stored on which data node. It's like an address book for the blocks. The secondary name node logs edits to the file system. The secondary name node logs changes to theFS image, which are then used to update the file on the name node. It's actually a fairly, poorly named though. The finger name node stores only edits, not the actual FS image file. So the name node dyes, the block Metadata is all gone. The resource manager is called the YARN for yet another resource negotiator. It allocates resources like C.P.U. and memory to applications running on the cluster. You run all the daemons on one machine. It's called pseudo-distributed mode. This is how I set up my computer when I'm working on MapReduce programs. In this lesson, you're going to build a cluster with one machine running the name node, the secondary name node, and Yarn. The rest of the cluster will be three data nodes. Let's get started deploying your cluster. Hadoop runs on Java so you'll need to install it now. For this, you'll use OpenJDK, an open source version of Java. I'm going to type in a bunch of commands now, but I'll make them available to you so you can copy and paste instead of trying to memorize them. The first thing to do is update and upgrade Linux packages. It's just good practice in general. This can take a bit, so, it's a good time to make a cup of tea. Once that's done, install OpenJDK. What you'll do is download a hadoop release, then extract it and move it to user local. First download the hadoop archive with wget. It'll probably take a bit to download, but your tea should be at nice drinking temperature by now, so take some time to enjoy it. Here, 2.7.2 is the version of the release that I downloaded when I recorded this. However, you're probably watching this from the future. It's likely best to use the most recent version in your time, so you should download that one instead. You also want to download it from a mirror close to you. I'll give you a link to a list of mirrors where you can find one. With that downloaded, you can now extract it to user local. It puts everything in a directory with a version number. Let's get rid of the number. Rename the directory to just hadoop. [BLANK_AUDIO] Now that Java and Hadoop are installed, you'll need to set a few environment variables on the instance. These will help applications locate Java and Hadoop. First thing you'll need to do is note where open JDK is installed. To find it you can use readlink like so. This gives the path to the Java executable. But here we want the path to the directory with jre which is the Java runtime environment. As you can see, mine was installed at usr/lib/jvm/java-7-openjdk-amd64. But yours might be different. Probably this directory you're going to use it next. You're going to set the environment variables in .bashrc. So open the file in your favorite editor. First you're going to have a variable that points to the Java installation. You're also going to add the bin directory to the path. You also have a variable that points to hadoop. Finally add a variable that acts as a shortcut to the hadoop configuration directory. This will come in handy later. With these set, save the file and exit. Finally load these variables with source and check to make sure those things are set correctly. Now that you have all this installed your instance is ready to be used as a node in the hoodoo cluster. You can easily save it and launch multiple copies by creating an amazon machine image. Usually just called image. When you use this image to launch an instance it will already have all the software installed. >From the dashboard go to the instances list. Select the instance, then actions, image, create image. Give it a name something like a dupe node then save it. Click AMI in the sidebar. After a few minutes you should see the image raster here. If you come back to start up another cluster you can launch this image and you'll have nodes with Java hoodoo already installed. [BLANK_AUDIO] When you're deploying Hadoop clusters, you need to provision enough storage to hold your data, as well as the output of your analysis. Suppose you're using instances with 300 GB of storage. If each block is replicated 3 times, calculate the smallest number of data nodes you need to store a 1 TB dataset. Since your replicating the data three times you need at least 3 terabytes of total storage. That means ten machines to hold the data. You also need extra storage for the output so you'll want to add another instance for a total of 11. If you're working with hardware instead of cloud services you'll also want to consider future data storage. Finally, you're ready to launch your cluster. On the name node, format the file system. Then star hdfs. Since the name known as connecting to the data node for the first time, it might ask you if you want to continue connecting. Say yes. Sometimes it might hang without another prompt. Just type in yes again, and get going. You can check that your node are running by going to the named nodes web interface in a browser. You can get there with the name node public host name on port five zero zero seven zero. Okay, this is what I see. And here, we see we have three live nodes. These are the data nodes. Now Start yarn. And finally, start the jobbers. And finally, start the job history server. Now you can see the daemons running. On the name node enter JPS. Here you see the JobHistoryServer SecondaryNameNode NameNode and a ResourceManager. These are all daemone that are running on the name node computer. And on the data node, you see a NodeManager and the DataNode daemone. Now that you have a cluster running it's time to test it out. You need to make sure everything is running as it should. To test the cluster's processing capability you can run terasort. Typically you test your cluster on a terabyte or more data but since you didn't provision that much storage you'll do less. You'll have to type in some long commands here. So like before I'll give you the command so you can copy and paste. First create a home folder for any data. Next you'll need to create some random data for the terasort example. Teragen writes rows of random data to a file. Here are our random data. Each row is 100 bytes long and we have 500,000 rows. So 50 megabytes in total. Remember this is replicated across the cluster so it takes up 150 megabytes across the three data nodes. Now run terasort on the data you generated which will sort the data and write it to a file sorted data If the cluster was configured properly everything should have run without feeling. Next, we can measure the input and output IO speeds with test DFSIO. This will write in removable files to and from H.T. F.S and record the average IO speeds for these tasks. Then to read files from disc. The results are written to test DFSIO results dot log. If you want to view them later. You're almost done with the lesson. By now you should have a small ado cluster ready to crunch some data. Next, I'll show you how to use an automation tool to deploy a larger cluster. Before you move on, I provided a dataset for you to analyze with your new cluster. See you in the next lesson. Welcome back. In the last lesson you deployed a small Hadoop cluster. I'm sure you can imagine that setting up a larger cluster the same way would be too tedious. Just ten more, just ten more, just ten more. Luckily for all of us, there is software that automates deployment. Apache Ambari makes it much easier to provision, monitor, and maintain Hadoop clusters. They'll also install common applications built on HDFS, such as Hive and Spark. We'll use it to create a larger cluster, like the one you'd have at a real company. [BLANK_AUDIO] Again, the cluster will be deployed on Amazon EC2 instances since you don't have server racks. However, Ambari can also be used with common hardware. The process is pretty straightforward, I'll lead you through it. It's time to launch the rest of your cluster from the Ambari node image. Launch six instances, these should also m3.large instances and given 30 gigabytes of storage each. Call it something like Ambari node, and otherwise, set all the options like before. Once the instances are launched, you'll want to configure the security rules. You'll need the internal host names, the private DNS of each of your instances, so copy these into a file. Mine looks like this. Those names also show the IP addresses of each instance, these numbers here. This one is 172.31.15.28. You need allow for communication between the nodes by opening all the ports on the subnet containing the instances. Look at the IP addresses. Note the network number, the first set of numbers in the IP address. Here it's 172, but for you it might be different. Check out the security group for the nodes, then edit the inbound rules. Create a new custom TCP rule. Set the Port to a range from 0 to 65535. And the Source to x.0.0.0/8, where x is your network number. In my case it's 172, but again, for you it's probably different. This allows the nodes to connect to other hosts on the subnet with IP addresses starting with x.172. Be sure this rule is added to the Ambari server as well. If you don't know about subnets and want to learn more, check out our networking course. I'll give you the link. Now, you're going to set up your cluster. Open a browser, go to the public hosting of the server on port 8080. You can see mine here. If you can't connect check the security rules for the server and make sure the port eighty eighty is open. Log on with username admin and it's pretty clear what you should do here. Launch install wizard, name your cluster. Next, select the stack, this determines the software versions installed on the cluster. Choose the newest one HDP2.3. This stands for hurting works data platform. Here, you list the host names of the computers in the cluster or the internal host names. I had a copy. Copy goes into here. Upload your SSH private key and change the user account to Ubuntu. Okay, hit register and confirm, everything should install register just fine. You should see a big list of services that you can install on the cluster. For now, just do HDFS, Yarn, Zookeeper and Umbari Metrics. Now, you can choose the hosts that the various services run on. Umbari has reasonable suggestions, but you can see here that there is one service per computer. Instead let's move the metrics collector to this computer, so it runs with the zookeeper server. So, this is 172.31 15 30, Metrics collector, so let's put it there, then hit next. Now, you need to assign data nodes and clients. Node manager here worked with Yarn to keep track of the data node resources. Client, installs all these clients on machines. Since these hosts are running master components, make these three data nodes with node managers. And next, you shouldn't need to customize any services right now. But look around to see which options you can change okay, and ignore this. Okay, finally you can deploy it. This might take a few minutes, so take a break by visiting your favorite website. All right, and there we go. Everything is running. Right now you can't reach the name nodes web interface on port 50070 like you normally would. Try it yourself. This is what I see. You can find which instance is running as the name node through the Ambari dashboard. What do you think is wrong and how would you fix it? The security rules for the instance don't allow connections on port 50070. To fix this, you should add a new custom TCP rule opening port 50070. If you collect more data you're going to need more storage and processing power. Luckily, you can simply add more Nodes. It's super easy to do it on Ambari, I'll show you how. Launch a new Node instance. Be sure to set the security rules correctly with all the ports open on the subnet. Copy the internal host name when it is up and running because you'll need that. Go to the host page from the Ambari dashboard, select Actions, then Add New Hosts. So, add the internal host name of the new instance here. And configure the rest the same as before. Here install the instance as a new Data Node with the Node manager. And all this the same and deploy it. There we go, three Data Nodes I basically do cluster has an unfortunate single fail point the NameNode. The NameNode machine crashed or became unreachable the metadata about the blocks we lost. But if you have a standby NameNode that a synchronize with the active NameNode the standby can take over. This is called High Availability mode and bar provides a wizard to lead you through enabling high availability, all right. Now ,it is time to configure high availability. Go to Services > HDFS > Summary. Click on the Service Actions menu then choose Enable NameNode HA. Name is guy anything you want. Those are in my cluster. So here, you can choose which host you want to be running, the JournalNode, NameNode and whatnot. I have this set up as a DataNode, and we've these guys are only running one service each, so I put them here. So now we have three hosts not remaster services that mean those are the DataNodes. Now the wizard will lead you through the process. It's pretty simple, but a few times, it'll prompt you to log into the NameNodes with their internal host names. However, you'll need to use the public host names, since you are in the network with your EC2 instances. All right? Now we see standby NameNode and an active NameNode. This cluster is now running in the high availability mode. If this NameNode dies, this one can take over. Congratulations. If you followed along with me, you should have a cluster of eight nodes including the Ambari server. You know to expand your cluster as well as configure it for high availability of the name node. For many situations, you won't need a cluster running all the time, and Idol cluster is a waste of money and computation time. A solution is to use an on demand cluster such as Amazon Elastic MapReduce. In the next lesson, you'll learn how to use Elastic MapReduce to easily create a Hadoop cluster. Welcome back! In the previous lessons, you deploy your own Hadoop clusters. A long running cluster is great when you have many jobs and it's in constant use. However, for many cases, you won't need a full cluster all the time. It will just be sitting there idle. Amazon EC2 instances are charged hourly, so if your cluster is idle, you're wasting money. A better option is to deploy a cluster only when you have jobs to run. When the the tasks are done, you can shut down the cluster until the next time you have a job. Maintaining the cluster also takes time away from other projects. For a small organization there might not be enough people to spend time worrying about a cluster. To solve these problems, you can use an on-demand Hadoop cluster. Let's look at some options. There are many services out there that let you create clusters on demand in minutes. This allows you to pay for a cluster only when you need the resources. Amazon's offering is called Elastic MapReduce, or EMR for short. It works well with other AWS services such as S3 and automatically installed applications such as Hive, Presto, and Spark. Google has a service too. Cloud Data Proc allows you to analyze data with MapReduce, Hive, Spark, and others. Microsoft also has a Hadoop cloud service, HDInsight. Which one you choose depends on your budget and how well it works with other cloud services you're using. For this lesson you'll be using Amazon's EMR. Log in to your AWS account and go to the EMR service. This is what I see all my past clusters. Can create a new one here. Here you see a quick set up for clusters. There are two different modes you can use. Either you can launch a long running cluster or do step execution. Which runs all the jobs you add then terminates the cluster. Here you can see various applications you can install in the cluster. And this is where you select the number of nodes and what type of instances you'll use. Before you get started launching a cluster though let's consider how we're going to get data into and out of our cluster. Since we won't be storing data on this thing over time. Typically transferring data into and out of EC2 instances costs money. But Amazon has a service called S3 for storing data. Transfers between S3 and EC2 are free. So it makes a lot of sense to keep your data on S3 then transfer it to an EMR cluster for analysis. Okay let's get started with creating an S3 bucket. Go to S3 manager AWS, this is what you should see. Click on Create Bucket, there. And give it a unique name. This can only contain lowercase letters, hyphens, and periods separating words. Make sure to choose a region close to you. Okay, go in here and create a folder to hold input data which is call input. Similarly, create an output folder called output. You also need somewhere to store mapreduce and other analysis files. Create a folder for that stuff too. Okay, now that you have somewhere to store your data, it's time to launch a cluster. Thank you editor, that's great. Now on to launching a cluster. It's better to use the advanced options so you have more control over the nodes in the cluster. You can choose which applications you want to install in the cluster. Let's just stick with the Hadoop and Hive for now. You won't be doing any additional configuration so skip this part. But do read up on configuration in the future as the default settings such as the block size might not fit your needs. But do read up on the configuration in the future as the default settings such as the block size might not fit your needs. Here you can add the steps that the cluster will run when it's created. You can have the cluster terminate itself after all the steps are completed. This is a convenient way to run a bunch of analysis jobs without having a permanent cluster. For now, we'll leave this unchecked since we want to keep the cluster around for a bit. Next, you can add nodes to the cluster. The master node doesn't need much computation power. So this could be a smaller, less expensive instance. Amazon suggests you should use M1.large instance. The core nodes are used to store and process data. You need to create enough core nodes to store your input data, taking into account the replication factor. By default, replication factors three for a cluster of ten or more nodes, two for a cluster of four to nine nodes, and one for a cluster with three or fewer nodes. For now, I’ll just choose two m3.xlarge instances just the default setting. Task nodes are used solely for processing data. You won’t use any now, so leave this at 0 and continue with the setup. You should point the login directory to a folder on your s3 bucket then continue on. Finally, continue without a private key or choose one you've used previously and is still around on your computer. You can use this key to SSH into the cluster instances later. Now create the cluster. After a few minutes, it'll be ready. You can add more core and task nodes here by clicking Resize. You run jobs on the cluster by adding steps through here. This is what you'll be doing next. All right, let's add a step now. I provided some fixed server log data. What we are doing here is counting the number of hits to the main web page for each day in the logs. You can use the mapper or reducer files I provided or write your own. Upload the extracted data, the mapper, and reducer files to your S3 bucket. The mapper and reducer are written in Python for Hadoop streaming. So, for the step 2 streaming program. So here is where you link to your mapper and reducer programs. Click the folder icons to select the files on your S3 bucket. And same thing for the input, and find the output. And give this a name, something like Hits. And run it, just hit Add. Down here you can see the step running. You can also view the logs to see what's happening on the cluster and you can also see the jobs running to make sure they're not failing. If they do fail you can look at the job logs to help to bug your MapReduce code. All right, now that is done running, you should see the output data on your S3 bucket. The process is similar for high events spark jobs. You put your analysis code input data on this three, then link to them when adding the step. And don't forget to terminate the cluster. If you leave it running, it can cost you quite a bit. Congratulations on completing this course. At this point, you've deployed Hadoop clusters manually, automatically with and on demand with Amazon's Elastic MapReduce. >From here, I suggest looking into the vast configuration options. Things like adjusting the block size, the number of mappers and reducers, and compressing the mapper output can make your jobs run much more efficiently. I'll give you some links to information on these configurations. Now you're ready to tackle big data projects.