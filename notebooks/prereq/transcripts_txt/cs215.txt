Welcome to our extra challenged problems. This is the first in will be a set of problems designed to test, refresh, and reinforce the various skills we’ve learned in the class. I recommend that if you’re still working through the class then you finish that first and then come back to this challenged problems later. Before we get to the first problem, I want to tell a quick story. I used to live in Chicago and I wanted to bike for my apartment in Lakeview down to work in the Loop, so I did what anybody would do and I looked for directions online and the suggested route took me along the Lake Shore bike path and basically cut across to work. This seems like a great route, it’s along the bike path, along the lake, which is a very pretty ride, unfortunately it’s also a very popular place for people to walk, for people to run, and for people to take the dogs for a walk and it’s actually a very hard place to bike. The suggested route along the Lake Shore might have been based on some factors such as distance or how hilly the route was and whether you want a bike path or a bike lane and there is probably some other ones but they don’t take dogs into account. One possible solution is to find two paths and then let the user choose between the two options. If you’re curious, I ended up taking a route down Clark Street and Wells Street to work. Now with that idea, in this assignment, we want you to take a graph and a starting node and calculate the top two paths every node – every other node in that graph. In addition to passing the greater you should try to write your algorithm as efficiently as possible. There is a link to the form where you can discuss your approach and your calculated run time. If you post code, please explain what you are doing at each step and why? Hi, and welcome to CS215 Social Network Analysis. My goal in this class is to give you an introduction to the analysis and design of efficient algorithms. We're going to focus on algorithms that will be useful to you if you're trying to analyze large social networks. I'm going to assume for this class that you have a familiarity with programming in Python and that you're comfortable with mathematical notation about at the level of high school Algebra 2. Unit 1 is going to be a bit of a warmup. I want you to think about how you design computer programs to solve particular problems. What we're going to focus on is how we can take advantage of mathematical properties of the problem that we're solving to speed things up. I'm also going to introduce some social network terminology, so that we'll be ready to dive into this topic a little bit more deeply in the later units. Hi everybody! Welcome to CS 215. My name is Micheal Littman, and I'm going to be your instructor. Let's get off with a little magic trick. All right! Here is a collection of actors from movies. This is Bruce Willis, Danny Devito, Cloris Leachman, Stanley Tucci, Tom Hanks, Haley Joel Osment, and Robin Williams. I'm going to connect together actors who were in movies together with lines that represents those movies. So Bruce Willis and Tom Hanks were in a movie together called, "The Bonfire of the Vanities" that seemed like it was going to be very good but it was not. Bruce Willis and Cloris Leachman were in a movie together called "Beavis and Butt-Head Do America"--it was not a very popular movie. Tom Hanks and Stanley Tucci were in a movie called "The Terminal." Haley Joel Osment and Bruce Willis were in "The Sixth Sense." They were the stars of that movie. I will not tell you the ending, but Haley Joel Osment was also in "Forrest Gump" with Tom Hanks. Robin Williams and Stanley Tucci were in a movie called "Deconstructing Harry." Cloris Leachman and Stanley Tucci were in a movie called "Gambit". Danny Devito and Robin Williams were in "Death to Smoochy." All right! Next I'm going to connect Bruce Willis and Danny Devito, who were in a movie together called "I'm Still Here." And lastly, I'm going to connect Bruce Willis and Stanley Tucci, who were in a movie called "Billy Bathgate." All right! Now that we have the diagram, I'm going to start off with one of the actors. Let's say Bruce Willis and I'm going to move around on this structure visiting all the movies exactly once and so let's see how that goes. So there's lot of different choices to start off. Let's say, I'm going to go from Bruce Willis to Tom Hanks, Tom Hanks to Stanley Tucci, Stanley Tucci back to Bruce Willis, Bruce Willis to Cloris Leachman to Stanley Tucci again, to Robin Williams to Danny Devito to Bruce Willis to Haley Joel Osment, and then to Tom Hanks, and now you can see that I've visited all of the movies exactly once and I ended up at Tom Hanks. All right! So let's try this again with a different set of actors. Now we've got Susan Sarandon, Dustin Hoffman, Julia Roberts, Kevin Bacon, Robert De Niro, Anne Hathaway, and Meryl Streep. And then we're going to put in some of the movies again. Robert De Niro and Dustin Hoffman who were in a movie together called "Wag the Dog." Robert De Niro and Meryl Streep were in a movie together called "Marvin's Room." Dustin Hoffman and Susan Sarandon were in a movie together called "Moonlight Mile." Dustin Hoffman and Julia Roberts were in a movie together called "Hook." Dustin Hoffman and Kevin Bacon were in a movie together called "Sleepers." Susan Sarandon and Julia Roberts were in a movie together called "Stepmom." Kevin Bacon and Julia Roberts were in "Flatliners." Nobody can tell from some of the movies and the actors that I've picked that I watched more movies in the 80s than I watch now. Meryl Streep and Kevin Bacon were in a movie together called "The River Wild." Meryl Streep and Anne Hathaway were in "The Devil Wears Prada" together. And Anne Hathaway and Julia Roberts were in "Valentines Day." We're going to do this as a little bit of a quiz so I'm going to mark each of these edges as something that you need to visit. You need to visit all of them. I strongly suggest that you actually check them off as you go because it's really easy to do one twice or forget one by accident. So this is what I want you to do, I want you to go through and start at Meryl Streep and move along the green lines here at the movies and each time you visit a movie, check it off as you go. So the right answer in this quiz is to have all the edges checked off and none of them checked off twice. It's really easy to mess that up, so be very careful. And then when you're done, I want you to mark the little bubble corresponding to which actor you end up with at the end of your path. You can follow any path that you want--you just have to make sure that you visit each of the movies once. All right! I'd like you to actually go and do this and check off the actor that you end up with. If you did this right, it should be the case that all of these edge boxes are checked off, and you ended up with an actor. Now--this is the magic trick part--I'm going to tell you which actor you ended up with. The answer is Kevin Bacon. Ta da! Pretty neat, huh? Let's try it. I'm going to follow maybe a different path than you did, but I'm going to do my best to try it. Just to try to thwart this, maybe I'll do directly to Kevin Bacon. I'll visit this edge. Now I'm at Kevin Bacon. I go to Dustin Hoffman. I go to Susan Saradon. I go to Julia Roberts. I can't go do Kevin Bacon now, because I'll be at a dead end. I visited all the other edges that have gone through Kevin Bacon, so I'll go to Dustin Hoffam to Robert Deniro to Meryle Streep again to Ann Hathaway back to Julia Roberts, and now I can go to Kevin Bacon and be done. It may or may not have been the same path that you followed, but I did end up at Kevin Bacon. You can try this a couple different ways, and you'll see that you'll always end up with Kevin Bacon. So that's our magic trick. This example illustrates two things that are central to this course. The first is the idea of a social network. We often think of social networks as being particular websites that exist these days, but really what a social network is is connections between individuals that capture relationships between them. These days, of course, they ubiquity and scope of social network data is exploding. Clever computer software is needed to reveal interesting patterns and answer important questions about this data. It's a really neat area. The second focus of this course is on the magic of algorithms. Algorithms are just how we organize computations to solve a particular problem. Some algorithms are really straightforward, but other algorithms take advantage of subtle mathematical properties to quickly and accurately provide an answer. Those are the ones that feel like magic. With that said, now I'm going to try to tell you how this trick actually worked and what it tells us about these kinds of structures. Let me first point out that this kind of structure where you've got a set of circles and lines connecting between them is what we in computer science and discrete math call graph. In this particular graph, the actors are playing the role of nodes, sometimes called vertices. And the movies are playing the role of edges or links. I'll probably mostly say "nodes" and "edges," which kind of mixes a metaphor, but that's how I'm used to saying it. We can also talk about the degree of a node in a network. For example, this Dustin Hoffman node here is a node in the network. It's got degree 4 because there are 4 edges coming out of it--1, 2, 3, 4. In the graph, we can talk about something called an Eulerian path. This is the path that starts off with some node on the graph, let's say A in this case. And it moves along the edges from node to node, hitting every edge exactly once, and then ending it from node to the graph. In this case, we have an Eulerian path that's started at A and ended with D. Euler by the way was a famous mathematician, Leonhard Euler. He made a number of contributions to discrete math and ultimately, computer science. Here's something that we can notice about a graph. If it has a Eulerian path, notice what has to happen to all the nodes along the path. Here's what we did in this example, we went from A to B, to D, to A, to C, to D. Now let's consider the degree of all the nodes in the graph. A has a degree of 3, B has a degree of 2, D has a degree of 3, and C has a degree of 2. Notice that these nodes B and C, which were not either at the beginning or ending of the path, the Eulerian path, have an even degree and that has to be. What happens if all the nodes that are not in the beginning and ending nodes have to had even degree because the path has to go in one edge and out another edge. Every time that node gets passed through, 2 of the edges are visited, and none of them are visited twice and none of them are skipped. So it has to be that all these nodes that are intermediate on the path have even degree. The beginning and ending nodes are different though. If the path starts off at A and leaves A along one of the edges and that consumes one of those edges. The beginning and ending nodes, A and D in this case, have to have odd degree because, in addition to any times that the path moves through them and comes up the other side, it either leaves in the beginning or enters at the end without a corresponding other side. Those have to be odd, all the other ones have to be even. It turns out that this relationship is in "if and only if"--that is to say if the graph is connected and has exactly 2 odd degree nodes, then it has an Eulerian path, and if not, then it doesn't. Now, there is one exception to this. What if all of the nodes in the graph are even degree? Let's take a quick look at a special case here. Here is a graph with 5 nodes and the degree of all of the nodes is even--2, 4, 2, 4, and 2. Can a graph like this with only even degree nodes have an Eulerian path? One possible answer is no, the start and the end edges of the path had to have odd degree, which is the ruling in this case, so it can't have an Eulerian path. Another possible answer is yes, but it depends on the particulars of the graph in question. Some graphs with only even degree nodes have Eulerian paths and some don't. Another possibility is that the answer is yes, all such graphs do. Any graph that's connected and has only even degree nodes will have an Eulerian path in a particular form. Another possible answer is yes, because all graphs do. You can't really have a graph that doesn't have an Eulerian path. All right. Let's look at the answer. Here's a particular graph where all the nodes are of even degree. Let's see what happens when we try to follow an Eulerian path. Let's just pick any node to start--say E--and go to C. There are some choices here. Let's follow an edge back to B. Then maybe to D to C to A to B and to E. So it looks like we were able to hit all of the edges exactly once. This graph does have an Eulerian path. But there's something very special about that path, and that it is started at E and it ended with E. Because the path actually started and ended at the same node, that node should have even degree, because each time we go into we come out, except for the first time where we go out but then the last time we come back in. Everything matches up and you end up with even degree. This is a special kind of Eulerian path called an "Eulerian Tour." "To tour" in the sense that we start off in our home city, and we go around, we visit lots of things, and we come back to our home city. We kind of did a tour of the graph, and it was very scenic. That's an Eulerian Tour. This first answer is definitely not correct. Such a graph can have an Eulerian path. No, it doesn't depend on the graph. It turns out that this is going to be fine no matter what, because we're going to end up starting and ending at the same node. We could have actually started and ended on any of the nodes and it would've worked out the same way. All such graphs do. All graphs that have only even degree nodes do have Eulerian paths, specifically Eulerian Tours, but it's not the case that all such graphs do. Let's make a quick example graph just so that you can see it. For this to work we need to have a graph where more than two of the nodes has odd degree. Let's make one where four of the nodes has odd degree. We have these four nodes--F, G, H, and I. Let's make it so that each of them has odd degree. Now they all have even degree. Now two of them have odd degree. If we connect these two guys, then they have odd degree as well. All the nodes have odd degree. It's not just 0 or 2. All of them have degree of 3. Let's see what happens when we try to make an Eulerian path. We'll pick some node like I. We'll go I to G, G to F, F to I, I to H, H to F, and now we hit a dead end. Maybe we didn't want to do that. Let's instead go H to G where we also hit a dead end. In fact, you can do this all day long, and what you're going to discover is there's always going to be at least one edge that you just can't visit, because again, each node in this case has an odd degree. Every part of the path has to come into the node and out of it, so it has to have even degree except for the endpoint. No matter what we're going to be stuck. Now, what I've shown you so far is not really an algorithm, but it has some elements in common with them. Algorithms are really cool. That's what the focus of this class is about. One of the reasons algorithms are cool is that they're really useful. Without careful algorithm design we just don't see fast enough responses from things like websites and user interfaces--stuff that we really depend on to be able to have the computers react quickly. Another is that they're really clever. They're really pretty, mathematically. Just like a magic trick, sometimes learning how it does what it does can be just as exciting as what it actually does accomplishes. The practical part of algorithm design is trying to figure out how to make your programs fly-- that is to say, go really, really fast. There are a couple different ways that you can do this when you're programming. One, you should take a great deal of care in organizing your programs so that they're not doing a lot of wasteful stuff. That goes without saying, but I said it anyway. There's a lot of time that you can spend tweaking loops and other things in your program to just get rid of little bits and pieces of inefficiency, and that's important. But perhaps the most important thing is good algorithm design. Whenever your program is doing something that involves a great deal of computation you need to think hard about how to organize that computation so it does what you want it to do, but it does it fast. That's the focus of this course. We can think of the problem of developing algorithms for particular problems as a kind of algorithm itself. Here I've written it as kind of faux Python. You can't actually run this. I would not suggest running this, but it should give you a sense of the flow here. What we start off with is some kind of problem specification. We'll talk about a couple different examples of those over the course of the course. For the problems specification we're currently concerned with, what we're going to do is we're going to start off devising an algorithm for that problem. That mean thinking about it, coming up with some kind of strategy or plan for doing the computation. We'll call that our algorithm. They're not really done yet. We need to make sure that this algorithm is actually correct. The first thing that we want to do when we propose an algorithm is actually analyze the correctness with respect to the problem specification to see if it actually accomplishes what the problem says you're supposed to accomplish. Sometimes that is kind of obvious and straightforward. Sometimes that involves a fair amount of mathematical analysis. We'll see different examples of those as we go. Once we've analyzed our algorithm to make sure that it's correct, we can analyze it's efficiency. It does what it's supposed to do, but does it to it fast enough? We eventually determine the running time of the algorithm. If it's not correct or if it's not fast enough, then we need to continue this process, redevise an algorithm, and reanalyze it, and keep doing this until we have something that both solves the problem and solves it fast enough. That's the algorithm that we declare to be our solution. Now, a lot of computer science students bristle a little bit when they take an algorithms class, because of the heavy emphasis on mathematics. A lot of us got involved in computer science because we really like doing stuff and not necessarily doing math. But nevertheless, I think there's a very strong case to be made for the importance of mathematics in computer science and in algorithm design in particular. I'd argue that there are three natural ways that theory stuff or math can really help. One is to just get you thinking clearly about what it is that you're trying to accomplish. It's very easy when you're in the depths of writing code to lose track of what it is that you want the code to actually do. Just thinking formally about what you're doing is something that using your mathematical background can help with. Another thing that it can be very helpful with is analyzing the efficiency of what you've produced. You can actually know where there are spots where you could be doing a better job and have the code be running more effectively and more efficiently without producing incorrect code. Just taking a moment to think a little mathematically can be a huge win and save you tremendous amounts of time. It sounds very important, right? Now, this notion of efficiency is actually very important to think about. What is it that you want your program to do efficiently. Do you want it to be fast in terms of time? Do you want it to be efficient in terms of the amount of memory on the computer that it uses, so it does its work with as little memory as possible? These days more and more people are worrying about energy usage. Are there ways of organizing your computation so that it is efficient in terms of the amount of power that it uses. The tools that we develop in this course are going to be useful across the board here, but we're going to mainly focus on issues of time. To get you thinking about algorithms and how they work and what makes them correct and how to make them more efficient, let's go through an example together. Here is a little bit of Python code that I wrote. It's a routine called naïve, because I'm not telling you yet what it actually is doing. What it takes in as input are two integer-valued variables that are none negative, and then it does some assignments and recalculations and a while loop. It runs for a little bit and then it returns z. What I'd like you to do is take a look at this. It's not very long. It's probably not completely obvious to you right away what it is that it's doing. I encourage you to run this in Python. Give it some example inputs and outputs. See if you can come up with a pattern for what it is that it's doing, and then convince yourself why. Once you have a hypothesis as to what it's doing, see if you can figure out how you can convince yourself that this program actually is computing what you think it's computing. Then I want you to fill out the following quiz. You can take this code, and you can run it for any particular value of a and b, but I want you to think about what it does in general as a function of a and b. When you run naive(a, b) does it return whichever of a or b is the larger one? Does it calculate a - b, or for that matter, does it calculate b - a? Does it calculate the sum of a and b? Does it calculate the product of a and b? Okay, now for the answer. Let's switch back to the code for a moment. We'll even put in an example just to see what it actually does. You should have already done this, but just in case you haven't let's run this and see what it does. We put in 4 and 5. It came out with 20. That actually should be enough for you to make out which of the answers that I gave you is correct. It's clearly computing the product of these two, at least in this example. But why in general is that what it's doing? Let's actually take a look at the code and see what happens. We start off with a and b, and we leave them alone, but we put copies of them into x and y. We have a z variable that we initialize to 0. Then what we're going to do is we're going to keep detrimenting x, keep subtracting 1 from x until it hits 0. Each time we subtract 1 from x we add y into z. We're starting off with z as 0, and we add y to it--how many times? X times. Z at the end of this is x added to itself x times--x y. It is, in fact, computing the product of these. Now, that was kind of an informal argument. Can we be a little bit more formal and mathematical to show why it is that what this is computing is the product of a and b? Let's go through and actually do a proof of the correctness of the naive algorithm we just spoke about. We're going to proceed by taking advantage of a particular observation. What we're trying to prove here is the correctness of the claim that naive(a,b) outputs the product of a and b. The observation that I'm going to make is that before or after the "while" loop in the implementation of naive, this statement is always true. The variables x and y multiply together and added to z is exactly equal to a times b. How are we going to show that this is the case? The first time we're going through the "while" loop in the very beginning and the top of the function, x is assigned to a, y is assigned to b, and z is assigned to 0. Let's check the expression with those variables plugged in. We're saying that's ab equals ab+0. Well, that's kind of obvious. Well, the next thing we're going to show is that if it's the case that at the beginning of the "while" loop, this condition holds that a times b is exactly equal to x times y plus z, then it's going to be the case that with the new values, so x, y, and z may change. The new values x prime, y prime, and z prime are going to satisfy this as well. If it was true before, then it has to be true after. Why would this be true? Let's remind ourselves what the code looks like again. What happens in each integration of the loop is that a new value of z is computed, which is the old value plus the value of y, and a new value of x is computed, which is the old value minus 1. We basically had the new value of x if the old value minus 1, the new value of y not been changed, and the new value of z is the old value of z plus y. All right! What can we say about x prime times y prime plus z prime? We know what x prime, y prime, and z prime are, so let's substitute those in. That's x minus 1 times y plus z plus y. Now we just do a little bit of algebra. Multiplying this out, we get xy minus y plus z plus y. and these y's, +y and -y cancel and so we get xy plus z. But, notice what we assumed--we say if it was the case the xy plus z is equal to ab, then what we're showing is that x prime, y prime plus z prime is equal to ab. Well guess what? We showed that x prime times y prime plus z prime does indeed equal ab if it was true at the top of the loop--so this condition that we're testing here this ab equals xy plus z is maintained through each step of the "while" loop. It starts out true but it remains true each time it goes through the "while" loop. What we know is that while this code is running each time we go through the "while" loop, this condition is always true and eventually, the "while" loop terminates. The "while" loop terminates when x equals 0, so what does that mean? The xy plus z has to equal ab, but x is 0, so that 0 times y plus z equals to ab. This has to be true. Well, 0 times y is 0, so this is actually saying that z has to be equal to ab at the end of the loop. Once x reaches 0, z has to equal ab. Keep in mind that's exactly what we return--the value of z. The thing that is returned is going to be a times b. Let's just take a look at the code again to see that that's what it's doing. Each time it goes through, it's decrementing x accumulating the values in z, and eventually when x is exhausted, it returns z and it is exactly a times b. What I'd like you to think about is how long does it take for naïve(a, b) to execute as we look at larger and larger inputs. These should be really fast. This should still be pretty fast. As we give it larger and larger powers of 2, you should notice that it's going to take longer and longer for this multiplication to actually execute. Now, I could keep doing this all day, which probably wouldn't be fun for any of us. What I'll do instead is have the computer actually do some of these calculations for me. I'll measure the time and then we'll plot it and see how it turns out. Here's our naïve algorithm again. There's a whole lot of Python code wrapped around it that's going to help us do some plotting What we're going to do is we're going to run naïve(n, n) with different values of n. The n's are going to be all the powers of 2 from 2^0 to 2^23. For each of those what we're going to do--and the details of this aren't important-- we're going to time how long it takes to do that, gather all those times together, and then plot them. Here we go. We run this. It generated a plot, and this is what that plot looks like. It's a little bit crufty to read at the bottom, but you can get a sense of the general shape of it. Across this access is the number that we're squaring. We're sending naive of this number, and it goes up to billions. This is the time in seconds that it takes for it to execute. You can see that it actually follows a very recognizable pattern. Our plot looks something like this. How does the running time t relate to the input that we're giving naive n? Is the running time roughly constant-- as n gets bigger the time stays about the same? Is it roughly logarithmic--as n gets bigger the time grows like the log of n? Is it roughly linear--as n gets bigger the time grows like cn for some constant c? Or is it roughly exponential where the time grows like c^n for some value of c? In this case--this wasn't intended to be a super hard question-- as n grows there seems to be a linear relationship. Doubling n seems to double the time. That was the answer that I was looking for. Of course, it makes a lot of sense when you think about what naive is actually doing. Let's switch back to naive for a moment and look to see what it's doing. It takes whatever value is passed in as a, and it runs a loop. The number of times that it goes through this loop is exactly equal to a. If you double a, it's going to double the number of times it goes through this loop and double the overall time. What I'd like to do next is describe a different algorithm that I learned under the name "Russian Peasant's Algorithm." If you'd like to learn more about it, you can go to the Wikipedia page called "Ancient Egyptian Multiplication." This is an algorithm that people actually implemented by hand before there were computers. Here's the Russian Peasants Algorithm in Python. You can see it follows pretty closely along with the naive algorithm that I described before. It starts off assigning a and b to x and y. It starts off z as an accumulator set to 0. It's going to repeat while x is bigger than 0 a bunch of steps that involve accumulating values in z and making x smaller. What makes this algorithm work really well is when doubling and halving numbers is really easy to do. If you're not familiar with this syntax, this is saying take whatever the binary representation of y is and shift it over 1 to the left. This is saying take whatever the binary representation of x is and shift it over to the right. Let's do a little quiz to see if you know what that would do. A straightforward question--what is 17 1 in Python? 171, 9, 8, 8.5, or 34? All right, this is very easy to check. We can just run it in the IDE and see what comes out. The answer is 8, but why is the answer 8? Let's take a look at what 17 looks like in binary, in base 2. It has the 1 position set--1, 2, 4, 8, 16. It's a bit pattern that looks like this--10001. What this operator does is it shifts the whole bit pattern one position to the right, dropping the lower order bit. This shift 1 to the right becomes that, which is 8. Roughly, what this operator does is it halves a number. But if it's odd first it subtracts 1 and then halves the number. That's why we get 8 and not say 8.5. Now that we remember what these operators do--this is actually doubling y by moving it one position to the left in binary. This is halving y or rounding down and halving y for x. You may recall that this percent sign is the modulus operator. What this is saying is take x, divide it by 2 and tell me what the remainder is. If it's 1 then do this. What does it mean for a remainder to be one? It means that it's an odd number. Whenever x is odd, it adds y to z. Then no matter what it doubles y, halves x, and goes back to the loop and continues doing this until x is 0. Unless this is obvious to you, this may seem a little bit magical again. Why would this actually be equivalent to naive? Why would this actually be computing multiplication? Let's just make sure that it does first. Let's print russian(14, 11). We'll run that and see what it gives us--154. All right. That is, in fact, 14 * 11. We're going to need to step through and try to understand why it works out the way that it does. If we give russian the input (14, 11), it's going to assign these to x and y and start off the z at 0. Let's take a look at that. Now it's going to go through the while loop. On each iteration, it's going to check to see whether x is odd or even. If it's odd, then it's going to add y to z. We start off at 14, which is not odd. What does it tell us to do? It tells us to double y and halve x. We get down to 7, 22, and 0 remains 0. The next time through we see that x is odd, so we add y into z. Makes it a 22. We halve x and double y. In this case halving and rounding down. That gets us a 3 and a 44. The next time through the while loop we see that x is 3, which is odd. That tells us to add 44 into z, giving it a total of 66. Then we halve x, rounding down and double y. Go back up to the top of the while loop, we see x is odd again. We add y into z. Then we halve x, rounding down and double y. At which point x becomes 0, and it returns the value of z, which lo and behold is 154. It seems to have done the right thing. What we've really done here is we've added several values of y. As x is counting down, each of the times x becomes odd we add those together. It ends up being the sum of those three numbers. You have to admit that's kind of cool, right? It's somehow doing the equivalent of 14 * 11, but not the way we would normally do it. Why does this work? We're going to do another proof of correctness. It turns out that the same strategy that we used for naive is going to work out really well here. In particular, what holds is that the product of a and b is always equal to the product of x and y plus z. And again, since x is going to be counting down and eventually reads 0, z is going to ultimately have to hold the product of a and b. Can we prove that this is the case? Again, we need to do 2 things. We need to say that it starts off with that being the case. That's the same exact argument that we had in the naive algorithm, because x starts out as a, y starts out as b, and z starts out as 0, so that holds. Now we need to show that if this condition holds at the beginning of the top of the "while" loop, then it's going to hold at the end with the new values of x, y, and z. Let's remind ourselves how x, y, and z changed in the "while" loop, so we can see whether the condition still holds. We're going to have to break this into 2 cases. First, if x is odd, and second, if x is even because 2 slightly different things happen. So in the case when x is odd, the first thing we do is add z and y together, make that a new value of z, then we do a bit shift on x, which in this case is equivalent to subtracting 1 to make it even and smaller than having it. And y, meanwhile, gets doubled. What can we say about x prime times y prime plus z prime, so that is on the bottom of the loop. We can substitute in these values--get x minus 1 over 2 times the new value of y just 2y, plus the new value of z, which is z plus y. We noticed that this 2 and that 2 cancel, and we get xy minus y plus z plus y. Again that +y and -y cancel, then we do indeed get xy minus z, which we had assumed holds in advance, so that's a times b. What about the case where x is even? So in some ways, this case is easier because the bit shift on x just halves it. Z doesn't change at all and y again is doubled. Let's look to see what happens in this case. What happens now is the value of z doesn't change at all, and in some sense, x and y just move the 2 around, so x get half, then y's get doubled. When we multiplied those 2 together, they cancel, and again we get xy plus z, which we had assumed, coming in to this, is equal to a times b. The new values of x, y, and z in either the case where x is odd or x is even, continue to satisfy this property. This is kind of strange--what is true in here is removing the factor 2 back and forth between x and y, actually generally from x to y. When if it's odd, then we had to shift x a little bit more than that to make it balanced. We move some of the value into z. This is kind of lucky. Just to make sure that you're paying attention, let's let you work out the value for russian(20, 7). Step through the algorithm and count up the number of time that an addition happens. Once you've worked it out, type the number into the box here, and we'll let you know whether or not it's correct. Here are the series of x, y, and z value that hold in the program as russian(20, 1) executes. As you can see, there are two times when the value of y is added into z. Once when x is 5--28 and 0 get added in. Another time when x is 1--28 and 112 get added in, and we get our correct answer of 140. An interesting thing to note here is let's take a look at the sequence of x values. We have one that's even, another one that's even, one that's odd, another one that's even, one that's odd, and another one that's even. Let's think for a second about the binary representation of the number 20. It looks like this--1, 2, 4 plus 8, 16--4 plus 16 is 20. This is the binary representation of 20. You can see that these zeros line up. The 1, the 0, the 1, the 0. Each times it's odd, we do an addition. Essentially--actually exactly--each time there's a 1 in the binary representation of this first value there's going to be an addition that takes place. All right. Now we've got two different algorithms--naive and Russian-- that can both be used to multiply numbers together. Who's better? How are we going to figure that out? Depends. What matters to us? It could be that you want the program to be really easily read, in which case naive is probably easier than Russian. But it could also depend on speed. Which one gives you the answer that you want fastest? How can we find out which one is faster? For starters, we can do some plotting again. Let's do that. I wanted to show you a plot of the running time of Russian for a range of different values for squaring. Russian(n, n) for lots of different values of n. But the plot was really uninteresting. The reason was that I just couldn't for the life of me get it to take more than 2 ms to multiply two numbers together. I used numbers as big as 2^1000, which is a very, very big number, and it returns instantaneously. After numbers get much bigger than this the plotting routine gets very confused, because it doesn't know how to deal with numbers this large. Whereas for--shall we say--only 2^23 naive was taking already 3 seconds, which is a lot longer. There's a huge difference between the running times of these algorithms. One of the things we'd like to do is have a way of comparing algorithms in terms of the time it takes to run them without actually having to go and do all the hard work of running them. Instead what we're going to try to do is analyze what the running time is in more of an abstract model of computation. To do that is very difficult. To get the exact timing information without actually running it is something that we probably can't do. But we can make a series of simplifying assumptions. These simplifying assumptions will allow us to do an analysis that is more tractable. Here are some of the assumptions that we're going to make. We count a simple statement--something like x = x + 1 as something that takes 1 unit of time. We're going to count of the units of time that an algorithm takes to run. The second simplifying assumption we'll make is that the time it takes to execute a pair of statements or three statements or ten statements in sequence is the sum of the times that it takes to execute each of the individual statements. This ignores a lot of issues that go on in the architecture of the computer-- things like pipe-lining and caching--that make this actually not true in practice. But we're going to hope that it's a good enough approximation of reality. Measuring the time that it takes to run something like an if statement, we're going to imagine that there's the time that it takes to execute the condition evaluation and then the time that it takes to actually execute the statement. This would be essentially 2 units of time. It follows from these assumptions that the running time for a loop is going to be equal to the time it takes for the body of the loop to run times the number of times it's repeated-- the number of iterations. A block of statements like this where i takes on a set of values 0, 1, 2, and 3 and prints each one--we'll say that this takes 4 units of time, because this unit statement is repeated 4 times. Let's see if you're getting the hang of this. Here's a little quiz. How many units of time will this little block of Python code take to run? Fill in your numerical answer in the box. The answer is--now let's take a look. There's a statement here that is executed. This s equals 0. There's a statement here that's print s. Those get executed. That's for a total of 2. This is one statement that is executed 10 times--so 10, 11, 12 for a total of 12. All right. Let's try another little quiz, but this one is going to move our understanding forward a little bit more. Here's a little bit of Python code. A subroutine called "countdown" takes an input x. It executes a statement, and then it goes into a while loop and repeats these two statements some number of times. Then when it's all done, it does one more statement. We can take for any given input--like this print countdown 50-- we could count up the time--the number of statements executed-- for this to execute. It's going to be something like this. We didn't talk about the number of steps that it take to do a print statement if there's a subroutine, but we're going to call it one for the print statement plus however many steps it takes to execute the subroutine call. In these case, countdown 50--what is it going to do? There's going to be 1 call there, 2 for each time that it counts down, which is going to be--what--10 times, right? It starts off at 50, going to go down by 5s until it hits 0. There is going to be 10 times that it's executing these two statements. That's 20--21, 22, and the print statement is 23. This--if we ask the time that this takes--is going to be 23 units. Here's the trickier question. What if we just say we don't know what n is. Someone is going to tell us n later. We're like to know the number of steps, the amount of time that it takes to execute this formula, as a function of n. We can't automatically grade a mathematical function--or maybe we can. The way that we're going to score this quiz is instead of you telling me a mathematical expression for this function, I want you to actually give me a function that takes as input n and produces as output the number of time steps that it will take to execute countdown or this entire block of code here. We already figured out what happens when n is 50, but we want a general form of this. Let's see if we can work out this function. Looking again, countdown of n--well, whatever n is, it's going to call countdown with that variable. It's going to do some statements. It's going to repeat this how many times? As x is counting down by 5s, it's going to keep doing this until it gets to be 0 or less. How many times is that going to be? It's like whatever this x is, which is really whatever this n is, divided by 5 but rounded up, because if you have something like 6, x is going to be 6. It's going to do this loop. X is going to get decremented to 1. Come back up here--it's still greater than 0. It's going to do it again. The only way that it wouldn't have done that if it was exactly 5. Put 5 in, it executes it once. Subtract 5 away form it and end up with 0. It falls through. It doesn't repeat the loop at that point. What we really want here is to take n divided by 5 and round it up. This math.ceiling function does that. It rounds a non-integer number up to the nearest integer. That's the number of times that these two statements are going to be executed. We multiply that by 2, and then there's 3 other statements that are going to get executed-- this print statement, this print statement, and this initialization of the y variable. This formula will actually tell us the number of steps as a function of n that this countdown will do. What is it? It really is roughly n divided by 5 times 2 plus a constant. Essentially linear in n. Here's the actual formula. This example is actually maybe a little bit easier and probably a little bit more useful and interesting. What we'd like to do is count the number of steps that it takes to run naive as a function of a. Just as before, I'd like you to write your answer as a function time that we can run, give different inputs to, and evaluate it that way. Function time should take as input a, and it should return the number of steps. it takes to execute naive(a, b) as a function of a. All right. The answer in this case is actually a bit easier than in the previous example. What happens here is put in some value a and b. It turns out the running time doesn't depend on b--the number of steps doesn't depend on b-- but it does depend on a, and what happens is a gets assigned to x. Then x gets counted down, so the number of times the body of this loop is executed is exactly whatever a is, and there's two statements in the loop. It's going to be 2 times a--this statement, this statement, this statement, for a total of 3. So 2a + 3 is the function that I was looking for. The fact that the number of statements that it takes to execute naive(a, b) is a linear function of a shouldn't come as a surprise to us at this point. We actually plotted the running time, and it really did give a very nice linear function. Russian is going to be a bit trickier for a couple reasons. One, is it seems much more obscure how many times it's going to go through this loop. It seems like some of the statements are actually conditioned on properties that we don't necessarily know as a simple function of the inputs. The plot that we made was basically useless. It just seemed like the time was more or less constant. It's not actually constant, and we can actually work out a function of what it is. The key step in analyzing the number of steps that this algorithm is going to take is understanding how many times this loop is going to be executed. The number of times the loop is going to be executed is going to be the same as the number of times that it takes to divide x in half before you get down to 0. This is, again, a half and rounding down. How man times can you divide x in half rounding down before you get down to 0? An important thing you need to know is how many times can you divide a number x in half, rounding down, before it hits 0? Here are some examples. If x can be something between 0 and 11, here's how many halvings. If the number if 0, then you don't have to halve it at all to get down to 0. If it's 1, you have to halve it once. If it's 2, you have to halve it twice--once to get you down to 1 and then once more to get to 0. It looks like it's a linear function so far, but actually it diverges now. The number 3 you have to halve it once to get down to 1 and then once more. Four you have to do three times--once to get it to 2, then 1, then 0. It stays that way until you get the 8, which takes 4 times--once to get it to 4, 2, 1, 0. It stays 4 for a while and the next time it changes is at 16. Which of these functions captures the relationship between x and the number of times x needs to be halved to get down to 0. Just to simplify things a bit, let's get rid of the 0 case, because it's a bit messy. The functions are x--seems to work for a little while-- x/2, the log base 2 of x, and the log base 2 of x floor--meaning rounded down if it's not an integer--plus 1. It shouldn't be too hard to figure out the answer in this case. All you have to do is try these examples that I've got here with these functions. The one that works is this floor log base 2 of x plus 1. If you don't know what the log base 2 of x is, well, that's essentially what it is that we're learning about here. Log base 2 of x is easy to define for powers of 2. It's just what power of 2 it is. The log base 2 of x is 0 for 1, 1 for 2, 2 for 4, 3 for 8, and 4 for 16, and so on. What's happening in this case is the log base 2 of x is something less than 2 up until here. We round it down. We get these as-- Well, let's do that. Let's do the floor. That gives us 1, 2, 2, 2, 3, 3, 3, and so on. We need to add 1 to it to capture this notion. Let's go back to now using this idea to count the number of steps in Russian as a function of a. The thing that you should be noticing here is we now know the number of times this loop is going to be executed. This is the floor log base 2 of a plus 1. For each of those times, how many statements get executed? Well, there are these two statements that get executed unconditionally. Then there is plus this conditional statement--this evaluation here get executed. These three things get executed, and this additional for statement only gets executed when x is odd. As we talked about before, that happens however many times as there are 1 bits in the binary representation of a. That actually is enough to get us our answers. Let me write it down. How many steps is it going to take to execute russian(a, b)? Well, as I said, the floor log base 2 of a rounded down plus 1 is the number of times the while loop is executed. There are three statements that are going to executed inside plus the additional three statements that are executed outside plus there is going to be one statement executed for each of the bits of a that it's on and particularly the summation. That's kind of a mess. We can make it slightly less of a mess if we notice that this is upper bounded by 4 times the rounded down logarithm of a plus 7. The reason for that being that the most number of on bits you can have in a number is if all the bits are on and how many bits can you have in a number. Well, if you have a number like this--a binary number like that, each time you halve it, you're chopping off one of the bits. The rounded down log base 2 of the number plus 1 is actually a count of the maximum number of bits that you can have on. Now, one thing that I'd like to point out here is that this quantity is much, much, much, much, much less in general than some linear function in a like what we get for naive. Naive grows a lot faster--in fact, exponentially faster--literally exponentially faster than the bound on the running time is for Russian. Naive--very, very bad. Russian--actually quite good and happy. We're taking a moment to try to understand what it is about the way that the Russian algorithm is designed that makes it so much better than the naive approach that is just repetitive addition. Let's just go back for a moment to what multiplication is--at least integer multiplication. It is repeated addition, a times b. Let's focus for the moment on the case where a is even. Can be written as b plus b plus b plus b, repeated 8 times. We're considering the case were a is even here--we can regroup them as 2 sums. B added to itself a over 2 times, and then b added to itself a over 2 times again, and those 2 things added together, but it's silly to compute the same thing twice. Clearly if we're doing this calculation, we could just compute it once, b added to itself a over 2 times, and then just double the result that we get. Doing this calculation here is basically now repeating the same operation over again. Each time we're doing part of the sum here, we're actually saving ourselves a tremendous amount of effort. The idea of divide and conquer is that you can break a problem into roughly equal size sub-problems, solve the sub-problems separately, and combine the results. And in this particular instance, the sub-problems themselves, these 2 sums, are identical. It only has to be done once--so you're saving yourself half the effort every time that you do this, half keeps compounding and that's how we get down to algorithmic number of steps instead of a linear number of steps. So this way if looking at the Russian peasant algorithm leads to a very interesting way of expressing the algorithm recursively. The idea here is that we're going to do is to multiply a and b together. What we're going to do is say if a 0 to start, we can just return 0 and be done with it. On the other hand if a is even, then just because of the derivation that we just worked out a moment ago, multiplying a times b is really the same thing as adding b to itself a over 2 times, so it's a over 2 times b, which we're going to compute recursively. So the Russian algorithm is going to go off and do whatever it does to compute a over 2 times b. And once we had the answer to that, we need to multiply that by 2 to get the answer to the original problem. So we can use the solution to the sub-problem to solve the big problem. In the case where it's odd, it's a little bit more complicated. Pull one of the b's at. We're actually adding a's and b's together, but a is odd, so let's pull one of the b's out and add to that, well what's left--there is a minus 1, repetitions of b that we're adding together, but a minus 1 is now even, so we can have that--compute what a minus 1 over 2 times b is recursively. Once we have the answer to that, we can multiply it by 2. Well, it's going to give us what a minus 1 times b is, which is a times b minus b. So we just add the b back in and we should be done. Using the solution to the sub-problem, we can compute the solution to the original problem. So this may be seems a little bit circular, but each time that the Russian peasant algorithm is being called here, it's being called with a much smaller value--a half that it was before. And that's where we're getting a lot of our mileage from. Let's actually analyze this algorithm. It is going to be the same answer as what we got for the Russian peasant algorithm, but it's going to introduce a new tool that's going to be helpful for us analyzing lots of other algorithms. What we're going to do right now is a recurrence relation, which is a kind of recursive mathematical function, which is a good match for this recursive algorithmic expression for Rec<u>Russian--</u> Rec<u>Russian recurrence relation.</u> Looking at the structure of Rec<u>Russian, if a is 0, then it's going to execute 1 statement--</u> basically the test to see whether it’s 0 and returns. Otherwise, if a is bigger than 0 and even, let's take a look at what Rec<u>Russian does in that case.</u> We come in here with a number that is even and greater than 0 is going to execute the condition of this if statement, which fails so there's 1 of that. Then 1 more to do this plus it's going to recursively workout the value of this quantity. Then one more operation to multiply that by 2. I call a total of 3 plus however long it takes to multiply a over 2 times b. We don't know what that is. We're imaging that we're going be able to create a function T that is going to give us the answer to that. Let's just leave it at that for now. Finally, in the case where a is odd, it's going to execute the condition of this if statement, the condition of this if statement, both of which will fail. Then it will recursively compute the product, and then basically execute the returns. A total of 3 statements plus however long it takes to do the recursive call-- so 3 statements plus this particular kind of recursive call. This now is a mathematical specification of a function. We don't know at the moment what the relationship is between a and T(a), but at least it's fully specified. It turns out that you actually can solve this pretty easily by using what we already worked out about the number of times you can divide a number a in half, rounding down if it's odd, before you get down to 0. See if you can put that together to try to answer the question what does T(a) equal from these set of choices. All right. How do we work out the answer to this? Well, the number of times that it gets divided in half before it reaches 0 is this expression--log base 2 of a plus 1. But notice what's happening. Each time that we divide it in half, we're actually adding 3 to the pile. This is not quite right. We actually need to do 3 times this, which is like this, but then when we get to the bottom, there's going to be 1 more added into the sum. This is the answer. In this particular instance, we got kind of lucky that we happened to have a recurrence relation that we could analyze using one of the formulas that we already worked out. One of the things we're going to do next time is look at these kinds of relationships more generally. How do we take a recurrence relation and turn it into a concrete formula without getting bogged down in all of the details. One of the things we're going to try to do is ignore some of these pesky constants like the 3s and the 4s and so forth, so we can take a function like T(a) and say, well, it kind of grows in a log-like manner and not really get too worried about the details. In Unit 2 we're going to focus on Big Theta notation. Now, Big Theta is a little bit like Big O, but if you don't what either of those things are, it's a concept that computer scientists use to talk about the running time of algorithms. It's especially useful for comparing the running time of different algorithms to see which one is more efficient. We're going to be looking at Big Theta in the context of understanding the relationship of individuals in a social network and the connections between them. Let's dive in. Hi everybody. Welcome back. We're now starting Unit 2, which is going to focus on graphs and their growth. Last time what we talked about was an introduction to the idea of an algorithm and how different algorithms can let you do different things fast or slow and starting talking a little bit about analyzing the running time of algorithms. Now, for reasons that are not entirely clear to me, even though this is a course about analyzing social networks, a vast majority of our discussion last times was on multiplying numbers together. There is a reason for that. It was a simple problem that is easy for everybody to understand and appreciate where the algorithm choice can make a very, very big difference in the behavior of the algorithm once it's implemented. This time, we're going to focus on tools for analyzing growth rates so that we have a way of counting up how long running time is going to be that we can analyse more easily than actually counting in detail what every statement is going to be. At the same time, we're going to develop a feel for the kinds of graph networks that are really important in analyzing social networks. But, before we begin, we're going to try another little magic trick. Here's a block of number that I mostly randomly generated-- a bunch of three-digit numbers--and I want you to figure out as quickly as you can whether the product of all these numbers is divisible by 5 or if it's not divisible by 5. Okay, how did you solve the problem? The naive or straightforward or brute force way of solving this problem is to actually take all of these numbers, multiply them together, divide that answer by 5, and see what the remainder is. If the remainder is 0, then it is divisible by 5. Otherwise, it's not. But you would be very silly to go through that. What you should do is realize that a product is going to be divisible by 5 if and only if one of the number that you're multiplying into the product is divisible by 5. And a number is divisible by 5 if and only if its last digit is a 0 or a 5. What you can do now, instead of actually doing any multiplying, you just scan your eyes down the columns here and notice that most of them are not 0s or 5s, but if you're eagle-eyed, you would have noticed this. There's one number in there that's divisible by 5--945-- and that's all it takes. The whole product is now going to be divisible by 5. The answer we're looking for is yes. The product is divisible by 5. Ta da! Just to remind you from last time, the structures that we're talking about mostly in this course are graphs--also sometimes called networks, which is a discrete structure consisting of a set of nodes--sometimes called vertices-- and edges--sometimes called links. Interestingly, usually the people who call the nodes "nodes" call the edges "links," and the people who call the nodes "vertices" call the edges "edges," because these both come from geometry. But, for whatever reason I mix them together. I apologize. I tried to change, but it just didn't work. Here's an example of a graph that has six nodes--the blue circles here-- and one, two, three, four, five, six, seven, eight, nine, ten edges connecting those nodes together. In these kinds of graphs, the edges just link up two of the nodes-- not more than two, not fewer than two. In what we're going to be talking about, the edges don't have any direction to them. They just link them together. The particular positioning of them on the page is really irrelevant. What matters is which nodes are connected to which other nodes. What we're going to be talking about for the next little while are special networks that have particular structures and that usually fall into different families. One particular family of networks is called chain networks. Chain networks consist of a set of nodes and a sequence of edges between them that basically connect them into a single chain. This is an example of a chain network with five nodes--one, two, three, four, five-- and four edges--one, two, three, four--connecting them together. There's essentially only one such network. We can imagine flipping it around or relabeling the nodes, but topologically, there really is only one way of connect five nodes into a chain. Now, I mentioned that this is part of a family of networks. Now, you get a different chain network for each different possible number of nodes. Now, a question I'd like to have you answer is how many edges do you have in a network that has n nodes if it's a chain network? We can see from this example that there is 1 fewer edges than there are nodes. So, the number of nodes minus 1 is the number of edges. Next, let's take a look at a ring network. Now, a chain network you can think of as being the relationship between a set of individuals with a direct chain--basically, for example, the relationship between me and my chain of ancestors. This might be my son, who is connected to me, and I'm connected to my dad. My dad is connected to my grandfather, and so on. I actually don't know who my great-grandfather was. That's a chain network. In a ring network, we actually now complete the loop, which isn't a good representation of my family, I'm glad to say, but lots of other things do have this sort of relationship. As we can see in this particular example, we have a ring of 5 nodes and there are 5 edges-- 1, 2, 3, 4, 5. One thing that might be useful to do at this point is to look at what a ring network like this might look like as a piece of code. For the purposes of this course, I'm going to represent graphs as dictionaries of dictionaries. The way that that works is--let's take a look here. What we're going to do is make a graph. We're going to call it a_ring. We set it equal to 5 and we initialize the ring to be an empty dictionary--just { }. Then what we're going to do is to this empty graph we're going to start adding in edges. The particular edges we're going to add in we're going to loop for each number from 0 to n - 1. We're going to make a link in the ring from node i to node i+1 mod n. That's going to get us the wrap around at the very end. What make_link does--and I'm going to use the same definition throughout-- is it takes a dictionary or graph and two nodes if we want to connect together, checks whether the first node is already in the graph. If it is not, then it creates an empty dictionary for that node. Same thing for node 2. Then it goes to the dictionary for node 1 in the graph and says that there is a connection to node 2, so this basically just establishes the connection between node 1 and node 2 so that later we can go and test for it if we want to. Once we've created that dictionary, we can ask questions like, well, how many nodes are in the graph and how many edges are in the graph? I actually ran this already. You can see that the number of nodes is indeed 5, which is good, because that's what we wanted it to be. The number of edges--the way that we're calculating that is by taking a look at all the nodes in the graph--in this case, the a_ring.keys() is giving us back a list of all of the names of the nodes in the graph. We're going to assign the variable node to be each one of those in turn. What we're going to do with that node is look up the dictionary for that node-- in other words, look up all the connections that that node has in the graph, and look up the length of that, put it together in a big list. This list has one entry for each node in the graph, and that entry is exactly how many edges that node has, which you may recall from last time is called the degree of the node. This expression--actually, from this bracket here to this bracket here gives us a list of the degrees of all the nodes. We sum all those up and now we've counted each edge twice, so we divide by 2 to get the total number of edges in the graph. In this case, I then printed out the actual graph itself just so you can see what it looks like. Five is the number of nodes. Five also happens to be the number of edges. Because this is a ring, there is one edge coming out of each node and then wrapping back around to the front. What it actually looks like internally is the list of the nodes. This could be any name at all as long as each node has a unique name. I used numbers here, because it was really easy to do. There's node 0, node 1, node 2, node 3, and node 4. What node 0 is is a dictionary of all the nodes that it is connected to directly. Node 0 in a five-node ring is connected to node 1, the node to its right, and node 4, the node that would be one behind it with the wraparound. Similarly, 1 is connected to 0 and 2, 2 is connected to 1 and 3, and so on. Grid networks are also very important. They represent the connectivity pattern you have if you lay out a bunch of things as a grid and connect them up, each one to their neighbor sort of street-wise if this was a grid-based city. Here's a particular example of a grid network with 20 nodes--1, 2, 3, 4 times 1, 2, 3, 4, 5 and 31 edges if I counted correctly. In particular, there are three edges to go across, and there are 1, 2, 3, 4, 5 rows like that, so that's 15, and there's 4 edges going down, and there are 1, 2, 3, 4 rows like that for 16 for a total of 31. So just to make sure that you understand how to do this, imagine we have 256 nodes arranged in a square, and we connect up the nodes with edges in a grid pattern just like this one here. How many total edges do we have in the graph? We can answer this question pretty quickly with the help of a little program. Like before I've got this make_link function, and I'm going to make an empty graph with 256 nodes which is going to be arranged a grid--a number nodes per row and column on a side here is the square root of n or 16. Then what we're going to do is we're going to run through all the pairs of nodes in the graph i, n, j, and for each one if it's not on the very edge, we're going to make a link-- well, if it's not on the bottom edge, then we're going to make a link downward. If it's on the right edge, we're going to make a link to the right. That's going to build our gird. So, we print out the number of edges in this graph that we created and we get 480. We could do that kind of by reasoning as well, that just like in this grid example before the little chain that we have going across is the number of nodes in a column minus 1, and there are five of them because there are five rows. In this case there will be 16 of them. There will be 16 chains of length 15 across. The downward ones are going to be the same, so we'll just double that. We also get 480 that way. It'd be nice to be able to answer this question in a little bit more generality. How many edges do we have in a grid graph? We're assuming that there are n nodes and the nodes are arranged in a square. The picture of that looks like this where you have nodes arranged in a square. For this to make sense, we're assuming that n is a perfect square, so that we can actually arrange the nodes and form a square as so. There's √n nodes on each of the sides, and if we fill in the edges, these green edges, what's the total amount that we get? Well, the same basic idea as we used last time applies. That is, the number of edges that go across in a chain here is going to be √n - 1. That's going to be repeated for each of these rows. And we're going to get the same kind of analysis for the edges that are going down. So, what is that? That's 2n - 2√n. Does that give us the right answer when n is 256? Yes. Well, that's good. In that last analysis of grid graphs, we ended up discovering that the number of edges is 2n - 2√n, which is such a terrible function, but the fact of the matter is as these analyses get more and more complicted getting detailed answers like this is more and more cumbersome. Researchers have worked out a scheme for talking about functions in a much more high level way while still being completely rigorous but without it getting bogged down in all the details, and this is the notion of asymptotic growth. You may have heard it described in the form of Big Oh notation, and we're going to get to Big Oh in a moment, but we need to start off with Big Theta. Θ(g(n)) is actually a set of functions, specifically the set of functions that grow equally quickly as g(n). To be a bit more formal about it, we say that some function f(n) is in the set of functions equally fast growing as g(n), if and only if there are some constants c1 and c2 and some threshold n0 such that for any n bigger than n0, we have something that looks like this-- the function f(n) for all these values of n--all these big values of n-- is sandwiched between c1 * g(n) and c2 * g(n). To illustrate this idea with a picture imagine we have some function f(n). It grows. As n gets bigger, f(n) gets bigger. We also have some function g(n) that we can multiply by c1 or we can multiply by some bigger number c2. What we find is that once we get out to the right of some threshold n0 f(n) is always between c2g(n) and c1g(n). In the beginning maybe weird things happen, but once we get out far enough asymptotically in the limit as things get really big, f(n) lies between c2g(n) and c1g(n). If that's the case, if such c1, c2, and n0 exist, then we can say that f(n) is in Θ(g(n)). So the definition is a little bit complicated, and it involves a bunch of moving parts like c1s, c2s and 0s and ends in f's and g's. Let's see whether the definition at least satisfies some intuitive properties that we want it to. So to the extent that we're saying that Big Theta means the two functions grow roughly the same as each other, then it ought to be the case that if some function f(n) is in Θ(g(n)), it seems like it should go the other way, as well. If f(n) grows roughly the same as g(n), then g(n) should grow roughly the same as f(n). Is that true? So here's some possible choices. No, as much as we would like this to be true, it just doesn't follow from the definition of Big Theta that we just gave. Yes, because Big Theta is like an equality. An equality is reflexive, so Big Theta ought to be reflexive. And then another possibility is yes, because we can use the c1 and c2 from the definition of f(n) is in Θ(g(n)), use the reciprocals, and that sandwiches g(n). If you're a little bit test-wise, you should be able to guess this even if you don't understand why. The answer is this, but let me explain why. What we'd like to show is that if we're given a function f(n) that is in Θ(g(n)) that we can turn that around and also infer that that means g(n) is in Θ(f(n)). The way we're going to show this is to go back to the definition of Big Theta. Here's the definition--that there is some constant c₁ and c₂ bigger than 0 and a a threshold n₀ such that f(n) lies between c₁ of g(n) and c₂ of g(n) for all n bigger than the threshold n₀. We can infer from this that by dividing through this equation dividing through by c₁, which is a positive number, we get a new set of inequalities--like so-- where g(n) is now sandwiched between 1/c₁ of f(n) and 0. We can do that same trick again dividing through this equation by c₂. We see that 1/c₂ of f(n) is less than or equal to g(n). We can combine these two facts to show that g(n) is sandwiched between 1/c₂ of f(n) and 1/c₁ of f(n)--like so. Notice that this1/c₂ and 1/c₁--those are just constants. It equivalent to say that this 1/c₂ is just a constant, and we can rename it c₁, and this one 1/c₁ is just a constant and we can rename it c₂, and this is exactly the definition of Big Theta, so we can infer from this that g(n) is in Θ(f(n)). What the Big Theta allows us to do is basically write complicated functions in a much simpler way. We can take a function like ½n² and just think of it as Θ(n²). And 8√n we can think of a just Θ(√n). Our equation from before, 2n - 2√n just becomes Θ(n). It's just a linear function asymptotically. A complicated expression like this where we have n⁴, which is the term that grows the fastest, becomes Θ(n). The ln n--in fact, any base log n is Big Theta of any other base log n as long as it's a constant. I like to think in terms of base 2 logs, because I'm a computer scientist. That's what we do. π² is something that doesn't grow with n, and it ends up being in the set Θ(1). It's just another constant. Just to beat this dead horse a little bit longer, let's use the definition of Big Theta to show that this expression that we determine for the growth in edges in a grid-- 2n - 2√n--really is just a linear function. It grows like Θ(n). The game plan is that we need to find constant c₁ and c₂--bigger than 0 and a threshold N₀ so that for all the n bigger than n₀, the function that we care about is sandwiched between these two scalings. Let's focus on this one first. What c₂ can we plug in here so that we're guaranteed that this will be above this expression. If we just copy this inequality down just flipping it around to make it a little easier to think about, we want c₂ so that c₂n is bigger than this. Divide through by n Now we need a c₂ that is bigger than 2 minus something that's actually growing. Two should work for that. If we set c₂ to 2, it will satisfy this inequality. Let's just summarize all that We can set c₂ = 2. What about c₁? Well, let's take c₁ to be 1. Intuitively, the idea being this function is growing like 2n minus something smaller than that. So, n should be underneath that. But let's just make show if c₁ is equal to 1, then we need n to be less than or equal to this expression. For what values of n is that going to be true? It's not true of all of them, but it's true for some of them. We can add 2√n to both sides, subtract n from both sides, and we get that. If we divide through by √n, we get 2 ≤ n/√n and divided by √n is actually √n. So, we have 2 ≤ √n. We square both sides. We have 4 ≤ n, or--flipping that around the other way-- if n ≥ 4, then this is true That means we have to throw away the smaller values of n, and we can do that very simply by setting n₀ to 4. This only has to hold for n that are bigger than n₀. That's what we've got there. There we have it. If we set the constants this way, n₀, c₁, and c₂, then what we find is that for big enough n this more complicated expression is sandwiched between two simple linear functions or to say it another way-- Big Θ is just one of a bunch of different functions that we can define, and there's a set that essentially corresponds to all the different ways you might want to compare a function. If f(n) is in little o(g(n)), that's kind of like saying f(n) is strictly less than g(n). It grows less slowly asymptotically. F(n) is in O(g(n))--there's our friend O--that's really like saying f(n) ≤ g(n). It might grow as fast as g(n), but it might be small. Θ is the one we just looked at, which kind of like equality--they grow roughly at the same rate. Ω--f(n) is in Ω(g(n)) means that it is an upper bound. F(n) is bigger than or equal to g(n). G(n) is a lower bound on f(n). The ω, analogously, is kind of like strictly greater than. Once again, Big Theta allows us to work with functions in a much simpler form without losing their essence. Let's just make sure that we understand how to do that. Let's take this expression 2n² + 6n + 20 log n. Which of these is it Big Theta of? Is it Θ of itself? Is it Θ of the term with the largest coefficient? Is it Θ of the highest order term--n²--but with the largest coefficient--20. Is it Θ of n² by itsef? It is Θ of a combination of these things--n² and log n together? More than one of these is true. Check exactly the ones that are true. The right way to think about this is of all the terms in the sum here, you need to take the one with the fastest growth rate. Log n is piddly as n gets big compared to n, and n is actually pretty piddly as n gets big compared to n². N² is really the only thing that matters here. This constant in front of the n² is not going to matter in Big Theta world, because we can go above it or below it. Any coefficient in here gives you the same answer. These terms drop out. This leading term doesn't really matter. So really, this is n² . However, in Big Theta world, this is also n². Every function is Big Theta of itself. It's very easy to sandwich it between 1.5 times the function and 0.8 times the function. By the definition of Big Theta, this is true. This is not true. This is true, because 20n² is also Θ(n²). This is not. Once you multiply another term in here, even though it grows really slowly, it's not a constant. It changes the asymptotic growth rate. This is Big Theta of those three. Another category of graphs that are very important and come up a lot are planar graphs. These are graphs that can be drawn in the plane on a flat piece of paper so that the edges don't cross. Here's an example of a planar graph. We've got five nodes and five edges. You see the way I drew it here. This node here and this node needed to be connected. I just drew a nice, little curvy line without crossing anything else. This is a planar graph. It still would be a planar graph even if I drew it with a crossing in it, because the issue is that there is some way to do it so that the edges don't cross. Now, in this particular example, we've got five nodes and five edges, but what we're interested in is what is the relationship in general between the number of nodes in a planar graph and the number of edges. This is more complicated example than any of the ones we looked at before in this unit. There is some kind of a relationship, and there are constraints by virtue of the fact that the graph is drawn in the plane. Let's take a look at that. Let's think about adding some edges here. We can add that edge. We can add this edge. We can add this edge. We can add this edge, and are there any more edges we can add while still being able to draw it in a plane? It doesn't look that way, right? This node is connected to all the other nodes. This one is done. This node is connected to three other nodes, but it can't reach this one. It seems like it's sort of all blocked out. That's going to be true in general. There's going to be some graphs that can't be drawn as a planar graph. If we add this edge between this node and this node, the resulting graph is no longer planar. How many edges do we get here all told? We've got one, two, three, four, five, six, seven, eight, nine. In this example, the number of edges seems to need to be less than or equal to 9. To try to understand the relationship between edges and nodes in a planar graph we're going to use a result due to Euler--same Euler as before. To do that we're going to need another concept about planar graphs. Here's a planar graph on five nodes. Again, we have a notion of nodes. We have a notion of edges. We're also going to now talk about a notion of regions in a planar graph. Regions are kind of areas that are boxed off in the graph, plus the region outside of the graph. Turns out that's a really useful thing to be able to talk about. This particular graph has three regions. We'll right it's got 5 nodes, 6 edges, and 3 regions. Now, our good friend Euler discovered a fascinating relationship between these values. For any planar graph, n - m + or is equal to 2. Really? Does that work in this case? Let's check this formula with the values for this particular graph. We've got n - m, 5 -6 is -1, plus 3 is indeed 2. Interesting. We're going to figure out why that is in just a moment, but let's let you practice first understanding what these regions are. Here's a planar graph--a bunch of nodes and edges. I want you to figure out how many regions are in this graph and then write it in this box. All right. Here's our answer. Each of these areas here is a region. I'm just numbering them. Then there's also a region which is the outside box. Seven regions in this example. Let's check Euler's formula again for this one. We've got n = 10, m = 15, and 7 regions. N - m is -5, plus 7 is 2. Ta da! Let's take a look at Euler's formula again. What we're going to do is prove by induction that this holds. We can build any planar graph by iteratively adding nodes and edges. Let's start off with the simplest thing, which is just a single node. Ta-da! We can start off with just the simplest graph of all--just a node sitting on the plane all by itself, a lonely point, and we've got one node, no edges, and one giant region around it, and 1 - 0 + 1 is indeed 2. That's our base case. Now we proceed by induction. Given that we're doing a proof by induction, we're going to assume that we have some planar graph and that Euler's formula holds for that graph. We have already that n - m + r = 2. What we're going to do is we're going to add to this graph so that it's still planar and see what happens to this formula. There are two different ways that we can add to this graph. We can add a node and an edge together, or we can add an edge between two exiting nodes. Let's consider this first case where we had a new node and an edge between them, and it's still a planar graph. What happens to m, n, and r? Well, we have one new node and one new edge but the number of regions hasn't changed. We're just jutting into some region inside or outside, but it doesn't change the total number of regions--(n + 1) - (m + 1)-- these ones cancel, and we get n - m + r, which by our inductive hypothesis we said is 2. In that case, the formula still holds. What about the case where we just add an edge. One way we can add an edge is inside of some other region, and let's look what happens in that case. The number of nodes is unchanged. The number of edges has gone up by one. But the number of regions has gone up by one as well. This used to be one giant region, and it's now been split in to two regions. Once again, these ones cancel. Our formula still holds. Let's just double-check what happens if we do something on the outside, sort of breaking into the huge vast region around it. Connect, say, these two nodes, what have we done? We still have the vast region around it, but we created a new region here. Again, the number of regions has gone up by 1. Similarly, if we click this over the number of regions has gone up by 1. If we add an edge without a corresponding node, the number of regions goes up by 1. If we add an edge and a node together, the region stays the same, but the number of nodes goes up by 1. No matter what you do, this formula keeps holding. Pretty cool. Now we're going to use Euler's formula to give us a handle on how fast edges grow relative to the number of nodes in a planar graph. We're going to make use of two other facts. One is that every region in a planar graph has to be encapsulated--has to be bounded-- by at least three edges for it to be a region. It might be more that that, but it has to be at least three. If you think about three times the number of regions, the number of edges has to be at least that big, though, we're counting each edge twice, because each edge can actually participate in two regions. Twice the number of edges has to be bigger than or equal to three times the number of regions. Rewriting Euler's formula, we have this--rewriting this equation, we have this. Substituting in, we have this. Let's multiply through by 3. We get 3m + 6 â‰¤ 3n + 2m. Subtract m from both sides and subtract 6 from both sides, a and we get m, the number of edges, can't be any bigger than 3n - 6, which this expression is in Î˜(n). The most edges that we can have in a planar graph is at most linear in the number of nodes. What we've seen so far--we've analysed a bunch of different families of graphs, and we've found that for all these examples the number of edges is like Î˜(n). It grows linearly with the number of edges. You might wonder at this point, is that all there is in life? Is it always the case that the number of edges grows linearly with the number of nodes. The answer, of course, is no. Let's consider the complete graph, also known as a clique on n-nodes. In a complete graph, every node is connected to every other node, and you get very pretty star patterns if you fill these in. In this case, we have five nodes and a total of ten edges. But this doesn't really tell us anything about the growth rate. It just tells us that in this one example, well, it could be that the number of edges is twice the number of nodes. We don't know. We need to generate a more comprehensive formula. In fact, I'm going to say you need to generate a more comprehensive formula. In this question, I'd like you to actually work out the number of edges in a complete graph on n nodes. The way you should submit this to us is by writing a function in Python that's called "clique" and is given one value n, which is going to be an integer greater than 0, and it should calculate from that the number of edges that will be created if you fully connect all those nodes, and then just return that answer back so that we can grade it. I told you to use a mathematical formula, so I cheated a little bit. I wrote a little piece of code to actually generate a complete graph and then return the number of edges in that graph. To make a complete graph, I just looped through all the pairs of nodes, and if one node was smaller than the other, then I made a link between them. That was mainly to just make sure I didn't make a link from nodes to themselves. There is no reason not to make a link the other way. It would have not counted against the total, but this seemed nicer and cleaner. Let's look at what happens if you loop on all the different values of n from 1 to 9 and print n and the number of edges in the clique. A graph with one node has no edges. A graph with 2 nodes fully connected has one edge, 3 has three--that's a triangle, 4 has six, which is the square with the x through the middle of it, 5 is that pentagram-looking thing that we just showed a minute ago, and it continues growing up like that. With that as our insight, can we actually write down what the formula is for a graph with n nodes? Essentially what happens is each node gets an edge to each other node, which we can write as each node has an edge to each other node. Notice what happens if we use that as our formula, we're going to count this edge, as it goes from this node to this node. We're also going to count it again, as it goes from this node to this node. We've double counted everything, so that should be our formula, which is Î˜(nÂ˛). Let's just double-check this. If we print n, clique (n), which I counted by actually creating the clique, compared that to the formula n * (n -1)/2, you can see that it matches up perfectly. So, really the answer that you should have given is this-- shouldn't have bothered with any of this creating of stuff. Now, in addition to all those graphs where the growth rate was linear, we now have one that's quadratic. The next kind of graph we're going to look at is called a hypercube, and the graph is almost but not quite as cool as the name. We're going to define a hypercube for any number of nodes as long as that number of nodes is a power of 2. Here we have four nodes. That's a power of 2--2Â˛. What we're going to do is we're going to connect edges up in a particular way. This is how we're going to do it. We're going to imagine that every node has a label. That label is a bit pattern. In particular, we're going to imagine that we're going to number each of the nodes, basically, from 0 up to n - 1. We're going to connect two nodes if their bit patters differ in exactly one place. We see this example here. If we label the nodes--these are now binary numbers 0, 1, 2, 3-- we connect these two nodes because they differ just in the first bit--11, 01. We connect these two because they differ in the second bit--10, 11. We connect these two because they differ in the second place--00, 01. And we connect these two because they differ in the first place--00, 10. We can generalize this to larger graphs. Here's a hypercube on eight nodes. We number the nodes from 0 to 7 in their binary bit patterns. Then we connect two nodes if they differ in exactly one bit place. Now, perhaps you can see why these are called hypercubes. This is a cube. The previous one was a square. What's the next one going to be? To generate the hypercube for n = 16, we can do this with a very interesting little trick. What we're going to do is we're going to take two hypercubes of size 8 and 8. Now 8 and 8 is going to be 16. Let's look at what we can do. We can leave these bit patterns the way that they are, and then we can extend them. Everybody in this hypercube is going to get a 0 in front. Remember what that means in terms of the binary numbers is that they say the same. A leading 0 doesn't change anything, but this second hypercube everyone is going to get a one out in front. Again, thinking about your binary numbers, what is that doing? It's adding 8 to all of the number of these nodes, giving unique bit patterns that we haven't seen before. Now if we connect the corresponding corners of these two cubes, which is going to be a little bit messy, and I can draw them deeper, but it'll start to make the picture a little distracting. That is a 4-dimensional cube, also known as a hypercube. We can keep extending this over and over again--5-bit patterns, 6-bit patterns, 7-bit patterns. Each time what we're doing is connecting two hypercubes of the previous size, extending them and then connecting the corresponding vertices. So, question for you. How many edges are there in an n-node hypercube? Possibly to make it easier for you or possibly to make it harder for you, instead of coming up with an exact answer, I'll just give you a list of the Î˜ values. Work out what you think the answer actually is, but you only need to match it with regard to the Big Î˜ of one of these functions. So the right answer is (n log n). Why is that? So here's the n = 4 hypercubed square and recall that the bit patterns have 2 bits in them, and, in general, the number of bits in the label of a node and then hypercubed with n edges is log n-- log base 2 of n, specifically. So that means the degree of each node, in this particular graph--it's 2--but, in general, the degree is log n. So that means each of the nodes in the graph has log n edges coming out of it, so we can think of n times the log of n as the number of edges, but we have to be careful, because we double counted again. This node is connected to this one, but this one is also connected back to this one by the same bit pattern. Exact answer is 1/2 n log n, but that's okay. In Bit Theta land, we can ignore the 1/2, and we have Bit Theta and then log n. Another graph type worth mentioning is a tree graph. A tree graph has two properties. The first property is that the graph must be connected, and the second property is that the graph doesn't have any cycles or loops. The best way to understand what a true graph is is to see some examples. So, let's have a quiz. Here are 8 graphs. For each graph, I want you to check the box if it's a tree. To see if it's a tree, the first thing we need to do is to see if the graph is connected. Looking through the 8 graphs, you can see that the fourth graph is not connected. Recoloring the edges, we can see that these three nodes are not connected to these three nodes. So, this one can't be a tree. All the other graphs are connected. The next thing we need to look for are cycles. The second graph is a ring, and it's obviously a cycle. In the seventh graph, there's a cycle between these four nodes connected by the edges in pink. So, that can't be a tree. That leaves these other fives graphs, all of which are trees. Now, some of these--like this one--it's kind of obvious that it looks like a tree. But the eighth graph doesn't really look like a tree. Why do we call it a tree graph? What we can do is number and rearrange the nodes to make it look like a tree. There are seven nodes in this graph, and I've numbered them 0 through 6. Let's start rearranging this. Let's start with node 6. Now, node 6 is connected to node 2, so we'll draw that connection. Node 6 is also connected to node 1, so there's that edge. We can see that node 2 is connected to node 4 and node 5, and so we can draw in those nodes and the edges. Then node 1 is connected to node 0 and node 3. Here are those nodes and edges drawn in. We can see that we can transform this graph, which doesn't look like a tree at all, into something that looks a little bit more like a tree. It branches out. Hopefully, that gives you some idea of why we call these tree graphs. Up to this point in the unit, we've been talking about very, very structured graphs, very clean and organized graphs--things like chains or rings or grids, hypercubes-- that have a great deal of structure, and they look very different from the kids of actual social network graphs that you see in the wild. Regular graphs tend to have little bits of clustering in them and some people who are more sparsely connected to some other cluster, and they start to look a little bit more randomly generated or at least not generated according to a very simple, straightforward rule. It's worthwhile to think about some different ways of randomly generating graphs. The classic kind of randomly generated graph that's been studied in the mathematics literature is generated according to what's called the ErdĹ‘sâ€“RĂŠnyi model. Let me say a little bit about that now. We start off and we imagine we have some graph with n nodes or we have some set of n nodes, and we have some probability p, which we'll call the connectivity probability and the generation process works like this. We start off by generating a set of n nodes with no edges at all. Then we loop through all possible pairs of nodes--i, j-- and we connect i, j, but we only connect them after we flip a coin that comes up heads with probability p. If it does come up heads, we connect the pair. Otherwise, we don't. We can do this for four nodes. We have to decide whether to connect the first two nodes. We flip a coin, comes up heads, so we connect them. Now, we look at the second pair--this one and this one--that also came up heads. The third pair-- this on and this one--also came up heads. The next pair is this one and this one--tails. No edge. This one and this one--heads. Finally, this one and this one--heads. I think I have a broken coin. But anyway, you get the idea that you end up with a graph with some of the edges including and some of the edges not included. Now, there's lots of interesting mathematical properties that you can derive given the simplicity of this generation process. You can say--well, what should the probability p be so that the graph is very likely to be connected? There's no disconnected nodes floating off into space or that there's relatively short paths from one node to the next. This has been fairly well studied. Researchers have found, however, that these kinds of randomly generated graphs don't seem to match very well the kinds of social network graphs that you see in practice in the real world. Alright, so next we're going to look at some recursively generated graphs. We start off with a set of n nodes, then we create a graph on half of the nodes recursively. We create a graph on the other half of the nodes recursively. Then we generate some edges that connect up between these two smaller graphs, and that's the graph that we return. So, we can get a bunch of different mechanisms for generating recursive graphs with this basic structure, depending on how we create the 1st sub graph, how we create the 2nd sub graph, and how we connect the two sub graphs together. So, let's start off with a really simple example. So, here is some pseudo Python for generating a graph with n nodes. What's it's going to do is--to make a graph with n nodes--if n is just 1, it returns a single node all by itself. But if it's not, we're going to assume it's some other power of 2. We make a graph with n/2 nodes, call it G1. We make a another graph with n/2 nodes, call it G2. Then we pick a random node from G1, call it i1, a random node from G2, call it i2, and we link them together. So, at a high level, create graph G1, create graph G2. We pick a random node i1, pick another random node i2, and we link them together. Now, in this picture it's not really clear what's going on inside of these. What I'd like you to do is figure it out. So, maybe take n to be some small power of 2, like 4 or 8 and trace through this pseudo code to try to figure out what kind of graph pops out. So, here are three choices to choose from what we get out. Is it a tree, is it a chain or is it a ring? So, let's build up to the answer in steps. We know that if n=1, it just returns a single node. If n=2, it's going to recursively generate a single node for G1, a single node for G2, then randomly choose nodes from those sets, which in this case are just the only nodes that are there, so it'll choose them, and then it connects them up. So, that's what n=2 looks like. For n=4, we have two n=2, right? G1 and G2 are both going to be an n=2 graph. Let me draw those. Then, it's going to randomly choose one of the nodes and one of the nodes and connect them up. So far, it looks kinda like a chain in all these cases. Let's go to the next case. Again, for n=8, it has to generate n=4 graphs, each of which has to generate a bunch of n/2 graphs, which all look like this. There's going to be some randomness about how they get connected up. So, maybe in one of the times we call it, we get something like this, and one of the times, we get something like this. Then, we connect randomly the two n=4 graphs to get the n=8 graph, and maybe that will be something like this. So, in this particularly case now, we don't have a chain anymore. We do, on the other hand, have a tree. And in fact, this property continues as we get bigger and bigger graphs. We're always connecting up some kind of tree structure with some other kind of tree structure. Here, I'll draw another one. And we connect up these two tree structures, and that just gives us a bigger tree structure. So, we get a nice, messy, recursively defined tree, but it's still a tree. So, we can define a recurrence relation for this particular tree generation process or graft generation process that matches the recursive structure of the generating routine. We have a graph with 1 node, it's going to have 0 edges, by the way that this generation process happens. If we have a graph with n nodes, what is it going to do? It's going to call the recursive generation process, with n/2, to generate however many edges it generates with that, and it's going to do that twice. So, it's 2 times that. And then at the end, it's going to draw 1 extra edge to connect the 2 components together. So this now is a recurrence relation for the number of edges. Now, we know already what this is going to be. There is going to be n-1 edges in a tree with n nodes. Let's go through this process anyway of solving a recurrence relation because this is going to be a useful tool. So, you can think of what happens as this process unfolds. To generate a graph with n nodes, it 1st breaks that sub problem into 2 things, where we work out the number of edges in a graph with n/2 nodes. but doing that requires working it out for n/4 nodes and so on. This goes on and on and on until we get down to a whole bunch of individual graphs, each of which has 1 node. That's where the recursion bottoms out. Two useful questions: First question is, "How deep is this tree that's being built here-- that bottoms out over here with the 1s, and how many leaves is it going to have?" So, why don't you fill that in. Is the depth that we get this way log n, n, or 2^n, and are the number of leaves at the bottom log n, n, or 2^n? What happens at the depth here? So, we keep dividing it in half until we get down to 1. We've already spoken about that. That is exactly base 2 log. So the depth is base 2 log. How many leaves do we get? There's one at the first level, then 2, then 4, then 8, then 16. So in general, it's going to be 2^d. But we just said that depth is log base 2 of n. So that's exactly n. Let's go back and take a look at this again. We now know how deep. It's log n. And we know how many leaves. It's n. At each of these levels, not only is it going to compute the subgraph of the given size, but then it's going to add 1 more edge that is the connector between the 2 components. So there's 1 of those edges gets added at this level. There's 2 that get added at this level. There's 4 that get added at this level. There's 8 that get added at this level. Until finally, at the end of the day, there's zero that get added at this level. But the level right above it, there's n. So the total number of edges that get added is the sum of all these things-- n + n/2 + n/4 to 1. That is basically 2n--so that we don't have to say basically we can say it's exactly Î˜(n), meaning it's basically 2n. What we basically rediscovered is that the number of edges in a tree, or one of these randomly generated trees, is still linear. That's not so surprising, but this particular way of doing the analysis with this tree is going to turn out to be useful in some other settings. We're looking at the same basic setup as the last time. We're making a graph with n nodes, and again we're going to assume n is some power of 2. If it's just a single node, we just return that node. Otherwise, we recursively generate the two subgraphs. Now what we're going to do is choose half of the nodes from G1 and half of the nodes that are in G2, and connect all pairs. So, for all I1 and S1, and all I2 and S2, we're going to link I1 and I2 together. We have some graph G1. We have some graph G2. We choose half the nodes at random to be called S1. We choose half the nodes at random in this graph to be S2. Then we connect up all pairs so that every single node up here is connected to every single node up here. It gets very densely connected here, but kind of much looser elsewhere. So how many nodes are in that graph? Well, we're going to do another recurrence relation. Once again, a graph with 1 node has zero edges. A graph with n nodes has however many edges are in 2 subgraphs, each of size n/2. Then we are going to add to that--what? We're going to take n/4 of the set of nodes and fully connect them with another set of n/4 nodes. n/4 from one side, n/4 from the other. We're looking at all combinations of those. This becomes our recurrence relation. It's a little bit cumbersome to work with this. It has funny constants in it, so what we're going to do is actually think about in Big Theta notation. This is just n^2. It's really 1/16 n^2, which is Î˜(n^2). Let's take a look at what happens in this same kind of tree structure as before, where the amount of edges that we add at each of these levels is some Î˜(n^2). So at the top level, it's going to be cn^2. Next level down, it's going to be c n/2^2, which is n^2/4--same thing twice, which is c n^2/2. Next level down, we square--we get 16-- but there are going to be 4 of these now. So it becomes a 1/4. In fact, each level down here we're getting c n^2 over 1, over 2, over 4, over 8, over 16, and so on. And we add all these up. We get cn^2*1 + 1/2 + 1/4 + 1/8 and so on, which adds up to something on the order of 2. So when all is said and done, we still have Î˜(n^2)--edges total. This recurrence relation solves 2n^2, which is interesting because we do n^2 really at just the first level. All the other levels that we do, the amount of work gets cut in half each time. So even though there's possibly a large number of levels, it doesn't add up to anything more than another constant times n^2. Here's our last random graph of this unit. We're going to again generate a graph on n nodes recursively. Same structure as before. If there's 1 node, we just return that node. Otherwise, we do it recursively. So we generate a graph on n/2 nodes--call that G1. We generate a graph on n/2 nodes and call it G2--different graph. Now we randomly scramble the nodes in those two, keeping the edges in their appropriate way. Just consider them in a particular order. Then we connect the first node in this ordering in G1 to the first node in this ordering in G2, the second one in this ordering in G1 to the second one in G2, third one to the third one, forth one to the forth one, and so on. Now we get a set of edges that now breach across these two graphs, and we call this G and return it. So based on this random generation process we can ask, what kind of graph structure did we make? Is it a ring, a tree, a hypercube, or maybe none of these? The answer turns out to be none of these, but if you though ring, that was true for a little while. If you thought hypercube, that we true pretty for the same amount of time, and I actually thought the answer was hypercube until I drew out a picture and discovered that it wasn't. Here's why. Again, in the case for n = 1, it's just a single node. In the case of n = 2, it's one single node that's half the graph and one single node that's the other half of the graph, and individual subgraphs have their nodes randomly ordered, but there's just one of them so there's only one order. Then we connect the corresponding nodes, but there's just one of them, so it's just that. In the n = 4 case, we have two n = 2 graphs, and then the algorithms arbitrarily orders the two nodes and the two subgraphs, but it looks the same either way. If we flip this around, it still looks the same. Let's pretend I flip this one around, and then we connect up the corresponding nodes. Now we get something that is a ring. It's also a hypercube. Actually, all these have been hypercubes so far. Let's move up to n = 8. In the n = 8 case we first generate two n =4 graphs, which we know what they look like from before. Then we arbitrarily order these nodes and the two different graphs and connect up the corresponding nodes. This is where it gets sort of weird. In a normal hypercube, we would connect this corner, this corner, this corner. Let's start off that way. Because the nodes are randomly ordered, we could actually get something like this where the top is nice and parallel and the bottom is crossing. This is not a hypercube, but it's sort of hypercube-ish. As we grow this to larger and larger n, we get tanglier and tanglier structures. So, the nice thing about these structures is they do kind of have an essence of looking like real social network graphs. They have some complexity to them. This is still not quite what we're looking for, but let's do an analysis of this graph then call it a day. We can express the number of edges in the tangled hypercube again using a recurrence relation. Now the recurrence relation has this form.. For an n-node graph we generate 2 n/2-nodes graphs, connect up the parallel nodes and that introduces n new edges. Once again, we can think about this as happening in a kind of recursion tree. We first consider generating a graph with n nodes. We do that by generating two graphs of n/2. Each of those actually requires two graphs of size n/4. Each of these levels we're going to be generating n new edges at this top level, n/2 for this subgraph, and n/2 for this subgraph, which adds up to n, n/4 for this one, n/4 for this one, n/4 for this one, n/4 for this one, which adds up to n. And I don't know if you can see what's going to happen here, but we get n edge contributions at each of these levels, and you may recall that there are log n levels to this tree. At the end of the day, the total number of edges that are created is n log n. This basic structure comes up a lot on algorithm analysis, which is one of the reasons that I was generating the graph this way. What we're going to get into next time is actually running some algorithms on graphs of various sorts, and trying to analyse the algorithms to see how long they actually take to run. So just to recapitulate some of the stuff we talked about in this unit, ultimately, we talked about various different kinds of graphs, various different kinds of growth rates, the whole notion of Big Theta and Big O notation to capture asymptotic growth rates and connecting these things up with various kinds of recurrence relations. There ended up being kind of a nice correspondence between several different kinds of graphs, several different kinds of recurrence relations, and several different growth rates. These growth rates are obviously important for understanding how these graphs work, but they are really important for algorithm analysis, as well, and we're going to see algorithms with various run times that look like these as we proceed. See you next time. Welcome to Unit 3 of CS 215. I'm so glad to have you back! Let's start off with a little magic trick. All right for this trick, I'm going to ask you to write your initials. First initial of your first name and your first initial of your last name on top of each other--like so. My name is Michael Littman, so I'm going to write my initials like this. Then, I want you to mark all the end points and crossing points that are created. All right. What I want you to do is count up all the dots and all the regions and all the segments, and segments in this case are the things connected by dots. So we've got one, two, three, four, five, six and then finally take the number of dots plus the number of regions minus the number of segments and write your answer in this box. You wrote 1. Tadah! All right! So, I hope that worked. If that worked, let's see if we can figure out how. So what we're really looking at here is a graph. Even though I had you write your initials, it's just some arbitrary graph that's written in the plane and we know that it's written in the plane because whenever 2 edges cross each other we've marked that as a node so that technically the edges don't cross each other any more. And then we use Euler's formula, which says that the number of nodes minus the number of edges plus the number of regions equals 2. But you'll notice that I kind of defined region differently in this trick. In Euler's formula, the number of region includes the giant region outside. But here I was just capturing the close regions and so that's why it's 1 instead of 2. So this should always work. One of the things that could go wrong is if your initial somehow just have a loop by itself. So I'm not exactly sure what happens in that case, but I think that we'd end up getting one additional segment and one additional region. So as long as you count this as one segment, I think it's okay. Social networks are just graphs like the kinds of graphs we've been studying. But the real world networks that you actually find have some distinctive properties. And these have been documented by researchers like Duncan Watts and others. One property of these graphs is that they tend to exhibit the small world phenomenon, which essentially means that there are short paths between arbitrary nodes in the graph. Another is that they're cliqueish, which means they exhibit a high clustering coefficient. I'm going to illustrate these two properties next. So this is me essentially. I live in Bernardsville, NJ. This is Chris M. He lives in Wellesley, MA and he's very tall. Finally, this is Chris S. and I'm not sure exactly where he lives but I'm going to say New Brunswick, NJ. So last summer, Chris S. and Chris M. were both at the New Jersey Shore at the same time and they got together for drinks. Chris S. told Chris M. that he worked at Rutgers University. And Chris M. said, "Oh! Do you know Michael Littman?" And Chris S. said, "Actually yes. Small world!" So it turned out that Chris S. was connected to me. We both worked at Rutgers and had worked on a project together. And Chris Metcalf and I went to the same undergraduate institution, and we're friends doing computer sciencey stuff. And here are these two random people meeting in some far away place hundreds of miles away from where they live and they had this connection between them and what we say when this sort of thing happens is it's sort of a small world. Even though there's lot people in lots of places, people are not as foreign to us as we might think. So the classic example of this kind of effect was from the experiment by Stanley Milgram. He very coarsely estimated that any person in the US was just 5 or 6 steps away from any other, and this gave rise to what we now think of as being the idea of 6 degrees of separation. In graph theoretic terms, what we say is that a graph exhibits the small world phenomenon if nodes have relatively small degree but also a relatively short path to other arbitrary nodes, and social network seem to exhibit exactly this property. So just to get you thinking about the degree of different graphs and the path links in different graphs, here's four different kinds of graphs we've talked about before. Clique where all the nodes are connected to each other. A ring where the nodes are connected into a ring. A balance tree where you have a tree structure, but there's a node that kind of separates the other nodes into these two different components that are about the same size. And a hypercube like this three-dimensional hypercube. And what I'd like you to do is think about in an N node version of that graph what is the degree and what is the path length between sort of the longest path link in the graph as a function of the number of nodes. Is it constant? Does it depend on the number of nodes in the graph? Is it logarithmic? Or is it linear? So a graph that has a low degree might be of 1 or log n, and if it has short paths it might be of 1 or log n. So which properties do these graphs exhibit? Well, this is a lot of boxes to check for one quiz but I don't think it's that hard to think about so let's go through this. In a clique, each node is connected to each of the other nodes. So the degree is linear in the number of nodes but think about path links. Every node is connected to every other node. So the path you can always get directly from one node to any other node. A very, very small world but it's a small world because everybody knows everybody. And we know that that's not what our world is like. Now if you go to a small private school you may have exhibit exactly this kind of effect. All right, next let's think about a ring. What happens in a ring? Well, the degree in a ring is always 2. All the nodes have a degree of 2. So it's a constant degree, but what about a path link? What's the longest path link in a ring? Well, some of the nodes are really nearby but some of them you have to go all the way to the other side to get to them and that could be as big as the number of nodes N divided by 2 because it's the opposite side. After you get further and further and further, you start to get closer again. So the path link is actually linear in this case. Balanced tree is really interesting. So the degree here, well a node might have a parent and two children. This is a balanced binary tree in particular. The root node has a degree of 2 and all the leaves have a degree of 1, so it's never more than 3. No matter how many nodes were in the graphs so the degree is constant. But let's look at the path link. Any node can get to any other node by going up to the root and then back down to the other node you want to get to. Getting up to the root takes logarithmically not many steps. And then going back down also takes logarithmically not many steps. So it's like two log n but that's actually a pretty short path for a big graph. It's not that many hops you have to make to get to where you're going. All right finally let's look at our friend the hypercube. In a hypercube, each node is numbered with log n bits and it's connected to all the other nodes in the network that have names that differ by at most one bit. There's at most logarithmically many of those. So each node in the hypercube is connected to log n and other nodes. The degree is a big data of log n. Now what about path link. If I want to get from one node to any other node, what do I need to do? So we didn't talk about this before but it's actually kind of neat. So let's imagine we want to get from some node and here's its label 010 to some other node and here's its label 111. We know in one hop we can go from 010 to 110. And then in one more hop we can go from 110 to 111. In fact, to go from any node in the hypercube to any other node in the hypercube, all you have to do is go through the bit positions one at a time and to reverse the corresponding edge if the source and the target differ in that bit position. So in at most log n hops we can go from any position in this hypercube to any other position. So again pretty short paths. So, that’s all I’d like to say about Path Links for now. Next I want to focus for a little while on the notion of a clustering coefficient. Now, you might be less familiar with this notion of clustering coefficient than you are with paths and graphs, so let’s go through how this is often defined. What the clustering coefficient is trying to capture is, as I said before, cliquishness. How likely is it that two nodes that are connected are part of some kind of larger highly connected group of nodes? So one typical way of defining the clustering coefficient is to start-off by taking about the clustering coefficient for a particular node in a graph, I’m going to generalize this to clustering coefficients for the entire graph next, but let’s focus on a single node. So say, V is that node and Kv is its degree and Nv of V is the number of links between the neighbors of V. So, let’s look at this example graph to ground out some of these concepts. So here we’ve got graph, let’s call this node V. Could point to any of them but I’m going to go with this one. Let’s take a look at the neighbors of V. Those are the nodes that are directly connected to V in the graph. We’ve got this one, and this one, and this one, and this one. So the number of those neighbors is exactly the degree of V, which in this case is 4. And now let’s look at the links between these neighbors of V. So, there is this one that goes between the two pink nodes and that seems to be it. This node isn’t connected to any other pink node. This one is not connected to any other pink node. This is not connected to any pink node and so on. So in this case, Nv is 1. And we are going to combine these quantities together as follows. We take twice the number of neighbor links divided by the degree of V times one minus the degree of V. Now, this may look a little odd but let’s compute what it is and then I’ll explain it. So in this case, two times Nv just one, divided by K times K minus one. So it’s four times three. And the result we get in this case for this particular V is 1/6. So what this represents is actually the fraction of possible interconnections between the neighbors of V. So, because it’s a fraction, we’re expecting it to be something between zero and one. And in fact we can get zero if we’ve got something that’s like a star. So V has edges going out but none of those connect to each other. And it can actually be all the way to one if we have a cleek. So V is connected to its neighbors and all the neighbors are connected to each other. So that’s why this formula has the form that it does, Kv times Kv minus one all divided by two is the maximum number of connections among a set of nodes of size Kv. So, we take the reciprocal of that multiplied by Nv that gives us the fraction that we’re looking for. So, in this case V is a sixth, it’s you know a six of the way from being a star to being a cleek. So it’s not that tightly connected and in fact you know that sort of what we saw as well that V is connected to a bunch of neighbors but they are not really connected to each other all that much. I am going to say that the clustering coefficient for graph is just the average of the clustering coefficients of the nodes in the graph. So if we wanted to compute the clustering coefficient of this graph, we’d actually have to go through node, to node, to node, to node, to node, to node, to node, compute the clustering coefficient for each one and then average. For this quiz, I just want to make sure that you understand the definition of the clustering coefficient. So I'd like you to compute the clustering coefficient for a node in this graph. Now, this graph is inspired by the flight graph for a major imaginary airline in the United States. And it has flights that go between Seattle, Los Angeles, Dallas, Chicago, Pittsburgh, Atlanta, Raleigh-Durham, Philadelphia, and Providence. This is just kind of a random selection of big cities in the US, but with emphasis on places I've lived. As usual, I've drawn edges between cities. In this case, if there is a flight between them. And what we'd like you to compute for this example is the clustering coefficient for Chicago with the airport code ORD. So compute that quantity and write it in the box. All right so to compute this quantity, what we need to do is first take a look at the neighbors of ORD. In this case, we've got Seattle, Los Angeles, Dallas Forth Worth, not Atlanta but Pittsburgh. So Kv which is the number of neighbors is 4. And then we have to count up the number of edges between these neighbor cities. So in this particular case we've got one edge between Seattle and Los Angeles. One between Los Angeles and Dallas and that's it. Pittsburgh doesn't have any edges that connect to any other pink nodes. So that makes this value 2. Now we need to apply the formula for the clustering coefficient to see what value we get from this case. And the formula says it's 2 times the number of links. Divided by the degree times 1 minus the degree. So in this case we get 1/3. Here's a little bit of code for computing the clustering coefficient-- at least one particular way of computing it. Here's our list of flights on our map. Catching between Chicago and Seattle, etc., and I just have that as a list of pairs. And to create the graph, I start off with an empty graph, and then for each of the pairs that constitute the end points of a flight, we make a link between them like we were doing last time. Then to compute the clustering coefficient for the graph at some node, we list out the neighbors. If there's only one neighbor, it turns out to be very hard to compute the clustering coefficient. So we have to give it some kind of value; I chose zero. It actually kind of matters, but zero seems like a reasonable choice. Then what we're going to do is we're going to count up the number of links between these neighboring nodes. So we start off at zero, we loop through all the neighbors, and for each of those we loop through all the neighbors again and then we ask if this pair-- WU--is there a link between them? And if so--in this case I'm counting it as a half because I wrote this inefficiently. It's going to do everything twice. So it's going to notice that Seattle and Los Angeles are connected and that Los Angeles and Seattle are connected. So I give it a half each time so that the total number works out. And then I apply the formula. Two times the number of links divided by length of neighbors times length of neighbors minus one. So now that we can compute the clustering coefficient for any node V we can actually loop through all the nodes in the graph computing the total of their clustering coefficients and then divide by the number of nodes to get the clustering coefficient for the whole graph. So in this case, it's a little bit of a quarter, which is pretty clumpy. So there's a fair amount of interconnection between a node's neighbors. >From what I've read, social networks will often have clustering coefficients that are in the 0.1, maybe even up to the 0.8 range whereas the grid graphs, chain graphs, and ring graphs that we've looked at actually have clustering coefficients of zero. So a useful thing to know about a social network is whether everything is connected to everything else or whether there's some subset of nodes that's isolated from the main body of nodes. To put it in another way: whether any given pair of nodes in the network can communicate by passing messages from friend to friend, from one place to another. So this graph is actually separated into two different components. We'll call them A and B, where A has three nodes and B has five nodes. In this case we can do it just by looking at it, but sometimes this problem is easier and sometimes it's harder. So it's good to have an algorithmic approach so that the computer can do it on our behalf. Just to make sure you understand, here's another graph. How many disconnected components are in this graph? If every node can communicate with every other node by passing messages along the green links, then you should put one. If there's actually two separate groups of nodes within each group, they can communicate but between them they can't, then put two. If there's three such groups, put three, and so on. Just write it in the box. Well, unlike the previous example, this is a little bit harder to see. Certainly it's not the case that there's nice white space in between separating them. It seems to be a little tangled up, but we can figure out the number of components, and one way to do it is we can just start systematically seeing what's connected to what. So starting from this node here, we can see this node can reach this one, and this one, well, and itself, and did we miss any? Yeah, this node, which is part of that set-- call that the purple group, group A-- can also reach this node. But notice that's as far as it goes. All the purple nodes are connected to other purple nodes but not to the nonpurple nodes, and in fact, you can see that the nonpurple nodes can all speak to each other. So in fact, there's two separate, disconnected components in this example. So let's write some Python code to solve this particular example. So here's a translation of that problem and the solution into Python. So first I have a list of the connections in that graph we were just looking at--nodes A through H. And for each of these pairs, I go through and make a link in a Graph G. Then I write a piece of code to list the component sizes for that Graph G, and here's what the output looks like. Then we'll go back and look at what the algorithm is. It goes through and identifies that there's two components. One we'll just name little A because it contains node little A; it has four nodes in it. Then there's a separate component that's not connected to the first one that contains B, and that also has four nodes. In fact, looking back at the graph here, that's exactly what we see. There's the purple nodes, which include node A, and then there's these blue-green nodes four of which are visible here that include B. So it finds the right stuff. How does it go about doing this? Well, it essentially uses the same algorithm that I did when I was doing it by hand, but of course it does it in code. The main routine that we call is called mark component. So that's kind of like what I was doing when I was turning nodes purple, and it works like this: we give it the graph that we're working on, the current node that we're marking, and data structure that keeps track of which things have been marked so far. Call this routine on this node, That's a new node; we haven't seen it before, and so we mark it. So that means we've visited that node, and we say that in this particular call to the routine so far we've marked exactly one node. Then we look at all the neighbors of that node. For each of them, we say if that neighbor of the node that we're looking at hasn't been marked so far, then what we're going to do is recursively call mark a component on that neighbor. It's going to go and get everywhere that it can get to that it hasn't already gotten to, mark them all, and return. What it returns is the number of nodes that it marked as part of this process. However many nodes it marked, we're going to add that to the total number marked. So we marked our current node, then we visited a neighbor and marked all the nodes connected to that neighbor, then we come back and repeat this loop for all the different neighbors of the node where we started. When that's exhausted, it means we've now reached out and touched all the nodes that are touchable from node node. And so we return the total that were actually marked as part of this process and then it's done. So that's the main routine that does all the hard work.. There's also a routine on the outside that takes a graph and starts off saying nothing's been marked so far. We just have an empty data structure. Then what we're going to do is loop through all the nodes in the graph because we don't know which ones are going to be the beginnings of a new component or not. For each one that we visit, we say if that node hasn't been marked in some previous iteration, then we've got a new component. This node that we're just visiting is a part of that component. So we say, "Hey, we've got a component containing node node." Then what we do is we recursively go and mark all the things connected to that node, count up how many things got marked in that way, and then we return. So what we know is there was a component, it contained node node, and it has a certain count to it. When it's done that, it moves on to the next node. A bunch of these have probably already been marked, so they just get skipped really quickly, but eventually it maybe reaches a node that hasn't been marked because it wasn't connected to anything so far. Prints that one out and continues. So this really is very explicitly what we were doing by hand, just one piece at a time. It's important to analyze the running time of this algorithm so we have some idea of how efficient it is. And again, that's going to involve counting up the number of primitive steps, primitive statements, time that the algorithm takes when solving a problem. We'll assume, as before, that n represents the number of nodes in the graph and m represents the number of edges. And it turns out that the basic strategy that we use for this algorithm is a kind of graph search. There's 2 principal flavors of graft search, depth first search and breadth first search. This one that we've been talking about is a form of depth first search. Later we're going to see a version of breadth first search when we start looking at shortest paths. But this particular algorithm is not concerned about the length of the paths, it's just trying to figure out which nodes can be reached by any kind of path. So taking a look at this algorithm here, one of the things that we can note right off the bat is statements like this one, marked(node) = True, can only be executed once for each node in the graph. So even though it's a little bit hard to tell exactly when this is going to execute, we know at the end of the day every node is going to get marked and no node is going to get marked twice. So that means once for each node in the graph we're going to execute this statement and this statement and this loop. And what does this loop do? It goes through and basically visits all the edges that come out of this particular node (node). And for each of those edges, well, it does some work. It checks whether or not it's marked, which we're going to assume is a unit time operation, some kind of nice hash table and it behaves nicely, and a recursive call, and we're going to somehow have to account for all the stuff that happens in this recursive call, but let's not worry about that for the moment. It's going to do this one main computation, this adding whatever value it gets back to this quantity here, then it returns. So these statements here are going to get executed once per node. This statement is going to get executed once per node. This statement is going to get executed once per edge in the graph because for each node it's going to visit all the edges emanating from that node. So even though it's very difficult to keep track of what's going to be executed when, we know that the total running time here is going to be big theta of the number of nodes plus the total number of edges in the graph. So that's what's happening in this marked component. And then what's happening outside here, this marked gets executed once at all, then for every node in the graph it may or may not execute this statement, which does the call to mark component. So we're going to iterate over all the nodes, and again, we're going to do some work but not more than constant work per node. And if we look at our running time here, adding another n in here doesn't change the big theta. Again, even though it's very complex to figure out which statements are executing when, the actual running time is big theta of n + m. It's worth pointing out that this is linear in the size of the graph. The graph consists of some number of nodes and some number of edges, and that's the whole story. So this is actually linear in the size of the graph. With this algorithm in hand, we can actually do some useful analysis beyond just discovering things about the size of the components. So what I'm going to ask you to do is to take the code that I wrote and see if you can call it in a way that will answer this question. We're given a graph G that includes a node v1 and a node v2. We want to call mark_component and use that to discover whether or not there is some path--even if we don't know what it is-- but some path that takes us from v1 through the graph to v2. We can do this by modifying the listcomponentsizes algorithm from the previous example. Call the algorithm check connection, and it takes a graph, but it also takes v1 and v2.It's going to do something similar to before. It's going to start off with the assumption that none of the nodes have been marked so far as reachable. Then we're going to call marked component on that graph, and the node that we're going to start marking from is v1. And what that's going to do when this returns--it's going to return a number, but we're just going to ignore it-- it's going to go through starting at v1 and mark, like basically touch, every node that can be reached from v1. And we want to find out whether or not there's a connection to v2, and that's really easy at this point. The answer is v2 in. So what this does is it checks whether the node v2 that we've been given as input was one of the nodes that got marked when we were spreading things out from v1. So it starts at v1, spreads things through the graph like some kind of an infection or a good idea, and when that's all through, all we have to do is look to see whether v2 was one of the things that got infected in the spread. So according to this, a and g are connected. Let's take a look back at the graph. g is reachable from a, but that's not so exciting to think about because they were pretty much connected directly. Let's try a more interesting one, say h and b. It realizes that h and b are in fact connected, and that's more interesting because it requires multiple hops to get from one to the other. And then finally, we can do a and b. We know that they're in different components because remember, there was component a with 4 nodes and component b with 4 nodes, but let's make sure that the program can figure that out. Yes, indeed. So when it spread things out from a and reached everywhere it could reach, b was not among the visited. So one extremely important operation in analyzing social networks is finding the shortest path between individuals in the graph-- not generally because we need the actual path but because we want to have some sense of how closely connected different pieces of the graph are. But the pairwise shortest path problem is of independent interest. It's actually kind of fun and interesting to see essentially what allows Google Maps or MapQuest or Yahoo Maps to plot shortest paths for you to drive. But another kind of whimsical version of this problem is the Kevin Bacon game. Kevin Bacon is an actor. He's been in a lot of movies. And for some strange reason, he's been in lots of different movies with lots of different kinds of people. And so it turns out to be interesting and not always very difficult to connect Kevin Bacon to any actor you want. So imagine the problem goes like this. I'm going to give you a graph, G, and some node in the graph, v1--call it Kevin Bacon-- and some other node in the graph--let's say Emma Watson-- and we're going to try to find the shortest path between them. In the movie version, the graph consists of the actors connected to each other if they were in a movie together. Kevin Bacon and Emma Watson have not been in any movies together, but there is a short path between them. We haven't designed an algorithm to do this yet, but, fortunately, there is a source on the Internet-- because there's always a source on the Internet-- that will help us with this kind of problem. There is a website called The Oracle of Bacon. There's Kevin Bacon now. And what it allows you to do is solve exactly this problem on the graph of movie actors and movies that they've appeared in. It defaults to Kevin Bacon, but you can actually put in anything you want here. But we're going to leave it as Kevin Bacon for the moment and ask it to find a connection to Emma Watson. You can see that I tried this in advance. And we look for a link and, bang, it found one. It turns out that Emma Watson from the Harry Potter movies was in Harry Potter and the Deathly Hallows: Part 2 (2011) and John Hurt was one of the actors in that movie. Well, it turns out John Hurt was actually in a movie with Kevin Bacon-- or is about to be--called Jayne Mansfield's Car. This isn't possibly the most interesting chain, but one of the things that really is interesting, it's very hard-- at least it was very hard for me--to find actors that are more than 2 steps away from Kevin Bacon. So I tried kind of recent people who you wouldn't think would be in movies with Kevin Bacon, and you find out that they're pretty closely connected. I tried really much older people like Cary Grant. He is also 2 steps away via Walter Matthau. So it's just crazy. People back from silent movies back in the very beginning of movies being made are 2 steps away from Kevin Bacon through people I haven't heard of. So there's something special about the position that Kevin Bacon occupies in this graph. We can find other people. Let's connect Charlie Chaplin and Justin Bieber. We'll see that that ends up being more than 2 links--at least I expect. Three? Four? Three. So Charlie Chaplin--oh!--to, oh, Marjorie Bennett to Fred Willard to Justin Bieber. 1, 2, 3 links. So it could be that the movie graph is just not really all that wide, at least among the people that are reasonably well known, but that gives you a feel for the way that the problem works. Now let's talk about how we can actually solve this algorithmically. To get a handle on how to best find shortest paths in a graph, it's going to help to compare a little bit between the kinds of depth first search algorithms that we were looking at with the kinds of breadth first search algorithms that we're going to need to look at. So let's consider what the check_connection algorithm that we were just talking about does if it's given this graph, G, and we ask it to check the connection between i and n. The flow of control is when we do check_connection on i and n, checking to see whether i and n are connected in the graph, the way that it proceeds is it starts off at i then visits the neighbors of i in some order. Let's say it visits j first and then it asks, "Has j been visited so far? i has." "Has j been visited so far?" No. "Okay, well, let's go to j and do a mark_component from j." It's going to consider the neighbors of j in some order. Let's say it considers k first. "Has k been visited?" No. "All right. Let's go mark_component on k." And so on. So at some point, it may actually check one of the neighbors that it's already visited, but at this point, once it's kind of heading in this direction, it's going to continue to explore the graph this way, and it will eventually hit n. Essentially, the path that it followed to get to n is 1, 2, 3, 4, 5 links long, which is not very representative of the shortest path, which in this case is just the 1 link. So partly what's going on when you run this kind of algorithm is it's diving deep into the graph. It starts off at i and it just keeps digging itself deeper and deeper and deeper and deeper and deeper. And really what we'd like to do is kind of check in circles around i. We start at i and check the things that are close to i before the things that are far from i. And that's the essence of what breadth first search is going to do. So, one of the wonderful things about depth-first search is that it can be implemented really straightforwardly with recursion, which makes it very easy to read and pretty easy to reason about. Not so with breadth-first search. Breadth-first search is going to require a little bit more care to implement it in a way that keeps track of things in the right order. So, as a step towards being able to do that, let's redo depth-first search just the way it was done before, but this time without recursion, and what we're going to do the way that we're going to actually make that work is by introducing a new data structure called the open list. The open list, I would like to think of it as a kind of a "to do list" because it keeps track of what it is I need to do next and essentially, I'm a slave to my "to do list." I've actually had students who occasionally add things to my "to do list" because I wouldn't know that I wasn't suppose to do them, and they get me to do things that I'm not suppose to do. I would not encourage you to be quite so short-sighted, but algorithms actually are going to that short-sighted and so this open list is going to help the algorithm keep track of what needs to get done. So, here's how we're going to do it. We're going to start all the nodes as unmarked so they are all just nice open blue circles, and we're going to put p onto the open list and we're going to follow this procedure here, and the procedure says go to the "to do list" get the thing at the end of the list that has to get done and do it. The way that we are going to do it is we pull it off the list then we go to all the neighbors of the node that we're working on which is p and any that are unmarked, we do two things to them. We mark them and we add them to the open list. So, the neighbors of p in this case are r, s, and q. So, we're going to add r, s, and q to the open list and mark them all as visited. Now, we've done that process, and we go back to the beginning and do it again. We take the ending little piece off the "to do list" and say "Oh, q" that's what we have to do. Go to the graph, find all the neighbors of q in this case just t, mark it and add it to the "to do list." Okay, we're done. No, we're not. We have to go back to the beginning and do it again. Grab the top thing off the "to do list" which is the t, check all its neighbors, q and u. Only one of them has not been visited and that's u. You mark it and add it to the "to do list," and we repeat again. Grab the last thing off the "to do list," which is u, look at all the neighbors of u which are r, s, and t. They've all been marked. Nothing to do. Yes, there is. There is still one more thing on the "to do list." We have to get the s, take the s off the list and check all the neighbors of s, u, r, p, and q. They've all been visited so we don't add anything new to the "to do list." We finished the duration and go back to the top. One more thing on the to do list r. Check r. R has three neighbors, p, s, and u, all of them have been marked. We add nothing to the open list. We see that there is nothing left open and now, we're really done. Ta-da. This procedure here performs just like depth-first search. It goes through and it adds nodes in an order that has to do with diving deeper and deeper into the graph, but it does it without any recursion. Instead of recursion, it just has this open list to keep track of things. All right--here's another graph that we're going to search and this is going to help us illustrate the difference between depth-first search and breadth-first search. So, what we're going to do is we're going to start a search from v and let's see how it proceeds. So, we start up v. We mark v. And our instructions again say that we look at all the neighbors of v, w and z. Mark the unmarked ones after we removed v on the list Mark the unmarked ones and add the unmarked ones to our open list. Now, we take the first thing off the open list which is the z and repeat. V is already marked. Y is not. Add y to our open list. Now, we work on y again. We're always working on the most recent thing. Y says we have to check x and z, those are the neighbors. Z has already been marked and x is not. Take care of x and then just as before and now that everything is marked, it's going to end up popping this off the open list and it finishes. Let's look at the order in which nodes were marked. This was marked first, then this, then this, then this, then this. Mostly, the flow is in the order of deeper and deeper and deeper. All right, this time we're going to do just a tiny bit differently. What we're going to do is we're going to search this graph starting from b and we're going to mark as we go the order in which we've added things to the open list. So, we started off with b, the only thing on the open list, we visit both the neighbors of b add them to the open list, pull c off the open list, add it's neighbors d, b, and we work on d, take it off the open list, add its neighbors e and c. Now, we can take e off the open list, add its neighbors f and, well this is already done. Take f off the open list, add its neighbors g and this is already done. Take g off the open list, its neighbors are all already done, nothing is added. Take a off the open list, its neighbors are already done, okay, we're finished. So, I want you to do this starting from d and make a note of the numbers as you add them and let me clean this up, okay. I want you to try this just like I did starting from d this time and write the numbers underneath, and when you're finished, when the search is completed, take whatever number you have under a, take whatever number you have under g, add them together, and put it in this box here. So, the interesting thing about this problem is you have a choice. Depending on whether you add c or e to the open list first and the first step. So, you take d out the open list, you add c and e, or you add e and c. If you do it in this order, then what happens next is you proceed to the end, and when you hit the dead end, then you reverse and fill in things in the other end. You may get a out of 7th and g out of the 5th as one possibility, in which case you have 12. But that's not the only possibility--you could have also, after you put d off the list, add e and then c because you don't know whether the neighbors are going to be considered, and that'll cost you to actually expand things in the opposite direction--so if you'd done this way, then g would be expanded 7th and a to 5th but you'll still get 12. So, we're going to change this to breadth first search, but we're going to do it really easily, at least for the people version, by grabbing the first element off the open list instead of the last element off the open list. Let's take a look at how that changes things. We start off with d as the first thing added to the list and d on the open list, and now what we do is, well the first and the last element are the same, so nothing changed yet. We grab d off, we mark any unmarked neighbors of that node of d, which in this case are e and c just as before and that's done for this time through. And now, we go back up to the top and things get a little different. This time, well, actually not that different, the order was sort of arbitrary and so first and last are sort of arbitrary. We're going to grab the first thing off the list just like the instruction say and that's e, add its neighbors in, they still get added to the end of the list, so that's a little different in terms of the numbering so far, but now it's where we go back up to the top and we grab the first element of the list in which this case is c, so we're alternating. Instead of continuing in one direction, we're actually expanding outward from d, with the d and then c and e, and then b and f. And now, what happens next if we expand from f with expanded g, add it to the open list, but now, before we touch g, we're always pulling off the front of the list now, we get to b. Right now, we've visited everything. The next thing we pull off is g, we notice that it has no open neighbors, then we pull off a, we notice that it has no open neighbors, and the open list is empty, the search is finish. You can see what happens in this case is we expanded out from d and now a and g, well the sum of the a and g are 13 instead of 12, which is not a really significant or meaningful difference at this point but it is indicating that the search is proceeding differently and the way it's proceeding importantly is out from the center. All the paths that it found, all the edges that it expanded in new shortest distances it was away from the beginning state. So, this is going to give us what we need to actually find shortest paths. And here's a graph--it's kind of a binary tree, starting from h and expanding the nodes using breadth first search, when node o be added to the open list. So, assuming that h is 1 and each of the nodes is going to get added one at a time into the open list, eventually o will get added. What level will it be. Write your answer in the box. What breadth first search does is it always expands out in levels. So h expands into i and j, one and the other first, and then one of the other of these has expanded but with only one level, and before we go any deeper, 3 has nodes, j's nodes get expanded next, and only after everything distance 2 away from h has been expanded, will o all get its chance. To provide a test network for looking at shortest path problems, I downloaded from the Internet a social network, as it were, of marvel comic book characters. It's a lot like the IMDB data and that there's sort of people and they are linked together if they appear together in a sort of movie, but in the comic book world, we're talking about something more like superhero characters in which particular comic books they both appeared in. So for example, Spiderman at least in this set here, Spiderman appeared in a comic book called COC 1 and also in one called MaxSec 3 and Thor and Hulk also appeared in MaxSec 3. So path in this network are kind of like paths in the Kevin Dating Game and that you go from say Black Widow to COC 1 to Spiderman to MaxSec to Thor cause in this set, Thor and Black Widow are not directly connected. No that's not true. Here's the path. So again you can play kind of like the Kevin Dating Game except it's kind of a Spiderman game. What we have here is a Python encoding of the breadth-first search algorithm we've been talking about for finding shortest paths in these kind of graphs. In this particular case, there are some infrastructure in the beginning for reading the data for the comic book characters from a file. The one that we're using here is tens of thousands of lines long. So, it would be very awkward to put it directly into Python. A pretty simple code, we're just going to reading it and then making links into graph G. That graph is stored as marvelG. That's the graph of the comic book characters and the comic books. Then, we've got a subroutine called path (G, V₁,V₂) and what that does is it takes graph G, a start node, let's say a character in the comic book and an ending node in another character in the comic book and it tries t find the shortest path between them. In fact, it will, unless there isn't one. So, in this particular case, the code here is only going to tell us the length of the shortest path. It turns out that's a pretty straight forward thing to do, but it's not that hard to augment this to actually produce the path itself but let's just start of with the--the length. So, we create a data structure distance from the start which is going to map the nodes of the graph to how far away they are from (V₁). We have an open_list, just like we've been taking about and it's initialized to just the very first node, and the distance from the start from that node is zero because it is the node itself. Now, we're going to proceed by checking to see whether the open_list is empty. If it's not, then we go into loop here that says-- first we're going to pull off the very beginning element of the open_list, and call that current. And then we're going to delete the very first element to the open_list, so that it's gone. We are going to talk a little bit later about how to try to make this operation as efficient as possible. But for now, this is--this is okay. It's going to be fast enough, so it takes that off the open list and now what we're supposed to do, we loop through the neighbors, so here we have a statement that says, for each neighbor in the list of neighbors (current), What is it going to do? It's going to check to see whether or not that neighbor's been assigned the distance so far. If it has not been assigned a distance, is because it's not been visited. It's not been marked. And if that's the case, then what we're going to do is we're going to calculate the distance from the start for that node or what is it going to be? The neighbor--the one--the node--the new node that we just discovered is just one step farther from the start than the node that we're expanding. So, we take the distance from the start to where we are now. Add one to it and that's the start to this neighbor that we're considering. So this is really the key step. It is finding the distance of the shortest path. Once that distance has been assigned, we check "Hey, was that the neighbor the one we're actually searching for?" If it is, we can just be done. We can just return that distance for that node; otherwise, we have to proceed and we take that neighbor and we just stick it on the end bar open_list, so that we'll catch up with it later. And then we're back into the loop and it's just going to continue pulling things off the open list. assigning the distances until finally we discover the node they were looking for, or if the open list eventually goes completely empty. All this falls all the way through the while loop is done and it returns false, so instead of actually returning a distance, it's going to return false meaning I wasn't able to find a path. This could also be infinity which indicates that there is no bound on the length of the shortest path--it's infinitely large. This particular case I'm running it with finding the path from a comic book character called A to one called ZZZAX. I figured that was pretty comprehensive if we go from A to ZZZAX. And it's had three or four step chain that you get when you do this with the data that we've got. All right, so let's go through the changes we need to make to actually return the path instead of the distance. What I'm going to do is actually ??? this variable distance from the start. It's not going to be a distance anymore. It's actually going to be the path, so instead of it starting at zero, it starts as just a list consisting of (V₁) by itself and the only other change that we need to do is when we extend it because we've discovered a path from the beginning through a current, now goes to neighbor, all we need to do is that. It says, this--this now being a list, when we add another list to it, it's just depending to the--to the end of it, which is what we want in this case. It might not be the most efficient way to do it but things don't get very large, and it actually works pretty well. So next, we're going to extend this idea of finding a path from V₁ to V₂ to the idea of finding a shortest path from V₁ to all other nodes. And one of the reasons we want to do that is to identify central node in a social network. Nodes that are likely to have a lot of influence, or well-placed, or very well-connected, and so we're going to look now at the question of given a node V₁, how central is it? Now, it turns out there's lots of different ways that you can define what central means. We're going to write now focus on a particular one which is the average shortest path length to all the other nodes. So to find this quantity, we're going to need to calculate the shortest path length from V₁ to all the other nodes. Now, based on the analysis we've done so far, finding the shortest path length from some node V₁ to some other specific node V₂, takes time Θ(n+m), nodes plus edges. So, following sort of a naive kind of algorithm design, well, this should work and it's not so terrible. It's to say, well, if we want to know the shortest distance from V₁ to each of the other nodes in the graph, we can just repeat this process once for each possible target node. So, we find the shortest path from V₁ to V₂ and from V₁ to V₃ and V₁ to V₄ and V₁ to V₅ and we can get all these distances and then say average them to get this measure of centrality. The running time for that approach is going to be big Θ of n, the number of nodes times the time that it takes to do a shortest path search. So, it's really n times n plus m which is (n²+nm). Now, the superhero data that we're looking at the moment has only order of 6000 superhero nodes and 6000² is, you know, a substantial number and we'll be nice to avoid this if we could. So, here are some choices for you, one possibility is you can't, just deal with it. and this is the fastest way to do it which is essentially to repeat individual pair y searches until you've exhausted all the nodes. Another possibility is well you can make it faster by just using a smaller graph. Another choice would be while you're searching for V₁, when not sure what to search for, just search for something that's not anywhere in the graph, and last choice is, actually, you can speed things up by doing the search backwards. Now, actually all these choices sound kind of weird, don't they? But the right one is this third one here and the reason that this actually works, is if you search using the algorithm that we've been looking at, if you search from V₁ for something that's not in the graph at all, what is going to do it's just going to keep searching and searching and searching and searching and building shortest paths to all the nodes it encounters along the way until it discovers that the node that it's looking for just isn't there. At the end of that process, every single node in the graph will have a shortest path from the--will be notated, annotated with the shortest path from V₁ to that node. So, the running time here is just the time it takes to do a single search, Θ(n+m) and then you get the answer for all the nodes in the graph. Now, actually all these choices sound kind of weird, don't they? But the right one is this third one here and the reason that this actually works, is if you search using the algorithm that we've been looking at, if you search from V1 for something that's not in the graph at all, what is going to do it's just going to keep searching and searching and searching and searching and building shortest paths to all the nodes it encounters along the way until it discovers that the node that it's looking for just isn't there. At the end of that process, every single node in the graph will have a shortest path from the--will be notated, annotated with the shortest path from V1 to that node. So, the running time here is just the time it takes to do a single search, big theta of (n+m) and then you get the answer for all the nodes in the graph. All right, let’s take a look at one more path type algorithm that is simple variation of what we’re just looking at. I’d like you to do is modify the path finding algorithm that we’ve been playing with to find the average of the shortest path distance to each reachable node. So, that will give for some node V₁ that we get to the program, it will give us a measure of the centrality of V₁. Okay, here’s the code that we left off with. Let’s make some modifications to this to compute the centrality measure. Program path (G, V₁, V₂) was returning to distance between V₁ and V₂ in the graph in the graph or false if the two nodes are disconnected. We don’t really need that anymore. What we really need is the centrality concept and that really is done with respect to a single starting node V and we’re going to do basically the same search as before. We’re just not going to terminate what we had in ending node because there isn’t any node. So, we start off distance from the start at V as being zero. Once again, we go to our open list, pull the first element off at the current node that we were working on, pop it off the list. We look at the neighbors of that node. If this neighbor is a new neighbor, then we make a note of it’s distance which is distance from the start to the node we just where plus one to get the distance to the neighbor then we put it on the open list and take out this line where stopped the search and when we’re going to return at the end of all these. Well, once this is loop is finish, once the open list is exhausted, we visited all the nodes that were reachable and put their distances from the start state into this data structure distance from the start. So now, what we really need to do is average all these values of distance from the start. There’s a lot of different ways that you can do this but Python give you at least one way that’s very sync, so I’m going to use that. You can also do this by setting a pair from loop but what it is in this case is I take the other values in the distance from start date of structure. If the return has a big long list, sum up all the elements at list, add zero to it, 0.0 to make sure that it’s a floating-point number, and then divide by the number of nodes for which we got a distance. So, these are all the reachable nodes. So, ultimately, this is going to return the average of the distances of all the reachable nodes, which is what we are looking for. One little here and we should be good to go. Here's one last topic, I'd like to cover in this unit. And again, it's related to the analysis of graphs and paths, and using variance of depth-first and breadth-first search to answer questions about the graph. Here's a little graph that I drew and one thing that we might notice is that, everybody is connected to everybody else, but there is this sort of weak edge. This is the weakest link, hello. If this edge were to be removed from the graph, like so, the number of components in the graph has just gone up by one. Now, that was the only edge in this graph that has that property. There's other edges that if we take them out, things are still connected. So, in some social networks, it's useful to know, are there any edges in this graph? Are there any pairs of people in the graph that are connected that if they were to say, stop being friends, there would be disconnection in the network as a whole. Let's put this edge back in and label the edges, sorry, label the nodes of the graph so we can talk about them. And we're going to carry an algorithm back to 1974 by Robert Tarjan. Tarjan is responsible for some of the most amazing graph algorithms. A lot of which are based on the ideas of building trees, spanning trees using depth-first and breadth-first search. Back in college, my friend ??? and I used to refer to him as Tarjan, Lord of the trees. So the basic idea of this algorithm is we pick any node in the graph. Let's say B, and we start to build a tree starting from B, and here's how we do it. Starting from B, we look at all the neighbors of B which in this case are A and C, and we haven't build any of them into the graph yet, so we can add them into the graph. And now, we do essentially a search just as we're doing before, consisted of A and C. So from A, we look at the neighbors of A. The neighbors of A are B and C. Now, the BA edge is already in there, but the BC edge is not. So, we're going to add a special kind of edge because this is an edge between two nodes that are already existing. We'll call this a non-tree edge. So, what we're going to do is we're going to take this entire graph and express it essentially as a tree, but some of the edges aren't going to be part of the tree. We're going to have to add them in separate, all right. So, that takes care of all the neighbors of B and the neighbors of C. We have B which the edges are already in there and we also have an edge to E which we haven't seen yet. We add that edges and then we proceed from E. Now, E has three edges back to C which we already have and also to D and G. Those are new nodes, so we build them in with three edges and we continue our search now, let's say from D and D has two neighbors, E and F. E is already in there but we can add in F at the tree edge. We look back at G, G has neighbors F and E. The E one is in there and the F one is not, so we add that non-tree edge in there, Because F is already in the diagram but that edge was no not. And then from F, there is connection to D and G and those were already in the graph. So basically, what we've done now is we rewrote the original graph as a rooted tree. So starting with some node and then proceeding downward and then any edges that don't really fit that pattern we put in is, we put that dotted lines. So, we rewritten the graph as a tree and the next thing we need to know is post order the nodes. Now, post ordering is something you can do to any tree structure, and it basically involves assigning a number, a sequential number to each of the nodes in the--in the tree and post order means is, what we do, is to assign a number to a node, we first make sure that all the children on its left are numbered. All the children on its right are numbered, children and descendant. Direct children also and their children's children and then the node itself gets a number after that. So, these number is going to come from first numbering A, well A has--and then-and then everything below C and then we can number B itself. A's number comes from neighboring--numbering it's children, which it doesn't have any, so it can get whatever the next number is, which in this case is 1, all right. So back to B, we have to label all the--all the children on it's left and all the children on it's right. We haven't done that yet, so let's do a numbering starting from C. Well C, before it gets a number, has to have all it's children numbered and we can see that they don't have numbers yet, so we have to proceed down the tree. E needs a number. Well, E can't get it's number until it's children have been numbered but those haven't been numbered yet, so let's number those. D--D should get a number but first it's children have to get a number. This is a, you know, children first sort of scheme. These children has one child and that's F. For F to get a number, it's children, oh it doesn't have any children yet. All right, so F can get the next available number and in this case, it's 2. So now that, that's finish, D can get a number because all it's children are numbered. So, D gets the next available number and that's 3. E, no E still can't be numbered because it's children haven't been all numbered yet. We have to go to G first. G has no children, so it can get the next available number. Now, E can get a number. Post order in this case is, post meaning after. After all the kids, the node itself can get numbered. There's also pre-order and in-order, but we're just looking at post order for this algorithm. All right, now C, all it's children have been numbered, so now C can get it's number. B, all its children have been numbered, so now B can get it's number. So we've now post ordered all the nodes in the graph. So, just to make sure that you're understanding that process, here's a different tree. This time, the branching factors are a little different in some places. And I want you to post order this tree and then type in this box the number assigned to node P once you've done that. All right--so just to repeat the process again--what we do is we always the number the children in order and their sub-children before we number the node itself. So what that's going to do in this case is--we're going to proceed down the left branches and sign those nodes before the parent. Now, once again, 4, 5, 6, 7, and once again, 8, 9 and now we can do j at 10. Alright, so I’ve redrawn the graph from the previous examples just to make a little bit more room and what we’ve got so far is the original graph and the – a tree version of the graph that we built by search, some kind of data depth-first, breadth-first – any kind of search actually will do, as long as we now have these green edges pointing downward and these red no-tree edges connecting nodes that need to be connected if the edge wasn’t actually part of this tree. Then we post ordered the nodes and now what do we do next.We now compute the number of descendents for each node in the graph and the number of descendents is the number of nodes that are either the node or reachable from the node by following green edges only. So this is easiest to start at the bottom here, F has just the node itself and no descendants, so it’s got a number of descendant of one, same thing with G.E has one descendant, one descendent plus itself for three. D has the three descendants of E plus itself for four. C has the four descendants of D plus itself is five. B has just itself and A has B’s descendants, C’s descendants plus itself for seven. So the next thing we’re going to compute is for each of the nodes, we’re going to consider the set of nodes that are descendants of the node or reachable from the descendants of the node by one hop of a non-tree edge and of all those nodes that are reachable, we’re going to look at the post-order value that is the smallest, we’ll call that L. So in the case of F, F has itself and it also can reach G, the smallest value there is two. G has no descendants but it can reach by one red edge F. So the smallest least values two and three is two. E has F and G and itself that it can reach no other nodes by non-tree edges and the smallest of those numbers is two. D can reach all of these nodes in itself but also by a non-tree edge it can reach this node and the smallest of those numbers is one. C can reach all of these nodes and through one of those nodes one tree edge gets it to one. B values itself as one so that’s going to be the smallest. And A includes everything, so that’s going to include the one, so that’ll be the smallest. H is the same idea except of those that same set that we computed for L, we want to know the largest value. For G between F and G the largest value is three. For E between E, F and G the largest value is four. For D, D,E,F, G and B reachable by a red edge, the largest value is itself five. C same set for adding in C now and the largest value becomes six and A set contains everybody so that’s going to include itself with the value of seven. Alright, now we’ve decorated these nodes with lots of interesting numbers, now we can get to the last step. So given all these nicely decorated nodes, we now have a rule for figuring out which ones are the bridge edges and this is the rule. All the bridge edges are ones that are green, they have to be three edges and the green number is less or equal black number and the red number is bigger than the blue number minus the black number or in words the – each value has to be less or equal to the post order value, the lowest value has to be bigger than the number of descendants minus the post order value. So apart from the fact that it’s not immediately obvious why this is the right rule let’s apply it to the tree to see what gets picked out. So, what we’re doing here is we’re looking at each green edge and we’re checking the numbers on the downward part of the edge. So here is a green edge and let’s look at the numbers here and see if they fit the pattern. We need the green number to be less or equal to the black number. So that doesn’t work, right. Here is another green edge. You need the green number to be less or equal to the black number, oh that works. So the red number needs to be bigger than the difference between the black and the blue. So black and the blue the difference is two. The red number is two, so that doesn’t work. Alright next edge to check is this one, so we need to check is green less or equal to black, yes. Is red bigger than black minus blue? Two is bigger than one, so yes. So this edge has been identified as a tree edge. That’s the right answer and none of the other edges are tree edges but let’s just check them just to be sure. For this edge, we have – we need green to be less or equal to black which we have. And we need red to be bigger than the difference between blue and black but one is not bigger than one, so this edge is out. How about this? We need green less or equal to black, yes. We need red bigger than the difference, no. Green number is five because the D node is reachable by a non-tree edge. Alright, so green less or equal to black, no. Alright, well that’s it. Those are all the edges. So in this example anyway the one edge that the algorithm finds is exactly the bridge edge. Can we get a handle on why this works? It seems kind of magic at this point. So the first thing to observe is that no non-tree edge can be a bridge. They have to be the green edges not the red edges, and why is that? Well, the red edges are the ones that get added for a node that had already been reached by some other path. So the red edges can't be bridge edges it's because deleting the red edge--the node from both sides of these were reachable from the beginning some other way. Next, we can test whether or not a given node is in the subtree rooted at--at some node W, pretty simply using the numbers that we've already calculated. Post order of W, the number of descendants of W, and the high and low values and why is that. So when we go to do the post ordering at W, we go it down here, we number somebody, then we number everybody, then we number W itself. The post order number of everyone in here has to be less than the post order of the value of W but the total number of numbers in these set they're all--they're all continuous, they're all in order and the total of numbers--numbers in this set is blue is the number of descendants. So black minus blue is one smaller than the smallest number in this collection. That property is kind of obvious, perhaps, but we were almost there. The only way that this edge is not a bridge is if there's some non tree edge that takes us out somehow and connects to something else in the tree. Now what would happen to the H value or the L value if we had one of these edges. Remember the H value and the L value were defined specifically to be well, you follow a bunch of green edges and then you follow one red edge. Well, if a red edge takes us to in a sort of in the left of this tree, then the in order value is going to be outside of this range and it pretty good will be too small and that we'll tell us that we've link to something outside to the left. If we link to something outside to the right then the H number is going to be bigger than the in order value of W and similarly if we hop up to high that same thing happens it's because that the blobs in here, the nodes in this blob all have exactly the values in here. Anything that connects outside of their is going to cause either the H value or the L value to pop outside of this range. So with those values in hand, we have a very easy test to see whether or not there's some red edge that will actually connect up the graph in a way that this is no longer a bridge and that's all we need to do. Now in the homework, I'm going to ask you to actually carry out this test on a real graph. Actually code it up and it's not terrible but it is a little bit tricky. You have to be very careful to figure out how to compute the post order values the number of descendants H and L without taking a lot of running time in the graph. Really needs to be able to do it essentially with one or two sweeps through the set of all the nodes and edges in the graphs. But you can do it--I have confidence. Social networks have been around as long as there’ve been people forming communities but the modern study of social networks is different because it’s now possible to put together social networks that include data on millions of people. Now that’s more than any one individual could possibly hope to understand in detail. Unit four explores the notion of statistics, numbers that summarize complex structures like social networks. We’re going to take a look at how we can compute statistics quickly, so we can get a handle on the biggest social networks around. Here we go. Hi and welcome back to unit4 of CS215. We call this unit putting things in order or who is your 17th best friend. So, by now, you might not be surprised that I actually have a magic trick worked out that is, actually, in this case relevant to the topic at hand. So, in this magic trick, I'm going to put a bunch of animals in order with your help. So, here's some data, I have it stored as a Python structure at the moment. We caught animals--it has 11 animals in it, alphabetized dog to sparrow, and I've gone on the Internet and looked up the speed, how fast each of these animals is; it's weight, approximately in kg; Life span, how long they tend to live, and how big their brains are, measured in g. I did not go and measured these animals' brains. So, pretty common operation in social network analysis and lots of other problems is to be able to put items in order. So, in this particular case, we have a bunch of animals, we like to put them in order, but there's lots of different dimensions on which to order them. What we're going to do is we're going look at a linear combination of these numeric features, and you're going to pick from me the weight, the positive weights, to put on each of these different attributes and then we're going to take a sum, in fact, I got the code right here for you to look at. What I'd like you to do is run this code on this data and what you do is you call importance rank on this data set animals and you give it the weights. I gave a weight of 1 for the speed, 0.4 for the weight, 2 for the life span, and 0.1 for the brain mass. So, if you would run it with this weights, it would actually compute a score for each of the animals and then print them out in sorted order, but I want you to give your own weight. Don't use mine because that wouldn't be very tricky with it. Any weight you want just as long as they are not negative or zero. But the program is going to print out are the numbers 0 through 10 for the 11 different animals and I want you to make a note of, I don't know, let's say the fourth highest in the list, the one that's associated with the number 3, and I want you to make note of which animal appears in this position and mark the corresponding circle. And what makes this as a magic trick is that the algorithm that I'm using, the running time is so fast, that it's Î¸(-n), that is to say I can compute the answer to this question before you've asked the question--so, I've already run this. I know what answer you're going to get, but now, you need to run it to make it so and let's see if you're right. And the answer is rabbit. Tada! I figured that was a good animal to use for a magic trick and have it come out of the hat. But how did that work? There wasn't anything tricky about the code itself. It really is literally doing what I said, which is taking the weights that you give it, multiplying them by the feature values for each of the animals computing a score and then sorting. Here's what happens when I gave it the values that I just had in the code here, and you see that just like the numbers that you used with the numbers that I used, rabbit ended up in this fourth position and in this position, that's labeled no. 3. But you can change these values. You get a different ordering for everything, but you always get rabbit coming out in that position. And so that's the little bit of trickiness, right? I had to set things up so that rabbit would always come out in exactly this position. The other ones can change values, but rabbit can't. And the way that I did this is by making sure that in all of these lists all these different feature values, the rabbit was always the fourth highest and it was always higher than mouse, frog, and sparrow and always smaller than dog, shark, lion, horse, human, hippopotamus, and elephant. They can have different orderings relative to each other, but they always have to have the same ordering relative to rabbit. And so once we've actually combined all these weights together, what we'll find is that whatever the weight is that the total score is going to be for rabbit-- for example, horse has to be bigger because it's bigger on every single one of the dimensions when we take this non-negative weighted combination-- it's going to keep that property as it is and that's the magic trick. There's one other thing I could've done, but I didn't do, which is to have two different animals who always have exactly opposite values on some feature. That no matter what combination I used, they're either, let's say frog and shark, that either frog is below and shark is above or shark is above and frog is below. But I wanted to stay as close as possible to the actual values that I was able to find on the Internet. As you might imagine, these aren't actually the values that I found. I found values and then I tweaked them to make sure that the property held. So one of the areas where this kind of ordering process takes place in social network analysis is in measuring the centrality of different nodes. We talked a little bit about centrality in the previous unit, but let me talk a little bit about some different types of centrality that you might find useful. One that we're going to return to, and we've already kind of touch on is this node of closeness centrality. The idea that a given node is central if it has short path to other nodes in the network. Between this centrality has to do with the idea that a node is central if most of the shortest path between arbitrarily chosen pairs of nodes in the network end up having to pass through the node in question. It's between most to the other nodes. It's kind of stuck between them. Node centrality is actually a much simpler concept, which says that the node is central if it has lots of edges If it's very well connected, and this is well connected in a very local sense just in terms of the degree of the node or the number of neighbors that it has. Let's look at this one first and then we're going to combine what we learned with other kinds of path base metrics next. Here's an example of a graph--imagine this is a social network graph, and we're going to look at the degree centrality of the nodes in the graph. Let's take a look at node 5. Node 5 has a degree centrality of 2 which really just means it has a degree of 2--there's 2 edges coming out of it. What I'd like you to do is go through the nodes in this network and figure out which node has the highest degree centrality in the network and then write your answer in the box. This isn't too hard--we just make a note that--degree centrality just means that degree which node has the highest degree. Well, node 2 has 3, node 4 has 3, node 6 has 4--that's the highest, node 6. One of the things we'd like able to do is to compute statistics on lists. Imagine that we've got a set of n nodes and we've got L, which is a list of values-- one for each of the nodes in the network and we want to compute statistics. What are statistics? Astatistic is actually quite a simple idea. It's just a number that summarizes a list of numbers. If we have a list L--say a list of the centrality scores. There's lots of different statistics that we can imagine that would summarized these list of numbers--for example, how many numbers are in the list. What's the largest number in the list? What's the total of all the scores in the list? How many scores are in the list that are between 2 and 3 inclusive? These are all different statistics and some of these are may be more useful than others but there's lots of different statistics that we might want to compute. In general when you're doing an analysis of large structures like social networks, we need some way of summarizing this large amount of data--you can't just present the data in the raw form--it's too much for people to think of all at once. Statistics and computing statistics ends up being a really important operation. For reasons that I don't really understand, lots of really important statistics start with the letter M. Today's statistics have been brought to you by the letter M. Here are some of the statistics. Maximum is the largest value in L, minimum is the smallest value in L, the midpoint is the average of the max and the min--sort of the halfway point between the largest value and the smallest value. The mean is the average of the values in L--we all say mean sometimes or average sometimes also expected value--basically it means the same thing. The mode is the most common value in L if there's repeat. The median is say the middlest number, the middlest value in L--basically the numbers are that half the numbers are bigger and half the numbers are smaller and that there can be ties, so we'll have to be a little bit careful how we define that. But essentially it is the value that would be in the middle of the sorted list. And all these different notions of statistics all give us a way of taking a really long list of values and then grinding them together to make a single number that fits comfortably inside of our brains. Several statistics we can compute in a pretty straightforward way if the list L is sorted. Let's imagine that these are list of values that we're concerned about and I put little pointy things on it to say that this is now sorted from smallest to largest. If this L is sorted the minimum value is sitting here in the very first position--so it's L(â‚€), the maximum is sitting all the way at the end here--L(n-â‚ ), midpoint is just the average of those two--so we can look up the minimum, the maximum add them together and divided by 2, and the median is just whoever is sitting in the middle of this list once it's sorted. In this case, all of these operations run in Î˜(1) constant time, given that the list is sorted. Things like the mean or the mode aren't quite so straightforward to do on a sorted list like this but these four we can do very, very quickly. Just to practice this idea, here's little quiztistics. Here's a sorted list of two digit numbers. I want you to find the midpoint and the median of this list and then tell me what's the mean of those two values. You can write it in this box. As I said we should be able to do all these pretty easy--constant time. The midpoint is the smallest number plus the biggest number divided by 2 so the average of those two. That's what? 114/2 which is 57. The median is the number in the middle of the sorted list which let's see--51. We've got one, two, three, four, five smaller and one, two, three, four, five bigger--so good. And now we want the mean of these two number, which is 108/2 or 54. What do we do about computing statistics like those in an unsorted list. Well, plan A, especially for mathematical-minded people, might be to sort the list first, which as we say reduces the problem to one previously solved so that is safe. We know how to sort. We can sort the list and then apply the ideas the we had just been talking about. We're going to be talking more about sorting in a little while, but for now, the important thing to know is that the running time of sorting is Î¸ (n log n) for a list of length n. this is under the comparison model, but the important thing to make note of here is that it's worse than just running through the list, right? You have to run through the list repeatedly log n times essentially and that may be overkill in some cases. If you're interested in a very quick and possibly entertaining introduction to sorting, I've got a music video on the topic called The Sorter, which you can find on YouTube, we'll try to send the link. Plan B is to try to extract the statistic that we're looking for in effectively one scan through the data so Î˜(n) and the time that it takes to zip through the list looking at the values we should be able to get certainly the min and the max. It turns out that we can actually get the median as well, which is really interesting result. Let's start off with the very straightforward one. The mean of a list with the average of a list this is how a statistician might write it. Âľ or the mean is the sum over all the elements on the list from 0 to n-1 We look up the value, we sum them all together, and then we divide by the number of values. It's just the definition of an average, and it's really straightforward to do that in Python as well. This is a pretty straightforward thing to write in Python. Let's imagine that we've got a list L--I made a list which is the node centrality values from one of the recent graph that I used as an example. And we're going to define a subroutine mean that takes a list and initializes total to 0, runs through all the elements in the list adding up their values, and then returning the total divided by the length. This really is just a straightforward application definition with this kind of intermediate accumulative variable. And so if we go and print the mean of this list, it calculates it as 2-2/3. Here's that subroutine again just copied over, and I'd like you to answer the question what is the running time of this little i with the mean. Is it constant, log, linear and logn or quadratic. I'm hoping if you've been paying attention that that was pretty easy to see that that's linear. It's just running through each of the elements in the list and doing constant work for each of those elements so the total of line time is going to be Î˜(n). Now, Python being Python actually gives lots of different ways of writing something this simple, and some of them are very very distinct so here's a more distinct way of writing the same thing. We can do is in one command sum up all the elements and divided by the length, and we've now computed the same thing twice. What do you suppose the running time is of the second statement here? It's just one little arithmetic statement. It seems like it should be Θ(1), but it's not. You have to keep in mind that this some command here is actually just shorthand for a loop like this. It really is running overall the elements in the list internally. Even though it looks like Θ(1), there's still a Θ(1) of the length of this list, your Θ(n). This is one of the things that is different between the language like Python and say a language like C. C basically doesn't allow you to hide running time in the individual statements. All the C statements run pretty much constant time. Whereas in Python, so that you can write more complicated programs in a nice distinct way, they do give you the shortcuts, but you have to be really careful that you know what the running time is with all the little primitive functions before you try to actually measure the performance and predict how fast your algorithm is going to run. This can really bite so you have to be careful. The extremals of a list are the extreme values, the max and the mean, which is a kind of cooler way to say it, the extremals, the X Games, but it's actually an awful lot like mean. What I would like you to do is actually modify the mean algorithm that we had to actually compute the max of a list. All right, here is our algorithm before for mean. We're going to change this to work for max--max as it turns out is the name of my son because I just find the stuff really important. In the very deep sense, the thing for computing is functionally equivalent to the mean. It's really very, very similar kind of calculation that's happening, but the details are going to end up being a little bit different. We're going to start off instead of the total of zero sort of a bit the max-so-far being whatever the first element of the list is, and then, so we should make sure that we don't compute that one twice then we're going to consider all the other elements in the list, and if the element that we're currently considering beat the max-so-far, then we have a new max-so-far. There can be only one and that's what we do, we just apply this task to every element to the list keeping track of the max-so-far, and then, when we're done, the max so far is the max-so-far of everything--woohoo! Now, perhaps not surprisingly Python--you don't want to do this. You don't want to write your own max routine because Python already has one, but just as before, you have to be really careful when you use things like max in your programs to keep in mind that this is a linear time operation--it's not constant time. Finding things like the max of a list or the min or the second largest or the second to last smallest, they are all examples of what are called order statistics. Statistics summaries of the values in a list that depend on the order that they are sorted. This notion is generalizing min and max, so the min can be expressed as order statistic 1. The second smallest element in the list is order statistic 2. The median is order statistic like n/2 floor. The second largest is order statistic n-1, and the max is order statistic n. To give you a chance to play with this idea a little bit of order statistics, I'd like you to write some Python code to find the second most popular name. I'm going to give you the list of the most popular names given to babies in the US born in 1995, it's a yob, for year of birth, 1995.txt and if you take a look at the file, you should be able to download this and take a look at the format. It's pretty simple, the name and the sex and essentially a measure of how popular the name is, and what you want to find is the second most popular female name given in the US in 1995. Once your program is completed, it should be able to output the name, and I want you to just check off the name that your program returns. All right, so I've got an interesting collection of names here. I had to include Michael because it is a relevant name to me, but in fact, in 1995, it was the most popular name given in the US, more popular than any of the names given to any of the women and in fact, Matthew is the second highest male name, but I didn't ask for male names, I asked for female names. The most popular female name given was Jessica, and I think that was actually most popular for a good solid decade or two. That's been very popular for a very long time as had Michael been in decades and decades and decades. This is no longer the most popular given name, but it was for a long time. The second most popular female name is Ashley. Well, Zuzana was among the least popular, and Taylor was pretty popular and pretty distinctive to that year but not the second most popular in that year. Here's some code for finding the second largest name in that list. It starts off opening the file that has the year of birth 1995 data in it and instead of having just one max value that they're keeping track of we keep track of the max name and the maximum value we've seen so far and the second highest name and value that we've seen so far. I initialized to zero because all of the values in this file are greater than zero, actually five or greater, and then, we loop through the lines in the file, for each line, we pull out the name, sex, and count using rsplit and make sure that that count is not a string anymore but an integer, and we check to make sure the sex is female because that's the way we're doing it for this question. If the count of this new person that we just encountered, this new name, is bigger than the best we've seen so far then what we're going to do is take what used to be the best so far and move it to the second best. What used to be the best is now going to get filled in with the current individual. On the other hand, if the count is not bigger than the largest we've seen so far, but it is bigger than the second largest we've seen so far, then we just replace the second largest we've seen so far. When this loop is all completed, and we've ran through all the names, we just print the value and name that has the maximum and the value of the name has the second maximum. We can run this, and we can see that Jessica is the highest valued name, and Ashley is the second highest. This brings us to a slightly more interesting problem called the Top K Problem. Here's the idea. Imagine we've got our list of values L, and there's n values in the list. We can compute the max or the second max, the second largest, just like we've been doing, but in some sense, it doesn't really give you a very representative sample of what the list is about. Sometimes, it's nice to have a bit more. Let's imagine that this list is sorted though it turns out at one half to actually be sorted. And imagine, they're going to focus on just the ones that would be at the k best out of the n. It could be the k small as to the k largest depending on the particular application that you're considering. But knowing a bunch of the ones near the top instead of just the very tippe top can very instructive. One natural strategy is to iterate the max procedure so go through the list, find the max, pull it out of the list, so the list is now in -1 long and then repeat this. Each time we compute the max, it will be actually the second, the third, the fourth largest from original list, but it's always the max of the current list. The simply pretty reasonable thing to do especially if k is small. You just need one or two or three at the top values. Another natural thing to do is just sort the whole list. Once we sorted the whole list, the first k elements of that sorted list are exactly the ones that we want. This is actually a very simple way of doing it. In Python, it can just be essentially one or two statements. Insertion is another strategy. This is the generalization of the solution that I gave for the find the second best example before. In that example, I kept track of both of the best and the second best, and then it was running through the list. In each new element of a list that was encountered, figure out where it would fit into that top two lists. If it was smaller than both of those, ignore it. If it's bigger than both of those, put it at the top of the list and bump the list down and so forth. As always, we're trying to find the best algorithm that we can use to run on our data. Which of these algorithms has the best Θ? And I'm going to give a hint. We're looking for the top k out of the list of n elements, but we're going to look at this question for different values of k. In particular, if k is half of the list, we want the largest half of the list. The question is are we better of using selection or insertion? Both of these are going to have the same Θ for all these examples so I just grouped them together or is it better to sort the whole list keeping in mind that sorting is a Θ of a log n operation. Let's say you want to find the best root and items. Is it better to use selection or sorting? If you want the top log n items, which one is better? This function is the logarithm of the logarithm of n. As slow growing as logarithm turns out to be, this is even slower or growing, the logarithm of the logarithm of n, so which one would you choose? In that case, n finally, let's say you want the top 100. This is a common thing that you see on the internet a lot, the top 100 movies, the top 100 Beatles songs, the top 100 times I've used the phrase top 100. There's a lot of different contexts in which this appears. And if you're going to do that, are you better of using a selection or insertion algorithm or to sort the whole list? What I would like you to do is for each of these columns, check this box or that box or if they are Θ of each other. In other words, from an asymptotic standpoint, in this algorithm, we'll run the same worse case asymptotically then check both of the boxes. We're trying to figure out what the running time is asymptotically Θ for selection and insertion as a function of n and K and then we can substitute in the value of K. If there's n elements in our list and we're interested in the top K, what the selection algorithm does is it goes through the elements one at a time and for each of these n elements it tries to figure out where in this list of K it needs to go. For each of these n, it does an insertion or a selection. Okay, let's do insertion first. It does an insertion in this list. That insertion takes Θ(k). It's basically running through this list, figuring out where the value goes, sticking it in, and shifting everything else over. The insertion algorithm is going to take n times that for a total of Θ(nk). Now, selection is the same. What happens in selection is you run through this list of n, and you pull out the maximum and delete so that takes Θ(n) and you repeat that once for each of the elements you want to pull out. It becomes also Θ(nk). Sorting on the other hand sorts this entire list, which I said before was n log n. Then, well, it's kind of done at that point but you could imagine also looping that through like the first K elements but we basically have them at that point and K is smaller than n so asymptotically that's the whole running time. Now, if K is n/2 that's n times n/2 where n². This is Θ(n²). This is Θ(nlogn). We're much better with n log n. With the very long list that were wanting to tap values of, you might as well just sort the whole thing. √n, well what happens with √n? √n where look we're comparing n log n to n(√n) which is n³/². n³/² is asymptotically larger than n log n so we're still better off just sorting the whole list. Now, let's go to the other side of the spectrum. What if there's 100 elements in the list? Well if K is a constant, K is 100. Then the big theta of this is just Θ(n). It's a linear time. So linear beats nlogn. So we'll definitely want to choose these algorithms over sorting if it's a small number of elements that you want to know. loglogn is similar. We end up with n(loglogn), which is better than nlogn. And finally I guess it'd be kind of obvious at this point if we substitute logn in for K, we have nlogn for selection insertion but we also have nlogn for sorting so they're actually the same. And what's cool about this algorithm is it finds the Top K elements in no particular order in Î˜(n) independent of K. Now remember, the best that we could do for the previous algorithm is if K is large we ended up using sorting which is n log n. This is always Î˜(n) or at least expected top running time. The worst case can be bad, but there's a way of fixing that as well. This is pretty remarkable, and you might find this to be quite counter-intuitive. For example, if the list has n elements in it, it takes us n log n time to sort it. How could it take us less than that to find the Top K if k = n? Well, it's actually this phrase here that's really important. In no particular order. Basically, if you ask me to find the top 100 elements in a list of like 100, I don't have to do anything, right. I can just give you back the original list even if it's not sorted. The top 100 is exactly these elements. This is what you asked for. That's why this is not actually equivalent to sorting. It can be actually much more efficient than sorting. Now, what happens in the case where we're looking for the top half? It seems a little trickier, right? In the top half case, we can still do this in linear time even though sorting the whole list is the natural way to do this to split the top half and the bottom half and that takes n log n. The key step to this algorithm is going to be the following simple subproblem. Kind of interesting maybe but a slightly counter-intuitive problem. And that is finding the rank of a given value in a sorted list. We're given L unsorted and we're given the value of V and we'd like to find out where V would appear if L were sorted. It's not sorted but if it were sorted where would this value V appear on the list. Now just for the simplicity of the discussion for now let's assume that all the values in the list are distinct. We're imagining here we've got our list L and we pick out some value V. It doesn't actually have to be in the list but let's say it's in the list and what we want to know is where would V appear in L if L were sorted say smallest to largest? Now, one way to solve this problem of course is to sort L and then look for V. But the insight here is that we don't need to do nearly that much work. So how do we do it? Let's look at a concrete example. Here's L. This is an unsorted list of unique numbers. They're all 2 digit numbers just for my own sanity in writing them down. And then this is the sorted list L. It's the same elements but I've put them in order. And let's consider number 66 say. So here's 66 in the original list. How can we figure out just by scanning through L where 66 is going to end up in this list? Well what makes it end up there. It's in position what? 0, 1, 2, 3, 4, 5, 6, 7, 8. It's in position 8. So why is it in position 8? Well, it's in position 8 because there are exactly 8 elements in the list that are smaller than 66. Right? Because once it's sorted, those 8 elements are going to be to its left and that's what's going to put 66 in the 8th position. We can just go through this list making note of the elements that are smaller than 66. 31. 45. Not 91. Yes 51. Not 82. Yes 28. Yes 33. Yes 11. No. No. Yes. Yes. For a total of 1, 2, 3, 4, 5, 6 , 7, 8 elements. That's how we can know where 66 is going to be in the sorted list even if I don't tell you the sorted list in advance. Here's a little bit of Python code. Very simple to do exactly this. Here's our list from the example, and we want to know where 84 will fall. What position 84 will be in in the sorted list? All we need to do is count. We start off with this pos variable set to 0. We loop through all the values in the list. And for each one we say, "Well, is it smaller than the value we're asking about?" If so, we increment the position and sort of bump it one space to the right in the sorted list. And then when we're done, we'll just return in that position. So in this list, 84 will be in position 10. We're going to use this idea now to do something a little more clever. Let's imagine we've got some list and it has some value V in it. We know now how we can actually figure out where V will go in the sorted list. Let's actually put it there. Let's actually put that value in the list at that position. We'll generate a new list Lâ€˜ and we're going to put in that list what's going to be a permutation of these values. We're going to shuffle around these values, but we're going to put V in its final resting place as it were the final sorted position and make sure that to its left are all things less than V and to its right are all things greater than V. And apart from that, we don't care what the order inside is. It's kind of a partially sorted list, and we call this partitioning around V. What I'd like you to do is modify the Python code that we're just looking at to take the list L and the value of V and return a new list Lâ€˜ that has the property that V is in its final sorted place, everything to the left of it is to the left of it, and everything to the right of it is to the right of it. We're going to modify this rank algorithm that we had before to do what I suggested. It's going to return a kind of slightly sorted or less than the one that was given with regard to some value of v. There is a lot of ways to do this some are cleaner and more efficient that others. I'm going to do the way I'm doing it just to be simple. This is a very simple idea all we're going to do is we're going to keep track of a list of values that are known to be smaller than v that are in L and the ones that are bigger than v. We're going to loop through all the values. Anytime we encounter one that's smaller than v, we put it in the smaller list. Every time we find one that's bigger than v, we put it in the bigger list and then we glue them altogether. First the smaller then v and the bigger. Now, this sort of plus equals a list construct. It's not immediately obvious what the running time of this is in Python. In fact, I think its probably not constant time, but you can create a version where each of these steps is constant time so that this entire routine runs in Î¸n because all we're doing is we're touching each of the values in the list to construct a new list Let's run this and see what it does. All right. It seems to be done. Here is 84, and you'll notice that these are all the elements of the original list. 84 is here. Everything to the right of it is bigger than 84. Everything to the left of it is smaller than 84. So we partition the list around 84. Let's go over now how we can actually solve the top K problem using this idea of partitioning. Now, remember what we're given her is the list L and some number K and what we're interested in are the top K elements of L, and for the sake of simplicity in this example by top--I mean the smallest, the one that have the smallest values. You can always put things around, turn the right greater ends into less ends and so forth and everything will be fixed, but for now, let's just imagine that we're interested in the smallest element-- the smallest K elements if the lists were sorted. Here's what we're going to do. We're going to start off--remember we know how to do a partitioning is giving some v, well I don't know which value to use, so let's just pick one at random. Here I am picking a value at random--let's say there. There's our value v, and now, we're going to run the partitioning algorithm on this, and there is a couple of different cases that can happen. We're going to look first at this case 2 because that's the super duper easy one. In this case 2, we pick some v at random and it just so happens that we totally locked out and the rank of v is exactly K. What happens when we run partitioning on that value v is it's going to separate all the values of L into the ones smaller than v, equal to v, and greater than v, and that happens to be position K, so the ones on the first K positioned here are exactly the top K for the list, the smallest K elements that we were looking for. In this case, we're done--we locked out. But that's only one of the three cases. We need to look at the other cases now. The next case is we pick this v at random, and it just so happens that it partitions to something bigger than K--so this is case III here. What happens at this point? Well, all the values get separated out. all the ones smaller than v end up over here, and all the ones bigger than v end up over here. And we haven't really solved the problem yet but we have made it a lot smaller. Consider at this point the list of the Lâ€™ from here to here. This is a list of elements that Top K of this list is going to be exactly the Top K of the original list because the values that we're throwing out here can't possibly be in the Top K. What we can do at this point is look again for the Top K, but now just do it on the sublist of L[0:rank(v)]. By picking v, we've actually just made this problem smaller. So great. Now we've handled two of the three cases. One case, it's completely solved. And another case, we have a smaller version of the same problem. What about the third case? So the third case is when the rank(v) is smaller than K. We do a partition and the v that we happened to pick is off to the left here. So what that means is we've now separated the list into elements that are smaller than this v value over here and elements that are bigger than this v value over here. And what do we know about this initial list? Well, we know that this actually is in the Top K. But we don't know about what's going on with the rest of the world over here. But we learned enough to have made some progress. Again we're going to recurse. Now what do we need to do? Well, this is the list that we are still unsure about. And there could be some off by ones in here but just to give you the basic idea from the rank of v, from this position here to the end of the list is what we care about. But we don't want to find the Top K of this. We want to find the Top K - rank(v). Why? Because we already know these elements here are already in the Top K. We only really need to care about what's left. The K - rank(v) that we haven't found yet. Those are the three cases that we care about, and we ought to be able to code this up and actually find the Top K in this list. So to solve this problem we're going to modify partition a little bit. That instead of actually just building a list for us, it actually separates that into three chunksâ€” the ones that are smaller than v, the chunk that is v, and the ones that are bigger than v. All right so given this modification to partition, we can run Top K as follows. We're given a list and some number K that we want that the smallest K elements of L. We start it off by choosing a random element of L. We just choose something random in the range of the length of L and call that v. Then we do a partition of L on v and separate it into the left, the middle, and the right. If the size of the thing on the left is exactly K, we're done. The thing on the left is what we want. Actually, I didn't talk about this before but if the length of the thing on the left plus 1 is equal to K then we still know the answer because it's the stuff on the left and including v. All right so it gets trickier when the stuff on the left is bigger than K. Well we went looking for the Top K, but we just found something more than the Top K. We have to whittle it down further and we can do that just by calling Top K on only the elements on the left and we just want the K best elements on the left. We've made some progress but we haven't completely solved the problem yet. And finally, in the last case, the length of left is smaller than K We know that everything on the left is something that we want as part of the Top K. And we know v itself is also part of the Top K but we need to search for the rest. And all the rest of the ones that we want are on the right so we're going to search through the list on the right. And how many things do we want off the right? Well, we wanted K but we found length of left plus 1 already. We can just subtract that off from the number that we want, call Top K recursively, and we're done. We call print top_k(L, 5) and we get back five elements not sorted. But these are exactly the five smallest elements of L so it did the work. It's sort of cool that Top K via partitioning as just described works. It gets us the Top K values in a not sorted order. Right. Let's do an analysis of the time it takes for a Top K to do its work via this partitioning approach. And because it's recursive algorithm, it's going to help to set this up in terms of recurrence relations. Here's the recurrence relation we want to solve. The time it takes to run Top K via partitioning on one element. Let's call that 1, and the time that it takes to run an n element is going to be hard to write down exactly but we can put down a bound. So what does it do? First, it does the partitioning operation, which runs through all the elements of the list that takes time n and then it's going to recurse assuming it hasn't gotten really lucky and since this is a less than or equal to let's assume it didn't get lucky. It's going to recurse on either the left or the right. Now, the partitioning algorithm is going to split this list L into a left and a right. In different cases, it's going to recurse on either the left or the right. But we don't know which one it's going to recurse on and we don't know which one is bigger. What do we know? Well, it's just as likely for the left to be bigger as it is for the right to be bigger and vice versa. Really you can imagine with justification that with probability of half the left side is bigger and in particular bigger than size n/2 and in half the cases right is bigger and again bigger meaning it's going to be at least size n/2. If the recursive algorithm gets called on the smaller half then the work that it has to do is whatever the recursive running time is on at least of size n/2 or smaller as this is an upper bound and that happens with the probability of half but we might also get unlucky and have to recurse on the larger half, which could involve actually something almost the size of the entire list. It's kind of a weird recurrence relation because it has probabilities in it but really we want a bound on the running time. How many steps it's going to take in this case on average for Top K to finish each stop. I mean this gives us an upper bound. This recurrence relation gives us an upper bound. All right so to solve this recurrence relation, I'm going to cheat a little bit because I know that 4n is going to be a good upper bound on the value of T(n). All we have to do is actually prove that that's true. We don't actually have to figure it out. To prove that this is true we're going to proceed by induction. Our base case is T(1) = 1, which is indeed less than or equal to 4. For inductive step, we know from the structure of the algorithm that the time it takes to handle an input of size n is on average going to be less than or equal to n+1/2T(n/2)+1/2T(n) just like we argued before. And now we're going to use our inductive hypothesis that T(n) â‰¤ 4n to get an upper bound on this quantity. T(n/2) we're assuming is upper bounded by 4n/2, which is 2n, and this quantity T(n) we're assuming is less than 4n. Now this is a little bit of a mathematical abuse of induction. We really don't want this to be n here because our inductive step is going to assume this true for everything smaller than n and not equal to n but the reality is this is actually a tiny little bit less than n. The worst possible case that you get in this algorithm is we pick the partition on the n, and then we recurse on everything but the n. There's t(n-1) here but it's--with apologies, you can make these details all work out but it's a little irritating. With that caveat, we proceed. What we have is this is equal to n+n+2n, which is indeed 4n. We've now shown that for each value of n, T(n) is always upper bounded by 4n. And that's what we wanted to show, but this is pretty remarkable. Because what that means is we actually find the Top K elements in Î˜(n). You know 4n is Î˜(n) so the running time is going to be linear, which is pretty neat. The top k algorithm that we have so far. We talked about using sorting, which runs in time and log n because you sort the whole list and then you can get any top k that you want for any k so it doesn't depend on k. The selection and insertion approach is both run in Î˜(nk) essentially because we're repeating for each of the elements that we want to get it out or repeating something like n work. But now, we have a fourth approach, which uses this partitioning idea recursively to get an expected running time Î˜(n), which is better and the worst case than sorting and better than selection/insertion for any k other than a constant n for constant they come up the same. This is actually pretty remarkable algorithm. I actually haven't found much useful in practice. I guess it's sort of nice to know that it's there. The algorithm that I have found very useful in practice is to use the selection/insertion approach but to use a different data structure than just a list to do the insertions into and that's a heap. And what the heap is going to give us is a running time that is (n log k), which is going to be better than sorting except when it ties it if k is or n, and it's definitely going to beat the selection/insertion approach. It doesn't do as well a partition at this particular problem of finding the top k, but it actually is pretty useful for bunch of other things as well so let's dive into the whole universe of heaps now. Now speaking precisely. A heap isn't actually any particular data structure with some particular running time. It's actually kind of an abstract data type. It's a data type that supports three different operations. We have some kind of data and we can add a new number to say the set. We can also ask, of all the numbers that are in the set what's the smallest that we've got? And we can say, if you don't mind please remove the smallest from the structure that we have doing whatever updates are necessary so that the rest of the operations can be fulfilled. And we can implement these operations using ideas that we've already got. So things like we can insert into a structure, we can ask what's the smallest element in the structure, and we can remove the structure from it. Let's use this as an excuse to do a little review. Here's two natural ways of implementing the heap operations of inserting and finding and removing the minimum from the structure. We can do it with an ordered list or we can do it with an unordered list. And there's tradeoffs to doing it this way. What I'd like you to do is, for how long does it take to do an insert into an ordered list of logn. Is that constant time, log time, or linear time? Same thing for an unordered list. How long does it take to stick a new item into the list? And finally, now that you've done that, we've inserted a bunch of elements into the list and they're in there now and we want to ask the question, how long does it take to find and remove the minimum from an ordered list? Is it constant, log, or linear? Form an unordered list, to find the minimum value and remove it. I'd like for each of these rows you should check exactly one of the boxes. All right. So let's think about this. So we have either ordered list or unordered list. We've got our list L. So let's first think about ordered list. An ordered list, let's say it goes from smallest to largest. Inserting a new element into the list and it has to stay ordered now because this is an ordered list that involves running to the list or possibly jumping around in the list. We could do what's called binary search to actually find the position where something needs to go quickly but then actually sticking it into the list involves copying all of the elements, moving them over, shifting them, and then sticking the new one in the list. This inserting into an ordered list is a linear time process. All right. We have to maintain this sortedness property. For an unordered list, it's actually quite a bit easier. Well, let's stick with ordered list for a moment. We're still with an ordered list. What about finding and removing the minimum? Finding the minimum in this sorted list is really easy. It's right there. Right in the first spot. So finding it is easy. And deleting it depending on exactly how we deal with this. We don't want to recopy the whole array. But if we could sort of just chop off the front and move over a little bit, then we can actually find and delete the min from an ordered list in constant time. That's really great--once it's sorted, we can actually find as many means as we want and it's very, very cheap. Alright. So now we consider the unordered case. In the unordered case, inserting into the list is so easy. We just stick it into the end and just extend the list a little bit. So that's a constant time operation. But what about finding and removing the min? Well, this is the problem that we're looking at earlier in the unit where we have to scan through to find where the mean is. And to remove it, we have to copy things over to fill in the whole. That is actually a linear time operation. We have these kind of two extreme choices that we can make. We can look at an ordered list where inserting is slow but removing the smallest is easy. Or we can use an unordered list where inserting is easy and finding and removing the min is hard. Notice we didn't use the sort of middle ground at all. What a heap is going to give us is a way of actually both doing inserts and finding and removing the min in a logarithmic time. A heap is a kind of data structure, a bunch of values that actually is a bunch of values arranged in a kind of a graph specifically it's a balanced binary tree and each of the nodes has a value in it and for the heap property to be satisfied the value in the node in every node in the tree has to be no bigger than the values in the children. Let me show you an example. Here's a balanced binary tree with 20 nodes. And I'm going to fill values in here that satisfy the heap property. And now I filled in the nodes of the tree with values in a way that satisfies the heap property. And so this is an interesting property. If you read left to right and top to bottom--13, 24, 21, 30, 29, 27-- It's not really sorted. Alright. The numbers kind of go up and down. The last number reading across is 41. But if you read down past, there's all sorted list. 13, 24, 30, 58, 74. 13, 21, 27, 43, and that's because as we're going down the tree the parent value is never bigger than the value of the child. That's the heap property so what that causes is that as we traverse down the tree we're always traversing down the sorted list. Because of the properties of the way that heaps are defined, they have some higher level properties that are going to turn out to be really important when they're used in algorithms. The way that we created nodes here is we filled them in left to right and top to bottom until we run out of nodes and given that that's how we do it what is the height from the root of the tree, the tippy top, down to the deepest leaf? How long is the longest pass in this heap? Is their constant depth? Square root depth? Logarithmic depth? Or as bad as linear depth? One way to think about this is, imagine this last level is filled in all the way to the end. Then what's happening here is we have, how many nodes we have at that level? We have half as many at the level above because each of the nodes shares a parent with one of the other nodes and again half and again half and again half. How many times can you half (n) before you get down to 1 which is the root? And that's what we've been talking about if that's (log n). So the height of the heap is Î˜(log n). The really important property of heaps is at the smallest value of all has to be at the top. Why is that? Because a node can't be below anything larger than that. The only place that the smallest number can be is at the tippe top. There may be copies of this number so you might find them at other places. In fact, the whole tree can consist of 13s, but the smallest one is still at the top. It may also be other places. The second smallest value can be, maybe immediately below it. What's the deepest level where you can find the third smallest? Level I will be here, level 2 is here, level 3, level 4, level 5, and so on. In this particular example, the third smallest, here's the smallest, here's the second smallest, and here's the third smallest. Well, it seem like a natural place for it but can it be any deeper? Write your answer in the box. Yes, indeed, we could have the 24 down here, but it can't be any deeper than the third level by the same argument that puts the smallest at the top. If you're the third smallest and you're, say, six or seven levels down, what are those numbers that lie above you in the tree. They have to be smaller than you, so you can't have been at least not the unique third smallest. So, the answer I was looking for is 3. Because the nodes are filled in the tree from top to bottom and left to right, we can number them very naturally as follows. We'll call this node 0, node 1, node 2, node 3, node 4, and so on. Because of the regularity with the way that the nodes are numbered, and the fact that there's always two children until you get down to the bottom and it kind of trails off. There's all sorts of relationships that you can figure out between nodes and their parents. We don't have to explicitly store them we can keep it implicitly because it can be implicit in the numbering scheme. I want you to stare at this for a little while and see if you can figure out the numbering scheme. Imagine we have a much bigger tree with, say, 500 nodes. Somewhere in that tree there is node 72. What would the right child of node 72 be using this numbering scheme? I want you to figure out the answer and write it in this box. But mostly, I want you to figure out the general rule and just apply that rule in the case of 72. Okay. What I'd like to do here is work out what the general rule is. Imagine that we've got some node and say it's numbered i. What can we figure out about the parent number of i and the two child numbers of i? The child numbers are actually quite easy if you look at them. It's always the node number doubled plus 1. That's the left child and the right child is adjacent to it so that is 2i+2. And we can work out from this what the number for the parent ought to be. All we need to do is subtract one from i and have it. And in Python that will actually cause it to get rounded down, but in Math symbols we'll just write a little floor here. Let's just double check that. Node 6 , 6-1=5. Half of 5 is 2-1/2 rounded down is 2. 5-1 is 4. Half of that is 2. We don't have to round it. Both 5 and 6 have the parent of 2. Coming up the other way, 2 doubled plus 1 is 5. 2 doubled plus 2 is 6 so that gives us this kind of local structure and this is repeated throughout. Therefore we could use this to try to figure out what the right child of 72 would be. It's 72 doubled, which is 144, plus 2, which is 146. So this heap property is a really useful thing, it's going to turn out. It's important to figure out how we can take a bunch of numbers and make them satisfy the heap property. Let's do that. Let's start figuring out how that will work. First thing to notice is that any one-node tree satisfies the heap property because it can't be bigger or smaller than the parent or the child cause it doesn't have any parents or child. That case is really easy, but now, so let's imagine that we've somehow managed to create two heaps and we've like to join them together by adding another node and have it the heap property hold. Both of these subtrees the heap property holds which means the smallest values in the entire heap are sitting here at the top of the relative roots, but now we're adding a new node. If this node is already smaller, then these two, then everything's fine but it might not be. It might be that this is a large value. Let's give an example. So what do we do at this point? We need a heap property to hold everywhere for this to actually be a heap. So what can we do? Well, so with respect to these three nodes, we're going to need to change something to establish the heap property of this locally. First thing we're going to do is we'll introduce some swapping around. Which value can we swap to the top to move things in the direction of more establishing the heap property? Which of these values should be at the root here? Write the answer in the box. The answer I'm looking for here is 10. Certainly if we're just going to rearrange these elements, the only way that these three elements can satisfy the heap property is if we put the 10 at the top and we can actually leave the 17 where it is and the heap property will still be maintained at this node. We'd swap the 10 and the 41 and now, this subtree may no longer have the heap property, but we can make a new picture that looks a little something like this where we have two smaller trees and these have to satisfy the heap property because the whole subtree that we had here used to satisfy it until we got that nice 10 out of the way and put in a nasty 41, but notice now we're in the same situation we were in a moment ago, but on a smaller scale. We have two subtrees that satisfy the heap property and a root that doesn't. We can now repeat this process pushing these nasty values down and down and deeper and deeper into the tree until they hit the bottom at a leaf at which point, it doesn't matter how bit they are, the leaves can be very, very large. All right, before we actually write a code that carries out this procedure, let's do an example. Here's a lovely heap. In fact, well, I just covered this up. It's a lovely heap where each of the nodes is smaller or no larger than any of its children, except for this pesky 50 at the root. Using what I described in the previous slide what we're just talking about, you can actually patch this up by moving the 50 around doing some various swaps. In fact, you have do do a series of swaps. And when the swapping is all done, which node in the tree, according to the red numbering, which node is going to be the resting place of 50 once the heap property is established? Just write the node number where 50 ends up in the box. All right--so let's do some swapping. So, here's how it works. We start out at the root where the problems is and we say "Okay." Well, what's the smaller of the two children and that's the 25 and we're going to need to swap the 50 and the 25 to fix things up at the root. We've bubbled that 50 down just a little bit. But now, we've created--well, kind of things are okay over here but not so okay over here. So, 50 is smaller than 56 but not in 26. We're going to have to swap 50 down with the smaller of it's two children. So, 50 and 26 are getting swapped. All right, so now, what horrible thing have we created now? Well, actually, things are pretty good now. This 50 is actually happy with respect to it's children, and if you're paying attention to what's going on elsewhere in the tree, everything is okay now, all right. Because we didn't actually touched this sub-tree. It had the heap property? It still has the heap property. We've bubbled the 50 down into here but we only pushed it to the right. We didn't mess with this sub-tree, so that one is still okay. And now, for more it is right now, everything is fine. We've now established the heap property everywhere and 50 ends up in node 4 here. Let's try to do this in a general way now. Right. So we call this procedure down heapify. Let's talk about how we actually will go with this. I've made a couple of helpful routines to help with the heap. given a position i in the heap, left (i) returns the position of the left child. Right (i )returns the position of the right child. Parent (i) returns the position of the parent. Root (i) answers whether or not the node that we're talking about is a root. There's no root. Leaf (L, i) returns whether or not we're looking at a leaf (i). In other words, it has no children. It has no children if what where the right child would be is off the end of the list and where the left child would be is off the end of the list. Then there's one other special kind of node and there can be at most one of these in the whole tree and that's a one_child node. That happens if a node has a left child, which is the very last element in the array. The right child doesn't exist. That's just what I said that the right child is at the very end of the list or off the end of the list then that's a one_child node. Down heapify is this notion that we've got. We apply this in the notion where we've got a heap rooted at node i that satisfies the heap property except perhaps i to its children, to its immediate children. We can apply this if the heap property is satisfied. Everywhere except maybe between the root and its kids, so the root in particular the node i and its kids. Let's just say that. If i is actually a leaf, then we're done. Right. We're satisfied everywhere except for between i and its children, and i doesn't have any children. So, good. That's great. Heap property is restored. If i has only one child, then we need to think a little harder. In the case where i has one child, we check the heap property between i and its one child on the left and if i is bigger than its child that's bad. That means the heap property isn't satisfied but all we have to do to fix it is we just swap the value in the left child and i. That's what we did and then now we know that that last node is the very last node in the tree and that node we just swapped into is the very last node in the tree so it must be that things are happy everywhere. What if i has two children? So this is some internal node in the tree now. Or possibly a root of a large tree. So now we check the heap property. I'm going to do this this way. I'm going to ask look at the left and the right children, and look at the minimum value of those two. If the minimum value is bigger than or equal to L[i] that means L[i] the value at node i is smaller than both the kids so we're good. The heap property is satisfied everywhere and we can just return. On the other hand, sadly if that doesn't hold, then we need to see which child is the smallest or the smaller of the two and swap i's value into that child. That's just what we're doing in the example by hand. Let's check it. If the left child is smaller than the right child, that means that the left child is the smallest one. We swap i and the left child's value and then we need to down heapify on the left child. We've basically fixed things everywhere except for possibly the tree rooted at left (i). It could actually not satisfy the heap property but that's okay. We can fix that by bubbling down the tree with down heapify once again. It's just what we did when we're doing it by hand and then we can return. Finally, we reached this point, if the heap property is violated and the right child of i is between i and the right child of i so we can just swap i and the right child of i possibly causing the tree rooted at the right child of i to be violated. We just run down heapify starting from there. When that's done, we can return. And we've established the heap property throughout the tree. Now that you've simulated and simulated the down heapify algorithm on an example and you've seen what the code looks like, tell me what the running time of down heapify would be if you have the heap of size n where the top node might be violating the heap property but it satisfied everyone else. We run down heapify on this and re-establish the heap property and it takes how long? For the work that it's doing is it's going to do a series of swaps possibly making it all the way to the bottom of the tree but never further than that. And the comparison, it does a constant number of comparisons at each level. It does a constant amount of work to do the swap and it doesn't have to touch anything else on the tree other than the longest path and the length of this path is the depth of the tree which is the log n. Given what we have build up so far, we can actually use the pieces to build a heap from scratch. We have the heap structure. In this case, with seven nodes and we filled them in with this random two digit numbers and it's not heap at the moment, but we can make the heap property be satisfied and this the way we're going to do it. We're going to start of at the root, which is node zero and we're going to say, okay, well, to make this into a heap, well first magically make this into a heap and make this into a heap and once we're done then we could do down heapify on this value and everything will be fine. Alright. How do we build a heap out of this smaller piece? Well, we can do it again recursively. We can say to build a heap, rooted at this node, make this new heap, make this new heap and then down heapify. Well, how do we do this guys? Well the single two nodes are already heaps. Any leaf, anything that is a leaf already were done. That's our termination condition. This is a heap. We'll check them off as we go. This is a heap. This is a heap. This is a heap. Alright so, now to make this whole thing a heap, we need to do our little swap thing. So 88 gets swap, this is down heapify 88 gets swap with 30 because 30 is the smaller of the children and now that down heapify is completed this whole thing is a heap. We need to do this subtree, again same trick. This guys are already heaps. To make this into a heap, we have to swap it with the smaller of the children 13. Now down heapify finish so this is a heap and now we've just got the last little step to make the whole structure here into a heap, we need to do down heapify on the root node. Which means swapping it with the smallest of the children that's the 13 and continuing recursively because that's what down heapify does, swapping it with the smallest until we reach the bottom and that's done. The whole thing is a heap. We made a heap. Woohoo! Perhaps even cooler. The running time is quite nice. The time it takes to build a heap out of an elements. Well, we build two heaps of size n/2 then there's a log in step to down heapify and this is a slightly hard piece to analyze. It's not so hard to figure out that we're essentially running down heapify which is a log in operation on each of the node sort as a root. We know that it's actually big O (n log n) and there's a tighter analysis, which I'm not going to give that actually shows that this T(n) is Θ(n). We could actually establish the heap property through out the tree on n nodes in the near time, that's pretty cool. The whole point of building up all this heap based technology is to allow us to do two things efficiently. One is getting and removing the min and the other one is inserting new elements into the heap. We like both of these to run in log n time, and if they are, then we can use them in the top k scenario that I was describing before. Finding the min is really easy right as the tippe top of the tree. In fact, getting the min done--easy, constant time. The problem is if where going to remove the min, look what happens. There's a sort of gaping volcanic hole at the top. Whatever will we do to fix this. We kind of have broken our big nice heap into two small nice heaps, but really want them to be one nice heap, so what can we do. Well, it would be nice if we had some kind of node someplace that we could fill in here and then maybe down heapify it. The natural place to get that node though is right there. The very last element to the heap. It some value, actually popping it off there doesn't cause any damage to the heap. We can move it to the tippy top and run down heapify. Once that concludes in about log in time, we've got our self a heap again. We're back in business. The steps were, remove the L zero node, copy the last node to the tippy top, then run down heapify on this no slightly smaller heap, write it fully. I've got n-1 elements in it now, but we run down heapify it on it and re-established heap property and we are done. Log n time. I want you to write some Python code to actually do this to remove min. It's only a couple of lines long. We'll give you down heapify and the build heap algorithm so that you can take a list, create a heap out of it and then remove the min from it. So here we have the remove min heap function. There's different ways to implement this, but this is just a simple two-line way. So, first what we do is we pop the last element from the list, and then we replace that by putting it in the position of the first element. This effectively removes the minimum element. And then the only other thing we have to do is run down-heapify on the first element to make sure we maintain the heap property. So, as a simple example, if we just wrote apply or remove min heap function to this heap, what it will do is it will take the last element here and move it up to the top, removing the minimum element. Then all we have to do is apply the down-heapify function. So what we do is we swap the 4 and the 1 here, and then we swap the 4 and the 3. That's the remove min heap function. This actually puts us in a really interesting situation. We can build a sorting algorithm out of the pieces that we just created. This is sometimes called heap-sort. You give it a (L). And what it does is it builds a heap out of (L), and then repeatedly, until there's no more elements left, it does remove.mn. So it gets the smallest and then the next smallest and then the next smallest until the list is exhausted. So what's the running time of this going to be? Is it (logn) like the other heap operations we've been looking at? Is it θ(n)? Is it θ(nlogn), which I had said most sorting can be done in (nlogn), but it's not clear if this actually achieves that. Or is it more like (n₂)? So the way this is working, it does build heap, which we said was Î¸n or more simply Î¸n (logn). Then what we're doing is nx. We're doing remove.mn, which is a (logn) operation, so we get n (logn) for this loop plus (L) for the build heap. It's just n(logn). It's pretty cool. There's only a couple of well-known sorting algorithms that give us such a good running time bound. So the last thing I promised I would tell you about is inserting elements into a heap. And that turns out not to be all that hard, but it does involve some coding that we haven't done yet. The idea is that the new element that we're going to insert we stick at the sort of bottom right corner of the heap. Or, if the heap was already full, then the far left. But now we have potentially violated the heap property, so this node to it's parent could be problematic. So, essentially, we need to do some sort of an analog of down-heapify that I call up-heapify that takes this value that might be problematic, swaps it up if it is, and then continues to swap it up the tree until it reaches a place where it is smaller than both of its children. At which point, the heap property is satisfied globally. So I'm going to leave that as a homework problem for you to actually code up-heapify, and that gives us the insert that we need. So now that we can insert and delete into heaps and (logn) time, we can use this to solve the TopK problem that we've been talking about throughout this unit. By running through the list of the N elements that we want the TopK of, and meanwhile we keep a heap of size K off to the side, and so we're trying to the largest K, in this case. So each new element that we encounter we ask, is this element bigger than the smallest value that we've kept so far? So does it deserve to be in the TopK so far? If not, we can just throw it away, because we know it's not going to be part of the TopK. If it is, then the smallest thing in that heap no longer needs to be there, because we found something better. So we delete min, and we insert the new value that we just got into the heap and reestablish the heap property by--well, we could do it as a down-heapify, actually, because we deleted the node from the top. We can replace it with something else. So, for each of N times, we do possibly one insert into the heap, which is a log K operation. So the total running time ends up being we do (nlogk) operations. So that's actually pretty efficient. It doesn't solve the TopK problem better than the randomized algorithm we talked about-- the partitioning algorithm-- but it does it pretty fast and there's other uses for this. We'll see that in the next unit. Thanks a lot, and good luck with the homework. Unit 5 combines ideas from graph search that we studied in Unit 3 with ideas about computing statistic that we look at in Unit 4 and it puts these tools together to give us the way of finding shortest path in these networks. Social networks come in many different flavors. Some of them consist of simple yes/no connections between the individuals in the network but others include more detailed information about the strength of those connections. We'll also going to look at approximation algorithms which are key tools for applying algorithms to a very large data seqs like these social networks. Let's get started. Well, hi and welcome back. We're now starting Unit 5. The topic of unit 5 is weighted graphs. I'd like to start up with a little magic trick. You may have seen this sort of plastic ring structure before. These are plastic rings that are use to hold six packs together and it turns out and I'm going to show you pictures of it, but you can find them in the Internet. They're very sad that maybe turtles can get themselves stuck in these holes and then as they grow, they continue to grow but the hole doesn't, and they can really damage themselves very badly. It is suggested that you cut your six pack rings. Actually introduce new cuts into this plastic structure so that is still connected. It's not a bunch of little tiny pieces, but there's no more rings left in it, and all the cuts have to be across little pieces of plastic not long plastic or cross multiple plastics at the same time, but each little bridges of plastic cut that and what I'd like you to do, follow me, let me show you. Earlier today, I went to the supermarket and I asked them if they'd be willing to give me one of this little plastic ring things. In particular, apparently I got a very fresh one because it's nice and flat--it just sits here. This wasn't actually going to hurt any turtles anyway. Better I should got one from the dump but I don't think my friends in Udacity would have like trash on their nice little pad here. Here is a ring thing and what would I do is make cuts in the structure so there's no rings left, but the thing is still in one giant piece. Okay, I did it. Now at the end of the day your little plastic ring thing, you should have a series of cuts on it and look there is no loops. Any place that there's a circle that a turtle can get caught in, I cut it open, but yet the thing is still in one piece. Alright, so what you need to do is save the turtles. You need to cut the bridges in this plastic structure until the plastic stays in one piece but there's no loops, no turtle traps. So you can make which ever cuts where ever you want, but when you're done write the number in the box. Don't tell me what it is. Just put it in the box. All right. Initially we have--I cut 9. So these are the cuts that I made and just to show that it's still connected. There's a little bit of plastic that goes around without passing through any cuts but there should be no loops--so every little region here that a turtle could get trapped in we have a cut in it and in fact, a series of cuts that gets you to the outside. So this region here exits out through here, here and out. This region say you exit here, here and out, region then run out, run out, this one here and out. Every region now opens up and then some other region will actually close up. So how many cuts did I make and I'm going to say--so did you. There are many, many ways of doing these cuts, but I still think you'll had 14. All right--so why is that--so it turns out that this little plastic ring thing, you can think of it as a graph where each of the little plastic connectors is actually an edge between nodes and the place that they intersect is a node. We've got a node here and a node here--they're connected. Node here and node here--they're connected, connected. This is a node and they're connected. Mark all the intersection points as nodes and put all the pieces of plastic in that connect them as edges. You can now just focus on that graph. So here's our graph. It has a total of 26 nodes and 39 edges and what we're looking for in this graph-- we're going cut edges, we're going to remove edges to make a tree. Right--the tree is going to be connected and have no turtle catchers which we in graph theory call cycles. How many edges are in a tree where 26 nodes, 25. How many do we have--39. Well, we're going to have to kill 14 edges to turn this into a tree. I first discovered this trick quite by accident that I actually was cutting up the little ring things that saved the turtles and a surprise, that no matter how I did, I always seem to to come out with the same number and then I realize it was just a graph theory problem. To motivate the algorithms that we're going to talk about in this unit. Let's go back to the Marvel data again. So here's three superheroes. We're going to call them Black Widow, Spider Man, and the Hulk. Remember that they're connected in the graph, in the original graph by an edge, if the comic book character is in a particular comic book. This is a set of comic book characters and a set of comic books, and we add links like so depending on which comic book characters are in which comic books. Now, we're often interested in connections between the individual characters. Up to now, we said that there's an edge linking, say Black Widow and Spider-Man, in and only if there exists a comic book such that Black Widow and Spider Man both appear in that comic book in the original data set. There's really two different graphs that we're working with here. There's the graph between comic book characters and then there's the bipartite graph between comic book characters and comic books. Now this edge appeared, as long as there's at least one book they had in common, even if that book had a zillion comic book characters in it and it was the only book that they were in together as opposed to, let's say, it's like Batman and Robin which is probably not Marvel. But anyway, two comic book characters that appear together very often, they would also just get a link between them and that doesn't seem quite right. What we'd like to do is put a weight on this edge where the weight equals the number of books they co-appear in. All right, so what you need to do is compute connection strength. Write a Python program to read the Marvel graph and put a strength value on each link. I wrote some code to read in the marvel data as a barpartate graph and then to build the character by character graph and then find the top k strength and I just reuse the heap code from the last unit to do that. The new part to focus on is right here. So it goes like this. What we do is we look through all the different characters in the list of characters and for each character look at the different books for that character appears then look at all the characters appearing in those books and for each of those triples, right. So this is a character, the books for that character and the characters for that book including the character itself by the way. It's going to make a link from the first character to the second character. This make a link. I modified this made link so that if you make a link twice it just keep it count. And so that's going to keep a measure of the total strength between character one and character two. Really, if you think about what it's doing here. It's actually for each pair character one and character two, it's going to add one each time there's a book they have in common. Its count up the number of books in common between character one and character two, which is really just what we wanted, but it's done in terms of a triple nested loop here. The running time here has to do with the number of edges right. It's not actually that the product of the nodes times the nodes times the nodes. Though if it's a completely connected graph it will be, but if it's a sparse graph for each character, we're only going to list the edges that come out of that. When I actually execute this, one other things we see is that this computer is faster than my laptop by about a factor of 2, so it took me 0.5 to read the graph, but anyway, it reached the graph pretty quickly. It computes all those strengths, which is the part that we're focusing on here in just a couple of seconds and then it run through the characters that are connected to other characters and finds the top 10 and I'm not a comic book expert, but I know enough to actually recognize a couple of the things that are going on here just the recent movies. You can see that there a very strong connection between the human torch and the thing. These are both members of the fantastic four. I'm going to say half of the members that fit fantastic four. Here's another one, Mr. Fantastic, and I think the invisible woman is the other one. What we see here is that books that have the human torch will often also have the thing in them and so forth. All the different pairings of the fantastic four end up having very high weight. They have a lot of books where they all appear together. Similarly spiderman appears very often with, I'm pretty sure that's his grandmother, and also with his editor cause Peter Parker is a reporter, so his editor J. Jonah Jameson shows up in a lot books with him. Captain America shows up with Tony Stark and I don't think I would know that at all except there's a current movie out that has them together and I think it's because they're both Avengers. They're individual characters but they also appear together often and I have no idea who Vision and Scarlet Witch are, but I'm sure they're awesome. Just as a quick aside, if you're familiar with matrix multiplication, there is a very nice connection between this little algorithm that we're just working on and matrix multiplication. To appreciate that, the first thing you need to understand is that if we can represent a graph on a set of nodes as a matrix and it's a matrix that consists of all 0s and 1s and if it's a sparse graph, say mostly zeros. But if there's a link between node I and node J, then the corresponding position in the matrix has a number in it, number 1. That 1 there means that there is a link from I to J. The graphs that we've been talking about links are bidirectional. So if there's a 1 in this I-J position, then there is also a 1 in the J-I position, which in this picture seems like it'd be really nearby, which means that this matrix is symmetric, and I'm not going to get into matrix terminology but again if you're familiar with it, you'll recognize what a symmetric matrix is. The matrix equals a its own transpose if you flip things the other way. So basically it's natural, it means for just interchanging the nodes since the connections are bidirectional, we'll have the same matrix again. So let's call this matrix (M) and let's think about what it means to multiply (M) times itself. In matrix multiplication, the way we fill in the I-J entry of the product, we take the I row of the first matrix and J column of the second matrix and I always need two hands whenever I think about matrix multiplication and you across this one and down this one at the same time. And what we do in this case since it's all zeros and ones and it's a square matrix, but what we're going through and figure out. Anywhere where there is a 0 and 1, and a 0 and the other, it's going to be 0. Only if there is a 1 in both, is it going to be a product that's not 0. So this multiplication of this row times this column is exactly a count of the number of 1's that the two vectors have in exactly the same positions. What does it mean for them to have a 1 in the same position? Let's check. Let's imagine that there is a 1 in position K here and at the same time in position K here. This position here means that our graph had a link from I to K. There, that's what this means that there's a 1 there. And similarly, if there's a 1 in this position, it means that very same graph that we're talking about this graph that (M) is representing also has a link from K to J and what we're doing is counting up all the Ks that have the property that they would take as you can get from I to one of those and then to J from one of those. It's going to add all those up. That's exactly what matrix multiplication does. This value here is going to be filled in with precisely the number of 2-step path from I to J which is exactly what we needed in that other example hat we're basically counting up the comic books that are uncommon between some character I and some character J. In the case of the bipartite graph of the Marvel comic book characters, any 2-step path that starts at a character goes through a comic book and then a character. So this is counting up exactly the number of comic books that I and J are both in. There we go, the square of the matrix which represents the connections of the graph actually gives us the same answer. The running time of this algorithm, of course, is n³, because we're looping through for each of the entries of this matrix that we're going to fill in and there's n² to them, we're doing this row by column multiplication which also takes time in. So it's in cubed. The algorithm that we've just been talking about. Something more like number edges times the number of nodes. The number of edges here comes from the fact that we loop over each of the character book combinations that we have stored explicitly. And then for each of those, we examined which characters are there connected to it. It could be as bad as N*M. If (M) is dense matrix, then this is (N)², so this algorithm ended up being in cubed again, but if it's a sparse matrix, so the number of edges is linear like in a planar graph, for example, then this is (N)² which is actually a lot better. In the example, we're just talking about with the marvel comic book characters, the strength of the link between some character i and some character j was the number of comic books they appeared in together. Now, this is an actually very useful metric for real life social networks. I have not appeared in any comic books with anyone. I have one friend who actually has appeared in a comic book, research her by the name of Micheal Bowling at the University of Alberta, actually had a comic book character made about himself, which is pretty awesome. But I didn't appear in that comic book with him, so that's not a very good metric. We won't really need to be studying weighted social networks just for comic book characters, but there's lots of natural measures of the strength of a connection between a person i and a person j. You could imagine saying they're more stronger connected as a function of how often they email each other, or maybe how many years they have known each other, the frequency at which they meet in a given week, how often they rate each others post on some kind of social networking site, or maybe the number of news articles both have read and you can imagine ask more examples like this. This notion of having a weight in a social networking is actually a pretty important and relevant concept. We had talked about shortest paths in a graph, but now let's extend this notion to the shortest weighted path in our graph. Here, in this example now, we're going to look at a high weight meaning a weak connection and a low weight meaning a strong connection. You can think of the weights as almost as being a kind of cost and we're looking for basically cheap roots from one person and a network to another. The example of this sort of thing might be that we want to send a message from one person to another in a social network minimizing, say a social awkwardness. For example, my son is really interested in American history and my Dean is an expert in American history, and he wanted to ask the Dean a question, but that's kind of awkward. They actually don't know each other directly. They accept that they do know each other directly at see. Random young man don't generally contact Deans of big school of arts and sciences. There's a sort of a high social awkwardness for that, but I was able to help get a message for my son to the Dean because I have very low social awkwardness with my son and I have fairly low social awkwardness with the Dean. The path of sending a message basically recommendation for a book to read about American history, traveled much more easily through me to the Dean than it would have to the Dean directly, and I should note that my son also has curly hair. Here's an example for you to take a look at. This is a graph and we're trying to get from point A to B which is often what we do in graphs, and you can see here I've actually annotated the edges of the graph with weights and we're trying to find the shortest path where the length is measured as the sum of these weights. And when you figured that out, write the length of the path in the box here. All right. Let's figure out what the answer is. Now, there's a couple of things that I want to try to illustrate with this example. One is that up to this point, our shortest path has been the shortest number of hops, and there's a one hop path from A to B, which also happens that it's actually fairly expensive. Paths that have a shorter total distance like 13 that actually consist of hops, and in fact, the shortest of all in larger lot of hops goes like this--4+3 is 7 equals 8+2 is 10+1 is 11. Now, let's take a look at how we can do this systematically so we can actually build an algorithm that can find answers like this. Let's take a look at an approach for actually finding shortest paths in graphs, and we'll use this example once again. It will help to actually have names for the other nodes, so I'm going to add these in. Just remember what breadth-first search does for this graph starting from A. We mark A as visited and we add it to the open list. We pull off the open list and add all the neighbors of A to the open list. Letter C, B, and D then we choose one of these nodes, Let's say C and add all its unexpanded neighbors to the graph, but all its neighbors are expanded. Do the same thing with B. B has F, and D's unexpanded neighbors are E. Now, F's unexpanded neighbors are G, and E has no more unexpanded neighbors. We finish this step for a search. What we get here is that by the assumptions of breadth-first search, the shortest path from A to B is this direct link from A to B. The search would have actually terminated here, but we ran that anyway. These are supposedly the shortest paths in terms of number of hops to all these nodes. It actually makes sense. One hop to C. One hop to B. One hop to D. Two hops to F, sure. Two hops to E, sure. Three hops to G, one, two, three. Yep. There's no faster way to get to G. This actually does the right thing in terms of number of hops, but let's take a look at what happen when we went to expand B. At this point, even though we have a shortest hop path to B, we don't have a shortest link path to B. All we know is that from A, you can reach C in three steps. Well, that's really all we know. Even this A to D, we don't know, there might be like a half weight path that goes from C to D, but we do know that there's going to be no faster way to get to C right because that is the shortest edge out of A. Any of the longer edges we're assuming we can't take negative weight edges that would cause this four to get smaller than the three. All we really know is that this three is the smallest. What we should do is not expand B, but we should focus on C. We now know that there's a path that actually can get us there in 13. This 10 edge plus the three that it takes to get to C. We can get to B faster than 15. We can get to B in 13. Now, is that the shortest possible path for B? We don't know cause we know that we could get to D in four and maybe there's a link one, I mean ignoring the graph for a second. Maybe, there's a link one path that would get us to B, which would be even shorter. All we know from what we've done so far is that the shortest path from A to D is four. Let's lock that down and pull D off of the open list, and let's focus on D. D has edges to B, F, and E. Here's B, and here's F and E. This path to F through D is going to add another seven for a total of 11, and this path to E through D is going to add another three for a length of seven, and remember there's also a D to B link, which would add nine to this, which would get us there in 13, and we already knew how to get there in 13, so that doesn't really change anything. Based on these three, we know the fastest way to A, C, and D, and once that we have also been able to reach, we know that E has the shortest distance, which is seven, and there isn't going to be any faster way to get to E cause there aren't any other nodes that we could get to and then get to E faster than seven. We can lock that one down, pull off the open list, add all its edges to the non-completed nodes. E can go to F, and it has a link of five. We could go seven steps to E and then another five to F for a total of 12. No, we can already get there in 11. That's probably not a good idea. >From E, we can also get to G in one step, which would have been seven plus the additional step for a total of eight, and that's all we can reach from E. Looking things over, we now know that the fastest way to get to G is eight steps because the only other way we could get to G would be to visit one of the other nodes, and then go to G, and that would have to be longer than eight. We're going to lock it down, and now we pull of the open list and look at the edges out of G. G can get to F in two steps, and that's an improvement because before the best we could get to F was 11 steps. Now, we can get to F in ten. Can we get anywhere else new? No, cause the only nodes that are complete now are B and F, and they're already in the picture, and in fact, now we see that the fastest way to get to F is going to be in ten steps cause the only other way to get to F that we haven't considered is getting there through B, and that's going to be longer. We can lock this down. Alright! So, let's look at F. What edges are coming at F to uncompleted nodes, just this one to B. That would have been F is ten steps plus one more would be 11 to get to B. That's an improvement over what we had before, and that's it for F. Now, the only node that we've got left to think about is B. There's no way to get to be any faster than 11 cause there's no other place that we can go and then get to B. We can lock it down, and that finishes the picture. We now know what the shortest distance is from A to === in the graph. The distance to B is 11. Now, we've kind of lost a little bit of information of how we get to B in 11? But, we'll deal with that in a little bit. I think it'd be a good idea to practice with this algorithm a little bit to get better into your head. Let's say that we're going to try to find the shortest path from A to F in this graph and we're going to use the algorithm that we were kind of sketching out together on the previous line. As you recall, what is does is it does a kind of breadth-first search from A but instead of expanding nodes at the bottom of the tree it always expands the node that has the smallest number written in the node. The very first time that E appears when we're building that tree, what number goes into the circle associated with E as we build up this tree anywhere inside. Simulate the algorithm, figure out what goes in the E, and then write the answer in the box. If you did this a little bit carelessly, you might say that the first number that goes into E is the length of the shortest hot path from A to E, which in this case would be, well there's two ways to get the distance 11 to here in two hops and then another seven. You might have said 18, but in fact, it won't expand D into E until it knows the shortest distance to D, and the shortest distance is going to be 10 because we can go this way. four plus one is five and five is 10 so it should be 17, but let's actually simulate and see what happens. We expand out A into C and B and once we've done that, the shortest non-completed distance is the one to C so we're going to lock that down and then we expand from that node C There is two edges to incomplete nodes, C to D and C to B. The C to B has a length of one plus length of four, we already had that actually improves this one to five and a new node joins the picture, D and C to D is seven on top of the four we already had for a total of 11. Alright, of the non-completed nodes, B and D, the one with the smallest distance is B with distance of five. We can lock that down and look at the outgoing edges from B, which is just this one that goes to D which had the length of five plus the five we already had for a length of 11 in D, which improves D, 10 and this the only node hit that we can read at this point that we haven't completed yet so we can lock it down, expand out its neighbors, which are F and E. F gets 20 and E gets the 17 and not only is that the first number written in E but it is the final one because the very next thing we lock down is E's distance. We don't lock down F's distance until we actually discover that there is a shorter path, the 19-length path. All right, so I think you understand this well enough that we can try to code it up Here's a sketch of the algorithm as we were just running it by hand. Largely, we're trying to find shortest distances from D to the other nodes in the graph. we initialize D's distance to V as 0, then we imagine that all the other distances to all the other nodes in the graph were as yet unknown, and each time we go through the loop, that there's still nodes left that haven't been completed yet. We find the node with the shortest known distance and then we locked it down. By marking it as known and then going through the list of its edges--all that edges out of W-- for example, just some other node and we check if the new distance, the distance to W that we just locked down plus the weight of the edge from W to X. If that improves on the best known distance to X, then we update X's distance to be this new value. And once we've done that for all the neighbors of W, then we could finished W and we go back up to this loop and say not all nodes are complete. You get a new W, a new shortest distance to lock down. So that's the basic structure. Let's actually look at that in--I'm going to say Python. So here's my attempt at translating that description of an algorithm into actual Python code ended up using helpful structures maybe a little different than before so I've got a Dijkstra's algorithm. Dijkstra is the name of the individual who first described and analyzed this algorithm for a single-source shortest path. So we give it a single source. We give it a single node in the network. Then we ask for the distance to all the other nodes in the network in graph G. So the distance so far is a structure that's going to represent a mapping from nodes to what we think the distance might be from V to that node. And in our hand simulated algorithm, these are the numbers in the non-locked circles. Some nodes might not have any numbers yet, and the ones that have numbers are represented in that mapping. So then we start structure off by saying well, the distance that we know of so far from V to the V, the node that we started at, is zero, and we do that in the hand simulation as well. Alright, now, there's an additional data structure, which I call final dist, which is once we actually figured out what the real distance is, we stick in this structure and so that's basically the numbers that are in the heavy circles here. When a circle becomes heavy because I'm locking it down, I moved that number into the final dist mapping, and I deleted it from the dist so far so that number doesn't exist anymore in the dist so far mapping. Now, we're going to iterate as long as the set of nodes for which we've computed the distance is less than the total number of nodes. Now, this is a little risky. I probably shouldn't have done this. Because if the graph is disconnected, what this is going to do is this is how we're going to die. Well, let's see where it's going to die, but it will keep trying to add nodes in their final distances even though they aren't final distances to add. So there's probably other test that might be better to determine when everything that's reachable has actually been assigned value. In general, this test isn't quite the right test, but it will suffice for a connected graph and that's we're going to try it on. What do we do as long as there's more nodes that we need to analyze, Take the node that has the shortest distance of all the ones so far, call that w, and lock it down. Locking it down in this case involves need printing a debugging message saying that the final distance for w is whatever we computed the distance so far as. We now know that this is the final distance and then we delete that from the dist so far structure. Then we go through its neighbors. All of the neighbors of w in the graphs called an x, and for each one, we'll say, well, if we completely solved that neighbor then we don't have to do anything, but if we haven't then see if it has a distance so far, and if it doesn't then give it one by saying, well, our best guess is the distance. It's going to be the distance that it took to get to w plus the distance from w to x. On the other hand, if it already has the distance, check if the new distance, the distance to w plus the distance from w to x, is better than the distance that we had so far, and if it is, replace it. This is sometimes called relaxation. It doesn't seem very relaxing but that's what it is. And so now we've handled that node. We handle all the nodes for the neighbors of w and that means we've handled w. We've locked it down, and we can move on. We go back up and handle the next node closest to the start state. And once we've gone through all of the nodes and assigned them all their final distances then we return that structure and we're done. This is the Dijkstra's algorithm in a nutshell. Let's analyze this. I want you to analyze the running time of Dijkstra and ideally, I would just ask you but given that you can't answer me directly, I'm going to have to make this a multiple choice question. And it turns out that you don't have enough information to answer it. So for you to be able to answer it, what were do you need to know. So the running time of Dijkstra in terms of n and M and here's a sketch of the algorithm again. Remember for each node it finds the shortest distance so far--it removes that distance from the set and then it checks each of the neighbors. I've got a node that it found the shortest node--it does a little bit of competition-- constant competition for each of those before going up and starting again with another node. But you will have enough to answer this question--is it because the running time depends on how find shortest is implemented, is that because it depends on the degree of the nodes in question or is it because it depends if the graph is dense or sparse. You'll get a different answer in those cases. So just check which one and I'll give you some feedback. It turns out it doesn't really depend on whether the graph is done through sparse that's captured in M. Right. So if the M is dense that means it's sort of order in squared. If it's sparse, it's lower n. So what have we already captured in terms of n and M, so it's not that one. It depends on the degree or amount--it turns out we're going to have to do the same amount of work if there's nodes that have very, very high degree or if there's nodes that have very, very low degree. We still end up having to do a certain amount of work for each edge. It doesn't matter how to clump the nodes--it matters how many they are total. It's this one--it depends on how "find shortest" is implemented. Let's talk about how we might implement the shortestdistnode function. We give it all the distances we've calculated so far for all the nodes that we could do that for and so that's just a big pile here. There is a bunch of nodes and for each node, we have some value and a lot of these values are temporarily assigned and they may actually change later, but what we want to know is which of these we'd like to pull out the smallest value and then we'd actually like to lead it from this set so we don't have to worry about getting the same value over and over and over again The next time we want to go in here we want to get the next smallest. How many different values might be in this blob? Well, there can be at most one for each node then probably less. There is probably lots of nodes that haven't been added in yet, but at worst, it's going to be Î¸(n). The nodes that are in the graph may have distances associated with them. To get the smallest distance in here, the nature thing to do is to loop through the list. Take all the distances that we actually have at the moment just loop through them and pull out the minimum, easy Î¸(n) and in fact, that's what I implemented here. The shortestdistnode, you give it a mapping distances and it starts off with the best node undefined and best value something big from probably really want something bigger than this and for all the nodes that we have distances for If the distance for that node is better than the best we've seen so far, reassign and when you're done, just return the best node. This is really quite straightforward and what is the running time that this leads to? These are some tricky run times than the ones I've suggested in the past. We've got n nodes and m edges, and we want to know, if we implement the dijkstra algorithm with looping through the list to find the smallest value. What is the running time that we get? Just make the selection. To be able to solve this, it'll be helpful to go back and take a good look at the algorithm. There's really two little blocks of things that are going on. For each node, we check all of its neighbors and once you add that up over all possible nodes whether all the neighbors of all the nodes, that's (M), so it's constant work for each of those edges. But for each of the node, we also have to do this shortest distance-so-far operation and also to remove the shortest distance-so-far operation and so that is, for each node, It has to do this which using the implementation that we're looking at is another end. However, the number of edges is always going to be between 0 and n² roughly. So this quantity really is somewhere between n² and 2(n²). All that is Θ(n²). So going back to the set of choices, Θ(n²) is what we're looking for. Now that's a larger running time than most of the things we've look at so far and in fact, we can do better and I'm hoping that you've already made the realization that if you got a bunch of values and the two things you want to do with them is, well, more than two things, you want to be able to find the smallest, you want to be able to delete the smallest, and you want to be able to take a value that's already in there and possibly change it to some other value. That seems like the sort of things that a heap would be really good at. All right, so let's actually figure out how we can use a heap in this particular algorithm to get a better running time. Let's look right now at the analysis of the algorithm. What's the running time of Dijkstra in terms of the number of nodes (n) and edges (m) when heaps are used? The kinds of heaps we've been talking about to be able to find the shortest distance so far so we can lock that down and run. Here's the sketch of the algorithm again. For each node, you find the shortest distance so far. Remove it from the set because that one's now done and we actually know the distance to that node. Then we check each of its neighbors possibly reducing the distance which wasn't a really big deal before, but it actually matters in the heap of limitation because once we reduced the value associated with the node, it needs to maybe be some place different in the heap. In your implementation, the thing you're going to have to really watch out for is making sure that when you update the value of the neighbor, you can find it in the heap. Update the heap so the heap property is restored and all of this information is communicated back and forth. All right, let's imagine that we can do that and in fact, reducing the distance of a value turns out to be, you can use up-heapify to do that because the value, here let me draw. If we have some value in the heap and we know that the heap property is satisfied and what does that mean? That means that this node has to have a bigger value or no smaller value than it's parent and it has to, itself, be smaller than both of its children. Let's take a closer look at the issues that arise in a heap if one of the values, let's say, we have this one, is suddenly reduced. They get smaller, so let's just use a concrete example. The heap property is satisfied. That means that whatever value this guy has, let's say it's 10, have to be smaller than both of it's children but bigger or no smaller than it's parent. What can happen if this node has its value reduced? Well, let's take it to reduce a lot. Let's take it's reduced all the way to 1. The heap property is no longer necessarily satisfied. It's actually fine from this node down. This is fine. The problem is that this node, to it's parent, might no longer satisfy the heap property. But the good news is, we talked about an operation that fixes exactly this problem. If this is node (I). If we run up-heapify on this heap with a pointer to (I), what it's going to do is bubble this small value up as high as it needs to be in the tree. It's going to take Î˜(log n) time and then the heap property will be restored and all will be well. That's actually not a big problem unless you already have implemented. The thing that makes this tricky is when you're talking about some particular node, let's say, node (R). You need to be able to find out very quickly the constant time where node (R) is in the heap and as things get moved around in the heap, we have to keep that information updated. That's really just that bookkeeping is actually the main thing that makes this challenging. But let's take a look at the running time given that we can do this distance reduction operation in logarithmic time. For each node, we find the shortest distance and then for each node and each of its neighbors, that is to say for each edge, we possibly reduce it's distance of a node. What does that give us for a running time for this entire algorithm? I want you to think it through and when you think you understand what the running time is, find it in this list and check it off. As I was describing it before, for each node we do this test to find the shortest distance so far. That's a heap operation--there's n things in the heap, so this is a logn operation. So this altogether is going to get run once for each node--so this part of the algorithm actually takes time and logn. For each of the edges in the graph, we do this distance reduction operation, which also is a logn time operation--so that becomes for each edge a logn or mlogn. Now, both of these have to get done so we actually add these together nlogn and mlogn, and into the assumption that the graph is connected, this m has to be at least as big as this n-- because this term dominates--so that gives us a total running time of Î˜(mlogn). Just I'll mention as an aside, there're some very clever data structure work that's been done by Fredman and Tarjan, the same Tarjan as before, that allows this actually to be decreased using a very special kind of heap that allows the running time to become nlogn+m. We need at least m just to visit all the edges. So this is a pretty remarkable result. I've implemented this in the past. It didn't run so fast for me. These are asymptotic results in that the overhead in keeping this fancy heap type structure updated was more than--well, I wasn't able to implement it so it was efficient enough. But the heaps that we were talking about the overhead is pretty raw and we get this kind of running time, the elapse should be pretty efficient. As we discussed in the previous Unit, to find the most central node in a social network, it's useful to be able to take each node in a graph and find out how far it is from all the other nodes in the graph to get a square for a particular node and then to repeat that operation essentially to find out the distance from every other node to every other node. Really what we want to know is the shortest distance between any pair of nodes. So all pairs we would like to know the distance--the length of the shortest path. Now, given that we can execute a shortest path from any given node, then m*logn time. We can just repeat than algorithm, just one at a time for each node run Dijkstra-- here's a node run Dijkstra--here's a node run Dijkstra--so you repeat it for all the nodes. So we get n the number of vertices times nlogn for the total time-- to get the distance between all pairs of nodes. This quantity can have a range of values depending on how dense the connect to the graph is. If the graph is connected but very sparse, m is the same as n--so we get nÂ˛logn. If the graph is very densely connected, then the number of edges is roughly the square of the number of vertices and so the running time, at least in terms of the number of vertices, now becomes nÂłlog n. Now, you can do a little bit better than this using the Fibonacci heap idea that I was mentioning before but may be this is the best that you can do in terms of practically implementable code. This on the other hand now we can beat this. I'm going to talk about an algorithm called Floyd Warshall after two of the inventors of this algorithm back in the early 60s, and it uses an idea called dynamic programming, which sounds so much cooler than its actually is. That means, it actually is a very cool idea. I have a professor friend who believes that I am predispose to see all problems at dynamic programming problems because it an algorithm design technique that I have been able to use successfully in a bunch of occasion, but it really is just an algorithm design technique. Its not particularly dynamic and it doesn't really change the way you think about programming. It just have to do with optimizing using tables. Alright. Let me start up by explaining this algorithm in terms of simple but possibly strange idea. First just for simplicity, let's imagine all the nodes in our graph are numbered from 0 to n-1, and lots of graph have exactly have their structure, but for example for our marvel data set, we have to all our characters be assigned numbers from 0 to n-1, but that's an easy thing to do, let's imagine that's already been done. What we're going to imagine now is that someone has given us matrix. Square matrix D of k and the entries of this matrix, just a big table like a spread sheet and is filled in with values as follows. The ij element of the i row, j column is filled in with the number and that number is the length of the shortest path from i to j hopping only all nodes numbered less than k. So I don't know about you but I used to play a game like this when I was younger, if I was in a building where the tiles were different colors, I'd sometimes declare let's say, the blue tiles are alligators, so I'm not going to step on any of the blue tiles and I'd try to walk stepping only on the tiles that I was allowed to use, so that the analogue of this in the graph is imagine we've got our graph, and we're trying to get a path from i to j, and some of the nodes are colored pink. Those are the okay nodes because they are numbered less than k and some of the nodes maybe including j itself are numbered higher than k, and we're not allowed to step on those. We're trying to get from i to j, we're allowed to step on j opposite but only the intermediate nodes that are pink. We only consider making paths along pink nodes and there's only one in this case. But there's some pink nodes that we don't use and their maybe multiple different pink paths, but we want to know the distance of the shortest path using only the pink nodes. Only the ones that are numbered less than k. So let's imagine, some very hopefully, has done this for us and filled in the matrix with all those values. There's big Î˜ of nÂ˛ values that have to get filled in, but someone has does that for us. That's all great. Our mission should we choose to accept it. Is to fill in Dk + 1. So this will be a new matrix just like the old matrix, but now when your going on a path from i to j, you're allowed to use node k. You don't have to use it but your allowed so that's all we have to do right now. Let's try to figure out if someone gives use Dk, how do we figure out Dk+1. Let's think about it for a little bit, you'll see there is really only two possibilities. If we're trying to find the shortest path from I to J that is allowed to use K and any of the nodes with numbers smaller than K. There are two possibilities. It may be that we don't need to use K at all for the smallest path. We just follow path from I to J only using nodes who have values less than K or we go and visit K and then we go from K to J, but in visiting K, we don't ever need to visit K on the way to K that wouldn't be a very good path. The shortest path from I to K only using nodes that are less than K then we follow another path now from K to J only using nodes whose numbers are less than K. Those are the only things that could happen. We don't want to visit K twice. We're not allowed to use any nodes that are greater than K those are the two possibilities and whichever one of these two things is shortest. The shortest path from I to K followed by the shortest path from K to J or the shortest path from I to J whichever those two things are shortest, is the shortest path from I to J possibly using K and things less than K. We can just write that in a straightforward way by taking advantage of the fact that someone has given us the K, not tooth decay but matrix decay. The length of the shortest path from I to K using only those less than K, we have that already. Someone gave it to us very helpfully. All we need to do is compare the distance of I to J not using K and the distance from I to K not using K and K to J not using K. It's weird, but it's the right thing to do. We can take advantage of this insight to actually create an efficient algorithm We can initialize D⁰ so this is the length of the shortest path from I to J using nodes only numbered less than zero which is to say no intermediate node, but that's just the direct cost of going from I to J. >From the cost in our graph, we can fill in this matrix. And once we have D⁰ initialize, we can run K from 0 to n-1 and for each pair of I and J, take Dk+1 to be the smaller of D⁰[k,i,,j] and D⁰[k,i,k} and D⁰[k,i,j] add it together just as I was describing before. When this is all over, when this loop is done, we have the D⁰n, but D⁰[n,i,j} is the length of the shortest from I to J only hopping on nodes that are numbered less than n, but that's all the nodes. All the nodes are numbered less than n. They are all numbered from 0 to n-1. so this is the length of the shortest path from I to J, unrestricted, full stop. so that's pretty cool, and if you think about what's going on here, this initialization step runs over all pairs of nodes so that's n². This loop runs over all pairs of nodes n², but it does this from each value from 0 to n-1. So altogether we're talking about n*n² or n³. This better than the n3/logn we get by applying repeat Dijkstra to a dense graph. So for a dense graph, you're better off using Floyd-Warshall n³ instead of the n³/logn. Log n is often not that huge a number so your mileage may vary. so you can actually try the overhead, but boy it's hard to imagine having less overhead than this It is very straightforward it runs through the n³ possibilities, and in fact, if we do this carefully, we don't need to keep a separate D matrix for each of this cases. You can actually do these updates completely in place so the storage is pretty low too. That's all I'd like to say about shortest paths and weighted graphs in this unit. There's some I think interesting questions on the homework for you to get to know this concept better and to work with it and apply it to some interesting social networks. I'm going to take this last topic for this unit to be estimating the clustering coefficient. This is kind of a different idea but I wanted to introduce the idea of a randomized algorithm and we did that a little bit in the context of the expected linear time top K algorithm. And in that case, the answer that it returned was always the correct answer. But the time that it took was actually a random variable that on average was linear in the size of the list. We're going to now look at computing the clustering coefficient approximately. This is a really useful thing to do if the exact answer doesn't matter very much which often when you're doing social network analysis it doesn't really matter that much exactly what something like the clustering coefficient is. You want to just get the ballpark whether it's heavily clustered or loosely clustered or slightly more clustered than the movie database or not. So just getting it correct to a couple of digits is probably sufficient. But for the case of the clustering coefficient, getting the exact answer is actually pretty expensive. So just to remind you, here's some code for computing the clustering coefficient of a graph with respect to a particular node v. So given a node v or computing the clustering coefficient involves looking at all the neighbors of v, going through them one by one, and looking at the pairs of the neighbors. And for each pair of neighbors, it does a calculation to compute how connected it is and then returns a measure of how densely connected the neighbors are. But that's for each node we need to look at the square of the degree to actually compute this So if the degree is rather high if the graph is fairly densely connected or even if there's just a few nodes that have a very, very high connectivity like maybe not that many edges, like a star graph for example. The star graph has just linear number of edges but it also has linear degree at least in that one node. And so computing the clustering coefficient for that node is n² well I guess in this case then the total running time is not going to be n³ because we don't have n² on each of those nodes. But the running time is still pretty high and n² is pretty high. And if this is actually a densely connected graph like a clique, we're talking about n² for each of the nodes in the graph which totals up to n³ and n³ for something like the Marvel Comics graph where you have 6000 nodes, 6000³ is a pretty substantial number even by current computing standards. So we'd like to have a way of getting a pretty good answer in time a lot less than this. Here's kind of a formula for the clustering coefficient. In the clustering coefficient of a graph, you have to sum all the nodes in the graph. We're going to average it for the n nodes in the graph. And then for each of those nodes what we do is we sum up for all the pairs of nodes that are neighbors of the node v. We sum up whether or not they're connected and then we scale that by taking the number of possible connections-- 2 over the neighbor size of v times the neighbor size of v minus 1. So would you think for a second about this random process that I've got this brace around here. So imagine that we choose a v at random from anywhere in the graph and then we choose a pair of neighbors of that particular chosen node v completely at random from the graph. And then we return either 1 or 0 depending whether those two nodes are connected. What is the average value of this quantity? And remember the average we're taking here is the average over these two sources of randomness that we're randomizing the overall nodes in the network and we're randomizing the overall pairs. So just to review the expected value of some random variable x is the sum of all values that the variable can take on, the value of that variable times the probability. So for example for a six-sided die, the expected value that we get by rolling the die it's the sum over all sides, the probability of getting that side which is a 6, times the value on that side which is the numbers 1, 2, 3, 4, 5, or 6. And so from that we can actually get the average value that comes up on the die or the expected value. So given that and if we're given a v, what is the expected value of c[wx]? Maybe I should ask you. So here's a skeleton to get you started. I'd like to know the expected value of the c, this connectivity variable, given a graph g and a node v of that graph and I'll predefine for you the set of neighbors of v and then the degree of v, the number of neighbors, the expression x in g(w) is true if x and w are connected in the graph. So those are the pieces that you need. Calculate this value exactly. The expected value of the connectivity of w and x for uniformly and randomly selected w and x from the set of neighbors. So write that code and we'll test it for you. All right--so if you did this right, you would realize that what you need to do to compute an expected value the sum of the overall values that it can take on the probability of that value times the value itself. So what that amounts to is summing overall the pairs w and x that are neighbors to v. We sum whether they're either connected or they're not connected and that's represented by the c[wx] thing and we need to multiply that by the probability that we picked that particular w and x. Well of all the w and x's are picked uniformly at random and the number of different pairings of them is the degree of v times the degree of v-1 divided by 2. That's the number of different ways you can pair up a set of size dv. The reciprocal of that is now the probability that we picked any particular pair. But this is independent of c[wx] so really we can move this out front and just do it as 2 times the degree times the degree minus 1 times whether or not the two nodes are connected. And all that summed up for all of the w and x. But the thing that I would like you to realize is this is exactly this. So the expected value of randomly choosing a pair and checking if they're connected is the same as actually computing this particular piece of the clustering coefficient formula and the same trick works in terms of the number of the vertices. So if the vertex is chosen at random and then this quantity is evaluated for that vertex the expected value of that is exactly the clustering coefficient. So what that means is we can iterate this particular expression that I put braces around and average that and if we did that forever, if we did that infinitely often, it would be exactly the expected value, which would be exactly the clustering coefficient. But since we can't do it forever, we'll run it for a certain amount of time and we'll hope that it gets close. Let me show you how it gets close over time. So let's do a little demonstration of this idea. We've got a little flight graph from one of the previous examples, and we can compute the clustering coefficient for any node. Let's say node 2. We're going to print the clustering coefficient for node 2. And then what we're going to do is actually randomly pick two of these neighbors. Now, I ended up having to go a little bit crazy here because you need to pick the neighbors without replacement meaning that we pick one neighbor and then the second neighbor we pick has to be different from the first one. So I went out of my way a little bit and said okay here's what we're going to do. We're going to run through all of the neighbors of v—these two. And for each of them, we're going to stick them into an array called the index. And so instead of randomly choosing from this set, we're going to randomly choose indices from this array and that will make it easier to make sure we don't repeat. So d is going to be the degree of node V. And as long as that degree is greater than 1 we're going to pick a random neighbor so that now the neighbor here is being chosen by its ordering from 0 to d-1. So v1 is the neighbor—the actual node name associated with that pick and then we do a second pick and this time we're going to do—we're going to again choose from 1 to d-1. We're going to add that to the pick that we already got with modular d to make it wrap around. So this is going to make sure that we're going to pick something that is different from what we just picked and we look up the corresponding ID. So we've got v1 and v2 are the two neighbors. We check whether they're connected and we add 1 to the total if they are. We repeat this whole loop a thousand times and at each point in time we take the total—the total number of times things were connected divided by the total number that we tried. And I claim that it ought to be the case that this number converges to the actual clustering coefficient. So let's try it. So in this particular case the clustering coefficient for node v is 0.3. And now let's watch what happens to the estimate. Well, certainly when there's only one sample that we've done so far it's either going to be 1 or 0; it's not going to be very close to 0.3. But as we repeat this over and over again you can see it's kind of settling in. It actually hit 0.3 briefly on the 48th try. But it's kind of bouncing around a little bit above 3, a little bit below 3, and the longer we run this the kind of less it seems to be changing— 0.26, 0.27, 0.26, 0.27—Oh, it wandered up to 0.3 again and ended up at 0.31. So if we do this long enough and repeat this in enough times what we'll find is that the number really is 0.3 but in any given run it's going to be a little bit more, a little bit less. The longer that it runs, the more digits are going to be correct. And so in this particular case, this is a terrible approximation because it's actually not very close to the real answer and it took us a long time to get this extremely bad answer. There's some nice mathematical tricks like the Chernoff bound that tells us how many times we have to do a random sample as a function of the variance of the distribution before we can get an estimate of a particular level of accuracy with a particular level of certainty. So if you want to be really careful about this, you can actually look up the Chernoff bound and apply it to figure out how many samples are needed but for current purposes run it as long as you can run it and hope for the best. All right. So that's really all I wanted to go over in Unit 5 and then we pick it up again in Unit 6 talking about computational complexity and its relation to social network analysis. See you then. Once you spend enough time designing algorithms to solve specific problems you start to realize that some problems don't seem to allow for efficient solutions. Unit Six covers the concept of NP-Completeness. It doesn't actually give us a way to solve these problems any faster, but it does give us a pretty good indication when it might be okay to stop looking for an exact answer. Welcome back to CS215. This the beginning of Unit 6, which is going to cover the hardness of network problems. All right, I made another magic trick and I'm going to show it to you now. So this is a map of a country that used to be part of the old Soviet Union called Tetristan and it consist of 8 counties, which I've labeled A, B, C, D, E, F, G and H. And what I'd like you to do is color the map each one with one of three colors so that no two counties that are touching show the same color and then I want you to number those colors--0, 1 and 2, and then depending on which numbers you assigned to each colors and which colors you assigned to each counties, I want you to calculate the color of county D plus the color of county F plus the color of county C times the color of county G minus the color of county A plus the color of county H. I want you to work out what that number is and then write it in this block, but before you write it let me see if I can guess what it's going to be. Yes--yes, I have a very good guess as to what it's going to be, but you write it in and let me make sure I'm right. I have magically deduced that you're answer is 3--tada. All right, now I'll show you how I did it. I will not leave you in suspense for very long. All right, so see if we can work out what's going on here. This region here, this county is going to have to get some color and it's ultimately not really going to matter which one it is. So I'm just going to call it that. G is going to have a color that's different from H, so we'll call it that. Now, we noticed that this region touches both of these two regions so it has to have a different color than both of them. This region touches both green and blue but not pink, so it has to be pink. And d touches both pink and blue, so it has to be green. And now we got these three regions and what do we know about these three regions. Well, these two can't be green and they have to be different from each other, so one of them is pink and one of them is blue and that means this one is going to have to be green, but we don't know about these two. So here's what I'm going to do, so I'm marking this sort of both pink and blue right there. This going diagonal lines are--this one's pink, this one's blue, and this going diagonal lines--this one's pink and this one's blue. Regardless, it's going to turn out not to matter. So now you don't have to assign number to green, pink and blue--0, 1 and 2, but notice that we have G-A here and G-A, no matter what numbers you assigned, they're going to be the same color. So this has value of 0. It didn't matter what color C is, C*0 is going to end being 0. So what we're left with is D+F+H, so D+F+H and notice that each of them are different colors, so no matter what numbers you assigned to those three colors, we're going to have exactly one of each. The colors get assigned number 0, 1 and 2, so the sum is going to have to be 3. So that's the trick Through this course, we looked at a number of different algorithms that we were trying to solve and for each of them, we worked out what their running time was so for finding the connectivity between two nodes in an undirected, unweighted graph, the running time is the number of nodes plus the number of edges. Finding the shortest path in a weighted, undirected connected graph took time m times the logarithm of n. Removing the minimum value of a heap is log n. For the problem of finding the shortest path between all pairs of nodes in a weighted graph is taking n続 times of at one of the algorithms. Okay, so you have a sense that these are considered to be pretty good algorithms. In fact, there's some cases very good algorithms and you see that they're all polynomially bounded, so there's some polynomial in n and m that actually is larger asymptotically than all of these. And by large the algorithms for which--that we can actually solve in a reasonable amount of time, all seem to be in this category of having a polynomial bound. And that is lead theoreticians at least to find efficient in one particular way sometimes and that is that an algorithm is efficient if it has a polynomial bound on its running time. And a problem can be efficiently solved obviously if it has an efficient algorithm-- that's a reasonable question to ask--well, do our problems have efficient algorithms. It turns out the answer is no, not all problems have efficient algorithms. They are some that actually require an exponential amount of running time. For example, an algorithm may have a bound of Î¸(2â ż). So an n gets larger, this is getting larger and larger and larger very very quickly. So this is considered not efficient. We're going to talk in this course about any other problems that have provable lower bounds that are exponential that is to say that there can be no polynomial time solution to them. There is one in the literature, which is a generalization of checkers that you can learn more about if you take a course in complexity. But what I would like to talk about is this sort of funny gap between the problems that we know have efficient solutions that have polynomial running time, and the problems that we know do not have efficient solutions. They have the fastest algorithms that could run for them have to take of this exponential time. Then in between here there's a gap. There's actually some very very important problems that we don't know whether they have a polynomial time solution of if they require exponential time. They are just kind off hovering in this no man's land. The great unknown. There's lot of different kinds of classes of problems that live in this gap, but I'm going to focus on for this unit is a class of problems known as the NP complete problems. They are not the only problems that fall in here, but they are very important class and probably the best studied class of problems that actually fall in this gap where we just don't know if they are polynomial or exponential running time, and it's really frustrating that we don't know where this problems lie, but we do know something, and that is for all the NP complete problems either they are all easy and they fit in the polynomial running time class or they are all hard and they fit in the exponential time running class. We don't know which it is, but we do know that they can all be hang together, and so sometimes it's very useful if your studying a problem to discover whether it belongs to this funny class of NP complete problems that fit in the gap, so that's what we're going to take a look at. The one we're thinking about this is that there is degrees of hardness of different kinds or problems. Kind of like rocks. If you've ever studied rocks there is a scale for how hard a rock a is. Well there's kind of a scale for how hard problems are as well and some of the entries on this scale include things like, well some problems are actually not computable. We can define a problem and we can show that there is no computer program that can actually solve that problem. That's hard and sometimes that's maybe as hard as you can imagine. An example of a problem in this class is the halting problem, and you can see I have music video about the halting problem if you want to take a look at it, but we're not going to talk about it more in this class. There some other problems like I mentioned before that are computable. We can actually solve them with a computer program, but they require at least exponential time. I mentioned generalized checkers is an example of that. There's problems that are computable and can be solved in polynomial time like the shortest path problem with Dijkstra's algorithm, and some are computable can be solved really quickly in linear time at the same amount of time it would take to just look at the example. You can actually get an answer. So graph connectivity is an example of a problem in that class. So the way we think about this is the hardness of a problem like how two nodes in a graph connected. The hardness of the problem is the running time of the very best possible algorithm for that problem. For some problems, we're not exactly sure how hard they are, but we have lower bounds and upper bounds. So for example, we might know that a certain problem requires at least nÂ˛ time to solve, but we know that it requires no more than nâ ľ. So the true complexity, the true hardness of this problem is somewhere between those two bounds. So here's a question for you. What's a good way to find an upper bound on a problem's hardness? We know an upper bound and a lower bound that gives us a sense of how hard the problem is. How do we find an upper bound? So here's some choices. One is device an algorithm to solve the problem, run it on a bunch of inputs, and then look on the graph to see what the shape of it is and that tells us the bound. So that's one choice. Another choice is. Again device an algorithm to solve it. Run that algorithm on all possible inputs and take a look at the running time that results from this that gives you the upper bound. Third you can imagine devising an algorithm to solve the problem and then analyzing it, and whenever the big O or big Î˜ is that you get from that analysis is an upper bound on the hardness of the problem, and then the final choice is, this is a trick question. The whole question of upper bounds must be some other class because it's not what we've been studying in this class. So decide which one do you think is the most appropriate and check it off. Fortunately, the answer is the third one. What you need to do to prove an upper bound are on the hardest of a problem is if you can find an algorithm for example that solves the problem--provably solves the problem and provably into the fourth time, then--well, we don't know if a faster algorithm exists but we know it's at least solvable in end to the fourth. So the big O or Î˜ that we get from that is exactly the upper bound. These sorts of things are really helpful to do--well, actually so this first one is really helpful to do, running on a bunch of inputs, look at the shape of the graph. I can give you a lot of insight that might help you decide what you want to prove about the algorithm, but this itself doesn't really prove anything. Running something on all possible inputs is impossible, unless it's a problem where the inputs are constrained, so there's only a few possible inputs. Generally, the running time scales with the size of the input and it can scale arbitrarily large. And yeah, actually studying algorithms and how to analyze it is exactly what we're doing in this class So computational complexity is the problem of actually finding the hardness of problems and from the prospective of a theoretician who studies complexity theory, the stuff that we've been doing so far talking about devising algorithms and proving their running times, that's just part of the story, that's just upper bound determination. There's a whole other set of work that has to go into finding lower bounds on the hardness of problems and that requires very different techniques from the kinds that we've talking about up to this point. So it's worth pointing out what theoreticians often really like to do is that they find an upper bound for the hardness of their problem and they can find a lower bound that matches it, then you know exactly the complexity of the problem. It has to lie between those and they match, so therefore, it's exactly that. So as I said, we're not going to get very involved in proving lower bounds for different problems, but this argument is actually not too hard and I'll give you a little flavor for how this can be done in some cases. Imagine that we're trying to compute the max of a list of (n). Now we know an upper bound on this problem is Î˜(n) because we can just loop through all the elements in the list find the maximum. But how do we argue about a lower bound for this problem? Well, one argument might be, well, constant time is a lower bound on this problem Because at the very least, you have to give the answer. It takes at least constant time to do that. It's possible you can even do that well if you're trying to compute the max of a list and you guessed a number, if may very well be the max, so that gives us a lower bound. Here's another one, Î˜(n) might be on lower bound, why? Because you have to at least look at all the n items; otherwise, there could be one item in the list that is the max that you didn't look at and so you can't possible return the right answer. All right another argument might be something like this. We have a lower bound of (log n) for the problem because finding the max is sort of like finding the winner of a tournament. You have to compare values against other values and we might be able to actually eliminate half each time, and if we could do that, then the number of rounds that it would take before we have a winner in a tournament of individuals in a single elimination tournament, it's like it's (log n), so which of these do you think is the best argument in terms of a lower bound for finding the max? Well it turns out all these are sort of right. You really do need to give the answer. Well this isn't quite right. You can't guess the answer. If you have an algorithm that just guesses the answers, that algorithm is going to wrong some other time and we're not talking about the complexity of algorithms that are allowed to be wrong. We're talking about the complexity of algorithms that have to be right. So what about this tournament argument? Well, in a parallel computing scenario, you actually can do something like this where you have the list of numbers that you're trying to compute the max of. You can separate this in two lists. Ask some group of processors to compute the answer for this half of the list and some other processors or group of processors to compute the answer for this half of the list and then you only have to do one comparison to find the max. The number of rounds that this takes actually really is (log n). So this problem is solvable in (log n) time but not under the computation model we've been talking about. It's a computation model where there's kind of an infinite number of processors that can share information between them. This Θ(n) argument is actually correct and just to kind of spell it out for you, imagine we've got an algorithm that runs faster than Θ(n). So that's means it can't possibly look at every element in the list. So whatever that algorithm has done, let's say, it's looked at a couple different elements of the list and it has a guess as to what the max is. We can make sure that this is always wrong by hiding a value in one of the spots in the list that the algorithm didn't check in. And if that number is bigger than everybody else, it can't have answered the question correctly without looking at it. So that's really cool actually because finding the max of a list, we have an upper bound of Θ(n) and we have a lower bound of Θ(n) and they match each other. Another algorithm that you could get matching upper and lower bound without too much work is first, sorting at least under the model where we're counting the number of comparisons of elements you have to do and it turns out that in fact, sorting under a comparison model has matching upper and lower bounds of (n log n). So this (n log n) that we've been having for sorting is essentially the best you can do. To make some of the issues, we're going to be talking about simpler. We're going to focus in on some sense very simple kinds of problems. Problems that take inputs just like what we've been looking at. Be the graph or less, whatever happens to be and processes that to get an answer, and the answer is just 0 or 1, yes or no. So that's the sense in which it's making a decision. It's making a yes no decision base on the inputs. An example is graph connectivity where you're given a graph and your ask, are all the nodes reachable from all the other nodes in the graph and the answer is either yes or no. It is either connected or it's not connected. So even though this output is very very simple just 0 or 1 one bit. It can actually be fairly complicated. Sometimes very very complicated to take the input and decide whether it is zero or one. Here's another example. If I give you a graph and node v and a node u and a number k. Those are the inputs. The question is can v be reached in fewer than k steps from u? Another question is given a graph, is it a tree? Yes or no. For it to be a tree, it needs to be connected and not have any cycles in it. No loops. Now the decision problem is given a graph, is there a bridge link somewhere in the graph? An edge that if it gets removed separates the graph into two separate pieces. Given a graph and a number K. Is there a pair of nodes that are K steps apart. That the shortest path between them is no more than K steps. Now the question is G bipartite. This was a homework problem or they were a closely related homework problem where you can take a graph and try to work out whether it is separable into two sets of nodes with edges only going between the sets. Now you could argue that in sometimes this problems are a little bit silly. We don't often want to know just one bit of answer, we want to know something like, is there a pair of nodes K steps apart but find me a pair of nodes K steps apart, or is the graph bipartite but separated into two pieces and show me what those pieces are, or can v be reached in fewer than K steps from u? Well if it can be show me the path. I really like to know the path cause that's what I'm going to be able to work with. So an interesting fact about decision problems is that they are often directly relatable to the version of the problem that your interested and that actually gives the answer, and sometimes you can actually connect them with only a polynomial amount of extra work. So basically, if the decision problem is efficient so is the problem that returns the more interesting input. All right, to introduce this concept, it's going to help to have another definition that I haven't given you yet. So here's a graph, we have been talking about things like shortest paths. So let's look at a path from a to g. So here's a natural path to choose. It's one of the shortest paths from a to g, but it's by no means the only path from a to g. So here's another path from a to g. a to b to e to d to a to b to e to g. It's a path. It's going from node to node crossing edges but it's not a simple path. So simple path is not allowed to have any repeated nodes. Any shortest path is going to be a simple path. Right. Why? Because if we had any repeats along the way from a to g, if we visit b and then sometime later we go back to b then there's going to be a path that just goes from a to b and then using this path from b to g. We don't need any of these links in between. So if the shortest path actually had on it a repeat, it was not simple path, then it can't be a shortest path, there must be a shorter path still, so that's a contradiction. Now the longest simple path that you can have, can have it most n nodes on it because there is only n nodes in the graph and we're not allowed to repeat any. So the longest simple path is at link n. This graph has seven nodes. So let's ask the question having to do with this graph here. I want you to check each box with number K next to it if there's a simple path of K nodes that goes from a to g. So for example, it can't be one beacause it has to have at least a and g on it, and we already solved path a, b, e, g that's a simple path that has four nodes on it, so I'll give you four. For each possible length, check the box if there is a simple path with that number of nodes on it from a to g. All right, so as I said there can't be a one. There can't be a two because a and g aren't connected. In fact, the shortest path has four nodes on it--one, two, three four. So four is definitely possible. What about five--one, two, three, four, five. There's one. What about six? So six of the nodes--one, two, three, four, five, six and finally, we can do a path that you saw the nodes. A simple path that you saw the nodes. So no repeated nodes and we go from a to g. (blank screen) Now we can look at this as a decision problem and we'll call it the long and simple path decision problem and it goes like this. Given a graph G and a link L and two nodes U and V, is there a simple path from U to V consisting of L or more nodes? We're not looking for a short path, we're actually looking for a long and simple path, and we're going to imagine and I'm saying imagine because I actually don't know whether this is true or not, but we're going to imagine that there is an algorithm that solves this decision problem, yes or no correctly, and its running time is n to the k for some constant k like two. Θ(n²) or 10 like Θ(n¹⁰) or a thousand Θ(n¹⁰⁰⁰) but k is not a function of n. It can't be an exponential or it's going to be bounded by some polynomial. It could be a nasty polynomial, but let's just imagine that it's bounded by a polynomial. Here's a different problem. Find a simple path from U to V in G consisting of L or more nodes. This one is just asking me yes no question. This one is actually asking for the path. And we'd like to know is there a polynomial time solution to this problem, we'll call it problem B. Assuming that there is a polynomial time solution for problem A. We can use the solution of problem A as subroutine in solving problem B. All right. Here's how we like you to do it. I want you to write a Python command called long and simple path that takes a graph G and node U, node V, and the link L and it's allowed to call long and simple decision with whatever it wants. As many time as it wants as long as it's a polynomial in the number of nodes and edges in the graph, and ultimately return the path that there is one. Here's a little skeleton to get you started. We call long and simple decision G, U, V, L and if it's the case that the decision problems says or the solution to this decision problem says there is no path of link L or greater from U, V, and G then you don't have to do anything, just return false. But otherwise, you actually need to build and return the path as a list of nodes and the first element, at least should be U and the last element should be V and all the steps in between should be actual edges and the total number of nodes on that path needs to be at least L. Here's the strategy that I would suggest. There's probably a couple of different ways to do this. We could take the original graph that we know there is some length l path, simple path from U to V and what we can do is for each edge in the graph say, well, what if we delete that edge? Is there still a path? And if the answer is yes, then we just leave it deleted. But if the answer is no, that we've now actually broken a path from the last path from U to V of length at least l in G, then we need to leave it in there. We keep repeating this until all the edges have been considered and all that's left at the end will be the simple path. Why, because if there's any edges in there that's not on the simple path then we can delete it and there still is a simple path and certainly we can't delete any of the ones on that simple path and still have it work. What's left is exactly what we need. Here's my attempted datas Python code, so again, we check first to make sure that there really is such a path and if there isn't, then we just quit before we try to find it, but if there is, so somewhere in there, the path exists, what we're going to do is run through all the edges, so node1 in G, look at all the neighbors. Look at node2 which is the neighbor of node1. So now node1, node2 is an edge in the graph and we're going to break the link. I probably should've mentioned that earlier but there is actually a break-the-link piece of code int he skeleton that I gave you. In graph G, get rid of the edge from node1 to node2. You can do that in constant time and then we ask, is it the case that there is still a long and simple path, we ask for the decision. Is there a long and simple path in G from U to V of length l? And if the answer is no, if there's not such a path then we need to put that edge back in the graph; otherwise, we just leave it out. We'll repeat this for all the nodes, all the edges, and then we're basically done. All that's left in G is exactly the simple path and now we have to read it out. It's a little bit messier than I thought it would be, but it's definitely doable. You could run that graph search on this graph and that would work Because all that's left is the path that you want but another way to do it that's more specific to this case is we start off building our path from U, we're going to have a variable node which is a particular point in the chain. In the variable next which is the next point in the chain and we start that off with whatever is the neighbor of U. Notice there can only be one neighbor of U because all that's left is the simple path from U to V. Okay, all right, so now, we say as long as that next node that we're looking for isn't V, we append next to the path so far so we have the first two steps and now we're at a new node, this node next is going to have two neighbors, one is going to be the node we just came from, node. And the other one is going to be a new node that we haven't considered yet. To figure out which one it is, what I did is I just pulled the two of them off. I said let the nextnext0 be one of those neighbors and nextnext1 be the other. If nextnext0 is the node we just came from, then we're going to move one step along the chain. Node gets next and next gets nextnext1, the one that we haven't visited and; otherwise, next gets nextnext0, the one we haven't visited. When that loop is all through, then next is V, the very last step on the path and which add that into our path and return it. All right, so now we're in a good position to define the set p of polynomial time decidable problems. So this is a set, this includes a bunch of problems, a problem is in the set if there's an algorithm for that problem such as that when inputs size n, the running time for solving that problem is in big O then to the k for some constant k. So there has to be some kind of polynomial upper bound of an algorithm to solve that problem for the problem to be in p. This k--this constant k is allowed to vary with the class of the problem, but any particular instance still needs to be solved and into the k. So it's nÂ˛--they're at the end of the fifth--whatever it happens to be. And just in case it's not obvious, running times like Î˜ of logn are in big O nk for some k. In this case, k=1 suffices. The class p is often contrasted with the class np that I'm going to define next. Formerly the class NP is the set of non-deterministic polynomial time decidable problems-- that is to say that it's a problem that can be dissolved by a program that runs in polynomial time that has non-deterministic elements in it. Now, I'm going to explain what that is in just a moment, but first what I'd like to do is give you a handle on intuitively what this means. I'd like to say the NP stands for nice puzzle problems, so let me explain what I mean by that. Think of a problem like solving a Sudoku problem. You might phrase this as a decision problem by saying here is a grid, can this be filled in to complete the Sudoku problem. It has to satisfy that if you don't know this is a very popular newspaper puzzle that involves-- you're given a bunch of numbers in a 9x9 grid and you have to fill in more numbers, you have to fill in all the grid cells and ultimately, the solution that you fill in has to have the property that every row in every column has the number 1 through 9 and each block into these dark blocks also has the numbers 1 through 9. So I can give you a Sudoku like this and it could be very, very challenging to know whether or not this can be completed to satisfy the Sudoku constraints, but the next day in the newspaper or if it's a puzzle magazine, may be at the back of the puzzle magazine. It's very easy to convince you if the answer is yes, so if the answer is yes, then i can prove it you by simply giving you a small what's called accepting certificate, which in this case just means a filled in Sudoku board. So if I filled in Sudoku board, you can very quickly do two things--check to see if it's correct, if all the rows and columns have the number 1 through 9-- that's a polynomial time decision problem that you can write very easily, and also check if the cells that you're given match what was given in the original puzzle and that also is a very fast thing to be able to check. All you have to do is run through the cells of the original puzzle and each time there was something that was filled in, check if it matches in the answer. So even though it maybe very difficult to answer the question if you're given the right kind of information you can check it very fast. One definition of NP says, that a problem is in NP, if it has a short accepting certificate. An accepting certificate is information that we can use to quickly show that the answer to the decision problem is yes, (if it actually is yes) and here, short means polynomial size and quickly means polynomial time. To define NP even more formally, we're going to say a problem is an NP if there is a verification in algorithm. This is a basically computer program on A like a subroutine A such that for any input for the problem that is a "yes", there is a certificate C such that the size of C is polynomial in the size of x, and the verification algorithm will say "yes" but for any x that is a no, there is no certificate C that's polynomially sized with respect to x and the verification algorithm says "yes", so for any yes answer to the problem, a verification algorithm will say "yes" for some small certificate; otherwise, it'll say "no" for all small certificates. For the Sudoku example, the verification algorithm is what we describe as something to take the original Sudoku problem with things not filled in and the answer is printed in the back of the magazine where the answer filled in, and it checks to see whether this is the actually a solution to that. If it is, then it says "yes" and if it's not, it says "no". Now this is going to fit the NP definition that we've got here because for any Sudoku problem that is solvable, there is some grid that we solve it and we can show what that grid looks like, but for any problem that it's not solvable, there is no way that we can fill in this grid with numbers. So that the verification algorithm will say that it's okay. It really is back to the definition of basically the Sudoku problem. The Sudoku problem says, "Is there a way of filling it in?" And the certificate that we use is, well, we'll show you how it's filled in. It fits both half of this definition, so this problem is an NP. Now let's be a little careful here because Sudoku as it's typically designed, doesn't really have an input size; all the inputs are of the same size. but you can imagine a generalization of Sudoku where you have a bigger grid and maybe more symbols and the same kind of verification procedure works. The bigger the grid gets, the longer it takes to run, but it's always a polynomial in the size of the input grid. Let's try to put this definition to work and answer the following question: Is P contained within or possibly equal to NP? So one possibility here is, I don't know, and if I knew that, I'd win a Fields Metal!, meaning sort of the equivalent of the Math Nobel Prize because this is a big important open question. The other choice is no, P is not contained within NP because if the problem is NP, it means it's decidable in polynomial time and we don't need any kind of certificate and so, it's not an NP and then finally the opposite of that is, yes if it can be decided in polynomial time, no certificate is needed. So if you have a problem that's NP, then you don't need any sort of certificate, so it's also in NP. And the answer is p--is indeed contained within np and the reason is if the problem that we had can be decided by some polynomial time algorithm s that takes the input, runs in polynomial time, and either says yes or no correctly for that input, then we can design-- remember that the main thing in the definition of np is that it has a verification algorithm, so we can define a verification algorithm like this--define a to take the input and any certificate that simply returns s of x--this runs in polynomial time and it accepts inputs and certificates correctly. So for any input x, there's some certificate that makes it say yes--that is to say any certificate-- because if the answer is yes, it's always going to return yes. And if the answer is no, there's no certificate you can give it that would get it to say anything other than no. That's again because it's a growing certificate and actually just giving the answer. So it's satisfies the two things that we need for the problem to be an np. So anything np is also an np. Okay, here's an example of a decision problem that comes up in social network analysis and that's the clique problem. Remember clique is a set of nodes in a network that are all pairwise connected-- everybody in the group knows everybody else in the group. We were given a graph G and a number k, let's say 4, and we're asked--is there a clique of size k in G. So remember clique is a connective set of nodes--so for example 1, 3, 4 is a three clique, but now the question is--is there a four clique in the graph. So just to make sure that you understand the idea of this problem, why don't you find the four clique--there is a four clique in this graph. Find the four clique, add the node numbers that make up the clique together, and enter the answer in the box. All right, let's see if we can solve this problem. We're looking for our four clique. Now, four clique is going to have to have the property that each of the nodes is connected to each of the other nodes, so the alt degree or the degree of the node has to be at least 3. So that actually means we can eliminate some of these edges from the graph. All right. So the nodes from this graph. Two can't be part of our four clique because two only has a degree of 2. So let's get rid of that guy. All right. We can device same argument. We can get rid of 6 because it only has a degree of 2. All right. Now, everybody that's left has degree at least 3. Let's take a look at node 4. So node 4 has the property that it has exactly line of degree of 3. So if four is part of the clique, then it's neighbors have to be the rest of the clique, 1, 3 and 8, but you noticed that 1 and 8 are not connected to each so 4 can't be part of our four clique. All right, now that we've taken those edges away, we can see 8 has now a degree of 2. So 8 can't be part of the four clique, so the only possibility left is that these four nodes make up a four clique and that would mean 1 needs to be connected to 3, 5 and 7, 3 needs to be connected to 5 and 7, and 5 needs to be connected to 7, which it is. So if we add these node numbers together, 7+3+5+1 is 16. Now, is this decision problem in the set NP? Well, remember that we need to show two things for it to be an NP-- it needs to have a short accepting certificate and it needs to have a fast verification algorithm. In particular, what we don't need to do actually have a solution to this problem that's fast. We just need to show that a certificate verification algorithm is fast. So you should by now have a sense that the certificate in this case is actually pretty straightforward. If I tell you that there's a four clique in the graph, the way I can convince you that there's one there is I give you the four nodes that make up the clique. The verification algorithm then that you run is you go all the pairs of nodes in the certificate and make sure that in the original graph G, they're actually connected. So for each one, you should have to ask the question--okay, is this pair in edge in the graph, is this pair in edge in the graph and that each of those tests takes unit time--constant time and there's order Î˜ of kÂ˛ pairs to check, so this verification algorithm runs in polynomial time. So just to drive this point home now, for every graph G that has a k clique, there is a certificate that would cause the verification algorithm to say yes, but for any graph G that doesn't have a k clique, there's no certificate you can give it that would cause this verification algorithm to say yes. And just to make certain here, the verification algorithm could also check to see that the certificate that you gave it actually consist of k nodes. Just to be careful. You could almost imagine that there's a conversation going on between a psychic and a detective to solve a murder mystery. The detective is at a dead end and can't figure out how to solve the murder mystery and the psychic has this sort of amazing leap--oh, you need to check the finger prints on the body. This maybe not that amazing of a leap and you'll find that the murderer is Smith. Now, we wouldn't arrest someone based on just the word of the psychic. This detective now actually needs to go and check it out and see that it is actually is true, and assuming that the psychic is giving checkable information and it's correct, then the two of them together should be able to solve the problem. Now the role of the psychic here is actually to pick the accepting certificate, I don't know out of thin air or somehow, just guess it and then the verification algorithm actually can check it. This actually explains this notion guessing a certificate pulling out of thin air. It's not a deterministic process. It's not something that we could just program an algorithm to do to just jump right to the right answer and that makes it a non-deterministic step. It's not the kind of thing that you can actually write into a program that proceeds step by step by step. It sorts of make this crazy leap, non-deterministic leap and as long as we can check that leap is correct with the verification algorithm, we can at least conceptualize that this is a way of solving problems. We can actually implement a computer like this, but we can imagine a computer that we call a non-deterministic computer, non-deterministic turing machine that could this sort of magical step and that's where NP gets its name, non-deterministic polynomial time. And just a quick aside, there are actually some researchers working on something like this. There's reasons to believe that a quantum computer can do certain kinds of guessing, essentially by in parallel kind of imagining all the different situations that all these different certificates that might exists, and then designing the algorithm in such a way that the correct certificate will actually bubble up to the top efficiently. This doesn't exist yet so don't get worry that what your learning in computer science is all completely irrelevant. It's still relevant for as far as we can tell into the future but the idea of a quantum computer is actually very interesting. We have reason to believe that the design and analysis of algorithms for quantum computer are going to lean pretty heavily on the stuff that we do for classical computers as well. It's a usual thing to note that there's a relationship between the class NP and the cluster problems that are solvable in exponential time. Any problems that's in NP is also going to be solvable in exponential time. So the set of exponential time solvable problems includes NP. Now that by includes I mean that it takes at most exponential time. None of the problems in NP can be any harder than that, say requiring double exponential time or factorial time or whatever happens will be, they can all be solvable in exponential time. So if the problem is in NP that means there's a bound on the size of the certificate needed. Say it's polynomial n to the c for some constant that's specific to this problem, where n is related to the size of the input and there is also a bound on the running time of the verification algorithm like n to the k where again is related to the size of the input and k is some constant that can depend on the problem but it can't depend on the input. Well, if we have those two things, we can actually create an algorithm that can solve the problem for any input. So what it needs to do is we can just run through all the certificates that could possibly be relevant for the input x and for each one we run the verification algorithm, and if the verification algorithm ever says--oh, yeah that's something that shows that x is true, then we can return true as the solution for the problem and if we run through all the possible certificates and none of them made the accepting, the verification algorithm return true then we can return false, and the running time of this algorithm is well the number of times we go to this loop is like 2 to the n to the c, if we imagine the size of this certificate is written down in bits. This is the number of different assignments of bits to certificates of this size. So if we run through all possible into those, we know that we've checked everything and then for each of those the running time is in the order of n to the k. So the total running time is like n to the k, which is a polynomial times 2 to the n to the c, which is exponential. So the total running time is exponential. It's not worse than exponential. It's not great. Unless we are talking about problems that are really tiny, this is going to be horrible, but sometimes problems really are tiny and mostly I'm introducing this as a way of just connecting these two concepts of the class NP and what it means to be able to find an exponential time. It's nice to know that at least any problem that's in NP is solvable. All right, we can write an algorithm that would run it. It just that algorithm may take a very, very long time to complete. This brings us to a point where we can ask one of the most fundamental questions in theoretical computer science and that is does P=NP and in particular, here's what we actually know. This is true. We know that every problem that's in P is in NP and every problem that is NP is in EXP, that is to say any problem that you can solve in polynomial time, we can certainly solve in non-deterministic polynomial time, and any problem that we can solve in a non-deterministic polynomial time, we can also solve in exponential time, but here's what we don't know. It could be the case that the class NP is actually equal to the class EXP. That is to say the set of problems that we can solve in a non-deterministic polynomial time might be exactly the same as the ones that we can solve in exponential time. So there's a sort of outer set and that's distinct from say this inner set, which is the set of problems that are solvable in polynomial time. There's one other thing that we know, we do know that there really is a difference between polynomial and exponential time. There's some problems that can be solved in exponential time that are definitely not NP. So we know those two things are different, but we don't really know. It could be that NP is equal to x. It could also that P is equal to NP. So the problems that we can solve in a non-deterministic polynomial time might be exactly the same as the ones that we can solve in polynomial time, which both would be then different from exponential time or could very well be that there are really three different categories here. That problems that are in NP don't necessarily require exponential time, but they may not be solvable in polynomial time either--we don't know. So this question of whether or not P=NP, whether we're in this case here is a pretty heavy question. So what happens if P does equal NP. Well, a lot people say a lot of different things. Let's turn this into a quiz to see what you think. So one possibility is that cryptographic protocols, so things were keeping secrets in encrypting data that are based on problems like factoring that are in NP could be cracked. Another is that there's a whole lot of computer science theoreticians who will be suddenly out of work because they will no longer be having to think about this problem. Another possible outcome might be that with P equal to NP that means the computers will be smarter than people. They will be able to solve problems fast that people can't. So I don't know, just tell me which one you think is true. I guess I think it's this one. None of these are exactly true and none of these are exactly false. This is kind of a fuzzy problem, but there are a lot of cryptographic protocols that are based on, on factoring like problems that are in NP. NP is a good source of secret keeping algorithm. For the same reason that NP problems are good for puzzles--that is to say, you might be able to look at an encrypted message and not be able to figure out where it came from, but if I gave you the secret information behind the scene, you should be able to read it. The people who have the key should be able to decode the message and other people not. There are definite cryptographic protocols based on factoring. If it shown the peak was NP, we actually have an algorithm that runs very quickly to solve all problems in NP, then this would definitely be true. A lot of CS theoreticians will be out of work. They will find something interesting to do. I'm not very worried about them. They're very smart people. They will presumably not work directly on this problem anymore, but there really only a handful people who spend a lot of their time directly on this. They're usually working on side problems that they hoped would build up to a solution to this problem. Whether this makes computer smarter than people or not, I really have no idea. My own research area is in artificial intelligence and machine learning, and there's definitely a lot of problems that we encounter that are NP type problems that we don't have the solutions for. If we could solve them quickly, that will be great, but I'll certainly able to believe that if the computers can solve it quickly, the human brain probably can solve it quickly too if we look at it the right way, and for now most of the problems that really seem to stump computers, these NP type problems, people seem to be able to do in practice rather well. If anything, it might help computers become similarly smart to people. All right, what we're going to talk about next is the idea that there're are some problems such that if we could solve them fast, we could solve all problems in NP fast, and we call those problems the NP hard problems. I don't find that to be a particularly useful name, but it's the name that everybody uses so I will use it as well. How can we say that some problem X is at least this hard, at least within polynomial factors, as some other problem Y. To phrase it anyway--if X is at least as hard as Y, what can we do with an algorithm for X. Well, we could use it to solve Y. In fact, that's maybe what we mean by saying that X is at least as hard as Y that we can use as solution to X to solve Y. The basic picture that you can have in your head is something like this, Imagine that we want to solve an instance of problem Y, we can imagine building a solution algorithm, an algorithm for Y, that what happen inside is instance of problem Y gets transformed in some way between instance of problem X or may be multiple instances of problem X. Then we run an algorithm for solving problem X to get solutions to those and maybe that generates more instances but after polynomial number of calls to this algorithm, what comes spitting out is a solution to the original instance of problem Y. We basically have solved problem Y by bringing to to bear our solution to problem X, or to say it another way, problem Y has been reduced to problem X. We can illustrate this idea of reducibility by introducing another problem on social networks that we'll call the S independent set problem and the problem goes like this. Given the graph H and a number S, is there a set of nodes of size S, the set is a size S not the nodes, in H such that node to node in the set are connected in the original graph H. In some sense, they're independent of each other. You can think of this as the find the strangers problem. So they need to be in the social network together but none of them know each other. So for example, if we look in this graph H for an independent set size of three, We noticed that there is at least this triple, this set of nodes where none of the pairs know each other. We can't make it any bigger than this at least including those three nodes because every other nodes that we didn't include is connected to at least one of these guys. I think this is probably the biggest independent set in this graph. All right, so now what we're going to try to do is reduce this problem to K-clique or more concretely show how a polynomial time solution to K-clique solves the S-independent set problem. So here's four approaches that we might be able to use to solve this problem. One is first note that the S-independent set problem can be solved by guessing a set of nodes, and then making sure that none of the pairs of nodes in that set are connected. All you have to do is check SÂ˛ edges or big Î˜ of SÂ˛ edges so this runs in polynomial time. Here's another possibility. Let's actually take the graph H and run the S-clique algorithm on it. That is to say the K-clique algorithm that we use when looking for a clique of size S, and whatever it returns, return the opposite of the answer. The idea being that if a graph has a very big clique in it, there's lot of nodes that are densely connected, then it's unlikely to have a large independent set. All right, here's another approach. Let's take the graph H and build a new graph G that is the complement of H. That is to say every place there's an edge in H. There's no edge in G. Every place there's no edge in H. There is an edge in G. Then we run the S-clique algorithm that the K-clique algorithm looking for a clique of size S on this new graph G and whatever answer it gives just return the opposite. So if it says that there's an S-clique in G return false and if it says there's no S-clique in G then return true. And this last option is the reverse of that, so you build the complement graph. You look for a size S-clique in that graph and if there is one say yes and if there isn't one say no. So think about these a little bit. Maybe even sketch an example problem for yourself to see which of these makes sense. All right, so the answer I'm looking for is this one, which is a pretty simple reduction. Let me step through it, just in case it wasn't clear. Just to illustrate this example, imagine we'd start up with some graph H. It's a little bit of a mess. There's a lot of edges in here but we're going to do is convert this to a new graph G, which is the complement of H, and so every place in H where there is an edge we leave the edge out, and every place where there is not an edge, we add an edge. For example, this node is not connected to this one or this one. So in the complement graph, we connect it that one and that one. This node is connected to three on the top, one on the bottom. So we connect it to this one on the top and these two on the bottom. This one on the top and these two in bottom. So we carefully constructed this complement graph G and the thing that you need to realize now is that G, this new graph, which is the complement of H has an S-clique if and only if H has an S independent set. In fact, it's the same set of nodes. So here's our four clique for example. This is similar to the graph that we looked at before. This is our four clique in G and if we look that corresponds exactly to a four independent set in H because it has all the pairwise edges in G and then it has to fail to have all those pairwise edges in H. So that's how can use the solution into the clique problem to solve the independent set problem. Now in this particular example, well it illustrates a general idea which is we take instances of one problem and we transform them into instance of another. In this case the transformation is really very straightforward. Maybe it wasn't obvious to you at first. Now that I putted that, it's probably pretty straightforward. Some of these reduction actually can be much more involved. This one actually is also really easily reversed. So if we have an algorithm for independent set. We can use that to solve the clique problem the same way. Invert the graph where I complement the graph, look for the independent set and that tells us what the clique is in the original graph, but sometimes again the connection is not always so straightforward. I mentioned NP hardness before. Now that we've talked about reduction, we can revisit that definition and say-- a problem X is NP hard if we can reduce some other NP hard problem to it, what that really means is we can use a solution to problem X to solve some NP hard problem, that means that solving X has to be at least as hard as the NP hard problem making it therefore NP hard. Now, there's something sort of irritatingly circular or turtles all the way downish about this. We want to show that some problems in NP hard we got some problem, we have to show that we can reduce some other NP hard problem to it. We need that problem to be NP hard. How do we show that that problem is NP hard. Well, we can just show that there're some other NP hard problems that we could reduce to it and so on and so on and so on, and it doesn't really seem like we could ever get anywhere this way. But the good news is that back in 1971, Stephen Cook gave essentially the original NP hard problem and that is SAT, so SAT we know is NP hard. And the way that he showed that is really ingenious, he actually showed that if you had something that could solve SAT, you could use it to simulate in polynomial time, essentially an NP computer--right. A computer that makes these non-deterministic choices, and since that's the case, anything that we could run on an NP computer, we can turn into a SAT problem and therefore, SAT is as hard as anything in NP, so that's great. Cook got us off the ground here by giving an NP hard problem. Now we have something else to target. We have some other way if we want to start that some problems are NP hard. We can just reduce SAT to it and then we're done, but at this point now, there's actually a huge number of problems that are known to be NP hard. If you encounter a new problem, it may already be known to be NP hard or it may very well be that some know NP hard problem can be reduced to it, so you don't necessarily have to go all the way to SAT to do it. So, now we're in a good position to finally define this notion of NP-completeness that I mentioned in the beginning of the unit. We can say some problem X is NP-complete if it satisfies two properties. First, it has to be NP-hard, meaning that nothing in NP is harder then X. Which is to say that if you could solve X in polynomial time, you could use it to solve everything in NP in polynomial time. And the second important property is that X itself is in NP. So its in the set and its as hard as anything in the set. So, it is in fact the hardest in this set. And though it may seem a little weird to satisfy both of these at the same time, there are actually tons of problems known to be NP-complete. Very often when you go to solve some, to develop some kind of algorithm to solve a problem and there just doesn't feel like there is anyway to get a foothold on it, uh, its a good idea to check: is NP-hard and is it in NP? If so, its NP-complete. And, uh, at this point now, tons of problems are known to be NP-complete. There is lists, there are online websites that list lots of problems. There's wonderfully written books that layout problem after problem after problem. And sometimes they are really natural extensions of each other and sometimes its really quite interesting to see how they are connected to each other. Saying that some problem is NP-complete tells us an awful lot about that problem. So, lets imagine there is some problem called graph partitioning - which is a real problem. And it is related to social networks. But, I'm not going to tell you what it is, but I will tell you that its NP-complete. Even if you don't know what it is, there are some things that you do know by the virtue of the fact that is is NP-complete, assuming that you believe me, you'd know the following: which of these, check all that apply, which of these do you know? if I tell you that graph-partitioning is NP-complete does that mean its NP-hard and therefore a polynomial time solution for that problem solves everything in NP. Do we know that the problem clique can be reduced to it? Given what we've said so far about what we've said so far about clique. Do we know that graph partitioning is in NP, and therefore an exponential time solution would exist? Do we know that it can be reduced to SAT, based on the things I've told you about SAT in this unit? Check all that apply. So the most important thing to remember when we say a problem is NP complete is the NP-completeness equation which says that NP-Complete means that a problem is in NP and its NP-Hard. Alright, so based on that knowing that graph partition is NP-Complete means that Yes, graph partition is NP-hard. And based on what we said about NP-hardness, a polynomial time solution to graph partition would solve everything in NP. We also know specifically that any problem like clique that we know is in NP can be reduced to it. so a solution to graph partitioning would solve clique specifically. >From this part of the NP-completeness equation we know that graph partitioning is in NP. And from what we know about NP means that an exponential time solution exists to graph partitioning And that it can be reduced to SAT. If somebody could solve SAT for us that would also let us solve graph partitioning. So we actually know all of these things by virtue that graph partitioning is NP-complete. Now in the beginning of this unit I did a little magic trick having to do with coloring in region in a map. This problem is sometimes called map coloring. and just to remind you, the idea there is that each of the region in the map we're going to assign some color to it with the constraint that there can't be the same color in two adjacent regions on the map. because then it would kind of make it look like one big region. We can actually think of this as a problem on graphs. In particular, we're going to make each region on the map correspond to a node on the graph and we're going to connect two nodes on the graph if they share a border and therefore can't have the same color on the map. "a" shares a border with "b" and "f" "f" shares borders with "g", "b", and "a" "b" shares borders with "a" and "f" that we haven't already and "g" and "c" "c" shares borders with "d" and "h" "h" shares borders with "g" and "e" and "d". "gcde" "d" and "e" share a border Did I get everything? "h" doesn't connect to "b". "g" doesn't connect to "c" - yes it does. "g" has borders with "f", "b", "c", and "h." Given that transformation, the equivalent graph coloring problem is going to be given a graph G and some number k, can we assign each of the nodes of the graph, a color, specifically some number between 1-K so that neighbors in the graph have different numbers assigned to them. turns out this problem comes up a lot - not just in geography. but also in computer science applications. for example, in compilers, trying to figure out how to use the registers of the computer to evaluate expressions and it also comes up in social network problems. you can imagine saying "ok i've got a group of people, here is their friend network, and what I'd like to do is group them into K different groups so that within each each group there is no pairwise friends" so you can think of this as building mixer, like a party where you want to invite your friends but you want them to interact with people they aren't already friends with So let's take a look specifically at the problem of 3-colorability. Can a given graph G be colored with three colors. So is this question in NP? Yes How do we know that it's in NP? Well we need to say two things: We have to say that it has a short accepting certificate and that it has a fast verification algorithm. So the accepting certificate in this case would be the actual coloring, the mapping of nodes to the colors. In this case, 0,1, and 2 or 1,2, and 3 How about the verification algorithm? Well let's actually code that up in Python. Alright, here's the skeleton of a program for doing the coloring verification. We're going to call an algorithm 'verify', which is like the A that we had before. We give it a graph, and a certificate, which in this case is a mapping from nodes to colors, and then the total number of colors K. Here's an example of just building up one of these certificates We've got nodes A-H, there's the connections between all the different nodes, we build a graph out of that, and then we build a coloring by looking at a list that has nodes and different colors 0 through K-1. And for each one it just sticks the color name into the corresponding node's id, and then calls verify. And I do this twice one with a coloring that's valid, and one with a coloring that's invalid. So write this verification algorithm and we'll test it for you to see if it works. Very cool. Alright, here's how my verify algorithm works. First, I make sure that we have a mapping from all of the nodes of the graph to-- Hmm, I didn't check everything-- Well here's what I checked Make sure that the number of nodes in the coloring matches the number of nodes in the graph. If not, then this is not a good coloring. Then for each node in the coloring list, well, if it's the case that the coloring for the node is not a valid color, then we say that's bad and then we check all of the neighbors of the node, and we check for each combination of node and its neighbor, whether the colorings match, and if any of the colorings match, that's bad, because that means two adjacent regions with the same color, so we return false. And if we survive all of those tests, then at the end of the day we return 'true'. The thing that I didn't check for that I probably should have checked for, is to make sure that the nodes that are in use in the certificate, are the same as the nodes that in use in the graph. That would be kind of weird. Or, if it has nodes that aren't in the graph, then this will actually crash, which is not good, so I probably should have checked for that. But the essence of it is just this: We're running through all of the edges in the graph, and making sure that the colors don't match. Since 3 coloring is an NP and SAT is NP complete, meaning that it's NP hard, it has to be the case that we could use SAT to solve 3 coloring problems, but it's actually a little bit interesting to see how you might do that. So let's take a look, here's a simple little graph and one, two, three, four nodes, four edges and imagine that we want to try to 3-color this graph. What we're going to do is we're going to create a ??? formula that is satisfiable if and only if this graph is 3 colorable, so that it's really the same problem, and the way that we're going to do that is we're going to create a bunch of boolean variables, 12 of them to be exact, corresponding to each of the four nodes--a, b, c, and d, and for each node, we'll a boolean variable saying whether that node is red or green or yellow, the three colors. For example, assigning the variable a red to true, a green to false and a yellow to false means that we're coloring the a node red, and so once we've assigned true values, trues and falses to all 12 of these variables that corresponds perhaps to some coloring. In fact, you have to be a little bit careful here because if we assigned two of these to true, which is a perfectly reasonable thing to do in a boolean formula, you can't really interpret this as a coloring except for may be a is orange, but that's not really allowed. What we have to do now is given these 12 variables, we have to create a formula that is true if and only if it corresponds to a valid coloring, meaning that both exactly one of each of these triples the variables are true and there's no collisions-- for example, we can't have a colored red and b colored red because they're connected by an edge. They will have to be different if they're connected by an edge. The formula for this can be generated automatically and I'll give you glimpse at it. Here it is, at least one version of it, and it's a little bit weird and complicated, but it actually falls into some nice structures. First there's a bunch of logic to say that it should be the case that say-- the a red, a green, and a yellow variables, one of them should be true. If they're all false, then it's not telling us what color a should be, and it also has to be case that no pair of colors can be true for a given node. It can't be the case that both a red is true and a green is true, so that's bad. There has to be at least one true and not the case that both a is both red and green not the case that a is both red and yellow and not the case that both a is green and yellow, and we do that for all four of the nodes and that tells us that there's a meaningful color assignment to the nodes. Then we have some additional clauses that say-- what can't be the case that a is red and b is red at the same time. It can't be the case that a is green and b is green at the same time and it can't be the case that a is yellow and b is yellow at the same time. What is that saying, that saying the color--whatever the color happens to be-- for a it can't match the color of b and that's because they share an edge in the graph. For each of the edges in the graph, we have three of these statements to rule out all possible color matches, so there's three of these for one edge, two edge, three edge-- there's four edges in the graph so we have four blocks of these. In this formula now as a whole has a satisfying assignment, has an assignment that makes this whole expression true if and only if there's a coloring, a 3 coloring of that graph. Now, in this case, I generated this formula by carefully looking at the graph and going back and forth in making the formula, but you can automate this idea by just generalizing the way that I did this here. Why don't you think about how to do that and I'll ask you a question just to make sure that you got the idea. What I want you to think about is, imagine that we've got a graph with n nodes and m edges, and we're going to try to find out whether it's colorable well with k colors by turning it into a big formula like the one I just showed you. I want to know how many clauses are in the formula, and just to make that a bit more concrete a clause is the sort of thing that I got here in parenthesis, it's between the n's. We have a list of things that are combined and then there's an n and then there's more things that are combined and an n, so how many things are being ended together in the formula. So I want you to figure out an actual expression in terms of k, n and m that will tell you the answer, but I'll actually ask you to do it for just a concrete example. So imagine we've got 3 colors, 8 nodes and 20 edges, how many clauses are in the formula that result from converting this 3 colorability problem into a satisfiability problem, and when you have your answer, write it in this box and we'll check it for you. All right, so let's think this through, what's going on here. So going back to the formula for a second. Remember all the clauses end up being in these groups, so there's one clause for each node that says that that node has to have one of the k colors. That's going to total up to n clauses in the formula. Then, for each pair of colors, we have to exclude that they are both true for every nodes. This is for every node, for every pair of colors in the node, there's a clause. For every node, for every pair of colors, there's a clause. All right. And that covers making sure that there's a color assignment that make sense. Then for each color, for each edge in a graph, we have a clause that excludes that that color is shared on both ends of that graph. That's for each color, for each edge, we have a clause. So that ends up being the total. There's no other clauses that we need to consider here that captures the notion of k color ability. You can just plug these numbers in 8+(832/2) is 32, 3*20 is 60, 92 was what I get, but the important thing that I wanted you to get at is this formula here, which most importantly is polynomial in the size of the graph. We have a graph of a given size that from that we generate a formula and the formula has given size and that size is polynomial. That's good because then when we run--imagine that we have some kind of satisfiability algorithm that runs in polynomial time--it's running on a polynomial size input, so it's not like we're running a polynomial time algorithm on an exponential size input, which would take exponential time to run. This way, it really does run in polynomial time. All right, this is where it actually gets really interesting and a little bit whacky. We just showed that if you a had a solution to SAT, you could use it to solve 3 colorability and maybe that's not so weird because 3 colorability is about finding some kind of a satisfying coloring and so we have the satisfying formula--not so weird. Now, what we're going to do is the other direction to show that 3 colorability is NP hard. We know it's an NP, but is it NP hard. If it's NP hard, it must be NP complete because it's an NP. But to show that it's NP hard that means we have to take some NP hard problem and solve it by way of 3 colorability. That means we have to be able to take a satisfiability formula, somehow express it as a graph coloring problem, and if we can do that, that means a solution to the graph coloring problem resolves satisfiability and therefore, anything in NP. I went and took a little bit of a short cut just so we can get to the punchline fast, and we're only going to deal not with the full satisfiability problem which gives any kind of boolean formula, but specifically 3-SAT. 3-SAT or just the satisfiability formulas have the form like so where the whole formula is a bunch of ends of clauses, so it's clause end, clause end, clause end, clause end and so on until all the clauses have been expressed. Each clause itself is the or of three literals and the literal itself is either a variable or a delegation of a variable and ???. So you could have something that looks like X or not Y or z and w or X or not z and that's a 3-SAT problem. It's not a very hard 3-SAT problem, but it's a 3-SAT problem. And with clauses, each clause has 3-literals, each literal has either the name of a variable or not the name of a variable. Now, just so happens that this problem is also NP hard, it's an NP complete, just like satisfiability but it's a lot easier to work with. So somebody walks up to us and says, "Well, I have a general satisfiability question." It might be easier if they walked up to us and said, "I have a 3-SAT question." Now, this is not easier in the computational complexity sense, it's just easier just working with it and writing an algorithm sense, but the algorithm--the problem itself is equally hard. All right, so to do the last little bit of this proof to show that 3-colorability is NP hard, we're going to show that if we had the ability to solve 3-colorability problems in polynomial time, then we could solve three SAT problems in polynomial time as well, and so what we need to be able to do to show that is if you walk up to me with any 3-CNF formula, I can quickly turn it into a 3-colorability problem, which is going to be a graph such that that graph is 3 colorable if and only if the original formula the 3-CNF formula that were given is satisfiable. So CNF here just means conjunctive normal form. It's just that form that I showed you before where the formula is the n of a bunch of clauses and each clause is the or of three literals and each literal is either variable or its negation, not the variable, so you walk up to me with 3-CNF problem, I turn that into a 3-colorability problem that can be colored if and only if the formula is satisfiable. Sounds like an interesting challenge. So let's get to it. What we're going to be doing is building a graph. We'll start off by creating some nodes with one node for every possible literal in the formula. So that's the k different variables of let's say this, just to be concrete. This formula that we come to has k variables and s clauses and we're going to add three more nodes. We'll call them true, false, and slack. I'm going to connect up the nodes initially as follows. Each literal will be connected to slack including true and false and we're going to connect true and false to each other. What that means because there's a little triangle here and we're trying see whether or not it's 3-colorable. The only way that this thing could be 3 colorable is if these three are given three different colors. So just for concreteness, let's say that the color that slack gets is red. The color that true gets is blue, true blue and the color that false gets is black, black. False won. All right, so the graph that we have so far has the property that each of these literals is going to have be colored either true or false, blue or black, if we're going to 3 color the whole graph. So that's good, that kind of make it seem like they're actually getting truth assignments. We have to be a little careful though because literal Vâ‚ and literal not Vâ‚ can't both be true and they can't both be false but that's an easy thing to fix in a graph colorability problem. We just connect them with an edge. So we add one edge for each variable connecting the pair of literals that can't both be true and both be false. Now what happens is the only way that we can 3 color this is if each variable is given a blue for either the variable and black for its negation or black for the variable and blue for the negation and that's like a truth assignment. A true/false assignment to that variable.. So any possible truth assignment, corresponds to a 3 coloring. Any possible 3 coloring, corresponds to the truth assignment. So far we're in really good shape as far as capturing the idea of the physics of a SAT problem. Now we have to introduce some extra constraints so that this truth assignment has to be satisfied. And to do that, we're going to introduce a set of nodes for each clause that is sometimes referred to as a gadget. These gadgets come up a lot in NP-completeness proofs. In this case, it's going to be a little piece of a graph but it's going to be a little piece of a graph that has particular mechanics to it that makes the graph act like a three-set problem. All right, now we're going to extend this graph by adding a gadget for each clause. A gadget is going to be a little chunk of a graph but it's going to have very particular property to it that it's going to be three-colorable in the context to the rest of the graph if and only if the corresponding clause is satisfiable in the context of the assignment. So let's imagine that we've got, this is our clause (l₁, l₂, l₃). All right, there's a couple different ways of making this gadget, Here's one that makes sense to me. Essentially, these are going to be little triangles, that kind of correspond to or expressions and the or of l₁ and l₂ or'd with l₃ is going to give us the answer for the clause. So, here's the sixth node. They're going to be connected to some of the nodes that are already in the graph. In particular, the three literals that are in that clause and the slack variable in the middle slack node in the middle and false node off the side and just to simply things, remind you that false node is colored black, the slack node is colored red, and this literal nodes are either going to be blue or black each one. And now, what I'd like to do is show you the different ways of coloring these either allow the rest of this structure to be colored with three colors or not. So let's start off with the case where all three of the literals are false. That's an important case because if the assignment is such that the literals are all false then this clause it's not satisfied. This clause is not satisfied so the whole formula is not satisfied. So this is going to be important to make sure that this graph is not going to be colorful. So given that these two nodes are connected to slack, neither of them can be red. So this one in particular can either be blue or black. If this one is color blue, then that means neither of this can be colored blue and neither of these can be colored black, so they have to be both colored red and that doesn't work. So this can't be blue and it can't be red, so it has to be black. This can either be blue or red, it doesn't matter which is which. All right, so now we have this node. Well this node is kind of obvious, it's connected to red and black, so it has to be blue. Now this node is connected to blue and black, so it has to be red. And this node is connected to red, blue, and black so we are in trouble. So the graph was not colorable when these three literals are all black. But if this literal is blue, well that means that one of the literals in clause is true, so the whole clause should be true so this better be colorable. Okay, by the same argument before, this is red and black so this one has to be blue as well. So looks like we're okay. We're able to assign colors to all these nodes so that none of the coloring constraints are violated. So that's okay. So this one is blue and these guys are black. Actually this one doesn't matter what it is, that thing is colorable. So this could either be blue or black. What if this one was blue, that's going to be okay too. We can just swap the two colors of these nodes. All right, so what that means is if this one is blue, the thing is colorable. Definitely. So one of them is true, that's if this one is true, that's enough. So let's consider the case where this one is black and one of the other guys is blue. All right, so this one is forced to be blue. Since this one is black and this one is blue, this one is forced to be red, which means this one's forced to be black which means this one's forced to be blue and now we're in good shape because this one is black and this one is red and now we have a valid coloring, actually no matter what l₂ is colored. In the same argument where it's to be switched, which one of these two is blue? We can just swap these two colors and everything is fine. So what did we just show? So, this gadget for this clause is colorable as long as the three literals it's connected to are not all black. And that's great because that's exactly what the satisfiability condition is. So we create one of these gadgets for each for each in the clauses connect them to the rest of the graph and now we get this giant graph and that whole graph is satisfied, if and only all the gadgets can be colored, which is the same as saying if and only all the clauses can be satisfied and since they're all connected with n, that's exactly what we wanted it to be. So this graph that we built is colorable if and only the entire formula is satisfiable. That's kind of cool. Right. So, even though a formula with boolean variables in it and a graph with different colors in it, feel like very different problems in a very deep fundamental mathematical way, they're really the same problem. All right, now we know that 3 colorability is NP hard. If I give you a general graph and say--can you assign colors to these nodes, at most three colors to these nodes so that no two nodes that are connected have the same color. Answering that yes or no for all possible graphs is probably computationally intractable. It's as hard as any problem in NP. What is that tell us about 4 colorability. So I want you to check all the things that are true because we know that 3 colorability is an NP, then 4 colorability is an NP because we can quickly verify a certificate that list the color assignments. So is it true that 4 colorability is an NP for this reason. It's not necessarily an NP because the number of colors is bigger than in the 3 colorability case. Is is the case that it is not necessarily NP hard because having four colors to work with makes things easier. So just because something is hard to color with three colors, it could be quite easy to color with four colors. Next, no, no--it is NP hard because the solution to 4 colorability also solves 3 colorability and the reason for that is we can just add another node to the graph, connect it to all the other ones and then try to color that graph, and then finally, since any graph that is 3 colorable is 4 colorable, the problems are basically equivalent, so it's the same. So check all that apply. There are two of them that apply. All right, so the two choices are that it is an NP and it is NP hard, so it NP complete-- 4-colorability is NP complete, and the reasons are--well, so this one's true, we can in fact quickly verify a certificate that list the color assignments. We actually did that in general already earlier in the unit, so yeah, 4-colorability is definitely an NP. It's not necessarily an NP because the number of colors is bigger. Well, no. The first argument is the correct one. It's not necessarily NP hard because having four colors to work with makes things easier. That is kind of true--four colors does give you a little bit more slack and there's things that you can 4-color that you can't 3-color, but this argument is less compelling than the next argument that says that it is NP hard. The reason for that is because the solution to 4 colorability solves 3 colorability. So if you give me a graph and say--is this 3 colorable and to help you out, I'll give you an algorithm for solving 4 colorability. All I need to do is add a node to the graph, connect that node t every other node in the graph that means that that new node is going to have to have a different color than everything else in the graph. And now you say--okay, is this graph 4 colorable and if it is 4 colorable, then the color for this newly added node would be different from all the other ones, so the rest of the graph has been 3 colored, so it actually solve 3 colorability, and that's why knowing that 3 colorability is NP hard tells us 4 colorability is NP hard. And, no no's. Since any graph that is 3 colorable is 4 colorable--that's true. It doesn't make them equivalent because it would also have to go the other way, every 4 colorable graph would also have to be 3 colorable and that's just not true. So this is actually the proof that 4 colorability is NP complete. I want to describe this next result just because it just shows how weird computational complexity results can be. Let's think about the colorability of planar graphs. In particular, here's a decision problem. We're given a graph G and a number K. Is planar graph G K colorable? And let's look at how the difficulty of the question differs as we change K. So K is 1. What kind of graphs are K colorable? Graphs with no edges. So we can answer this question in constant time if the number of edges is greater than 0, no. As soon as there is a single edge, then you're going to need to two colors to color the nodes in that. All right. What if it's two? We want to know if this graph two colorable. We'll you know this one already because two colorability is exactly the same as testing whether the graph is bipartite and in the previous unit, you showed that that could be solved with depth-first search and therefore, runs in big Î˜(n+m). In case three, well 3 colorability we showed was NP complete, but we didn't do that for planar graphs, we did that for general graphs. but it turns out that you can actually introduce another gadget into the mix here that actually let you deal with edge crossings. So if you have a piece of the graph that looks like this and there's a crossing in it, you can actually introduce some nodes here so that the information from this node travels along and gets to this node without ever having to cross another edge. It becomes a planar graph. I didn't explain how to do this but it turns out to be true. A three coloring of planar graph isn't any easier than three coloring in general graph, it's NP complete. What about coloring a graph with four colors if its a planar graph. Well, remarkably, the problems gotten harder and harder from constant to linear to basically it seems like it requires exponential time, it goes back to constant time, and that's because of the celebrated 4-color map theorem that says that any map can be colored with four colors and in this particular case, any map can be translated into a planar graph and so we know that every planar graph can be colored with four colors. So the constant time algorithm for this problem is just to always say yes. You give me a graph and tell me this is planar and I say, "Yup, it's four colorable," and therefore, it's also going to be five, six, seven colorable because you can just use the first four. So that's kind of cool. So one of the things to point out is that the peak of the hardness comes at K=3. That actually shows up a lot in NP complete problems. Usually if there's two of something that you have to choose from, the problem is pretty easy because you can try one of them and if it doesn't work try the other one right away, but when if there's three, you have to keep considering lots of different combinations of things and the difficulty level really, really blows up. That actually does it for Unit 6. We basically got to the main points that I wanted to tell you about. I wanted to kind of express the notion of NP completeness and give you a chance to play with that a little bit, and in the homework, we're going to look at that in a little bit more depth. So I will give you more practice problems to think about. In a smaller class, you would probably be writing proofs, but for this class, you're going to write some programs and do some multiple choice questions. So good luck on the homework, hang in there. We're almost done. The next unit is just going to be interviews and a song that I put together for you, and a kind of a summary of what we did in these units. So thanks a lot for listening. Hi, everybody. Welcome back. We're in unit 7, which is the last unit of our class. In this unit, we're not going to cover any new material, and there's no quizzes or homework that count for anything. Instead, I want to accomplish two things. My main goal for this unit is to remind you of what we learned and to put it into a broader context. I've asked some colleagues to help me out by sharing some of their experience with us over Skype. Our first guest is Peter Winkler who is a world expert in discrete mathematics and combinatorics and a mathematics and computer science professor at Dartmouth College. His work has had substantial impact on the design of algorithms and theoretical computer science. He is also a celebrated collector of mathematical puzzles, having written two books on the subject as well as a monthly column for the communications of the ACM. Now, since I've started off every unit with a mathematical magic trick, I thought Peter would be an excellent person to beguile us with a few fascinating tricks of his own. [Peter Winkler, Professor, Dartmouth College] I have a puzzle from my book about a spy who is trying to convey information to her control, but the only means she has for conveying information is that there is a 15-bit radio broadcast every morning, and she has the ability if she wishes to alter 1 bit of that broadcast. But she doesn't know in advance what the broadcast will be. So, this is a case where, potentially, there are 16 different things she can do. There are 15 bits she can alter, or she can choose not to alter any bit, which means that in theory perhaps she could convey as many as 4 bits of information this way. Well, surprisingly, she can actually do that. All right. Let's make sure that we understand the setup here. I'm going to do this as a magic trick. So, I'm a spy, and I'm in enemy territory, and I've got a message--0110-- that I need to transmit to my boss. Now, that's going to be tricky to do, because I don't want anybody to know that I'm a spy in enemy territory. Fortunately, there is a radio broadcast tower in enemy territory, and everyday it sends out a 15-bit message. There's an example of a 15-bit message. Now, I can use that message to communicate with my boss, but I can't just change it arbitrarily to something completely different, because then people will know that something weird has happened. But I can flip just 1 bit. I can zap the message and change just one of the bits, and my boss then is going to hear that, and we'd like him to actually interpret that as the message that I want to send. Let's see if it worked. Hmm...based on the 15-bit broadcast I'm hearing, and the protocol that my spy and I arranged in advance, I know that the message he is sending to me is 0110. Nice work, agent P. Ta-da! Here's what you can do, and the solution involves nimbers, which come up in other problems as well. A nimber is just a binary number. What makes it different from an ordinary number is that when you add two of them, you add without carry. So, they're added like vectors in a vector space over zÂ˛. They were called nimbers by John Conway, because, as you may know, they are useful in analyzing a game of Nim. So, what we can do is we take these 15 bits and we number them from 1-15, but instead of thinking of those numbers as ordinary decimal numbers, we think of them as nimbers. Each position has a non-zero nimber starting with 001 for the first position and ending with 1111 for the fifteenth position. What the spy wants to do is to convey some nimber of her choice to her control. For example, suppose to wants to convey the nimber 1100 to her control. What she's going to do is she's going to simply tell her control, "Look at this radio broadcast after I've altered it, and add up the nimbers of all the positions of where you see a 1." Now, when the spy sees the radio broadcast, she looks at what the nimbers currently add up to. Say the nimbers currently add up to 1010. Then if she changed the 1010 bit, they would add up to 0000, so that's what she would do if she wanted to convey 0000. She wants to convey 1100 so she adds that to 1010. That gives her a nimber. She changes the value of the bit at that nimber. It works. Marvelous. All right. Now that you know the trick, let's see if you can decode a message yourself. Here's the message after it's been altered by the spy-- the 15-bit code that you see in front of you. What I want you to do is figure out what message the spy was trying to send and type it into the box below. Now, the message itself is in binary, but just type the digits--the bits in--the four bits. All right. Here's how we decode the message. We start off with the 15-bit message. We write it down. Then we write the bits down in a column in order, and we label each bit with its position number in binary from 0001 all the way to 1111. There's 15 bits. We're starting with 1. That's how we get the codes. Then we make note of which bit positions have 1s in the broadcast message, and then we pull out those numbers, and we add them together with wraparound. So, in the right-most column, we have 00110, and that's three 1s, so the sum is going to be a 1. It's an odd number of 1s. In the second column from the right, it's 011010. That's three 1s again, so that sum will be 1 in that position. In the third column from the right, 111001. That's four 1s, so we'll have a 0 in that position because it's even. In the first position, again, three 1s gives us a 1. So, that is the message that we're looking for--1011. What is it about math and these kind of tricky puzzles that just intrigues people so much? Have you spent time thinking about that? I have a little bit, and I don't take it for granted that it intrigues everyone. Meaning it doesn't actually intrigue everyone, but it intrigues a lot of people. Even mathematicians and computer scientists-- many of them run when they hear a puzzle. Oh, no. They'll say, "I'm no good at puzzles. I'm leaving the room." It is certainly true that you can good--in fact, you can do great mathematics-- without being a puzzle-solver kind of a person, and great computer scientist, great computer science as well. There are some kinds of math and computer science where you create structures of things, and you don't try to solve puzzles. You try to create structures. You try to define things. You can do all kinds of great stuff this way. Not all mathematicians or computer scientists like this kind of puzzle or like puzzles of any kind. On the other hand, lots of people who are not mathematicians and computer scientists love mathematical puzzles. There are loads and loads of amateurs out there who adore this kind of puzzle. There are lots of people with mathematical talent, logical talent, people just with good creative minds who are not doing mathematics or computer science or any thing like that who are really good at this sort of thing. If you like puzzles, obviously, they're good for mental exercise, keeping the mind sharp, but mathematical puzzles really do introduce you to techniques that you can actually use in solving problems that you run across, and I gave examples of this as you just saw. These problems arose and the techniques used to solve them can be used to solve other problems. They give you new ideas. They help you think creatively. On the negative side, solving puzzles could distract you from doing something more important. But I like to think that you can't do important stuff 24-hours a day. You need to spend some time doing stuff that's just fun, and if there's a blurry line between what's fun and what's hard work, so much the better. But there's also something very satisfying at the end when all the pieces fall into place and you didn't know why it had to be 15 prisoners or whatever it had to be, but everything kind of makes sense at the end, and I think some of us find that extremely gratifying. I think I agree. It's just a little bit like--many puzzles are like riddles. Once you see the solution, once you can really grasp the solution, you've "ah-ha!" It's that kind of thing. I have to say that there are also other kinds of puzzles, not sort of the riddle-type puzzles, but puzzles which challenge your intuition, and I like those a lot too. Those have a different function, which is to keep your mathematical intuition from running off the rails, to get you to distrust certain notions of, for example, probability. Probability, which we humans invented to try to deal with all the uncertainty that's in the world from our perspective, is something that we don't always know how to deal with. Puzzles involving probability, especially ones with very counterintuitive results that you try to prove or disprove, can really help you keep sharp. This one became quite famous. It's sometimes known as the Names and Boxes Problem. In this problem, there are 100 prisoners, and the warden comes and tells them that they will be given their freedom if they can win the following game: 100 boxes are going to be put on the table. In each box will be put one of the prisoner's names on a piece of paper. They'll be lined up on a table in a room, and the prisoners will be brought into the room one-by-one, and each prisoner is permitted to look in 50 boxes, trying to find his own name. When he is done, he is escorted out of the room. He must leave the room exactly as he found it. He has no further communication with the other prisoners. In order to win the game, every prisoner must find his own name. Now, this sounds really completely hopeless. Just say again how many prisoners and how many boxes? There are 100 prisoners. &gt;&gt;100 prisoners. 100 boxes. One name in each box. &gt;&gt;100 boxes. Oh, you can only look in 50 of the 100. But each prisoner can only look in 50. Any individual prisoner's probability of finding his own name is 50%--one-half. Nothing he can do about that. So, how do you do this? What's your algorithm? Well, one possibility is everybody looks in a random 50 boxes. That gives you a probability of winning the game of 2⁻¹⁰⁰. Not very good. Another algorithm is everybody looks in the same 50 boxes. That has a probability of success of 0. Right, which is essentially the same as the first one. &gt;&gt; Indistinguishable. All right. So, the story with this puzzle is that Peter Bro Miltersen and Anna Gal, a theoretical computer scientist that you may know, were working on a problem, and they needed to be able to show that in a certain version of this problem, that the prisoners had no algorithm which would give them any decent probability-- one over a polynomial or anything like that-- and they didn't see how to prove it, and one day Miltersen happened to be passing a problem onto a friend of his, Sven Skyum, over lunch, and Sven Skyum said, "Well, have you tried this?" And Miltersen told me he fell off his chair. It turns out that there is an algorithm which will guarantee the prisoners a better than 30% chance to win the game. Irrespective of what N is here, it works for 100 prisoners, but also for 1 million prisoners with probability of winning as better than 30%. Even after you see the solution, it's hard to believe that this really works. Remember, every individual prisoner has probability of only 50%. There's no communication. How could you possibly get enough correlation into this problem to have the probability of all of them finding their own names to be 30%? It's ridiculous. But here's the algorithm that's absolutely wonderful. The prisoners first assign ownership of the 100 boxes randomly. In other words, they choose a random permutation of the boxes and assign ownership of the boxes, and every prisoner gets to know all the ownerships, gets to memorize all the ownerships and everything. Of course, these ownerships may have nothing to do with the names that are inside the boxes, but that's okay. So, now prisoner A goes into the room, and he opens his box--he opens the box that's been assigned to prisoner A. He looks inside, and it's got some other prisoner's name in it, maybe prisoner G. Then he goes over and he looks like prisoner G's box, and maybe prisoner Q is there. Then he opens the box for prisoner Q. He opens it and finds prisoner C there, etc. He keeps doing this, and he's hoping that he'll find his own name. If he doesn't, too bad. Then he leaves the room. The next prisoner does exactly the same thing. Prisoner B goes in, and he opens the box for prisoner B, looks for at the name inside, opens that box, and hopes that eventually he'll get back to his name. That's the algorithm. Right? Why in the world should this work? Right? What's going on here? Why would this be better than just going through in their permutation order or something like that? It's not at first at all clear why this would work. In fact, the only way I know people come up with this algorithm is they're looking for something for prisoners to do. The problem is once you think of this algorithm to now dismiss it but instead to think about what would make it work. What's going on here is that this random permutation combined with the permutation of the prisoner's names in the boxes gives you a permutation of the prisoners, which is a uniformly random permutation. But the permutation is defined in the following way. You take a prisoner, you look inside his box, you see whose name is there. That defines the permutation. I said that prisoner A's box contained the name of prisoner G. That means that in the permutation, A goes to G. So, what's going to happen is that prisoner A will find his own name if in this permutation the cycle that his name is in, the cycle that A is in has length at most 50. If that cycle has length less than 50--length of 50 or less-- he comes back and find his own name. Otherwise, he doesn't. So, what does that mean? Well, that means that the prisoners will actually win this game if there are no long cycles. Because then everybody will start in their own cycle come back around, and find their own name. Now, it turns out that it's actually very easy to compute the probability that a permutation contains a long cycle. Wait. Hang on one second. Not only short, but it has to be short and contain that prisoner's information, that prisoner's name. Right? Well, we know it contains that prisoner's name, because he starts at the box that he owns. For example, suppose the cycle is A, G, C, A. Oh, the only way to get back to it is if you found the name. &gt;&gt;That's right. Ah. Okay. All right. So, now your next step was you can find the probability that there is a long cycle. It's to compute the probability that a random permutation contains a long cycle. Well, it's actually quite easy to show. It's just a little--just write down the binomial coefficients-- that the probability that the permutation contains a cycle of length k where k is bigger than n/2 is exactly 1/k. So, the probability that our permutation contains a cycle of length greater than 50 is 1/51 + 1/52 + 1/53 + … + 1/100, which is about n log 2--0.69 something. So, the probability of the prisoners winning is about 0.31. You know, you could sit and stare at this thing and still not really understand why it works so well. &gt;&gt;Crazy. So it's go harmonic numbers in it and logarithms and a kind of simple tail-chasing kind of algorithm. That's right. It's just nice reasoning, right? Nice, undergraduate reasoning. Our second guest is Tina Eliassi-Rad, a computer science professor at Rutger's University. She studies data mining and machine learning with a special emphasis on the data that comes up in network science. There's been an explosion in interest in studying networks and network science. If you could comment a little bit about where Big Data is coming from. [Tina Eliassi-Rad, Associate Professers, Rutgers University] Big Data is being generated by us. We go on Twitter and we tweet away. We go on Facebook, and we post photos and we post status updates. That's actually where a lot of the Big Data nowadays is coming from. Also, the other aspects, if you think about healthcare-- all the MRI imaging and all these other kinds of tasks that now are done with computers, that's where Big Data is coming from. In particular, because the idea is that if you analyze the data, that data will tell you something about your customers and then you can better serve your customers and then your profits will go higher. Could you speculate a little bit about how a company like Facebook could use information about things like users clicking on a like button on something to suggest possibly people to be friends with? [Tina] Right. A simple way is this idea that -- Let's assume we have Mary, John, and Jill. And Mary is friends with John and Jill, but John and Jill are not friends with each other. Every time Mary posts something, both John and Jill like it. So, because they keep liking the things that Mary posts, we think, wow, they should like each other as well and want to be friends. In fact, another reason that it makes sense to have them be friends is that they're both friends with Mary, and so they share a common friend and they like a lot of the stuff that Mary is posting. So, that would be a good friend suggestion. The more friends they have in common, the higher this idea of, oh, they would want to be friends, because they share a lot of common friends. This is a whole area called tie strength about how strongly am I tied to you or would I be tied to you? We actually recently had a paper at a web science conference on this. When I actually go out of my way and like something, it means that--and you go out of your way and like it-- it means that maybe we should be friends, because we like the same kinds of things. It's also connected to the concept of homophily-- this idea of like attracting like. It's been around in history for a very long time. In fact, it dates all the way back to Plato, which says similarity begets friendship. Or for example in more common day we say "Birds of a feather flock together." In a social network, links tend to connect people who are similar to one another. So, I'm similar to you and so I would like to be your friend, and so I have a link to you. If, for example, you want to find out about whether I'm a good guy or a bad guy, you can look at my friends and see if the majority of them are good guys or bad guys. If the majority of them are good guys, then you would guess, well, Tina must be a good guy, because she's friends with a lot of good guys. And that's where also this idea of egonets comes along. An egonet is a term in sociology, and it refers to the ego. And then I have my alters, which are my friends. So, my friends are my alters, and the person you're considering is called the "ego." The egonet is the network that connects the ego to the alters and that connects the alters together. You can have egonets that look like a star where you have the ego and the alters, but none of the alters know each other. Or you can have a clique where everybody knows each other in this egonet. The ego obviously knows all the alters and is friends with all the alters and all the alters are friends with each other. In fact, going back to this idea of recommending links, the closer--the more cliquey--your egonet is, the more links there are in your egonet, the more likely it is that the alters that aren't connected with each other would want to be connected to each other. Now, we looked at something in the class called the clustering coefficient. Is it safe to define that as the density of the connections among your alters? Or, if you want to think about it, the number of triangles that exist, right?, in your egonet. Selection is the tendency of people to form friendship with others who are like them. If you wanted to target an outreach for a particular product, you're assuming that there is selection going on. Influence is the fact that we're already friends, and people may change their behavior to bring them more closely into alignment with their friends. That's where, for example, a lot of viral marketing is good for. If you're trying to push something where the underlying cause is social influence, then you would want to go with viral marketing. So, with selection the individual characteristics drive the formation of friendship links, and then with influence is the existing links in the networks to people's characteristics. So, I buy iPhone and really like it. I play with it. That influences you to go buy an iPhone. Usually when people talk about influence they also talk about contagion. It's basically the same kind of term. People use it interchangeably. There was this very famous study back in 2007 that got a lot of publicity that was done by Professor Christakis at Harvard and Professor Fowler at University of California, San Diego, and they looked at how obesity is caused by influence, that obesity is contagious. They have this thing where having an obese friend increases your chance of obesity by 57%. That was when it got like everything was contagion. Happiness was contagious, everything was contagious. There's all these--like Time magazine had a front cover, and Wired magazine and all that. Then when people looked more into the model they found that, well, wait a second. If I follow the model, then tallness is also contagious. Unfortunately, it's not, because I would like to be taller, but I'm not. It really gets down to this idea of correlation and causation. Lots and lots of interesting work with real applications. Like if I have a product that I want to spread through a network or if I want people to change their behavior and stop smoking, is it influence or is it selection? Because depending on which one it is, I should do either viral marketing or targeted outreach. So, it's very important. In her interview, Tina mentioned the concept of homophily. What is homophily? Is it the observation that two words tend to be pronounced similarly if they're in a social network? Is it the fear that two people connected in a social network might develop a blood disorder that interferes with clotting? Is it the political movement that suggests that people should be allowed to marry people of the same sex? Is it the idea that nodes that are connected in a social network are also more likely to share other characteristics? Or is it the term used to refer to individuals who reside in the largest city in Pennsylvania? Of course, the idea that nodes connected in a social network are also more likely to share other characteristics is what homophily is about. Can you say a little bit about how social networks fit into the history of the use of technology in political protests? Right. This is a really an interesting topic in that with the dictators, what they want is a lot of people to believe that everybody is happy with what's going on. They don't want a lot of people to know that people are unhappy. What happened was during the 1979 Iranian revolution, people were passing information through cassettes, for those of us who are old enough to know what cassettes are. During the Tiananmen Square uprising, people were passing information through fax and with the use of fax machines. Now, with smart phones and Twitter and Facebook, people can pass information very rapidly, so I would know if there was a protest how many of my friends are going to go or how many people will go. If I feel like a good mass of people are going to go and that we can really damage the government, then I would go. We have models that show whether the particular movement will spread or whether it will stop and so on and so forth. It's very exciting work. Network science and social network analysis has come into service here, because if you think about cybersecurity, what is cybersecurity? What is cyber? Cyber is just a collection of machines that are networked together. I send an email from my machine and so on and so forth, and that's then an information network. Underlying it is a physical network of machines that are linked to each other. You can think of it, well, if I'm a bad guy, and I want to spread a virus through this network, how will I do it? It's similar to this idea of epidemiology like between people networks and how does it spread and whether you are more susceptible to get a virus than not. If your PC is more prevalent, maybe you're more susceptible to some of the viruses that are out there, because the majority of viruses are for PCs right now and less for Macs. Or, for example, if I'm a good guy, how would I stop a virus from spreading? Which links should I take away or which nodes should I take away? Nodes here being computers, right? Actually, this also is analogous to people networks. If I don't want a piece of information to spread, which person should I take out? For some definition of take out. I've been watching a lot of murder mysteries. You have worked for the government. So, this is how it comes in cybersecurity, and in fact if you think about it social networks are built on top of information networks and that's built on top of physical networks, and so the interdependency between these networks is very interesting to study and see, well, if I take out part of a physical network how will that affect the information network and the social network. In fact, we have seen recently, for example again, with the demonstrations when the governments take down their Internet, because they don't want people to talk to each other, but we're seeing that even though they take down the Internet, because there's a phone network and the Twitter apps and all these other apps, are also through phones and voice that people can communicate information now, which is awesome. Our next guest is Andrew Goldberg. He's a principal researcher at Microsoft Research, Silicon Valley, and his work focuses on creating algorithms for real-world problems. One of the interesting problems he's studied is how to find the shortest path in really, really large networks in microseconds, so he's going to tell us a little bit about algorithms for doing that. [Andrew Goldberg] [Principal Researcher, Microsoft Research] Recently I've been working a lot on shortest path algorithms, especially motivated by GPS navigation applications, so basically how to get from A to B. That's great. When you analyze this problem, that problem itself is fairly classically studied. What are the new wrinkles in it that come up, and why do they come up? So, the new wrinkles came up when GPS navigation became very, very widely used, and also GPS maps became continent sized and detailed, digital maps. And then you really wanted to solve the problem much faster than in linear time. Basically when Bing Maps or Google Maps gets a request it doesn't have the time to look at the whole map, so linear time algorithms like your classical Dijkstra's algorithm are not good enough, and the new wrinkle was preprocessing, so you want to preprocess your graph to be able to answer queries very, very quickly, sort of in polylogarithmic time if you want to put your theory hat on. During the last 15 years or so there was a lot of research both at our group and also in many other places, and there have been very nice algorithms developed which 10 years ago I wouldn't have believed that it's possible, but basically these algorithms can answer queries in microseconds on continental-sized networks. Wow, and it doesn't pre-store all possible pair wise-- No, so just to give you an idea of scale a continent-sized network has tens of millions of nodes, so 10 billion squared is too big even for today's huge disks. Are there any relationships between the kinds of graphs that you get in highways and the kinds of graphs that would come out of a social network? We did recent variation studies under submission now, and we studied some of the publicly available networks, and the sub-labeling algorithm I want to talk about next has actually worked fairly well for this kind of network like quarter networks and so on. But for example, it doesn't work so well for small world kind of networks. Okay, interesting. All right, good, so if you wouldn't mind telling me a little bit about the algorithm, I think that would be really interesting. Let's talk about implementing just the distance oracle, so basically given 2 points, you want to tell the distance between those 2 points. The algorithm first preprocesses the graph, and for each vertex it computes labels, and let's say for simplicity the graph is undirected. Then a label of a vertex is a set of vertices which we call hubs and distances to the hubs from the vertex. Each vertex has a label, and these labels must have the following property. If you take any 2 vertices, the set of hubs has to intersect, and the intersection mark contains a vertex on the shortest path between them. And why it's important is that for this vertex if you sum up the distances to the 2 hubs, which you have, you will get the shortest path distance. The sum of results, so all the hubs' shortest path distance is stored? No, for each vertex you have a set of hubs. The distance from this vertex to each hub. [Male] Oh, I see, and they have to share a hub. On the shortest path. That is also on the shortest path, I see. Right, so this is a fairly strong property, so the easiest way to do it is you say, okay, for each vertex all other vertices are at hubs. [Male] Then you're guaranteed. And then the property holds, but then your queue at a time is order of n, and what you really want is small labels, and it turns out that some graphs have more labels, and the reason why this works well in road networks is that we can compute labels for, say, the graph of Western Europe with about 18 million vertices. We can compute labels of size about 70. 70, 7-0.&gt;&gt;[Andrew Goldberg] Yes. Out of how many million did you say? 16 million? 18.&gt;&gt;[Male] 18 million. You only need 70. It's very, very fast. If you think about that if you sort these hubs by node ID we have 2 arrays, and you just need to intersect these 2 arrays of size 70, which you can do like a merge sort that is very good locality. This time becomes below a microsecond. It's very, very, fast. But this is not a very intuitive concept to me so what are the 70-- so we're talking about like individual intersections in Europe, right? This is like Piccadilly Square or something like that or the northeast corner of Piccadilly Square, and you only need to know from there the distance to 70 other places in Europe. This is the amazing thing, and that's why people thought that this wouldn't be practical because if you think about it there are probably thousands, but the surprising thing was that you only needed a much smaller number, so if you think about this long range, it's basically intersections of major highways, and there are not that many of those, and sort of locally it's intersections of state highways and more locally it's intersections of major streets. The bad graphs of this kind of algorithm are grids, but fortunately there are no big grids in the maps, and even though there are grids like in Manhattan it's only 10 avenues wide, so it's not very big, and there is Broadway, which breaks the symmetry, so things are not as bad as one might think. That's fascinating. Of the 70 places, I could imagine that some fraction of them are for really long distance travel, like between countries, and some other fraction of them are for within the country but far distance, and then another fraction of them are for within the region of the country, and is it sort of equal buckets for each, or for really far distance you only need a small number, but for local you need more? What's the distribution like? It's roughly uniform as you increase the scale exponentially. [Male] Wow, wow. That really depends on where in the country you are because in that densely populated area you need more local things. If you are in the middle of nowhere-- [Male] There's only one way out. Andrew Goldberg described a data structure that can be used to very rapidly compute shortest-path distances between nodes in a network. The way that it worked was each node of the network has a label. What a label is is a list of other nodes in a network that he called hubs along with the distance to that hub. The hubs are chosen for each node, the labels are chosen for each node, so that they have the property that for any two nodes in the network, any two nodes where you want to compute a shortest-path length between them, will have some hub in common in their labels that is on the shortest path between those nodes. Let's make sure that you have a feel for this idea, and let's look at a particular kind of network--a star network-- where there is one node in the middle and all these other nodes directly connected to it. How many hubs do we need to list in the labels of each of the nodes in a star network so that we can make sure that we satisfy this property that we can compute the shortest path length sufficiently, specifically because for any pair of nodes in the network their labels will have some hub in common. Why don't you type the number into the box. You can use this particular star network as your example and give the answer for this star network, but it turns out it actually works for star networks of any size. With this kind of star network in particular, the datastructure that you need to set up is actually quite simple. We need to be able to compute the distance between any pair of nodes called say a which is labelled here in the graph, and b labelled here in the graph and they have to have lists of hubs such that the uh... the hub has to be on a shortest path from a to b, well all the paths from a to b in fact all the paths between anything in the network have to pass through x so as long as everybody has their distance to x which is 1 if it's an unweighed network but there could be weights on these edges as long as each node knows its distance to x we'll be able to intersect the lists for any pair of nodes and know the total distance from node a to node b. uh... one little bit of trickiness there though is that notice that x's list also has to have x on it but that's ok. It can just have x with a distance of zero so it can be its own hub and then we still should be fine we'll get the distance from a to x which is whatever the distance from a to x is plus zero so one is the answer I was looking for. Do you think that doing good theoretical analysis and doing that in conjunction with the development of algorithms leads to better, more practical systems? Yes, of course. Data sets nowadays are mostly fairly big. You cannot just rely on hacking to get setups you need. You really need something which works well in an asymptotic sense. So, asymptotics is very, very relevant now. On the other hand, there is a big gap between theory and practice often, and we really have to close this gap. It's hard for developers--they don't have the background and also they don't have the time to read the latest paper within the time they have to develop a product to implement that. I think it's very important to come up with algorithms and investigate their practicality and describe their practical variance. Do you see that happening by people working in teams? Or do you think like each individual has to be expert in both the engineering side and also the analysis. No, it's best done in teams, because there are some individuals who are good at both, but in general there are people who are better at inventing new math, and there are people who are better at engineering new algorithms. It can be done in teams or also in a process of scientific publication. Someone publishes their theoretical algorithms, and then someone else gets a good implementation of it. For me, algorithms have several components. There is a mathematical component of algorithms, which is formal analysis of algorithms, and then there is an engineering aspect where you try to implement the fastest algorithm you can on a real problem. I'd like to think the approach which is data-centric method, which combines the two approaches. So, you sort of observe the real world and make some computational models, and then to try to design an algorithm for it and prove some things about it. Then we implement this algorithm and see if it performs as theory predicts, and if not, you try to modify the theory and does that explain that behavior of this algorithm or invent a better algorithm. How do you even measure the asymptotic running time in practice? Do you run bigger and bigger random examples? That sort of thing? It's easy on synthetic data, yeah. If you have a certain generator which you can set parameters, then you can measure how an algorithm acts. On real-world data it's much harder. Right, because it doesn't necessarily scale in a natural way. And also for old random examples there are only two natural examples. Namely, Europe and North America. Our next guest is Vukosi Marivate, who is currently a graduate student in computer science. He studies machine learning, but he's also a keen observer of the impact of technology on world cultures. He's been in several technology companies, and he's going to share some of his experiences with us. We have with us Vukosi Marivate. Vukosi lives in South Africa, and he's currently going to graduate school in the United States to get his PhD in computer science. He has interned at a number of different companies, and this summer he interned with a company called Meebo that does some social networking. Can you tell me a little bit about Meebo? What is it that they have been doing and where are they now? [Vukosi Marivate, Graduate Student, Rutgers University] Meebo was a company that did a couple of things. One was that they had a chat app. Another is that they also did advertising through something called Meebo bar. They also then had a new product that was being developed to give you news, so you would actually log into the service, put up what your interests are, and you would either be friends with other people on the network, and from there would actually mine the data that's created by all these people to actually now give you news and information that's relevant to you. Do you have a sense of what sort of algorithmic work they had to do to support their company? In most cases when you're looking at networks, the things that are very interesting to do first are dead simple ways to actually recommend who people should be friends with. This goes across the spectrum whether you're on Facebook, on Linked In, on Twitter, or Meebo, and Google+. The other things that become more interesting is finding out who's important in a graph. This is actually not as simple as just looking at who has the most connections. Either has the most followers or the most friends and things like that, but actually be who is the person where information flows through. There are algorithms that try to actually do this, and people are developing them using different metrics to try to figure out who's important given a topic, let's say. Finding out who is very important given ice cream in California. Those kinds of problems normally require you to do some mathematical modeling of the networks that you have. Do you have a sense of what a really simple example of an algorithm might be that would determine importance? Maybe not what people are really doing. But something related to it. One could be looking at giving keywords that people use. Given who uses those keywords, and then looking at if a person uses that keyword, who then after that shares that keyword more and more. Just looking at something that's called a "cascade"-- how information moves through a network, but if you can look at the origin of where that keyword started and see how fast it grows, maybe, or how many people it gets to, you can then go back to figuring out who then is important. So, if you can figure out, let's say, who is important in burgers, right, then you could then target advertising towards that person. You're advertising your product, and if they spread it, then you can then get to more people. So, it's different to advertising a burger to me. I probably don't have a lot of people who follow me because of my knowledge of burgers. But there could be other people out there who people follow because of their information, so if you can figure out who those people are, then you can advertise to them, and they can spread information. I did a project at Rutgers where it tried to figure out who was important given an edu-tainment TV show. In this case, I was trying to found out who was important in trying to spread information about HIV. In this case, it was not advertising but it's actually very important in educating people. If you can figure out who spreads that information well, you get to more people and you can have more impact. That seems really important--yeah. I mean, not just--all over the world, right? There's these issues that are taking place. What's your sense of what social networks are like in other countries outside the United States? Is it similar to the way it is here or are there variations? Because of differing bandwidth constraints, you have some social networks more used in some countries than others. You have countries that use Twitter, because it's very easy. To get on it's very low bandwidth, and also it spreads information very quickly. More quickly than Facebook, and it's open. Obviously, everybody knows about the Arab Spring now, and Twitter was very pivotal in that. In some countries there are other social networks that are very big there. They might not be big in the United States. In South Africa and some of most of Africa use something called "Mxit"--M-X-I-T, which is based in South Africa. It was created there. It has a huge user base in South Africa. I think Orchid is still very big in South America, which isn't quite as big as Google, but it's still spreading. Just locally it's very big there. Can you give me a hint about why Microsoft cares about any of this? Well, Bing Maps is an obvious example. Bing Maps computes driving directions. Once you have the algorithms which are that fast, you do other things like taking real-time traffic into account. Another application is in local search. The reason it was an efficient way to find the nearest restaurants or the nearest Italian restaurant which is open now and accepts American Express and things of this nature that can be very useful in local search. The latest stat is that we have potentially more jobs than there are graduates with these type of skills. It's become more and more important to kind of mine the social graph in that we have way more data that we know what to do with, and we're starting to actually recognize that we have so many networks out there. Now, we're collecting the data more than every. Being able to now leverage that data into something that can add value to a company is becoming more important. What would you suggest for students to be studying in school so that they could be prepared to contribute to this area? I think getting a good background in basic computer science, including a mathematical background like discrete mathematics and programming and so on is important. Also nowadays knowing systems is important because there are lots and lots of hidden power of modern computer systems such as parallelism, concurrency, various levels of memory hierarchy, and so on, which is something that clearly doesn't capture very well, but you can get several orders of improvements in your algorithm by taking advantage of this. I highly recommend the Easley-Kleinberg book called Networks, Crowds, and Markets. I have the book jackets here, so you can see it. But it was published by Cambridge University Press in 2010. A free version of it is available on the internet, so you don't have to buy it. Again, it's called Networks, Crowds, and Markets. It's a great read. They went to great pains to have very few Greek letters, so they are just trying to get the concepts to you. Also, at the end of very chapter it has like advanced materials, so if you want to delve more into it and they have beautiful citations and lots and lots of reference, but it's a great, great book. Knowledge of algorithms is good, looking at social networks in a different way. Not just as social network but actually starting to look at how social networks actually work. There are papers that come out very regularly, and you don't need to go to like Google Scholar. There are different industry websites that keep track of social networks, like Mashable, that you can actually look at, and they sometimes send out very nice analysis posts on what's actually going on, and just keeping yourself abreast of this will make it more advantageous if you are interviewing, because you will be able to actually speak, use the right wordings and will also get you to actually appreciate the math and the science behind the networks. I see. So, spending a certain amount of time wasting time on the Internet, is actually a really useful thing. Not all the time, mind you. Yeah, in this case it's good wasting. &gt;&gt;Yes. Good wasting. In some sense the trend in web pages-- in the beginning it was only really technical people who could make their own webpages, and now it's getting easier and easier to just make use of sites that exist to put up a webpage of your own. Could you imagine social networking somehow going that way as well, where people can just try out and make their own little social networks just like blogs? So, people go on things like Twitter, because you have so many people you can actually mine information from. That's what people do when you search the search tags on Twitter or on Facebook or on Google+ is actually you're mining the social network. You're getting information from the network itself, not just from the webpages. Creating your own, unless there is a specific use for it-- so, let's say you were doing a medical practitioners network in the U.S.A-- maybe that would be useful, because then you already have a target audience, but just making a general social network will probably be harder and harder as time goes on, because the big ones will steamroll over the other ones. But it's interesting, though, with the blogs and being able to come out with your own blogs now, there are more and more tools now to integrate them into such networks. Right now when I do blog, it actually posts everything everywhere-- on Twitter, on Facebook, and on Google +. And while you go to the blog, you can actually then share from all these different networks. So, the web is becoming more social. I don't think the need to create new social networks is that big unless you already have a target audience that you can actually reach. Do you have a feel for the direction these trends are taking us? What do you think the future of crunching social networks might look like? What I've learned in the last year or so is that everything is experimental. People try it out and then we see what works. Some things don't work. So, at the moment, the future is unwritten. There is going to be a lot of innovation in the next few months to a year, because there's a big battle, as you see. A lot of social networks now are starting to get tied into a lot of other products that are being put out on the Internet. Duncan Watts is one of the world experts in analyzing social networks and learning about what’s going on inside of that. He has been a professor at Columbia University and also a network scientist at Yahoo Research. And most recently, he has moved to Microsoft Research in New York City. We weren’t able to get video footage of our conversation together but I think you’ll find it interesting anyway. What are some of the factors that have contributed to the explosion of interest in social network analysis? So what’s changed recently really just in the last few years is that with web data, it’s finally possible. some instances to see precisely how information spreads from one individual to another. I wrote this book called Six Degrees almost ten years ago now, it was published in2003 one of the messages that I tried to convey in that book was that what we now call network science is both in theory and also in practice a multi-disciplinary endeavor. The origins of what we think of this network science now really began in the social sciences. There was an enormous input of physicists into this field in the late 1990s and then as the web continued to evolve and you see the computer scientists get more and more involved because they are probably the best equipped to handle very large data sets. Many people are interested in this question of who did you so replicate the success of Facebook and the best of our scientific understanding is that, you sort of can’t really. There is a lot of very sort of specific algorithms that network analysts use, everything from very simple breadth first search algorithms to community detection algorithms. Often where challenges lie at least in the recent years has been implementation, particularly scaling issues; pretty much everything these days is done using MapReduce, right. So I think that if you want to work with very large scale network data then some familiarity with the MapReduce framework and whichever implementation I guess on these days is going to be extremely valuable and there’s been some interesting theoretical work showing how algorithms like breadth first search can be converted into a MapReduce parallel framework and that’s actually generated a great deal of recent progress. Can you tell us a little bit about the idea of community detection? You want to assign everybody in the network to a community such that the communities you choose have the property that most of the links are within the communities not across the communities, one of the methods that was developed Jack Hoffman at Microsoft but when he was at Columbia, he where you are starting with a network and you are assigning – you start to some sort of prior assumption about how many communities around the network and then you sort of make a guess about who goes into which community and then you measure the relative densities of internal versus external links and then you shuffle back and forth making various random changes, so try to optimize the allocation of people to communities in order to get this sort of maximization of modularity. As you might have seen in some of the preceding units, I like to write parody songs about computer science topics. I provide links to my songs about sorting and the halting problem, and I also have one about binary numbers that turned out to be pretty popular. When I was recording the units of this class, I decided I wanted to make a new song for this class. I told my friends at Udacity, and they were very supportive. In fact, they joined in and helped me make my first collaborative academic parody song. I wanted to use a currently popular song and use it to touch on the main topics in the class. It was very fun to make, and I'm so appreciate to my friends at Udacity for making it happen. I think it came out really well, and I hope you enjoy it. Now and then I think of when they were together. Like when you found a route that went from X to Y All those paths, they went through A and B, which isolates them from that whole subtree. It was a bridge and it’s a bridge that now is severed. You can try computing when a graph is all connected. You put the neighbors on a list, the open list. So when I tried to run a depth-first search. And found that half the graph was in I guess it means it makes the search that more efficient. But you didn’t have to cut the path. Drop that singular connection, start a new component. Now each is in a separate graph, so their reachable descendents have been split in half. No it doesn’t change the time, big O or the clustering coefficient in the social network. I guess that I don’t need that though. Now there’s just no pathway that can use two nodes Now there’s just no pathway that can use two nodes Now there’s just no pathway that can use two nodes Now and then I think of all the times you chilled me over. But had me believing it was always something that I'd done. But I don’t wanna loop again. Though my running time is. You said you’d use efficient codes and you wouldn’t waste your cycles finding pathways that can you to know But you didn’t have to cut the path. Drop that singular connection, start a new component. Now each is in a separate graph, so their reachable descendents have been split in half. No it doesn’t change the time, big O or the clustering coefficient in the social network. I guess that I don’t need that though. Now there’s just no pathway that can use two nodes [No pathways] That can use two nodes. Now there’s just no pathway that can use two nodes [No pathways] That can use two nodes [No pathways] Now there’s just no pathway that can use two nodes [No pathways] That can use two nodes That can use two nodes. That can use two nodes [No pathways] Here's another version of the video This time with a focus on what was going on in the network in the background. As you can see it draws up many of the concepts that we discussed in the class so i think it serves as a pretty good summary of the topics we covered. All right. That just about wraps it up. I hope you enjoyed the course. We certainly enjoyed putting it together for you. Before we sign off, I want do to a few thank yous. I want to thank Sebastian Thrun for inviting me to do the course and for giving me this opportunity. I want to thank the smart and energetic staff at Udacity who provided support of various kinds in creating and delivering the course. And I'll single out Katie, Kathleen, Clark, Calvin, and Grant. Grant, I guess, couldn't manage to get a K on the front of his name. We'll call him Krant. All of whom were just a delight to work with. Thanks also to Mor Naaman, a professor at Rutgers, who gave me some hints about how to gather social network data, and of course the experts who participated in my interviews, Andrew Goldberg, Vukosi Marivate, Duncan Watts, Peter Winkler, and Tina Eliassi-Rad. They graciously shared their time and their knowledge in their interviews. A big thanks to Eliot Escher, who gave me feedback on my vocal performance in the song and also contributed her own beautiful voice to the music video. You can probably figure out which one was her. I want to thank David Evans at Udacity for providing very detailed feedback on my lecture notes to make this as Udacity-ish as possible, even though this was my very first time. I want to thank my family for putting up with this extra drain on my time, and finally, thanks to the class T.A. Job Evers, who made the course happen day after day. So, good luck to all of you on the final. It's been a great experience for me, and I hope to see you around. For this problem, we have a social network. Tyrone is connected to Marcie. Marcie is connected to Javier. Javier is connected to Christian. And so on. We want to know if we started with a Eulerian path where will it end? Pick the correct answer. The correct answer is Marcie. To answer this question, we first want to look at the number of degrees of each node. A degree is the number of edges coming in and out. Tyrone is a node with degree 2. Marcie a node with degree 3. Sayo is a node with degree 2. Christian is a node with degree 2. Javier, our starting node, is a node with degree 3. Hubert is a node with degree 2. As we showed in lecture, in order for there to be an Eulerian path every node in the network needs to have even degree, except for the starting and ending nodes, which are odd. In this example, Javier was our starting node and it had degree 3. All the other nodes are even, except for Marcie, which has degree 3, which is odd. Marcie will be our end node. For this question, we have our example social network. We want to know how many Eulerian paths does the graph have if we start at Javier. Fortunately, this graph is small enough that we can enumerate through all the possible Eulerian paths. Starting at Javier, we can go through Christian to Sayo, over to Marcie to Tyrone to Hubert, back to Javier, Marcie--for one. We can co Javier to Christian to Sayo to Marcie back to Javier to Hubert to Tyrone finishing at Marcie for our second one. We can go Javier to Hubert to Tyrone to Marcie to Javier to Christian to Sayo to Marcie for our third one. There's a fourth path through, and there's a fifth path. Finally, there's a sixth path. That is all the Eulerian paths through the graph starting with Javier. With this assignment, we want you to write a function create_tour that takes as input a list of nodes and outputs a graph as a list of tuples representing edges between nodes that have an Eulerian tour. For example, if nodes was 1, 2, 3, one possible solution is a graph that has an edge between 1 and 3, and edge between 1 and 2, and an edge between 2 and 3. We've also provided some simple testing code, which you can use to see if you're function is working properly. One simple solution is to take our list of nodes— let's say we have four of them— and connect the first one with the second one, the second one with the third one, the third one with the fourth one, and the last one with the first one. Then we have, trivially, and Eulerian Tour through this graph. Here is my code to do what I just described. I wanted to create a graph that was a little bit more fun but still had an Eulerian Tour. Here's another version that introduces a little bit of randomness. Let's say we start off with 8 nodes. In the first step, I pick two random nodes and connect them and choose a random node among the nodes that are connected—say this one— and connect them and repeat this process until all the nodes are connected. Then classify all the nodes as being odd or even. All the nodes in orange have odd degree, and the other two are even. We then pick a node in the set of nodes with odd degree—say this one— and connect it with another node that has odd degree. We'll connect it to that one. These both now have even degree. We repeat this process—pick this node, connect it with that node. Now we have two nodes left that have odd degree. Unfortunately, they're already connected. What we can do is pick one of the nodes with odd degree and then randomly pick another node with even degree that it's not already connected to and connect them. So, maybe this one down here. Now this new node has odd degree. The one node no longer does. Now we have two nodes with odd degree that we can connect. Just to verify that there's an Eulerian tour, we can go through here over to here connecting up with this guy and there. Here's my code that does what I just described. In the first step, we pick two random nodes and add them to the graph. While we still have unconnected nodes we continue connecting everything. While we still have odd nodes, we continue connecting the odd nodes. One question that I'd like you to think about and answer on the forums is whether this loop is guaranteed to end. If so, why? There's a link below in the instructor's comment where I'd like you to post your answer and read what some other people have to say as well. In our last question, we used a list of tuples to represent a graph where each tuple represented and edge between nodes. We have an edge between node 1 and 2, an edge between node 2 and 4, and edge between node 5 and 9, etc. I want you to think about and write a couple sentences about what are the advantages and disadvantages of representing a graph as a list of tuples. There aren't any right or wrong answers to this question. Just fill in a couple sentences explaining your response. In this problem we're going to look at the naive multiplication algorithm from lecture. Let's say we're computing naive( 63, 12). At some point in the executionâ€”so we'll pause hereâ€” we'll have x = 20 and y = 12. The question is, what is z? In lecture we showed that a * b = xy + z. We need to solve for z, substituting in values for a = 63, b = 12, x = 20, and y = 12, plus z. Finishing the rest of the math, we get that z = 516. For this problem, we've written a recursive version of the naive multiplication algorithm. We want to know how many additions does it take to compute rec_naive of (17, 6) = 102. The answer is 17. The first time we call rec_naive a is 17. We make one addition and call rec_naive again with 16 as its argument. We make another addition and call rec_naive with 15 as the argument. This process continues, doing one addition and another call 14 more times until a = 0, and so we will have made 17 additions. For this problem, we'll look at the Russian multiplication algorithm. Let's say we're computing russian(63, 12). Let's pause the execution at some point where we have x = 7, z = 84, and we want to know what y equals. To answer this question it was shown in lecture that a * b = x * y = z. Solving for y, we get that 96 = y, and that's our answer. For this problem we have a function clique, and we want to know how many units of time does it take to execute clique(4). To count the time, count each print statement as 1 unit and count each time range is evaluated as 1 unit. To answer this question, we can just list off each operation in clique that takes 1 unit of time. First, we print. Second, we evaluate range of 4, which returns (0, 1, 2, 3) Then since j is 0 the first time, we evaluate range of 0 and nothing gets printed. Next, we loop through again and j = 1, so we evaluate range of 1, and we print once. Now, j = 2, and we evaluate range of 2. In range of two, we print twice. Going through the loop again, now j = 3. So, we evaluate the range of 3. In the inner loop we print 3 times. We can count this up 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 operations. So, our answer is 12 units. For this problem we have the same clique function, but instead of counting how many units of time it takes to execute clique(4), we're going to write a more general function that calculates how many units of time it takes to execute clique(n) as a function of n. To solve this, let's look at the steps that we counted for n = 4. These first two always happen regardless of n. We have 4 range calls, which corresponds to this range call. This one is going to happen n times. If n = 10, we would get 10 range calls. Then we have 0, 1, here's 2, and here's 3 print statements that correspond to this. In general, we're going to have 1 print statement plus 2 print statements plus 3 print statements plus etc, etc, all the way up to n - 1 print statements, which equals n( n - 1)/2. Adding up these three parts we get 2 + n + n(n - 1)/2. We can quickly check. In the case of n = 4, we get 2 + 4 + (4 * 3)/2, which equals 12. For this problem, we want you to write a function that takes in a graph represented as a list of tuples and returns a list of nodes that you would follow on in an Eulerian Tour. For example, if the input graph was an edge between 1 and 2, an edge between 2 and 3, and an edge between 3 and 1, a possibly Eulerian Tour would be to go from node 1 to 2, from 2 to 3, and from 3 back to 1. Write your code here. We haven't provided any testing, and we recommend that you write your own test to verify if your answer is correct. The first question is about star networks. A star network is a network that has a single node in the center that is connected to all the rest of the nodes in the graph directly. So, here's a star network with five nodes. It has one, two, three, four edges. Now, what you should do, is write a python program that returns the number of edges in the star network with n nodes. This problem we're going to start of with a template for a recursively generated graph. If the template says if I want one node, it'll just return a single node. For other values, make a G1 and a G2 that are half the size. And then we make log n connections between G1 and G2 by picking a random point and connecting with a random node, another random node. And connecting with a random node etc. And then return. So this has the recurrence relationship that only have one node with zero edges. And if we have n nodes, we get 2 times the number of nodes we have for a graph of half the size plus big theta of log n. And so what we want you to do is to solve this recurrence. But we are going to do this one little differently, than some of our previous homework assignments. Instead of a multiple choice question, we want you to explain how to actually solve this. So below there's a form link, and at that link you'll see this question restated. We want you to do is wirte up your answer, post your response to the form, get the link for your answer. And then insert it here. I'll give an example to make this more clear. So here's the question on the forum. And you should write up your answer. Some possible answers might be that T of n is theta log n, theta n log n, theta n, theta n squared. There's any other function. And then you should explain why you've answered like this. Or just submit your answer. Hit the link button. Copy out the link. Now, if you've copied the link, paste it into the box here. For this problem, we're going to look at the relationships between different kinds of graphs. We want to know if x is the subset of y or y is the subset of x or both or neither. We'd select this first option. If every star graph is also a tree graph, we pick this option. If every tree graph is also a star graph, select this option, if both of these are true; if every star graph is a tree graph and every tree graph is a star graph. Or we'd pick this option, if neither is true. For example, the first one is true. Every star graph is a tree graph. In order to get from any node, exist this simple path through the center. To our target node. If we start from the center, it's trivially easy to get to any node. The reverse is not true. This is an example of a tree graph that is not a star graph. Here are the rest of the graph types for you to figure out. For this problem, we also take a function 4 n squared, plus log n to the seventh power plus 9 n times log n squared plus n to the two thirds power. And state whether it's a number of different sets. Specified by theta and big O notation. For this question, what I want you to draw, a planar graph with eight nodes. And then connect those eight nodes with fifteen different edges. Then the question is, how many regions does the graph have? This problem, we going to introduce a new graph. The Combination lock. A Combination lock consist of a chain through these edges and the edges of a star for the edges in red. So, we want to know a function that describes the number of edges given the number of nodes is theta of n, theta of n log n, theta of n squared. In this problem, we're going to use the same combination lock as before. And once you write a program, create combo lock, to generate a combination lock graph given a list of nodes. Imagine generating an Erdos-Renyi graph with n equals 256 and p equals 0.25. On average, how many edges will it have. In the star network the description is that there is a center node to which all other nodes are connected using the edge. There are n-1 other nodes if you choose one center, and each is connected to the center node with one edge so there are two n-1 edges and that seems good. The first part of the subset question asks about planar graphs and trees. The first thing to note is that not all planar graphs are trees. For example, here's a ring, which is a planar graph and it's not a tree. And so planar graphs are not a subset of trees. On the other hand, all trees are planar graphs. For most trees, it's fairly intuitive to see how it can be drawn in the plane. For example, this one is already planar. But for other trees, it might not be so obvious that we can rearrange everything to lay it outside in the plane, and so it'd be nice to have a proof that all trees are planar instead of relying just on intuition. One way to prove this is to show that any tree can be drawn inside of a triangle. And then larger trees can be constructed by combining these triangles. More formally, what we want to show is that any tree can be drawn in a plane inside a triangle with any given node at the apex of the triangle. And so for the base case of one node, we can draw the node at the apex of the triangle and this is trivially in a plane. And so, our inductive step is to assume that the claim is true for all graphs with less than n nodes. And we want to show that the claim is true for all graphs with n nodes. So we'll start with a tree with n nodes. We use this for example. We're going to pick any node kind of at random and we'll call it r. For example, one of this node be r. And then we remove r. Because our original graph was a tree, this separates the graph into separate disjoint trees. And now since each of these trees have less than n nodes, we can draw each one inside of a triangle. First, we can take this tree of four nodes and draw it inside of the triangle. Notice that the node that was connected to r is at the apex of the triangle. And then we could do the same for the other two trees. And then in our last step, we'll make a bigger triangle and we put r at the apex and then connect it to the apexes of the other triangles. This verifies our claim and shows that all tree graphs are planar. They are definitely neither due to the definition of trees. A tree can have no cycles and a ring must have a cycle. The answer is neither. &gt;&gt;Yeah. &gt;&gt;Cool. I never got a really specific definition of a chain besides just the example which was enough but-- Yeah, same idea kind of a chain doesn't have a cycle whereas a ring does. And the other thing we got definitely was that the edges in the chain is the number of nodes minus 1 and the edges in the ring is equal to the nodes. Therefore, there can be no crossover between them. They're mutually exclusive. All chains are trees but not all trees are chains and so its the first one. It satisfies the definition of a tree graph that it's connected but there are no cycles, and then the other description of a tree graph that every pair of nodes in the graphs has a unique path between them. I mean a chain does that also. This is definitely one that answer intuitively by looking at the examples. It gives me 100% confidence in making that claim. Yeah and just kind of a quick example of--that's a tree and that is not a chain so-- Correct. And the way I convinced myself of that is drawing an example of a hypercube that was not a ring and a ring that was not a hypercube. Unless you have a ring that is anything but four nodes it's not a hypercube. Okay. &gt;&gt;And any hypercube that's anything but four nodes is definitely not a ring. Great. So yeah I'll draw one for you and it's just a normal cube. Right. &gt;&gt;And our second to the last problem is just grids and chains. All chains are grids but not all grids are chains so it's the--chains are the opposite of grids. Yeah. &gt;&gt;Any chain within nodes is a 1xn grid. Once you get a grid that has more than one in either of the two directions, it's no longer a chain. And no matter how long you make a chain-- &gt;&gt;Yeah. So this is-- You can add a node to the chain--a node of n to the chain, that makes it not a grid. Kind of the first thing you said we have like a 1xn grid and it's also a chain but yeah. Yeah. This is for example not a chain. &gt;&gt;Correct. &gt;&gt;Cool. N² kind of beats everything here. So it's Θ(n²). It's order of n², as well as order of n³. Okay. So basically (log n)⁷ beats 9n(log n)² and n²/³ beats (log n)⁷ and 9n(log n)² beats n²/³. I think it was the most dominant one. So let's see what the options were. Θ(n²), O(n²), and O(n³). I'm going to add a little like for n³, this is valid because the big O notation is an upper bound. &gt;&gt;Upper bound. Okay. &gt;&gt;Yup. We had a planar graph. &gt;&gt;About the planar graphs. &gt;&gt;Yup. And then--so you said draw yourself a planar graph but we know the relation n-m+r=2. I did actually draw a graph as well, in fact two. So it comes out to be 9. Well we can draw a graph. We can basically make a grid of sorts. &gt;&gt; And I just moved the grid width. &gt;&gt; Yeah. I think it's fine to use the formula. &gt;&gt;Yeah. You know, it gives you confidence that it's going to work for all possible arrangements of graphs, so-- Part of the amusement is when it works for every graph. Basically, I calculated the chain first and then I calculated all the other ones where I couldn't count between the 0 in the first element because I'd already done that and so it's just one list and write down a whole lot mixed--that one was straight. So it was less than 2n for the combination ??? so it has to be Θ(n). &gt;&gt;Okay. &gt;&gt;And then n-1 links to get the chain. &gt;&gt;Yup. And then n-2 links to get all the rest. So that's a total of 2n-3 which is Θ(n). Right. Right. Yeah. I guess that's kind of another way to think about it. I mean in this case I guess it's kind of simple but one thing I'd like to-- a more complicated guess is like what happens when you add one more edge. Like you have your combination lock and you add another edge you can add two more nodes to kind of each edge and add two nodes and so it's going to continue to grow literally. Then you think about what you have if you have one node. Yeah. I guess kind of the jump from the first node to the second node is— Sorry from the first node to the second node is a little different because you only had one edge but you know sort of when you think of big theta and big O notation you're dealing with bigger numbers like how it grows as it gets very large. Because we're doing big theta you don't really have to think about the base case of zero nodes or one node or two nodes. &gt;&gt;Right. &gt;&gt;But if you think about in your case adding any node. I'm going to add two more edges. So that's ???. Okay. Yes. Just. &gt;&gt;Yup. Just part of the definition is like you know for all n greater than n₀, okay, like you have fn is less than--like C₁gn or C₂gn and it's this part right here it's like-- as it gets bigger you can--you don't have to worry about those cases. You don't have to worry about all the numbers that are less than some and some maybe not but-- I mean for this one it's simple. It's kind of--it's pretty easy to think about. Here's one example of a possible solution for our program to create a combination lock. And this code is based on a code from Amit Gupta. So we take on a list of nodes. We'll use 6 nodes in this example. On the first line, we create an empty graph. On the next line, we create a length between zero node and the first node and then we have a loop that runs through the rest of the nodes. On this part of the loop, we make a chain connecting the first node with the second node, the second node with the third node, the third node with the fourth node, and so on. And on this line, we create the star part connecting zero node with the second node, zero node with the third node, the fourth node, etc. And this is the combination lock we wanted. For this problem, we are to calculate the expected number of edges in an Erdos-Renyi graph with 256 nodes and a success probability of 0.25. But before we answer that question, I want to look at a simpler question where we have 4 nodes and a success probability of 1. In the case where the success probability is 1, every possible edge will be created. And we'll have a complete graph. In the case of 4, we'll have 6 edges. And we've already discussed that the number of edges in a clique with n nodes is n(n-1)/2. To get the expected number of edges when pâ‰ 1, we have to multiply the total number of possible edges by p. And so, in the original problem, we have n is 256 times n-1 is 255 divided by 2 times p that is 0.25, which is 8160. For this problem we have 6 graphs, and we want you to order them with a clustering coefficient of the red node of each graph. Order them from the lowest clustering coefficient to the highest clustering coefficient. For example, if you think f has the lowest clustering coefficient put that first and then maybe b, e, a, c, d. And the second question asks what is the maximum number of edges B can have? And the second question asks what is the maximum number of edges B can have? Using the same bipartite graph, B, with 5 edges on the left and 3 edges on the right we ask what is the maximum possible path length in B? And what is the maximum possible clustering coefficient for a node in B? Only consider nodes with degree greater than 2. For this question, we want you to implement mark_component using the open list as discussed in lecture. This means that mark_component should not use recursion. In lecture, we introduced a centrality algorithm that measures centrality by taking the average distance from a node to all the other nodes that it can reach. We want you to modify this algorithm to return the maximum distance from a node to all the other nodes it can reach. This algorithm captures a different notion of centrality. A node is central if no other node is very far away from it. This question is optional, but now that we know some more about searching a graph, testing for connectedness, and finding bridge edges, you might be able to improve your Eulerian Tour finding routine from the first unit. If you go back to the challenge problem in unit 1 and add the line usehardertests = True to your code the grader will use more complicated graphs. For this problem, we have to order these six graphs by the clustering coefficient of the red node, and the graphs will be ordered from the lowest clustering coefficient to highest. Looking at the six graphs, we can see that c is a star network, and star networks have a clustering coefficient of zero. That is the lowest clustering coefficient. You can see that d is a click to the clustering coefficient of one as that will be the highest clustering coefficient. Both b and f have two edges connecting the neighbors of the red node. Let's take a look at those two. Intuitively, since b has seven neighbors and f has five, b has a larger click and f has a smaller click, and since they both only have two neighboring edges and b has load larger click, and actually calculating the clustering coefficient, we can see that yes, b is smaller than f. For a, the degree of the red node is five and there are five links between the neighbors, so we have a clustering coefficient of half and for e degree of the red node is seven, and they are seven links between these neighbors, so the clustering coefficient is 1/3. So, the final answer is c, b, f, e, a, d. Now we have four questions about bipartite graphs. The first question asked, what is the minimum number of edges needed to make B have a single reachable component consisting of all the nodes where B is a five by three bipartite graph. We know that trees are connected and that they have n-1 edges. In fact, it takes at least n-1 edges to connect any graph within nodes. The question is, can we make B a tree? And yes, they are lots of ways that we can make B into a tree. This is just one example. By this reasoning, we can see that it would take at least seven edges needed to make B have a single reachable component consisting of all the nodes. The second question asked, what is the maximum number of edges in B. We wrote a constructed graph with a maximum of edges, and take each node and connect it to everything that we can. This first node, I can only connect to the three other nodes on the right side. I can't connect to anything else on the left side or else it wouldn't be a bipartite graph. The second node also has three edges connecting to other nodes in the right side, and we do the same for the other remaining nodes on the left side, and we'll look at the nodes on the right side and this first node can't connect to anything else. The second node can't connect to anything else and the third node can't connect to anything else. For this graph, we can have 15 edges, and in general a bipartite graph can have a maximum l * r edges--where l is the number of edges in the left group and r is the number of edges in the right group. The third question asked, what is the maximum possible path length in B? It's pretty straightforward to see the chains have a maximum possible path length of n-1 and this n-1 is an upper bound on the path link of any graph with n nodes and so we are checking to see if we can make B into a chain. If we can, then n-1 will be the maximum possible path link and this is about as close as we can get into making B into a chain. In fact, it doesn't matter how many nodes are on the left side. We can still only get a chain of six. For another node on the right side, we can make our chain to it as longer, and so the maximum possible path link in a bipartite graph depends on the number of nodes in the smallest group, and so in general the maximum possible path link in a bipartite graph is the minimum of 2 times the number of nodes in the left group or 2 times the number of nodes in the right group or the number of nodes in total minus 1. For B, we get that the possible maximum path link is six edges. The last question asked, what is the maximum clustering coefficient for a node in B? Here's the formula for a clustering coefficient. If you look at any node in the left side say the middle one. We know that all of its neighbors will be in the right group, and since B is a bipartite graph, they're can't be any edges in between numbers of the right group. This implies that Nv will always be zero, which means the clustering coefficient will always be zero. Code for an example solution is given in the Instructor Comments box below. Code for an example solution is given in the Instructor Comments box below. Code for an example solution is given in the Instructor Comments box below. Another way to build a heap out of a set of values is insert the items one at a time into the heap. Here is one way to implement this. We want to know what is the running time of this code.