Congratulations! You've reached the end of the course. Well, the course videos anyway. Now if you remember nothing else, here's what I would say is the biggest idea of this class. When it comes to algorithm and data structure design, there is life after RAM. In this course, you lived three such lives. In the form of PRAMs, distributed memory machines, and two-level memories. And of course you'll need, in the afterlife, hybrid models that mix and match any of these. The main point is that there are ways to formalize and reason about communication, which you now know. So good work, you! As for what comes next, remember that this class was mostly about algorithmic and data structure foundations, meaning theory. But there's a whole other side to HBC which is about how do you make these ideas actually work on real machines? Many projects give you a little bit of a flavor of this but there's a lot more to it. It's an especially complex topic and one which I've wasted, not wasted, spent, think positive, think positive. The bulk of my career thinking about. So who knows? Maybe you'll join me in another course where we can spend some time together thinking about that. That sound like a pick up line? Oops. Awkward. I'd really like to give big hugs to all the people who made this course possible. That of course includes the whole production team, especially Amanda, Catherine, and Morgan, as well as Charlie and Robert for their work on the mini projects. All these good people deserve credit for the really good parts of the class, and none of the blame for the less good parts, which of course falls on moi. And finally, I want to thank serial computers everywhere for their patience and understanding. [SOUND] Especially when I was staying up way past everyone's bedtime, keeping the lights on. So with that, I'm out of here, toodles! Lights out! Cut. [MUSIC] Viva la HPC! Viva la HPC! Viva la HPC! Viva la HPC! Viva la HPC! Kids, hello and welcome to Introduction to High Performance Computing or Intro the HPC for short. My name is Rich Vuduc, and I'm a professor here in the School of Computational Science and Engineering at Georgia Tech. I'll be your guide in this course as you explore what I think are some of the big ideas in how to extract parallelism and data locality from algorithms and data structures. I thought we'd kick things off here at the Klaus Advanced Computing Building. It's one of the main places on the GT Campus where HPC is proudly spoken. Oh, Viva la HPC. Now there's another reason to start here to. Although you might be attending the course from some remote part of the globe, I wanted to remind you while your in my course at least, you're still here with us in spirit. And that makes you part of the Georgia Tech family. So with that, lets head to my office in Klaus, and I'll tell you a little bit more about the course. Welcome to my office. I thought before we'd start I should give you some tips on how to do well in the class. Step one is take a look at the course readiness survey. It's the minimum CS and Math-101 that I think you need to even get started. Next, there are these videos. The videos are about building your intuition. And like other courses in the series, there are little quizzes sprinkled throughout. I'd advise you not to skip the quizzes because I like to stick new concepts there. And beyond the videos, there are readings. Where the videos are about building your intuition, the readings are about spelling out all the gory, formal details. And you're going to need those details in order to do the mini projects and the exams. Finally, there are the mini projects. The mini projects are about writing real code because you don't really understand an algorithm or a data structure until you've tried to implement it yourself. Especially, if you're trying to understand something about engineering tradeoffs. Okay, so that's it for kind of logistics and philosophy of the class. With that fluffy stuff out of the way, let's talk HPC. The first thing I want to say about high performance computing is that it's the dumbest name ever. I mean, like seriously? High performance? As opposed to what exactly? Low performance? Unfavourable performance? Sober performance? 2.5 out of 4 stars performance? So I went and I talked to a bunch of my colleagues from all over the world, and I asked each one of them, what do you think about HPC? One of them, John Shalf, from Lawrence Berkeley National Labs, had this to say. Well first off, I like to call it super computing. HPC sounds kind of boring. Super computing, now that is compelling. Well said, John. I couldn't agree more. So with that, let's go super compute. [SOUND] So the super in supercomputing is about solving big, hairy computational problems as efficiently as possible. And here at Georgia Tech, there are many examples of scientists everyday using supercomputers to do all kinds of important stuff. Stuff like simulating the dynamics of the earth, or studying biomolecular systems. Or analyzing social networks, or understanding the cosmos, just to name a few. Now, these are incredibly complex phenomena and many scientists believe we won't gain any insight into them without some combination of your mathematical models, leaner algorithms and data structures, more and accurate data, as well as enough processing color. Now ours is a CS course on super computing. So the main question we'll be interested in is, given a computational problem and a machine, how do you compute at the absolute limits of scale and speed. You'll encounter many such limits throughout the course. But one of the more interesting ones, come from physics. These are things like limits based on the speed of light, all the amount of energy, power or heat dissipated by the system. In fact that's the reason we're here in this crazy mad scientist lab run by my crazy mad scientist friend and colleague Ken Brown. Ken Brown and his crew are thinking about how to build Quantum Computing Systems. That represents one of the ultimate limits of physical computation. Now we won't actually talk about quantum computing stuff in this class, but I wanted to start there to sort of stimulate your imagination, not only about the limits of conventional computing systems but also about what might lie beyond. Before we dive in, let me explain how I've organized the topics of this course. There are three major parts, or units. These units reflect three different ideas about what a computer looks like. Now, to explain what that means, let's go back to CS 101. There you probably learned about a machine model called the sequential, or serial RAM model. [MUSIC] There's a single serial processor connected to memory. This processor issues instructions that operate on data. Still computing. [MUSIC] The operands of these instructions always live in memory. When you analyze the cost of a serial ram algorithm, you assume that all instructions have a cost that is bounded by some constant. Now from that starting point, you do a big O style analysis in terms of the input size. So, that's the serial ram model. I hope you already know it by heart, and maybe even love it a little? Now, what are the alternatives to the serial RAM model? Well, a first alternative is the Parallel RAM or PRAM model. Instead of one processor, there may be many. They all see the same memory, and you still assume a bounded constant cost per operation. But you get to work with more than one processor. And since the processors all see the same memory, they can coordinate and communicate by modifying shared variables. Now in this model, your algorithmic analysis will involve big O as before, but you'll try to reduce the total cost by up to a factor of P, the number of processors. A PRAM style model is arguably the simplest one that you might use on, say, a shared user multi-core machine with uniform memory access. Using a what's it? Don't worry, you'll get there. So that was one alternative. Here's another, a distributive memory network model. It's basically just an interconnected network of RAMS. That is, you start with multiple RAM computers. Each computer has a processor with its own private memory. No processor can read or write the memory of any other. Instead, they join forces connecting to one another over a network. They coordinate by sending and receiving messages over this network. Now, in this model, your analysis will count the number of messages, and the total volume of communication. Okay, now here's a third alternative that we'll consider in this class. It's a two-level input output, or ("I/O") Model. In this model, there are one or more processors connected to a main memory. It's sort of like RAM or PRAM, but you assume that there's at least one level of fast memory that sits in between the processors and the slower main memory. This model forces you to think about the following question. How much data needs to move from memory to the processor using this small, but fast, intermediate memory as scratch space? Now, if you know about things like caches or virtual memory, then you might already know this basic model. But even if that's the case, do you also know how to design algorithms that exploit it? So, to recap, we will discover the big ideas of this course using three machine models. PRAM, the distributive memory, or network model, and the two-level, or I/O, model. So, with that as background, I think we're finally ready to start. Woot! To design algorithms, we need an abstract model of a parallel computation and a way to write down the algorithm. This lesson is about one such model, which is sometimes called the Dynamic Multithreading Model. It has two parts. The first part is that a computation can be represented by a directed acyclic graph, or DAG, sort of like this one. Each node is some piece of computational work or task. Each edge is a dependency which says that a given task can't start until all of its predecessors have completed. Now, from the point of view of exploiting parallelism, a good DAG is one that has relatively few dependencies compared to the number of tasks. After you learn how to analyze abstract DAG's more precisely, we'll arrive at the second part of the model. It's a pseudocode notation, or a programming model, for writing down the algorithm. This notation will be defined so that when you execute one of these algorithms, it generates a computational DAG, at least conceptually. Now, one caveat before we start. You may have done multithreaded programming already using libraries like P threads or Java threads, where your program explicitly creates virtual threads and then assigns units of work to them. If that's the case, you need to forget what you learned, at least for this lesson. The pseudo code notation that we'll talk about separates how to produce work from how to schedule and execute it. That means that you focus on creating an algorithm that has a nice DAG. Separately from that, there will be a physically multi-core machine and runtime system that given your DAG figures out how to map it to cores and execute it. Now there will be some limits to the kind of DAG you can produce in the model we'll describe but I hope you'll see that it's still a really natural, elegant and powerful way to express a parallel algorithm for a broad class of interesting problems. In the multithreaded DAG model, you represent a parallel computation by a directed acyclic graph or a DAG, like this one. Each vertex is an operation, like a plus or an addition or a function call or a branch. The directed edges show you how operations depend on one another, where the sinks depend on the sources. So this would be a sink and this would be a source. And this would say that c depends on the output or result of b. Now to keep things simple, I'll always ask you to assume that there's exactly one starting vertex and one exit vertex. So, in this example, here's the start and here's the exit. And of course, if you have a DAG and there are no such vertices, the start and exit ones, it's pretty easy to add them. Now, suppose we have a PRAM machine and you want to use it to run the computation. You start by trying to find any operations that are ready to go. That is, all of their input dependencies are satisfied. So, in this example, take a look at the starting vertex. That one's ready to go. Since it's ready to go, meaning it has no dependencies. We can assign it to any processor that's free, and go ahead and start executing. So here, I've assigned starting vertex s to processor three. And processor three will start executing it. Now, as soon as a processor finishes its operation, it enables any of its successors to begin. So, in this example, a and b only depend on s. So as soon as s is finished, a and b can start. So now, a and b are ready to go. So we can look for any free processors and assign a and b to them. So, for example, it looks like processor one and processor three are free. So let's assign a and b to them. Now, when processor one and processor three finish their units of work, then that will enable other successors. In this case, vertices c and d, and this process will repeat until we finish. Now, at every step of the computation, this problem of how to take free units of work and assign them to processors is called a scheduling problem. Scheduling is a huge topic and you will hear more about it soon enough. Given a DAG and a PRAM, here's a question. How long will it take to run the DAG? To answer this question, you need a cost model. For now, I'm going to use a cost model that makes three assumptions. The first assumption is that all processors run at the same speed. The second assumption is that each operation takes one unit of time. And the last assumption is that these edges don't have any cost associated with them. So let's start with these assumptions and let's apply them to some sample DAG's. So let's start with this example program which, given an array A, computes the sum of all of its elements. That sum will be returned in this value s. This operation is actually an example of a pattern called a reduction. Now let's suppose we only care about the cost of addition and the cost of array access. That is, let's assume we can ignore the cost of all these other operations. Like indexing operations, branches, and so on. Here's how the DAG for this computation might unfold. Let's start with iteration one. And we need to load A sub 1 and then perform the addition. So in the DAG there would be a node for A sub 1and a node for the addition. And there would also be a dependence edge since the addition can't start until the load is complete. Now what about the second iteration? We'll repeat the process, so they'll be a node for a sub 2 and a corresponding addition. Now in this case there will be two dependence edges, one from A sub 2 to the plus. And the other one from the other plus. Now technically there's another dependence which goes from this plus to this load. So why is that? The reason is that the code is being executed sequentially so there's actually what's called a control dependence between executions of the body of the four loop. That is this statement that i = 2 can not execute until the statement that i = 1 has completed. For now we are going to ignore these controlled dependencies. So if we ignore the control dependences and we continue this process, then you'll get a DAG that looks something like this. Now, suppose I give you a PRAM with P processors. How long will it take the PRAM to execute this DAG? Let's call that time Tp(n). Now, notice that the loads have no input dependences. They're all ready to go so you can take them as a group. Assign them to the P processors and execute them in batch. So in our cost model that we take c-ling of n divided by p units of time. That is their n loads which we divide among the P processors. Each load takes one unit of time. And since we're dealing with integers, we need this ceiling. So, what about the additions? These additions have dependencies between them. So, it looks like I can't ever execute a given addition until I've executed the previous addition. And it doesn't matter how many processors you have, you've got to execute these additions one at a time. Therefore, you'll need at least n units of time to complete all of the additions. Now both of these conditions have to be true so we can write this in a more compact form. Now p is always at least one since presumably you're not going to run a machine with no processors. So that means the ceiling of n over P is always going to be less than just plain old n. So, we can simplify the time. So, what we just showed is exactly what we expected, which is that a sequential reduction will take n units of time on a PRAM. So what if you organized the reduction DAG as a tree? Now we need a specific technical assumption, which is called associativity. That is, assume that a + (b + c) = (a + b) + c. In other words, assume that you can put parenthesis wherever you want and you'll still get the same answer. Let's start by writing down the end loads, just as before. Then, let's add pairs of inputs. Then, let's add pairs of these intermediate results. And let's keep going until the final sum remains. Here's my question: if I give you a PRAM with P = n processors, what's the minimum time it could possibly take to execute this DAG? Let's do this as multiple choice. Here are your options. Constant time, log time, linear time, and log n time or n squared time. Choose one. The answer is log n time. Let's start by thinking about the loads first. They have no input dependences and there are n of them, so it should take ceiling of n divided by p time in order to execute them. Now in this case I told you to assume that P equals N. So N over P is just a constant, in fact it's exactly one. Okay, that was easy, what about the additions? Let's look at the first level of additions. Now notice there are N over two of these vertices and we have N processors. So we have plenty of processors to execute all N over two additions in one shot. So that means this first level of additions will also take one unit of time or just constant time. And that's basically the same at every level. So we're basically executing the DAG level by level. Each level takes constant time, so the only thing we need to know is how many levels are there. Now this thing is a complete binary tree, so there should be log n levels given n leaves. And that completes our analysis. Given these two dags, which both compute reductions, which one is better? Now, the linear dag has a sequential chain of dependencies, so it can never really use more than one processor at a time, other than the loads. The tree, by contrast can use parallelism at every level. So the sequential dag will take On time, where the tree-based dag will take log n time. Now, intuitively, if I give you two dags and they both compute the same thing, than you generally prefer the one that has more parallelism. But is that the only thing you care about? Does anything else matter? I'm going to walk you through a formalism called work span analysis. It will help you answer these and a number of other interesting questions. Given a dag like this one, I will always ask you two questions about it. The first questions is, how many vertices does it have in total? And I'm going to give this a name. And I'm going to call it work. And I'll usually write it as W of n, since the total number of operations should somehow depend on the size of the input n. Now computing the work is easy, you just count the number of vertices. The second thing I'll ask you about a dag always, is how long is the longest path through the dag? This also has a special name, I'm going to call it the span. And I'm going to use the symbol D of n to represent the span. Now in this dag, here is the longest path, which I've circled, and this path also has a special name. It's called the critical path. Now as a historical aside, the length of this critical path or what I just called the span, historically was called depth. And that's why I'm using the symbol D of n. Back in my day, we didn't have no span. We called it depth, and you liked it. So, in terms of work and span, can we now say something about the time to execute the dag on a PRAM with p processors? Here's some simple things we can say. First, if all the operations have unit cost, then the time to execute this dag using only one processor should be exactly the work W of n. Now let's consider the other scenario. Suppose I give you an infinite number of processors. What's the time? Well if you have an infinite number of processors it really doesn't matter, because you still have to execute the critical path and that has length T of n. So the time given an infinite number of processors is D of n. So, those are two handy facts, but I think we want to try to say more. Let's check that you understand the basic definition of work and span before we move on. So here's a graph what's the work and what's the span? Go ahead and put your answers in these two boxes. And were looking for the actual values so type and integer. Now the work is just the total number of vertices. In this case, there's a 4x4 grid so that means the work is 16. What about the span? Now the span is going to be the longest path from the start to the exit. Now in this DAG, all paths happen to have the same length, which is 7. Here's an example of one such path. Let's apply the work and span concepts now in a more abstract setting. So, consider the two reduction DAGs. My question is, what is the span of each DAG? We'll do this again as multiple choice. For each DAG, choose the best option. For the sequential DAG, the critical path goes through all of the additions. And since there are n such additions, the answer is all O(n). Now what about the tree? Now in the tree, all paths include at least one vertex from each level. Since there are log n levels, the answer is log n. Let's develop your intuition a little bit more. We'll use this hypothetical dag. Now suppose the dag has work W(n) and span D(n). The ratio of work to span or w divided by d has a special interpretation. It basically measures the amount of work per critical path vertex. So, think about that for a second. At every critical path vertex, there's an average amount of work. So this ratio basically tells you the average available parallelism in the dag. So imagine you're a critical path vertex like this one. When that critical path vertex executes, there's going to be a bunch of work sitting around and on average, the amount of such work is W divided by D. So what does that imply? Suppose I give you a PRAM with P processors. How many processors should you use? It looks like it would be nice to have about w over d processors. And if that's the case you can basically keep the processors busy on average. Work and span give us some more information. You know for example that the span is a lower bound on the execution time. We are going to give this a special name, we are going to call it the Span Law, punishable by death. Now that's a lowerbound in terms of the span but it's not the only lowerbound. Another scenario I could imagine is taking all of the work, and if there is no critical path, then I would just divide that work evenly among the p processors. And that would give me another lower bound. Basically the ceiling of W divided P. And we'll give than an ominous name as well. The work law. Now I have a pea-sized brain. I don't like remembering all these laws, so I'm going to write this more compactly. Both laws hold, so I can combine them. Basically the two laws have to be true simultaneously so I can combine them. So remember, W/D tells you the average available parallelism, and W and D together given P gives you a lower bound which is this combined work span law. Commit these to memory. Now the work in span laws tell you something about the minimum possible time. But I want you to ask yourself a more interesting question. Is there an upper bound on the time to execute the DAG? The answer is, yes, according to a theorem by Richard Brent. Now, I love Brent's result. I think it's really cool and really elegant. Personally, I think you can get a lot of insight into parallel algorithms by re-deriving it. And you can do so without a lot of work. Now having said that, there are a bunch of steps involved so I'm going to break this derivation into two parts, and we'll have a small quiz in-between. So we have a DAG with a certain amount of work and a particular span. Just as a reminder, our goal is to estimate an upper bound on its execution time. Now suppose I give you a PRAM with P processors and let's say you run the DAG. Now let's imagine we're doing some forensic analysis and we go back and we look at the execution of the DAG on the PRAM. Let's break up that execution into phases, where each phase satisfies three conditions. The first condition is that each phase has exactly one critical path vertex. Note that this condition immediately implies that there have to be D(n) phases. So let's say I've numbered the critical path vertices from 1 to D(n). Then this condition implies that there will be D(n) phases. So here's a cartoon of what the phases might look like. Now since the critical path vertices are numbered, and there's one critical path vertex per phase, then I can number the phases by the critical path for text number. So for example here's critical path vertex 2 so I'll call this phase, phase 2. Okay, that's the first condition, what about the second? The second condition is that all non critical path vertices within a phase are independent. Let me show you what I mean. Let's take any phase with its single critical path vertex, like this one. Now consider any non critical path vertices that have been assigned to this phase. These vertices can only have edges that enter the phase or exit the phase, but they can never depend on one another. So for example, this is okay, but a dependence like this would not be allowed. Now this condition is always possible to satisfy, and a good exercise is to see if you can figure out why. As a hint, just think about paths, and the fact that the critical path vertex lies on the longest path. Now, there's a third condition which is that every vertex has to be in some phase, and further more it should only be in one phase. So suppose this is our DAG, and let's say we've divided it into phases. Now every phase K will have some number of vertices associate with it. Let's say that number is WK. And this value WK will include the critical path vertex. So by this third condition, which says that every vertex has to appear in exactly one phase, what does that imply? It basically tells us if I add up the WKs across all the phases, then I should get the total number of vertices. That is, the sum of WK for K goes from 1 to D should be W. Okay, here's a question for you. How long will it take to execute phase K? Let's call that time tK. Now if I have WK units of independent work, which I get from condition two, then that tells me that the total time to execute phase K should be ceiling of WK divided by P. And that in turn implies that if I sum over all phases then the total time is just the sum of these tK's. So plugging in tK we get this summation. Okay, I know what you're thinking, this seems really unhelpful. Are we any closer to achieving our goal? Mathematically these ceilings are really ugly, and can we make them go away? Suppose I give you two positive integers A and B. Heres my question. Which of these following identities are true? Here are your four choices, and I want you to put a mark in all that apply The answer is that they're all true In the instructor's notes, we'll put a link to a nice Wikipedia page that derives several of these. Back to our regularly scheduled program. Let's see where you are. You want an upper bound on time. You took the DAG and you broke it up into a bunch of phases. And each phase k does w k units of work. And the time to execute each phase is the ceiling of w k over p, and it's these pesky ceilings that we want to get rid of. Let me help you. I don't want you to be angry like this little guy. Grr. Here's a fact. I can take a ceiling of a quotient and turn it into a floor. Plug that in and see what you get. Now I know what you're thinking. Curse you that gets rid of the ceiling but the floor seems just as bad. Remember our original question was to get an upper bound. So can we get an upper bound on the summand? Now the floor of something is always less than or equal to that something. Aha. That gives an upper bound and eliminates the floor. Oh no, I'm falling into a pit! Okay, this looks a lot cleaner. Now you just need a little algebra and the fact that summing all these Wks will give you W. And putting that all together, you get your first interesting result. Congratulations. This is Brent's theorem. And what you've done is very interpretative to interpret to Brent's Theorem says that the time to execute the DAG is no more than the time to execute the critical path, plus the time to execute off of the critical path using the P processors. This result also sets a goal for any scheduler. So let's say someone is trying to sell you his or her scheduling algorithm, you can look at him or her and you can say, now wait just a unit of time there. Can you scheduler execute any DAG within Brent's time bound? You will look so smart and sophisticated, if you do that. Let me close by emphasizing one final point. What you just derived is an upper bound. That is, given a DAG, Brent predicts the upper limit on time. But you also had these lower bounds. Now an interesting factoid is that this upper limit and this lower limit are both within a factor of two of one another. Can you see why? In any case, what this means is that you may be able to execute the DAG in a time that is less than what Brent predicts. Though of course you can never go any faster than the lower bound, at least in theory. Let's see how this works. Let's say you want to run this DAG on this 2-processor PRAM. Here's my question. What is the upper bound predicted by Brent's theorem? Enter your answer, which should be a positive integer in this box. There are six units of work and a critical path of length four. So I can plug those into Brent's Theorem and I'll get five units of work. Brent's Theorem is an upperbound on the execution time. That means if I give you a DAG and a PRAM machine, the PRAM machine might run the DAG in less time than PRAM would predict. So, looking at this example, it has a work of six units and a critical path length of four. So on a 2-processor system Brent would predict that the time is less than or equal to five time units. Now looking at the stag you can probably easily see a why to execute this. Using only four time units. For example when we divided it to phases like this. So there are four phases and every phase has at most two units of work. So I can keep my 2-processor PRAM busy and the total time will be four instead of five. So this tells you that Brent's Theorem has some slack in it. So here's my question for you. Can you find a different, but valid assignment of vertices to phases that instead takes five time units instead of four. So what I've done is I put a set of boxes next to each vertex, and in each box I want you to enter a phase number. And the phase number should be one, two, three, or four. And remember the rule about phases every phase has to include exactly one critical path for text, no more and no less. Okay, here's one possible assignment. And notice what I've done. After one completes, any one of these three phases can begin executing. So I've assigned them all to phase 2. Now that's fine, it's a valid assignment. But my PRAM only has two processors, so I'm going to have to break that phase up into two steps. That's essentially the slack that the ceiling creates. Okay, so hopefully you now have some intuition for what Brent's Theorem means and why it's an upper bound. The other point I want to make is that choosing this assignment of work to processes and essentially breaking down the execution into phases is exactly what makes scheduling such a tricky subject. Now, given a dag and a weight estimated execution time, how can I tell if the dag is good or bad? Let's derive an answer. First, we'll identify a metric of goodness, and then we will optimize the metric. Let's use speedup as our metric. Speedup is defined as the best sequential time divided by the parallel time. In terms of symbols, I'll use T* to denote the best sequential time, and of course we'll use Tp(n) to denote the parallel time, as before. Now, a quick comment about the numerator and the denominator of speedup. The denominator will, in general, depend on the work, the span, the problem size, and the number of processors. The numerator will depend essentially on the work done by the best sequential algorithm. So for notational consistency, since I'm using work in the case of the parallel algorithm, I'll use another work symbol for the sequential algorithm. The best sequential time will essentially always be equal to the best sequential work. Now, you might be wondering, why am I saying best sequential time? What's special about best? The answer, is I'm a good professor, and I want to make it hard to cheat! After all, you can always make speedup artificially large by choosing a terrible baseline. If any of you are into marketing, you know what I'm talking about. Now if I give you a PRAM with P processors then ideally, you'd like the parallel algorithm to be P times faster than the best sequential algorithm. This condition is called ideal speedup or sometimes linear speedup, linear scaling, or ideal scaling. But basically, they all say you want the speedup to be linear in p. Here I'm using big theta notation because you usually won't care about the constant factors, at least not at this pencil and paper algorithm design stage. Now essentially, what this says is if p doubles, then you want Sp to also double. Of course, that's an ideal, we might not always achieve it. Let's write speedup in terms of best sequential work and parallel time. Now, in the general case we can use Brent's Theorem to get an upper bound on time and therefore a lower bound on speedup. So let's go ahead and plug in Brent's Theorem. For notational ease, I've dropped this paren of n in the right hand side. Just remember that there's a dependence on n there. Let's do a little bit of algebra on the right hand side to get it into a form that we can analyze more easily. Okay, written in this form, you can now immediately see what has to be true in order to get ideal scaling. First notice the numerator, which has the number of processors in it. So relative to this goal, I will pay a penalty, which is determined by the denominator. In other words, if I want to get linear scaling, I need the denominator to be a constant. So what would it take for this to be true of the denominator? Let's look at each term in turn. For this term to be constant, the work of the parallel algorithm has to match the work of the best sequential algorithm. This principle is something we call work-optimality. It's a necessary condition to ensure ideal scaling. Intuitively, it prevents another form of cheating. It says that if you get a very parallel algorithm by dramatically increasing the work relative to the best sequential algorithm, then that's actually bad for speedup. Now let's look at the other term in the denominator. For this term to be constant, it essentially says that p should be proportional to W* over D. So this is similar to the idea of the average available parallelism. And the main difference is that you have a W* here instead of just W. Now there's another way to write this. This other way to write it is to say that W* divided by P has to be big omega(D). Let's think about this for a second. W* over P is basically the work per processor. This expression says that the work per processor has to grow, and in particular it has to grow proportional to the span. And the span, remember, depends on the problem size, n. So in other words, this says that the work per processor has to grow as some function of n. In the parallel computing literature, this is called weak scalability. So basically what it says is as you increase the concurrency of the machine, then if you want to get good scaling, you might need to increase the problem size. Okay, that was a little messy. Let's try to recap the algorithm design goals you just derived. So starting form speedup we set linear scaling as our goal. Now, to achieve linear scaling, you derived two fundamental principles of good parallel algorithm design. The first is work-optimality, which says that the work of the parallel algorithm should match the work of the best sequential algorithm. And the second principle is weak-scalability. In one interpretation, it basically says that the work per processor should grow as a function of n, and that function is determined by the span Quiz time. Here are two parallel algorithms with the work and span as shown. My question for you is, which parallel algorithm is better? And my follow up question is why. Now, this is a, somewhat open ended question. And so there are no wrong answers. What I want you to do is think about it and then enter your response as free form text in this box. Okay, so here's my answer. It depends! Lame! What I really mean is, it depends what you want. Do you want work optimality? Do you want low-span? Do you want linear speed-up? Do you want better execution time? Depending on the metric you choose, you can derive a different response. Now there are a lot of cases to consider. I won't go through them all. Instead I'll refer you to the instructor's notes, which includes a pointer to a short reading that compares these different scenarios. In theory you take a DAG and you scheduled it on a PRAM. But here a question where do DAGs come from? The answer is from you. But you need a programming model to generate the DAGs. Let me show you one that, I think is especially clean and elegant. Here's the DAG for a divide and conquer algorithm to computer reduction. You have argued that it's work optimal and it has logarithmic span. Now, I want to give you an algorithm that produces this DAG. Let's start with just a sequential algorithm that implements the divide and conquer scheme. So here is the pseudo code for such an algorithm. It takes an array A of length n, and if there are at least two elements it does divide and conquer. So recursively calls itself on each half of the array, and returns the sum. Otherwise the array is just of size one and it returns that element. Now what you can observe is that the two recursive calls are independent of one another. Now since the two recursive function calls are independent. I'm going to give you a special keyword to mark that fact. That keyword is called a spawn. The target of a spawn is always either a function call or a procedure call. In this case I'm showing a function call. Now the spawn keyword is a signal to either the compiler or the runtime system that the target is an independent unit of work. By inserting the spawn you're effectively saying that the target may be executed asynchronously from the collar any time a processor is available. Now even though these two calls are independent units of work, notice that they produce intermediate results that then have to be combined. In other words there's a dependence from a and b and the return statement. That means in addition to spawns, we also need a way to indicate these kinds of dependences. For that, I will give you a second special keyword called sync. So, one question. To which spawns does a given sync apply? So we will use a particular convention which is that the sync waits for any spawn that has occurred so far within the same stack frame. If you need a refresher on call stacks and stack frames, please see the instructor's notes. Let me make one final remark about spawn and sync. Now, suppose you leave the sync out. Even if you leave sync out, there will always be an implicit sync at the return immediately before going back to the caller. Let me show you what that means by transforming this code to match the note. So instead of return of a+b, we'll generate three statements. The first will evaluate the operand a+b. The second will perform the sync. And then finally will return the value to the caller. And notice where the sync appears, and you transform the other return in the same way. Now, an important point. Even with this transformation, the program is still wrong. Can you see why? Notice that the sync appears after the sum. Now the two spawned calls are only guaranteed to be complete at the sync. Therefore the values of a and b might not yet be valid at that point. Now the fact of Implicit syncs will constrain the kinds of DAGs that this programming model can produce. The style of parallelism in such DAGs is sometimes called nested parallelism. Now once you see how this pseudo code gives rise to a DAG you'll understand where the term nested parallelism comes from. Let's go back to the correct code. Now what I want you to do is try simulating the algorithm to see how the DAG unfolds. So in particular let's say we start to execute a reduce on an array of size four. The first step is to enter the reduce call. That creates the first unit of work within the DAG. Then there's the conditional test. Then you encounter the spawn. Here's where things get interesting. The spawn creates a new branch in the DAG. Essentially the spawn signals that there's a new independent of work that's ready to go. And it creates a new path as well. Now in the mean time, the current path will continue. So this is an extremely important point to that the spawn creates essentially two independent paths. One path carries the new work and the other path is the path that continues executing immediately after the spawn. Now while the main path is happily going about its business, the newly spawned path is a recursive call. And therefore it has to generate its own subgraph. Now you are travelling along the main path and so you'll encounter the second spawn. This branching will happen again. Next, you'll reach the sync. The sync waits for the previous spawns to complete. In terms of the DAG, that should create some dependence edges between these subgraphs and the sync point. Control then goes to the return statement, and you'll evaluate a+b and then finally return that value. This example of a recursive reduction uses two spawns. But I wonder, are they both necessary in this specific example? Let me pose this as the following question to you. Which of these spawns may be eliminated without increasing the span? And when I say without increasing the span, I mean asymptotically. Here are your answer choices. Is it the case that you can eliminate A but not B? B, but not A? Or is it the case where you can eliminate A or B, but not both? Or finally, is the answer that you can't eliminate either one of them. That you have to keep both spawns. Why don't you pick one, then stay tuned for the answer. The answer is that you can eliminate B but not A. Now there's both an intuitive way to see this fact, as well as a formal way to see the fact. What I'll show you where is the intuitive argument and I'll show you the formal way after the quiz. The intuitive argument is based on this picture. Let's call the two spawns A and B and lets suppose you keep them both. In the DAG, A and B appear here, and here. Now notice that this picture has three paths in it. The three paths are important because they're all potentially critical paths, so we need to understand how they grow. The first path essentially follows the spine of the DAG. Now what you should notice, is that there's actually a constant number of nodes along the path. That means it's probably not our critical path. The second interesting path follows the A branch. Now, what's interesting about this path is it goes through this subgraph. And since the subgraph involves a bunch of recursive calls, that's where you'll see the growth in the length of a path. So this will be an important path to keep in mind. Now the third interesting path follows the B branch. And notice, just like the A branch, it goes through a piece of sub graph. Now, lets consider some hypothetical scenarios. Suppose I eliminate the first spawn, that will change the structure of the DAG. So in particular, this edge goes away and this edge goes away and a new dependence edge goes from the end of the sub graph to the next spawn. This follows from the fact I have to execute this reduce before proceeding to statement B. So, now this DAG really has two interesting paths. The first one which follows the A statement. The second one goes through the B Statement. What I want you to notice is that both the A path and the B path overlap substantially. The overlap occurs here. That's the part that involves recursive calls and so that subgraph could expand and could be very long. Now notice that the B path continues along and goes through its own recursive subgraph. The critical observation is that now with this transformed DAG, this recursive subcall happens before this one does. And so that means there's basically no concurrency between these two subgraphs. So basically what this says is if you eliminate the first spawn, you've eliminated the concurrency. So eliminating the first spawn is a bad idea. What about eliminating the second spawn? Let's do the same exercise and transform the DAG. Now let's follow the two paths. Now notice that even if you eliminate the second spawn that is the spawn associated with statement B, these two subgraphs can be executed concurrently. So this is saying that if you eliminate the second spawn, you won't materially change the span of the graph. So eliminating the second spawn is okay. Here's the beauty of the Span sync framework. You can analyze the work and the Span of algorithms in almost exactly the same way that you do for sequential algorithms. So almost everything you learned in Algorithms 101 you get to reuse here. Hooray! Let me illustrate this by example. Here's pseudocode for doing a sequential reduction using divide and conquer. If this were CS algorithms 101 and we were analyzing this running time, we'd start by writing down a recurrence relation and then solving the relation. So in this algorithm we divide the work into two pieces and do recursive calls. And that translates into a recurrence that looks like this, where we solve two sub-problems, each one of half the size. And you can solve this recurrence in any number of ways. For example, you can use the master theorem. What you'll find is that the time is linear in N. If you don't remember what the master theorem is, we'll put a link in the instructor's notes. Okay, so what about the parallel version? Remember, you want to analyze work and span. Now let's assume that each spawn and sync is a constant time operation. It turns out, this is not a bad assumption in practice. Now recall that analyzing the work is just counting total operations. Therefore, if spawn and sync are essentially constant time, then for the analysis, we can effectively ignore the spawns and syncs and just do the usual sequential analysis. So the recurrence for work looks just like the recurrence for sequential execution time and we'll get linear work. Hey, that's pretty neat. It means algorithm analysis with respect to work is no harder than it was for sequential algorithms. Okay, what about span? Span is a little different. Let me explain this by a simpler example. Now remember that a spawn creates a branch and a dag which yields two paths. The critical path will be the longer of these two paths Therefore if I knew the length of the path going through A and the length of the path going through B, then the critical path would just be the longer of those two. Mathematically, we say the span is the maximum of the span going through A versus the span going through B. Now for our divide and conquer reduction, the span only depends on the problem size N. And our cursive call solve problems of roughly equal size. Therefore when we write down the recurrence for span, we'll get something that looks like this. So the recurrence in this case is just a constant plus the span of n/2, and I'll let you solve this one. So for this quiz, I want you to answer the question. What is the solution to this recurrence? Let's do it as multiple choice. So pick one of these. Is it constant time, log time, linear time, or (n (log n) time? So the answer is (log n) time. And again, you can solve this using a master theorem or you can look at the recursion and write out a few terms. Before moving on, I want to make a brief comment about your goals as a parallel algorithm designer with respect to work and span. One goal is to achieve a degree of work that matches the best sequential algorithm. I called this goal one of achieving work optimality. Now I want you to hearken back to the days of sequential algorithm design yore just for a sec. Now from what you learned back then, your holy grail was to find a linear time or even sub-linear time sequential algorithm. This isn't always possible but it's a good goal. And since you want to be work optimal, that grail is the same with respect to work in the parallel case. So what's the holy grail for span? So what I'm going to do is, I'm always going to ask you to find algorithms with polylogarithmic span. So polylogarithmic means log n to some power. So that might be log n, log squared n, log cubed n, and so on. So always remember this principle of poly-logarithmic span. That's what we'll mean when we say low span. So what's special about poly logs? Well, since log n grows asymptotically a lot more slowly than n, and since O(n) is our work goal then poly-logarithmic span ensures that the average available parallelism grows with n. Now, you'll always have to use your judgement about whether a given parallel algorithm has good working span but please use these ideals as a guide. Here's another handy concurrency primitive. It's a parallel for loop, or par-for for short. Now if you ever run off on your own to read classic papers in the parallel algorithms literature, you'll sometimes see such loops described as for-any loops or for-all loops. So what does a par-for mean? When you use a par-for, you're saying that all iterations are independent of one another. In other words, you can execute the iterations in any order. To put it in DAG terms, a par-for creates n independent sub paths. You can think of this as executing n independent spawns simultaneously if such a thing were possible. Hey, what does he mean if such a thing were possible? That's smells suspiciously like a quiz. Now by convention, the end of a par-for loop will include an implicit sync point. The implicit sync point will force all of these independent paths to join. Now suppose every iteration has a constant cost. That is, imagine calling foo(i) is bounded by a constant. If there are n iterations then the work of a par-for is just O(n). So what about the span? Now in theory if all these foos have constant cost then you would think that the span is basically a constant. But in practice what you should expect to see is that the span will actually grow with n especially when n is really big. If you have, say, an electrical engineering or circuit background, imagine trying to build a circuit that has 100 million paths coming out of a gate. So to understand this point, I want you to think about two different implementations of a par-for using only spawn and sync, and see what happens. Let me give you one way to implement a par.for using only spawn and sync. It's a regular, sequential for loop that goes from 1 to n. And each iteration does a spawn. And finally, since we need an implicit sync, I'll stick a sync immediately after the for loop. So for this question, again I want you to assume that the cost of calling foo is a constant. Then the question is, what's the span? Is it constant? Log n? Linear? N log n? Choose one. The answer is that the span is actually linear. Can you see why? Now with respect to our holy grail of poly log span, this is terrible news. To see why the answer is linear time, let's look at how the dag unfolds. Now although you create a new path for each iteration, you actually execute the spawns one after another sequentially. Thus, you've actually created a sequential bottleneck in the dag. Now if it happened to be the case that calling foo was super expensive, you might not notice this for a while. But since we asked you to assume that foo's cost with a constant, the fact that the span grows with n is bad. Now suppose you implement par.for in this way. Let's replace par.for with a procedure call. This procedure call takes the function, or the loop body, as well as an iteration range. In this case 1 to n. The procedure call itself is implemented as follows. We figure out how many iterations there are, call that little n if there's only one iteration, and we'll call foo on a. Otherwise, we'll do divide and conquer. Essentially breaking the iteration range a to b into a to m-1, and m to b. Where m is approximately the midpoint. This scheme will give you the usual sort of binary tree recursion. So if you ask, what's the span of this DAG, it should be logarithmic. Now what I've just given you is a much more realistic way to implement a parallel for loop. So what I want you to do from now on is to assume this implementation. That means if I tell you that the loop body of a par.for has constant cost, and I ask you what the span is, the span will be log n. And, again, this is in contrast to assuming that the span is, say, constant cost. Which is what it would be in theory. Let's see how well you understand all of the ideas we've discussed so far. Here's a loop nest which implements a matrix vector multiplication. That is, suppose you're given a matrix A which is of size n by n, and two vectors, each of length n. Then this loop nest computes y = y + A times x. Now the complexity of this operation is O(n squared) since I have nested loops, each with n iterations. Here's my question. There are two for-loops. Which ones can I convert into a parallel for-loop? Your options are, you can only safely convert loop 1 or you can only convert loop 2. Or you can convert both loops or you can't convert either loop. Choose one. So the answer is only loop 1. The iterations of the i loop are completely independent of one another, because they always update different values y sub i. Loop j however is a different story. Notice that at different values of j, we still might be updating the same location i. So different iterations of the j loop are not independent of one another. In fact, if you have a compilers background, then you know that compiler writers would like to say that the j loop carries a dependence. And the problem that you create if you try to paralyze the j loop is called a race condition. I want to briefly explain why in this matrix vector multiply example, it's not safe to make the innermost loop a par four loop. First, observe that all iterations j write to the same location y sub i. This situation is called a data race. So more precisely, a data race is at least one read and at least one write that can happen at the same memory location at the same time. So let's take two iterations j just as an example. Let's assume that i is equal to 1. And let's look at j equals 5 and j equals 100. And furthermore, suppose that y sub 1 has the initial value of 0. Now, suppose I start executing the two statements simultaneously. And let's say I evaluate this multiplication and this multiplication at the same time as well. Maybe hypothetically, I get the values 72 and minus 100. Now, suppose I simultaneously read y sub 1. Then both statements will get the value 0 when they read y sub 1. Now, suppose that j equals 5 executes its right first, then y sub 1 will get the value of 72. But in the meantime, the j equals 100 statement is operating with a stale value of y. So when we do this sum, we'll get minus 100, and when we do the right, we'll overwrite the value of 72 instead of accumulating it. So it's clearly an error. Now, data race that leads to an error is called a race condition. So when you're designing parallel algorithms, you need to be careful to avoid race conditions. As a heuristic, it's often helpful to avoid data races as well. But an important point is that a data race does not always lead to a race condition. You'll see some examples of that throughout the course. Now here's a safe parallelization of the matrix vector multiply, that essentially just replaces for I with a parfor. We'll leave the inner most loop alone. Now the work is n squared. My question to you is, What is the span? Your choices are constant time, log time, linear time, and long n time, or n squared time. The answer is linear time. The reason is because the j loop is linear and is being executed sequentially. Let's think about this in terms of the DAG. Here's the DAG that would be generated. The parfor proceeds to divide the iteration space using divide and conquer. So, that'll give us log in levels and a DAG tree. The j loop, however, remains sequential. Some terms of the DAG, we'll get leaves that are essentially linear in length. So technically, the span is log n plus n. But that's basically subsumed by n. You should by now see that this parallelization of a matrix spectrum multiply has work that is O of n squared, and is span that is O of n. So the average available parallelism is O of n, and that seems pretty good, but I claim you can actually do better. Essentially the trick is to recognize that this innermost for loop is nothing more than a reduction. So here's some pseudo code that embodies this idea. Uses a temporary array of length n. It uses the temporary array to store the intermediate products, which are all computed in one shot using a parallel form. And then finally, we reduce the temporary array, getting a single value which we accumulated to y sub i. Now, for this to work, you're going to have to assume that this temporary array is private to each iteration of i. So that's a little bit of a problem, because there could be a blow up in intermediate storage. But for now lets just ignore that possibility. Now my question to you is which of the following options best expresses the span of this implementation? Your options are logged in, logged squared in, logged cubed in. The answer is 0(log n). Can you see why? This matrix vector multiply example gives us a chance to introduce a couple of pseudocode shortcuts. The shortcuts are all based on expressing operations on arrays as vector operations. So you see a hint of that here. Notice the declaration of the temporary array t. I've used this, what's called vector slicing notation. That is indexing with a range of values, in this case 1 to n. Let's see where else we can insert that notation. Let's take a look at this inner most parfor loop. What's it doing? Well, it's basically taking a slice of this two dimensional matrix and multiplying it component wise or element wise with this vector x. So I can express this parfor loop in a more compact single line of pseudocode. So I basically recognize that all these objects are arrays. t, A sub i, and x. The arrays are all of length n. And they're all indexed in the range 1 to n. Now this notation is common in a bunch of modern languages. That includes Python, Matlab and Cray Fortran. Cray who? Now if the range is the entire range of the array object, then we can replace the explicate range in this case 1 to n, with just a colon meaning all elements. So that will give us an even sightly more compact notation. Let's go ahead and plug that in. And of course we're all mature enough to see that this vector operation can be easily converted into a parallel for loop. So when you are analyzing pseudocode that includes these kinds of vector operations, just go ahead and use the right work and span. So in particular, these element wise operations will have linear work and logarithmic span. Now we'll allow ourselves to use a second simplification which is to get rid of the temporary array. So it's understood that this element wise product has to produce a temporary intermediate element wise result. Which we then pass to the reduce, so we can combine these into a single statement. Again, the only subtle thing is that this produces a bunch of intermediate values. So that may require a temporary. So if I ask you to analyze storage costs, then you might need to take that into account. Okay so what were the big ideas of this lesson? Well one idea is that good algorithms are work optimal and have low span, imagine that a DAG is a person where dependencies go from head to toe. Now a good algorithm is a person who has low mass and is short, and really wide, rather than being say really massive, super tall and super skinny. Our related idea is the concept of weak scalability. To see if you understand this concept, here's an exercise. Go and read about this thing called Amdahl's Law which you might already know about a different udasity course. Then ask yourself, self, what's the relationship between Amdahl's Law and this weak scalability concept? A second big idea in this lesson is divide and conquer which you know and love from sequential computing and it's an excellent tool and paradigm for thinking about parallel algorithms as well. Now in your toolbox of algorithmic tricks it should often be the first thing you try. The last big idea is to separate how you express concurrency from how you execute it. Now this is a really neat idea, which makes it possible to get truly elegant algorithms. Unfortunately, this model also ignores communication, which it turns out is absolutely necessary to be vast and scalable. At least once the problem size and system size get big enough. To model communication, you're going to need some other abstract models. But for now, let's just see how far we can take dynamic multi threading. This lesson is about parallel algorithms for sorting in the dynamic multi-threading model. There are two parts. The first part consists of the videos,which cover the idea of a sorting network. You can think of a sorting network as a special kind of circuit that sorts. It's a topic that's not usually covered in your standard undergraduate Algorithms 101 class. So I hope that many of you will find it interesting and, heaven forbid, even a little fun. Now separate from the video part, I also want you to read a short chapter on parallel merge sort. I hope you'll find this reading helpful. It should reinforce the main ideas behind the dynamic multi-threading model. You'll also see some additional techniques to analyze algorithms in this model more formally. In the beginning of parallel sorting algorithms, people were obsessed with the idea of sorting networks. So what is a sorting network? A sorting network is a fixed circuit that sorts its inputs using a special type of circuit element or gate called a comparator. For instance, here are two examples of comparators. Each one takes two inputs and produces two outputs. The increasing or plus comparator puts the smaller of its two inputs on the top wire, and the larger on the bottom wire. By contrast the decreasing or minus comparator does just the opposite. For example, suppose you feed the values of 1 and 6 into a plus comparator, the output would just be a mirror of the input because one is the smaller of the two input elements. Now suppose instead we replace them with a minus comparator, then the output would be reversed relative to the input. Like an electrical circuit, you can compose comparators by connecting wires. For example, suppose we have these three values and you want to sort them. I claim that this circuit will do it. It takes three inputs, produces three outputs, and contains three plus comparators. Let's trace its execution. The first comparator would produce this result. What about the second? The second would produce this result. Notice that the second takes its two inputs, three and one and puts the smaller one on the top wire and the larger one on the bottom. Finally, the last comparator would produce this result. Here's some symbolic mumbo jumbo that describes what this circuit does. Yikes. Stare at it for a minute and convince yourself that for any input it will produce a sorted output. Now I want you to notice something else about this circuit. As we said before it has three comparators. It also has a depth or critical path length of three. Here I've drawn arrows show the dependencies among the comparators. This analysis should remind you of the concepts of work in spam. So if you can take an algorithm and encode it as a circuit, we can analyze a circuit with respect to how many operations it performs, and what the critical path length is. By the way, here's a question. Suppose you're only allowed to use these two types of comparators. Compared to this circuit, do you think there's a way to sort three elements using fewer comparators or in a way that has a shorter critical path length? Suppose you want to sort four values. I claimed that the following circuit will do it. Note that this is a minus comparator and the rest are plusses. Can you see how it works? I want you to show me. At each wire I put a box. What I want you to do is fill in the box with the value of the wire. Let's visit each comparator and look at the outputs. Remember that plus comparators take their inputs and produce outputs in increasing order. So for this example, 8 and 7 would become 7 and 8. Minus comparators do just the opposite. In this case the inputs are already in descending order so the outputs will be the same. Now this comparator takes 7 and 6 as its inputs. So it will flip them. Same thing happens here. Finally, you can check that this column of inputs matches this column of outputs. Now, here's a thought question for you. Suppose I change this minus comparator to a plus comparator, lets update these values. Now, lets propagate the new values. Notice what just happened. The sequence is actually already sorted at this stage. Now that would seem to suggest that these two comparators are actually unnecessary. But is that really true? That is if I eliminated these plus comparators, would the leftover circuit sort any input of four values? I'll let you ponder that one. Filling in the intermediate values of this circuit, I want you to notice something. The first step produces sequences where the first half increases, and the second half decreases. In fact, this observation leads to a nice algorithm, so let's run with it. Here's a sequence of 32 values as shown schematically. Notice how it starts increasing, and then decreases. Such a sequence is called bitonic. Note that the word bitonic contrasts with the probably more familiar monotonic, or even the more familiar gin and tonic. Feel free to pause and grab one of those if it will help you get through this lesson. But only in moderation. Here's a more formal definition. A sequence of values is bitonic if it goes up and then goes down. To be a little bit more precise, the less than or equal to signs and greater than or equal to signs indicate that a bitonic sequence is initially non-decreasing and then becomes non-increasing. Now that's not the complete definition of bitonic. There's one more condition. If this up and down property holds not for the original sequence, but it does hold for some circular shift of the sequence, then the sequence is still bitonic. Now that was a lot of words, so let's look at some examples. Let's quickly check that you understand this definition of bitonic. Here are two sequences. Which of these is bitonic? Note that the answer might be only one, none, or both. Take the sequences and try to apply this formal definition. The answer is only the first sequence is bitonic. Remember that the definition asks us to check all circular shifts. So, let's imagine the values of the first sequence arranged in a ring. Next, let's check whether consecutive values go up or go down. Going clockwise, 2 to 3, 3 to 6, and 0 to 2 all increase. By contrast, 6 to 1 and 1 to 0 both decrease. With respect to the ring, all the pluses are consecutive and all the minuses are consecutive. So the sequence is bitonic up to a circular shift. Makes sense? Let's try the same thing with the second sequence. First, we draw a ring. Then we take differences going, say, clockwise. Phooey, the pluses and minuses are no longer consecutive along the ring. Therefore, there is no circular shift that will satisfy the first condition. Recall the definition of a Bitonic sequence. I claim that once the sequence is Bitonic, it's easy to sort. Let's see how. First, let's conceptually divide this Bitonic sequence into its two halves. That is, the half that goes up, and the half that goes down. For simplicity, let's assume that this division happens in the middle, though it doesn't have to in general. In a moment, you'll see why. Now after this logical splitting, let's pair elements of one sub-sequence with elements of the other. Let's start by pairing the first element in each sub-sequence. Then pair the second elements of each half, and so on. Now, so you can better see what just happened, let me reorganize the elements by pair. So the pairs are together now. Here's a question for you. What if for each pair you are to take, say, the smallest element? That would be the same as taking the min of each pair. What do you notice about these mins? That's right, the minimum values all form a Bitonic subsequence. What if, instead, you took the larger element of each pair? That would be the same as taking the max, and yes, again you would get another Bitonic subsequence within a circular shift. What you just did has a special name. It's called a Bitonic Split. That is, a Bitonic Split is when you pair the elements of a Bitonic input sequence, and then you apply the mins and the maxes to pairs. The result is two Bitonic subsequences. Did someone say split? It gets even better. All elements of the max subsequence are greater than or equal to all elements of the min subsequence. Naturally that suggests a divide and conquer scheme. I eat your freedom frites for petit dejeuner. I hope it's also not hard for you to see that you can split in place, that is without any extra storage. So the end result of a Bitonic Split would be two Bitonic subsequences, visually that looks like this. Recall that a bitonic split takes a bitonic sequence as input and produces two bitonic subsequences as output. Now suppose I give you an eight element bitonic sequence. You can implement an eight element bitonic split using a comparator circuit. I want you to show me how. I want you to do that by adding plus comparators to this circuit. Insert no more than one comparator per column. Here's an example of what I want you to do. Suppose you wanted to connect this input and this input by a plus comparator. You would do that by checking these two boxes. If you left these three columns blank, then this would be the output. Okay, now I want you to try. Here's one possible solution. Remember that a bitonic split pares elements from each half of the bitonic sequence. For each pair it then pulls out the smaller and the larger. That is of course the perfect task for a plus comparator. So here notice how the plus comparators pair elements of the input sequence. The mins of each pair will show up on these wires and the maxes will show up on these wires. Let's check the output. Both halves of the output are bitonic up to a circular shift. Awesome. Now I happen to place the comparators this way. But of course, any permutation of the comparators would work. So, a valid final bitonic split circuit would look like this. By the way, what's the work and span of this circuit? I'll let you ponder that while I get ready for the next topic. Recall the length eight bitonic splitting network. Given an input sequence that's bitonic, it computes two bitonic subsequences. Notice that the elements of the first subsequence are all less than or equal to the elements of the second subsequence. Now you could view this circuit as a DAG of independent comparators. Here are the nodes. Here's the corresponding DAG. Now from this observation, you can write down a very natural parallel scheme. Your scheme takes an input sequence of length n, and for simplicity, let's suppose that n is even. You then iterate over all pairs in parallel. For each pair, you determine the min and the max and you override the corresponding elements of the output. Easy. There is one subtle point. In the work-span model, the fix size circuit has a constant depth or span. But remember that for general and our convention was to assume logarithmic span for things like parallel for loops. A bitonic split gives us a natural divide and conquer scheme for sorting any bitonic sequence. Let's see how. Given an input sequence, split it. That means you pair elements from each half. In this case there are 32 inputs, so you'll pair elements that are 16 away from one another. Here's the logical pairing. Here are the mins and here are the maxes. That ends the first splitting step. The result is two bitonic subsequences each of length 16. You can then repeat this process on each subsequence. Start the splitting by pairing elements that are eight away from each other. First pair, then take mins, then take maxs. That leaves you with four bitonic subsequences, each of length eight. And you'll just repeat this process until you run out of pairs What you just did is called a bitonic merge. That is you started with a bitonic sequence as input and you sorted it. Let's write out some pseudo code. You're given a bitonic sequence, let's say it's of length n. If there were at least two elements then you do the following, again for simplicity, let's assume that n is even. You start by splitting the sequence. This will give you two bitonic subsequences. You then merged each of those subsequences. Now remember that all elements of one subsequence are less than or equal to all elements of the other. That makes them independent. Therefore you can spawn one of them. Yippee, it's quiz time. I think this one will be fun. Recall that, given an input sequence that's bitonic, a bitonic merge produces a sorted output. Your assignment is to create a Bitonic Merge Network. That is to start with this empty comparator network, and then do the following. In each column, I want you to insert one or more non-overlapping plus comparators. You can do that by selecting an even number of boxes in each column. For example, suppose you checked these two boxes. That implies a plus comparator between those two inputs. Now, if you were to evaluate this network, leaving all these other boxes blank, then here's the output you would get. Now, clearly, these intermediate outputs don't match the final outputs. So this can't be the full network. Let me show you one more example. Suppose instead that you chose these four boxes. Recall that comparators within a column cannot overlap. Therefore, those four boxes correspond to these two comparators. Were you to evaluate this network, you'd get these values as outputs from the two comparators. As you can see, they still don't match the outputs. So if you start your network this way, you'll need to keep adding comparators in the other columns. Of course, you can chain the output of one comparator to the input of another. Oh, and one more thing, your network needs to be able to sort any length a bitonic sequence, not just the example I've shown here. Have fun and good luck. Remember that a bitonic merge is a sequence of splits. And a sequence of splits is just a set of minmax pairs. Now the input is a length eight, so the first split should pair elements that are four away from each other. Now with this pairing, the first four wires will be one bitonic subsequence, and the second four wires will be another. You should then split these two subsequences. Again, within each subsequence, which is of length four, pair elements that are two away from each other. Let's see, so what would that look like? This should give you four bitonic subsequences. We then repeat with the remaining pairs. To make this a little bit easier to see, here is what the final network would look like. Check it by computing a trace of the values on each wire. A good follow up exercise is to compare this circuit with the bitonic merge pseudo code. Given a Bitonic sequence, a network like this will sort it. But given an arbitrary input, how do you generate a bitonic sequence? Here's what I'd like you to do. Create a comparator network that takes an arbitrary sequence like this one and produce a bitonic sequence like this one. Do that by inserting one or more nonoverlapping comparators in each column. You can use either plus or minus comparators. Now to help you get started, let me fill in the first and the last columns. Here's another hint. Here, I've traced the values of the first comparator bank. That should remind you of the difference between a plus comparator and a minus comparator in case you've forgotten. So insert your comparators by typing pluses or minuses into appropriate subsets of these empty boxes. To figure this out, let's think about what this first set of comparators is really doing. What it's really doing is creating up and down pairs. If that's the case, then what must be true? Well, the first four elements are bitonic, as are the second. Does that mean it's time for a celebratory cocktail? Not yet. Patience, my child. Now, you have two bitonic subsequences, but your goal is to get just one. So, what you need to do is to turn one of these into an increasing subsequence, and the other into a decreasing subsequence. Let's take the top half. Given a bitonic sequence, how do you get an increasing sequence? Well, the increasing sequence is sorted. So, you'd use a bitonic merge, of course. Let me fill in the circuit. This top sub circuit is just a bitonic merge network for an input of size four. The bottom one is the same, but with minus comparators, so that you get a decreasing sequence. In fact, you can view this first layer that I filled in for you as little bitonic merges of size two. And so you can verify what's happening, here's a trace. Okay, let me summarize all of this. You start with an arbitrary input. Then, you run little plus minus bitonic merges of size two. After that, you run plus minus bitonic merges of size four. And if you add larger input sequences, you would just keep going. In other words, to go from an arbitrary input to a bitonic sequence, you just run a bunch of bitonic merges of increasing size. Neat, but not on the rocks. For completeness, let's briefly summarize where you stand, with respect to sorting, using bitonic ideas. First, given an arbitrary input, you have a procedure to generate a bitonic sequence. A pseudocode algorithm would look something like this. You can think of it in terms of divide and conquer, first you create two bitonic subsequences. Then you turn one into an increasing sequence and the other into a decreasing sequence. The main thing you need are both plus and minus variations on bitonic merge. Now once you have this primitive, sorting becomes really easy. Use this routine to generate a bitonic sequence from an arbitrary input, then run a bitonicMerge on it. Schematically, that's just glueing the genBitonic circuit with a bitonicMerge circuit. You can show that the work in span, a bitonic sorting is as follows, notice that with respect to span, it's polylogarithmic. That's good, but what about work? With respect to work, this algorithm is not work optimal. You know from CS101, that practical comparison based algorithms are possible with just O of n log n work. Of course, that begs the question why bother with bitonic sorting? For me, one really neat thing about the bitonic sorting algorithm is that it has this nice visual representation, which connects it to the dynamic multithreading model. You've also seen that it has a fixed regular parallel structure. So, you could imagine a very natural implementation using, for instance, the old programmable data rays or fpgas. Now the regular structure also means that it will map well to fixed data parallel hardware. So this includes stuff like sim-D or vector processing systems as well as graphics coprocessors or GPUs. The downside is that it is not work optimal even when restricted to the class of comparison based algorithms. So in practice, you'll have to make an engineering tradeoff based on the platform and the scale that you're interested in. The best way to try and understand this tradeoff is to try implementing these and competing algorithms on real systems and see what happens. So until we meet again friends, Cheers. Delicious. [LAUGH] Mm. Mm. All right, I can totally do this. I can totally do this. Shh. Ssh, don't be so loud. [SOUND] Ha! See, I told you. You. Am I offscreen? Am I even onscreen? I don't know. It's like butter. Like butter in a glass. It's like you I love you In this lesson, I want you to think about computations that, on the surface, don't appear to have any parallelism. In the process, you'll learn what I think are some really neat tricks about how to extract parallelism when it's not obvious how to do so. As a teaser, I want you to imagine the following scenario. Suppose you're standing in an absurdly long line in a very large room. The room is completely dark, so you can't see anything. Maybe you're waiting in line at the registrar's office to pick up your OMSCS degree. You ask yourself, how many people are in front of me in this line? And, to find out, you can only talk to the person right in front of you or right behind you. That's not so hard, you say. After all you're young, smug, and smart, and you know that the first person knows that she is the first person, so she can turn to her neighbor behind her and say, hey, I'm number one. That person now knows that he must be number two. So he pays it forward or, in this case, backwards and tells his position to the next person, and it goes on and on like this. Now that's a lovely and simple solution to this problem, but suppose there are 10 million people in this line. If it takes 1 second to do an information exchange, then it's going to take about 10 million people x 1 second / 3,600 seconds per hour / 24 hours per day, or 115 days for the last person in line to learn that he or she is number 10 million. That person, in nearly four months' time, could have probably finished a whole other OMSCS degree. Now, this scenario is an analogy for a well-known problem in parallel algorithms called list ranking. You are given a singly linked list and a pointer to the head. What you want to compute is the position of every node in the list. Sequentially it's trivial to do, but if you have multiple processors, can you use parallelism to do it faster? You might want to pause the video for a second and see if you can think of a way, and unless you already know how, I bet you'll have a very hard time doing so. To get an efficient parallel scheme, what you might find is that every thing you've been told about the best way to store a linked list is completely wrong. File it under lies my other teachers told me. So, without further adieu, let's see if we can undo a couple years of bad sequential computing habits. I want you to think about the prefix sums problem. Suppose I give you an array like this one. The prefix sum at some position like, say, this one, is the sum of all elements from the beginning of the array to that position. So, in this example, the prefix sum at this point is 3 plus 4 minus 2 minus 1, or just 4. What about the prefix sum at a different position like, say, this one? The prefix sum at this point is 6. And you could get that, of course, by taking the prefix sum at this point, adding 7, subtracting 5. So just to recap, the prefix sum at position i is simply the sum from a sub 1 up to a sub i. Just to make sure you're paying attention to the definitions, I want you to compute the remaining prefix sums of this array. You can do that by just writing the appropriate value in the blank. Here are the answers. I think you can imagine an easy sequential algorithm for doing this computation. It would simply start at the front and go to the back. It would maintain a current sum and add values to it. I think you could see it would be easy to do in place, that is, without creating a separate array. So a sequential algorithm for this problem would be super efficient in time and space, at least from a sequential point of view. Now you needn't prefix yourself to just sums. Consider this example, imagine this as a gorgeous terrain of hills and valleys, and I'm sorry, Georgia Tech and Udacity only pay me to do eight bit landscapes. Now suppose at each point in the terrain you actually know the elevation. Here are labeled as integers. Now, suppose you have a friend, who stands here and looks straight ahead, what will he or she be able to see without occlusion? I claim one way to figure it out is to do a prefix-max computation. The input array to a prefix max would be the list of elevations. A prefix max computation would ask for each point in the terrain, what is the largest elevation I've seen so far? Your friend could see as long as the elevations in the terrain were no higher than his or her position. Let's compute the cumulative max at each point. Now to figure out whether your friend could see this awesome landmark in the distance, just look at the maximum elevation at that point and compare it to your friend's height. In this case is 4 is a lot bigger than 1.5, so your friend has no hope of seeing my beautiful Eiffel Tower. Now the generalization of a prefix sum is sometimes called a scan operation. To use a scan in an algorithm, all you have to do is say that you're doing it and say what the operator is. For example if we were doing a prefix sum then we would say we were doing an add-scan, that's a scan and the operator is a plus. Or as in the previous example we did a max scan, you could have product-scans, logical add scans, or-scans, whatever your heart desires. Here's a simple sequential algorithm to compute a scan. The input is an array A, of length n. The first element of the array is its own scan. For all the remaining elements of A, we take the previous scan value, and accumulate the next value. And we store everything in place. Consider the following claim. You can parallelize this sequential scan by taking four and replacing it with parallel four. True or False? Go ahead and choose your answer here. So in this case, the answer in false. The reason is that the iterations aren't independent. For example, I equals 5 depends on the outcome of i equals 4, which was produced in a previous iteration. So you'll need another way. You can't replace for with parallel for, because these iterations are not independent. So what should you do? Now you know that a single reduction is parallelizable by using a reduction tree. So here's a thought. What if we did n completely independent parallel reductions? Here's what the code might look like. My question to you is, what is the work, and what is the span for this algorithm? Choose one option for each. The span in this case is logarithmic. The reason is because par for has logarithmic span as does reduce. What about the work? The work is O(n squared). In each iteration you do a reduce, which costs you O(i) operations. So to get the total work, you would sum overall i from 1 to n. Notice that's a lot worse than the best sequential algorithm, which only has O of n operations. So this algorithm is totally lame. Lame! So how do we parallelize scan? Let's first start with the prefix sum problem. Once we understand that we'll have a pattern for parallelizing any scan. By analogy to reducing an array to a single value, if we assume associativity, then we can rearrange partial sums. So somehow we want to use this idea for parallelizing scan as well. So assume associativity and consider a scan of this input array. You could try by doing the same thing you did in a reduction, for example let's take every pair of elements and let's combine them. The result will be a new set of partial consecutive sums. For example, this node is the sum from 1 to 2 and this node is the sum from 3 to 4. In fact let me label each intermediate sum with a notation A colon B. The important point is that these are consecutive partial sums. So with this notation, this is what the final output should look like so let's go back to the partial sums. Suppose you could magically and recursively do an add scan on the partial sums. That is, I take these partial sum values and I somehow magically apply a scan to them. What would I get? In this case I would get 1:2 as the first scan element. Then for the second scan element I would get 1:2 plus 3:4. In other words, 1:4. And for the third element, I get 1:2 plus 3:4 plus 5:6. In other words, 1:6. And then finally, 1:8. Notice what that gives you, all of the even results. So, voila. We've done half the work. Now what about the odd results? How do we get those? Well, that should be easy. If I went 1:3, I just take 1:2 and add 3. You can do the same thing for 1:5 and 1:7. Voila. You're done. Stop for a second and make sure that that makes sense. Now I claim here's some pseudo code that implements this scheme we just described. Let's do a quick analysis to make sure we got it right. Now here I sub O and I sub E store the odd and even indices respectively, that is, 1, 3, 5, 7 and so on and 2, 4, 6, 8 and so on. Now this particular algorithm is in place adding the odd and even elements over here corresponds to this line. By in place, I mean we take this result and we overwrite the even elements. This magic recursive step appears here. And this final combine step appears here. Now, note that I've used this python slicing notation to mean starting at element 2 and going to the end of the array. If you can convince yourself that n equals 1 produces the correct result, then you can use induction to prove that the overall algorithm is correct. Here's a question. What are the work and span of this parallel scan algorithm? So here are your answer choices, and for each of work and span, choose the correct one. That is, choose one from each column. The answer is linear work and log squared span. Now I want to make a couple subtle points about the analysis so rather than derive the solution here, let me start afresh. Notice that this algorithm does n over 2 additions at this statement. Then at this statement, it does n over 2 minus 1 additions. And finally, the add scan operates on a problem that's half the size. So what does that mean? Basically, the recurrence for the total work is this. So the work is linear in n. Before moving on to the span, let me make a quick remark. I hid a constant in here. What's the constant? As it happens, the constant is about 2. Now that 2 is counting only addition operations. Now this factor of two is pretty interesting. Think about what that means for a second. The purely sequential algorithm only has a constant of about 1. So evidently our parallel algorithm does twice the work. Now wearing your theory hat, that's not a big deal. But it does raise an interesting and more general question. Will you, in general, need to pay some sort of price in order to get parallelism? Okay, Voodak, enough with the distractions, what about the span? Now the first observation is the recursive call, which always operates on a sub-problem of half the size. So you'd naturally expect about O (log n) levels. All the other operations are data parallel. That means you can implement them using parallel fours, which will also have logarithmic span. Putting that together, here's what the recurrence might look like. If you solve the recurrence you will get O(log^2n). Again the master theorem makes solving it quick and easy. Alternatively, you should be able to see the log squared and behavior intuitively. There's login span from the statements within this function, and that's repeated log in times. Let's apply scan in an interesting way, namely in parellelizing the quicksort algorithm. And I know quicksort is your favorite algorithm from computer science elementary school. First, let me remind you how to do a quicksort sequentially. It's all based on what I'll call a quicksort step. A quicksort step is a kind of dance move for people like me who have two or more left feet. The first part of a quicksort step is choosing a pivot element. Now, any element can work, but a good choice is to select one uniformly at random. So for this 12-element array, you might roll a 12-sided die and pick, I don't know, this element. The next part of a quicksort step is to partition the input around the pivot value. That means get all the elements that are less than or equal to the pivot and put them on one side. And get all the elements that are greater than the pivot and put them on the other side. By doing so, you now have two subparts of the array that can be sorted completely independently from one another. That ends the quick step. Now, since the two halves are independent, you can spawn them as independent tasks. And you just keep repeating the quick step until you're done. Choose a pivot, partition and spawn. Here's the algorithm more formally. First, it chooses a pivot at random. Then it partitions the input around the pivot. Then it conquers these two halves. And finally, it glues these two results back together. Quick and easy. Well, except for one detail. How do you do this partition step in parallel? Yeah, it's quiz time. So here is pseudo code for an algorithm that I claim will gather all elements that are less than or equal to some pivot value, extracting them out of this input array A. The algorithm stores the output in another array L, returning it here. Take a minute to ponder this algorithm and then answer the following question. So if I take this for and I replace it with a par for, will the computation be both correct and efficient? Your choices are yes, the algorithm will be both correct and efficient. Or, the algorithm will be neither correct nor efficient. Actually, if it's not correct you don't care whether it's efficient or not. So no just means it won't be correct. So choose one. Here is the answer. No. If you replace four with par four, the algorithm will be incorrect. Replacing four with par four is a bit like running with running with scissors, don't do that. If by chance you've taken the other awesome systems and architecture courses that are a part of the OMSCS. Then you probably know about multithreading and locks. You could use those constructs to protect these updates to the variable K. But were you to do so, you really wouldn't get much parallelism anyway, because any simultaneous threads would most likely contend for the lock. I think what we need is a different and more algorithmic approach. You've been thinking about scans. So you should suspect that you can use a scan somehow, in order to address this parallel partitioning problem for quick sort. Can you see how? Here's an idea. Suppose this is our input and the pivot value is, I don't know, this three. First, why don't you compare every element to the pivot? If you have some auxiliary storage, you can do all of these comparisons and record the results entirely in parallel. For the results, you'll get a 1 if the element is less than or equal to the pivot and a 0 otherwise. Now, here, I've written the Boolean results as 0, 1 integers. Here's why. Suppose you do a scan on this array of 0, 1 values. What would you get? Here is the result. Now, I want you to notice some things about this output. First, the last element of the scan is the total number of elements that are less than or equal to the pivot. This is especially convenient, because it would allow us to, say, allocate an output array of the appropriate size. Here's something else to notice. In this output sequence, you get a little bump everywhere you encounter a new element that is less than or equal to the pivot. That's really neat. It means, for example, that the elements you want are easy to spot just by looking at the scan. You look at a value, compare it to it's previous value, and you can see right away, oh, I must have encountered a new value here. So just by inspecting the outputs, we can find the elements we want. Now, there's a much cooler thing to notice. These scan output values that I've highlighted are both unique and consecutive. So what does that mean? It means, you can use these values as indicies into the output array. Think about that for a second. You can use these scan values to do a fully parallel right with no conflicts. So for every element that I want, there is a corresponding unique integer. The integers are consecutive and they range from 1 to 5 which is the length of the output. And there is a 1 to 1 correspondence between the value I want and its output index. So let's go ahead and write those all out in parallel. Yes! Let's summarize your scanned based approach to partitioning with some pseudo code shown here. Here, we compute the flags. Here, we do a scan to get the indices. And finally, and we do this in parallel, we inspect the flags. And if a flag is set, then we can safely do this output. And luckily for us, k provides these unique, consecutive integer addresses. I hope it's easy to see that this procedure has both linear work and logarithmic span. If it isn't, do not pass go until it is. Now, this procedure of doing a flag scan followed by a write is an extremely handy primitive. So let's refactor these lines of codes into a tool that becomes a part of our algorithmic toolbox. So I've taken these lines of code that were here and I've extracted them into a general procedure called gather If. Gather If, as you can see, takes an input array a of length n along with a flag array. Then, it does the scan and finally, the parallel outputs using the flags as a guard. You can now insert this new primitive. And in fact, I want to introduce one additional notational shortcut. Rather than having to write gatherIf, if you give me an array and an array of flags then a sub f will do the gatherIf. With this new primitive, the original quick sort algorithm we had been talking about becomes easy to write in a very compact fashion. Let's take a look. Here's the original quick sort, and here's the partition step. Let's replace these with our new notation. So here, I'm computing the flags on the fly. And then I'm using them as indices into the original array to extract just the elements that I want. Cool beans! I want to tell you about a neat variation on scans called Segmented Scans. Now, suppose I give you this array. And let's say that you want to perform a scan not on the entire array, but you want to do a bunch of independent scans on segments of the array. In this case the example is broken up into four segments. Now suppose to tell you where the boundaries start and end I give you an array of flags. Flags are Boolean with a true value wherever a segment begins. Were you doing things sequentially you might use the following algorithm. So the algorithm iterates over all elements and it looks at the flag array. If the flag is set, meaning it's true, then the algorithm does nothing, basically leaves the value A sub i alone. Otherwise, it does a scan. This computation looks tantalizingly like a scan, except for this pesky conditional check. So what if we did the following? I'm going to define a new kind of value, which I've labeled here as Xi. This Xi is basically a two pole, or in this case a pair. The pair consists of one value from the array and the corresponding flag. Now from this new data type, let's define a new funny operator that I'll call op. Op takes two pairs and basically implements something that looks a heck of a lot like the body of this for loop. It checks the flag of this second object. Now if that flag is false, meaning we don't take the if, then it just returns the second value. Otherwise, it returns a special value that combines the two inputs. Now notice this is the usual addition, which we would see in a scan. And this is a logical or operation, since we're operating on Boolean flags. Now I claim, if you have this operator you can re-write the segmented ad scan from before in a different way. Let's see how. Here's our operator from before, and here's the new implementation of segmented add scan. First, we'll declare a new array to hold these pairs. And notice it starts at zero instead of one, so they're actually N plus one such pairs. This next snippet of code actually builds the array X. You'll notice it builds the pairs but it also builds a special initial value, which is set to zero and false. And fact, zero and false are the identity elements for the two operators, addition and logical or. Once X is all set up, we can do a scan using our funny new operator. And finally the routine only needs to return the updated scan values. So there's a final little loop here to extract the left most element of each pair. The left most element being this guy. Now you should compare this new segmented scan with the old one and convince yourself that this new implementation computes the same thing. To convince yourself that this new scan is correct, there's something you need to prove about this operator. What is it? Here are four choices, and I want you to check all that apply. The choices are does the operator have to have constant cost? Or does it have to be in place? Or does it have to be associative? Or does it have to be commutative? Okay, here's the answer. You need to shop that op is associative. You don't need all this other stuff. In particular, associativity means that op of a, b, then op with c has to be the same as a, op with b and c. Basically you put the parenthesis where you want. So let me just quickly run through some of the other options. So the constraint of constant time only affects the work in the span of a scan, not its correctness, so this is not relevant. What about being in place? Well, our algorithm uses op as a function, so being in place doesn't matter. Now the fourth option commutativity is interesting. Like a sociativity, it's kind of rearrangement property. But actually, it's a stronger property than what we need. In fact, you should go back and look at the parallel scan, and you will observe that it always only combines consecutive values. So that means, we really only need a sociativity, It's not too hard to show that our definition of op is in fact associative. But it's mostly messy algebra. I'm super lazy so I'll leave that as an exercise to you. As an aside there's a really neat and very general framework for many other kinds of recurrences. These include things like segmented scan, which you just looked at. For moe information, please see the instructor's notes. You are now ready to think about one of my favorite basic parallel computing problems, the problem of list ranking. It's a notoriously hard problem to speed up which is what makes it fun. And along the way to parallelizing it, I think you'll get a lot of insight into the differential between sequential and parallel computing. Now suppose I give you a collection of values stored as a singly linked list with a head pointer. The list ranking problem asks for the distance of every node from the head. So for example, for this list, we compute the following values. The head is a distance of zero away from itself, so it gets a zero. Its successor gets a one, its successor gets a two, and so on. Sequentially, this problem is super easy. Start at the head, maintain a counter, and walk the list. To be more precise, here's pseudo code for an algorithm. Notice what it does? It maintains a counter, starts at the head, and it iterates over the rest of the nodes, storing the rank, and updating the rank as it goes. Now, I want you to stop and think about this for a second. How the heck would you parellelize this? In principle, I'll claim that this problem is sort of like a scan. Can you see how? Suppose it were possible to formulate the list ranking problem as a scan, what values would you scan? That is, imagine you have this list and you're allowed to put any value into each of these boxes. And you want to to choose those initial values, so that if you were to do a scan on the list, you would get the ranks shown here. So here's your mission, enter the values as integers into each of these boxes. Here's the answer. Stick a 0 in the first node, and a 1 everywhere else. So if we scan these initial values, we'll get a 0, 0 plus 1 is 1, 1 plus 1 is 2, 2 plus 1 is 3 and so on. In principle, a list ranking is a scan. The essential problem is this data structure. It just has a single entry point, which is the head. And the interface as we've defined it leaves no option to get other elements of the list without traversing them one by one. Now put another way, I could ask you the question what made scans on arrays easier? Essentially, it was random access. So what you need to do is you need to take this list representation and turn it on its head, so to speak. Let's imagine a different way to represent a linked list based on what I'll call an array pool. And this representation has two features. First, we'll find a way to put the values in an array. And secondly, we'll replace the concept of a pointer, or a next pointer, with a next integer index. Let's see how this works. As an example, suppose I allocate an array to hold the values. Let's call this array V. Now notice that our input list has 12 nodes, even though the array has 14 spaces. So in general we'll let our array pool representation be at least the size of our input list. Now below each node in the input list, I've written an integer. Each integer will be the position of the corresponding value in the V array. For example, a node with value zero resides in position 13 of the V array. So let's put it there. As another example, let's take the first node whose value is 1. Its position is 12 so let's put it there. What about the second node whose value is 1? Well that's in position 10, so we put it there. You could of course go on and on and maybe you should. Mm, smells quizzy. So that's our story for values, but what about the next pointers? For that, lets allocate another array, call it capital N. Now I've made the length the same between N and V, so that for every value, there's a corresponding next pointer. Now although I'm calling these pointers what I'll put there is not a memory address but rather the index of the node. For example look at this node. It points to the node whose position is 9. So it's next pointer should also be 9. For the two nodes whose value is 1 the successors are at positions 5 and 8, so we should put those in capital N as well. This array pool representation is critical because it give us a way to have multiple entry points into the list. To do a list ranking you still have to walk the list somehow. But at least we'll be able to do things like data parallel operations because we basically stored everything as arrays. Here's a linked list which I want to represent in an array pool. I want you to fill in these boxes. Now, the array pool is of size 14, whereas we only have 12 nodes on our linked list. So that means there are going to be some unknown values. If you have an unknown value in the V array, enter a question mark. And if you have an unknown value in the n array, use the value zero. Zero will also be our proxy for a nil. So for this nil pointer here, enter the value zero. Here's the answer to the quiz. You can check it by walking the array pool list. The head is at position 12, and it points to 5. So let's see. The head has a value of 1 at position 12, and points to 5. Check. Now 5 has a value of 6, and points to 10. So let's see. 5 has a value 6 and points to 10. Woot. Now two of the nodes are unallocated. For those, I put question marks. I also set their next pointers to nill as instructed. And I did the same thing for the tail. Here's a procedure, foo, that takes a linked list in the array pool representation and does something funny with it. My question to you is, what does foo do? I want you to enter your explanation as free-form text in this box. There are several possible interpretations of what foo does. Here are two things I might have said. One is that it reverses the list. The second is that it computes an array P such that the collection V, N, and P is now a doubly linked list. You can see that it iterates over the next pointers. For each next pointer, it sets the next pointer's P value to itself. Let's recap what you've done. You've replaced the conventional linked list representation with one based on arrays. With that you're now ready to go parallel. To help you do that, I'm going to give you a handy primitive called a jump. Here's a picture of a jump. Oh wait, sorry, wrong picture. Let's try again. Suppose you're looking at a specific linked list node. In a jump, you move the next pointer so that it points to the neighbor's neighbor. So take this next pointer, for example, and let's replace it with a pointer to the neighbor's neighbor. Now, think about this for a second. What the! Bad billy goat! What would happen, if at every node in the list, you did a jump at exactly the same time? Let's try it and see. So here's the list. At every node, do a jump. Notice what you've just done. You've effectively split the list into two sub lists. As you can imagine, that's exactly what you need to do divide and conquer. Now if you keep doing this, these sub lists will get shorter and shorter and you'll get more and more of them. Jump. Given a set of next pointers as input, you can perform a jump in parallel using this algorithm. And notice that the new algorithm puts the jump pointers in a new output array called N sub out. So for each node i, we check to see if the next pointer is nil and if it's not, we do a jump. By placing the output in a separate output array, we avoid collisions and that's what makes it safe to use parallel for. Okay, you're almost ready for the parallel list ranking algorithm. So far you've seen three ideas. The first is to store the list as an array pool. The second is to recognize the list ranking is roughly an add scan. Of course you need to set the right initial values. And finally if we have a jump operation, we can do divide and conquer. Go ahead and put these ideas together. You've established that a linked list has an equivalent array pool representation. So when I sketch the algorithm, I'll draw a linked list in the old fashioned way, but you always know that there's an array pool equivalent. What about treating the list ranking as an add scan? Remember that the key idea here was to choose the right set of initial values at each node. So we put a 0 at the head and 1's everywhere else. So now we can apply the third idea which is to use jumps to get sublists. If you jump repeatedly, you know the sub list will get shorter and shorter. At the same time, you'll be transforming this list, so you're going to have to do something with these initial values. How about this, let's maintain an invariant. The invariant will be, if we take any node i, and we start at the head of its sub list, and then let's say we traverse the sub list, and we add up all the values. Then we'll get exactly the rank of i. So for the initial values, you can easily check that this holds. Now suppose I've done a bunch of jumps and I'm looking at a particular sublist. So here I'm just in the middle of some sublist. And let's further suppose that the invariant holds. Now let's jump everywhere simultaneously. You'll get a list that looks like this. Now notice what happened. For this node here, we effectively cut 4 out of its sublist, but we need to maintain this invariant. So we can't just drop the value 4, we've got to incorporate it somehow. An easy way to do that would just be to absorb 4's value. So 12 would become 16. The way this can work is before we do a jump from 4, we take the value of 4 and push it to its successor. Okay. Let's give this a try. Let's do one step of the update and jump. For the update every node pushes its value to its successor. Then we do the jump to create two sublists. And let's just repeat this process. Push, jump, push, jump. Once every node is a stand alone node you are done. Now I already gave you a subroutine to perform the jumps. What about the updates? Here's an algorithm for just the update step, which is parallel. So for each node i, we take its value and we push it to the successor.. We'll do all the rights to a separate array of ranks. This way we can do all these updates in parallel. Okay. You're now almost ready to see the final algorithm. If you do a jump on every node on the list, I'll call that a jump step. So here, I'm illustrating one jump step. Here's the question. Suppose the array pool is of size n, what's the maximum number of jump steps you can perform? Here are your answer choices, pick one. The answer is log n. The maximum list length is n nodes. And a jump step reduces the list into roughly two sub lists. Therefore should be a logarithmic number of jump steps. I think you can see how to write the algorithm at this point, but just to be complete, let me formalize it. First, my algorithm maintains two copies of the rank and next buffers. You need two copies because the interfaces to update ranks subroutines, always wrote their outputs to a separate copy. Remember that the extra copies ensure that rights can happen in parallels safely. The algorithm itself basically just does a sequence of sealing of log m update and jump steps. You can see the two copies here. After we do the simultaneous update and jump steps, we do a swap. There's one last step you need, which is to return either R1 or R2 depending on which was the last copy updated. Here's a question. What's the work and span of this parallel list ranking algorithm? Here's your long list of choices. So please choose one for each of work and span. For the span, the answer is log squared. This outer loop does log sequential steps. In each of update ranks and jump list consist of parallel fors which have logarithmic span. What about the work? The work is actually m log m. This outer most loop contributes the log m part. And since updateRanks and jumpList visit every node in every iteration, we have linear cost. So the total is m log m. Now seeing this result you should be disappointed. Although the algorithm is parallel it does not work optimal. Remember the naive sequential algorithm only has linear costs. In addition the constants are very low because it really just touches every node just once. So this log m cost and all these extra buffers really add a lot of additional overhead to this ranking. This means you need a really long list and lots of processors to see any speed up in practice. Now there are ways to do work optimal list ranking and get decent speed ups. I'll put a link to one or two of these algorithms in the instructors notes. The big idea in this lesson is scan or parallel prefix. It's a very powerful primitive for exposing data parallelism. It vectorizes well and it can turn seemingly irregular serial computations into those that are both regular and parallel. Now there's a catch. If you count carefully, you might have noticed that scans actually move more data in total than their sequential counterparts. That fact is hidden in the asymptotic analysis. But lets save the topic of data movement, communication, and algorithms that minimize them for a different lesson. Hey, kids, welcome back. I thought we'd kick off this lesson in one of my favorite spots in Atlanta, Piedmont Park. This area used to be a dilapidated fairground, but then sometime in the early 1900s it was transformed into the lovely and public open space that you see today. This transformation, by the way, was done by the Brothers Olmsted, sons of Frederick Law Olmsted whom you might know as the lead landscape architect of Central Park in New York City. Okay. But why are we here? Well, this lesson is about parallelizing computations on the heart and soul of computer science data structures, the tree. You're going to learn a bunch of tricks for extracting parallelism from tree computations. These include, for example, taking the tree, converting it into a line, and then parallelizing operations on the line. Now at some point in this process, you and I are going to get a little bit stuck. So instead of trying to figure out what to do, we're going to reach for a classic trick in the lazy computer scientist's tool box. It's called randomization or more simply flipping coins. Lucky me. In addition to the video, there is also a reading. So be sure that you do both, and while you're busy doing that, I'm going to go off and feed the ducks which is probably illegal. And then I'm going to take a nap. Good luck to you and wake me when you are done. [MUSIC] Here's a little warm-up exercise to get you thinking about Parallel Algorithms on Trees. Consider the following tree. By the way, you can store this tree in an array pool style, just as you did with linked lists. In particular, let's start by numbering the nodes. Then let's store the tree by storing the parent of each node in an array. As a mnemonic device, I've converted these formerly undirected edges to directed edges. Later in this lesson, you'll see other representations. Now this array of parents is named P, and it has one entry per node. For example, node two points to node three, that corresponds to an entry of three at position two of the parents array. Let me fill in the rest for you. Now let's say you want to find the root of this tree. Here's an easy Sequential Algorithm. Start by picking any node. For the sake of example, let's pick node one. Then follow the parent pointers until you reach the root. The root will be a node with no parent. Let's quickly trace this algorithm's execution. First, you'll look up the parent of one, which is eight. Since eight is not equal to null or zero, you'll move up the tree. Then repeat. In this case, the algorithm stops when it hits three, which has no parent. Now the running time of this algorithm is linear in the number of nodes. Why? In the worst case, you might have a very unbalanced tree. One might be a the leaf and you'd have to march up the entire tree in order to get to root. So, how do you do this in parallel? Well, you could explore all nodes simultaneously. That is, for every node, change it's parent, to point to it's grandparent, if one exists. Does this process sound familiar? [SOUND] I'm back. In these example, nodes one, five, six, seven, and four all have grandparents. For example the grandparents of seven and four are both one. So according to this idea, you would visit seven and four in parallel at the same time and you would rewrite their parent pointers to point to the grandparent. Now if a node has no grandparent then it must be pointing to the root, so you'd leave it alone. Here, neither nodes two or node eight have grandparents. So you would repeat this test and jump process on all nodes yielding this tree. Notice that after step one nodes one, two, five and eight all point to the root which is node three. Since the remaining nodes still have grandparents, you'd need to repeat this process. And by the way, you may be asking yourself, why can't I just terminate the search right away? Why indeed? Anyway, if you keep going, eventually every node will point to its own route. Here's some pseudo code summarizing this procedure. The pseudo code will use two auxiliary routines. The first one is called hasGrandparent. HasGrandparent test whether a given node, K, has a grandparent give the set of all grandparents P and it returns a true or false value. Take a minute to parse the condition. The second helper routine is adopt. Adopt sweeps over the nodes in parallel. If a given node has a grandparent, then it records the grandparent's ID otherwise, he records just the parents ID. Adopt essentially implements this pointer jumping step. Now giving these helper routines, the algorithm is very concise to state. It double buffers the parents array. It's outer loop is sequential and taking it over all the possible levels. The maximum number, which is log n. Now think about that last statement. Does it make sense why? At each level it performs this adoption procedure. Then before moving on to the next level, the algorithm just needs to update current by next. Now in a practical implementation you would probably just swap these rather than copying them. The final values are the roots and that's what's returned in R. Okay, quiz time. Recall the parallel algorithm for finding the roots of a tree. The key idea was to look for the root from all nodes, simultaneously. That's hidden in the adopt call. Now consider the following claims about this algorithm. Look at each of these claims and check off the ones that you think apply. Notice that I've left another box, in case you think of anything else you want to say. The box is just an ungraded free response. Here are the claims that are definitely true. The adoption step is pointer jumping. Wee! However, the method is not work optimal. This outer loop of log n iterations is the root of the problem. No pun intended. That pun was obviously intended. What about the span? Well the span is polylogarithmic. This outer sequential loop is polylogrythmic and the parallel four loops of adopt are polylogrythmic. Now what about this claim, works on a forest, not just one tree. That is in fact a feature of the algorithm. An array pool might contain a forest rather than a single tree. So this procedure would make every node point to the root of it's own tree. That's kind of neat. Recall the list ranking problem. You're given a link list. You want to compute the ranks of every node. The ranks you'll remember are the distances of every node from some endpoint, say the head. Now one idea is to apply a scan or a prefix sum to this problem. How does that work? You start by assigning a value of zero to the head and ones every where else. You then use parallel pointer jumping to combine these values yielding the ranks. This idea, by this way, is due to this fellow, I'm going to call him Wyllie, because it makes me think of coyotes. [SOUND] [SOUND] Now recall the cost of this scheme. Although it was very simple and elegant, you will notice that it's not work-optimal. But there's at least one trick to making it so. You'll need this trick when parallelizing tree algorithms. So I want you to think about it through this quiz. Here's the trick. Start by shrinking the list. That is, let's say you start with a list of size n. Suppose you can magically compress this list into an equivalent one which has m less than n nodes. Then run Wylie's algorithm, but on this smaller list, instead of the original one. Since you're running on the smaller list, this would have a work cost of m log m. Further suppose that it's possible to take the results of this intermediate step, and then extend to a ranking of the full list. At long last, here is my question. The question is, how should you choose m? For the moment, just ignore steps 1 and 3, that is assume step 2 dominates the overall work. Choose the best option for m among this list. Here's the answer, n over log n. Let's go through the choices. Both log n and square root of n are impossible, why? Because then m log m would be asymptotically less than n, that can't be right. You need at least n steps just to walk the list. Similarly n log n and n squared don't work. Those give you sub-optimal algorithms whereas you want a work optimal one. So by process of elimination, n/log^n is probably the best answer. But you can also see this explicitly just by plugging it into m log m. If you have a list that you want to shrink in parallel, a really handy trick is to use something called an independent set. Consider a linked list like this one. For each node i, let N[i] = i's successor. For this example here is what N would look like. Okay. So what's an independent set? Given a list, an independent set is a subset of the vertices. Such that any vertex that's in the set does not also have it's successor in the set. For example, here's an independent set. Notice how none of the successors of 3, 7 or 8 have neighbors that are also in the set. What about this example? This set corresponds to these vertices. Notice that 8 is a successor of 6, so this is set is not an independent set. Now computing an independent set sequentially is super easy. Start with an empty independent set, then traverse the list from head to tail, assigning every other node to i. For instance, you could put the head in i, then skip 2, instead move to 7 and add it to the independent set. Then you'd skip 1, and so on and so forth. Okay, that was easy, so what's the big deal? Computing an independent set in parallel is a little bit trickier. Let's see why. Consider a list. Let's take any vertex in that list, let's say i. Now suppose you're processing all the vertices inside a parallel for loop. So how does any vertex know whether or not it should go into the independent set? This problem is sometimes referred to as the problem of symmetry. Basically all nodes look the same. Now what you need is a scheme. A scheme to break the symmetry. Ooh, a scheme. How delightfully sinister. A scheme. Ooh a scheme. Evil. Evil. Here is one way to do it which is based on a gamble. At every node in parallel, flip an unbiased coin. Each node will get a head with probability one-half and a tail otherwise. Let's adopt the convention that any node with a heads is a candidate to be placed in the independent set. Any tail will be left out of the independent set. Now this scheme has a slight snag. It's possible that both a given node and its neighbor are both heads. Suppose a node has a head and sees that its neighbor has a head. Then what the node will do is change its head into a tail. So with this tweak, you can now see that any head is adjacent only to a tail. Therefore, if you now take all the vertices with heads, that will be a valid independent set. Woot. So to summarize, here is the randomized parallel independent set scheme. You're given a set of neighbors, and you want to produce an independent set. And of course you want to do this in parallel, so you need some space for coins. Now, it will also be helpful to have a copy. You'll see why, but basically you need to change some coins and you want to do that in parallel safely. First take each vertex in parallel, then flip a coin at each vertex. You'll get either a head or a tail. And stash a copy for later use. That's step one. Here's step two. Again visit each vertex in parallel. At each vertex, check whether you're in a double heads situation. If so, replace a head with a tail. The last step is just to collect the list of vertices that have heads. You'll recall that you can use a gather If to do that. That's it. In just a moment you'll analyze this scheme. But first, take a look at a pseudocode and just make sure you understand what it's doing. Let's see if you're paying attention. Consider this list. Suppose the initial coin toss produces the following result. If you finish running the independent set scheme, which nodes will end up in the independent set? Type the list of nodes in this box. Here's the answer. The only place where you have consecutive heads is at the end of the list. The only tricky bit to remember is that you have to run the test for double heads at every node in the vertex simultaneously. So, both 5 and 6 will detect double heads. Therefore, both 5 and 6 will flip to tails. That leaves just three nodes with heads. Hence, the answer of 1, 4 and 8. Quick! I want you to analyze the randomized parallel independent set scheme. My question is what's the work and what's the span of this algorithm? Type your answers symbolically as a function of N here. Here is the answer. The work is linear. The loops all go from 1 to n, and the amount of work per iteration is constant. Now the span is log n. That follows from our usual convention. A parallel for-loop with a constant amount of work per iteration has logarithmic span. I've also asked the grading auto box to accept O of 1 time. I want you to derive another fact about the randomized parallel independent set algorithm. On average, how many nodes end up in the independent set? Here are your answer choices. Pick one. The answer I was looking for is n over 4. Remember, from the perspective of each node, there are four equally likely possibilities after the initial flip. You then apply a correction for the case of double heads. That leaves just one out of four options with heads. Now you might be asking yourself, self, what about those other heads? Hmm, good question. What are you waiting for me to tell you? Do I really have to tell you everything? Recall our basic sketch of a work optimal list ranking algorithm. The idea is to first shrink the list, then run the sub optimal list ranking algorithm, and then expand the list back to the original. The key step is shrinking the list. So how do you shrink the list? Let's use Parallel Independent Set. This process is similar to Pointer Jumping except you'll only jump over the independent set elements. Let me sketch this idea by example, first, let's maintain a list of ranks. Initially, let's assign a temporary rank of zero to the Head and one to each of the other nodes. To shrink the list, first identify an independent set. Remember the two steps. The first is flipping coins. The second is removing double heads. Here's the independent set. Now remove it. Removing it entails removing the vertices for one and eight. It also means rewiring seven to point to three. That should remind you of pointer jumping. Let's start by removing four in the four two edge. In addition to removing four, you should push it's temporary rank to the neighbor two. In this case, the temporary rank is zero, so two's rank will remain the same. Remember, that you did the same operation in pointer jumping for list ranking. So that's it for vertex four. Now, let's move to vertex one. You should push its temporary rank of one to its neighbor three. Now let's look at the jump from seven to three. Jumping means updating seven's next pointer. These are the basic steps. You do them over and over until you've removed the entire independent set. Now you might have to repeat the independent set process because remember. Your goal is to get to a list that has a size of n/log n. For example, let's say you haven't gotten there yet. So you'd flip coins again. In this case, five and seven are in the independent set so we need to remove them. Let's start with seven which requires a jump from two to three. So here's the jump. So we should also push sevens temporary rank to three, and you repeat the process for vertex five. Now let's suppose that this point the list has the right size. Then we can run list scan on it. The temporary ranks or the ranks to scan, so running the scan gives you the following result. Now let's compare these computer ranks to what they're supposed to be, based on the original lists. They're exactly what they need to be. Neat. Now what about these other ranks? You basically need to run the process that you just ran to contract the list in reverse. It's not hard, but it's a lot of bookkeeping, so for the sake of clarity, let me skip those details and refer you to the downloadable section for my detailed notes and pseudocode. Aside from its bookkeeping details, here's the bigger question about work-optimal list scan. How many times do you need to run the parallel randomized independent set in order to shrink the list? And when I say shrink the list, I mean shrink the list to the target size which is n over log n. Here are your answer choices. I want you to choose the best one. The answer is log, log n. Here's a quick and dirty analysis. Remember that each time you run independent set, you expect to pick about and over four notes. That means that the length of the list will be about three-fourths N on average. Now suppose you run parallel independent set k times. Then the expected length of the list, after k calls, will be three fourths to the k, times n. And remember your target list length. It's n over log n. So to determine the number of times you need to call parallel independent set, just solve for k in this inequality. What you should get in log, log n. Uh-oh! This result means that I lied a little. We were hoping to arrive at a Work-Optimal Scheme, but we're actually off by a little bit. The good news is that this log, log n factor doesn't grow very quickly. For instance, when n is 10 to the 18th, log log n is about 4 or so. In other words, you only need to run parallel independent set a few times. That's terrific news. Woot! Now there are a couple other details to check. The first one has to do with the fact that this was kind of a rough, average-case analysis. That's not entirely satisfactory. You don't only want to know what happens on average. There's going to be some distribution around the average. You want to know, is that distribution skinny or is it fat? And how much mass is in the tail? So that's one detail. The other detail is how much bookkeeping is necessary to implement steps one and three? I'm going to claim it's doable, but you shouldn't believe me unless you can work it out yourself. Before going parallel, let's review a very common CS101 algorithm on trees. It will seem very sequential at first blush. Why, I do declare I find sequential algorithms so embarrassing. Now suppose you want to compute a post-order numbering of the nodes of this tree. The classic procedure is recursive. You're given a root. You want to assign each of the nodes of the tree a unique value starting at a given value, V zero. You maintain the last assigned number. I've shown that here as the variable V. The last assigned number is what you'll eventually update, assign to the root, and return. But before you do all that, you'll visit and number all of the descendants. Each recursive descent returns the last assigned value. The next assigned value is the last one, plus one, which you then overwrite in V. So let's trace the execution of this routine on this sample tree. We'll start with the root which happens to be numbered zero, and we'll assign values starting at the initial value of zero. The first part of the algorithm will descend to, say, leaf number four. This descent carries the initial value of zero all the way to that point. Since node 4 had no children, it gets the initial value. It also returns that value. The parent caller, which is node 1, will receive 0. The next child will get the next initial value, which is 0 + 1 = 1. So continuing, you should get this final numbering. So how do you parallelize the scheme? On the one hand, it looks sequential. It looks like you have to number all of the nodes' children before you can number a node itself. On the other hand, doesn't this look and smell a little bit like list ranking or list scan? Quack. If only you had an efficient way to convert a tree to a list. Time for a little quiz which I hope will be an easy refresher. Now consider this post order numbering of the tree. What if instead of post order, I asked you to number the nodes of this tree according to a preorder traversal? Well, I'm asking. So hop to it. To help you get started, I've gone ahead and numbered the root for you. And if you've forgotten what a preorder traversal is, feel free to look it up. Here's the answer. Recall what a preorder traversal looks like. It numbers the root first before descending. Now with respect to the pseudocode, there's actually a slight ambiguity. It doesn't say in what order to visit the children. So that means the following answer is also valid. Anyway, I hope the grading goblin accepted valid alternatives and rewarded you for thinking outside the left to right, right to left box. Okay, it's now time to take this show on the road by taking an Euler tour. You know I'm bad, I'm bad, you know it! Oh! In your life, or maybe in this lesson, you've observed that some tree traversal tasks, like computing pre and post-order numberings, look sequential. Yet, they also seem eerily like list ranking. What you need is a way to view a tree as a list. Let's start with the following observation. You can take every undirected edge of this tree and represent it by a pair of directed edges. Now, notice that at every node, the number of incoming edges equals the number of outgoing edges. For example, look at node 1. It has four incoming edges, it also has four outgoing edges. This fact makes this particular graph Eulerian. Yee-ha! That's Eulerian with an E-U, as in Leonhard Euler, the famous Swiss mathematician and physicist. Now for any Eulerian graph, there exists a directed circuit. A directed circuit is a round trip path that goes through every edge exactly once. I hope you can easily see the Euler circuit in this example. Now the point of all this is that this circuit gives you a linked list. So how does this help you do something like, I don't know, compute a post-order numbering? Let's try to do a scan, or a prefix sum, on this list with a very clever choice of initial values. First, start by recalling the post-order pseudo code. In the recursive call, it just passes along the current value of v, upon return it adds 1. Aha, so what if you did the following? First, assign a 0 to the head node. Then consider any sink node that's a child. This sink is an example, it's the end point of a parent-to-child edge. So go find all the sinks and mark them with a 0 also. Now what about the remaining list nodes? Notice that they're all sinks for edges that go from children to parents. That corresponds essentially to returns from the recursive call. So at all the remaining nodes, let's put the value of 1. Now notice what happens when you do a scan along this circuit starting at the root. Take a closer look at all of the child-to-parent sources. Together with the final end point, these values are exactly the post-order numberings. That's the basic idea behind the so-called Euler tour technique. Here's a summary of the technique. Again, step 1 is to take the tree and convert it to a list. The list is the Euler circuit. You then mark the nodes of the list, and these are operations that can be done in constant time. Lastly, you apply a list scan or list prefix sum. Suppose you apply the Euler Tour technique to the post order numbering problem. And lets further suppose that the list scan or list prefix sum that you use is work optimal. Under that assumption, it turns out that the overall work of an Euler Tour is linear, but what about the span? If you just look at step three, its span is logarithm make an end. But does that mean the overall Euler tour approach will also have logarithmic span? Or do you think the span should somehow depend on the shape of the tree? Consider the claim that the overall span or depth of this algorithm is log n. True or false? The answer is true. You should pause for a moment to savour this fact which is really cool. It's likely that your intuition from sequential algorithms tells you that the complexity of a tree algorithm somehow depends on the height of the tree. So, that would lead you to believe that the claim is false. The tree after all could be very unbalanced like a linear chain. But the whole point of the Euler tour technique is to turn the tree into a list. Once you've done that, the tree-shape doesn't matter anymore. Jeez-o-pete. Of course, all this hinges on being able to convert a computation into an equivalent Euler tour computation, which is not always possible. Here's an example tree. Suppose you wish to compute the depth or the level of every node in the tree. The level of any tree node is the minimum distance to that node from the root. For example, here are the levels of the nodes of this tree. The root is a distance 0 away from itself. Its children are all one away. Its grandchildren are all two away and so on. Now suppose to compute the levels, you decide to apply the Euler tour technique. Recall that the first step is to compute an Euler circuit. That gives you a list, which is effectively a traversal of the tree. Here's what I want you to do. I want you to initialize the nodes of the list. So that a listed scan computes the level for each node. You'll put your initial values in these boxes, and I filled in a 0 for you to help you get started. Now I want you to choose the values, so that when you do a list scan. The level of every node ends up in the starred boxes. The starred boxes are exactly the source nodes for child to parent edges. What if you did the following? Consider each sink. If the sink is at the end of an edge that goes from a parent to a child, then that means you're descending into the tree. So, let's put a +1 at those sinks. Intuitively, the +1 signal an increase in the level count. The remaining sinks terminate edges that go from a child to a parent. Those nodes correspond to decreasing the level. So let's place -1 values at them. To see if this works, let's run a list scan. Notice that as you descend, you pick up +1 values. And as you ascend, you pick up -1 values. So now to get the final result, we just need to keep all the starred nodes. If you do that, this is what you should get. Conceptually, an Euler Tour is fairly simple, but there's some important details. For example, how should you store the tree, and how should you compute the tour? Here's one scheme. Start with a version of the tree in which you represent each undirected edge by a pair of directed edges. For each node v, we'll define its adjacency list to be the set of its outgoing neighbors. Here, d sub v is the number of v's outgoing neighbors. For example, here's the full set of adjacency lists or the adjacency table representation of this tree. Take vertex 1. It points to 4, 5, 6 and 0. Its out-degree is 4. Its entry in the adjacency list or adjacency table has these four neighbors as well. D sub v is 4 for this vertex. Now to compute the Euler Tour, we're going to define a special function. That function is called the Successor function. Here's its formal definition. Given an edge that goes from ui to v, the Successor functions returns the next neighbor in v's adjacency list. Notice the modulo operation. That operation effectively makes the adjacency list circular. Let's look at an example. Consider the 0, 1 edge. Let's apply the Successor function to it. Recall the definition of the Successor. It tells you that applying s to 0,1 is the same as this. So what will the next edge be? That's right, it will be v, u0. In other words, the 1,4 edge. To make sure you've got the hang of it, let's do it once more. Apply s to 1,4. Remember the definition of s. This gets you to the edge, returning 2,1 from 4. If you keep going, you'll end up traversing the entire Euler circuit. A good exercise is to prove this fact. The key observation is that this successor function flips the relative positions of u and v. If you get stuck trying to convince yourself of this fact, try heading to the online forums and see if your peers can help you out. Now one important implementation detail remains. Is the cost of applying the Successor function constant? Because if it's not, then you can't claim that an Euler Tour is possible in linear work. Here's the problem that should give you some doubts. Suppose you recently visited the 1,6 edge. You want to apply the Successor function. That means you need a fast way to jump from here to here. Otherwise, you can't apply the Successor. An easy way to do this is to augment the data structure with some extra pointers. In fact, you can do that cheaply when you're creating the adjacency list data structure. If you insert all these cross-edges, you'll get something that looks like this. Egads, what a mess! But anyway, I hope you're convinced that it's at least doable. Let's verify that you will understand the definition of successor. My question is, what is s(s(s(6,8)))? I want you to type your answer here. For example, suppose you think the answer is the edge that goes from one to four. Then you type something like this into the box. Any of these forms are acceptable. Here's the answer. It's the (1,0) edge. Consider the first application of s(6.8). Apply the definition of s. This will return (8,6). Apply the definition of s again. The result is the (6,1) edge. Now just one more time and then you're done. Now you have some useful parallel tree algorithms. One is built on top of work optimal lists. And the other on the rate compressed framework for evaluating expression trees. Now before wrapping up the lesson, I wanted to point out one of the neat ideas common to these methods, which is the idea of linearizing the tree. Why is that cool? Well linearizing helps you achieve load balance which is what you need to make a practical parallel algorithm scale. As an analogy imagine that I gave you a pretzel and I asked you to cut it up into say seven equal pieces. Like leaving the house and forgetting to put on your pants, it's awkward. But if you unroll the pretzel back into a line, it becomes much easier to measure out into equal sized parts. Though I know what you're thinking, it will be a lot more fun to just eat the pretzel,um. That is delicious pretzel. I doesn't taste like rubber at all. One of the most important abstract data structures today is a graph. Graph analysis is everywhere. Every day, people are analyzing physical structures, like buildings and bridges, electrical circuits, social networks, just to name a few. Now, this lesson introduces parallel graph algorithms for one especially important primitive. It's called a a breadth first search, or BFS for short. [SOUND] That's breadth as in wide. This kernel is so important that there's even a ranking of the world's supercomputers based on how quickly they can do a BFS. Now there's another lesson to look at supercomputer scale algorithms for BFS. But in this one you'll consider a practical algorithm for the Dynamic Multithreading Model. So with that quick intro, let's go explore! Given a graph and a source vertex S, what's the distance of all other vertices of the graph from S? To find out, you can perform a breadth-first search or a BFS for short. Here's how you might do it. For each vertex, we'll keep a shortest distance. Now, the source is a distance of 0 edges away from itself, so let's set its distance to 0. Now for all the rest, you don't know any of the distances yet, so let's set them all to infinity. Next, start the source and look at its immediate neighbors. These neighbors are one away from the source, so let's update their distances to be one. Now, let's visit the neighbor's neighbors. Now, most of these are not yet visited, so they must be one more away from the previous nodes we visited. But one of these, we've seen already, so that means we already know its shortest distance, and so we can leave that one alone. And you'll continue this process until you've seen all the vertices. The way a BFS works is a little bit like dropping a drop of water into a bucket. The drop hits the source, and that causes a ripple of waves through the graph where we propagate the differences. These waves are sometimes called frontiers. Here's a pseudocode for a sequential algorithm that implements the scheme we just saw. Let's write down some pseudocode for a sequential BFS algorithm. This algorithm will take as input a graph G represented by its vertices and edges. As well as a source vertex S. What it will produce is a map of distances D where D of a vertex X is the distance from S to X. Now initially, we don't know any of the distances. So we'll initialize D to be infinity for all vertices except S. S is just zero edges away from itself. Notice that according to this initialization convention, if there is no path from S to X, then we'll get an infinity at D of X. Now the algorithm will also maintain a Q F of unvisited vertices. Initially, F just contains the source s. Now as long as F is not empty, the algorithm does the following. It first extracts an unvisited vertex v. It then loops over all of v's neighbors. Let's call a given neighbor w. If w does not have a distance yet, meaning its distance is set to infinity, then, we will update it's distance to be the distance of v plus one since w is just one edge away from v. In addition, if it didn't have a distance yet, then it wasn't visited. Therefore, we should throw w into the visited queue. In this example, I have given this vertex a label of a and I have thrown it into f. Now let's consider the next neighbor of the current vertex V. It's also unvisited. So we repeat this process of updating its distance and adding it to F. Now would be a good time for you to pause and continue to trace the algorithm, to see how it works and convince yourself that it's correct. Now what is the cost of this algorithm? Since you traced it's execution as I suggested, then you observe that it only inserts a vertex into F if it has not yet been visited. Therefore the number of iterations of the while loop shouldn't be more than the number of vertices or the size of V. And since each vertex appears in F at most once, we'll visit each edge at most once if the graph is directed, or twice if the graph is undirected. That means that the body of the innermost for loop will be executed a total number of times that is at most big O of the number of edges. In other words, the total cost of the algorithm is big O of the size of v plus the size of E. Again, if you trace through the algorithm yourself on a sample graph, you'll see how this happens. Let's check that you understand how a sequential BFS works by having you simulate its execution. Start by recalling the sequential BFS algorithm. Let's say you run it on the following input graph. The vertices are labeled a through f, except for one vertex which is the source labeled s. For this quiz, assume that the graph is directed. Thus, when you hit array over the edges of a particular vertex v, you'll look at the outgoing edges. Let's take an example. The vertex F has one outgoing edge from f to s. So if v were equal to f in this example, then the loop would only visit the edge f goes to s. Here's my question. Let's call this particular line of code L1. That is the line L1 tests if F is not empty before deciding whether to continue or to break out of the loop. So on the third execution of L1, meaning the third time you evaluate this condition, F not equal to the empty set, what does F contain? Your answer will be a list of vertices that you type into this box. You can just list the vertices by their labels separated by spaces. So for example suppose you thought the answer was the vertices a, b, and c. Then you type in the box a, b, and c separated by spaces. Now one more bit of clarification, there can be more than one correct solution because there's an inherent ambiguity in the algorithm. In this quiz, I want to force you to think about that. As stated in the question, there are actually multiple correct answers because of a harmless ambiguity in the algorithm. Let me give you one possible answer. It's the vertices c, d, and e. Let's see how I got that and where an ambiguity arises. Initially, the queue F contains just the source vertex S. So the first time you test whether F is non-empty, F will contain the vertex S. Now after extracting S, we visit the neighbors. In this case, the neighbors are just B, C, D and E. Notice that they're all unvisited. So the algorithm will update their distances and then insert them into F. The ambiguity is the order in which the vertices are updated and inserted. That will affect the order in which we later extract them. Now from a correctness point of view that doesn't actually matter, this ordering. Now let's just walk through an example. Suppose we really did update and insert the vertices in the order shown, b, c, d and e. And assuming F is a FIFO, first in first out queue, then the next extraction will first pull out b. Now the vertex b has just one outgoing neighbor d. D has already been visited so in this iteration of the while loop nothing else will happen. That brings us to the third execution of the while condition. In that case F contains c, d, and e. Okay so that assumes that we inserted them as b, c, d, and e and extracted b first. What if we had instead extracted c rather than b? Then you should be able to convince yourself that the contents would be this instead. It's the vertices b, d, e and a. As it turns out, this ambiguity is a feature, not a bug. We'll, in fact, exploit this ambiguity in designing a parallel algorithm, which is the reason for this quiz. That is, the quiz is not designed to torture you. It's actually designed to teach you something. Imagine that. Okay so where's the parallelism? So one problem is F and this while loop. Notice that we pull out vertices and we put vertices back in. The vertex that we just put in depends on the vertex that we pulled out. So that appears to create a sequential bottle neck and since they while loop executes O of V times, then the span would appear to be O of V, as well. This makes the average available parallelism, essentially, a constant plus the ratio E divided by V. So, is that good or bad? In real life, graphs are sparse, and so what that means, is that we should expect the size of E to be O of V. That means the average available parallelism will be a constant and that's definitely bad. So this concept of sparsity is a fact of life. Think about a social network. The vast majority of people will have some maximum number of friends. Let's say, I don't know 1000, independent of how many people are in the network. Or imagine a worse case, which might be a linear graph that's shown on the right. This might represent, oh, I don't know, a bunch of hermaphroditic anti-social worms who only associate with their immediate offspring. Anyway, since sparsity's a fact of life, we're going to have to live with it and therefore the sequential algorithm is bad. So is there any hope of doing better? In fact, there's a big clue in the way we've described the algorithms so far. Remember that the BFS visits the graph in waves. Now this wavy nature of a BFS has two really important implications. First, an upper bound on the span shouldn't be the number of vertices. Instead, it ought to be the number of waves. Lets give each wave, which consists of all the vertices at the same distance from the source, a special name. Call them levels. So here's a question for you, what is an upper limit on the number of levels? One answer comes from a particular property of a graph, called it's diameter. The diameter is basically the maximum shortest distance between any pair of vertices. Now here's an important point about diameter. It's a property of the whole graph, so that makes it only an upper bound on the number of levels from any starting vertex. For example, I claim that this graph has a diameter of 5, and that's despite the fact that the shortest distance from S to any vertex is 4. Can you see why the diameter is 5? I smell a quiz. Now the process of visiting in a graph, level by level is sometimes called a level synchronous traversal, and that leads us to the second big idea. If you perform a level synchronous BFS, then it shouldn't matter in what order you visit the vertices of any given level. So if I take level two for example, the fact that I visit this node, before this node or this node, before this node. Really shouldn't matter. After all, they get the same distance. Let's be a little bit more precise. Imagine running a BFS, and stopping it at some level, call it L. Now let's look at any two unvisited neighbors of the set. Now these both have to be a distance of just one away from the set, so it doesn't matter which one we see first. In face, we can even look at them at the same time, because they'll both have the same value. Now, lets look at our example graph from before. Now, I claimed that this graph had a diameter of five. So there's gotta be a path in here with five edges. Can you find it? And what I want you to do, is mark the edges of some path of length five. So for example, the path from here to here, involves three edges. So if that was your answer, you'd mark the edges as shown. Here's one path from, say, this node, let's call it a, and this node, let's call it b. You should convince yourself that there's no shorter path between a and b. Okay, I think we're now ready to derive a parallel version of BFS. To summarize, let's recall the two big ideas. First, you should carry out a BFS level by level, not vertex by vertex. Secondly, you should process an entire level in parallel, so remember, why does this work? Essentially you perform the same action on every vertex of the level so the order in which you visit those vertices shouldn't matter. So here's a high level pseudocode that embodies these two big ideas. Now it looks almost exactly like the sequential algorithm but there are two really important differences. First, the algorithm is level synchronous. Notice that there's a level counter and there are these level-specific frontiers, F sub L and F sub L+1. Now since this loop is going level by level, the span, as defined by the while loop anyway, ought to be no more than the diameter of the graph. Now to implement the second idea, I've hidden everything in this function called process level. We're going to have to say more about how you actually implement this thing. But essentially what it does is it takes the graph and the current frontier, and it'll produce a new frontier. It'll also update the distances of course. Now to actually make all of this work, we have to say, what is the data structure that we can use for f? And for that, I'm going to tell you about a special data structure called a bag. Dun da da da, super bag. Okay, so what's a bag? A bag is a container having the following properties, and these properties will make it just perfect for holding the frontier in our parallel BFS. First, a bag will be unordered and will allow repetition. Remember that a BFS has to maintain a frontier, so it enables as much parallelism as possible, we should allow redundant insertions of the same vertex if need be. So here, for example, you can see that this bag has three copies of the element c, and that's okay, why? Because even if we process c at the same time, by say, different threads They should all get assigned the same level. Now the second thing a bag will allow is fast denumeration of its elements. Remember, the BFS has to traverse all the vertices in the frontier, so being able to do so quickly will be extremely helpful. Now, also remember that our best parallelization technique so far has been divide and conquer. So one really special operation on a bag is the idea of splitting it into two roughly equal pieces. And, of course, the inverse operation should also be fast, and that's union. Now union will also be special in that it will be logically associative. So what does that mean? Suppose I call this A and I call this B. Logical associativity means that A union B is exactly the same as B union A, at least logically even if that's not physically how we store things. Now logical associativity will also give us the property or the ability to apply reducer hyperobjects. Now to implement a bag we need the concept of a pennant. So what's that? Here's the formal definition. A pennant is a tree with two to the k nodes and a unary root and that root has a child. And the child is the root of a complete binary subtree. Okay, that was a lot of words, here's a picture. This pennant has a root and one child. And notice that the child is the parent of a complete binary subtree. Now a pennant has to have a power of two number of nodes. Now remember that a complete binary tree has to have almost a power of two number of nodes, in particular, 2 to the k minus 1. And therefore, when we add the root, we get a power of 2. Okay, now lets see how we can quickly do interesting operations on pennants. Let's call the root little x and let's call its complete binary sub-tree child, capital X sub s. Now suppose I give you a second pennant. This pennant has a root node, little y, and a child sub-tree, capital Y sub s. And let's further suppose that these pennants have exactly the same size even though I haven't really drawn them to scale. So how do you quickly combine these two pennants into a new pennant? Here's one way, let's choose one of the roots to be the new root of a combined pennant. Let's pick x for example. So let's chop X off and let's make it the root of the new pennant. Let's take the other root and let's make it the new only child of X. So that X is unary. Now since we have two complete subtrees left and the subtrees are both exactly the same size, I can make them both children of y and together they'll still be a compete binary sub tree. Now notice that this new blob is still a pennant. It's got a unary root and it's got a complete binary sub tree. Now both of the original pennants were size 2 to the k. So that means the combined pennant will have 2 to the k plus 2 to the k or 2 to the k plus 1 nodes. So combining two equal sized pennants into a new pennant is pretty quick and easy. Similarly, you can split a pennant into two smaller pennants and it will be just as easy, essentially by running the same steps in reverse and rearranging pointers accordingly. Okay, let's see if you understand the concept of pennants, and how pennants work. Here are two pennants, one rooted at a, and another rooted at e. Here's my question, if we try to combine these two pennants, what will the outcome be? Is it this thing, this thing, either of the above, or none? Choose one. So the answer is none. What we just tried to do is actually illegal. We've tried to combine two pennants of different sizes. And notice what happens in both of these first two choices. They both violate the complete binary sub-tree rule. So this was kind of a trick question. But the point was to reinforce an extremely important fact out pennants, which is that you cannot combine two pennants and obtain a new one unless the pennants have the same size. Okay, so pennants are really great for representing collections that are powers of two in size. But a bag needs to be able to contain any number of elements, so how can we use pennants in order to build bags? And it turns out a basic idea from computer science preschool is what is going to help us. First, I want you to remember a basic fact about binary arithmetic. In particular, you can represent any non-negative integer as a binary string. So, let's take an integer, I don't know, how about 23. How do you write this in binary? The answer is 10111. In base two. Now each bit position represents a distinct power of two, so let's number the bit positions. So this bit is 2 to the 0 which is equal to 1. The second bit is 2 to the 1 or 2 and we can go on and on and on. And notice that 16 plus 4 plus 2 plus 1 is 23. Okay. So, what does this have to do with bags? Here's a collection of 23 elements. And let's say I want to throw them in to a bag. How do I do it? Here's an idea! Let's use pennants. So, you can see what I've done here. In the same way that you can take the integer 23 and write it as a series or a sequence of powers of 2, I've taken the items in the collection and I've grouped them into powers of 2. Now I'm going to need to connect these in some way. So to do that, I'm going to use an array of pointers to the roots of each pennant. And I'll use a null pointer for the empty bit. Now, this array, I'm going to give a special name. I'm going to call it a spine. So just as a brief recap, I've taken my collection of elements. I've broken it up into these power of two sized pennants. And I've connected them together with this thing that I'm calling a spine Now this duality between a spine and binary arithmetic is the key to understanding how to perform all these operations on bags like insert, union and splitting. Let's start with binary math. Suppose you want to compute 23 + 1, but do it in base 2. Let's start by writing out 23 and 1 in binary. So here's what you did in computer science preschool. See 1 + 1 is 0, carry the 1, 1 + 1 is 0, carry the 1, 1 + 1 is 0, carry the 1. Finally, a little variety. 1 + 0 is 1 with no carry, and finally drop the 1. Let's see, so did we get this right? 1, 2, 4, 8, 16, 16 + 8 is 24. Voila. Now, if you want to put a single element into a bag, you're going to do the morally equivalent thing to binary addition. Let's see an example. Suppose you have a bag with 23 elements and you want to insert a new element. First, try to put it into the least significant bit of the spine. Rats, there's already a pennant there. Okay, no big deal. Let's create a new pennant of twice the size. The only gotcha now is that this pennant is in the wrong place. It's trying to occupy a slot along the spine for pennants of size 1. So, we'll need to carry it into the next slot and replace the current slot with a null pointer. Now let's repeat the process and try to insert this new pennant into slot one. Again, the slot's occupied so we gotta move on. Let's combine the 2 size 2 pennants into a size 4 pennant, carry it, and then 0 out the slot. And then you basically just keep going til the pennant that you're carrying eventually reaches a spot where you can insert it. So here's a question, what's the cost of doing this insertion? So what's the cost of insertion? I want you to figure it out. Suppose I give you a bag B as in bravo. Let's assume that the bag has n elements in it. Here's a new element to insert. The question is, what's the cost? Is it constant time, log time, linear time, or n log n time? Okay so the answer is log n. Why? Now notice when you insert an element you might need to traverse the entire length of the spine because you might have carries that take you all the way from 0 to the last bit. Now the spine has a length of ceiling of log n. If you don't remember why or you don't see why, just remember the duality with integers. An integer of size n needs log n bits to store it. Now each time we visit a slot on the spine we might need to combine two penants. But remember, combining penants is just a matter of reshuffling some pointers. Therefore, a single combine operation can be done in constant time. So we have an overall cost of of log n times constant time and it gives us an overall log n. So, that's pretty efficient. Lots of other kinds of data structures we know and love like heaps or balanced binary trees also have log and insertion costs. So, bag it baby. [SOUND] Ha ha, so just when you thought it was safe, it's time for another quiz. So suppose you have two bags. Here's my question. What's the running time to combine these two bags if each bag is of size n? Your choices are constant time, linear time, log time and log n time, or n squared time. Ooh, scary. The answer is log n again. Starting to see a pattern? So the easiest way to see this, is to remember the duality between bag operations and binary arithmetic. Well let's say I'm adding these two binary integers. I'll go bit by bit. Each bit I'll do some operation. And then I may or may not do a carry. And all those operations are basically constant time pennant combine operations. And of course, again I might have to scan the entire length of the bit string. Therefore, the cause should be logarithmic in the size of the bag. I want you to think about that for a second. That's a super cool result. Why? Because it basically says that it takes the same amount of asymptotic running time to insert n elements into a bag as it does to insert one. In fact, that means the amortize time to insert elements into a bag, is basically a constant. Okay, so we've talked about bag insert and bag union. But what about bag splitting? Remember, we need bag splitting because we want to implement efficient divide and conquer style algorithms very often. So how do we do it? Just to see if you were paying attention. Even before I show you how to do it, I want you to answer the question of what's the running time of bag splitting? And again to be a little bit more precise, a bag split means breaking it up into two roughly equal parts. Your answer choices are constant time, log time, n time, linear time I mean, or n log n time. Choose one. Okay, so if you've been paying attention then you'll randomly guess the correct answer is log n. The answer is a little bit involved. So let's move on and I'll explain how. Okay, so here's a bag we'd like to split into roughly two parts. How do we do it? Now so far, we've relied on using patterns from binary arithmetic to derive algorithms for bag operations. That begs the natural question, what's the right analogy for splitting in binary. So here's an idea. What about right bit shifting? After all, in binary a right bit shift is like dividing by two which would be a kind of split. Let's try it on our now favorite integer, 23. Shifting by one effectively chops off the last bit. And that will give us 1 0 1 1, and lo and behold, 1 0 1 1 is 11 in decimal and that's approximately 23 divided by two. Okay. Bur how do we actually apply this idea to bag splitting? Okay, let's see how this works on a simpler example. So here's a bag whose smallest pennant is of size 4, in other words, the 2 slot is full and 1 and 0 slots are empty. Now, to shift this by 1, or in other words, slide this somehow into the 1 slot, we're going to need to figure out how to chop up this pennant. Because the 1 slot can only hold a pennant of size 2. Now let's assume that we also have a spare bag which is initially empty. And we'll need this so that as we're breaking things up or breaking things apart, we have some place to put them. Let's start by splitting the pennant into halves. Now we can move one of these smaller halves into the 1 slot. This is now a legal operation. And let's sort the other half into our spare bag. Now in the worst case, we might have to move or touch every pennant. But remember that each pennant split only takes constant time. Therefore, the whole splitting should be possible in just log n steps. So again, that's log n splits, constant time per split, log n overall cost. Okay you now have all the pieces you need to finish the parallel algorithm. The only piece that's left is this process level. Remember that bag splits are very efficient. Therefore we should try to use our favorite scheme, divide and conquer. So here's some pseudo code that implements exactly a divide and conquer scheme. It's got two parts, first the input bag is big enough then we do divide and conquer. Using splitting. If the bag is not big enough, then we just fall back, essentially, to the sequential code. Now, the one difference is this parallel for loop. So let's see how that works. So again, given a bag at level L, the first step is to split it if the bag is big enough. Since we can process the elements in any order we can use divide and conquer on the two halves. And I said the base case was the same as the sequential algorithm, but I planned out this one difference. The difference is this neighbor loop. Notice that i'm iterating over the neighbors using a parallel four instead of a conventional four. Now that might give you pause because there might be tasks trying to update the same neighbor W, but since were using a level synchronize album all those updates are going to be doing this exact same thing which is writing the value of L plus 1 into the D of W. So even though there's a data race here, it's actually perfectly safe. Now the bag inserts are trickier. And for those, we're going to have to exploit the fact that bags are logically associative, and therefore we can use these reducer hyper objects that we talked about earlier. Okay, we're almost done. The last thing we need to do, is analyze the cost. Now, the cost analysis is really tricky, so if you want all of the gory details, I refer you to the paper by Lyzer-shin and Shettle, which we posted on the instructor's notes. For now, let's just sketch the analysis. First, I'm going to claim that the algorithm is work-optimal. If you want to go into details again, please see the paper. Now what about the span? The The span is effected by a couple different factors. The first factor is the number of levels, that's essentially the outer most while loop. Of course the while loop is not shown here that was the pseudo code that was on the left side of the screen earlier. The second factor is the span of process level. Now the first factor we said was bounded by the diameter of the graph. let's call it little d. The span of process level has three parts. There's the depth of the recursion, there's the cost of splitting and then there's the cost of the base case. Now the recursion will be logarithmic in the size of Fl. And since Fl can only be the total number of vertices, in the worst case the depth of recursion should be log in. The splitting we argue was log in, now what about the base case? Now in the base case, the size of Fl is a constant, because we have a constant cut-off built in as we usually do, therefore we can bound this by a constant. So let's put that all together. It will turn out that the cost is basically bounded by the diameter and something that's polylogrithmic in the size of the graph. And we love polylogrithmic, so we're done. Yay. [APPLAUSE] So what'd you think? I thought that was a really cute algorithm. And if you know anything about binomial heats, which a lot of data structures 101 classes cover, then you might of recognized the basic idea right away. And if not, you now have a new trick for your bag. The original paper on this approach is a very nice read. So I'd also encourage you to take a look at that. You're now ready to think about really big computations, computations where, for instance, the problem can't fit in the memory of a single computer. Or calculations that need so many operations, it would take hundreds of years to finish them using only a single computer. Now when the computation is really that big, you might ask, can I harness the collective power of many computers? But to design an efficient algorithm for such a machine, you need a suitable abstract machine model. And that's the goal for this lesson, for you to develop just such a model. This particular model and its variants go by many names, which you are bound to encounter as you read the literature on your own. These names include the Network Model, the Distributed Memory Model, the Message-Passing Model, the Communicating Sequential Processes Model and probably a zillion others that I've forgotten. Now personally, I like message-passing and for reasons you'll soon see, alpha beta. Now if any of that seems confusing, don't worry the basic abstraction is really simple. You have a bunch of computers collectively carrying out a computation. To coordinate, they communicate with one another by sending mass messages back and forth. This message-passing style stands in stark contrast to shared memory. There, processes or more likely threads, communicate by reading and writing shared variables. Now before you start, I think it's really important to think big. That's because distributed memory computations are all about solving problems that are really, really huge. How huge, you ask? Well, let's start with a quick warm up exercise to get you thinking about computations at the right scale. You may have seen a recent news story about a team of Japanese and German researchers who wrote a program to simulate the human brain. Brains! And in fact, their program didn't simulate the entire brain, just 1% of it over a short period simulated time. Now 1% of the brain is about 2 billion neurons and 10 trillion synapses. Very roughly speaking, neurons and synapses form a kind of network where neurons are like nodes, and synapses are like edges. The synapses serve as kinds of communication channels connecting the neurons. Now as it happened in the simulation, these synapses, because there are so many more of them, accounted for most of the storage and computation in the model. And in particular, the reason these researchers needed to run their simulation on a super computer is because every synapse required about 24 bytes of storage. I think you can imagine that 24 bytes times ten trillion is a lot of bytes. Now, suppose I gave you a computer work station with 16 gibibytes of main memory. If you don't know what a gibibytes is, please see the instructor's notes. Here's my question to you. How many of these 16 gibibytes machines would you need to store a full model? That is, all 100% of the brain. I want you to type your answer in this box, and please enter it to 2 significant figures. So, for example, if you thought the answer was 232.7, you would just write 230. I hope you found this calculation to be a no brainer. [NOISE] Okay, so here's the answer that I came up with. 1.4 million computers. So, how did I get that? Well, 10 trillion is ten to the 13th. That's the number of synapses. Each synapse needs 24 bytes. And that's for 1% of the brain, so to get 100% we multiply by 100 so that's the total storage required by the model. Now each workstation had 16 GiB of RAM, and 16 GiB, as it turns out, is 2 to the 34th bytes, so dividing this by 2 to the 34th, you should get something like 1.4 million machines. So, how many computers is that? A heck of a lot. The super computer with the largest number of compute nodes is probably Sequoia, which is an IBM machine at Lawrence Livermore lab. That's of June 2014. The sequoia machine has about 98,000 nodes. So what if we took a look at data centers. According to a 2012 analysis of Google's data centers using satellite maps. Someone with a lot of time on their hands estimated that all of Google's data centers might have about 1.8 million servers. Again that's of 2012. So, they probably have more. 1.8 million just barely big enough to hold the whole problem. Hey that's huge. To start designing algorithms for a cluster or a supercomputer, you're going to need a machine model. How about this one? In this model, a machine is a collection of nodes connected by some kind of network. Each node consists of a processor connected to a private memory. By private, I mean that the node can only directly read or write its own memory. It can't directly access the memory of other nodes. I'll refer to this type of machine as a Distributed Memory machine. Now this abstraction of private memories is critical. It implies that to share data, nodes will have to send messages to one another. So for example, suppose this node wants to send data to this node. The only way to do that is the sender or source has to package up a message and then put it on the network. So this message will have to find some path through the network to get from the source node to the destination node. In contrast to shared memory where we read and write shared variables, this style of parallel communication is called message-passing. Now this machine model has a few simple rules. The zeroth rule of the model is you never talk about the model. By that I mean, you need to master and internalize these rules. Okay, so what are the real rules? The first rule is that you should assume the network is fully connected. That means there's always a path from any node to any other node in the network. The second rule is that the network links are bi-directional. What does that mean? Well, suppose I have two nodes in the machine, and they're connected by a link. Bi-directional means that the link can carry a message in both directions at the same time. So, while one message is going this way, another message can be going this way. The third rule is that you will allow a node to concurrently perform at most, 1 send and 1 receive at a time. This rule is important because it effects the cost of communication. So for example suppose this node wants to send the message on each of its outgoing links. In order for this node to send four messages it's going to have to send them one at a time by contrast it could do one send and one receive simultaneously. The fourth rule is about the cost of a simultaneous send and receive of n words. So suppose this our computer, and let's further supposed that this node wants to send a message to this node. There are several different paths, here is one path and here's another. Regardless of which path the message takes, rule number four says the following. The time to send the message if it contains n words, is a constant alpha plus another constant beta times the size of the message. In other words, the cost of sending the message is linear in the message size. Now, it may seem strange that this cost, somehow, is independent of the path that's taken. Now, that's not entirely true. And I'll clarify a little bit later in the lesson. For the time being, let's accept the formula, as stated. Now, the formula has two terms. The first term is called the latency, and it has units of time. It's a fixed cost that you pay no matter how large the message is. The second term has a parameter beta. Beta has a name. We'll call it the inverse bandwidth. It has units of time per word. You'll think a little bit more about where this cost model really comes from momentarily, so just sit tight. One subtlety is, that this fourth rule really only applies when there are no messages competing for links. So we have a fifth rule which tells us what to do when messages are trying to go over the same link, at the same time. This situation is called congestion. This rule says if there are k messages simultaneously competing for a link, then in terms of the cost of the message, will change only the beta term and in particular we'll multiply it by k. For instance, let's consider this link that connects these two nodes. The last rule governs what happens if two messages try to go over the same link simultaneously. So suppose there's a red message going this way and a blue message going this way. The last rule says that the effective cost of this operation is the same as if the data transmission part, which is the beta term, is serialized over the link. So if these two messages are being transmitted in parallel Instead of observing a parallel execution time of alpha plus beta n, supposing that the messages are the same size n. What you'll see instead is a cost of alpha plus beta times n times 2. Okay, so take a second to look at these rules and commit them to memory. I admit that I pulled this alpha-beta model out of thin air. So you should be asking yourself, where does this model come from? Let's consider a linear or 1-D network having P nodes. Now suppose node 0 has a message of size n that it wants to transmit to the last node in the network. Let's say it does that by taking the message, breaking it up into n individual words and transmitting one word at a time. Now, let's assume this step of preparing the message for transmission costs you a fixed amount of time, little a. Then let's suppose that any time a word goes from one node to another that it incurs t units of time. My question to you is, what is the minimum time to send this n-word message? Type your answer here. Now your answer should be symbolic, so you should write it algebraically in terms of a, little t, n, and p. There's a little grading gnome running behind the scenes that will do its best to compare what you enter automatically against our proposed solution. Okay, how do we figure this out? Suppose you were sending a message of just one word instead of n. That is, suppose n=1. Then the time would be this fixed startup time plus the time to traverse all the links which is t(P-1). Now suppose instead of sending one word, we're actually sending two words. So we'd pay the startup time, little a, as before. Then the first word would hit the network and find its way to node 1. Eventually, it arrives. Then in the second step, word 1 continues to node 2, while the second word starts its way from 0 to node 1. Now this step and this step happen simultaneously, so we only see the cost of this step. And this continues. Eventually, the first word makes it all the way down the network to the destination. The second word always lags one step behind. So what does that mean? That means to send two words, we pay the same cost as sending one word, plus an additional T units of time for the second word to go the last hop. I think you can see how this would work for n = 3, n = 4, and so on. So for any n, there's the time to send the first word plus the time for the remaining words to arrive at the destination. Let me reorganize this algebraically into this form. Now this formula has three terms. There's one that depends on P, and there's one that depends on n. And then of course, there's the startup time. So let's interpret these. The first term is just the message startup overheard. What about the second term? In practice, the time t effectively measures the message transmission time along a link. So you can interpret the second term as a kind of wire delay in going across the network. And finally, the third term captures a dependence as a function of a message size. And as it happens, even when P is very large, let's say tens or hundreds of thousands, it's actually still small, or only comparable to little a. Little a is typically associated with a software overhead for preparing messages for delivery. In fact, a is typically many orders of magnitude larger than t. So we typically treat these two terms as being effectively a constant. That's what becomes alpha in our alpha-beta model. And then beta is something that's sort of proportional to one over t. Anyway, this is a really rough analysis. The textbook actually has a more detailed explanation. So head to the instructor's notes for our reference. My hope is that if Alpha-Beta seemed mysterious before, hopefully you have a little bit better of an intuition for it now. With the Alpha-Beta Model, an algorithm has three costs. The message cost defines the first two. It's essentially the latency, alpha, and the inverse bandwidth, beta. Now our algorithm will, of course, do other kinds of basic operations like comparisons and arithmetic. Let's denote the cost per compute operation by tau. Now in practice, it turns out the following is true. Tau is typically much less than beta, which is in turn much less than alpha. To give you an idea, you should be thinking of something like 10 to the minus 12 seconds, versus 10 to the minus 9 seconds, versus 10 to the minus 6 seconds. So something like three orders of magnitude in between. Let's suppose you buy this. Here's my question. Which of the following claims is true? And I want you to check all that apply. Is it true that computation is less than communication, so you should avoid communication? Or is it true that it's faster to send a few large messages, rather than many small messages? Or is neither of these statements true? The answer is that both the first and the second statements are true. Computation is much less than communication, so you should probably avoid communication. It's an unfortunate fact of life that many computer scientists are very good at this. The second statement is essentially about the relationship between beta and alpha. So think about that. According to this, in looking at this formula, it's evident that you can deliver something like on the order of, I don't know, 1,000 words or bytes in the same time that it takes just to start a message. So, that suggests it would be better to send a few large messages rather than many small messages. Recall the five rules of message passing and consider the following scenario. Suppose I have a linear network of eight nodes. So suppose that node zero wants to send a message to node two, at exactly the same time that node six wants to send a message to node three. How much time will it take for these two simultaneous messages to transpire? Here's the path from node zero to node two and here's the path from node six to node three. Notice that these messages occupy independent paths in the network, therefore, they can happen simultaneously. So if the messages are both of size n then the time is just alpha plus beta times n. Now I want you to try. Recall the five rules of message passing and consider an example on the following linear network which has eight nodes. Suppose one wants to send a message to node six at exactly the same time node seven wants to send a message to node four. What's the minimum time it will take for these two messages? I want you to type your answer in this box. And I want you to do it symbolically. So for alpha use little a, for beta use little b, and for n you can just use n. Let's start with an observation. 1 going to 6 occupies this path, whereas 7 going to 4 occupies this path. Now the two paths overlap, but they're going in opposite directions. So by rule number 2, which is the existence of bidirectional links, these messages can be carried at the same time. So you should have written something like a + b x n. Recall the five rules of message passing. I want you to think about the following scenarios on a linear network with eight nodes. Suppose node 1 wants to send a message to node 6 at the same time that node 4 wants to send a message to node 7. How much time will this take? I want you to type your answer in this box. You can use little a for alpha, little b for beta, and n. Let's start with an observation. Here are the two paths that these two messages are trying to take. Notice that the paths intersect and they're going in the same direction. That means the messages will need to serialize in their bandwidth term. And since there are two such messages, we'll take bandwidth term and multiply by two. This is essentially an application of rule 5, k-way congestion. Recall our five rules of message passing. Now, I want you to think about them in the following scenario. Consider this two dimensional mesh network of nine nodes arranged in a three by three grid. Suppose node zero wants to send a message to node eight. And suppose at the same time node four wants to send a message to node six. Here's my question. What is the minimum time to send n words? Type your answer here symbolically. You can use little a for alpha, little b for beta. And of course, n for itself. The answer is it depends. There's a deliberate ambiguity here. For example, suppose node zero sends to node eight via this path. If the second message takes this route, then the two paths intersect going in the same direction and there will be congestion at this link. If instead the second message takes this route, then there's no contention. So, in fact, if these are the only two pending messages, you can overlap them by choosing good routes. In genera,l for some analysis, you're going to need to reason about whether or not congestion or contention are occurring. We'll discuss some techniques to do so in a different lesson. Now the question asks for the minimum time, so if you assume optimistically the non-colliding paths, then you would get just a plus b times n. An important class of distributed memory algorithmic primitives are the so-called collective operations. No, not that kind of collective. One example of a collective operation is a reduction, which in the distributed memory case, I will call all-to-one reduce. The phrase all-to-one makes it clear that all nodes participate to produce a final result on one node. Let's take an example. Consider this linear network which I've numbered from 0 to 8. These ID, numbers I'll sometimes call ranks. So, for example, the rank of this node is 4. Now suppose each node has a single value in its private memory, let's say these values. In an all-to-one reduction, you want to produce a final result which is the reduction, in this case sum of all the values on a root node. So the sum of these values it turns out is 55, and let's say we want that result to appear on node 0. Now intuitively, you expect a tree base scheme like this one to work well. It's span is logarithmic in the size of the input. In this case, there are P values. Now, this observation suggests that you'll need to perform at least log P communication steps. Now, initially, the data is distributed across all nodes, so no node can do any actual work. Therefore, you'll need to do some communication. Okay, well that's easy. Let's have all the odd number nodes send their data to their leftmost neighbor. So 1 will send to 0, 3 will send to 2, and so on. None of these sends conflict so all the sends can happen simultaneously. So this round of communication will cost you alpha + beta time since you're only sending one word. After the communication, the even ranks add the incoming value. The new state of the system then looks like this. Now in this next step, you'll notice that only even numbered ranks have values. So notice the even ranks have partial sums. So we can repeat this scheme, except this time with odd partners of the remaining set sending to the closest even partner. So we might do these sends, for example. Again, there are no conflicts in the paths so this costs us alpha + beta time. And you'll just repeat this process until a final result remains. Now, to implement this scheme, I want to point out a useful fact. Suppose we replace the ranks by their equivalent binary strings. Recall that in the first round, the odds send to the evens. Notice the odds all have a 1 in their least significant bit, and the evens all have 0. In the next round, the only participating processes have a 0 in their least significant bit. And notice what happens. Nodes with a 1 in their second bit send to nodes who have a 0 in their second bit. So again, it's basically odds to evens where you drop off the last bit. Neat. If you've heard of things like hypercubes, binary reflected Gray codes, or Hamming distances, then this bit manipulation or bitview of the world will seem very familiar. And if you aren't, we'll give you some pointers in the instructor's notes. Shave and a haircut, two bits. I don't know what that shave and a haircut two bits thing is, but I got it from somewhere. Beyond the model, you need a pseudocode notation for formalizing algorithms. So let's start with our traditional sequential pseudocode notation and let's make the following changes. First we'll write our algorithms assuming a single-program, multiple data style. HPC nerds refer to this as S-P-M-D or SPMD style. The way SPMD works is as follows. First, you'll write some pseudocode algorithm. Then you'll imagine running that algorithm on some cluster, and you'll assume that that pseudocode is replicated on all the nodes. We'll call every running copy a process. Each process runs independently and asynchronously from the others in the absence of barriers or any other kind of explicit synchronization. Now, to distinguish the copies from one another, we'll assume that the pseudocode algorithm has access to two global variables. One called RANK and one called P. Since the memories are private, these variables are private to each running process. RANK will be the ID of the running process and it'll be unique. P will be the number of processes. Now for the moment, the concept of a process and the concept of a node are basically interchangeable. But in practice, a process virtualizes the concept of a node. That means you might have more than one process assigned to a node if, for example, you're running on a multi-socket, multi-core system. Now, to see how this works, suppose that this pseudocode algorithm contains a line of code which reads as follows. So when all the processes run, they'll all print statements of the following form. I am 0 out of 5, I'm 1 out of 5, and so on. They each print their own rank and the total. Here's a second change to a sequential pseudocode. I will give you a primitive which performs an asynchronous send. Think of it as an API call that looks like this. Now it has two arguments, one is a buffer of size n. The second is a destination rank, basically the rank of the process that's supposed to receive this message. Now, an important and subtle point about sendAsync is what does it mean when it returns? When it returns, it does not mean that the buffer has been sent. It just means that a send is registered with the system. So in particular, until you know what's happened you should not modify buf. To find out what happened, sendAsync will return a handle. You can do some testing on the handle. Now, for this send to eventually complete, the destination rank has to post an asynchronous receive. The signature looks the same. It names buffer of some size, and it names a source rank, source being the sender. Just like sendAsync, when recvAsync returns, it does not mean the data is available. Rather, recvAsync will return a handle and you should do some testing on the handle. This business about handles brings us to the final primitive which is called wait. In particular, wait is a blocking operation that takes one or more handles as arguments. Now, wait will pause until the corresponding operations complete. Now there's also a special form of wait called a wait all, abbreviated here by wait with an asterisk. This is a shorthand that says wait for all outstanding sends and receives so that we don't have to track and check all the handles all the time. Now here's a very important and subtle point about the completion of a wait. When wait returns, the only thing that you know is that buf is safe to reuse. If the handle is for a receive, then it must mean that the message was delivered. But what about send? For a send, buffer being available for use doesn't really tell you anything. For instance it could mean the message was delivered since the wave might actually wait until the message is delivered, but it doesn't have to mean that. In fact it might not even be the case that the message is even in flight yet. The precise action is implementation dependent. Implementation dependent. But why? The answer has to do with buffering implied by these primitives. Remember that the processes are executing asynchronously. Also remember that a send completes when the caller may reuse the message buffer. So let's imagine what might happen. Here are two ranks, i and j. Rank i is computing, and then decides to send a message to j. In the meantime, j is running off asynchronously for a long, long time before it decides, oh, I'm ready to receive a message from i. So the send could arrive way before the receive is actually ready. So should the send stop and wait until the receive has been posted before it starts delivering? In this scenario, rank i would have to wait a long, long time. What rank i might do instead is make a copy and then later on decide to start the delivery. That way rank i can kind of keep going. So by leaving the meaning of the completion of a send a little bit loose, you give more freedom to the implementation to make a good trade off. Okay, so what's the punchline of all this? Well, it means that when you're trying to determine if your algorithm is correct, meaning that a sent message is received, you need to prove that for every posted send, there is a matching receive. This protocol that sends, match, receives is called two-sided messaging. Two-sided suggests there might be a one sided, and in fact that's kind of what shared memory communication is. I'll let you think about that. Okay, time for a quiz. Suppose there are two processes, each of which executes the following operations. So, they each try to send messages and they each try to receive messages. The buffer for receiving is called text. Here's my question. What is the value of text when the processes complete? Does rank one's text have the value hello, while rank two has the value world? Or is it just the reverse? Or maybe it's either, or maybe it's neither. Or maybe the processes don't even complete. You decide. The answer is that the behavior is undefined. The processes don't complete. They both initiate sends and then wait for the send to complete. But remember, there's an ambiguity about what complete means in the case of a send. It might be that there's some copying going on, but it might be that the message is very large and the implementation has decided to wait for delivery. In the latter case, that's a problem because it means that neither process can ever execute its receive. The result is deadlock, each process is waiting for the other to do something, but no one does it. Consider six processes. Each of which has a buffer named x. X is large enough to hold a single value. The initial values of x are as shown. Suppose we run this pseudocode program. If this is the initial state, what is the final state of x on all the nodes? I want you to fill in these blanks with the final values. And if for some reason, you think a value is undefined, put in a question mark. So here's my solution, which is basically exactly the same values. To help you see that I've included a little trace, here on the left. This row is the set of initial values. This row is what happens after i = 0 completes. Basically all the values shift by one. because shifting by one is essentially what this funny indexing corresponds to. If you understand the SPMD model and the asynchronous communication primitives, then you're ready to write the pseudocode for an all-to-one reduction. So recall the problem and our algorithmic idea. There are p processes. Each process has a value, and we want to compute the global sum of these values, leaving the result on some root process, let's say process zero. Our algorithmic idea was to use a tree-based reduction. The algorithm proceeds in about log P rounds of communication. The rounds iterate over bit positions. In each round there's a current bit position. Processes with a one send to processes with a zero in that bit position. Okay so let's sketch some pseudocode. For simplicity, let's assume that P has a power of two, so we'll ignore this oddball. Let's have an outer loop that maintains a bitmask and we'll shift the bitmask after every round. The next part of the algorithm is to have pairs of processes communicate. The partner of any given rank differs only in the bitmask position, so this is an exclusive or. Remember that senders had a one in the current bit position, so if you're a sender, you need to send a message. So if S is our local value, then the send can just send that value to a partner and then wait. Once the send is complete, the sender drops out. Now if we're not a sender, then we might be a receiver. And if P is a power of two, it will turn out we must be a receiver. That case, we'll receive the value from our partner and then accumulate it. Now in this scheme, the final result will end up on rank zero. So for rank zero we can print the final result. This algorithm performs an all to one reduce leaving the final result on rank zero. And it works provided P is a power of two. That seems like an annoying restriction let's see if we can get rid of it. When P is not a power of two I claim you can patch the code by inserting something here. So what I want you to do is come up with a patch and stick it in there. Okay, here's my solution. I suggest that you check whether a partner is less than P, and you only post the receive if that's true. So if partner is less than P, you'll post the receive, and you'll continue on. If partner is not less than P, then you just won't do anything, but you'll keep executing the while loop. Now if you go back and you look at the animation, you'll see that in fact there was an oddball processor who kind of stuck around until the very end. And I claim this patch will give you that behavior. Let me put it a little differently. Start by considering the following two facts. First, only senders drop out. Second, senders always have a 1 at the current bitmask position. Let's think about that. Suppose the current bitmask looks like this. So it's 0 everywhere except 1 in the current position. The two facts imply the following. Any process that's still participating, meaning it hasn't dropped out yet, must have a bit string that looks like this. In particular, All the least significant bits will be 0. P xs are don't care bits. Here's another handy fact. A sender must have a 1 in the bitmask position, so its bitstring is going to look like this. Its partner will have a flipped bit, so the matching receive bitstring will look like this. This means that the sender's rank is always greater than the receiver's rank. And, of course, to be a valid sender, the rank has to be less than P. So a sender's rank is less than P. And a receiver's rank is less than the senders rank, which in turn will be less than P. So a candidate receiver who's partner has a rank greater than P can't be valid. Let's consider a slight generalization of a reduction called a vector reduction. In it, we'll assume that every node has not a single word of data, but a vector of data. A vector reduction will be the elementwise reduction of these vectors. So if we're doing a sum reduction, then we'll take vectors, and we'll sum them elementwise. The result will be a single vector of length n, whose elements are the sums of the corresponding elements from the other nodes. Now the corresponding algorithm would change only very slightly. In our scalar pseudocode, anywhere we sent a scalar, we could instead send the vector. Here's another question for you. What is the total communication cost of doing a vector reduction? Assume a linear network and our usual tree-based algorithm. You can also assume that the number of nodes is a power of two. I'll make this multiple choice with the following options, pick one. This is the answer. We're sending full vectors at every round, and we've got log P rounds of communication. So far, we've talked about all to one reduce. Start with a set of nodes on a linear network. Initially, each node has a vector of data. Performing a reduce produces the combined data on one of the nodes. This node that stores the result is called the root. Now a reduce has a natural partner. It's called a one-to-all broadcast, or just broadcast. In a broadcast, one processor has all the data initially. It wants to send a copy of this data to all other processes. So it's basically one process shouting in a room to all others. So in principal if you know an algorithm for reduction, then you also know an algorithm for broadcast. You basically run the reduction in reverse. Because of this relationship between Reduce and Broadcast, I'll refer to them as Duals of one another. Here's another useful collective operation. It's called a Scatter. In a Scatter, one process starts with a data. It then sends a piece of its data to each of the other processes. A scatter also has a natural dual. The dual is called a Gather. In a Gather, everyone has a piece of data, and one node collects it. How do you like my primitive cheese people? Another useful primitive is an All-gather. Initially each process has a little bit of data. After the All-gather, all processes have a copy of all of the data. And All-gather also has a natural dual called a Reduce-scatter. In a reduce-scatter, initially all processes contain a vector of data. They then globally reduce this data using some kind of combining operator like a vector addition or vector element-wise multiplication. The result is distributed to all processes. By contrast, recall that a normal reduction, an all to one reduction, leaves all of the results on just one process. I'll show you some techniques for implementing all of these collective operations efficiently. I learned these techniques from a fellow by the name of Robert van de Geijn. Once you've learned those methods, you can then simply invoke them as collective operations in your algorithms whenever you need them. In fact, to make it easy to use these as building blocks for your algorithms, let's define some pseudocode interfaces. Suppose every process has a private array of size n. To perform a vector reduce on all of those arrays you would just write this. Every process needs to call this reduce, supplying as input its private array, along with the ID of the root process. This operation will perform the reduction. It will leave the final output on the process whose rank is root. And that result will be in root's private array. Now, this operation is called a collective, meaning that your algorithm or program must be structured in such a way that all processes execute it. Let me show you what I mean. Here's an example of a program. Remember, all processes are executing this program concurrently. Notice that only the process whose rank is 0 will actually call the reduce. None of the other processes call reduce. So this program is actually invalid and its behavior would be essentially to deadlock. Put another way, this program hangs because rank zero starts to reduce but not other process does. So that leaves rank zero hanging. Here's a different example program. What is its output if there are say, P = 3 processes? Say initially we have 3 processes. Each one initializes its local private array with its RANK+1. Then all processes enter the reduce. Notice that the root is the process whose rank is 1. So the final result should appear on rank 1. So after the reduce, the result should look like this. A_local is unmodified on ranks 0 and ranks 2. The final output appears on the root. Okay, let's move on. For a broadcast, the dual of reduce, I want you to assume this signature. The process whose rank is root will have the initial data in its local buffer. When the broadcast completes, every process will have the same data in its local buffer. So for a gather, I want you to assume this signature. Remember that in a gather, everybody has a little piece of data and you want to collect all the data on one process. So every process has an input buffer of size little m. The output is all m buffers from all processes collected on the root. Notice that the output buffer is two-dimensional. Now the output buffer need only be defined on the root process. So what does that mean? That means in your algorithm, when everybody calls gather, output is only valid on root. In fact, you can assume that the other processes have no output buffer, unless the algorithm needs it for some reason. Now notice I've used the symbol little m instead of little n. The reason is so that when you do analysis, I want you to assume that our usual problem-sized variable, n, is defined to be m times p. That is the size of the combined output. This is basically just saying that we always want to assume that p divides n. Of course, in a real message passing library there will be ways to send variable amounts of data per process. Now the dual of the gather, or a scatter, looks essentially the same. The difference is that the input is of size m times P defined on the root. Finally, we come to allGather and reduce scatter. Here's allGather. Note that there's no concept of a root rank. The allGather essentially replicates the input buffer everywhere. Now reduceScatter is the same as an allGather just going in the other direction. So in particular, the dimensions on the input and output buffers are reversed relative to allGather. Now we've been using these two-dimensional objects, so I want to introduce one more useful primitive. It's called a reshape. It has two forms. One takes a two-dimensional input and produces a one-dimensional output. The other goes from 1-D to 2-D. This primitive is a purely logical operation, rather than a physical one. By that, I mean it doesn't actually copy a 2-D object to a 1-D array. It just says that it changes the interface of the object so that you can start viewing it and operating on it as if it were 1-D. In fact, if you're familiar with multi-dimensional arrays in languages like C and C++, you know a dense multi-dimensional array always maps to a linear address base anyway. As such you can assume it has constant cost. Just to be concrete, here's a picture of going from a logical 2-D representation to a logical 1-D representation. The convention I'll use linearizes the 2-D array column by column. This is sometimes called column major storage. Let's say you want to implement an all gather, and let's say you can only do so using these four primitives or reshapes. What I want you to do is give me a pseudo code algorithm for an all gather. Type your answer here. Here's one solution. It does a gather followed by a broadcast. And it uses reshape to make sure that the inputs can form to the right format. So, output is 2D as we can see here, but broadcast requires a 1D input. So, you can see why I wanted you to have some reshapes. Here's a picture of what the algorithm does. It first does a gather. It follows that with a broadcast. Now a good follow up question is, is this a good way to implement an all-gather? Hm, so many things to ponder, and so little time. Before we chat about ways to implement collectives, I want you to think about what our cost goals should be. For example, let's take reduce. We described a tree-based scheme. Is this good or bad? You showed that the cost was essentially this. Basically, the time to send a message of size n times log P. So put another way, you're sending log P messages and you're sending a copy of the array log P times. So is that good or bad? Let's first think about the fact that we're paying for log P messages. That's alpha times log P. Can we do better than that? At least on a linear network the answer is probably not. Let's see why. Remember that any process can only perform one send and one simultaneous receive during any round of communication. So this process can send and receive simultaneously. If this process wanted to perform a second send, it's not allowed to do that. The second send has to wait until the next round. So in the very best case, you can pair sends and receives, no matter how you choose to arrange the computation. And if you're pairing you'll need at least log P rounds of communication. Since the alpha terms essentially measures the number of rounds, then this says that log P rounds is good. So what about the beta term? Is beta times n times log P good or bad? Let's start with an intuitive and somewhat trivial lower bound. Each process has n words of data. So, at some point in time, every process except the root is going to have to send its n words somewhere. So, in the very best case, you'd need to send a total volume of n (P-1) words. Since there are (P-1) of these processes. If all (P-1) processes could send their data simultaneously, wouldn't that be great? The time we would pay, would be proportional to, the total volume divided by (P-1). So we need to pay to send this many words. And the speed of communication is one over beta. Therefore a lower bound on time with respect to beta would be just n times beta. Looks like our tree-based scheme might be sending too much data by a factor of log P. Now you can apply a similar line of reasoning essentially to all the other collectives. So for all the collectives we've discussed, the lower bound on communication is this. For collectives like scatter and gather remember that we use the symbol m, m denoted the local problem size. So in this lower bound, this size n is really the combined size. If we implement all-gather using the building block approach of a gather step followed by a broadcast step, is that good or bad? Well let's suppose that both gather and broadcast somehow attain the lower bounds. Will the combined primitive also attain the lower bounds? I want you to tell me. Yes or no. If we're being optimal in an asymptotic sense, the answer is yes. That is, a constant number of optimal operations is still optimal. Certainly simplifies our algorithm design and software engineering, if we can use some collectives to build the other collectives. Nice. Remember how Scatter works. A root has all the data, and it distributes that data to all the other processes. Suppose I give you the following scheme. The roots sends each piece of data, and all other p minus 1 processes receive. My question is, how much time does this take? Here are a bunch of options. I want you to choose the best one. That is, even if an option doesn't seem exact, just choose the asymptotically closest. Remember that a process can only do, at most, one simultaneous send and receive. In this case, the root is doing all of the sends. So the sends have to proceed one send at a time. That means you'll have to pay for at least P minus 1 sends. So then the best answer is this one, which is linear and P. I'm about to walk you through an efficient way to implement scatter as well as other collectives. I learned these techniques from a terrific colleague of mine, Robert Van Deguine. I can't do his presentation justice, but I'll do my best. So, here was my first implementation of scatter. It's fairly naive because the root does all the sending. That made the communication time linear in p. And that's sub-optimal. Remember that our goal was to find an alpha log p algorithm. Here's a different approach. Suppose this is the root. Let's start by logically dividing the network in two pieces. Then let's send the lower half of the data, and we'll just pick some process over here to the destination, let's say this one. So we'll pack up this half, and then we'll send it. Surprise, surprise, you are dividing and conquering. Did someone say conquering? I think you can imagine that you would continue this process. Voila. So what is the cost of this scheme? Pick any round of communication, little i. The rounds go from 1 to log p assuming p is a power of 2. Now in round i, this scheme sends a message of size n over 2 to the i. So the time for round i is alpha plus beta times n sub i. So we just need to substitute and then sum over all the rounds. Plug some stuff in. Simplify. So if you work it out, you should get something that looks like this. Notice how awesome you are. You've attained the lower bound, both with respect to latency and with respect to bandwidth. Nice job, you. In fact, at this point, I can let you teach the class while I retire to a beach community, sipping pina coladas and rolling naked in my wads of OMSCS cash. Now, since gather is a dual if scatter, running the same technique backwards, at least conceptually, gives you another good algorithm. Recall the tree based reduction where its dual or broadcast. Now, even though, it doesn't attain the lower bound on end, it doesn't mean it's no good. So, even though, this term depends on log p, it doesn't mean that it's a terrible algorithm. The question is, under what condition, is it still a reasonable algorithm? So I've listed a bunch of choices here, and I want you to check all that apply. Here is what I would say. Let me explain myself. One way to think about the question is to ask an analytical question. Mainly, when does the alpha term dominate the beta term? When that happens, then we might not care that the beta term depends on log p. Algebraically, this term dominating this term is the same as saying that it's much, much greater. You'll notice the log p's cancel. That leaves us with something like this, which basically says n is much less than the ratio of alpha to beta. So let's compare that to our options. The first option is algebraically equivalent. The fourth option, even though it's qualitative, kind of captures what this says. So this says that we want n to be small compared to something. So to me, this is kind of an acceptable intuitive answer. What about the other options? Option two has the relationship algebraically wrong. Option three is just kind of irrelevant, there's no dependence on p here. The last option is also a bit of a red herring. I don't know what that's supposed to mean. In fact, even if you assume n is constant, it doesn't satisfy this relationship. You also remember, it's far from reality. Remember, I said at some point you expect, in real networks, for alpha to be much greater than beta. The algorithm you've analyzed for reductions and broadcasts is a tree-based scheme. You showed that it was sub-optimal in the beta term. What that really means is that it's a perfectly fine algorithm, as long as the messages are small. But what if you care about the case when n is very large? After all, if beta is much smaller than alpha, then you can utilize a network bandwidth better by sending larger messages. Let me turn this question around on you. Why is the beta term so large? That is, can you explain to me intuitively in English, what is the problem with the algorithm that leads to this extra factor of log P communication? Type your explanation here. Note that the question is somewhat open ended so I'm not necessarily looking for a fixed answer. Well actually, I am looking for a fixed answer, but I want to see what you come up with. Here's one answer I'm hoping to see. The tree-based scheme talks too much. In particular, at every round it sends the full message over and over again. Somehow it seems redundant. Let's see if we can fix this excess communication. Here's a neat algorithmic trick. You can use it to improve the bandwidth term on some of the collectives. It's called bucketing. Let me illustrate the idea using all gather. Remember that an all gather starts with all of the data distributed across all the nodes and ends with the data replicated on all the nodes. One scheme might be to first do a gather followed by a broadcast. So the gather part is fine. It attains the lower bound. What about broadcast? If you only have the tree based scheme, then you'll be off on the beta term by a factor of log P. I think you need a fresh perspective on all-gather. Let's go back to the initial state. Intuitively, the bandwidth or beta term is fundamentally about using as many of the links as possible. One way to do that is to have every process perform a send to a neighbor in say a ring like fashion. So, for example, we'll have zero send to one, while one sends to two, while two sends to three and so on. Now we also one need P minus one to send back to zero. You might be thinking to yourself, wait a minute, there's no link between P minus one and zero. But in fact there is. Remember that the links are bidirectional so in fact you can send from P minus 1 to 0 by just going in the reverse direction. So if we try this scheme and we execute one round of communication, let's see what we get. Let's keep going, in particular, let's have every process take the data it just received and pass it along and continue on and on. All told, that will be P-1 communication steps. So what's the total cost? Well, each process and each step sends, M equals N divided by P words. But it does so in parallel. So, writing down the cost, each message is alpha, plus beta, times N over P, and there are P minus 1 rounds, and that gives you something that's roughly alpha times P, plus beta. And with respect to the beta term, this is optimal. Woot. But you can also see that there's a catch. It's sub optimal with respect to the alpha term. But that's okay. If the message is large so that this term dominates this term, then the following relationship has to hold. The per process message size has to be much less than the ratio of an alpha to beta. So, provided that's true, we'll say that this algorithm is good. So, now you have a bandwidth optimal all-gather. Since reduce scatter is the dual, you also have a bandwidth optimal reduce scatter. Given what you now know, I want you to give me a new broadcast algorithm that is asymptotically optimal in bandwidth. That is, I want you to define the implementation of broadcast. As a hint, think about combining bandwidth optimal collectives. Following the hint, one way to do it is to combine scatter and all-gather. So visually, that would look like this. First, scatter the input, then do an all-gather. The key assumption is that all-gather uses bucketing. Now there are many possible ways to write the pseudo code, which our auto grader probably doesn't even really fully check. But hopefully you came up with something that looks like this. Remember that these reshapes are not actually physical copies, they're just different perspectives on their inputs. This code uses a temporary array to hold the result of the scatter, and then sends that temporary array. Here's a new collective, called an All-Reduce. It's the same as a reduction, except that instead of having the final result on one process, it has a copy on all the processes. So in other words we add all the distributed vectors, and then we make sure that the result is available everywhere. So suppose we want a bandwidth optimal implementation. Which pair of the following collectives do you need to combine? I want you to choose two of them. If it were up to me, I would use reduce-scatter followed by all-gather. Let's see why that works. Reduce-scatter will perform the reduction, leaving a piece of the result everywhere. All-gather will collect the results everywhere. All-gathers and reduce-scatters are duals of one another. And since we have a bandwidth optimal version of all-gather we have a bandwidth optimal version of reduce-scatter. Their combination is therefore also bandwidth optimal. The alpha beta model is messy, especially if you compare it to the dynamic multi-threading model, which you might use on a shared memory multi course setting. But, the basic argument for message passing is that you need to get your hands dirty if you want to be efficient at large scales. And that's the big idea of this lesson. How to think about efficiency, specifically in terms of computation versus communication. Now, the message passing model also has at least one major weakness. It forces you to think about a lot of thinks like who and how many processes there are, when and how those processes communicate, as well as how are the processes are connected. So an interesting and largely open research question is, are there efficient algorithms that are network oblivious? That is, is there a framework for writing and analyzing algorithms that will be efficient independent of the network. I wonder. [SOUND] Armed with an abstract model of a message-passing algorithm, what about a programming model that implements it so that we can write an actual program. That's the topic of this lesson. It's a particular library standard called the message passing interface, or MPI for short. Now I've picked out a couple of things for you to read so that you can get familiar with how MPI works. The main things I want you to keep an eye out for are how to do hello world in MPI. How to do asynchronous sends and receives using the routines MPI I send, I receive, wait, and wait all. How to use the built in collective operations like barriers, reductions, scatters, gathers, all to alls. And finally, what the heck is a communicator? MPI_COMM_WORLD, what is that? Now, sometime after you're done, I'm sure there will be an assignment of some sort to give you hands on experience with MPI. So pay attention! Algorithms in the message passing model, always assume some topology for the network. But suppose you design an algorithm for one kind of network, and then run it on another one. How well will it do? This lesson considers network topology, which will give you some techniques for thinking about this question. Now in the early days of parallel and distributed computing, network topology was a big deal. Then, through better engineering of networks and through virtualization, it seemed like it didn't matter so much anymore. That led to cost models, like the alpha beta or the latency bandwidth model, where you could just sweep away a lot of the details of network communication under the hood. But communication layers are getting faster all the time. And the scale at which we operate systems today, is getting bigger and bigger. Thus my personal opinion is that once you have a billion or more processor systems, the network topology is going to start to matter again a lot. And so we shouldn't forget its basic principles. I want to start by introducing you to a couple of really important properties about a network whenever you're thinking about designing algorithms. Remember that abstractly our model of a distributive memory machine is a bunch of computing nodes connected by a network. Let's take three examples. The first is a linear or 1D network. It's a set of p nodes connected in a line. A second example is a 2D mesh network. Again it's a set of p nodes, but this time arranged in a grid. Notice how every node is connected to its north, south, east, west neighbor. Now this third network is a fully connected network. Notice how every node has a direct connection to every other node. That brings us to our first important property which is the number of connections, or links. So given P nodes, a linear network evidently has P minus one links. A 2-D mesh network has this many links which is about 2 times P. A fully connected network has about P squared over 2 links. You care about the number of links, because it's a proxy for cost. That is, a network with many wires will probably be much more expensive than one with fewer wires. I don't know what they paid the person who wired this machine. But whatever it was, it's not enough. A second important property of a network is its diameter. To calculate it, you take all pairs of nodes, and compute the shortest path. Then you take the longest such path. That longest shortest path, is the diameter which I'll denote by capital delta. For the linear network, the longest shortest path is the one that connects the end points. The end points are separated by P- 1 links, therefore the diameter is P- 1. What about the mesh? The longest shortest path is the one that connects either of these end points. The distance is just the Manhattan distance. That's basically, root P- 1, followed by root P- 1 or 2 time root P- 1 links. A fully connected network has a link between every pair of nodes so it's diameter is just one. Diameter's an interesting property. Essentially it's a proxy for the maximum distance that any message must travel in the absence of network contention. I really wish I was running on a network with smaller diameter. Quiz time, suppose you were given a linear network of nodes. I claim you can add one link to this network, and reduce its diameter by roughly a factor of 2. I want you to add this link. To add a link, just pick the two nodes that represent the endpoint. For example, if you think the link between 2 and 3 will cut the diameter in half, then just click on 2 and click on 3. Remember that the diameter is defined as the longest shortest path. For a linear network, that path is the one that connects the endpoints. So, a good candidate is to connect those. In fact, if you do that, then you get what's called a ring network. Topologically, a ring is just a circle. So the longest path is half the perimeter and that's just about half the diameter of the original network. Suppose you're given this 2-D Mesh network. It has a diameter that's approximately 2 times the square root of P. Here's my question. Where do you need to add links to reduce the diameter by half? Let's label every node by its coordinates, starting at (0, 0) in the upper left hand corner and going to (root P- 1, root P- 1) in the lower right. Here are your choices. Is the answer to connect the opposite corners? Or is the answer to add a ring of links that connect the corners? Or is the answer to add wraparound links from left to right and from top to bottom? Check all the ones you think are correct. If you think none of them will cut the diameter in half, then submit without checking anything. Here is the answer. In fact, all three options will reduce the diameter by half. The longest paths connect the corners. So, anything that shortcuts the corners will reduce the diameter. Now, the last option is special. It turns our 2-D mesh into a 2-D torus. Oops. Wrong torus. Another property of a network that you should really care about is its bisection width and there's a related concept which is the bisection bandwidth. The bisection width is the minimum number of communication links that you have to take out in order to cut the network into two equal parts. Equal is measured by the number of nodes. For simplicity, lets assume that the number of nodes is even so that bisection is well defined. Eep! For linear network, cutting this link will break the network into two approximately equal size pieces. Again equal is measured in terms of the number of nodes. In this case, you only need to break one link, so the bisection width is one. I'll denote by section widths by capital B and in general they'll be a function of the number of nodes. What's the bisection of a 2D mesh? Here's a cut that breaks the network into two equal size pieces. In this case each half has eight nodes a piece. This cut goes through four links. In fact, there is no cut with fewer than four links, so the bisection width is four. For instance, here's another cut that breaks the network into two equal sized pieces. However, this cut is not a bisection. It goes through one, two, three, four, five, six links. Now in general, a mesh with P nodes, will have a bisection width of square root of P. So, why do you care about bisection? Well, one really important communication pattern is something called an all to all personalized exchange. An all to all is a collective, where every node wants to send a piece of data to every other node. Sort of like a bunch of nodes shouting at each other. And what this means is that every node in one half of the network will want to send messages to every node in the other half of the network. So all message traffic will have to go through the bisection. Of course networks with larger bisection widths will have a better capacity for carrying all that traffic. Now, for the fully connected network, we said there were about P squared over two links in total. Remember that every node has a direct link to every other. So to cut in half, you'd have to take about half the links. That yields a bisection width of about P squared over four. Now, related to the concept of bisection width is the concept of bisection bandwidth. Let's explain this by example. Suppose you have a 2D mesh. Let's say the speed of every link is beta measured in words per unit time. If all the links have the same speed, as in this example, then the bisection bandwidth is just the product of the bisection width with the link bandwidth. In other words, the speed across the bisection is the bisection width times the speed per link. What if you have links with unequal speed? In that case, you would look for a set of links that cut the network in two and have a minimum total bandwidth. Suppose you're given this 2D mesh. It's bisection width is square root of P. Suppose you want to double the bisection width. Where do you need to add links? Will connecting the opposite corners double the bisection width? Or will adding a ring of links connecting the corners double the bisection width? Or do you need to add wraparound links from left to right and top to bottom? I want you to check all that apply or check none if you think none of them will work. Of these options, only the third one doubles the bisection width. The other options reduce the diameter, but they can't asymptotically improve the bisection width. As an example, let's take option two. Now, the original bisection cut is still a cut of this new network, but notice that the number of links only increases by one, two links That is, it doesn't double. By contrast, consider option three. Notice what happens at the cut. There are in fact, square of P new links that go through this cut. That doubles the bisecting width. Yeah, I'm awesome. Do bulls moo? Moo, moo. In the event you're ever spelunking in the parallel computing literature, you will come across a variety of other kinds of networks besides rings and meshes and torii. I want to mention a few here in passing. The first is a tree network. The compute nodes are the leaves of the tree. The interior nodes are just routers. So you can assume these higher level nodes in the tree are just moving traffic. They don't do any actual compute. Now in this example, these three nodes at the top are binary, but they needn't be in general. Now, what about the network properties of a tree? A tree with P compute nodes at the leaves will have P links, it'll have a diameter of log P, and it will have a bisection width of just 1 link. The scaling of links and diameter is very good. But the scaling of bisection width being just a small constant is terrible. The bisection width is poor because to cut the network in half, all I have to do is cut one of these links near the top. Now in practice, network designers typically fatten up the bandwidth as you move towards the top of the tree. Here's an example. At each higher level in the tree, I put a lot more wires to help carry more traffic. This variation on a tree network is sometimes called a fat tree. Fat trees are quite common in medium scale cluster environments. By medium size, I probably mean on the order of thousands of nodes today. Another kind of important network is a natural extension of 2D meshes and torii. The extension is to higher dimensions. A d-dimensional measure of torus is basically a high dimensional cube that is, say, the dth root of P nodes per edge. If the object is a torus, then it will have these properties. Naturally, these values depend on the number of dimensions in an interesting way. For example, the diameter decreases by the d through the P. So that's good, because as you increase the dimension, you'll decrease the diameter on the one hand. On the other hand, the diameter also depends linearly on d. So as you increase the number of dimensions, the diameter goes up. Now d-dimensional meshes and torii are very important in practical high end computing systems. In fact, many of the world's top supercomputers use low dimensional toroidal networks. As of the time of this recording, there's even one that has a dimension of six. Related to meshes and torii is a type of network called a hypercube. Woah, easy there, fella. Very roughly speaking, a hypercube is a log p dimensional torus. More specifically, a P node hypercube has these properties. It has P log P links. It has a diameter of log P. And it has a bisection width that scales like P. Compare the hypercube and torus formulas. Hypercube is much more expensive in terms of the number of wires, but it has a lower diameter and a much larger bisection width. So, what does a hypercube look like? The easiest way to explain it is by visual construction. A hypercube is parameterized by a dimension little d. The number of nodes is a power of 2 in d. The smallest hypercube occurs when d is equal to 0. It's just a single node. To make a one-dimensional hypercube, start by making two copies of the lower zero dimensional hypercube. So here are two copies. You then connect the corresponding nodes of the two copies, so that's a one-dimensional hypercube. What about d = 2? Again, start by making two copies of the lower, one-dimensional system. Then connect the corresponding nodes of the two copies. For a higher dimensional hypercube, just keep repeating this process. Make two copies, then connect the corresponding nodes. For d = 4, you do the same thing. Two copies, connect the links, voila. >From this construction process, I hope you can easily see that the bisection width is just P/2. That's because you just build the larger network by connecting the corresponding nodes of the smaller network, each of which is half the size. Now these days a hypercube network is more of a historical intellectual curiosity than something people actually build. There's a lot of elegant theory around them, and you'll see lots of algorithms designed for them in the literature. Now these have just been a few examples of network topologies, which is a very active area of research. I'll put some more references to some recent ideas in the instructor's notes. Consider this network, I have a bunch of questions for you. How many nodes does it have? How many links does it have? What's its diameter? What is its bisection width, and what, pray tell, is the meaning of life? Type your answers as integers in the boxes. Let's start with a number of nodes. There are eight nodes. What about links? There are ten links. What about bisection? You need to divide the eight nodes into two subsets of size four each. There are many possible cuts, let me show you one. This partitioning cuts one, two, three edges. As it happens, all other partitionings will cut at least three edges as well, therefore the bisection width is three. What about diameter? For any pair of nodes you can check that the shortest path between them is never more than three. For example, consider the corners. I'll let you check all other pairs. What you should find is that the diameter is three. When you design an algorithm for a distributed memory system, the communication pattern of your algorithm implies a network. For example, you designed algorithms for scatters and gathers that worked well on a linear network. By working well, I mean concurrent messages never overlapped, and so would not contend for links. Now, suppose you design an algorithm for a linear network, but then decide to run it on something else, like a Mesh or Fat Tree. How well would it work? To answer this question, you need to map a logical network, to the physical one. Consider an example. Suppose you design an algorithm assuming this network. It's a ring with, in general, p nodes. Diameter of p over 2 and a bisection width of 2. Now suppose the underlying machine is physically a 2D Torus. You first need to decide how to assign nodes of this network, to nodes of this network. Let's use a very natural row by row numbering. So the first 6 nodes of the ring go here. The second 6 nodes go here. And, so on, row by row. In fact, let's redraw the linear network, according to this numbering scheme. Notice, that the edges of the 2D Torus, are a superset of the edges of the ring. So, what does that mean? If there's no contention for length in the linear network, then there will be no contention for links in the torus either, at least not under this mapping. So, what if you did the reverse? That is, what if this is the logical network, and this is the physical one? Intuitively, there are a lot more links over here, than there are over here. So even if there's no contention in the torrus, there might be a lot of contention in the ring. To see what would happen, let's start with the same node numbering scheme. Let's look at a couple of edges. For example, let's consider this edge from 0 to 6. In the ring, where's the shortest path between 0 and 6? It's here. Now, let's take a different edge. How about 1 to 7 in the 2D Torus? In the ring, it must go along this path. So what if messages, from 0 to 6 and 1 to 7, are trying to go at the same time? Then the messages might have to serialize, because the paths overlap. To help model this problem, let me introduce a new property of the mapping. This property is called congestion. Achoo. I'll define congestion as follows. Given a logical to physical mapping, the congestion of the mapping, is the maximum number of logical edges that map, to a given physical edge. That's a mouthful. For example, suppose the logical network is a ring. Suppose the physical network is a 2D Torus. Suppose the mapping is this row by row numbering. But suppose that a path, in the logical network, maps two a shortest path in the physical network. You already argue that there's a one to one mapping, from each logical edge to each physical edge. Because of this one to one correspondence, the congestion of a ring, going to a torus, is just one. Let's consider the other direction, with the same node to node mapping. That is suppose the 2D Torus is logical, and the ring is physical. Now let's pick an edge. How about this one? Now this edge also appears on the logical network. But remember your earlier analysis? What if you take this edge from 4 to 10? Or this edge from 3 to 9? These map to shortest paths that intersect the 5 to 11 edge in the physical network. In fact, you can keep going. All of these edges in this row have to go through the 5 to 11 edge. So, we have so far, that at least 6 logical edges map to this 1 physical edge. Therefore, we have that the congestion has to be at least 6. Consider the following general case of a mapping. The logical network is a 2-D Taurus. The physical network is a 1-D ring. The numbering of mapping is as shown. Here's my question. What is the congestion of this mapping? I want you to type your symbolic answer here. If you need to put in the symbol square root of x, just type sqrt (x). The answer is sqrt(P)+2. Let's take an edge, I don't know, how about this one? Let's see which logical edges map to this physical edge. There are square root of P edges that come from the matching edge as well as all of these vertical edges, but that's not all. There are also these pesky wraparound edges. This wraparound edge corresponds to this path. And what about the other wraparound edge? The shortest path for this wraparound edge would start here, go along the first row, down this wraparound edge in the ring, and back across the bottom row. So notice that it also has to touch this edge. So the wraparound edges give me the + 2. If you were off by one or two, the auto-grader should have forgiven you. Asymptotically, the important point is that congestion grows by square root of P. I have a confession, picking a mapping and counting edges to compute congestion can make you more than a little crazy. Eep. Let me help you out. Here's a shortcut to estimate a lower bound on congestion. Consider two hypothetical networks, a logical and a physical one. Now suppose you find a bisection in the physical network. By the definition of bisection width this bisection will divide the network into two pieces of roughly the same size. Here I am using V1 to represent the nodes of one half. And V2 to represent the nodes of the other half. Now the bisection cut will also go through a bunch of links. The number of such links is the bisection width. Let's call that B sub X. Now bisecting the physical network implies a cut of the logical network as well. So schematically, it might look like this. Now the cut in the logical network will go through some number of edges. Let's call the number of such edges on the logical network capital L. So in other words, what we're saying is that there are at least L, logical edges that map to B sub X, physical edges. So the congestion of the mapping has to be at least L divided by B. We can say more. The minimum possible value that L could have is the bisection width of the logical network. That's because the size of the cut that splits the network in two has to be at least the bisection width. Again, that follows from the definition of bisection width, therefore a lower bound on the congestion is the ratio of the logical bisection width to the physical bisection width. Neat! Let's see if this works. Lets go back to the 1-D ring and 2-D taurus example. So let's say we start with a logical 2-D torus. And suppose we map that to a physical 1-D ring. Do you remember the bisection widths of these two networks? They are 2 edges for the ring and approximately 2 root P for the Torus. Let's plug these in. What you'll get is square root of P. Now the true value of the congestion of this mapping is square root of P + 2. So you can see the lower bound in this case is pretty good. Here's what knowing the congestion allows you to do. You can estimate how much worse the cost of your algorithm might be, if you run a physical network, that has less bisection capacity, than your logical network. Now I want you to apply the lower bound on congestion, consider this list of networks. For each network I give you the network's diameter, the network's bisection width, and the number of links in the network assuming p nodes. Now suppose you wish to map a logical network onto a physical one. Here's a list of pairs of mappings. The pairs go from logical on the left hand side, to physical on the right hand side. My question is, which of the following pairs would you expect that it might be possible to have no congestion, assuming the lower bound analysis? Here's the answer. The lower bound analysis says you take the bisection width of logical network and divide it by the bisection width of the physical network. So for example, let's take fully-connected to hypercube. Fully-connected is o of p squared. Hypercube is o of p, so the ratio of fully-connected to hypercube is p. So the lower bound suggests you should expect some congestion. What about complete binary tree to ring? Complete binary tree has a bisection of 1, and ring has a bisection of 2. So the ratio, one half is less than or equal to 1. That tells us there might be a way to do a congestion free mapping from complete binary tree to ring. So you can do the same exercise on all the options. One cool thing about a higher dimensional network is figuring out how to exploit it both for fun and profit. Let's start with a low dimensional example. Suppose we're doing an all-gather on a linear network. Here's the before and the after picture. This is a lower bound on the execution time. N, in this case, is the length of each output vector. Now, if you did the all-gather using a bucketing scheme, then you would find its execution time to be approximately alpha times P + beta n. So this matches the lower bound beta, but not in alpha. Here's a question. If I gave you a higher dimensional mesh, could you do better? Let's take a 2D mesh as an example. Initially, suppose there are m words per node. So, since this is an all-gather on every node, we have the capacity to store all of the words from all of the processes, but, initially, we only have one little piece. Now, if you do an all-gather, here's the output. That is, when it completes, all nodes have a copy of all the data. So how do you implement this operation? You could certainly run the 1D linear algorithm. The question is, can you exploit the extra capacity that comes from extra links? Here's an idea. Why don't you start by doing a 1D all-gather within each process row? When that completes, each node will have a complete row of data. Now do an all-gather, but this time within each column. When it completes, you will have the final output. So how much time does this take? The row and the column gathers are one dimensional. Let's suppose we use the one dimensional bucketing algorithm. Remember the cost of that algorithm. Let's consider the row all-gathers first. Within each row, there are square root of P processes participating. The total output size is m times the square root of P. Therefore, this is the cost of the row part. Notice that relative to the one D algorithm alone, we've already improved the latency. It scales like square root of P, rather than P. What about the column communication? The data volume is now N, but you still only have square root of P processes participating. Therefore, the execution time to run the column part of the algorithm is the 1D algorithm on an input of size, n, over square root of P processes. Let's sum up the row and the column times. Asymptotically, the total time is now proportional to just square root of P in the alpha term. With respect to the beta term, it's basically optimal. In other words, taking advantage of a higher dimensional network gets us a little bit closer to the ultimate lower bound, at least on the number of messages. Suppose you want to do a broadcast on a 2-D network. Here are two schemes. In the first scheme, you run a tree-based algorithm broadcast in each row followed by a tree-based broadcast algorithm in each column. The second scheme has four steps. Scatter in the rows, followed by a scatter in the columns, followed by a bucket all gather in the columns, followed by a bucket all gather in the rows. Stop for a minute and convince yourself that these two schemes are plausible. Then once you've done that, I want you to answer this question. Which scheme has the lowest alpha term if you estimate the cost? Is it Scheme 1 or Scheme 2? Choose your answer here. The answer is scheme 1. Remember that the alpha term cost of a tree-based algorithm is proportional to log of the number of nodes. By contrast, any sort of bucketing will be linear. Now in this case, since we're operating on a 2-D mesh, it'll be linear and square root of P rather than linear and P. But that's still worse than log. So, at least with respect to the alpha term, scheme 1 is better. Now, had I asked you which scheme has the largest beta term, what would you have said? Let's discuss another collective. It's called the All-to-all Personalized Exchange. In the business, you can just refer to this as all-to-all, for short. In an all-to-all, every node wants to send a unique message to every other node. Now, this picture is not quite accurate, because each node is just sort of shouting a message to all other nodes. But in fact, in an all-to-all personalized exchange, every node has a personal message to send to every other node. As an example, consider a matrix transpose. Start with a matrix, initialize to some set of values. A transpose rotates the elements. More formally, every element A(i,j) swaps positions with A(j,i). Now suppose our initial matrix A is distributed column wise among several nodes in a linear network. So that might look like this. After the transpose we want the system to look like this. So how do you implement it? To make this a little easier to see let's switch from a linear network to a ring network. And let's suppose that the size of the data that every node wants to send to every other node is m. So each of these little boxes is of size m and the total length of a vector on any node is n which is m times p. Before I give you an algorithm I want you to come up with a performance target. Start by considering one of these nodes, and the P- 1 messages that it wants to send. So, let's start with node 0 and let's look at the ith message that it needs to send. The ith message needs to go to node i. So, how far away is that node? I claim that that distance is min(i, P- i). Why? On a ring, the message could go either this way or this way, depending on which one is shorter. Now, from this fact, you can estimate the average distance that a message must travel. So, each of the messages from 1 to P-1 need to travel a distance of this. We take the average over all messages. For simplicity, this calculation assumes that P-1 is even. If you work it all out, the average turns out to be P over 4. So let's remember this handy fact. Now all P nodes in the network need to do these sends. What's the total volume of traffic that the network has to carry? Well, it's the number of nodes times the total volume per node for all messages times the average distance per message. Here's another question, how much bandwidth is available to carry all of this traffic? Well the total bandwidth is the number of links in the network times the speed per link, which is just one over the inverse bandwidth. Again we're setting a performance target, so we're making optimistic assumptions. Now from these two facts, we can estimate a lower bound on the communication time. It's just the total volume divided by the total speed. So now what we want to do is write down an algorithm and then compare that algorithm to this lower bound. Okay, let's talk about a specific algorithm. Here's an idea, which is to perform a sequence of circular shifts. Here's the initial state. In the first step, each node will send m times P-1 of it's data to its neighbor. For example, node 0 will send these words to the right. What it is sending is all of the data that needs to go somewhere. So while node 0 is doing that, node 1 is doing basically the same thing. It's packing up the data that needs to go to other nodes and sending it. Node 2 does the same thing. It keeps the one piece of data it's supposed to hang onto and ships off everything else. And so on. Here's what the system will look like at the end of this first step. Compare this to our goal output. So after one step, you can see that every node has two of the elements that it needs to retain. So node 0 has a0 and e0, node 1 has a1 and b1, node 2 has b2 and c2. This is exactly what they need in the final output. So that means in the next step, we could repeat the same process, again retaining the data we're supposed to keep and shipping off everything else. So in step two, every node would send m(P-2) of its data. And this process will repeat until we're done. To analyze the cost of this algorithm, let's consider some round. Call it i. In round i, a node sends m times P-i of it's data to a neighbor. So how much time does this whole thing take. First, the overall scheme needs P-1 steps. In each step i, nodes send m times P-i words, so plug in our usual per message cost model, then simplify and finally enjoy your handiwork. Now remember, our usual convention is to let total size n be m times P. So then this expression becomes n over 2. So notice that the bandwidth term is now linear mP. Compare that to this lower bound. Hey, your algorithm seems pretty good. It's within a factor of 2 of our lower bound estimate. Tofu the parting hypercube says oh yeah. You know I 'm SCS no one knows you're a tofu. Recall the running time for an all to all personalized exchange on a ring network. Here is Mr. Tofu's question. What network has the best chance of reducing the asymptotic running time from being linear nP to being, say logarithmic in P? I'll give you four choices. Complete binary tree, d dimensional torus, a hypercube, and a fully connected network. And I want you to check all that you think apply. This is actually a really hard question, and probably way too unfair for a quiz. Yeah, I'm a jerk like that. So I would say that your best bets are the hypercube and fully connected networks. Here's my thought process. An all to all is intrinsically bi-section, limited. So, of these choices, only hypercubes and fully connected networks have linear bi-section widths or better. That's what you would need to knock down this bandwidth term from being linear in P, to something that is logarithmic in P, or maybe even constant. Okay, so that's not a rigorous proof that these are the right answers, it's just an intuitive idea. So, what I'll do is I'll put some pointers in the instructor's notes where you can do a little bit more reading if you're interested. I know you'll be interested, because if you do the reading, Mr Tofu will give you a cupcake. This cupcake is for you, Mr Tofu wants to give you a cupcake. [SOUND] Oh, good, you made it to the end. Or did you take some longest, shortest path through the lesson to get here? Well, here's what I think are the two big ideas of this lesson. The first one is the [COUGH] excuse me, [COUGH] the first one is the idea of congestion. [SOUND] Boo. This concept lets you design for one topology and then estimate whether it will map well or poorly to another. The second big idea is to exploit higher dimensional networks, both for fun and for profit. For instance, you should have noticed that there are big algorithmic scalability wins from taking advantage of a 3D mesh network instead of a 2D one. Now, in closing, let me pose one last question. As systems get bigger, will topology matter again? If it does, you will be ready. This lesson is about distributed memory algorithms for multiplying matrices. Matrix multiply appears in lots of applications, from simulations in quantum computational chemistry, to backpropagation in neural networks. And when the HPC community ranks the worlds supercomputers for its top 500 list, it does so using a benchmark that, to first order, is basically a really big matrix multiply. Now that's not the only reason to study it. The other one is that it's not so hard to analyze and that makes it a great starting point for practicing some basic analysis techniques. A matrix multiply is the following operation. Given two input matrices, A and B, and given a third matrix C, perform the update, A times B + C, and overwrite C. Schematically, I like to draw matrix multiplies this way. Matrix A has m rows and k columns. Matrix B has k rows and n columns, and the matrix C has m rows, same as A, and n columns, same as B. The matrix update formula may be interpreted as follows. Consider some output element Cij. This scalar update formula at the bottom tells you how to update it. Essentially, you compute a dot product between a row of a and a column of b. The dot product comes from multiplying corresponding elements of the two vectors and then accumulating them into the output. Let's look at an example. Suppose row i of A and column j of B have these values. And let's further suppose that Cij has this initial value. The dot product is the sum of the element-wise products. So in this case, we take the element-wise products 3 times 1, 4 times 2, -2 times 5. Add them all together, accumulate them with the initial value of C, which was 6, and we'd get 7. Here's what a matrix multiply looks like as pseudocode. It's just three nested loops. This form makes it easy to see that the algorithm takes time O of m times n times k. And of course, when all the dimensions are equal, that's big O of, say, n cubed. So, where's the parallelism? Both the picture and the code suggests that all the output elements are independent of one another. So, if you were to parallelize this code for a PRAM type system, the first easy thing you would do is to replace the outermost for loops with parallel for loops. In fact, that gives you another useful fact about matrix multiply. Suppose you want to compute a group of elements of C. Then you can do a group update by doing a sub matrix multiply or block matrix multiply. For example, this block would depend on this block of rows, and this block of columns. Put another way, everywhere you see a matrix element in this picture, you could instead imagine not a single element, but an entire submatrix. The computation is essentially the same with respect to blocks. Okay, back to parallelism. What about this innermost loop? I hope you recognize that it's basically a reduction. Notice that the element-wise products within the loop are independent. Once you've computed them all, you just add them all up. So we can replace the inner-most l loop with the following. So there's a temporary array, a new loop to compute the independent products, and then a reduction. Now if all the dimensions are equal to little n, then the total work is n cubed, and the span is just log n. So, I hope you find a matrix multiply to be easy to state and rich with available parallelism. Let's check to see if you understand the basic definition of a matrix multiply. Here's an A, B and C with four missing values. I want you to determine the missing values. You can assume that initially, C was set to all zeros. So, what I want you to compute is the state of the matrices once the matrix multiply is finished. Here’s the answer. Let’s see where it comes from. Remember that each output is computed from a dot product. So, for example, let's take this output. There are two missing values, so let's give them some arbitrary symbols. How about w and x? The dot product is just this equation, basically, the element-wise products summed together equaling the output, and we can repeat this for each of the other output elements. For each blank, I introduced a new symbol. As you can see, we have four equations and four unknowns, so we should be able to find a solution. Let's start with the third equation, which only has one unknown. Solving for z, we get the value 2. We can then plug this into the fourth equation. Solving for y, we get 7, so that eliminates the fourth equation. Since we know y, we can solve the second equation for w. You'll get w equals 1. Eliminating the second leaves us just the first. You'll find x equals 30. While we're on the subject of this sort of picture of a matrix multiply, and I mean the picture on the left, I want to mention a related geometric way of thinking about how a matrix multiply works. This geometric view will give you a handy fact which you'll need later when I ask you, what's the minimum amount of communication a matrix multiply needs? Oh, the intrigue. The intrigue, oh! Starting with this initial picture, I want you to imagine a cuboid where the faces are the matrix operands. Notice how the edges match the same way the dimensions need to match. For example, where A has k columns, and B has k rows. There's a matching edge. The same holds for the columns of B and the corresponding edge of the cube. Columns of B, and corresponding edge. Recall that an element i, j of C depends on row i of A and column j of B. In the cube, there's also an element i, j of C. The dependents on A and B is a line that cuts through here. And if line is a projection of a row from A and a column from B. Now each of these scalar multiplications is like a point inside this cube. Let's think about what that means. Suppose I pick a set of matching rectangular blocks from A, B and C. So here I picked an r by s block of A, a matching s by t block of B, and of course there's the corresponding matching output which is r by t. If they're perfectly aligned, then they define a volume of the interior of the cube. In other words, this volume is the set of all multiplications you need to do in order to update this output block with these two surface sub-blocks. How many multiplies is that? Well, it's just the volume of the interior, so that's r times s times t. In terms of the sizes of the sub blocks it's just the square root of their product. In fact, there's a general result from the geometry of discrete lattices like this one, for any integer cube of point, suppose I give you a subset of its surfaces. Let's call them SA, SB, and SC. Now there may be some blob of intersection in the interior, we'll call that I. The theorem is this, the volume of I is at most the square root of the size of A, times the size of B, times the size of C. This is a theorem due to Lumis and Whitney in 1949 and a sighted in the instructors notes. Let's reinforce the Loomis-Whitney Theorem. Suppose I assign you part of a matrix multiply. That is, let's say I give you a piece of A, which is 3 x 5, a piece of B which is 5 x 4, and a piece of C which is 2 x 2. Here is my question. What is the minimum and maximum number of multiplies you could possibly do? I want you to fill in these blanks with a suitable integer. Here's the answer. The minimum possible value is zero and the maximum is 34. The minimum is zero because there might not be any intersection among the elements of the blobs. For example, here are three blobs that are in opposite corners of their respective faces. And although the perspective is totally messed up hopefully you can kind of believe that. They don't intersect. The maximum would occur under some best case alignment of the three operands. The Loomis Whitney theorem gives us an upper bound. If you take the square root of the product and the sizes, you'll get about 34. So a multi threaded Dag algorithm for matrix multiply is easy enough to write down as we did before. What should you do for a distributed memory machine? Let's start with the case of a linear network having P nodes. For simplicity, let's also assume that the matrixes are squared n by n objects and that P divides n. The first question is, how should you distribute the matrix operands? There are lots of potential choices. I want you to start with the following convention, distribute the operands by block row in an identical fashion. That means you give each node n over P consecutive rows of each matrix operand. Let me draw a picture. For each node, I've assigned n over P consecutive rows. Notice I've used the same distribution for each matrix. While it's not strictly necessary to do so, it simplifies the design of practical interfaces to the matrix objects, because essentially you can always assume they have the same distribution. Now consider some process, what is the most work it can do given this data distribution? In the best case it could update it's entire block row of C using the entire block row of B and one sub block of A. It could not for instance do anything with a sub block of A that's say over here or over here. Why not? Well recall the definition of matrix multiply. To fully update this block row of C, you need to multiply this little block of A by this entire block row of B and then this block A by this entire block row of B, and so on. So on. But, given this distribution, this process doesn't have access to this block row labeled b0. So to help the algorithm make progress, you're going to need to shuffle blocks around. Now again, there are many choices. One convention is in order compute strategy in which you keep the block of C in place. You also keep A in place. If you do that, then you have to move B around. Since we're on a linear network, one strategy is to shift the block rows of B. For example, if I were to shift B downwards, then I would get the following result. Now that this node has b0, it can do a partial update with the block a0. If you keep circularly shifting, eventually, every node will see all of B. That's a total of P shifts, which by the way, leaves B in its original distribution once you're done. Very convenient. Let's write down some pseudo code to express the algorithm. We'll let A hat, B hat, and C hat represent the local parts of A, B, and C. Notice that they're of size n over P by n. Now to implement the shift we're going to need to send the block that we have and we're going to need some space to receive a new block row of B. So here I've declared another little temporary storage matrix B twiddle. These two variables compute the neighbor below me, r next, and the neighbor above me, r previous. The loop iterates over circular shifts. We can start by doing a local matrix multiply on the data we have. To keep the pseudo-code simple I've used placeholders, where you would compute the appropriate indices of the local variables as a function of L. Following the partial update is the communication. Step 1 is to send the local buffer to the next processor. Step 2 is to receive from the previous processor. Then we wait for these two communication operations to complete. So, that ends one circular shift operation. And, finally, before moving on to the next round, we just swap the temporary buffer used to receive with the current local buffer used to compute. Easy breezy. Here's the 1-D Block Row algorithm for matrix multiply. Let tau be the time per floating point operation, or flop for short. In this case flop is a shorthand for say a floating point multiply or a floating point add. It does not mean a terrible movie. So if tau is the time per flop, then the total time this algorithm spends doing multiplies and adds is 2 tau n cubed / p. Pause to convince yourself that this is true before you move on. Here's my question. What is the time spent on communication? I want you to type your answer symbolically here. As usual, assume our alpha beta model of communication. Type a little a for alpha, and little b for beta. Here's how to get the solution. In any iteration of the for loop the only data communicated is B hat. It's size is n over p words by n columns. So that's n squared over p words in total. Now there are p rounds of communication because of the loop bound. Therefore, you have to pay for p sends each of sides n squared over p. So that would be a total communication time of alpha times p times beta times n squared. Recall the 1-D block row algorithm and its pseudocode. You know that it's running time is (2 tau n cubed) / P, for flops, + alpha P + beta n squared, for communication. In fact, you can make this code a little bit faster. All you have to do is rearrange the statements of the loop body. Notice, I said rearrange the statements of the loop body. You don't have to actually change the statements themselves. In the best case, the rearrangement I have in mind might improve the performance by up to a factor of 2, relative to this running time. Your task is to show me how. I will pre-fill this text box with these statements, and I want you to rearrange them to get this possible factor of 2 improvement in speed. Here's the solution I had in mind. Now this running time charges you separately for flop time and communication time, but, in fact, doing the local multiply and doing the sends and receives are completely independent. So a natural idea is to try to overlap them. That suggests that I start the communication. While the communication is pending, do the matrix multiply and then wait for the communication to complete. So that's it, overlap, computation and communication. Now, why do I say that might improve the running time by up to a factor of two? Well, if you overlap computation and communication, then the running time should be the max of the time to do the flops against the time it takes to communicate. This is a factor of 2 faster because of an algebraic fact. Namely, the sum of a and b is always less than or equal to twice the max of a and b. That's why I said up to a factor of two. So there's one D block row algorithm. Is it good or bad? Start by recalling the running time. Now, it's the first algorithm we've talked about in a long lesson, so it has to be bad in some way. Let's pretend to be more analytical. Start by computing the speed up. The best sequential algorithm pays only for flops so its time is just two times tau times n cubed. Next let's plug in the parallel time for the 1D block row algorithm. Let me massage it into a different form for you. Now this form makes it easy to see how the speed up compares to our ideal, which would be something proportional to p. That is, if you want to know when the speed up divided by p is a constant. In fact, this concept of speed up divided by p has a special name. It's called parallel efficiency, which I'll denote by capital E. So a parallel system is efficient if its parallel efficiency is a constant. And higher constants are better since they correspond to higher fractions of linear speed-up. Now in order to have constant efficiency you need the denominator to be a constant. So when will the denominator approach a constant? In this case when N is big omega of P. Now think about what it means to be efficient in this sense. Let's say you want to double the number of nodes In order to solve the problem faster. Then this condition you've just arrived says you also have to double the problem dimension. But if you double the problem dimension, the size of the matrices will quadruple because they're N by N. Even worse, the number of flops will have to increase by a factor of 8. In other words, if you double the problem dimension, you have to double the amount of memory you need per node and you have to spend more time doing flops. Dang, that's huge. Yes, thank you, that's hilarious Tiny. Put another way, if you don't or can't double the dimension, then you'll quickly see diminishing returns as you increase the parallelism of the system. By the way, this function or this relationship between n and P has a special name. It's called the isoefficiency function. That is, the isoefficiency function is the function of p that n has to satisfy in order to have constant parallel efficiency. Now there's one other consideration, which is the amount of temporary storage space. Remember, you need temporary storage for these B twiddles. So, you have to store the three local matrix upper ends plus the B twiddle. That gives you four times n squared / p. Okay. So, that gives us a pretty complete analysis of the time, speed up, efficiency and storage requirements of the one D block row algorithm. Good job, you. Consider an-all-to one vector reduction. Now suppose tau is the time per scalar addition. Assuming a tree-based scheme this is the time to reduce an input of size N on a linear network. Here is my question. Which of the following best describes the isoefficiency function of a tree-based all-to-one reduce? Here are your choices. The answer is none of these. Start by writing down the efficiency of the algorithm. Remember that's speed up divided by P. By the way, if you expand it, you get this expression. This denominator has a special name. It's called the parallel cost. So let's plug in these two times, T star and T of n P. And let's do a little algebra. In this case, I'm going to divide the numerator and denominator by tao times n times P and you should get something like this. Notice that there is no setting of n that will make E of n P a constant. The problem is this first term which goes to infinity as P increases. In other words, the efficiency will always tend to zero. Here's a thought question for you. Is a bucketing or pipeline scheme possible? And if so would it fix the problem? Hmm. So what about a 2-D Algorithm for a 2-D mesh or torus? In fact, somehow intuitively, a 2-D mesh or torus should be a better match for matrix multiply given the fact that matrixes are in fact themselves two dimensional. You can't keep me on a linear network, weeee. Here's an elegant algorithm called the summa algorithm, that's summa not sumo. We'll put a reference to a paper on summa in the instructor's notes. Summa starts with a 2-D distribution of the matrix operands. Each node is responsible for updating a part of the C matrix that it owns. For example, consider this node. You know that eventually the owner of this block needs to see all of this block row and all of this block column. The SUMMA algorithm accomplishes this as follows. First, it iterates over vertical strips of A and the corresponding horizontal strips of B. At each step, let's denote the current strip by the index little l. Now the strip can have any width in A or height in B. Let's denote the strip dimension by little s. Algorithmically, you can think of all processes synchronously executing a for loop that iterates over these strips. Now in order for all nodes in this block row or this block column to proceed, every node needs to see the current strip. That means the owners of the current strip need to send a copy to all relevant nodes. For example, this strip of A needs to go to all processes in its block row. The owner of the strip can simply do a broadcast within the block row. Now a similar thing should happen in the block column. Once both strips have arrived each node can do a local update. So what's the running time of this scheme? To keep things simple let's assume square matrices and a square process grid. And though it's not hard to generalize let's also assume that the square root of P which is the dimension of the mesh and little s the strip size, both divide the matrix dimension in. First observe that there are n over s iterations of the loop, and each iteration does the same thing. Now for the cost of each iteration. Let's start with the update step. Each of these strips is n over root p by s. We can use that fact to compute the number of flops for this local update step. This is the cost of the update step, assuming that each flop costs tau. Multiplying this out, the total flop time is 2 tau n cubed / p. What about the communication time? The communication time is essentially just the time for these two broadcasts. Mm, I wonder how much time that costs? Let's do a little quiz where you calculate the communication time of the summa algorithm. You argued previously that the running time to do all the flops is 2n cubed over P. But what about communication? Here's my question. Which of the following estimates are plausible estimates of the overall communication running time? For the alpha term, I want you to choose one of these. For the beta term, I want you to choose the matching one of these. Now, I put a big O in front of this so that you can ignore constants and just focus on asymptotics. Now, there may be more than one correct pairing, so the key is, based on your choice for one of them choose something that's plausible for the other one. The communication all comes from the two broadcast operations. Their cost depends on what you assume about the cost of a broadcast. The little m in these formulas denotes the size of the message. Now for this question, little m corresponds to the size of these little blocks. How large are they? Recall that the dimensions are n over root P by s. Now to get the total volume of communication, remember that we execute this pair of broadcasts, n over s times. Thus the total communication volume we have to pay for is n/s times the size of each message. That, it turns out is n squared over square root of P. So assuming a choice of these two implementations, here are two possible correct pairings. If you assume the tree based algorithm, then a correct pair would be log P and log P. If, on the other hand, you assume the bucketing based algorithm, then a correct pairing would be P and 1. So is the 2-D summa scheme intrinsically more scaleable than the 1-D block row scheme? The answer is quite possibly. First note that the strip width is a tuning perimeter of the algorithm. Its range is between 1 and N over square root of P. Again, that's assuming square operands and a square process grid. To analyze the efficiency, let's start by assuming the tree based broadcast implementation. If you compute the efficiency, you should find that it's equal to this. Now if you stare at this long enough, one of two things will happen. One is you'll glaze over. The other is being observant, you'll see that this factor determines the isoefficiency. In particular the isoefficiency function is as follows. Let's compare this to the 1-D algorithm. The isoefficiency function of the 2-D scheme is asymptotically lower than the 1-D scheme. That means the 2-D scheme is intrinsically more scalable. You don't need to increase the problem size as fast as you need to with the 1-D scheme. Now you can of course do the same exercise with a bucketing scheme. In that case you'll find a slightly different ISO efficiency function. Asymptotically, this is a little bit worse than the tree based scheme. If you work it out you'll find that it comes from trading a lower communication volume with a higher latency cost. Here's a quick follow-up question about the SUMMA algorithm. Compared to the 1-D scheme, does SUMMA require more memory, the same amount of memory, or less memory? By way of reminder, remember the 1-D scheme's memory requirement was basically 4 n squared over P. You'll remember that came from having to store a piece of A, a piece of B, and a piece of C as well as an extra buffer for messaging. Now, if you think the answer depends on something, you can check more than one of the options that you think are possible. As you might have anticipated from my hint, the answer is of course that it depends. So it could be more, it could be less or it could be the same. It all boils down to how you choice the value of the tuning parameter, little s. To see how, let's write down an expression for the storage. First, each node has to store one block of A, B and C. So that of course gives us three n squared over P. Now you also need buffers for the two broadcasted strips. The amount of storage depends on the strip width, a little less. So here are the cases. S has to go between one and N over root P, as you'll remember. When S is less than one half N over root P, then the total storage is less than four N squared over P, meaning less than the 1D algorithm. Remember that the problem with a smaller s is it increases the latency time. Now if instead s is greater than 1/2 n over root P then of course the storage will be greater than 4 n squared over P or more than the 1D algorithm. In fact what happens when s is its maximum value. In that case the SUMMA algorithm might need five times n squared over P worth of storage compared to four times for the 1D algorithm. The interesting bit of course, is that the strip width serves as a tuning parameter which adjusts the relevant importance of the latency term, the bandwidth term, the overall scaling and the memory requirement. It's the overall simplicity of a summa algorithm, combined with this tuning parameter that lets you trade off time and storage. That made summa the gold standard for dense linear algebra at least for the better part of the last 20 years or so. So the Summa algorithm seems pretty good, but how close is it to the best possible scheme? Imagine a machine with P nodes connected by some topology, further suppose each node has M words of main memory. Now let's say you take the matrix operands and you chop them up into little bits and distribute the little bits across all the nodes. What I want you to do now, is pick one of the nodes, zoom in on it and study its possible behavior. Now recall that each node has M words of main memory. Let's further suppose that during the entire distributed matrix multiply, this particular node does W multiplications. We don't know what W is, but you'll determine that later. Now let me pose the following question to you. How many words must this node send or receive? To figure this out, let's start by imagining that the word performs its work as a sequence of computation and communication steps. We'll visualize what the node does by a hypothetical timeline, like this one. So time precedes horizontally. And the node alternates between phases of, say, communication followed by some computation, followed by some more communication, followed by computation, and so on. Now let's divide this timeline into L distinct phases. A phase is special. We’ll assume, in each phase, that every node sends and receives exactly M words. The exception is the last phase. In last phase, you might send or receive fewer than M words. The reason is simply that the total number of words sent and received might not be divisible by L. Here's a question. What is the largest number of multiplies that each phase can possibly do? To figure this out, let's consider one of the phases. Let S sub A be the number of unique elements of A seen during this phase. And similarly, let's let S sub B, and S sub C, be the number of unique elements of B and C that we see during this phase respectively. By Loomis-Whitney, you can compute the maximum number of multiplies performed in this phase given S sub A, S sub B, and S sub C. So all we need to figure out now is, how big are these sets? Let's start by considering just S of A. Now imagine the node and the state of it's memory. It's possible that at the very beginning of the phase, the memory is full of only elements from A. And of course, because the memory capacity is M, there can't be more than M such words. Now remember, that by definition the phase performs exactly M sends or receives. So let's think about what that might mean. It's possible that during the phase, all of these words are used to do computation. It's also possible that all M of these words, at some point, get pushed out. And when I said pushed out, I don't necessarily mean being communicated, I just mean we get rid of them and make space for other words. It's then possible that all these communications bring in a whole new set of elements of A. These are M completely brand new set of elements and it's possible that before the end of the phase, we use all of them to form more multiplications. So what have we said so far about the capacity of S sub A? Basically, we just said that it's possible, during this phase, that we might have seen up to 2 times M elements from A. Now remember, this is just an upper bound. And I realize it might seem a little loosey goosey, as far as bounds go. Honk. Now even if this bound a bit loose, remember, we're just after a bound, so there's nothing incorrect about this reasoning. It just might not be as precise as it could be. Now by similar reasoning, you can find that the maximum size of S B and S C are also 2 times M. When you plug all that in you'll get 2 root 2, times M to the three-halves. You might want to stop and check that you agree. What we've just said is that the maximum number of multiplies that might have occurred during this phase is 2 times root 2, times M to the three-halves. A natural follow-up question is, how many phases are there? Let's go back to the timeline. I told you this node did a total of W multiplies. Remember, that every phase does at most this number of multiplies. So what might L be? Well, L has to be at least equal to the number of full phases. And the number of full phases has to be at least the number of multiplies that the node does, divided by the maximum number of multiplies per phase. And of course, you just computed that, so we can plug that in. And then, of course, get rid of the floors, leaving you with this. Now remember, that every full phase does exactly M transfers. That means you can get a lower bound on the total number of transfers, which would just be the number of full phases times M. So the number of words communicated by at least one node, has to be at least the number of full phases times M. Let's plug all this stuff in. Okay, whew, you're almost there. Now what about W? If the matrix operands had been of size mby k, k by n, and m by n, then you know the total number of multiplies is just mnk. Now, if that's a total number of multiplies, and all the processes are executing in parallel, then at least one node has to execute mnk over P multiplications. That is, there's some node for which W is at least mnk over P. Let's plug that in. What you now have is a lower bound on the volume of communication by at least 1 node. Let's simplify this a little bit for the square case. That would be n cubed over P multiplies, per node. Now there's just one more piece of useful information which is the value of M. Remember, that we distributed the matrices, evenly, over all the nodes. So that means M has to be proportional to n squared over P. So if you plugged that in and simplify, you'll get this. So to summarize, at least one node sends and receives n squared over root P words. Let's connect this back to the big picture of this segment. This analysis is about the volume of communication seen by at least one processor. In terms of the time to do communication, it allows you to conclude that the bandwidth term should have this form. At least one node sends this much data. And it can only send it at a rate of beta. That's because beta is the minimum time per word. And your lower bound tells you the minimum number of words that at least one node must have sent. So that gives us a lower bound on this beta term. That's super cool. The only question now, is what is this? So far, you've shown the following lower bound on the running time of matrix multiplied with respect to communication. It assumes square matrices and, furthermore, that every node holds an equal part of a matrix operation. Given this information, can you figure out what goes here? Put another way, can you determine a lower bound on the number of messages that a node must send? I want you type your answer symbolically. You can use our usual shortcuts like square root of x for root x, or x carat y for x to the y. Remember the main idea of the lower bound analysis. You computed the minimum volume of data sent by at least one node. So if a node sends n squared over root p words, how many messages is that? Well remember every node has a memory of size M, so that means the largest message that a node can send is M. That means the number of messages is at least the total volume divided by M. So simplifying, you'll get this. Since that's the minimum number of messages that the node can send, that becomes the alpha term. Let's recap where things stand by connecting the idea of the lower bound to the one d-block row algorithm and the two d-suma algorithm. First, there's the one d algorithm, who's communication time is this. Then, there's the two d-suma algorithm. Remember, there are two variants depending on whether you assume a broadcast is tree based or bucket based. Also remember that the suma algorithm has a tuning parameter, a little s. At the maximum value of s, a lower bound on the communication time would be this. So what I've done is taken the maximum value of s, plugged it into these two parameters, and just taken the best case latency and bandwidth bounds. This way we don't have to fuss with tree based versus bucket based analyses. So, these are times for concrete algorithms. Then, there's the lower bound on communication. So, relative to the lower bounds, SUMA looks pretty good. Matches in the beta term, and is just off by a little bit in the alpha term. So, can you do even better than SUMA? In fact, you can, using an algorithm called Cannon's algorithm. It has a communication time that exactly matches the lower bound. It's an impressive algorithm in the sense that it predates the lower bound analysis. In fact, Cannon developed the algorithm as part of its PhD dissertation in 1969, groovy baby. I wish Mike Myers was here. Unfortunately, Cannon's Algorithm suffers from a few restrictions that make it not quite as easy and practical to implement compared to the SUMMA algorithm. Anyway, if you want more details head to the instructor's notes. Now what about beating the lower bound? The lower bound analysis makes a critical assumption. Can you spot it? The assumption is this one. That is, you assumed you only had enough memory on each node to store n squared over p data. Recall the cube of multiplications concept. This assumption is akin to distributing surfaces of the cubes across nodes. So a good question is whether having more memory would let you replicate some data, and thereby reduce communication. One example of such a scheme is a three dimensional algorithm. It says rather than distributing the surface is why not distribute the volume. That is suppose you took the nodes and arranged in them in a 3-D mesh, instead of a 2-D mesh. So suppose you have capital P nodes now put them in a 3D mesh of size cube root of p on a side. You could then assign chunks of this n cubed work to each node. Each chunk consists of n cubed over p multiplications. Ideally you'd like to set up this scheme in such a way that every node can update its cube concurrently. Now to do that you're going to need to have cube root of P copies of each submatrix. For instance, consider these three processes. So you either need to predistribute copies of the matrices, or depending on how the operands are allocated, do some kind of broadcast. Here's what such a broadcast might cost you. In this case, i've assumed a tree-based scheme, and of course, you'd incur the same cost to ++broadcast b and c. Now, once all the upper ends are replicated, you can do local multiplications, and depending on where you want the final results to be, you might need to combine the copies of C. You can, of course, do that with a reduction, and since reduce is the dual of broadcast, the overall communication cost still matches the broadcast cost. At least asymptotically. You could also do an all reduce to get replicated results within a block column. So let's compare this to our 2-D lower bound. If you have enough memory to replicate by a factor of cube root of P, you can get a cube root of P reduction in the communication time. So what if you don't have this much storage? Well, you could imagine a hybrid or 2.5-D scheme. Rather than full replication, you could use partial replication. That could reduce the memory requirement, but perhaps at a cost of a slightly increased communication time relative to the 3-D case. But wait. You know what? Let's save the two point five d scheme for another time. Like, an exam! Oh, no, not an exam! This point you're probably sick of matrix multiply, but I can't say enough what an important computational primitive it is to study. Now I say that, not only because I think it has important applications, it's also because you can use it to see a lot of different analysis techniques. For instance, in this lesson you saw different algorithmic approaches like 1-D versus 2-D versus 3-D distributions. As well as a way to argue about communication lower bounds. It won't always be easy to do those sorts of analyses on other algorithms, but I hope you'll try. Another reason to think about matrix multiply is that for better or for worse, the machines we build today, if you're a wee bit cynical like me, are tuned for it. For instance, you should have noticed something. When the problem is big enough, there's a lot more work than communication. That means matrix multiply ought to scale well as systems and problems get bigger. It's in that sense that things like the top 500 list, which measure peak capability, might not translate to more communication-intensive applications. Now speaking of big, and I admit this fact as a bit of a non sequitur. Did you know that a Rubix cube has 43 quintillion 252 quadrillion 3 trillion 274 billion 489 million 856 thousand possible configurations? Try telling people that fact at your next cocktail party, and I'm sure you'll be a really big hit. Just like me, because I'm a huge hit at cocktail parties with my numbers and my constants out to ten digits and stuff. Very cool. This lesson is about sorting in a distributed memory system. [SOUND] I know, I know. Why we gotta sort all the time? Sorting is worse than oatmeal cooked with water. [SOUND] [LAUGH] Sorting is the opposite of fun. [LAUGH] I'd rather jab myself in the eye with a hot poker than learn yet another sorting algorithm. [SOUND] Sorting gives me a rash. Sorting. Okay. Are you done whining? Sorting is really important. Not because you're going to go through life with people asking you to write sorting algorithms all the time. It's partly because it's a fundamental primitive. for example, every time a map reduce computation executes somewhere, data is being sorted. But more importantly, you should pay attention to the sorting lesson because it's good practice, for learning how to think about algorithms in general. And since it's a core part of CS101, I think it's really helpful for you to see how to sort in different computing environments. For example, sorting sequentially, versus sorting for a vector parallel system, versus sorting for a shared memory system versus, of course, distributed memory systems. So, please do your best and stick with it. I think you'll be pleased to learn some really neat ideas, especially when you compare them to the ideas about sorting that you probably already know. The most important thing about a bitonic sort is that everything boils down to by doing bitonic merges efficiently. Let's start with an network for a 16-element bitonic merge. I'm going to draw it in a simplified form as follows. The left-hand set of dots are the inputs. Now remember that the first step of the algorithm is a bitonic split which is computed in place. I'll represent the split using this next column of dots. So for example, this dot is the part of the split that computes the min of this input and this input. The max you would, of course, compute here. So these edges indicate the dependencies between these two outputs and these two inputs. Now the remaining pairs of dependencies would look the same. The result is two bitonic subsequences at stage one. So here is one bitonic subsequence and here's the other. Now remember we're talking about a bitonic merge so the computation in dependencies will repeat within each subsequence. For instance, here's the first pair from the first subsequence. Within this first subsequence, the pattern repeats. And we will see the same thing within this second subsequence. Let me fill out the rest to the end. So this picture of a bitonic merge shows us where the inputs are, where the outputs are, and the pattern of dependencies. So what does a distributed algorithm look like? For the distributed case, let's divide the elements among processing nodes. For example, here's one division. So in this example there are four processing nodes and I've divided them by assigning consecutive sets of inputs to each node. So let's think about this. Where does communication happen? Well, anywhere a dependence edge crosses a process boundary, there has to be some sort of communication. For example, consider these edges. According to this picture, this process has to send these elements to this process. Similarly, the same process has to send data back to the first process. This pattern is sometimes called a binary exchange, as there are two processes swapping data with one another. Okay, you ready for something cool? Notice that edges cross process boundaries only up until this point. In fact, there are just log P stages in this first block, which means only log P rounds of communication. In the remaining log n over P stages, no edges cross process boundaries, meaning there is no communication. So from this analysis I hope you can see how to estimate the communication time for bitonic merge. Now, this isn't the only way to distribute data and work. Here's another scheme. Here I've assigned each row of the network to a different process in a round-robin fashion. The scheme is sometimes called a cyclic distribution or cyclic assignment. Although it's a little bit harder to see, the communication versus local computation phases are similar to the previous block distribution scheme. For example, consider this input. Let's follow its outgoing dependences at every stage. During the first log n over P stages, there's only local computation. The remaining log P rounds require non-local exchanges. The patterns you've seen so far tell you something about the number of messages that need to be sent, but what about the volume of communication? Again, let's ask how much data does the first process send? You can verify that in each stage, this first process needs to send n over P words. This is the same as the block distributed scheme. Recall the communication pattern for a bitonic merge assuming a block distribution. Here’s my question, for the following list of topologies, tell me which ones would allow for a congestion free exchanges. For simplicity let's assume that P is equal to n. Here are your choices, choose all that apply. So the correct answers are the hypercube and the fully connected network. First remember that we're assuming P=n. So for this example picture there are 16 processes. Notice that the first eight processes have to exchange data with the second eight. Therefore you need a network with a linear or better bisection width. In this case, a hypercube and fully-connected networks are really your only options. In fact, a fully connected network is probably overkill, because it has many more links than you actually need. A hypercube, on the other hand, is a pretty good fit. Can you see why? Just think about how you build hypercubes, and how that compares with the basic pattern of exchange. By the way, there's another kind of network topology called a butterfly topology. The butterfly network is exactly this picture. Butterfly networks remind me of an old joke about Europeans comparing the word butterfly in their various native tongues. What is wrong with Schmetterling? Consider a distributed bitonic merge using a block distribution scheme. How much time is needed for communication, assuming a hypercube with P processes? I want you to type your answers symbolically here. You can assume that the problem size n and P are both powers of 2, and that P divides n, and furthermore, let's assume our usual alpha-beta cost model for messaging. When you type your answer, use little a for alpha and little b for beta. Here is the answer. Remember that the blocked binary exchange scheme only communicates during the first log P stages. And each process has to send n over P words at each stage. So that's n over P words at each stage, the cost of sending a message os size n over P. And log P stages of communication. So far, you've derived two potential schemes for a distributed Bitonic Merge. The first is a block distribution scheme. Remember that it is log P stages of communication followed by log n over P stages of purely local computation. The second scheme was the cyclic scheme. It has log n over P stages of purely local computation followed by log P stages of communication. Now, the running time for the two schemes is the same. It's basically alpha log P plus beta times n over P log P. Now, looking at these terms, you might worry about the beta term. If n over P is really large and the beta term dominates the alpha term, then it looks like you're paying to send n over P words log P times. So, a natural question is, is there an alternative that would let you reduce the cost of the beta term, possibly at the cost of increasing the alpha term. In fact, there is. First, let's start cyclic. Starting cyclic means there's no communication initially. Then, let's switch to block. Switching to block means that at the end there's no communication. Of course, to make this scheme work, you're going to need to reshuffle the data in between. This reshuffling is called a transpose. You can view the transpose as either a matrix transpose or also as an all to all personalized exchange. Let's take the first process, for instance. Notice that it has to send p minus one messages, each of size n over p squared. Notice that it has to send these messages to each of the other processes. The same thing goes for every other process. For example, let's look at the last process. So how does this scheme compare to the block of cyclic schemes. Again let's recall the block and cyclic communication times assuming a hypercube. Now since the transpose needs to do an all-to-all let's make a stronger assumption, which is that the network is fully connected. In that case, computing the cost would be really easy. So if the network is fully connected, and then each process will send (P- 1) messages, and the messages will be of size n / P squared. So as you can see, each scheme exhibits a latency bandwidth trade off relative to the other. So here we get rid of the log P factor on the beta term at the cost of increasing the alpha term factor from log P to P. Now in practice, it's typically very hard for the block or cyclic schemes to actually beat the transposed schemes for typical values of n over P. And that's true even given the much stronger assumption of a fully connected network versus a hypercube. Hm, can you think of any reasons why that might be true? You've derived several schemes for a distributed bitonic merge. But in fact this computational pattern of a butterfly network appears in at least one other very famous algorithm. Sadly, that algorithm was lost on the way to developing this course. I wonder if you've seen it, though. Do you know what algorithm I'm talking about? If you do I want you to type its name here. If you don't then just type in a funny joke. Here's the algorithm I had in mind. It's the FFT, or the fast Fourier transform. It's used in lots of places, including signal and image processing, turbulence simulation, molecular simulation among many, many others. And, since you've mastered bitonic merge, you basically know how to parallelize an FFT as well. Indeed, there is a fantastic lesson on the FFT in another OMSCS class. We'll put a link in the instructor's notes. As far as jokes go, here's one. Oh dear, that is really awful. I think even a tenured professor can get fired for telling a joke like that. You're going to edit this out, right? For completeness, I want you to work out the total overall running time of a distributed Bitonic sort. Let's start with computation and ignore communication. Here's a picture to remind you of what a whole Bitonic sort looks like in the circuit model. In this case, for an input of size 16. Now consider this first stage, you'll notice it's a bunch of Bitonic merge operations on pairs of inputs. Now what this stage will produce are Bitonic sequences of length 4. So that's stage 2. Let's look at stage 2. It similarly consists of Bitonic merge operations, this time of length 4. And of course what it produces are Bitonic sequences of length 8. And, of course, this pattern repeats. This third stage uses Bitonic merges of size 8 to produce a bitonic sequence of length 16. Now, what about the last stage? The last stage is just a single Bitonic merge of length 16. What it gives you is the sorted output. Now, notice there are log n merging stages. And remember what each box looks like. So let's consider some stage K. In this case, K equals 3. It performs simultaneous merges of size 2 to the k. Now suppose there are p processes and you use a block distribution scheme. In this example, P equals 4. Each process owns n over P elements. Now the kth merging stage performs K x N over P comparisons, and if each comparison costs tao time units, then the total time to do the comparisons is tao n over P x K. That, of course, is at merging stage K. Now recal there are log and merging stages. So you need to sum this cost over all stages. And if you work it all out you should get this. This cost makes sense, right? For instance, you already knew that bitonic sorting was not work optimal by a factor of log n compared to the usual n log n comparison scheme. But at least all the comparisons are perfectly paralyzable. Hoot. So this is the parallel cost of bitonic merge for a computation. Now what about communication? Let's assume the usual alpha beta cost model for sending messages, and let's further make the nice assumptions that n and P are powers of two and that P divides n. Let's further assume that we use a block distribution. That's in contrast to say, a cyclic only or hybrid block cyclic scheme. My question is, how does the communication scale in a big O sense? For the alpha term, choose from among these options. And for the beta term, there's an n over P multiplier, and I want you to choose the additional factor, if any. Here's the answer I was looking for, log P for the alpha term, and log squared P for the beta term. The exact calculation isn't hard, but it does have some tedious algebra. So what I'll do is just walk you through the setup, if you want the details, see the downloadables section for this video. The key question is where does communication happen? Recall that the bitonic sort network is a sequence of bitonic merging stages. These stages start small and grow larger. Let's number these stages from 1 to log n. Now pick some stage and call it k. What is the size of each bitonic merge? Well, it's 2 to the k. Now, suppose you use a block distribution with P processes. Notice that you only need to communicate when the size of the merge exceeds n over P, which is the amount of data per process. In other words, there's no communication here, and you only start communicating at this stage. Now, in one of these stages where the merge is spread across multiple processes, how many processes are interacting? Here's one way to express the number. As a sanity check, notice that when k is equal to log N, then this thing simplifies to just P, the total number of processes. Now, bitonic merge of size M on P processes, takes this much time to communicate. We just need to sum this time over all the stages in which communication happens. Remember you only communicate when k is greater than log n over P. The tedious algebra part is just plugging this into this. So if you plug in and simplify you should get this. You may remember from CS101 that any comparison based algorithm for sorting scales like this. That is in the sequential case. And in fact, you can't go any faster, asymptotically, if your only primitive is a simple comparison. [SOUND] But that's not the only tool in your sorting toolbelt. For instance, in CS101 you might have learned about algorithms like Bucket sort. Me hunter, not gatherer. Bucket sort can achieve linear time if you set it up just right. Here's the basic idea. Start by assuming that you know the range of possible values. So these might be integers from, say, 0 to m minus 1. Further assume that the values you want to sort are uniformly distributed over this range. You would then start by dividing the possible values into a bunch of bins or buckets. Each bucket will hold a subset of the possible values. For instance, this first bucket will hold the first m over k values, assuming that k divides m. The second bucket will hold the second set of M over K values, and so on. Now suppose we have some values to sort. A bucket sort first figures out to which bucket each element belongs. So for instance, maybe it turns out the three smallest elements belong in the red bucket. And you'd do the same for all the other colors. The last step is to sort within each bucket and then concatenate the results. So how is this a linear time scheme? Start by asking yourself how many elements are in each bucket. Now suppose there are n elements and they really are uniformly and randomly distributed over this range. Then the expected number of elements per bucket will be n over k. Now, there's a constant which I've hidden. Now within each bucket, you can use your favorite sorting algorithm. Tiny bubbles. Something, something. Let's supposed you use an asymptotically optimal comparison based sort. Then the expected run time would be n over k log n over k in each bucket. Since there are k buckets, you just multiply the time to sort each bucket by k. So, if you can choose k to be some constant times n, then you get a linear time sort. Now there's one more minor, technical detail here about the choice of k, but for that, see the downloadable section. Other than being linear time, the nice thing about bucket sort is that it's easy to distribute, at least in principle. Just assign each bucket to a compute node, assuming you have say k = p of them. Now let's assume that the n input elements are also equally distributed among these nodes. Then you'd expect there to be about n over p elements per node. Lastly, let's also assume that all the nodes know all of the bucket ranges. Here's my question. What is the running time of a distributed bucket sort in a big O sense? By our usual conventions, let tau be the time to do a comparison or some other local work, and let alpha and beta be the latency and per word messaging costs. I want you to put your answer here. And I'm looking for something of the form tau times something + alpha times something + beta times something. Now since I haven't given you a specific algorithm, you might feel the need to explain the answer you come up with. So for that purpose, here's an extra little box. Use that box to explain your reasoning. Or if you're stuck and you just don't feel like explaining your reasoning, you can also use this box to vent about your boss or your frenemies. Here's my answer. How did I get this? I hope this argument will make sense, even though it's not completely precise. First, ask yourself what steps do I need to perform? There are really just three. First, each node needs to scan its local list of elements and decide which elements go where. All nodes can do this in parallel. And the local work is linear in the number of elements per node. Next, the nodes need to exchange elements. That's an all-to-all operation. In this case, every node has about n over P elements so in the all-to-all, you'll expect them to send about n over P squared elements to every other node. To get a concrete cost for the all-to-all, let's assume a fully connected network. Oh, that assumption is good fodder for this box. In this case, the all-to-all will have this cost. Now the last step is just the local sort. Assuming we do a sequential bucket sort within each node, this step could have linear cost. Voila. Bucket sort is a really neat idea but it has one big problem. It's this assumption about a uniform distribution of input values. If that doesn't hold, then you won't get nearly equal numbers of elements in each bucket. By the way, for some really fascinating examples of non uniform distributions that somewhat mysteriously arise in real life, please go look up Benford's Law. So to build a robust linear-time sorting algorithm for the distributed setting, you'll need a better idea. Blam-o! That better idea is sampling. Not that kind of sampling, though my inner grad student does enjoy the occasional freebie. Indeed, all of the state of the art sorting algorithms that run at scale use some form of the following idea, which is really easy to state. Basically, you'll still do a distributed bucket sort. But instead of equal width intervals, let the interval widths vary according to the data. To decide those widths, you'll use sampling. Here's the algorithm on an example with 3 processors and 24 input elements. First, let's assume the elements are initially equally distributed among the three processes. Then, each process will sort its elements locally. So after the local sort, you'll get this. Next, each process will pick a sample of P-1 of its elements. It should choose these elements to be equally spaced from the sorted lists. So for example, we might choose these. Each of these subsets are the local samples. Next, let's gather all the local samples onto some root process. Then, let's have the root process sort the samples. >From the sorted sample, let's choose a final sample of P-1 elements. I don't know, let's pick these two for example. These become what are called the splitters. The splitters define the global bucket boundaries. For instance, using these splitters, we might assign the first process to the 0 elements, the second, to elements 1 through 3, and the third, from 4 to the end. Now having chosen the splitters, we need to broadcast the splitters so everybody knows who owns which range. Now each node can partition it's local elements using the splitters. Now all the nodes can exchange values. Every node now only has elements in its bucket. Finally, each node can sort its local bucket, thus completing the algorithm. Now a good exercise is to go back and compare this result to what you would have gotten if you had assumed a uniform distribution of values. That is, divide the intervals 0 through 9 among three buckets, two of which have three elements each and the last having four. You should find that the sample sort result is better balanced, at least by a little. Assuming the sample represents the full distribution well, you might guess that the asymptotic cost of a sample sort are similar to a bucket sort. That is true, but only sort of. [SOUND] Sort of? Get it? Ha-ha. Suppose there are P processes in the system. With respect to P, what's the largest asymptotic scaling term in the running time of a distributed sample sort? That is, if you were to compute the running time of the sample sort and you looked at all the factors that involved P, the largest factor will have which of these options? The answers I was looking for are either P squared or P squared log P. Why? Take a look at steps three and four. The root has to sort how many samples? Well, it's P times P minus 1, which is about P squared. So then sorting these samples would cost you either P squared or P squared log P, depending on what local algorithm you assume for the sort. The main thing to remember is this fact of a P squared factor, could be a severe limiter to scalability if the system is truly, truly massive. You might know that there's an annual contest to implement the fastest sort. Visit sortbenchmark.org for details. If you didn't before, you now have the chops to enter and kick butt. Now, that sorting competition focuses on disk-to-disk sorting. But if you go read about the basic schemes that different teams have used over time, they're basically variants of the sampling or the bucketing schemes. And now that you've finished this lesson, you know what that means. Well, more or less, hopefully more. As of September 2014 the fastest system managed to sort seven terabytes in a minute. Now compare that to the first year of the competition, 1995, when a team sorted just over 1 GB of data in a minute. That's a 7,000 fold improvement, over 20 years, 7,000! Think about that, that is [BLEEP] crazy! Crazy! That means that the amount of data you can sort in a minute has doubled roughly every 1.6 years which, curiously enough, is right in line with Moore's law. This lesson is about doing a search or BFS on a distributive memory system. But we're going to approach the problem from the perspective of linear algebra. As it happens, there's a way to view a graph as a matrix. And since we have a pretty good understanding of how to do linear algebra in distributive memory, we can exploit this mapping to get a scalable algorithm. Now one suggestion I have for you before we start is for you to quickly review some of the ideas from our other lesson on distributive matrix computations. You'll use several of those ideas here, too. When thinking about graph algorithms in the distributed setting, it will often be very helpful to look at a graph as an adjacency matrix. Now, if you haven't seen that before or you just need a refresher, here's a crash course on how it works. Start by giving each vertex an integer label. Next, create a matrix to represent this graph. Let's call it A, with row and one column for every vertex. Now for any edge that goes from a source vertex i, to a destination vertex j, we'll put a one in the ij entry of a. Otherwise, if an entry is empty, assume that it has a value of zero, which means no edge. For instance, let's start with vertex zero. It's adjacent to vertices one and five. So, let's go to row zero and place ones at entry one and entry five. Now, this graph is undirected, so both rows one and row five will also have a one in column zero. Put another way, the matrix for an undirected graph is symmetric. In linear algebraic terms, we say A is equal to A transpose. Now, let's do the same thing for vertex one. It's adjacent to vertices one, two, and six. So let's mark those entries of A. And of course we'll update the symmetric image. When you're all done you should get this matrix. In general consider any undirected graph, call it G. Let's suppose that it has n as in Nancy vertices and m as in Michael edges. It's adjacency matrix will be n by n. And since it's undirected, it will be symmetric with two m non-zeroes. Now consider a directed graph. I want you to number the vertices, and then, fill in the entries of its adjacency matrix. Do that by typing a 1 in the appropriate entries. If you want, you can insert 0 entries, where there are no edges. But if you're feeling lazy, feel free to leave any entry blank, and we will interpret that as a 0. The vertex numbering is arbitrary, so any choice will do. Here's one. Given a numbering, just go through each vertex, and, in the corresponding matrix row, record its outgoing neighbors. For instance, I'll start with 0. Its outgoing edges lead to 1, 3, 5, and 6, so you can record these in Row 0 of the matrix. Repeating this process should lead you to the following matrix, or, if you did a different vertex numbering, some permutation of the rows and columns. By the way, you might notice that this matrix has three empty rows. These correspond to vertices with no outgoing edges. Similarly, there are two empty columns. These correspond to vertices with no incoming edges. Consider this directed graph and its adjacent matrix. Suppose we treat the matrix as a boolean matrix. That is every one value is true, and all zeros or blanks are false. Let's call this matrix capital B as in buzz. Now suppose you want the undirected version of this graph. How do you compute the boolean adjacency matrix of the undirected graph, given the directed adjacency matrix B. Assume you have the following logical explanations to help you out. Not of x takes a boolean matrix x and converts every true entry to false and every false entry to true. And takes two boolean matrices x and y and computes the element y's logical and of the entries. X and y, of course, have to have the same size. Or does the same thing as and except with a logical or in place of logical and. Lastly, trans of x takes the matrix x and computes its transpose. Type your solution in this box. Here's my answer. It's the logical or of B and its transpose. So, why does this work? Start with the transpose of B. Remember that entry ij of B is true if there's an edge from vertex i to vertex j. Transposing B puts that ij value at position ji. Thus, to retain both the edges ij and ji in the final matrix, you take the logical or. Your next task is to understand how to implement a graph algorithm in the language of linear algebra. Once you've got the hang of that, you'll be ready to go distributed. Go distributed! Yay! Let's start with a level synchronous variant of BFS. BFS, not an acronym for best friends sometimes. Recall the setup. We're given a graph G of vertices and edges. And you're also given a start or source vertex S. What you want to compute is the minimum distance of every vertex from S. For example, suppose S equal zero. Initially S is a distance of 0 away from itself. So I've labeled S with its starting distance and I've put an entry in the distance vector. All the other vertices which are unvisited start infinitely far away. The level-synchronous algorithm processes one level at a time starting at level 0. Now for any given l, there's a frontier. The frontier consists of all unvisited vertices which are a distance l away from the source. In this case, level 0 contains just the source vertex 0. Processing the level means visiting all of the frontiers unvisited neighbors, in this case the neighbors are 1 and 5. We can update their distances with the current distance plus 1. These previously unvisited neighbors now become the frontier at the next level. You've just lathered and rinsed, so now repeat. Look at all the unvisited neighbors of 1 and 5, which are 2 and 6. Update their distances to 1 plus 1 or 2. Now 2 and 6 become the new frontier. Update the next set of unvisited neighbors which are 3, 7 and 8. Then they become the new frontier and, in this example, just once more. So, what's the cost of this procedure? Let's consider just the sequential case. Suppose there are n vertices and m edges. Then the sequential running time will turn out to be big O of m plus n. That's because you might, in the worst case, visit every edge once or every node at least once. Now I want you to translate the level synchronous breadth-first search algorithm into the language of matrices. Consider some unvisited vertex, i. As the BFS is running, there's some frontier of vertices at level l. You should ask yourself, self, is there any edge from the frontier to I? Because if so, then I should update i's distance. By the way, if you're actually asking yourself this question, you should be worried. Only crazy people talk to themselves about BFS late at night while sitting in bed or otherwise staring at a screen. Now remember, if there's any edge from vertex j to vertex i, then there'll be an entry of 1 in the adjacency matrix, a ji. That is, think of it this way. Consider a column i of A. Also consider the frontier at level l. Here I'm showing the frontier as a boolean vector with 1 entries wherever there's a vertex from the frontier. Otherwise, assume an entry is zero or blank. The question was, is there any frontier vertex that points to i? That's the same as asking whether column i has a 1 in its corresponding entries. If that's the case, then you might want to record this fact. For instance, here's another boolean vector, call it u for update. You might mark position i of this vector as true, if any of these frontier entries of column i are also true. Now this check you've just done can be written algebraically as follows. You want to update i if any vertex j is in the frontier and points to i. This V is a logical or operator, and this caret is a logical and. I hope you recognize this scalar formula as a boolean matrix vector product. Now, being the crazy person who talks to yourself that you are, you might be saying, self, hold on now. There might be n vertices to update and up to n entries per vertex. So does this thing cost O(n squared) per level? If you implement this correctly, that won't be the case. In particular, for a sparse graph, meaning many of the entries are zero, you can maintain both the adjacency matrix and the frontier vertices with sparse data structures. This makes this product a sparse matrix times a sparse vector multiply. If you feel so inclined, you should head to the downloadable section and read up on exactly how to do this. If you implement this operation as described there, you can get a work optimal matrix based implementation of BFS. Now there's just one last detail, which is, how to go from the update vector to the updated distances. >From the update vector you can figure out which entries are unvisited. You would then update their distances and put them into the frontier for the next iteration. Again, exploiting sparsity, this update can be done in work proportional only to the selected vertices. Those notes in the downloadable section explain how. Here’s a graph and its corresponding adjacency matrix. Now suppose 1 and 6 are in the frontier. Which vertices might need to be updated? Here’s an update vector. I want you to mark the entries of the update vector u that might become part of the next frontier. Do that by typing a 1 in the appropriate entries. To get the answer, remember that you need to perform a boolean matrix-vector product. You'll focus on rows corresponding to the frontier. You'll mark an entry in u for any column in which either entry 1 or 6 are set. Here are those entries. So, let's just scan the columns and mark the right ones. The matrix view gives you an easy to think about distributing a BFS. Suppose you have P = 3 processes. You can do a 1-D partitioning of the matrix by, let's say, columns. That is, group these three columns together on one process, these three columns on the next, and these three columns on the last. This column partitioning corresponds to a partitioning of the vertices. So, what does this partitioning suggest? Well, it assigns vertices 0, 1, and 2 to one process, 3, 4, and 5 to the next, and 6, 7, and 8 to the last. Note that the column partitioning also implies a partitioning of u. That's u the vector, not you the viewer. What about the frontier vector? This partitioning suggests that you'll need to replicate the frontier vector on each process so that the process can do its local update. Schematically that looks something like this. Now what happens if you replicate? Well, when you create the next frontier by analyzing the local updates, you're going to need to re-replicate the frontier vertices. Put another way, here's what happens when you create the next frontier. You use the local update vector to update the local frontier. You then need to do an all-to-all to exchange the local frontiers. Okay, that was a bunch of steps. So let's summarize the distributed 1-D algorithm. First, divide the columns of A and the entries of u. Then, do a matrix-vector product to determine which vertices may need distance updates. Then, update the local distances. Then, determine which of the local vertices need to be a part of the next frontier. Finally, do an all-to-all exchange so that everybody knows what the full frontier looks like. Notice that the all-to-all exchange is the only communication step. So, what does that cost? Unfortunately, it's actually hard to write down an exact communication cost because it depends on the structure of the graph. Despite that, we can say something concrete about its form. Because of the all-to-all, you should expect some term that is linear in the number of processes, P. There's a paper linked to in the instructor's notes. It has more analysis. And I'd encourage you to download it and take a look. The 1-D distributed breadth first search scheme has a communication cost that scales linearly in the number of processes. Here is my question. Suppose you switch from a 1-D scheme to a 2-D scheme. How would you expect this linear scaling to change in the 2-D case? Write and explain your answer in this box. Here's the answer I was looking for. Square root of P, rather than P. Why? With a 2-d scheme it might be possible to arrange the processes like so. This is a 2-d grid of size, say root P by root P on a side. I hope this idea reminds you of the collective algorithms on multi-dimensional process grids. With this kind of distribution, you only need to use collectives to merge local frontier updates within process columns and rows. So the big idea in this lesson was to do a BFS by recasting it in matrix terms. You did so in order to reuse your basic ideas from distributing matrix operations. This is a really neat connection, and you can take it even further. Now it might not always work, or work as well. So, here's a fun exercise to do at this point. Go read about or review other kinds of graph computations. Then ask whether you can do the same sort of mapping from graphs to matrices. For instance, is there a matrix based way to do a Depth first search as opposed to a breadth first search? What about all pairs shortest paths? Triangle counting? Computing betweeness centrality? Maybe others. For me, this lesson is about one of the most fun and interesting topics in parallel computing. Namely, the problem of graph partitioning. A common thread in a distributed memory algorithm is, how do you distribute the data? For instance, there's a lesson in this course on distributed breadth-first search or BFS. Distributive BFS needs a way to choose an initial partitioning of the input graph. Now that lesson left us choice of the black box. This lesson fills in the box. Before we start, I want to hint at one of the techniques which I happen to think is especially neat. It's called spectral partitioning. Spectral partitioning exploits the connection between graphs and linear algebra, but it also has an unusual interpretation, which it turns out is really kind of fun. It's a physics-based interpretation based on systems of springs. Let's motivate the graph partitioning problem through a different problem. Suppose I want to multiply a sparse matrix A times a vector X. Remember the duality between matrices and graphs. Meaning this sparse matrix has this equivalent graph representation. The rows and corresponding columns of the matrix are vertices and the non-zeros are edges. One way to do a distributed breadth first search is to use a computational primitive that looks like this linear algebra operation. Now, suppose you decide to distribute the work by first dividing the matrix row-wise. You would then assign block rows to processes. Note that this assignment corresponds to a partitioning of the graph. This example is a vertex based or just vertex partition. Observe that this partitioning usually also implies a partitioning of the vectors x and y. That's because there's a one to one mapping of vector entries to graph vertices. Now the amount of work in a sparse matrix vector multiply is proportional to the number of non-zeroes. So, when you divide up the rows, you might want to do so in a way that balances the number matrix non-zeroes per partition. So that's one goal. Now, you have another goal in choosing a partition. Hm, what is that goal? Recall that the vertex x is partitioned. Now suppose you want to update the first block of Y. Because of these non-zeros you're going to need these corresponding elements of x. That requires communication with the processes that own these elements of x. In the language of graphs, these communication exchanges happen anytime an edge crosses a process boundary. For instance, these two non zeros, correspond to this edge. The two processes that own the endpoints of the edge, will need to communicate. This observation suggests a second goal. To minimize communication volume, you prefer to reduce the number of edge cuts. This example motivates the classic graph partitioning problem. You're given a graph and a target number of partitions as input. Your goal is to divide the vertices into, say, P partitions. You'd like this partitioning to have the following properties. First, these partitions should cover all the vertices but should be disjoint. The partition should also be about equal in size. And the number of cut edges should be minimized. That is the graph partitioning problem in a nutshell. Consider a block row partitioning of a matrix for a sparse matrix-vector multiply. Recall your two computational goals. The first is to achieve load balance, and the second is to reduce communication. So this is where we started, then I introduced the graph partitioning problem. For this problem I said you want to divide up the vertices into partitions so that the partitions are approximately balanced. I also said you want to minimize the edge cuts. Here's my question. Is the graph partitioning problem as I formalized it, the same thing as the goals for a sparse matrix vector multiply? Choose your answer, yes or no. And then explain yourself in this box. As we've described it so far I would say not exactly. To see why, consider the following three way partition of a graph. Each partition is balanced in the sense of having two vertices each and, as it happens, no other partitioning of this graph will have fewer than the 9 edges you see cut here. Now this partition corresponds to a splitting of the rows of the sparse matrix. Notice the 1 partition has 10 non-zeroes, while the others have 7 each. In other words, the vertex counts per partition are the same, but the work is not. In fact, there's actually a partition that both minimizes edge cuts and exactly balances the number of non-zeroes. Can you find it? Alternatively, how can you augment the graph partitioning problem in order to model the balancing of work? As you might have guessed, graph partitioning is n p complete. So your first intuition about graphs is that you need to look for good heuristics and try to exploit special structure wherever possible. Here's a first simple heuristic. It's called graph by section and it's based on the principle of divide and conquer. The idea is simple. Suppose you want P partitions. Start by first using an algorithm to divide the graph into two partitions. Then divide each half into 2. You would repeat this process until you obtained the desired number of partitions. That's nice, but not very satisfying. After all, how do you get two way partitions? We'll get there but first let's build some more intuition. Now some graphs are special. For instance this graph is Planar. A Planar graph is one that can be drawn in the plane with no edge crossings. So for instance here's another Planar graph. By the way this thing is supposed to be a tapir. For Planar graphs there's a really neat theorem. If theorem is in fact due in part to Georgia Tech's very own Dick Lipton. Anyway, the theorem goes like this. Given a graph with n vertices, the theorem says that there's a way to partition the vertices into 3 disjoint sets, A, B, and S, such that the following super cool properties hold. The first one is that S is a separator for V. That is, there are no edges that directly connect A and B put differently, the subset of vertices S acts as A kind of wall between A and B. In other words, if you take the separator out of a connected graph, you get two or more connected subgraphs. That's the first property, here is the second. Each of the subsets A and B have no more than two-thirds n vertices each. That's really neat too. Because if one of the partitions has at most 2/3 n vertices, then the other has at least 1/3. That in turn means that the larger of the two partitions is no more than twice the size of the other. If the two partitions are within a factor of two of each other then that's pretty balanced. Well, sort of, maybe if you crane your neck a lot and look at a funny angle. Well a factor of two was good enough for government work dag nab it. The third property is the neatest. It says that the size of the vertex separator is about square root of N. For the grid or lattice graph, this result should make intuitive sense. A natural separator for this grid graph would be say, some row or some column of vertices. Such a row or column would of course be root n in size. Now the existence of a plane or separator does not guarantee you can minimize the edge cut efficiently. But keeping this lattice graph in mind an algorithm that can find the separator should heuristically find a decent partition. In fact Lipton and Tarjan in their original paper described a polynomial time algorithm to find such a separator. Anyway there's a ton of cool work related to more general separator theorems. Head to the instructors notes for some pointers. Here's an idea for a graph partitioning heuristic. Use breadth first search or BFS. In the following box I want you to tell me how. That is I want you to sketch an algorithm that uses BFS and type that algorithm here. If it helps assume that the input graph is connected, meaning you can assume there's a path from every vertex to every other vertex. I know what you're thinking, why do I have to do all the hard work and explain this to you? Isn't that your job? Oh, quit your whining, you big baby, and just answer the question. Let me describe one simple scheme and illustrate it on a sample graph. Pick any vertex as a starting point. Then, run a level synchronous BFS from this vertex. The BFS will assign levels to every vertex. Notice that every level serves as a separator. Because of this fact, you can stop the BFS after you visited, say, half the vertices. For this example, that midpoint is about here. >From the stopping point, you can assign all visited vertices to one partition and all unvisited vertices to the other. Now, that's not your only option for a stopping criteria. Can you come up with any others? Anyway, BFS based schemes tend to work pretty well on planar graphs, and they're fairly cheap. But there's at least one odd bit about using a BFS to do a partitioning. Weren't we motivated to solve the graph partitioning problem in order to efficiently compute a distributed BFS? Yet, we're using BFS to do that. Is it just me or does that seem perversely recursive? Dun dun dun! Oh no, recursion. The most well-known heuristic for graph partitioning is the Kernighan-Lin algorithm. By the way, the g in Kernighan is silent. That's gnu as in the wildebeest, not the the free software thing, which has a hard g, as in ga-nu. So it's gnu, not ga-nu. Got it? The idea behind the K-L algorithm is simple. Given a graph, divide the vertices into two subsets of equal or nearly equal size. Yes. Any split will do. I'm serious, any one you want. It's okay. Go ahead. Oh, for goodness sake, just pick a partition already! Let's define the cost of this partitioning to be the number of edges that go between V1 and V2. Now imagine that a magic bean hands you two equal sized subsets of V1 and V2. Let's call these subsets X1 and X2. Imagine that you were to swap these two subsets. Naturally you would expect the cut size to change. But by how much? To answer this question in a formal way, let's start with some definitions. Consider one vertex, let's call it a, which lives on one side of a partition. Define its external cost as the number of edges that land in the other partition. In this cartoon, a's external cost, denoted E 1 of a is equal to three edges. To find a similar external cost, E 2 of b for any vertex b that lives in V 2. Next, define the notion of an internal cost. The internal cost will count the number of edges from a vertex to other vertices that live in the same partition. >From these definitions, you can write down the cost of a partition as follows. It's the cost of the partition, ignoring a and b, plus the external cost of a, the external cost of b, and minus some constant. So what's this constant? You need that constant to account for a possible edge between a and b. The constant is one if there's an edge there and zero otherwise. But why subtract it? That is a terrific question and you should take a moment to ponder it. Now suppose you swap a and b. What's the new cost? Well, any edge that was incident on a that was previously external is now internal. Same thing goes for b. So to get the new cost, let's start with the old cost, ignoring a and b, and then add some adjustments. The adjustments involve the internal costs of a and b plus the constant again. Hey, what's with the plus constant, now? Now let's look at the change in cost. That's the old cost minus the new cost. After some algebra, you get this. The larger this change, the better, because it means a larger decrease in the cost. Let's call this cost the gain from swapping a and b. By the way, do you see that it's possible that gain of ab could actually be negative? Okay, it's quiz time. Consider the computation of the gain function, given a and b. Assume the following, first, every vertex is labeled by it's partition. You can access this label in constant time. Secondly, the maximum degree of any vertex is given. It's a constant, which we'll call d. Here's my question. How long does it take to compute the gain, given a pair of vertices a and b? Write your answer, in terms of, little d, n1, and n2, which are the sizes of v1 and v2, here. For your analysis, I'm interested in the asymptotic, or the big O sense. Here's the answer, it's just O(d) time. Why? Well you need to sweep over the neighbors of a and b. There will be at most d such neighbors. To determine if a neighbor is internal or external, you just need to check its partition label. And that's a constant time operation by assumption. By now, you should've gained everything you need to know to understand the full Kernighan-Lin Algorithm. Recall the setup. You're given a graph and a partition of its vertices, v1 and v2. You'll try to improve this partition by swapping two subsets of the elements, x1 and x2. So how do you choose x1 and x2? Here's the KL procedure. First, compute the internal and external costs for every vertex. Next, mark all the nodes has unvisited. Then, carry out an iterative procedure. I'll fill this condition in momentarily. Go through every pair of unmarked vertices, and pick the one with the largest gain. Mark that pair as visited. Now go through and update all the internal and external costs as if a and b had been swapped. Note that you aren't actually swapping a and b, just updating the costs as if you had done so. Then repeat this procedure until you visited all the vertices. At this point, the algorithm has computed a sequence of gains. That is, this is the first part of the algorithm, and the output of the first part is the sequence of gains. Now, for the next part of the algorithm, let's define something called the cumulative gain. That's Gain with a capital G. A capital G Gain is the sum of all the little gains up to a certain point, a j. Kernighan's and Lin's idea was to keep all the swaps that maximized the cumulative gain. If this cumulative gain is non-zero, then you have a candidate x1 and x2 subsets that actually improve the partition. You can then swap these two subsets and update the overall cost. Now for the last detail. You'll repeat the entire procedure we've just described, parts one and part two, until there is no more gain. In other words, if this condition is false, then the algorithm terminates. Oh, boy. That was a lot of steps, but hopefully the general idea is clear. Now the main issue with this algorithm is its cost. Its sequential running time is big O of v squared times d. Where again, d is the maximum degree of any vortex. There are actually some more complex variations of this algorithm that can reduce the per iteration cost to big O of the number of edges. Anyway, for more of my notes on analyzing the overall cost, head to the Downloadables section. So a different kind of partitioning strategy is known as multi-level graph coarsening. It's a form of divide and conquer. Let's start with a conceptual illustration. You start with an imput graph, pretend this is an input graph. You would then replace it with a smaller version. In the graph sense, the coarse graph would have fewer nodes and edges, but would somehow look like the original graph. We might repeat this a few times until you get a version of the graph that is small enough to partition quickly. You then partition the smallest graph. If you've done a good job preserving the relationship between the coarsest graph and its parent graph, then a split in the course graph will correspond to a roughly equivalent split in the fine graph. And you'd repeat this reverse step until you have a partition of the original. So that's the concept, but how do you actually coarsen a graph? Most schemes have the following flavor. First, identify at least one subset of the vertices to collapse or to merge. For instance, here's a subset of five vertices. I'm going to replace this subset with a single super vertex. Now remember that one of your partitioning goals is to obtain balanced partitions. So it's probably a good idea to track the fact that there were five vertices here that became one vertex. An easy way to do that is to assign a weight of 5 to the super vertex. You'll also want to track edge weights so that later on we can cut edges accurately. Let's see how that might work. Consider two vertices in a weighted graph. Remember, even if the original graph is unweighted, we repeatedly coarsen by aggregating nodes and edges and so will accumulate weights. In this example, wi is a function that maps every vertex to some weight. Similarly, the function xi will map edges to weights. Now, let's suppose that we join just these two vertices into a super-vertex. Let's call this new vertex s. Its weight is the sum of the weights of u and v. Now, what about the edge weights? The main case to think about is the following. Suppose there's a vertex t that's connected to both u and v. Then these two edges will become a single super-edge in the coarsened graph. So what is its weight? A natural idea is simply to sum the weights of the original edges. So the weight of the new edge is just the weight of the two old edges that got merged. The reason we do this is so that if we partition or cut this edge in the coarsened graph, we'll know that it maps to two edges in the original graph. Now before you move on, it's a good idea to check that this procedure makes sense to you. Okay, let's go back to the original sample graph and see what would happen by applying these rules. Consider this first merge group again. There are 5 vertices, which become 1 super-vertex. There are also several cases in which multiple edges should merge. Here is the result. Now, let's suppose someone gives us a sequence of merge groups, and let's coarsen the graph accordingly. [SOUND] Coarsen, coarsen. [SOUND] So this is the final result. That's the basic process of coarsening a graph. This final result kind of looks like something. Oink, do you know about Atlanta's PINK PIG ride? Consider this graph. Suppose you want to coarsen two subsets of vertices into super vertices. That is, suppose you want to merge these two vertices into a super vertex, and you also want to merge these three vertices into a super vertex. Structurally, you would get the following coarsened graph. Here's what I want you to do. I want you to compute the weights of the vertices and the edges in the coarsen graph representation. Type your answers into the boxes. These red boxes are for edges, and the brown boxes are for the two super vertices. Assume that all vertex and edge weights in the original graph, on the left, are equal to 1. Let's do the vertices first since they're easy. These two become one, and these three become one. So we should have entered a 2 in this box and a 3 in this box. For the edges here are the answers. The interesting cases are this edge, which comes from these two edges, and this edge which comes from these three edges. This one on the left is probably easy to see, because it's the simple case of, well, there are two vertices connected to the same vertex. The other case might be a little bit trickier to see, but it's the same principle. You could, for example, imagine taking these three vertices, collapsing them into a single vertex. In which case, now I have a single super vertex over here connected to these two guys. If it helps, let's take a look at an intermediate graph. So what I'm showing here is the graph you get if you first collapse these two in a super vertex, but before you collapse these three. In this case notice that these two edges would combine into this edge. So there should be a two here. I hope you can now easily see that when you collapse these three vertices, these two edges will have to combine. So that's two plus one, or three. To give a complete algorithm, for coarsening a graph, you need a scheme to decide which vertices to combine. One idea is to compute a matching. So what's that? Here's a formal definition, a matching of a graph is a subset of edges, such that, no two edges have a common end point. It's, basically, an independent set, but for edges, as opposed to vertices. For instance, take these three highlighted edges. They form a matching because they don't share any end points. That is, suppose you wanted to add this edge to the matching, bad OMSCS student. You can't add this edge because it would touch an existing edge of the matching. So that's a matching. What's a maximal matching? A maximal matching is one to which you can't add anymore edges. In fact, this matching is maximal. In fact, if you go through all the remaining unmatched edges, you'll find that you can't add any of them to the current matching. I want to contrast maximal to maximum. A maximum matching is the largest matching, for example, this graph has at least one more matching, with more than three edges in it. Can you find it? Here's one, though there might be more. Okay, back to maximal matchings. Maximal matchings are easier to compute, so we'll focus on those. Indeed, this is a good time to check that you understand the concept of a maximal matching. Consider this graph. Let's call it G. Suppose I tell you that I computed a matching in order to generate a coarse graph, G hat. Here's G hat. Here's what I want you to do. Find a maximal matching in G that would lead to G hat. Do that by checking off the edges of the matching in G. For instance, if you wanted a matching that included this edge and this edge, then you would check off this edge and this edge. Here's one solution, the matching consists of these fat, fuzzy red edges. To see it more clearly, let me highlight the edges of G, and mirror them in G hat. Take a moment to compare the left and the right-hand side and make sure you're convinced that, in fact, this is a valid matching that leads to this. I want you to derive a conceptual fact about using maximal matchings to coursing a graph. Consider some initial graph, G0. Now suppose you find the maximal matching unit and use it to course it. And let's say you repeat this process producing a sequence of graphs from 1 to k. Now, let's also say that the original graph has n vertices. And suppose the final graph has s vertices. Here's my question. How large must k be in terms of n and s? In other words, I'm interested in a lower bound on the asymptotic value of k. You can type your answer symbolically here. Here's the answer, log (n/s). Why? Imagine the best case scenario. A single run of the maximal matchings algorithm will match every vertex. So, for instance, here's a sample graph, with eight vertices, and notice that under this particular matching, every vertex is part of a matched edge. This also means that this graph with eight vertices will, when coarsened, become a graph with four supervertices. Okay, so let's turn to the general case. How many vertices must this graph have? Compared to the previous level, it has to have at least half as many vertices. This idea has to hold at every level. In other words, relative to the original graph, the final graph has to have n/2 to the k or more vertices. So what does this tell you? Well, it says you need at least a logarithmic number of coarsening steps in order to shrink the graph. Of course, this is a lower bound on the number of steps. So you might need more if for some reason it's hard to find a lot of large matchings in the graph. In fact, can you think of a worst case graph to coarsen? Maximal matchings are a useful tool for getting a coarsened graph. All we need now is an algorithm to compute one. Let me describe a simple scheme to you based on randomization. At each step of this scheme, you'll choose any unmatched vertex at random. So initially, no edges are matched, so let's pick any vertex. How about this one? Next, you'll match this vertex to one of it's unmatched neighbors. Again, since we're just starting, all of the neighbors are unmatched. So how do we pick one? Well, there are lots of strategies. For example, you could just pick one at random. A different approach is something called a heavy edge matching strategy. So heavy. So heavy. The idea is to look at the unmatched edge with the highest weight and then choose it. For instance, suppose these are the edge weights. Then among the two unmatched neighbors, this edge has the higher edge weight, two versus one. So you'd choose it. But why is the heavy edge matching strategy a good one? In fact, there's not a whole lot of definitive rigorous analysis out there. But there is a lot of experimental evidence. What I want to do now is just give you some informal intuition. Consider some graph Gi. Suppose we were to match these two edges. Contracting them would yield a course in graph. Let's call it Gi + 1. Note the edge weight of two that connects the two super vertices. Now, if you were to sum the edge weights in the original graph, you'd get 12. That's because there are 12 edges each of unit weight. Here let me use the notation X of some object to count the number of edges in that object. Now the sum of the weights on the final graph is 10. This is just the total edge weight of the original graph minus the edge weight of the matched edges. Here, let Mi denote the matching. Now, the strategy of selecting heavy edges will tend to try to increase this factor. And if this factor is bigger, then this factor will be smaller. In other words, the heavy edge heuristic is really about increasing this term in order to decrease the overall edge weight in the next graph. The heuristic idea is that the smaller this term is, the more likely it is that all the edges you'll cut will be small. Again, this is clearly not a rigorous proof. In fact, there are a ton of other heuristics. But most of the evidence is empirical. Anyway, if this is a topic of interest to you, head to the instructors notes or the class forum for pointers to papers that will discuss all these issues in a lot of detail. Here's a graph for which I computed some mapping. If you were to course on this graph based on this matching, here's what you get. Now, suppose I use the Lipton and Trajan result to identify some vertex separator in this course graph. Hey, that's not too shabby. The two halves are pretty balanced. The total vertex weight of the upper half is eight as is the lower half. To get the value of eight, remember that each of the super vertices corresponds to two vertices of the original graph. So then counting up you'd get, say, 2 plus 2 plus 2 plus 2, that's 8. And 2 plus 2 plus 2 plus 2. Now, to make the mapping from the fine graph to the coarse graph a little bit easier to see, let me highlight the matched edges and their correspondents in the coarse graph. Here's what I want you to do. Go back to this original graph. I want you to click on all the vertices and edges associated with the separator in the coarse graph. Okay, so what does that mean? For example, consider this edge in the coarse separator. In the original graph, this super vertex corresponds to these two vertices plus this edge, so you'd mark them. The other coarse vertex corresponds to this vertex, so you'd mark it too. Lastly, this edge maps to this edge. Check. In other words, the first course graph separator edge corresponds to this subgraph. Okay, now I want you to try. By the way, what's the point of this exercise? Well, I want you to discover by example, a subtle detail about what partitions in the course graph mean for the fine graph. Here's what you should've marked. This set of vertices and edges in the original graph is called the projected separator. Let me first point something out about this separator, and then I'll walk you through how I got it. See this triangle here? It includes two unnecessary edges. You really only need this sub-path. So the main point is that if you use the coarsening technique, you will probably need to refine the partitions that you get. The readings discuss this point in more detail. Now, in case you didn't get the same separator, let me walk you through the process. I did the first separator edge for you, so let's move on to the second. This edge comes from the original graph. This edge comes from the super vertex. Okay, what about the next edge? It's basically the same thing. This next edge is the interesting one. The separator edge maps ambiguously to these two edges of the original graph. The last two separator edges are similar to the first few. Suppose I give you this course graph. It happens to have a perfectly balanced partitioning, with an edge cut of four. There are other balanced partitionings of this graph, but none with a lower edge cut. Here's my question. Will this edge cut necessarily minimize the edge cut of the next finest graph? The claim says yes. I want you to choose. Is that statement true or false? The answer is false, here's an example, here's a fine graph that maps to this course graph under this matching. Notice that the matching is even maximal but look at the projected cut. This cut is sub-optimal. It would have been better to cut the graph down the middle, the important point to remember is that corsening is a heuristic. An interesting follow up question might be well, what if you had coarsened based on a maximum rather than a maximal matching? For me, one of the most fascinating classes of graph partitioning methods are the so-called spectral methods. That's spectral as in rainbows, not ghosts. Spectral methods go back to the linear algebraic view of graphs, and they have what, for me, is a surprising physics-based interpretation. We'll get there, but first, let's remind ourselves about the connection between graph structure and certain matrices. Consider an unweighted, directed graph such as this one. Let's call this graph G. Now let's represent it, not by its adjacency matrix, but by an incidence matrix, C of G. In this form, each row is an edge, and each column is a vertex. How do you mark edges? Let's take this edge as an example. It points from vertex 0 to vertex 1. Let's make it the first edge in the incidence matrix, and let's mark it using the following convention. We'll put a positive 1 at the source vertex, and a -1 at the sink. Let me fill in the rest. >From this incidence matrix, let's construct a new object called the graph laplacian. We'll define it as the product of C transposed times C. To see what this product is, I want you to compute it algebraically and then interpret it. Start by writing C as a sequence of its m edges. Here's C shown symbolically as a list of row vectors where each row vector corresponds to an edge. Here's what C transposed times C would look like. Let's multiply this out. That is, C transpose C is the sum of a bunch of products. Each product is an edge, represented as a column vector, multiplied by itself represented as a rho vector. To see what this is saying, let's take a closer look at one of these products. Each e sub k is some edge, let's call it ij. Remember that it's a vector with a plus 1 in the Ith position and a minus 1 in the Jth position. So what about the product? This product will be a matrix, and, in fact, this product will only have non-zero entries in four places. Let's do the diagonals first. A diagonal entry will either be plus 1 times plus 1 or minus 1 times minus 1, which in either case is just plus 1. In effect, the diagonals are tallying the number of incident edges on each vertex. That's the diagonals. What about the off diagonals? The off diagonals will always multiply a positive 1 times a -1. In effect, the off diagonals indicate the presence of an edge. However, since both j, i, and i, j are minus 1, you've effectively lost the direction information. Put another way, the Graph Laplacian is telling you something about the undirected form of the original graph, in the event that the original graph was directed. Now this is what one of these products tells us, but what about the sum? Well, let's see. The diagonals count the degree of each vertex. Let's represent that by a diagonal matrix D, where the diagonal entries are the degrees of each vertex, and let's say there are n vertices in all. The off diagonals mark all the edges. That's basically just the adjacency matrix of the undirected form of the graph. Remember, the adjacency matrix would have a positive 1 anywhere there's an edge either connecting i, j or j, i. And, of course, to get minus 1s in the off diagonal positions, we'll subtract the adjacency matrix. Okay, so let's construct a Graph Laplacian on an example. Step one is to throw out the directions, then build a diagonal matrix, D, that contains the degrees of each vertex. This example isn't very exciting because every vertex has a degree of 3. Then build the adjacency matrix, which marks all the edges. Again, this example isn't very exciting because it's basically fully connected. So there are ones in all of the off diagonal entries. Finally, the graph laplacian is just D minus W. In this case, that's this thing. Take a moment to contemplate the Graph Laplacian. Once you've done so, you can move onto a more interesting question of, how do I use this doohickey to actually partition a graph? As an exercise, I want you to build the graph laplacian for this thing. Fill in the entries of the laplacian here. And you can just leave an entry blank if you want to indicate a zero should go there. Recall our shortcut for building the Laplacian. Start by throwing out the directions on all the directed edges. The diagonals of the Laplacian are just the degrees of each vertex. So for instance vertex zero has one, two, three, four, five, six, seven incident edges. Therefore there's a seven in the zero, zero position. Now the off diagonals mark at the edges. For instance vertex zero is connected to every vertex except seven. So it has a minus one in all the positions of row zero, except for column seven. As a sanity check on the answer, you can do some simple tests. For instance, the matrix ought to be symmetric, look symmetric. Symmetry follows from the definition of the Laplacian in terms of the incidence matrix. Symmetry. Here's another sanity check, the rows and the columns should each sum to zero. So does that hold for this solution? One super cool thing to me about spectral partitioning is that you can derive it using the graph laplacian as it appears in a particular physical system. Imagine that you have a bunch of sticks of infinite length. Let's suppose the sticks are fixed meaning they don't move. This dashed line is just a reference. We're going to measure distances relative to the reference. And suppose each stick has a weight attached to it. And let's suppose each weight has a mass of little m. We're talking about a physical system so the mass would be measured in, say, kilograms. And for simplicity, let's assume all the weights have the same mass. Now imagine that you connect these weights by springs so then if you take all the weights and you displace them by some amount relative to one another, the strings will stretch. And the weights will move up and down. So what I want us to do is study the motion of these weights. Let's zoom in on one of the weights. Now, relative to the reference line, weight i will be displaced by some amount at any point in time. Let's call this displacement x of i of t. Now, in Physics 101, you would have analyzed the motion of these weights, using Newton's laws and Hooke's laws for springs. Yaaar, did someone say springs?! Here's a quick reminder. Newton's Law says that the net force acting on a object causes it to accelerate in linear proportion to its mass. Hooke's Law says that the force from a spring is proportional to how much it stretched. Hooke's Law holds if this degree of stretching or displacement is relatively small. Now, if that all sounds like a bunch of nonsensical mumbo jumbo, don't worry, I'm not testing you on physics. Just hang in there with me, and I promise something neat will happen. So let's go back to this ith weight. Its motion is determined by the net force acting on it. By Hooke's Law, that force will depend on how much the strings are being stretched. And in fact, each string will be stretched by the difference between the displacements of the masses. So, the right-hand side is just the sum of these forces. Now, rewriting the right-hand side you'll get the following. Notice the values of the coefficients. So, the net force acting on a given mass is proportional to the displacements of its neighbors. In fact, I can rewrite this entire system for all the masses as a system of differential equations. Let's define a vector x arrow, whose components are all the springs displacements. Then the entire system of differential equations can be written in the following form. I've used matrix notation here to express the entire system very compactly. So, does this matrix look familiar? Why it's nothing more than a graph laplacian. In particular, it's the graph laplacian for a line graph. I know what you're thinking, okay Vudoc, what on God's green earth does this have to do with partitioning? Therein lies the neat surprise. Start by asking yourself what would the motion of this system of weights and springs look like? The top plot is the displacement of all the weights. Every weight is a dot and the springs are the solid lines connecting the dots. The bottom plot will show you the velocity of all of the weights. At the beginning suppose we give a little kick to the middle weight so that its velocity is one unit of distance per unit of time and the other weights are at rest with velocity zero. That's how you read the initial picture shown in the bottom plot. So from this starting point, let's see what happens. Notice how the springs are bobbing up and down with a kind of wavy motion. In fact, this motion can always be described as a sum of fundamental displacements. Which are basically sines and cosine curves at various fundamental frequencies or modes. If you have a background in music, you know about harmonics, and harmonics are modes for say, vibrating strings. Now for this particular spring system, it turns out that the fundamental modes are given by the eigenvectors of the graph laplacian. In fact, when we finally arrive at our ultimate algorithm for spectral partitioning, you'll be taking a closer look at the eigenvectors. But more on that later. Suppose I give you a set of nine weights arranged in a three-by-three grid. Again, each of these weights is attached to a stick of infinite length, and they're connected to their neighbors, in this case, the north, south, east, west neighbors, by springs. Now, suppose I number the weights as follows. So, I'm going from zero to eight, row wise. My question is, what is the graph Laplacian for this system? Fill in the entries of the Laplacian here. As usual, if you think an entry should be zero, you can just leave it blank to save you some typing. Remember our shortcut for constructing a graph laplacian. Along the diagonal, we'll fill in the degrees of all the vertices. So for instance, vertex 5 has a degree of 3. As for the off diagonals, they'll just mark all the edges. For instance, 5 is connected to 2, 4, and 8. So if you look at a graph laplacian, you'll see -1 entries at 2, 4, and 8. By the way, as you'd expect, the motion of the system is wavy. [MUSIC] Whoa, groovy, baby. [MUSIC] I like to dream. Yes, yes, right between my sound machine. On a cloud of sound, I drift in the night. Any place it goes is right. Goes far, flies near, to the stars away from here. Well, you don't know what we can find. Why don't you come with me, little girl, on a magic carpet ride? Oh, you don't know what we can see. You are now ready to connect the graph Laplacian, its eigenvalues and eigenvectors and graph partitioning. But first some handy facts. I'll just state these without proof. If you need that, please see the paper by Pothen et al, posted in the instructors notes. The first key fact is that the Laplacian is symmetric. This follows from its definition. An important consequence of this fact is the second fact. The eigen values of the Laplacian will be real-valued and non-negative. I say real-valued as opposed to complex-valued. The Laplacian's eigen vectors will also be real-valued. And we can define them to be orthogonal. If you've forgotten what an eigenvalue and eigonvector are, here's a quick reminder. Eigenvalues and eigenvectors are paired. Let's let land to i and Q sub i be an eigenvalue and an eigenvector respectively of L of G. And the following holds, multiply L of G by it's eigenvector gives you a scaled version of the eigenvector. The scaling factor is the eigenvalue. You can also write this differently in matrix form, L(G) Q = Q times lambda. Q and lambda are matrices. In particular, the columns of Q are the eigenvectors of L(G). Lamb dot is a diagonal matrix where all the eigen values sit on the diagonal. What about this orthogonal bit? Orthogonality tells us that the dot product between any pair of eigen vectors will be zero if they're different. Or one if they're the same. In more compact matrix notation, Q transposed times Q is the identity matrix. Now what about this non-negative eigen values bit? That tells us that all the eigen values are at least zero. Now a common convention is to assume that we can sort the eigen values So let's do that too. We'll let the eigenvalues with the smaller indices, be the smaller eigenvalues. So in this case, lambda0 is the smallest eigenvalue, and lambda1 is the second smallest. Now here's a third fact which I find especially interesting. It says that the graph G has K connected components if and only if the K smallest values are identically zero. This is a fact due to Fiedler. And when I first learned this fact, I was like, wow.. I mean omg wow, like seriously, as in srsly seriously. It's a cool fact because it says, the graph laplacian spectrum, meaning it's eigen values, tells us something about the underlying connectivity of the graph. Here's one more important fact. Let's say you have a graph that's been partitioned into two pieces, one piece is the plus piece, and the other piece is the minus piece. Let's let v plus be the set of vertices in the plus piece and v minus the set of vertices in the minus piece, now this is all graph language. What about the linear algebra side? Let’s define a vector x with respect to this partition. Each entry of x will correspond to a particular vertex. We’ll assign that entry a positive one if the vertex lies in V plus and a minus one if the vertex is in V minus. Now, suppose you want to count the number of edges that are being cut in the graph. This leads us to our fourth fact. The number of cut edges is this product. One-fourth x transpose L times x. This fact is also very nice. It says that if you want to minimize edge cuts, you should try minimizing this product. Okay, so let's quickly summarize the four facts. Take a moment to digest them. Once you've done so, you're ready for the spectral partitioning algorithm. Hey, remember this fact? >From a partitioned graph you create a special vector x with plus minus one entries depending on the partition. And then we said, well, if you want to count the number of cut edges then you look at this product. Let's write this product out explicitly. So x transpose l times x it turns out is this sum. It's a sum over all pairs of vertices above l i j, times x i, times x j. Now let's go one step further and actually break this sum apart into five components. Yikes. Looks complicated. But don't worry it's not that bad. You can with some effort interpret these sums for instance, what is this first sum? Well, it's a sum over the subset of INJ where I is equal to j. In fact, that simplifies the sum and. For example, lij for i equal to j is just the diaganol of l and i equals j of xi and xj is just essentially xi or xj squared. In other words, the sum and is just lii times xi squared. But you know exactly what that is. The lii are the degrees of the vertices. And xi squared is just always one, independent of whether xi is plus one or minus one. So if I were to ask you to give me the values of say, lii and xi Xi squared, you would say lii is di, the degree of vertex i, and xi is plus 1. By the way, what is this? Well, if I'm summing the degrees of all the vertices, then what am I doing? I'm basically counting the number of edges twice. My question is, can you do the remaining sums? For instance, can you interpret this sum and then figure out what lij, xi, and xj are? So, think about that and write your answers in the boxes. Here are the answers. This first sum is over all i, j where i is not equal to j. So these are off diagonals, and I'm asking for the vertices that live in V+. Since it's a sum over off diagonals, lij will be -1. And since all the vertices live in V plus, Xi and Xj both will be one. So this whole thing basically counts the number of edges wholly contained within V plus. And it counts them twice. And it's also negative. In other words this thing is minus two times the number of edges in V+. So if you repeat this reasoning for all of these sums and you combine all the sums together, you should be able to see how this product is basically four times the number of cut edges. Let's take stock of where you are. You start with a graph G, from G, you can construct its Laplacian, L of G. Now, suppose you have a partition of G, that implies the petitioning of the vertices into two sets, call them V+ or V-. In linear algebraic terms, you can translate this vortex partition into a partition vector. Each vertex i is assigned to one partition or the other designated by a +1 or -1. Now suppose you want to know the number of cut edges in G. Then you can count it algebraically. That suggests that partitioning could be viewed as choosing an X to minimize this quantity. So this becomes a common notarial optimization problem. Formally we want an x that minimizes the number of cut edges subject to two conditions. First you want to assign every vertex to one partition or the other and you want the same number of vertices in each partition. This problem is unfortunately NP-Complete. Rats! So to work around that, let's relax the problem a little. Let's start by taking away the requirement that we assign exactly a +1 or -1 to every vertex. If you do so, then there's a really neat fact we can use. This fact combines everything we know about Graph Laplacians, along with a cool theorem from linear algebra called the Courant-Fisher Minimax Theorem. If we're allowed to use any vector y, where y is normalized in a certain way and it's elements sum to 0, then the vector y that minimizes this quantity is actually q1. Q1, you'll remember, is the eigenvector corresponding to the second smallest eigenvalue. And in fact the minimum value simplifies to something that is proportional to that eigenvalue. So what does this mean? Let's go back to the original optimization problem. Our most recent fact tells us that the optimal value is at least this quantity. To turn all this into an algorithm you need one more idea. Choosing x to be q1 give us a lower bound on the thing we want. But how do we take a q1, and actually turn it into something that is a partition vector. In other words, something that has plus or minus 1 values in all the entries? Do you remember our spring analogy? In fact, the Laplacian derived from the spring system gives us a second smallest eigenvector that looks like this. If you plot the components of the eigenvector for the second smallest eigenvalue, you get something that looks like this. It's sine curvy, because that's what the fundamental modes of the spring system look like. And as it turns out, half of its elements are positive, and the other half are negative. So that suggests the following algorithm. First, compute the Laplacian for the graph. Then compute its second smallest eigen pair. Then determine the partition using the sines of the components of the eigenvector. So, that was a lot of work, but the final algorithm is very compact. And indeed for plainer algorithms it works really, really well. Here are two examples of what the spectral partitioning algorithm produces on two graphs. Hey. Someone should make a t-shirt out of these. Before moving on, let's quickly recap a few of the big ideas of this lesson. These ideas all stemmed from the fact that graph partitioning is indeed complete. That means you'll naturally want some algorithms with good heuristics. Some of the heuristics that we talked about included multi level partitioning, based on divide and conquer, exploiting special structure, like plainarity, and the no gain is pain improvement techniques, like what you saw in the KL algorithm. The last method we talked about was spectral partitioning. That exploits the spooky relationship between graphs, and matrices. Now, you might be confused about at least one more thing in this lesson. Both KL, and spectral partitioning are really expensive compared to the original motivation for partitioning which was a relatively cheap BFS. So, why would you ever partition for BFS? To answer that question, you will need to think about how all of the ideas in the lesson connect. Real machines have memory hierarchies. That means, that between a processor and a primary storage device like a disk, there are layers of memory in between. And as you get closer to the processor, those memories get faster but also smaller. Now the difference in all of the size, latency, and bandwidth between successive levels, might be an order of magnitude. Unfortunately, our usual model of an algorithm doesn't distinguish between the size and speed of these different memories. Yet, if you want to achieve high performance, and you know you do, then you need to design your algorithms to take advantage of the memory hierarchy. Now, sometimes the hardware or operating system can manage these memories for you, automatically. But using these memories optimally is hard. So try as they might, these automatic schemes can't completely solve the problem for you. And that means the onus is on you to work your algorithmic magic. This lesson is designed to help you get started. To design a locality aware algorithm, you need a machine model. We'll use a variation on the von Neumann architecture. Here's a picture of von Neumann from 1956. The von Neumann architecture has a processor. For the moment, assume the processor is sequential. This processor does basic computing operations. These are things like addition, subtraction, branches, and so on. Now the processor connects to a main memory, you regard this memory as being nearly infinite but really slow, hang on, I'm a coming, I'm a coming. Between the processor and the slow memory there's a fast memory, however the memory is also small. Let's denote the size of this memory by capital Z measured in words. You're probably already familiar with two-level memory hierarchies. Consider, for example, a real processor like an Intel Ivy Bridge multi-core processor. The fast memory might be, say, a last-level cache that sits between a slower main memory, not shown in the photo and the processor. Or what about this example? This is an Adapteva Parallella board which is collecting dust in my office. It's collecting dust mostly because I'm sitting around drawing cartoons instead of doing, say, real hacking. Do I even know how to program anymore? The Parallella has a slow, non-volatile SD card which acts as a disc. It also has a smaller amount of main memory. Pulling data out of main memory is a lot faster than pulling it off the SD card. Okay, where was I? Oh, right. There are two rules about how computations run in this model. Here's the first rule. The processor cannot do any operations unless the operands sit in fast memory. I'll refer to this as the local data rule. Here's the second rule. It's called the block transfer rule. It says that when data moves back and forth across this channel, between slow memory and fast memory, it does so in chunks of size L words. That is, consider the following scenario. Suppose you want to load a word at address x from main memory. The block transfer rule says you actually have to pay to move an additional L minus 1 near bywords. Now which words come with the word at x depend on how your data is aligned in slow memory. In other words, when you design an algorithm in this model, data alignment might be another issue you have to consider. Still, most real-life memory systems do block transfers. So you need to think about multi-level memories and block transfers when designing high performance algorithms, like it or not. As I've stated it, this model implies two major costs you need to think about when designing algorithms. First, how many operations does the algorithm require? In other words, what's the computational work that the processor needs to do? Just like the concept of work in the work span model for a parallel machine, the concept of work in this I/O model will generally depend on the input size n. Then there's the second cost. How many block transfers does the algorithm need to do? As you might expect, the number of transfers might depend on both the size of the fast memory as well as the block transfer size. Let's denote the number of transfers by Q, and refer to it as the algorithms I/O complexity. To make this more concrete, let me walk you through a very simple example. Suppose you want to sum the elements of an array of size n. You know the processor needs to do at least n-1 additions. Or asymptotically, that's big omega of n operations. What about memory transfers? Intuitively, you know you need to make at least one pass through the data. That suggests a natural lower bound on transfers. That's the ceiling of n divided by L. The ceiling accounts for the fact that if n is not a multiple of L, then you need to pay for a partial transfer somewhere. Now, there's another thing to notice about this expression. It does not depend on capital Z, the size of the fast memory. That makes sense, right? You only need to touch each element once. So whether the fast memory is very large or very small, it really doesn't matter. Reduction does not reuse data. And not reusing data is bad. Two level memories appear everywhere. So I want you to tell me which of the following are examples of slow and fast memory pairings? Here are your options. As an example, the first option is hard disk, as the slow memory, versus main memory, as the fast memory. In fact, I already gave you this one so let's check it off. The other pairings are L1 cash versus CPU registers, tape storage versus the hard disk, remote server RAM versus local server RAM, the Internet versus your brain, and I've even left you an additional option if you want to be creative and come up with your own. The answer is, they are all examples. Well, the only questionable one in my case is this last option. I think my brain is an example of the rare, yet possible, small and slow memory. So for me answering, I probably wouldn't have checked this one off. But other than that, like I said, two level memories are everywhere. And you've probably had a lot of first hand experience mostly over the frustrating variety with trying to speed up some computation by moving data from one of these memories to another. And if you haven’t just wait until the project. Braaah. Suppose you're summing the elements of an array, but I tell you nothing about how the array is aligned, with respect to the transfer size L. In the worst case, how many transfers might you need to read the array? Note that I want an exact answer, not an asymptotic one. And if you need things, like floors and ceilings, feel free to use these in your symbolic answer. Here's what I came up with, the ceiling of N over L plus 1. It's easiest to see this by example. Suppose N equals 4, and L equals 2. Consider two cases, in the first case suppose the array is aligned on an L word boundary, then you would just need ceiling of N over L equals 2 transfers. But what if the array is not aligned? Then you might need one extra transfer. Now the purpose of this exercise is just to make you aware that this can happen. But for most of what we'll do it's a minor detail. We'll often ignore it, especially when N is much greater than L. Suppose you want to sort an array of n words. Let's say you want to use a sequential comparison based algorithm, but do so on a machine with a two level memory hierarchy. Recall that comparison based schemes need to perform at least n log operations. Here's my question. Can you come up with an asymptotic lower bound on the number of slow fast memory transfers? Your lower band need not be very tight. A trivial bound is fine just try to make it as precise as you can. It's a little bit of a contradiction, but anyway, you'll figure it out. And if you want to use floors and ceilings in your answer, feel free to do so. Depending on how clever you are and how much you already know about this topic, a variety of answers are possible. Here's the one that I was really looking for. It's really just the trivial lower bound of reading the array. So that would be an asymptotic cost of ceiling of n over L, or just n over L. There's an n because you need to touch every word at least once. And you need to divide by L because, in the best case, you'll read these elements one block at a time. The ceiling would just account for the possibility that L does not divide n, but if you left that out, that's fine. Now, if you happen to come up with the following answer, I would've been really, really impressed. We'll do that result soon enough. Lucky you, it's quiz time again. Suppose you want to do a matrix matrix multiply on a machine with a two level memory hierarchy. Let's say the matrices are all square end-by-end objects. Now if you ignore the possibility of a strassen or faster algorithm, than the work of the matrix multiply is O n cubed. My question, surprise, surprise, is what is the minimum number of transfers? I'm looking for an asymptotic lower bound. Your lower bound need not be too tight but, do try to make a reasonable guess. Here's what I was looking for. It's just the trivial lower bound of n squared over L. The n squared just counts the number of matrix elements dividing by L diverts n squared into some number of transfers. Now if you're really way ahead of the game, you might happen to know the much tighter lower bound. I would not have expected you to guess this, but if you did I am definitely impressed. Anyway, we'll hopefully get to this at some point in the near future. Recall the reduction example. The work is linear, and the number of transfers, is at least n over L. Now let's analyze a concrete algorithm to see, whether or not, we can achieve this lower bound. Now here's the natural thing you would write in the conventional RAM model. That is the one without the fast memory. It maintains an accumulator s, and makes a sweep over all the elements, accumulating them. Now you need to modify this procedure, to think about when to move data, between slow and fast memory. Now, the variable s is easy. You can just assume that it starts locally, in the fast memory. It's the same way you might treat any temporary scalar or local variable in your favorite imperative programming language. For my pseudo code, I've just declared s to be local. I'm doing that just to be clear. Most of the time you can ignore these kinds of local variables. Now what about the array X? For simplicity, let's assume that little n is, much bigger than, the size of fast memory. Let's further assume that X is aligned,on an L word boundary. With these assumptions, let's transform the program, to make slow and fast memory transfers explicit. Yikes. Let's start with the outer loop. As with the original algorithm, it steps through the elements of the array but it does so, one block of size L at a time. Next, there's the computation L hat. This mumbo jumbo just determines whether the block that starts a position i is of length L, or a little bit smaller. It's a detail. I just wanted to be clear. But most of the time, we'll ignore this sort of detail. Next, there's the assignment to little y. This is an explicit load or read, from slow memory, to fast memory. Notice that it requests at most L words, which is one block transfer. Now, since little y and S are both local to fast memory, the processor can execute this innermost loop. So what's the work in the number of transfers for this algorithm? The work is just the same as this conventional algorithm in the RAM model, that is, it's just theta of n. Here's the number of transfers, it's just the ceiling of n over L, hopefully, you can see how this falls out, very naturally, from the structure of the code. So how do these compare to the lower bounds? Why they compare very well, indeed. Before moving on, I want to make two remarks about what you've just done. First, I realize that was a lot of painful detail to do what must seem like a really simple analysis. We will make simplifications, as we go, but I wanted to start by just being clear about exactly what has to happen under the hood in this model. Now, my second remark. Yes, I know about caches. They're those fast memory thingys that the hardware manages automatically. In fact, there's an awesome OMSCS class that covers their design in a lot of detail. While caches are very useful, what you'll eventually see is that they aren't sufficient to guarantee high performance. That is why we're discussing an explicit model of locality, like this one. Suppose you wish to multiply a dense n by n matrix by a vector. Recall that the work, ignoring any structure in A, is proportional to n squared. Now suppose the matrix is stored in column major order. Column major order means that the elements of the matrix are laid out in memory column wise. That is elements have consecutive addresses within a column with one column following the previous column. In other words, viewing A as a 1-D array, the IJ element is indexed as I + J times N. Now consider two algorithms to compute a matrix vector product. Here is algorithm one. Rows are indexed by me and columns by j. In algorithm one, the outer most loop iterates over rows and the inner most loop iterates over columns. Take a moment to make sure you understand the iteration structure. Algorithm two does just the opposite. The outer loop iterates over columns and the inner loop iterates over rows. In the basic ram model, these algorithms are identical. Here's my question. In our IO model, which one does fewer transfers? Is it the one with i as the outer most loop and j as the inner most loop? That is outer most rows, inner most columns. Or is it the one that does the opposite? That is, outer most loop over columns and inner most loop over rows. To help you answer this question let me give you a few simplifying assumptions. First, assume that the fast memory is large enough to hold two vectors. You can also assume it has a little bit more space to store a few extra blocks of size L. Secondly, you can assume that L divides n. Lastly, let's assume all the arrays, and matrices are aligned on L word boundaries. These last two assumptions essentially avoid the data alignment issues, so you can ignore floors and ceilings. One last hint. >From these assumptions, you might as well assume that the algorithm preloads x and y at the very beginning, and stores y at the very end. What does that, in turn, imply? Well, the number of transfers will be at least 3n over L. In other words, the real question is, how many additional transfers does loading the matrix require? The answer is that algorithm two requires fewer transfers. Let's quickly walk through this. Consider algorithm one, the outer loop of algorithm one iterates over rows. Within row i, it loops over columns j starting at zero Notice what happens. Touching an element of the row causes a block of L- 1 additional elements from the column to be loaded into fast memory. You might not be this exact block, this is just an example. This load is a consequence of column major layout. Now consider the next iteration. It will require loading a completely different block of elements. Remember that the fast memory only has space for two vectors plus a little bit extra, so eventually, the first block will have to be displaced. In other words, traversing this column major matrix, rho wise, incurs block transfers for each row. I hope you can see that that will lead to a total of n squared transfers to read a. Okay, what about algorithm two? The outer loop goes over columns. The innermost loop traverses within the column In this case, the traversal order matches the storage format. That is, this access pattern does a better job of amortizing the cost of loading a block. The total number of additional transfers to read a, then becomes n squared over L. In other words, for large values of n, you expect algorithm one to do L times more transfers, making algorithm two, L times faster. The important point is that in the sequential RAM model, these algorithms look identical. But in this simple two level model with block transfers, they look very different. Oh, RAMy, I hardly knew ye. Before leaving this quiz, let me ask you one last thought question. This is for those of you who know about caches. Suppose the fast memory is a fully associative cache with capacity Z words and line size L in words. Do you think that caches alone would help algorithm one match the performance of algorithm two in terms of the number of transfers? An important question about this model is, what are the goals? That is with respect to these complexity measures, what makes an algorithm good? What you want is actually pretty simple. The first goal is work optimality. That is, your two level algorithm should do the same asymptotic work as the best ran algorithm. Here W-star is intended to mark the work of the best RAM algorithm. That is, the one that doesn't have a memory hierarchy. The statement is no different from what you would say about parallel algorithms. Namely, don't blow up the asymptotic work. Ka-boom! There's a second goal. You want an algorithm that achieves high computational intensity. Here's a more formal statement. Computational intensity or just intensity is just the ratio of work to words transferred, denoted by a capital I. Intensity has units of operations per word. Operations per word, put another way, computational intensity measures the data reuse of your algorithm. If this ratio is large, it means you're doing more operations for every word that you transfer to fast memory. That's a good thing, but, you don't want an algorithm that maximizes this ratio at the expense of sub optimal work. In other words goals 1 and 2 go hand in hand, and I hope this fact reminds you of work and span. Suppose I give you two algorithms. Algorithm 1 does work proportional to n and number of transfer proportional to transfers divided by l. Algorithm 2 is a little different. It has n log n work and n divided by l log z transfers. My question is, which is better? You have several options, you can choose one, both, or none. Checking both means you think they're equally good. Checking none means you can't tell. Whatever your choice I want you to explain your thinking, here's a handy box for doing so. I don't know about you, but I'm very noncommittal. I would have check nothing on the grounds of insufficient info, or maybe just laziness. So why did I make this choice? Well, remember our goals are low work and high intensity. In this case, Algorithm 1 does less asymptotic work. But what about intensity? Intensity of Algorithm 1 is a constant, the intensity of Algorithm 2 grows asymptotically with n and z. So knowing nothing else, we can't tell if we can satisfy both our goals. Now consider the relationship between work, transfers, and execution time. Suppose the processor takes tau time units to perform an operation. Thus, the time to perform just the compute operations is tau times the work, W. Next, let alpha be the amortized time to move one word of data between slow and fast memory. Alpha has units of time per word. Then the time to execute Q transfers is alpha times L times Q. Now let's assume perfect overlap between data transfer and compute. Then the minimum time to execute the program would be the maximum of the time to compute, and the time to move the data. Let's refactor this expression a little. This refactoring shows the execution time relative to the ideal running time, which is just tau times W. It's ideal in the sense that it assumes data movement is free. Now relative to this ideal computation time, you need to pay a penalty. This communication or transfer penalty is the price you pay for having to move the data. Now this penalty has some structure. Let's start with the denominator of the second argument. Why it's nothing more than the algorithms computational intensity. Recall that intensity has units of operations per word. Now what about the numerator? The numerator is th time per word divided by the time per operation. It's a ratio of parameters that only depend on the machine. In the literature, it's sometimes referred to as the machine's balance point, or just machine balance. And just like intensity, it has units of operations per word. In other words, this ratio tells you how many operations can be executing in the time that it takes to move a word of data. Machine balance is kind of a special parameters, so let's replace it with a symbol, capital B. That's B as in buzz. Okay, so what we have now is a general formula for the minimum possible execution time in terms of the balance of the machine and the intensity of the algorithm. For completeness we could also estimate the maximum time for computation in this model. It's basically the sum rather than the max. That's because if there's no overlap, you have to pay for computation and communication one after the other. Okay, so we have a formula to estimate the minimum execution time. Sometimes, rather than asking about execution time, we'll ask about a certain measure of performance. I'll denote that measure by R. Its numerator is tao times W star. In other words, the best time in the pure RAM model. So what does dividing by T do? Why? It gives you the following formula. It says the best possible value of the normalized performance is W star over W times the min of one and intensity divided by balance. This measure of performance is inversely proportional to time. Therefore higher values are better. Take a moment to ponder it as you'll explore it in more depth momentarily. Let's recall some basic facts about this type architecture. Assuming perfect overlap of computation and data movement, you can estimate the maximum normalized performance. It's this expression, which I've denoted by Rmax. Rmax is defined in terms of W star, which is the work of the best sequential algorithm. It also depends on the work of your algorithm, W. It depends on the works overall intensity, I. And finally, it depends on the machine's balance point B. In performance analysis, one way to visualize the relationships among these parameters is through something called a roof line plot. Start with a pair of x,y axes. It's usually drawn as a long log plot, but let's ignore that fact for the moment. On the x axis, you plot intensity. On the y-axis, you would plot normalized performance. For this problem, let's assume that W star and W remain fixed, but I might vary. This would happen if you had, say, a bunch of algorithms or a bunch of implementations that all did the same amount of work, but varied the communication. The curve you would get if you plotted R max as a function of I would look like the following. Because of it's shape it's sometimes called a roofline plot. So what do you see? Well the plot has a linearly increasing slope followed by an inflection point, followed by a plateau. The interesting features are the value of the plateau and the location of the inflection point. Let's call these Y0 and X0 respectively. Here's my question. What are the values of X0 and Y0? Express your answer in terms of i, b, w, and w_star. And when you type out w_star, use the notation w_star. Here is the answer. That is the plateau occurs at y0 = w*/w. It's the maximum possible value this expression could take. Just take i and let it go to infinity and you should be able to convince yourself that that's true. Now what this ratio also tells you is that if you design an algorithm that is not work optimal with respect to w*, then you will pay a penalty. That penalty will be reduced maximum performance. So what about the critical point, x zero. Again, according to this formula, you reach the critical point as soon as I is equal to B. Overall, this says that when you design an algorithm, a good target is to try to get an intensity that is B or greater. By the way this is a good point to introduce some terminology, if your algorithm is to the right of x zero we say it's compute bound, if on the other hand it's to the left we say it's memory bound. Estimating the intensity of an algorithm can tell you whether you need to work harder to come up with a better one. Just don't over do it. Consider non matrix matrix multiply. Let's say you were to implement it with the conventional three nested loops algorithm. It essentially loops over the elements of C. For each value of C, it computes the dot product between a row of A and a column of B. Now suppose you run this algorithm on a machine with a two level memory of size Z. And let's make some simplifying assumptions. First, let's suppose the transfer size is equal to one word, this way you don't have to worry about stuff like alignment. With respect to Z, let's suppose that it's big enough to hold two vectors plus a little bit of extra space. Now let's transform the three nested loops into something IO aware. I've inserted some comments to indicate where you might load pieces of A, B and C. So, at the beginning of the body of the i loop is a good place to load row i of A. And at the beginning of the body of the j loop is a good place to load both C i, j and column j of B. Finally since we're done computing C ij after the K loop we can store it immediately afterwards. Notice these loads and stores respect our assumption two that Z holds two vectors plus a little bit. Here's my question. What is the asymptotic intensity of this algorithm? Enter your answer symbolically in terms of N and Z in this box. Here is the answer, in an asymptotic sense is basically just a constant, why? First note that the algorithm does n cubed work, what about transfers? Start by considering the reads of A. The read is a vector of length n elements which you repeat n times. So for A only that's n squared reads. Next, let's look at C. There's a read and a write of one element each. These are repeated a total of N squared times. So our tally of transfers now increases to three N squared. Lastly, there is B. The algorithm reads N elements of B N squared times. That's N cubed reads. Thus, it's the reads of B that dominate the overall transfer cost. And since the intensity is the ratio of operations to transfers, the intensity is constant. An interesting question to ask yourself at this point is, can you do better? I hope your intuition says, yes. in particular, there are n-cubed operations but only n-squared data. That suggests there might be a factor of n reuse available. Consider the conventional matrix, matrix multiply algorithm. Suppose you compute it block by block. That is let's conceptually divide A, B and C into a little b by b blocks. An algorithm might then look like this. The algorithm iterates over blocks of C. It then reads blocks of A and B. Given the blocks, it multiplies them. Finally it stores the block. All these reads and writes of blocks are basically slow fast memory transfers. I want you to count them up. After doing so, you should be able to tell me the asymptotic intensity of this blocked matrix matrix multiply algorithm. Before you start, let's make some simplifying assumptions. First, let's assume L=1. Next, assume that b | n. You can also assume that n | Z. Lastly, since you need the blocks of A, B, and C to fit into fast memory in order to multiply them, let's also assume that the fast memory size is 3 b squared plus a little. Express your answer in terms of n, little b, or z here. The two answers I was looking are either little b or square root of Z. These answers are equivalent because of our assumption about the size of fast memory. The work is still n cubed. What about the transfers? Each block read or write involves the transfer of a b by b, or b squared, sized block. Each loop nest does n divided by b iterations. >From these two bits, we can now count out the total number of reads. For example, let's take the C blocks. Each block read involves b squared words, and this is repeated, n/b squared, times. So that's a total of n squared reads, same thing goes for the writes. What about a and b? In each case, there are b squared reeds nested in n/b cubed iterations. So that's a total of n cubed, divided by b reads. The same thing holds for matrix b. The total number of transfers is therefore dominated by an n cubed over b term. The intensity is just the ratio of the work and the transfers, or O of b. And by our assumption of the size of b in relation to the fast memory size, that's the same thing as square root of Z. Remember that the conventional matrix matrix multiply algorithm has an intensity that is constant, so blocking is a much better idea algorithmically speaking. One of the neat things you can do with this kind of analysis is to inform the design of computer architectures. Let's look at a simple example. Suppose your computer architecture colleague wants to build a machine that does deep learning better. As you probably know, deep learning is kind of a synonym for neural networks, which pop up in machine learning. Take my OMS class on machine learning. It's totally deep, man. So, suppose you have machine that's really, really good at matrix multiply at a particular size. Now, suppose that in the next generation machine, the balance doubles. Recall that the machine balance is defined to be alpha divide by tau. Alpha is the time to a transfer from slow memory to fast memory. And tau is the time to do an operation once the data is local. The fact that balancing has doubled is bad. It means you have to be able to do twice as many operations locally on the processor, in the time that it takes to move data from slow to fast memory. So, if machine balance doubles then your computation might slow down, unless you compensate. I claim you can compensate by increasing the fast memory size. My question is, by how much does z have to increase in order to compensate for the machine balance doubling? The answer is by a factor of 4. Why? Recall the formula for, say, Rmax. The machine balance enters the formula through the communication penalty. Furthermore, recall the value of intensity, for matrix multiply. It's the square root of the fast memory size. So, all things being equal, if B doubles, then the numerator also has to double in order to keep the communication penalty the same. Therefore z has to increase by a factor of 4. That's kind of neat. The fact that machine balance increases over time, is a fact of life. Can you explain why? If you think you have an answer, head to the forums, and see what your peers think. I realize the two-level model may seem like a bit of a cartoon compared to real memory hierarchies that you know from either other courses or maybe even real life. So why bother? Well, the two-level model really does capture the most important performance effects of real memories, namely capacity and transfer size. Now, there's been a lot of research on locality-sensitive algorithms based on this model. So, it's helpful to know it in order to come up with sensible ways to extend it. To exploit a memory hierarchy algorithmically, what can you do? The main technique you thought about in this lesson is to organize your data access to increase data use. The other important pair of concepts in this lesson were those of computational intensity and machine balance. Here's a general rule of thumb based on those two concepts. If you want your algorithm to scale well to future memory hierarchies, then you want the intensity of your algorithm to at least match, but preferably exceed the balance point of the machine. You'll take this idea much further in some of the other lessons. At MIT in the early 1980s, a bright young iconoclast named Danny Hillis was trying to build a new super computer. He called it a connection machine. It looked something like this. You can tell it's a super computer because its covered in blinky lights. Fun fact about these lights, they would actually blink according to where communication was happening in the system. I don't know why we don't make computers anymore with blinking lights. We really should. Anyway Hillis's PhD dissertation on the connection machine is one of my all time favorites. In fact it even won the doctoral dissertation award from the association of computing machinery in 1985. That's one of the highest honors a CSPHD can receive. Now the kicker is that it won this award despite its concluding chapter. New computer architectures and their relationship to physics or why computer science is no good. [NOISE] That's right, but the best CS PhD of 1985 poo-pooed the entire discipline. Now, Hillis was critiquing parallel algorithms research of the time. He argued that, at the end of the day, an algorithm has to run on a physical machine which has to obey the laws of physics. Yet algorithms researchers were, in his opinion, abstracting away way too many details about the physical costs of computation. Now Hillis was worried about the speed of light which fundamentally limits how fast information can travel. But today you could easily imagine the melting butter experiment as a physical cost that we largely ignore. But should we? The answer Answer is a resounding, I don't know, but I do think Hillis might have been on to something. So what I want to do in this lesson is to start from this line of thinking and ask what would it mean to consider physical costs in designing an algorithm? I want you to do a series of calculations, partly inspired by Danny Hillis' thesis. These calculations will lead you to one conclusion about the limits of computation. Consider a processor from the year 2015, like this quad core Intel Ivy Bridge CPU. As it happens it can, in the best case, execute a about 100 billion operations per second. Or 100 gop's If you're keeping count at home that's 10 to the 11th operations per second. Very roughly speaking this peak rate of performance or through put doubles about every two years. So, how fast will be a processor be 10 years hence in 2025? Type your answer here and leave it in units of gop's. Here's the number I came up with. 3200 gigaops, or 3.2 trillion operations per second. Let's see how I got there. In 10 years there will be 5 doublings. 5 doublings is a 2 to the 5th, or 32x increase in performance. So a 2025 processor, relative to a 2015 processor will run at 32x100 or 3,200 gigaops. Wow, that is fast. I mean really fast. As a point of reference, suppose I gave you three trillion pennies, head to tail. Those would go around the Earth about 30 times. That's a [BLEEP] ton of pennies. In fact, you might already know or have experience with specialized processors that, even in 2015, are capable of trillions of operations per second. The point of this exercise is to give you an intuitive feel for peak performance and the rate of growth that exponential trends bring. When you think about it, the numbers and scale we're talking about should really blow your mind. This quiz is about computational speed limits like this one, or if you prefer metric distances, this one. Start by considering a 2-D mesh of physical processors. You can, if you wish, imagine that this mesh is actually a many core processor that fits on a physical die of a certain size. Let's say that size is L by L. In this mesh every interior point is connected to its eight nearest neighbors. This means each processing unit can communicate along diagonal routes. Now consider a single operation defined as follows, it starts at the processing unit at the center. The operation then travels as a signal to a unit at one of the corners. It then turns around, making a round trip back to the center. Let's say you want to be able to do say, 3 trillion of these operations per second sequentially. So you want to do this round trip operation, center to corner to center, center to corner to center. And then do that 3 trillion times per second. And by way of reminder, a trillion is 10 to the 12th. Here is my question. If a signal travels at the speed of light, what is the maximum physical dimension of this mesh? In other words, what's the largest l can be? I want you to give your answer in micrometers, or a micron, for short. And since we're doing a back of the envelope exercise, you can round your answer to one significant figure. A handy fact is that the speed of light is about 300 million meters per second. So, what'd you get? I got 70 microns. One round trip, from center to corner and back, is l square root of 2. Recall that your goal is to do three trillion of these round trips, per second. So how do you get to 3 trillion operations per second? Each operation is a round trip, each round trip is l root 2, units of distance. So think of this distance, l root 2, as being in units of, say, meters. You can travel this distance at the signaling speed, which is the speed of light, by assumption. Solve this equation for l. Assuming I did the math right, you should get about 70 microns to one significant figure. That is really tiny. In fact, it's about the width of a human hair. Here's the point of this exercise. To hit the target compute speed, assuming information propagates at the speed of light, a sequential computer has to be really, really tiny. Let's say I give you a memory chip whose area is the same as the cross section of a human hair? That's about 4900 squared microns. In this case, I'm not very cool, so my hair, like this chip, is square. Now suppose you wanted this memory chip to store one Terabyte of data. In this case, let's take a Terabyte to P 10 to the 12th bytes. Here's my question. What is the area occupied by a single bit? Note that I'm looking for the area of a bit as opposed to a byte. Here's a little bit of space for your answer. Type your answer here in units of squared microns per bit. So what do you think? I think the answer is 6.125 times 10 to the -10th squared microns per bit. Start with the area. Then divide by the capacity. You'd end up with something like this, area divided by the capacity in bytes convert bytes to bits. So what kind of screwball number is that? Let's zoom in on a bit. Assuming this area is square, let's take the square root of our answer. That will be about a quarter of an angstrom on a side. Whoa, man, that is small. It's basically the radius of an atom. A single classical bit like this one, as opposed to a quantum bit, probably doesn't get any smaller. And that in fact is the point of this exercise, at some point you won't be able to squeeze any more bits into a given volume of space. And at that point you won't have any choice but to think about locality in order to run fast. Another important trend in systems, is the growing gap between compute speed and communication speed. Recall the basic Von Neumann architecture. It has a processor, a local fast memory, and it's connected to a large but slow memory. Now, let's say the processor can perform R operations per second. As it turns out, this rate is related to transistor density. Transistor density is basically the number of transistors that can fit in a given amount of space. Here's the classic plot of transistor density over time. Essentially this plot shows that over the last 40 years or so, the number of transistors that you can fit in a given unit of area, has increased by a factor of about a million. Now very roughly speaking, as transistors get smaller you get more of them, and the signaling time between transistors shrinks. This makes computation faster. With respect to this plot, just think of the number of operations you can do in a unit of time, as growing with transistor density. Transistor density and therefore performance has grown very, very fast. This plot suggests that performance doubles roughly every 1.9 years. The other feature of the Von Neumann system, is the slow fast memory transfer cost. Now, let's say you can move data back and forth between slow and fast memory at some rate. Let's call that rate beta, measured in units of words per unit time. Like R, beta also has a natural historical growth rate. There's a standard benchmark called stream that measures this growth. So stream measures beta, and beta has essentially doubled once every 2.9 years. Now, recall that the balance point of a Von Neumann system is the number of compute operations it can do per word of data transfer. Let's call the balance point capital B. It's defined as R divided by beta. In other words, it's the peak compute throughput divided by the peak memory bandwidth. Here's my question. What is the doubling time of B? I want you to enter your answer here in years, given to 2 significant figures. So, what did you get? I got 5.5 years. Here's my reasoning. Let t denote time, measured in years. Now R we said doubled every 1.9 years. Mathematically, that means R is proportional to 2 to the t/1.9. Beta is similar. It's 2 to the t/2.9. The balance point is just the ratio of these quantities. Simplifying it, you should get roughly 2 to the t divided by 5.5. The doubling time is there for 5.5 years. Now you might be suspicious. This is just a quiz, right? Or is this a real life trend? Well, let's check it on Wikipedia, purveyor of all things good and definitely 100% true with no questions asked. Here's some stats on two mobile GPU processors. The top one was launched in June 2008, and the bottom one in March 2015. That's a period of time of almost 7 years. Let's call it 6.75 just to be a little bit more precise. Now, the balance point is supposed to double every 5.5 years. So, after 6.75 years you would expect the balance point to increase by a factor of roughly 2.34. Now, what I've been calling beta is given by these bandwidth parameters. R is given by the peak processing power parameters. Those are these two. So, are these data consistent with the predicted rate of change in the balance point? For a sequential processor with a slow and a fast memory, recall the basic concept of the balance point of the machine. In terms of our cartoon, it's R, the peak rate of processing, divided by peak memory bandwidth. Let's also recall the historical growth trends of R and beta. The rate of improvement in computation far outstrips the rate of improvement in communication. This gap doubles about once every five and a half years. So, for your algorithms, what does this mean? It suggests that it might be beneficial to trade off more computation for less communication. Let's be a little bit more analytical about this claim. Start with the DAG model of computation. In it, you characterize a computation by two components. The first is the work or the total number of operations, which I'll denote by W or W(n). The second is the span or critical path length. We call that D(n) or D for short. Let's augment this representation to reason about slow fast memory communication. In particular, remember that you can, at least in principle, count the number of slow fast memory transfers. I'll denote these transfers by Q. Q will in general be a function of the problem size, the size of fast memory and the transaction size. I'll also assume by convention that W includes the count of Q. So, for example, if Q is three and W is ten, then that means there are ten minus three or seven operations that are non-memory transactions. Let's also modify our machine cartoon a little bit as well. As before, there will be a large slow memory, as well as a small finite capacity fast memory of size Z words. When data moves between slow and fast memory, it does so in transactions of size L consecutive words. In addition, let's let the processor have P processing cores. Let's assume that each core can execute some number of operations per unit time. We'll call that rate R0. Let's model the operation of memory by analogy to the way the cores work. Each transaction initiates a data transfer across these L wires in parallel. The time it takes for a word to go across a wire is beta 0. In other words, beta 0 is the analog of R0 in this cost model. And remember, W, D, and Q count the number of operations ignoring these costs. Put differently, we usually compute W, D, and Q assuming unit cost operations. But in a high performance setting, let's try translating unit costs into real costs to see what they imply about the overall system. Conceptually, you can account for non-unit costs by transforming a unit cost DAG. For example, consider some node in the unit cost DAG. Suppose this node is one of the compute operations. Then executing it costs 1 over R0 time units. So we could replace this single unit cost vertex with a sequence of 1/R0 unit cost vertices. Now, what if a node in the DAG instead represents a memory transaction? Let me suggest that we model transactions in the following way. First, there's a latency cost which is the same as the latency cost for the compute operations. In other words, the memory transaction became 1/R0 unit cost nodes. After all, a memory transaction is just another instruction. So, it should roughly share the same instruction processing cost as any other instruction. Next, let's also say that the words of the memory transaction can be in flight concurrently with compute operations. In terms of the DAG, that's an additional set of L over beta zero fully concurrent nodes. Inserting them as concurrent nodes mean they shouldn't increase the critical path link. However, by putting them in as explicit nodes, we'll have to pay for their cost. Most real memory systems behave something like this. That is, there's usually a separate memory controller or network processor onto which you can offload communication. So, what's the best case execution time for this DAG? To start, the usual work and span laws apply, just scaled by the processor speed. There's just one extra cost due to communication. That is, for each transaction, we have to pay for the cost of these nodes. Now if you've done a good job designing the algorithm, the critical path is short. In this case, when is this right hand side minimized? That's right. It's basically when these two arguments are equal. Now remember the historical growth rate trends? Of course you do, because what better way to beat insomnia than to always be dreaming of exponential scaling trends. >From the trends, if you want to benefit from transistor scaling, then you need the compute time to dominate the communication time. I like to call this idea of making compute time dominate communication time, the principle of balance. That is, our collective goal whether we design algorithms or we design systems, is to make it as easy as possible to achieve balance. That's what gives us our best shot at scaling into the distant future. Now, let's start with this relation and massage it a little bit more. On the algorithm side, your goal is to make the left hand side as large as possible, knowing that the right hand side is subject to inevitable scaling trends that cause it to grow over time. What about the system side? On the system side, your goal is to try to keep this small to help the algorithms people out. Now would be a great time for some quick exercises to see what this balance principle, derived from the modified DAG model, implies. Quiz! Woohoo! Let's see if you can work through what the principle of balance means yourself. Let's start with a multicore Von Neumann system. Suppose you start with one version of the system, whose parameters are perfectly tuned for sorting very large arrays. Then your boss comes along. Like many bosses he's a bit of a primate. But hey aren't we all? Your boss has the brilliant idea of selling a new version of the system with twice the cores. Because he lives in America and more is always better there. My question to you is this, can you adjusts to the other parameters of the system in order to maintain balance? Now to do this, you're going to need an important fact. For comparison-based sorting, the best case ratio of comparisons, which are the compute operations, and memory transfers will be able L times log z/L. Here are your answer choices. You can half the bandwidth, beta 0 and double the peak. Or you can square both the fast memory size and the transaction size. Or you could double the fast memory size or you could try doubling the bandwidth. Now I want you to think about these options and decide for yourself which ones do you think have the best chance of maintaining balance assuming the number of cores doubles. Here's what I think. Of these four options, I think only squaring Z and L or doubling the bandwidth will work. Let's go through each option and see why. First, recall the balance principle for this system. If the original system was perfectly balanced for sorting, then you can plug in the given ratio of W to Q. Of course, these L's cancel. Now suppose you double the cores, let's then go through the options and see which ones work. Option one says half the bandwidth and double the peak. Your boss must have suggested this one because it is a terrible idea. Why? Because if you do this, then this factor quadruples, so you've effectively made the system even more imbalanced. Bad monkey! Ooh ooh sorry. What about option two? If you square Z and L, that will effect this term, but that just might do the trick. Squaring the factor inside the log results in a factor of 2 outside the log. So this factor of 2 on the left-hand side will balance, or cancel out, the factor of 2 on the right-hand side. Voila! Unfortunately, if you stop and think about it, it's a really expensive way to maintain balance. It sort of suggests that you might have to go from, say, a one megabyte cache in one generation, to a one terabyte cache in another. That does not seem without a little heartache and pain. Let's move onto option three, doubling the fast memory size. That will not work. The factor of 2 will just pop out as an additive term, not a multiplicative one, so it can't compensate for the cores doubling. That brings us to the last option, doubling the bandwidth, that could work. Doubling the bandwidth is a factor of 2 in the denominator which cancels out the factor of 2 in the numerator. In principle, a great idea but can it actually work? Well, think about those exponential trends you're always dreaming about. Increasing transistor density gives you the ability to increase on chip resources, like, the number of cores, or the size of fast memory, or even the speed. But the historical trends say that bandwidth just does not grow as fast as, say, R0 times P. Okay, so these are these four options, are there others? Well, there's at least one other, what about cutting our zero in half? Now, if you cut our zero in half, I wonder what the problem would be with doing that? Dun, dun, dun. All told, the results of this quiz are just a little bit depressing. It would seem that for a computation like sorting at least there would appear to be some fundamental limits that just make it hard to build balanced systems. Anyway, it's a research frontier. Maybe one of you, my pretty little geniuses, will come with a brilliant idea that circumvents it all. Today, one of the major physical constraints on computing platforms is power. You may be familiar with this fact if you've taken, say, a computer architecture class. Even so, let me give you my take on the basic problem here, perhaps by way of review for some of you. This is a well known chart created in 2001 by Pat Gelsinger, then at Intel. The chart tracked power per unit area, or power density across several generations of Intel's microprocessors. It tracked these up until about 2001, the time of the talk, and then extrapolated into the future. So what is power? Power measures the rate of energy burned or consumed per unit time. Power is usually measured in units of, say, Joules per second or watts. Now as it happens, the way people used to make sequential computers run faster up until the early OTTs was to increase the clock rate. But as you'll see momentarily, when you do that, Gelsinger's prediction was that the amount of power per unit area would skyrocket, almost literally, and we know today how the story ended. Basically, in the early OTTs, people stopped increasing the clock rate and went multi-core. Let's see why. Let's look more closely at the power consumed by a computer program. This chart comes from real data that one of my former graduate students collected. That student was Jee Choi, shown here doing what I gather is actual science. The chart shows measured power taken at a bunch of samples while the application was running. You can see that there's some fluctuation. Presumably the power behavior of the application is changing as it's doing different things. A computing system at some point in time has two parts. I will call the first part constant power. Constant power is basically some baseline amount of power that the application or the computing system consumes, independent of what it's doing. Now, you may have heard the term static power and idle power being bandied about in your daily life. Because I mean, who doesn't walk into a grocery store or yoga studio and hear people always talking about static power this or idle power that? Very roughly speaking, constant power is a general term that I'll use to encompass things like static and idle power. And if you don't know what I'm talking about, just remember, constant power is baseline power. Now, suppose a computation runs. Depending on what parts of the system it's using, the power above and beyond constant power will fluctuate. Let's call those fluctuations dynamic power. Ew! Well, maybe not that dynamic. Now real life circuits can be much more complex than this simple model suggests. But let's not worry about that. Instead, let's take a cue from the famous statistician George Box. Recall that the power in a computing system has, roughly speaking, two parts. The first is constant power, which is what you pay just to keep the system on. The other part is dynamic power. That's what you pay above and beyond constant power while the program is running. Let's take a quick look at some of the basic circuits 101 drivers underlie dynamic power. Imagine a simple logic gate, taken out of a larger circuit. This gate has a certain amount of physical energy. It is both right and natural to represent that energy by analogy to a snifter of mighty fine Atlanta-based beverage. Quick extra credit if you can name this brewery. Now suppose a gate input changes. Then the energy dissipates and also, quite naturally, has to be refilled. But differently when the circuit switches there will be some energy loss. This energy loss then has to be restored. Let's first ask how much energy that is and then ask how frequently this switching happens. Let's see how to compute dynamic power. Start with the amount of energy consumed by this gate during a state change. In electrical engineering terms C is the gate's capacitance. Capacitance is a function of the material properties and geometry of the gate. V is the supply voltage. Supply voltage is part of a circuit design. it can, for some circuits, be tuned over some range. More importantly is how V enters into the equation. It enters as the square. Now for our class, it's not too important to understand exactly where this relationship comes from. But do remember that the product CV squared quantifies energy. So how often does switching happen? Well there are two factors. The first is the frequency or clock rate of the circuit. It has units of cycles per unit time. The clock rate determines the maximum number of times that the circuit can switch or change states per unit time. Now the gate doesn't necessarily switch on every cycle. Depending on what the computation is doing, the switches might happen only once ever few cycles. That brings us to the second factor, which is the activity factor. It's the number of switches per cycle. If you think about how a clock works, it has a maximum value of one. In general, it will depend on what the computation is doing. Taken together, these parameters tell us how to compute dynamic power. It's basically the energy per gate, or CV squared, times the switching frequency. Which is a times F. This formula is sometimes called the Dynamic Power Equation. Before moving on, let me give you one more quick fact about it. The clock rate, F and the supply voltage, V usually need to be kept in proportion to one another. That is, suppose you've cut F in half. Then you also need to cut V in half. Increases are the same. Suppose you increase F by two. Then you also need to double V. So why do F and V need to change together? The short story is that it has to do with maintaining the stability and reliability of the circuit. In particular, its Its reliability in the presence of environmental noise. Okay, now's a good time to pause and see if you can understand what this this formula implies can be done if you want to increase performance while reducing power. If you've taken the OMS class in high performance computer architecture, then you might already know the answer to this question. Let's say I give you two possible chip designs. The first one runs at 4 gigahertz and consumes 64 watts of dynamic power. Let's also suppose that if you run some program on this chip, it finishes in a given time, let's call it t. Now suppose I give you a different design, the main difference is that it's slower, it runs at 1 gigahertz instead of 4. Now I want you to tell me two things, what is the dynamic power of chip two and what is it's execution time? In terms of execution time on the first chip. For the dynamic power consumed of chip two. Express your answer in watts. And for time, again, express your answer symbolically in terms of T1. As needed, assume that all other factors between the two designs are equal. Here's what I got. 1 Watt for the dynamic power and 4 times longer for the execution time. I got this from the dynamic power equation. Bling! You've reduced the frequency by a factor of 4. So, the program will slow down also by a factor of 4. Now, what about power? Let's think about it relative to the dynamic power of the first design. First, since the frequency went down by 4, then from the dynamic power equation, so too will the dynamic power decrease by at least the same factor. Why at least? Well, remember that frequency and supply voltage are supposed to be adjusted together proportionately. So that's an additional one-sixteenth reduction in dynamic power. Overall that's a 64 fold reduction to just 1 Watt. In fact, this exercise is the usual argument people make for going parallel through multi-core designs, rather than, say, increase in clock frequency. In this case, suppose you had enough parallelism to use multiple cores. Then you could take this chip, replicate it four times in order to get these programs to run in the same time, and you'd still save power overall. It would just run at 4 Watts instead of 64 Watts. Now this quiz is actually a pseudo quiz. By that I mean it's not designed to test what you've learned. Rather it asks you to make an educated guess and in coming up with that guess hopefully you learn something in the process. Start by recalling the dynamic power equation. Notice that it has 4 nobs for controlling power. The first is the capacitance of the circuit element, like a gate. The second is the supply voltage of the circuit. The third is the clock rate or the clock frequency. Lastly, there's the activity factor. The activity factor was the frequency of state changes relative to elapsed cycles. Now put on your algorithm and software hat. Do you have any guesses or intuition about which of these knobs might be within the control of algorithms and software? I want you to think about it, and if you don't know, it's okay. In fact, it's a little bit of a research question, and there isn't necessarily a right or wrong answer. The main point of this pseudo-quiz is to make sure that you stop and think about the question before I tell you what I think. Here's how I would have answered the question. I would have marked supply voltage, clock frequency, and activity factor. If you disagree, let's duke it out in the forums. Let's start with capacitance, C. Why didn't I check it off? Partly it's because I never really understood what capacitance is. Electricity and magnetism have always been a bit of a weak point for me. Curse you, Michael Faraday, with your brilliant intellect, chiseled jaw, haunting eyes and delicate, albeit slightly sooty 19th century industrial good looks. Personally, I wouldn't pick capacitance because it's a geometric and electrical property of the materials used to build the chip. I don't normally think of controlling that at the level of my algorithm or software. Though in the future, who knows. Next, let's consider voltage and frequency together since they're supposed to vary together. In fact, there's a feature of many modern processors and memory systems for changing these. It's called Dynamic Voltage and Frequency Scaling, or DVFS for short. DVFS is typically controlled in hardware, but there are some processors and operating systems that offer software level control. In Linux, for instance, there's something called cpufreq. Speaking of freaks, look, it's CPU Rick James! She's CPU freaky. [SOUND] Lastly, there's the activity factor. I've checked it off, but it would be fair for a hardware person to tell me I have an overactive imagination. Remember the activity factor is about how frequently state transitions occur. So if you know what at the algorithm or software level that there are big chunks of the processing system that you don't need you might imagine turning those chunks off. For instance, suppose you're doing a reduction. Algorithmically, you know this operation is mostly about streaming the input data. So, that implies there's no reason to cash it. Therefore, if the hardware gave you a knob to turn the cash off, you might do that. Anyway, traditionally the topic of the dynamic power equation is one which mostly hardware and low level software folks think about. But, in the spirit of Danny Hillis's question, I wanted to get you thinking about whether there are interesting algorithmic implications as well. Let's quickly check that you remember the definition of power as it relates to energy and time. Consider two systems, A and B, each one is characterized by its execution time to do the same computation as well as its execution energy. Now, suppose A uses less energy than B, but B takes less time than A. If it helps, let's visualize this relationship. This is an energy time phase diagram, and here's system A and here's system B. Here's my question, which system has lower average power, A or B? If you think they have the same average power then check them both. Or if you think there's not enough information to tell, then check neither. I don't know about you, but I'm going to go with system A. I started with the definition of average power for the two systems, which is just energy divided by time. Then, I recognize that this inequality on time can be written a little bit differently. Namely, 1 over the time of A is less than 1 over the time of B. If you now compare these two relations to the definitions of power, you should see that the power of A has to be less than the power of B. You can see the same thing in the energy time phase plot. Power is energy over time, and energy over time in the plot corresponds to the slope of the lines. And since the slope of B is steeper than the slope of A, then it must use more power. Consider two systems A and B. Suppose these systems differ in only two respects. System B takes twice as much energy as system A, but it also runs three times faster. Now, suppose you use dynamic voltage and frequency scaling in order to rescale B. So that it's average power is the same as A. My question is this. After rescaling, will B still be faster than A? It's a yes or no question. And for this question, I want you to ignore constant power, which we called P0. In other words, just think about dynamic power. Well, if you want to know what I think, and I'm sure you don't, I think the answer is yes. Intuitively, system B is three times faster than system A but at only the cost of twice the energy. So in terms of energy and time you get more bang for you buck from system B. Now to really convince you, let's be a little bit more precise. First, observe that as given system b uses six times as much power as a. So what about DVFS? The key relationship that DVFS enables you to control is what is the one in which power goes like frequency cubed? So we can define the power of systems A and B as follows. The power of either system A or B is basically some system dependent constant, kA or kB, times frequency cubed. Now we rescale B to match the power of A. Let's call this new system C notice that it's the same as B, except for its frequency. The frequency of C is what we're adjusting relative to B in order to match the power of system A. Now write down the power relationships that you know. The first one is that the power of B over A is 6. Notice that this determines the relationship between the frequency of B and the frequency of A. The second power relationship is between C and A. Which have the same power. Again, this tells something about the relationship between their frequencies. Now let's consider the execution time on system A relative to system C. If this ratio is greater than 1 then system C is faster than system A. So let's see if it is. This ratio can be re-written, namely the time of A divided by the time of C is equal to the time of A divided by the time of B, times the time of B divided by the time of C. The Bs cancel. Now systems B and C are the same except for their differing clock frequencies. This tells us how the ratio of their execution times will differ. That's because time is inversely proportional to frequency assuming that we can neglect any other non local compute cost. Now, to simplify this ratio, let's do a little bit more algebra kung fu. Hi-ya. Inserting these two fa factors, which cancel, allow us to substitute the known for the frequency relationships. In particular, their product is just the constant, 1 divided by the cube root of 6. Neat. All the unknown constants went away, which is great because man, life is complicated enough when you know the constants. Am I right? The last step is just recognizing that you already know what the ratio of TA divided by TB is. So plugging in all the constants, you should get the cube root of nine halves. That is ugly. In regular people numbers, that's about 1.65. That's greater than 1, which means system C is still faster than system A. Let's see if you can connect time, energy, and power as a function of the algorithm a little bit more directly. Here's the main high level difference between time and energy. You can reduce time by hiding it or overlapping it using, for example, parallelism. Energy is different. With respect to energy, every operation has some energy cost and you have to pay for that cost for every operation. Now let's go back to our simplest model of parallelism. That was the work-span or multi-threaded DAG model. In that model, you had a bunch of abstract algorithmic costs that you thought about. Do you remember them? One was work, another was span. There was also the ratio of work to span, or the average available parallelism. There was also the execution time given P processors. For time, we had both lower bounds and upper bounds, the upper bound coming from Brent's theorem. And there was also Speedup. Note, that this definition of Speedup is a little bit relaxed in the sense that it's (Self-) Speedup. For (Self-) Speedup, I don't use the best sequential time in the numerator. Instead, I use the time of the parallel algorithm when run on a single processor. But remember, if your algorithm is work optimal, then self speed-up and conventional speed-up relative to the best sequential case are basically the same, asymptotically. Okay, my question is this. Of these five five measures, which one is the best metric to use to quantify energy? Now, think about these informal definitions and your options carefully before you answer. And I want you to choose just one option. I don't know about you, but I would argue that the best answer is work. The definition of energy says that you need to pay some energy cost for every operation. Well, the whole point of work is to count all operations. So, if you're interested some measure of asymptotic energy, then work is the right analogy. By the way, this idea implies something kind of neat. It says that at an algorithmic level, if what you care about is energy, and supposing that the energy per operation is bounded by some constant, then finding work optimal algorithms is super important. Recall the metrics of interest in the work span or the multi-threaded DAG model. There's a total number of operations for work, there's a critical path length, or span, there's the ratio of those two, or the average of available parallelism. There's execution time bounded from below and above according to Brent's theorem, and finally there's self speedup. Self speedup is relative to the same algorithm run on just one processor rather than relative to the best sequential algorithm. Lucky you, I have a question for you. Of these five measures, which one best expresses dynamic power? For the purpose of this exercise, you can ignore constant power, and you can assume a constant energy per operation. The case of variable energy per operation is also a lot of fun, but let's not consider that for the moment. Dun, dun, dun. Well, I went with speed up. My reasoning stems from the definition of power. It's energy divided by time. Energy is like algorithmic work and the algorithmic work would basically be the same thing as the time to execute the algorithm on a single processor. So the time on one processor divided by the time on p processors, looks like energy, which is work, divided by the time on p processors. Now think about that for a second. If you need to run at low power, then either you need to use very little energy, or you need to go really slow. So if you're energy optimal, then there's a fundamental tension between speed and power. High speed is less time, which would appear to imply more power. This probably explains why my de-identified no-name brand cell phone has a pretty good battery life but runs slow as [BLEEP]. Oops, did I say [BLEEP]? Can you drop f bombs in OMSCS? Here's something to ponder. Can you use algorithmic parallelism to go faster without increasing power? For the sake of argument, let's consider an algorithm in the work span model. Now suppose Brent's theorem holds, provided you run it at some base clock frequency on a PRAM type machine with p cores. Now you suppose you slowed down the clock by a factor of sigma. Let me give you an example. Suppose the initial base frequency was 1 GHz and sigma is 2. And the new frequency is F over sigma or 500 MHz. Ok, so that's what sigma means. Now, a frequency change also means the power per core goes down. >From the dynamic power equation, it should go down by a factor of sigma cubed. Put another way, you can actually use sigma cubed more processing cores without increasing the overall system power. That's assuming constant power is 0. So, my analytical question for you is what value of sigma should you use? I want you to type your answer symbolically in this box, in terms of W, D, P, and numerical constants. Okay, this is a messy one, that is assuming I did something reasonable. Blah. I basically got the cube root of a bunch of stuff. Stuff in this case is 2 times W-D over PD. The exact answer isn't as important as how you get there, so if you got something different or the grading robots didn't give you credit for an equivalent expression, don't worry. Okay, here's what I did. First, I used Brent's theorem. This gives an upper bound on time given that we've slowed down the cores but we get to use more of them. The first sigma models slowdown, the second sigma cubed models the increase in processor cores. Then I thought to myself, self, let's pick a sigma to minimize the righthand side. Let me give the righthand side a name. G of sigma seems like a sufficiently creative name for [INAUDIBLE] work, so let's go with that. To minimize g, let's return to the dark recesses of Calc 101. In particular, let's take the derivative of g with respect to sigma. We'll get something, we'll set it to 0 and solve for sigma. When you do that, you basically get this mumbo jumbo. You can also take a second derivative of g just to verify that it's positive, meaning that sigma is really a minimizer. The more interesting question is, can you get a speed up? And the answer depends on the value of P. But the short answer is, sometimes. I'll leave figuring that out as an additional exercise for you. There's no question that even simple models of computers, the models that ignore the memory hierarchy, parallelism, and communication have, nevertheless, been extremely productive. In other words, the CS community has produced an amazing array of applications without having to think too hard about the physical realities of machines. Or so it seems. But if we are in fact approaching the end of Moore's Law and as we hit new performance bottlenecks like limits on power and energy. Then I think you have to stop and ask yourself the kind of basic questions that Danny Hillis was asking in the early 80s. These are questions like, how do we make the most of the machines we have? Is there a role for physical reality and the design of algorithms and software? And how exactly would we do all that and still be productive developers? These unrelated questions are at the frontier of HPC research. They include wackier ideas like quantum computing and biological computing. These are among the main extremes of physical computation. Now, while our course won't go so far as to talk about these wackier ideas, lucky for you, there's plenty of time afterwards for you to think about that. Given a machine with a two-level memory hierarchy, what does an efficient algorithm look like? That's the topic of this lesson. Input/output avoiding or I/O avoiding algorithms. In this case, I/O refers to the transfers of data between slow and fast memory. Now in this lesson, you'll assume that these I/Os are the dominant cost, and you'll ask how to minimize them. You'll also see examples of how to argue lower bounds on the number of IOs so that you can see whether a given algorithm you've got achieves the lower bound. So, let's go avoid some IOs. [SOUND] One of the main results of this lesson is going to be a lower-bound on the amount of communication needed to sort on a machine with slow and fast memory. And here is that lower-bound now. For an input of n items on a machine with a fast memory of size Z and a transfer size of L words per transfer, the lower-bound is n over L, log base Z over L n over L. Again, the base of the log is z over L rather than our usual two. Now what's a lot of symbolic mumbo jumbo. But what does it really mean? For instance, how would this function compare to, I don't know, plain old n log n? Or maybe even other quantities. I have an idea, let's have you find out. Now suppose you're given the following. The first is an input data set of total size 1 pibibyte, or 2 to the 50th bytes of information. A pibibyte is the power of 2 analog of a power of 10 petabyte. Now, this input consists of records or items, each of size 256 bytes. Initially, let's say this input all resides in slow memory. Slow memory in this case is probably disk. Let's say that the fast memory is DRAM. Suppose the DRAM can hold 64 gibibyte, or 2 to the 36th bytes. Finally, let's say that the minimum transaction size is 32 kibibytes. That's also 2 to the 15th bytes. That's probably a reasonable average for an efficient disk to memory transfer. Now suppose I give you a bunch of algorithms, each of these algorithms performs different numbers of transfers. Algorithm a does n log n transfers. B does n log n / L transfers, c does exactly n transfers, d does n / L log of n / L transfers. E does n / L log of n / z transfers, and finally, f does n / L lob base z / L of n over L transfers. Notice that these are log base 2 where a log appears except for the last case which is log base z over L. What I want you to do is evaluate each of these expressions given the numerical values. In each case, I want you to compute the value in trillions of operations or Terra-ops abbreviated Tops. So one Tops is ten to the twelfth operations. In this case, all the operations are transfers. Let me make this slightly less tedious, and you are welcome. So I've given you the answer for a, which is 185 Tops, and c, which is 4.40 Tops. In each case, notice I've written the answer to three significant figures. Here's what I calculated. The calculation, itself, is not very interesting. It's basically just plugging and chugging numbers. In this case, n is 2^50 bytes divided by 2^8bytes per item or records. So, that's a total of 2^42 items or records. Similarly z is 2 to the 28th records and l is two to the seventh records, so plug those values in, round to three significant figures and you should get these numbers. The more interesting bits are the numerical improvements relative to the baseline of n login equals 185 terra ops. One big improvement comes from reducing n to n over l. This factor is really about ensuring that, when you make a pass over the data you do so in L size transactions, as much as possible. The other big improvement comes from going from log base 2, to log base z over l. Now, when you calculated this you may have used the following handy fact about converting logs. Log base z over L of x is log base two of x divided by log base two of z over L. This improvement involves the capacity of fast memory, z. So this win comes from ensuring that your algorithm utilizes the fast memory capacity, z, to the greatest extent possible. Okay, one last remark. Now these speedups, are notional. When we do algorithm analysis we usually do it in an asymptotic sense, and so there's always some other hidden constants. The main point is to build your intuition about how performance changes as you change the algorithm to improve with respect to L and Z Let's now think about the problem of sorting n elements in a two-level hierarchy. Initially, let's assume a processor is sequential. Here's a natural scheme based on the merge sort idea. Start by logically dividing the input into chunks of size proportional to, but no greater than, z, so that a single chunk fits in fast memory. Next, read a chunk of the input from slow memory into fast memory. Then sort this chunk. I'll refer to this sorted output sitting in fast memory as a sorted run, or just run for short. As a mnemonic, this left to right arrow is to remind you that this chunk is sorted. Since the run is now sorted, write it back to slow memory. You'll repeat this process on each input chunk. Assuming each of the input chunks was of size F times Z, where F is some constant less than one, then you'll end up with about N/F times Z sorted runs in total. I'll refer to everything we just did as phase one of the procedure. Again, that's partition the input into approximately Z size chunks. And then for each chunk, you read a chunk, sort it and write it back. Following the merge sort idea, in phase two, you want to merge all the sorted runs into a single final sorted run. Before discussing that, let's analyze phase one the partition sorting step. Recall the initial sorting phase, or Phase 1 of the external memory merge sort scheme. You first partition the chunks, and then for each chunk, you read it into fast memory, sort it, and write out the result. In this quiz, I want you to count the number of asymptotic transfers and comparisons incurred at each step. That is, in each of these boxes, I want you to enter the number of slow fast memory transfers or the number of comparisons aggregated over all iterations. Enter them symbolically in the big O sense. Enter your answer in terms of n, z and L and maybe other numerical constants, but ignore f. F was just some fudge constant to make sure that the size of the input buffers fit into fast memory along with any extra working space we might need. And just to repeat, enter your answer as totals taken over all n over Z iterations. For simplicity, you can assume that everything divides everything else. Or to be more precise, the L divides f times z, and f times z divides n. Lastly, assume that the local sort is an optimal comparison-based sort. Here's what I was looking for, for the reads and the writes a total of n/L transfers. For the comparisons, n log Z. Let's look at the transfers first. The reads and writes involve about Z/L transfers each and of course you repeat that for all iterations. So that's a total of n/L transfers. Now what about comparisons? An optimal comparison based sort will incur about Z log Z comparisons. Doing that for all n/Z iterations gives you n log Z. The good news is, so far your scheme is giving you something proportional to n/L transactions. That's good because it means you're doing a good job utilizing each transaction. Good job, you! Here's a candy bar. You're nuts for HPC. [SOUND] Suppose you have m sorted runs in slow memory where each run is of size s items. The total number of items, by the way, is n equals m times s. Your task is to merge all of these sorted runs into a single sorted run that would complete the merge sort algorithm. An easy scheme based on the classical merge sort idea is to merge pairs of runs. And then pairs of pairs and so on until you get a final single run. Now observe what happens at each level. At each level k starting at 0, the size of the run is 2 to the k times s. Now it's probably not too hard to sort out the details. But just to be clear, let's quickly walk through the steps. Start by considering a pair of runs, each of size 2 to the k minus 1 times s. Let's call these A and B. Initially A and B live in slow memory. Your goal is to produce a merged run, C, which will hold 2 to the k times s items. Let's call the output buffer C. To execute this merge, let's maintain three buffers in fast memory. Each fast memory buffer will hold L elements corresponding to the transaction size. Let's use two of these buffers for storing elements from A and B, and the third one for storing elements of the output. I'll refer to the fast memory buffers as A hat, B hat and C hat. To perform the merge, start by reading one L sized block from each of A and B sitting in slow memory and move them to fast memory. Then carry out the following steps. You'll iterate until you've either read all of A or all of B. You'll locally merge elements from A hat and B hat into C hat. Do that until you run out of elements from A hat or B hat or until you fill the output buffer of C hat. If you empty A hat or B hat then just read more elements from slow memory. If you fill C Hat, then just flush the result to slow memory and empty C Hat. Finally if you exhaust A or you exhaust B you just need to copy the remaining elements from B or A. So when all is said and done, what does it cost to merge the pair of runs A, B? This scheme only ever loads elements from A or B from slow memory once. And it only writes a given output block once. So, that's a total of 2 to the K + 1 x S / L transfers. Regarding comparisons, there are also just a linear number of them. Now, this is all just to merge one pair A and B. So, let's go back to the original merge tree. The analysis you did was just for one pair. Now, at each level you can count the number of pairs. That will be n divided be 2 to the k times s. What about the total number of levels? That's just log base 2 of n divided by s. Combining these costs over all levels. Here then are the total costs. The number of transfers is two times n over L log n over s. The comparisons are n log n over s. A good question to ask yourself now is, self, is this good or bad? Recall our overall template for a merge sort on a two level memory hierarchy machine. Phase 1 has two parts, partitioning the input into chunks, and then sorting each chunk. Phase 1 produces a bunch of sorted chunks, or runs. The goal of Phase 2 is then to merge all of these runs. Now suppose you implement phase two using the the two way merging scheme. What then become the overall asymptotic costs of the entire merge sort. I want you to give symbolic answers here and here for the number of comparisons and the number of transfers respectively. Express your answer symbolically in terms of n, z, and L. As usual, you can assume that things always divide evenly, and they're convenient powers of two, etc., etc., so your answer isn't too messy. You've previously analyzed Phases 1 and 2. For Phase 1 the comparisons grow like n log Z and the transfers like n/L. For Phase 2 the comparisons grow like n log n/Z and the transfers like n/L log n/Z. Let's combine these. The comparison terms combine to just n log n. That's good news. It means the mergesort scheme is work optimal, relative to any other comparison based algorithm. What about the number of memory transfers? Basically, Phase 2 dominates and you get n/L log n/z. That's not too shabby. To see how far you have to go it's good to cook up some numbers for n, L, and Z and compare them to the known lower bound. That lower bound turns out to be this. I'll leave this last bit as an exercise for you. Doing a merge sort with 2-way Merging is pretty good in terms of the number of slow fast memory transfers. By way of reminder, that number asymptotically is n over L log based 2 of n over z. I've rewritten this in a funny way for reasons that you'll see momentarily. Now this number of transfers still falls short of what it turns out is the lower bound. And here's that lower bound now. It's n over L, log base z over L of n over L. Here is it is rewritten in terms of log base twos. So this scheme relative to this scheme is off by a little bit. That little bit is approximately log base 2 of z over L. And, in fact, it's not even a little bit. For typical adjacent levels of the memory hierarchy on a real machine like disc and main memory or main memory and the last load of cache. It might be roughly of factor of ten or as high as a factor of 100. Hence, my question for you. Why doesn't merge short, based on a 2-way merging scheme, obtain the lower bound? This question is deliberately open-ended, so there isn't necessarily a very precise single correct answer here. I'm mostly asking to force you to think about how two-way merging works. Force you? I know what you're thinking. What? Is this Russia? Are you forcing me to Gulag? And if you are Russian, I am truly sorry if I have offended you. It's just that my wife is Polish, and there's this natural distrust there. If you don't believe me, just go ask any mid 20th century European historian. As I said, there's no correct answer per se. So what would I have said if I were in your shoe or pajamas or whatever it is you're wearing right now? Maybe I would've said this. Two-way merging does a lousy job of utilizing fast memory capacity. Remember, the merging procedure only works on pairs of arrays at a time, and it just needs one block of size L of fast memory per pair. So it's very sensitive to L, but it's not at all sensitive to Z. Hm, someone ought to fix that, I'm looking at you. Yes, you, I'm looking at you. Hello, it is I. Recall the basic idea of how a two-way merge works. You're initially given a bunch of input runs, each of, let's say, the same size, and each one sorted in ascending order. Your goal is to produce a single sorted output. A natural scheme, based on the classical merge sort idea is to take pairs of runs and combine them in a tree-like fashion. One problem with two way merging is that it doesn't really use fast memory to its fullest potential. Remember how merging and fast memory works? At any point in time it uses very little of the available fast memory space. In fact it uses just three L-sized blocks. There's one block for each of A and B, and one block for the output C. So can you do better? A natural idea is to merge not two runs at a time, but a bunch of runs, let's say K. In fact, let's start by considering one of these K way merges. You're given a set of K input runs, each of size S. Let's say they start in slow memory and they're sorted in ascending order. Let's also suppose that you've chosen this value of K so that at least K plus one blocks of L will fit in fast memory. Your choice of K is not arbitrary. Let's say you've chosen it so that K plus one L-sized blocks will fit in fast memory. This way you can reserve one block for each of the inputs, as well as an additional block for the output. Initially, you would fill the inputs with blocks of the input runs. Now at each step of the local merge, you need to know which of the K input blocks has the next smallest item. Let's say that these shaded items are the next ones to consider from each of the K input blocks. Of these, you need to somehow find the smallest. Let's say that's this item. We'll come back to precisely how to find it in just a moment. You can move this smallest item to the output buffer. The next item from the source buffer now becomes active. You then repeat this process. For instance, maybe this first block has the next smallest item. So move it and make the item that follows it active. As with two way merge, you will at some point fill the output block. When that happens you just need to flush it. Similarly, you will at some point exhaust one of the input buffers. Again, if there are any unread blocks of the input that remain, you just refill it. Now let's return to the question how should you pick the next smallest item from the input frontier? You have several natural options. For instance, one simple way is to do a linear scan. This is fine if K is small. Otherwise, you might consider some type of priority queue data structure, like say a min-heap. Let's say we go with a min-heap. You would then have the following operational costs. After loading the first K blocks, you'd have to build the heap, which would cost you O of K operations. Then anytime you wanted to find the next item to merge, you'd call an extract Min, which would cost you log K operations. And after you extract an item, you might need to replace it, which would also have a logarithmic cost. Now remember, these are all fast memory operations, so when we consider these costs, we'll just count them as comparisons. Now, assuming a heap-based implementation, what's the cost of a single K way merge? Let's start with slow-fast memory transfers. You only ever read distinct input blocks once, and you only ever write distinct output blocks once. So that's 2Ks over L block reads and writes. What about comparisons? First, there's the initial cost to build the heap, Then every one of the K times s items is eventually either inserted or extracted. Since each insert or extract costs you O of log K comparisons, then that's an additional asymptotic cost of about Ks log K comparisons. Remember, that's for a single k-way merge. Now's a good time to pause before seeing what happens in the full K-way merge tree. Recall the merge tree for a multi-way merge. The initial input has n elements. At the very start, which is the top of this tree, the input is divided into a bunch of sorted runs. Let's say each of these runs has about z items. It would probably be some constant fraction less than one of z. It would probably be some constant fraction of z less than one, so that you can do the initial sorting step to produce the runs. Now, suppose you do k way merging as illustrated in this merge tree. The total number of comparisons it will turn out is n log n. That matches what you would expect for any comparison based sort. My question is, what about the total asymptotic memory transfers? Assume that k is some large fraction of z over L so that you can always to a k way merge in fast memory. And because I'm so nice let me give you another small hint. The maximum number of levels L in the merge tree is theta of log base z over L of n over L. Although I've given you this hint, you should probably take a moment to convince yourself that it is in fact true. Here's what I got. n/L log base z/L of n/L. Let's see how I got there. Let's consider some run that's produced at some level of the merge tree. Let's say that's at level i. At the previous level, i-1, each run contains K to the i-1 times s items. That's assuming that at the very top of the merge tree, each run had s items. So how big is the output run? It should be K to the i times s. Now recall the number of memory transfers needed to produce just this one run. It's proportional to the size of the run divided by L. So how many runs are there at level i? It's basically just the length of the input, divided by the length of any run. So that's n/k to the i times s. Thus, the total number of transfers at level i is just the product of these two quantities. In other words, theta of n/L. Notice that this quantity is independent of the level. And, lucky you, I gave you the number of levels. So, multiplying that by the number of transfers per level gives you the result. A mergesort based on multi-way merging has a memory transfer complexity of n/L log base of Z/L of n/ L. So, is that good or bad? As it happens, it's as good as we could hope for, asymptotically speaking. Let's see why. You start with n items. And for simplicity, let's say that these items are unique. Initially you haven't seen any of the items yet. So all you know is that there are n factorial possible ways in which you might order the data, right? Yikes, that's a lot of orderings. Of these orderings, the goal of sorting is to find a specific one in which all the items are in, for example, ascending order. >From this point of view, here's a sketch of one way to determine a lower bound. Suppose you read some data from slow memory. >From this data you learn something which reduces the number of possible orderings. Now, let's be a teeny bit more precise. Suppose there have been t-1 transfers. Let K of t-1 denote the number of possible orderings that remain. And remember, initially, there are n factorial such orderings. Suppose that after you do t-1 reads, you now do another read of block size L from slow memory to fast memory. If you've never seen these items before, then there are L factorial ways in which you can order them. Now, if there are any other items in fast memory already, let's suppose that you already know their relative ordering. So, how many ways can you order up to Z-L old items, plus the L new items? It's at most Z choose L times L factorial. This is the factor by which the number of possible orderings might go down. Then after t reads, here's the lower bound on the number of possible orderings. It's n factorial divided by Z choose L times L factorial quantity to the power t. Now this count is actually a little bit more conservative than necessary. The L factorial assumes that we don't know the order of the L items that we just read. But if you've read the L items before then that won't be the case. That's because you can only perform n/L reads of items that have never been read together before. Thus we can refine this estimate of the lower bound on K of t. In particular L factorial to the t becomes L factorial to the n/L. We're almost done. What you want to know now is when does this right hand side equal one? That is, when does only one ordering remain? The smallest value of t such that this happens is our lower bound on the number of transfers. That turns out to be what we were after. n/L, log base Z/L of n/L. To get this result, you just need two common approximation identities plus a little algebra. I'll post a nice set of notes by Lars Arga that explains all this in detail in case you're interested. And you're definitely interested. Because you love this stuff. You love it. [SOUND] That's you making out with the stuff. Shame on you! This is a family course. Let's think about the algorithm for binary search. Suppose you're given a sorted array A containing n unique elements. Now let's say you're given a target value, v. Here's your computational goal. You want to find the largest index i such that A of i is less than or equal to v. Just to be clear, consider this example. You're given this sorted array of keys and you want to find f's position in the array. The standard binary search algorithm starts with the median element of A, comparing v to it. It then repeats this process on the left or right half depending on whether is v is less than or greater than the median. Suppose A resides entirely in slow memory. My question is this. How many asymptotic transfers might this algorithm incur? You can make the usual simplifying assumptions. Namely, L divides n, A is aligned and all these quantities L and z are powers of 2. Here's the answer, log base 2 of n over L, let's see why. During the search, at some point all the elements being considered will all fall within the same block of size L. Let's write down our currents for the worst case behavior. While the size of the interval n is greater than L, you'd incur a new transfer and then recurse. Once the interval size falls to L or lower, you only need one transfer to process the entire block. The solution to this recurrence is the answer to the problem. The more interesting question is can you do better? Here's a technique for thinking about lower bounds on memory transfers, which is inspired by information theory. I want you to think about it by example for the problem of search given an ordered collection. In this problem, you're given a sorted array A containing n unique elements. You want to find the largest index i such that A sub i is less than or equal to v. The standard binary search algorithm does big O of log n over L transfers in the worst case. Now let's think about what you're trying to compute a little bit differently. To store the index i takes about log n bits, would you agree? Now let's say that when you read a block of L items from A, you learn at most x bits of information about this index. If you had an upper bound on x, then you could get a lower bound on the number of ios during the search. It would just be the number of bits you're trying to learn divided by the number of bits you learn each time you do an L-sized read. Now for the question. What is x in a big O sense? I want you to type your answer symbolically in this box. Here's what I was looking for, log base 2 of L. By analogy to the size of the index, reading L words should reveal about log L bits of information. Consider the block of L words. The search key either goes between these words, or to the left, or to the right. So, you learn log L bits of the index. The lower bound then becomes something like log n over Log L. You can also write this more compactly using log rules. This is the same as log base L of N. Compare this lower bound against the bound for nieve binary search. Remember that was log N over L, which is the same as log N minus log L. So compared to the lower bound, there's about a log L factor difference Consider the problem of searching in an ordered collection. Here's a list of several classical data structures from CS 101, that you might use to store the collection, besides a sorted array. An ordered doubly-linked list, a binary search tree, a red-black tree, a skip list, and a B-tree. Note that the sorted array A is what you get if you do an in order traversal of a binary search tree, and then throw out the left and right pointers. Here's my question for you. Which of these classical data structures, if any, attains the I/O lower bound? The I/O lower bound for search, you will recall, is log n over log l or log base l of n. Note that this quiz is actually a lazy quiz. By that I'm referring to myself, not you. Hey, sorry I need my beauty rest. By lazy, I mean that after you submit your answer, I will tell you my answer and I'll explain part of it, but I won't fully explain it. To do that would be quite involved as I'd have to explain what each one of these things is. That takes all the fun out of you thinking about it on your own. Here's my lazy answer to the lazy quiz. Just B-trees. Rather than telling you why the others aren't I/O efficient, let me tell you why a B-tree can be I/O efficient. Briefly, and hopefully by way of review, let's quickly go over what a B-tree is. Dang union artists. Let's try this again. Briefly, and hopefully by way of review, this is a B-tree. Each node contains a set of keys and a set of child pointers. The branching factor at each node can vary, but it must lie within a specific range. The keys are sorted. Sorry, that's sorted, not sordid. Let's look at one of the nodes. Let's call it X. A moment ago I said the branching factor has to lie within a certain interval. that interval is between B + 1 and 2B- 1 for some B that's at least 2. B is a user-defined parameter. Consider the ith key value at x. Let's call that key case of i. Now consider any key within the subtree rooted at the ith child of x. Let's call that key c sub i. A B-tree data structure maintains the following invariant. The key k sub i lies between the key values of its children to the left and right. It's not too hard to show that the height of this tree is log base b of n. So, in order for B tree search to attain the lower bound on slow fast memory transfers, you just need to pick B appropriately. Namely, pick B to be proportional to the transfer size L. Now the key point is that a B tree can be made to IO optimal, but only if you choose the right branching factor. In particular, you have to make the branching factor specific to the machine. So much for algorithmic portability, or is it? [SOUND] I/O avoiding algorithms can be messy, much messier than their conventional RAM model counterparts, anyway. But we started this lesson by trying to argue that this effort can be very worthwhile. Remember the exercise where you looked at potential reductions in I/Os given realistic memory hierarchy parameters? You saw that there's a lot of potential to make computations faster. Now, this would happen if you could make memory accesses contiguous and exploit fast memory capacity to the greatest extent possible. And those wins can happen even if the factors of improvement are only log L or log Z, in the case of mergesort. In closing this lesson, let me make one final meta comment for you to think about. Our model assumes that the time spent moving data dominates. That means you should look for ways to reduce I/Os. But how do you know whether data movement dominates? In thinking about that question, let me suggest that now is a good time for you to pause, go back and revisit some of our other high-level concepts, like computational intensity and machine balance. In real life, fast memories are managed automatically in hardware or by the operating system. Caches are one example. Now, these automatically managed fast memories are great. You can go about your merry way, writing code and algorithms that are oblivious to what's happening under the hood. The only problem is that, when you do that, performance can suffer. Think of the resource oblivious binary search compared to a resource aware bee tree search. But bee trees have a problem too. You need to tune that bee perameter in a machine specific way, which effects portability. So, this leads us to a really interesting question. Is there a different algorithm or data structure that would be efficient, no matter what the memory hierarchy looks like or how it's managed? In 1999, Matteo Frigo, Charles Leiserson, and a bunch of other MIT yahoos, discovered a way to be cache oblivious, at least for basic problems like matrix multiply and sorting. And that friends, is the topic of this lesson, cache oblivious algorithms. Let me start with a somewhat trivial example just to contrast the notion of fast memory obliviousness to fast memory awareness. Consider the problem of sequential reduction. Let's start by reviewing the fast memory aware version. It scans the list in units of size L to match the memory transfer size. Notice that l is an explicit variable or parameter of the algorithm, but at least the algorithm is I/O optimal. It performs just n/L transfers. By contrast, suppose that the hardware in the operating system managed fast memory for you. For instance, maybe you already know about caches from the OMSCSHPCA course. If so, you could regard this hardware managed fast memory as a cache. Or, maybe you know about virtual memory and translation look-aside buffers from the OMSCS operating systems class. If so, you could imagine the operating system and the hardware acting in concert to move pages of data from disk to main memory. They would swap pages out of main memory, which would be the fast memory in this case, back and forth to disk whenever main memory is full. In either case, the purpose of the automatic systems is to let you write the conventional algorithm. It's a lot simpler, because fast memory is automatically managed, the algorithm doesn't need to move data around explicitly. The automatic system takes each read or write, checks that the data is already in fast memory, if so, it returns the value and otherwise it goes to slow memory in order to retrieve the value. And if it retrieves the value, of course, it'll do transfers in blocks of size L. Notice that the conventional algorithm makes no references to z or L. Thus, we say that the algorithm is oblivious to fast memory. So if we had a good model of how automatic data movement works, we'll be able to analyze conventional algorithms, and say how many memory transfers would they incur. Given such a model, we'll be able to analyze conventional algorithms, and in this case show that the conventional way to do a reduction also incurs just n/L transfers. This discussion points out two questions, which is what this lesson is all about. First, what should we assume about how the automatic system behaves? Secondly, can we create algorithms without z or L parameters that nevertheless match the performance of algorithms that do use z and L parameters? That's what we mean by oblivious, though, of course, we mean it in a positive sense. One last note about terminology. We will by convention refer to this fast memory as cache. It’s just a shortcut. Sometimes cache really means say, virtual memory backed main memory for disk. It helps that it's a lot easier to say cache oblivious than automatic fast memory oblivious. To analyze a cache-oblivious algorithm, we're going to need a model of how a cache works. Here's the model I want you to assume, which we sometimes call the ideal cache model. Start by assuming that slow and fast memory are divided into blocks of size L words. This L is the same as the transfer size. We'll refer to one of these blocks in fast memory or cache as a cache line. So you have a machine in which fast memory is being managed automatically. Consider your algorithm or program. As it runs, it issues sequences of load and store operations. These loads and stores reference addresses in slow memory. For this lesson, let's assume that the algorithm issues these operations sequentially. Now, consider some load operation. Suppose it reads from an address call it a and wants to load the value into a register, call it r. The hardware will check to see if a copy of a is already in fast memory. If it's there, then it returns the value and completes the store operation, which writes to register r. Let's refer to this case as a cache hit, because the value that we want is in cache. If the value we want is not in cache, then it's a cache miss. In this case, the hardware grabs the value from slow memory, but also stashes a copy in the cache. Keep in mind that the hardware has to transfer an entire cache line. Now, which L consecutive words around the address a get transferred depends on how a is aligned. So, what about a store from say a register s to a memory address b? It will behave kind of like a load operation. There's a copy of b in cache, then it's a cache hit and we update the cached value. Otherwise, there's no copy of b in the cache and it's a cache miss. The hardware would load the block from slow memory into cache. In other words, a store miss like a load miss causes a memory transfer. So those were the basics of load and store operations. Here's the next assumption in the ideal cache model. Cache is fully associative. So, what does that mean? Remember that a cache consists of a set of cache lines or cache blocks. Now suppose you load a new block from slow memory, full associativity means that this block is allowed to go into any block or line of the cache. Now you may know about set associative caches and direct-mapped caches. If you do, then you know the real caches typically don't implement full associativity. Rather, they implement one of these schemes, which has the effect of restricting the possible cache lines that are given memory address can go into. Full associativity says, you can ignore this restriction. It's a simplifying assumption that will make our ideal cache model more powerful than real cache is. Now at some point, the cache will be full of previously used values. To make room for new values, the hardware will need to choose some line to kick out or to evict. If the value being evicted hasn't been written to main memory yet, because say, it was a store hit previously, then that will cause another memory transfer. I'll refer to those transfers as store-evictions. So if we have to kick something out, what do we kick out? That leads us to the next assumption of the ideal cache model, optimal replacement. Optimal replacement means that the hardware managing the cache actually knows the future. In particular, the hardware knows all future accesses. It looks at all the blocks currently in the cache and then evicts the one that will be accessed the most distantly in the future. At first glance, this might strike you as being extremely idealistic or optimistic. But in fact, we'll do an analysis of just how powerful this assumption really is in a moment. Let's do a quick summary of all the assumptions of the ideal cache model. We'll model the program as issuing a sequence of load and store operations to slow memory. The hardware manages the z words of cache, which is divided into lines of size L words each. These Z over L cache lines are sometimes called cache blocks. As in the conventional I/O model, slow memory to cache transfers will happen in lines or blocks of size L. If the value for some slow memory address is already in cache, it's a cache hit. And otherwise, it's a cache miss. The cache will assume it's fully associated. Lastly, when we need to evict a cache line, we'll assume an optimal replacement policy. This policy sees the future. One final point. Remember that in the conventional I/O model, we counted memory transfers. In the ideal cache model, we do the same thing. The number of transfers is really equal to the number of misses plus the number of store-evictions. Now I think is a good time to see, if you really understand how an ideal cache might work. Consider an ideal cache of size Z equals four words. Assume that the transfer or line size L is equal to one word. Now consider a program that consists of the following sequence of 10 load operations. Pretend that these 16 bit hexadecimal values are word aligned addresses. Now, suppose our cache is initially empty. Then suppose you execute the first instruction. It loads the value at hex address BEEF. Since the cache is empty, the hardware can store the value at this address somewhere in cache. I've denoted that action by writing the hex address in brackets in a cache line. Remember that by the rules of how an ideal cache operates, we can assume full associativity. That means for an empty cache I could have placed this address anywhere. Here's your first task. Suppose you execute the first nine instructions, that is, all the instructions except the last. Here's my first question, what's the state of the cache at this point? That is, I want you to fill in the addresses that would be in cache after the first nine instructions have executed. You can just type the addresses in the same notation square bracket and hexidecimal address into the lines. Now that's not all. Here's your second task. As you simulate what the cash would do, I want you to count the number of evictions. Enter that value in this box. Here's what I came up with. Hex BEEF in the first entry, FOOD in the second, C0DE in the third and AB8D in the fourth. Note that by full associativity, any permutation of these addresses would've also been a valid response. As for the number of evictions, I got just one. Okay. Let's see if I can convince you of what I got. First, consider the first four unique addresses loaded. Notice that the first four unique addresses actually corresponds to the first five instructions. I picked the first four unique ones because the cache is empty, so it can hold up to four values, namely, the first four unique addresses. This second occurrence of x F 0 0 D will be a cache hit. The next load to x A B 8 D will cause a state change. The cache is full so something has got to go. So which address will be evicted? Well, remember that an ideal cache can see the future. So for each of these addresses in cache, the ideal cache will look at the future sequences of accesses and kick out the address that is touched furthest in the future. So in this case, that's the address xD00D. No love lost here. That dude is a total loser of an address anyway. With a newly freed slot, you can load the value at address xAB8D. After that completes, the next three accesses are all addresses that are in cache, so they're all cache hits and you're done. Notice that in this process, there was a total of just one eviction, so that's what the ideal cache would do. Suppose you execute the first nine loads of this program. Assume that you start with an empty cache. In the ideal cache model there would be one cache eviction, and here's what the state of the cache would look like. This is what happens with an optimal replacement policy which sees all future accesses when deciding what to evict. And you might be asking yourself, self, how does that compare to a more realistic replacement policy? One candidate that you might know about is the least recently used replacement policy or LRU. As its name suggests, LRU evicts the address that was accessed most distantly in the past. So here's my question. Under LRU replacement how many evictions would you expect to see? Again, start from an empty cache. Now I'll put some empty boxes here so you can use this for scratch space. However, the grade-o-bot won't be checking these, it'll just be checking the number of evictions. So I got three evictions. Since the cache is empty, the first five instructions, which contain four unique addresses, yield the following state. Notice that the fifth instruction is actually a cache hit. So what about the next instruction, which accesses a new address, hex AB8D? LRU evicts the least recently used address. So for this program, that would be hex BEEF. In other words of the cached addresses, it was the first one to be loaded, and hasn't been touched since. So out with the beef. And if watching from Texas or Argentina, lo siento. And so, in with AB8D, which I'd been waiting with bated, but not abated, breath. Now notice what happens. All but one of the remaining instructions is a cache miss. It seems that the least recently used heuristic evicted exactly the addresses we would have preferred to have kept. The result is three evictions, compared to just one from the optimal replacement policy. Of course, this is just one example. A natural and interesting question to ask at this point is, what can you say in general about the gap between, say, LRU and optimal replacement? So, how ideal is an ideal cache? Does ideal compare to real? I want to go over some facts that help justify the ideal cache model. Remember that the ideal cache model seems to have some super powers. So a technical question is, are the assumptions of the ideal cache model too strong? What I want to do now is go over the justification from the original cacheable VS model paper, which was by Frigo, et al, and appeared in the Foundations of Computer Science, or FOCS, in 1999. The first fact is about the assumption of optimal replacement. It's a lemma, and here's what it says. Suppose you take your algorithm and you count the number of cache misses it incurs on a machine with an LRU replacement policy. Now suppose you're given a different machine. This machine has the same line size, but it only has half the cache. It also implements an optimal replacement policy, rather than an LRU policy. In other words, it has a better replacement policy, but it has less space. It's both slightly better, being optimal, but also slightly worse, having less space. The lemma says that the number of transfer on the LRU machine will be within a factor of 2 of the number of transfers on this slightly better or slightly worse optimal machine. Now, I know what you might be thinking. Huh? What the? [SOUND] At first glance, this lemma may seem a bit odd, but here's a very natural interpretation. Suppose you design an algorithm assuming optimal replacement, which is what we're going to be doing. Then the performance of that algorithm on a more realistic LRU machine will be asymptotically close. In other words, optimal replacement isn't as strong an assumption as you might think. Now, there's a more specific, technical requirement which is one of regularity. We say QOPT is regular if it's big O of QOPT with twice the cache. In other words, let's say you design an algorithm and you find out what QOPT is on a machine with an optimal replacement policy. What you calculate for QOPT is regular if QOPT is big O of QOPT with twice the cache. So, if you can show that QOPT is regular in this sense, then QLRU will be proportional to QOPT. Recall the Lemma that relates the number of misses in an LRU cache to the number in one with optimal replacement. >From this Lemma there's a corollary about regularity. You can design an algorithm for an ideal cache and if that analysis shows that the number of misses is regular, then an LRU cache will perform just as well. So let's see how to use this. Suppose you design a matrix multiply algorithm, assuming a conventional non stronsense scheme and you do so for an ideal cache. Let's say that your analysis shows your algorithm does n cubed over L root z, misses. For this example let's further assume that the line size is one word. So will this algorithm perform well on a machine with an LRU cache? Let's check the regularity condition. What happens when you double the cash size? Well, it introduces just a small constant factor. So the regularity condition is satisfied. And because Qopt is regular, QLRU will be proportional to it. By the Lemma, you could also evaluate the right-hand side of this relation. Here's what you get. Basically an upper bound on the number of LRU misses. This brings us to one of life's great pleasures, well assuming you're a bit of a geek, or is it nerd? Let's just say enthusiast, shall we? The pleasure is that the Lemma is not hard to see. For simplicity, let's assume that the transfer size is one word. Now imagine the sequence, or trace, of all load and store operations performed by a program. Divide this trace into phases. Let's define these phases in a special way. In particular, suppose that each phase references exactly Z unique addresses. Just to be clear, each phase might contain many more than Z loads and stores. The phases might also have widely varying numbers of loads and stores as the illustration suggests, but the number of unique addresses is always exactly Z in each phase. Now let's say the machine has an LRU cache of size Z. Think about what happens during some phase, let's say phase i. At the beginning of the phase, it's possible that the cache is full. So, the first time each unique address of phase i is seen, it could cause an eviction. Thus, by the definition of a phase, the number of cache misses could be as high as z. Now think about what happens on a cache with optimal replacement, but half the capacity, Z over 2. Remember this optimal cache can gaze into its crystal ball to see all future accesses. So during the previous phase i- 1, what might the optimal replacement policy have done? Well it may have arranged itself so that the first z/2 unique addresses seen in phase i are exactly in cache, that would mean no evictions. But since the optimal cache is only of size z/2, then it must, at that point, start loading addresses. In other words, the smaller but all seeing cache has to perform at least z over 2 transfers. The competitiveness Lemma then follows from these two facts. One is at most Z and twice the other is at least Z. Time for a quick quiz. Please contain your enthusiasm. Suppose you have a sorting algorithm that, on an ideal cache, does n over L, log n over L divided by log z over L cache misses. Here's my question. How many transfers would you expect, asymptotically, on a machine with an LRU cache of the same size and length size? Type your answer symbolically here. The answer is it is the same as the optimal algorithm. Of course you could have also typed this expression. To get this result, just observe that Q opt is regular. I mean in the sense of what happens when you double the cache size. The dependants on z appears here. So, expand that. It's just log z over L+1. In other words, doubling the cash size doesn't really change the asymptotics of Q opt, so Q opt is regular. Therefore, nLRU cache will perform just as well as the optimal cache, asymptotically speaking. Consider the problem of the matrix-matrix multiply operation, C gets C+A*B. Suppose we run this operation on an ideal cash machine. And let's make our usual simplifying assumptions. Namely, that the matrix is square, and stuff conveniently divides other stuff. Now if you want to minimize data transfers in a memory hierarchy one way is to reorganize the computation to multiply b by b sub-blocks. Again for simplicity let's assume that b divides n. The corresponding pseudocode algorithm would look something like this. Now to make this algorithm cache efficient, you would just choose b so that these three b by b blocks fit in cache. In other words, b ought to be proportional to the square root of z. Now there's a subtle detail here about what fitting in cache means. To see it, suppose the matrices are stored in column major order. That means the elements of each column are laid out consecutively in linear memory addresses, with one column following the other. This layout is common in linear algebra packages, though note that languages like C and C++ assume, by default, that a two dimensional array uses row major order. A little more formally, the ij element maps to a linear address. The offset is i + j * s, where s is the stride or sometimes leading dimension. It has to be at least n. Now let's take any b by b block of one of the matrices. Let's say we just want to fit this block into cache. Then it has to be the case that z is at least L squared. In literature, this is sometimes written as the following lower bound. This condition is called the tall-cache assumption. The tall-cache assumption says the cache should be taller in terms of the number of lines than it is wide in terms of the number of words per line. Let's take a simple example. Suppose our matrix block is exactly the size of the cache z which holds 16 words. Now suppose the line size is four words. In this case, the Tall-Cache Assumption holds assuming the matrix is stored in column major layout columns would map to complete lines. Now suppose instead that L is equal to eight words, in this case the Tall-Cache Assumption does not hold. Columns no longer fill full lines, thus the four by four block doesn't actually fit in cache even though the cache has enough capacity. Now the Tall-Cache Assumption is actually a really interesting one. It's basically an artifact of how we chose to lay out the matrices. Put differently, in a memory hierarchy model and efficient algorithm might be linked to your choice of data structure and how you choose to lay out the data. We'll see more examples of this soon enough. Let's look at some examples of real memory hierarchy parameters. A register file with 16 registers of 16 bytes each. An L1 cache that can hold 32 kibibytes with 64 byte lines. An L2 cache of size 8 mebibyte with a 128 byte lines. A 256-entry TLB or translation lookaside buffer with pages of 8 kibbibytes each. In this case the slow memory is main memory, and the fast memory is the TLB. And finally a virtual memory setup with 32 gibbibytes of physical RAM as the fast memory, 32 kibbibytes pages and disk as the slow memory. My question is this, which of these is a tall cache? To answer the question, you'll need to assume a word size, which I'll tell you is 8 bytes. Note that there might be more than one correct answer, so please check all that apply. My answer is everything but the TLB. Here's the condition that you need to check, does the cache have more lines than words per line? For instance, let's take the register file. A cache line is just a register, and the line width is 16 bytes. So the number of lines is 16, and the words per line is 16 Bytes divided by 8 Bytes, or 2. So the register file is indeed tall. Now let's take the L1 cache example. The number of lines is 2,048. That's 32 kiB divided by 64 bytes. The number of words per line is 8. So it's also tall. Now let's skip to the TLB. The number of lines is just the number of TLB entries, which is 256. But that's actually less than the number of words per line, which in this case is 1,024. Thus, this quote-unquote cache is short. So here are the final answers again. The main conclusion of this exercise is that most levels of the memory hierarchy are indeed tall, but not all of them are. An efficient cache aware matrix multiply performs a sequence of of submatrix or block by block multiplies. You choose the block size so that, nominally, three blocks fit in cache. The corresponding algorithm looks like this, and notice that the block size is denoted by little b appears directly in the algorithm. Now you can show that the number of cache misses is asymptotically n cubed divided by L times b. And since b is proportional to square root of Z, that's just n cubed over L root z. As it happens, for any non-Strassen matrix multiply, no algorithm can do asymptotically better than this. Now, since b is a function of the cache size, and it appears in the algorithm, we say this algorithm is cache-aware. Here's a question for you. Is there a different algorithm that dos not refer to L or Z, yet somehow still attains the lower bound? The answer is yes. It's based on the the idea of divide and conquer. Let's assume for simplicity, that the matrices are n by n, where n is an integer power of two. The algorithm first partitions the matrices into two by two submatrices. Then performs eight recursive matrix multiplies to update the two by two C block. To be more precise, here's the pseudocode algorithm. The base case occurs when A, B and C are scalars. The algorithm just performs a scalar update. Otherwise, the input matrices are logically partitioned into quadrants as shown in the figure. The algorithm performs eight recursive matrix multiplies to update the four quadrants of C. Now as a pseudo code reveals, this algorithm is cache oblivious. Unlike the block algorithm, it makes no reference to the size of the cache, or of the line size. Now we need to analyze this algorithm Let's forget about cache misses for the moment and suppose you just wanted to count the number of adds and multiplies. How would you do it? Well the algorithm's recursive so you'd write down a recurrence relation maybe like this one. The first case counts the eight recursive calls on half the problem size. The base case counts the multiplies and adds or flops and namely there are two. If you solve this recurrence exactly you would get 2 n cubed flops which is exactly what you would expect. Now, what about cache misses? Imagine the recursion tree. Each node is a function call. There's eight way branching at each node, and the leaves correspond to the base case. And of course the problem size shrinks progressively from n down to one. At some point in this recursion, all the operands of a subtree fit in cache. Let's say that level is little l, and call that size n sub l. At that point, it must be that three blocks fit in cache. If the matrix is stored in row or column major layout, then you'd also need to assume tall caches. Then you can conclude that n sub l will be less than or equal to some constant fraction of z. I'll denote that fraction by the fudge constant f for the moment. So a recurrence for the cache misses will have two cases. The first is when the operands fit in cache. The number of cache misses is just proportional to n sub L squared divided by L. That's the base case. Otherwise, there's the recursive case. You pay for misses at each of the eight branches plus, let's assume, a constant number within the function call itself. If you solve this recurrence, you should get the following, n cubed over L root z. This matches the cache aware algorithm, as well as the lower bound, yet remember that the algorithm itself never refers to z or L. That's oblivious, baby! Let's think about the binary search algorithm. We're given a sorted array, A, containing let's say n unique elements. Then, given a target value, v, you want to find the largest index i such that a sub i is at most v. You can find this index by binary search. To compute the number of cache misses, you'd set up recurrence, like so. Once the search interval falls within a cache line, there would be just one cache miss. Otherwise, you'd pay for one miss, plus any additional misses to search the remaining half of the array. Solving the recurrence, you'd find the following. Big O of log n over l. Compare this to the lower bound. Binary search differs from the lower bound by about a factor of log L. So it's not optimal, but one nice thing about binary search is that it's already cache oblivious. It makes no references to the cache size Z or the line size L. But it begs the question, how do we get to the lower bound? In fact, there's a way to do it. Leave the logic of the binary search algorithm intact, but change the data layout. Remember that a binary search tree maintains some ordering of its elements. Let's number this tree according to an in-order traversal. If you interpret these numbers as addresses or index positions, then the layout of the tree nodes is equivalent to sorted order. But there's nothing sacred about this layout. In fact let's consider a different ordering which is called the Vanamdebos layout. I’ll sketch this now but if you want some details, see the nice tutorial by Erik Demaine. The idea is to use the following recursive layout. Suppose you start with a complete binary search tree. If it has about n nodes it should have log n levels. Now divide the levels in half. So there would be about one half log n levels above the cut line and about one half log n below. This also means the upper subtree will have about root n nodes. Below the cut line, there will about root n subtrees, each of size root n. Here's the Van Emde Boas layout idea. You have a binary search tree that you'd like to lay out linearly in slow memory. After partitioning the levels, lay out all of the upper sub-tree elements together. And then concatenate them with the lower sub-tree elements. And when I say lay out the elements together, I mean recursively apply the Van Emde Boas layout to each sub-tree. So, what does this buy you? Let's zoom in on the tree, looking at the point where the sub-trees fit within cache lines. That is, in the figure the elements in each of the smallest sub trees shown fit within a cache line size hole. A binary search in this tree takes some path from the root to the leaf. Since the sub trees are a size L, you only generate a cache miss when you hit the root of one of the sub trees. Now the maximum height of one of these little cache line size sub trees is log L. So, starting at the root of the tree, how many of these size L sub trees will you visit? Well, the height of the tree is log-n. So, on any path from root to leaf, you'll encounter log-n over log-l sub-trees. That's totally awesome because that is optimal. Woot! The important lesson is that data layout matters. What we did hear that was so cool is we took the standard binary search algorithm, reshuffled how we stored the data in order to get an IO optimal algorithm, and the layout is itself cache-oblivious. Suppose you're given this complete binary search tree. Here's what the conventional in-order ordering of the nodes would look like. The numbering serves as offsets for laying out the nodes linearly in memory. And, by the way, if this is an actual tree data structure with left and right pointers, let's assume that those pointers are stored along with the node values. But, what about a different ordering? For this quiz, I want you to generate the sequence that would result from a Van Emde Boas ordering instead. Enter the node values and the linearized array that appears below the tree. So, you'd type a, b, c, d, e, f, g, or whatever in these boxes. Here's the sequence I calculated, h-d-l-b-a-c-f-e-g-j-i-k-n-m-o. So how did I get that? Recall that the first step is to divide the levels in half. Everything in the upper subtree goes into the output array first, followed by the remaining subtrees. So what about the order within a subtree? To get that order, you apply the partitioning recursively. So this result HDL is the first thing you'd get after a whole recursive partitioning of the upper subtree. Let's make one last observation about this ordering. Notice the traversal path. If you know anything about space-filling curves, the pattern should kind of remind you of that. I think it's kind of neat how the path snakes and slithers its way around the tree, don't you? Thibbit. That is the sound a snake makes but The idea of cache obliviousness is one of those great frontiers of HPC. And it's a terrific starting point for new research. In particular, there's a lot of solid theory but in my opinion there haven't been quite as many successful practical implementations or evaluations of these algorithms. Now if that's the case, you might ask yourself, why bother with cache oblivious approaches? Well, there's at least one argument that I find especially compelling. Before I say it, you should pause the video, and see if you can think of an argument on your own first. [SOUND] Okay so, what did you come up with? Here's the one I like a lot. The present and future is about virtualized or shared environments, like the cloud. So, your program might have to fight for resources, like caches or network bandwidth, and do so in an unpredictable way as other jobs try to push yours out of the way. In such cases, you might not have a better choice than to try to be as resource oblivious as possible.