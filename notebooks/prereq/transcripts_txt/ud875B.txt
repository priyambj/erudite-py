Welcome to Android TV Games. Your game is going to look awesome on the big screen. Android TV is your opportunity to develop for TV and mobile at the same time and in the same APK without much extra effort. That's because Android TV is, after all, just Android. While this will not be a course on how to develop a complete game from scratch, we will teach you how to extend your existing game to also work on Android TV. We'll show you how to work with TV remotes and game controllers using a simple arcade game called Asteroids. We'll also cover a variety of core and advanced topics like nearby connections for second screen input. Ready? Let's get started. Your game is going to look awesome on the big screen. But before we look towards the future, let's start with a trip to the past. First you need to be aware of something called overscan when you're designing your game. Overscan refers to a problem with many older, and some modern TV's. Basically, some older CRTs had trouble centering images, and there was some variability in how different models did it. The solution was to center an image as best they could and then slightly enlarge it to make sure it would take up the whole screen. Unfortunately, this meant that the edges of the image would sometimes be cut off. So here, for example, our Android's feet might not be visible. Surprisingly, even modern flat screen TVs suffer from this problem, although for different technical reasons and to a lesser degree. Here's what to do. I recommend establishing a safe zone for important content. Your best bet is to avoid placing important screen elements within about 5% from each edge of the display. Five percent is a rough guideline and that's because the overscan amount varies on different television. Here's how that might look on a 1080 pixel screen. We could add a 96 pixel margin on the left and right edges and a 54 pixel margin on the top and bottom edges. This is just for illustration. Check out the instructor notes, for more details on the specific margins you should add. If you're designing a TV app, then you don't need to worry about over scan if you're using lean back fragments. For example, the browse fragment you see here automatically applies overscan margins to your layouts. If you're building a game that will run on both a mobile device and a TV, then at run time, you might want to detect what type of device you're running on so you can adjust how the screen's base is used. You can do this using the UI Mode Manager. And there's more details on this in the building TV app section of this course. The biggest difference between TV games and mobile games is how players interact with them. Instead of using a touch screen, players control Android TVs using remote controls or game controllers. There are two basic types. The first is a D-pad or directional pad. This is the basic style of remote control that most Android TVs ship with. For that reason, it's very important to design for. Just like the controllers you may have used as a kid, it's important to realize that none of these buttons are pressure sensitive. So pressing harder won't make your car accelerate any faster. This is a game controller. Of course, the biggest difference with remote control is the joysticks. Whereas you can think of the buttons on the D-pad as basically being on and off, the joysticks, of course, have a range of motion. In addition, Android TV games can also be controlled using devices connected via the nearby connections API. This lets players use their phone or tablet as a second screen input device. This opens up very cool options for multi screen gaming. You can imagine that if you have friends over to your house to play a game, you don't necessarily have to purchase additional game controllers for them, they can simply use the phones they already have in their pockets. Check out the video in the instructor notes to learn more. Let's talk a bit about designing your control scheme. First, always plan for a D-pad, this control set is the default for Android TV. At a minimum, you should at least be able to navigate your menus this way. Even if your game is best played with a game controller, make sure players can gracefully exit it through your menus using a D-pad. That said, it's best to support both D-pads and game controllers in your game, whenever possible. Be explicit about what kinds of controllers your game works best with. Do this both in your Google Play description and when your game starts up. A good strategy is to detect at runtime what types of controllers are connected and then prompt players if they're not an ideal type. This sets expectations properly and maximizes your chances of getting a good rating on Google Play. It's good practice to show visual controller instructions. If you do so, be sure the images you use are free from branding. Down the road, it may be possible to detect what brands of controllers are detected at runtime, and show matching graphics for them but for right now, please use this template. You can find this image, and more like it, by following the link in the instructor notes. Here are three tips for designing your control scheme. Start with default controls that are in line with what users expect in different regions. For example, if you plan to release both in Japan and the United States, make sure you set the defaults appropriately for each region. When possible, the back button should always work as expected. Usually, it should navigate backward, bread crumb style through the previous screens the player has been on. For example, if the user is playing your game and presses back, you might take them to the pause screen. If they press back again, you might take them to your game's home screen and if they press it yet again, take them all the way back to the Android TV home screen. Because TV remote controls though have a limited number of buttons, it's also appropriate to use the back button to bring up a context menu in your game. If you choose to do so though, make it clear how to move back from there. Last, don't rely on special buttons. Not all controllers have start, search or menu buttons, for example. You can learn more about best practices by following the link in the instructor notes. Now it's time to download the sample code we'll use in the rest of this lesson. Download controllersample.zip by following the link in the instructor notes. Next, unzip it and import it into Android Studio. And finally, run it on an emulator. Once the game is running, you can steer the ship using the arrow keys on your keyboard. And you can fire the lasers with the space bar. The first time you press one of these keys, your ship will appear. If you need more details on any of these steps, watch the Building Android TV Apps lesson first. Android automatically detects when input devices are attached. This means that when a player connects the remote controller game controller Android begins reporting its input events to the active view. And whether controllers happen to be connected physically or via Bluetooth doesn't matter, that's handled by the system. There are two types of input events you should be aware of. Key events are triggered by button presses. And this is important. Note that the movement of a D-Pad is a button press. Such as up down left or right. Likewise the deep pad it has a center button. And if you press it guess what you get a key event. Next up are motion events. These are triggered by joystick movements. Both of these are subclasses of input event so they have some common functionality. Every input device has a device idea associated with it. You can get this by calling the get device ID method. This is really simple, but it will become important later. In multiplayer games, you can use the device ID to map which controller belongs to which player. To find out which button press triggered a key event, retrieve the key code by calling get key code. A key code is a constant integer that indicates which button was pressed. Keep in mind it's a constant not the Unicode character that corresponds to the key. The most important key codes are listed here on this diagram and you can find a complete list by following the link in the notes. For example, when the user presses the B button, the key code will be, you guessed it, Button B. And by the way, Axis_ Hat refers to the D-Pad on a game controller. Whereas Dpad_Up and Dpad_Down refer to the Dpad on a remote control. We'll get to the codes that begin with keyword Axis later, these are for joysticks. And by the way, here's something I didn't know when I started developing games. Notice that we've two joysticks, of course. The Axis on the first joystick are called X and Y. The axes on the second joystick are called Z and RZ. To capture key events in your game, override these callback methods in the active view. For convenience, the key code is available directly from the parameter. It's also available from the event. You can also capture key events from an activity, and you can find out how to do that in the instructor notes. Next, notice these events return a boolean. Return true if you've handled the event or false if you like to give the parent method a chance to handle it. Conceptually you can think of input events as working a little bit like exceptions in Java. They propagate up the stack until the method handles them. Let's see how this works in code. I've just downloaded a controller sample and imported it into Android studio. The Asteroids game in this project contains code that demonstrates best practices for handling input events. The rest of the code though is super hacky. So consider anything that's not demonstrating how to work with input events and input device events to be suspect. Our game is divided into two packages. One which contains the view in the game logic, and another which makes it easier to work with input events across different Android versions. For now let's just focus on game view because that contains all the logic we need to handle key events. There's a lot of coding game view, but the basic design is pretty simple. Here it is to make it clear. First, when an input device is connected, we'll retrieve the deviceId. We'll will use this to create a new ship for that player. And we're going to cover exactly how to do this later in the course. The next step is going to be to override the on key down and on key up methods in our view, as we saw earlier. When a key event is received, the first thing we'll do is get the deviceId, so we know which player it's for. Next will check the key code to figure out which button was pressed. And finally we'll update that ships heading. Let's see how this looks in code, will start in a GameView class. Here's the on key down method that we overrode. And here's where we get the device ID from the input event. Next we have some game logic that looks up the ship that these device ideas associated with. It does this by maintaining a mapping from device ID to Ship. To handle the actual KeyEvent, this game just passes it directly to the ship. Before we look at that code, notice that this method returns true if the event was handled, and if not, returns whether its parent handled the event. Now let's check out the onKeyDown method of the Ship class. We use a switch statement to check for different types of key codes. And if we scroll down, we can take a quick look at the isFireKey method. And this is just showing a quick way of checking for multiple key codes at once. Not bad right? Because Android automatically detects when input devices are connected and begins reporting their input events to the active view, all you need to do is override a method and plug in a controller. None of the other coding game view is required to capture key events. If you want here's an optional exercise for you. If you're an experienced programmer this isn't necessary. But if it's your first time dealing with key events I'd recommend you knock it out. Start a brand new Android TV project. Create an activity containing a view. And write code to log which keys are pressed. Refer to the developer training by following the link in the instruction notes if you get stuck. And check out the solution if you'd like. In this version of controller sample, I've removed almost all of the code from the game view. To detect key events, all I've had to do is override the onKeyDown method. This is because Android will automatically detect when a controller is connected, and begin reporting its events to the active view. Motion events are triggered when a play moves the joystick or shoulder buttons. And in the same way, we overrode methods in our view to capture KeyEvents, we can override another to capture MotionEvents. And that method is going to be onGenericMotionEvent. Of course, you can capture this in activities in a similar way. And check out the note for more info. There's more to MotionEvents than KeyEevents, and to understand them, let's use an example. Pretend that the player sweeps the joystick up into the left into counter clockwise motion, as you can imagine, joysticks are pretty sensitive. Each dial on this diagram represents an individual sample that Android took of the joystick's position as the user moved it up into the left. Each sample contains a set of axis values, for example, the x and y positions of the joystick. Event creation in Java is expensive, so a MotionEvent isn't created for each individual sample. Instead, the system batches up samples into Windows. Each window becomes a MotionEvent, so a MotionEvent can contain many individual movement samples. Keep in mind that each MotionEvent contains all the samples since the previous one, so you never miss any. Each MotionEvent contains a list of historical samples and the current sample. You can figure out how many historical samples it contains by calling getHistorySize. You can then process the individual samples in sequence. If your frame rate can sustain it, you can redraw the screen after each one. Alternatively, you could update the player's position for each sample, but only redraw the screen when you can. Also keep in mind that MotionEvent still come in extremely fast and this won't be a source of any lag in your game. And by the way, the number of samples in each MotionEvent is variable and is determined by the system at run time. You can get the access value for the current sample by calling get access value. And you can get access values for historical samples by calling getHistoricalAxisValue with the index of the sample you're interested in. We always get the value for a single axis at a time. And the integer access parameter corresponds to the axis code that we saw earlier on. Here's how this looks in code. We're back inside Game View, inside our onGenericMotionEvent that we overrode. In addition to game controllers, many devices can trigger MotionEvents, including touch screens. In this line and the next, we figure out the event source, meaning what type of device it came from. To be honest, this code is a bit of a mess, and I wouldn't recommend doing it this way. If you're targeting API level 18 or higher, there's a helper method to do this for you. And if you're targeting an earlier version of Android, then I'd recommend writing a well test method and encapsulating this logic. There's another property of MotionEvents you should be aware of, and that's that you can get an action code. For joysticks the action code is always going to be ACTION_MOVE. You should be aware that other types of input devices can generate other types of action codes. For example, if you're working with a touch screen, when the user first taps the display, the action code might be action down. You can check out the notes for more detail. Let's check out the onGenericMotionEvent handler of the ship class to see how the MotionEvent is actually processed. Here's where we determine how many historical samples the event has and we'll use this method to handle an historical sample. And the current sample, in the same way. We begin to work with the MotionEvent itself inside processJoystickInput. The first thing we'll need to do is figure out the values for the x and y axes. Inside the getCenteredAxis method, the first thing we need to do is check whether the joystick's centered. Joysticks have a flat area. A flat area is a range of values near 0,0 with the joystick should be considered centered. We can find the flat area for an access by querying a range object we retrieve from an input device. And remember, you can always get the input device that generated an event by calling event.getdevice. Finally, we get the actual access value, depending on whether we're working with a current sample or a historical sample. To recap, lets see how its all put together. Here's the diagram from our previous lesson. I've slided it over to the left a little bit and added all the methods we talked about for motion events. If you're thinking,holy crap there's a lot here all at once!, don't worry its not so bad. Again with the MotionEvent always call getHistorySize first to determine how many historical samples you have. Then, call getAxisValue, or getHistoricalAxisValue to figure out the axis position for a sample that you care about. With joysticks, the action code is always going to be action moved, although this can be different for different types of input devices. To figure out whether the joystick should be considered centered, you have to get its flat area. And to do that you can figure out the input device that generated the MotionEvent by calling getDeviceId. Once you have the InputDevice, you can call getMotionRange to determine the motion range for an Axis and then call getFlat on that object. Also keep in mind that different Axis can have different MotionRanges. Check out the link in the instructor notes for more detail on that. Notice that the Android TV home screen displays games on a separate row than apps. To make your game appear here, as well as to have it listed as an Android TV game on the Google Play Store, there's a couple quick updates we need to make to your manifest. First, we'll add this line to indicate your app is a game. Next, declare support for game controllers. And be sure to always set the required flag to false, even if your game requires one. This might seem a bit counterintuitive. And the reason is that game pads are accessories and they might not be connected at the time of purchase. So if you accidentally set the required flag to true, users might actually have trouble installing your game from the Play Store if their game controller isn't connected. If you actually do require a game pad, instead just check if it's connected when your game starts up. As a refresher, carefully check your manifest hardware requirements. Also pay attention to implied hardware requirements, that can come about because of permissions. When you find a piece of hardware that's supported on a mobile device, but not a TV, like a sensor, mark it as optional. Then at runtime, check what device you're running on, or whether your device has a certain type of hardware, before you try and use it. If you're used to working with mobile devices then your primary input device is a touch screen that's always going to be connected unless your device is having a very bad day. By contrast, TV remote controls and game controllers are usually connected via Bluetooth. Because they're battery powered controllers often sleep to save power, shutting down their radios after a couple minutes of unuse. In this lesson, we'll learn how to handle connect and disconnect events from input devices. The InputManager fires events when devices are connected or disconnected, and it provides a method for us to register an InputDeviceListener, which we can use to subscribe to these events. Here they are, you can probably guess what two of these mean. OnInputDeviceAdded and onInputDeviceRemoved are triggered when devices are connected or disconnected. OnInputDeviceChanged is a little bit different. Typically, input devices don't change but they certainly can. For example, you can imagine a game controller might have two different modes. A good strategy for dealing with a change event is to simply disconnect and then reconnect the device. Let's see how our asteroids game handles this, and we'll start back in the GameView. The fist thing to notice is that our view implements the InputDeviceListener interface, and we'll see where this is defined in a sec. Once we've implemented that interface, all we need to do is override those three methods we discussed. Here's onInputDeviceAdded. What's cool here is that when a controller is connected our game gets the device ID and then creates a ship for it. We maintain a mapping from ship to input device and in this way we can support multiple players easily. When a controller is disconnected we simply remove the ship corresponding to that device ID. Pretty easy, right? Here we are in the constructor of our game view. The last step is to get a reference to the InputManager and then register our view as Listener for it. Rather than directly accessing the system classes, we use the InputManagerCompat provided by the controller sample. This is code that you're free to reuse. It abstracts the way differences and how input devices are handled across different versions of Android. Instead of designing your game to work within InputManager from a specific Android version, you can design your game logic against InputManagerCompat. This class contains a factory method which behind the scenes will instantiate an appropriate concrete implementation depending on which version of Android your game happens to be running on. And you can see that Factory method inside InputManagerCompat right down here at the bottom. And by the way, the Factory designed pattern is one of my favorites, and I highly recommend learning it if you haven't seen it before. You didn't think we were just going to be playing Asteroids games forever, did you? This is Pie Noon, it's a more sophisticated example you can refer to in order to see how a variety of how more advanced techniques are used. It's a simple multiplayer party game, where you can throw pies at your friends. It's written in C++ and supports up to four players using a multiscreen mode with Android TV. In this section, I'll introduce topics in technologies that help you create a great gaming experience, as well as something to take advantage of unique capabilities of Android. We won't go into great depth with each of these, but you will get an idea of what the different components have to offer. And when you're ready, you can explore further on your own by following the links in the instruction notes. First up, we have Google Play games services. You can think of Google Play services as a toolbox, which is tightly integrated with Android. It helps you with common tasks in game development. These include storing and synchronizing game data on the cloud, as well as sending invitations and auto-matching players. Also included are tools to help you build real time and turn based multiplayer gaming experiences. It also helps you build achievements and quests into your game, which are great ways to hook players. You can also collect analytics data to help you understand how players interact with your game. All you have to do is create an account on the Play Developer Console, and this works a lot like the Cast Developer Console we covered earlier. All these features are free of charge, aside from a small one-time fee to verify your account. After that, you can access all these games features via Google API client, just like when you use any other feature with Google Play services. And don't forget to add this scope games when you create your API client object, that's a common mistake that developers do all the time. Next up are nearby connections. The nearby connections API lets you discover other devices on the local network and exchange messages with them in real time. This lets you do two great things. First you can allow one player to setup a multiplayer game on the local network and then have other players join in. Or much cooler in my opinion, one player can setup the game on a shared device like an Android TV. Then other players can join in using their mobile device as a display controller combination. Individual players see a customized display on their mobile device as well as a shared display on their main screen. This is a very unique and extremely catchy capability, and it makes your game super easy to use and fun to interact with. Inside the instructor notes, you'll find a video demonstrating exactly how this works, and doc you can follow to build this into your game. In Android 5.0 Lollipop or higher, multiple networks are supported. This means that there can be more than one connecting network on an Android TV device at one time. For instance, both ethernet and Wi-Fi. You can check for this case using the connectivity manager. And if so, you can actually select the type of network you'd like to use in your game. Although there's usually not a tremendous difference, some developers prefer ethernet for the slightly lower latency. Next up we have WebP. WebP is an open sourced image format developed by Google. It's more space efficient than JPEG or PNG without a noticeable loss in quality. You can use this to compress your textures and reduce the size of your final game package. WebP supports transparency in loss less compression as well as loss compression if you need even smaller file sizes. As a default this is supported by Android TV, so you don't need to add any external libraries to encode and decode WebP images. There are also open source libraries for other platforms. And speaking of Pie Noon and C++, you should be aware of the Android NDK, or Native Development Toolkit. This tool set allows you to implement parts of your app using native code. A really important note here, the vast majority of apps will not benefit from this approach, and you should do this only if you have a very good and very specific reason. The best reason I can think of is if you have a large, pre-existing code base you'd like to reuse, like in the case of High Noon. Generally, the performance tradeoffs of using C++, if any, are not worth the complexity cost. Now on Android TV, NDK almost always works without any problems at all. However I'd strongly recommend you test your code on a real device, not on an emulator. I hope you've enjoyed our lesson. As you've seen, it's easy to extend your existing mobile game to also work on Android TV in the same APK. And if you're porting a title from the console space, you have an opportunity to develop mobile and a TV version at the same time. If you really want to knock it out of the park, you can take advantage of unique features like nearby connections for second screen input. I can't wait to see your game on the big screen. Today we'll learn how to build a great Android TV app. Android TV is just Android on a new form factor. So, if you're all ready proficient with Android development, your existing skills will still apply. First, we'll work through a starter project and explain how the key pieces work, step by step. Next, we'll apply these concepts that we've learned by enhancing the mobile version of the universal media player to also run on the TV form factor in the very same APK. Here's how the universal media player looks when we start and here's how it looks when we're done. How cool is this? It's complete with high quality animations and a responsive cinematic UI. [MUSIC] To build our app, we're going to leverage the Leanback Support Library. The Leanback Library is so easy to use and so versatile that it's becoming synonymous with writing Android TV apps. In fact, this entire UI was built using Leanback, with very little coding on our part. That said though, Leanback is just a library. You don't have to use it at all. And you have complete freedom over how your app looks and feels. Now, if you have an existing app you'd like to run on the big screen, it's important not to reuse the same activity layout that you already have for phones and tablets. Although, technically this will work on Android TV, it'll make for a poor user experience. And if you do that, Timothy Jordan will get angry and you don't want to see him angry. In reality, Timothy's a super nice guy but just to be sure, your UI should be simple, cinematic, and fun. For this lesson, all you'll need is Android studio. We'll use a TV emulator to run our code so everyone can participate. If you happen to have a real TV device though, you can use that as well. In fact, I'd strongly recommend using a real device instead of an emulator for developing production apps. Sound good? Let's get our app running on the big screen. In this lesson, we'll cover the minimum changes required to get your app running on Android tv. We'll also discuss how to prep your development environment. Users will launch your app from the Android tv home screen. Your app will appear in the apps row, and will be represented by a 320 by 180 pixel banner image. When the user chooses our banner, Android tv will launch an activity we specify in the manifest. But first, I want to show you there's an even better way to attract users to your app. That's recommendations. This is the recommendations row, and your app can suggest content to appear right here, front and center on the Android T.V. home screen. For example, let's say the user just watched the first couple of episodes in the series. Your app could recommend the next episode to appear right here. And of course, since Android TV is built by Google, users can find your content by searching for it with their voice. More on Reqs and Search later. Let's take a look at our application manifest. The most important thing to note is the leanback launcher intent filter. This identifies our app as being enabled for TV, and it's required for our app to be considered a TV app in Google Play. It's also used to specify the activity that will be launched when a user chooses her app on the Android T.V. home screen. Above that, you can see an activity that will be launched when the user runs our app on a mobile device. If you plan to use the lean back support library, as we will in this class, the new must include Theme.Leanback. This is required by most of the fragments provided by the support library. If you'd like to create your own theme, no problem, but be sure to inherit from Theme.Leanback and override specific styles. Now, if you're modifying an existing app to also run on TV, it's important not to reuse the same activity layout as for mobile devices. Building a slick user interface like the one we saw earlier can be challenging, but I've got great news for you. The leanback support library includes fragments and widgets that will make this hella easy. Leanback provides fragments and widgets for building T.V. apps, including a catalog browser, a details view, and playback controls. Also included are nice card views for displaying text and pictures. Recyclerview helps us scroll through and display large lists of media items efficiently. This is important to do on a T.V. device where media is rich and memories limited. In order to be listed as compatible with TV devices and Google Play, your app must declare that you use the feature android.software.leanback. If your APK will also run in other devices like phones, tablets or wear. Then you must declare that required is false. Also note that leanback here refers to the Android TV home screen launcher app, not the support libraries. So you use this regardless of whether you use the support libraries or not. Now, if you're using Android Studio, all of the dependencies for the Leanback libraries will automatically be added to your build.gradle file. On the other hand, if you're using Eclipse, see the instructor notes. Also, a quick note for completeness, Leanback depends on the v4 support library. Users will interact with your app using a TV remote control, or a game controller. Because of this, make sure your app supports a d-pad control scheme. Android automatically applies directional navigation between your layout elements automatically. So you usually don't need to do anything extra to have your app work with a remote control. Just in case navigation doesn't work out of the box or it doesn't work the way you prefer. You can create a custom control scheme by checking out the instructor note. Let's talk for a moment about T.V. hardware. Now obviously T.V.'s have a lot of hardware differences with a mobile device. I don't know about yours but my T.V. doesn't have an accelerometer for example. Will cover these in detail later. But for now there's one feature you always have to specify is not required in your manifest. And that's a touch screen. More detail soon. Last, let's talk for a moment about Android versions. Android T.V. support first appeared in Android 5.0. Before you begin building Android T.V. apps update your SDK to version 21 or higher. Also update your S.T.K. tools to version 24 or higher. And be sure that your project targets at least A.P.I. level twenty one. More details on these are in the notes. Okay, that's everything you need to know to get started. In the next lessons, we'll talk about hardware differences and the lean back support library. Then it's on decode. What's the name of the intent filter, you need to specify for your main activity on T.V. Refer to the developer training if you get stuck Use the Leanback Launcher category to specify the activity that will be launched when the user selects your app on the Android TV home screen. Most mobile devices these days have tons of hardware. Touchscreens, cameras, GPS, accelerometers, gyroscopes, you name it. TVs, of course, don't. So if you're developing an app that will work on both TV and mobile, then you have to handle hardware that's not supported on one of the devices. For example, let's say we have a camera app for a mobile device. Imagine it has two activities, one to take pictures and another to display a photo gallery. If we're extending this app to also work on TV, then we could disable the picture taking activity and create a version of the photo gallery for the big screen. First, though, we'd have to update our manifest to make sure we don't require any hardware that's not available on the TV. Here's how we declare the uses camera feature as optional in our manifest. This will enable our app to be installed on both mobile devices as well as TVs in the same APK. Before using the camera, we check to see if our device has a camera and runtime, and you can use the PackageManager to do so, like this. You can also check if your app is running on a TV device at runtime by using the uiModeManager like this. Use this same strategy to handle other hardware that's not supported on TVs. First, mark the hardware as not required in your manifest. And second, check whether it's available at runtime before trying to use it. One gotcha to be aware about is permissions. Permissions can imply hardware requirements. This means that requesting some permissions in your app manifest can exclude your app from being installed on TV devices. So if you use the permission, be sure to mark the hardware as not required. Be sure your app can handle events from dedicated media buttons found on remote controls like play and pause. Also, be sure your UI can handle key events from buttons like A and B on game controllers. If you're using the Leanback Support library fragments and a MediaSession.Callback, then all of this is handled for you and this is the approach we take in our sample apps later in this course. If you're designing your own UI from scratch though, you can learn more about key events in the game section of this class and see the instructor notes for more detail. It's common for TV apps to use location information, but of course, most TVs don't have a GPS because, obviously, they're stationary devices. So here's a tip, you can often get the static location that was optionally configured during the TV device setup. If this fails, you can prompt the user for their ZIP code. Last tip, TV remote controls and game controllers occasionally go to sleep to save power. Remember, they're powered by Bluetooth, and the radio consumes battery. When they go to sleep, this triggers a disconnect event. To prevent this event from interrupting your app, just add these lines to your manifest. If you're building an Android TV app, disconnects are not such a big deal but if you're building a game, see that section of the class for much more detail. Let's say you have an existing mobile app, and you'd like to extend it to also run on an Android TV. Your existing app has this permission in its manifest. Now remember, permissions can imply hardware requirements. So what two lines do you need to add to your manifest to mark the implied hardware as not required? The instructor notes have a link to developer training that you can look at to answer this question. Here's the answer. The access find location permission, implied these two hardware requirements. So, we mark them both as not required. Leanback provides a set of pre-built widgets that are specially designed to help you create an Android TV app. Particularly, but not exclusively one that's designed to browse and play media. This UI that you see here was created entirely with Leanback. All we have to do is provide the assets like images and instruct Leanback how we want them laid out. We also get all of this smooth animation for free. Let's talk briefly about the principal widgets. Everything on the screen that you see here is the browse fragment. It provides our basic layout. On the left, you can see a bunch of column headers, and on the right you can see rows. Conceptually and behind the scenes, the browse fragment is composed of a headers fragment on the left and a rows fragment on the right. This is only important to know because the headers fragment can be hidden if you'd like. And of course, each header is tied to its own individual row. Beyond layout, browse fragment does way more for us behind the scenes, and it actually contains the code for all of these animations as well as for very efficient display of large lists and media items. This is a card, and cards are a very important part of the Leanback library. A card constitutes a small view. In the media browser application, you might use cards like this to display songs and movies. If we select a card, we'll see the details fragment. It's useful for, you guess it showing more details about a card. In addition to details about a card, the details fragment contains actions. Like watching a trailer or buying or renting a movie. And if you scroll down, to see a row of related videos. If we decide to watch a trailer, we'll see the playback overlay fragment. Not surprisingly, this provides controls for playing back media. Let's try it out. [SOUND] You'll notice that after about five seconds, the UI will fade away so the content is front and center. In addition to playback controls, you can add custom buttons to the overlay fragment. For instance, to like or dislike media. The basic strategy for building an app with Leanback is to wrap your media library, so it can be displayed as a series of headers and cards on the browse fragment. We'll see how this works in code in the next lesson, but first let's see how it works by design. Leanback follows the model view presenter design pattern and if you're not familiar with them, a design pattern is just a high level way of describing how code is structured. There are three pieces to start, and the good news is that they pretty much all just sit there. By that I mean, they don't take any actions on their own. First, we have a model. A model is just a fancy word for a database, or a content provider, or however you decided to organize and store your data. In this case, let's say we have a database of movies. Each individual movie in the database might have a text field for the title and a bitmap for the box artwork. You can see a single movie here displayed on this view. And a view is just a regular Android view that's used to display some data. Here, let's say we use an image card view that's included with Leanback. On your UI, you create one of these views for each movie you'd like to display. Now, a presenter's job is to take an item from the model, say, a movie, and map it to the view. Basically, all the presenter does is say the title should go here, and the box artwork should go here. Simple, right? Now as I said at runtime, all these pieces just hang out. They're orchestrated by another class called an ObjectAdapter. An ObjectAdapter has two jobs. One, it collects item from the model, and two it creates a view for each item. In order to create that view it uses the presenter. In our application code, we create an object adapter and give it a presenter in its constructor. Next, we add items from our model to the ObjectAdapter that we'd like to be displayed. At runtime, Leanback uses our object adapter to create views for our items on its fragments. In the next lesson, we'll see how these concepts work in code. The best place to start learning is with the sample app that comes with Android Studio. We'll use this to understand the principles and then apply them to uamp in a later lesson. Fire up Android Studio and start a new project. Give your application a name, and hit Next. Choose TV as the form factor and make sure the minimum API version is at least 21. Now of course in the real application, we wouldn't just target Android TV. We'd build our mobile and TV UI into the same APK. Just for right now, I want to keep the code to an absolute minimum, so I'm just targeting TV. Make sure you add an Android TV activity to your project and hit next. These will be the names of our first couple of lean back fragments, but the defaults are fine. So hit finish. Let's take a fair amount of code here, but no worries. A good learning strategy is to learn what the big pieces are and how they fit together, and that's exactly what we'll do today. Let's start by looking at the manifest. The most important part is how we specify a main activity for Android TV. As we discussed earlier, this is accomplished by using the Leanback launcher and intent filter which you can see here. If we were also writing an app for other devices, we'd have their main activities and corresponding intent filters above or below this section. Scrolling up a bit did you notice these two users' feature tags? The first indicates that a touch screen is not required as we discussed. The second is necessary for app to be listed as an Android TV application on the Google Play Store. And by the way, the sample set this required attribute to true, but I've changed it to false, and this is how it would look if we also had a mobile version inside the same APK. I mentioned earlier that Android TV comes with a nice theme called Theme.Leanback. Here's where we apply it. Because we're using the Leanback support library fragments, it's necessary that we use this theme or one that extends from it. And by the way, you can notice that this theme is actually applied at the application level. If we were writing an app that had a mobile version as well,we'd apply it only to the TV activity. Now let's open up MainActivity. The entry point for our application. What's this? There's nothing here. Correct. That's the end of the sample app and we're all done in this lesson. To recap, we learned how to open up the Android manifest. Just kidding. The reason why there's nothing here is we're going to start by using the browse fragment from the lean back library. All fragments have to be contained within an activity in order to be displayed. Our strategy here is just to use this activity to specify a layout that contains the browse fragment. Here is the layout file, it doesn't do much except reference that fragment, so lets head there next. Finally, code. Our main fragment extends the browse fragment which is provided by the lean back library. Scrolling down a bit, we see there's way too much code to dive right in. Really to learn a concept, you need to fit all the code on about one page. Let's start by looking at onActivityCreated. I want you to trust me here and comment out these lines. In this lesson we're going to take out all the functionality and put it back in piece by piece. Now it's time to set up an Android TV emulator runner app. Using a real Android TV device, as covered in the notes, is strongly preferred. It's much faster and less buggy. That said though, in this lesson I'm going to use the emulator, so everyone can participate. To start an emulator, bring up the Virtual Device manager. Select Create a Device. In the Category tab, select TV. Here, I'm going to choose an android TV running at 720p for performance reasons. On the next tab, we have a couple of choices between API level and architecture. Here I'm going to select an emulator with api level 21 and architecture x86 for performance reasons. Recall that api level 21 is the minimum version supported by android tv. Also, for notes on why I picked version 21 and not 22 see the instructor notes. On the next screen give your TV a name and make sure use host GPU is selected. This will improve performance. For more tips on improving performance via hardware acceleration, see the instructor notes. Now click finish. Start your emulator by selecting it and pressing play. It may take some time to start your emulator, so don't close it when it finishes. Also, you'll notice that recommendations aren't available right now. The reason is we don't have any apps installed to provide those recs. Now lets run our app on the emulator. Here's our emulator. Often, when you're developing for multiple form factors you'll have a variety of emulators running. And you can check whether or not your app is compatible with one via this column. When our app starts up, you'll see an almost blank screen. This is the empty browse fragment with no content, but there's actually more going on here than it seems. On the emulator, the arrow keys function as the d pad remote controls. Press right and watch what happens. The header section of the browse fragment collapsed. Pretty cool, right? Hit left to bring it back. The escape key functions as the back button on the emulator. Press that once to return to the home screen. And here's where our app was installed. That might not seem like much so far, but trust me, things are going to get awesome very soon. Slow and steady wins the race. If you'd like to follow along in code, create a project in Android Studio, and do the steps as discussed in this lesson. Alternatively, you can wait to do your own coding live, when we get to the exercises in you-amp The browse fragment is very powerful. In this lesson, we'll bring some of that power to life. First, our UI is looking a little bit drab, so let's add some content to spice it up. Inside the on activity created method, uncomment setup UI elements. Now, let's take a look inside this method. First off, notice that all these methods are being called on the browse fragment provided by the lean back library. Most of this code is self-explanatory except for setHeadersState and setHeadersTransitionOnBackEnabled. These indicate to the browse fragment you want headers to be visible by default and that we want the back button to move the focus back to the headers from a row, if it's pressed while the user is scrolling through cards. The browse fragment can display either a badge or a title. If you prefer a badge, uncomment this line. Let's run the project now and see how it looks. It looks a little bit better but there's still not much going on. That's going to change really soon. Now it's time to add content to our layout. Return to onActivityCreated and uncomment loadRows. Here's the loadRows method. There's still more than a page of code here, so we're going to comment some out to make it easier to understand. First, I'm going to comment out all these lines except for mRowsAdapter, which is an object adapter that I'll explain later. Next, I'm going to change this i to a 0. Let's run the project to see what's up, and then I'll explain what happened line by line. Now we're getting somewhere. One thing to notice is that we have really smooth animations, and it's easy to browse our layout with the D-pad. There's even sound if you have speakers attached. On the left, we see a category name. Pretty soon we'll have many. And on the right, we see a list of three cards inside of a row. We only added a few lines of code, so where did all this functionality come from? Well, it turns out the browse fragment goes way beyond helping with layout. It also provides smooth animation, easy D-pad navigation and memory efficiency for displaying large lists of media items behind the scenes. When you use the browse fragment, you get all this for free. Here's the code that created our category name. The first parameter is unique ID for the item. The three cards we saw were created by these lines. Cards of course can get much fancier than just a line of text, but for now let's stick with this. How are these cards created? Well, that's the job of a presenter. Recall that a presenter takes an object, in this case a plain text string, and returns a view that represents it. Let's take a look inside this class and see how it works. And by the way, in case the name confuses you, we really could have called this class, Textcard Presenter. Inside, there are two important methods. And at run time, LeanBack will call both of these for us. First stop is onCreateViewHolder. This creates a view that becomes our card. A ViewHolder, by the way, is a simple class that Android uses to improve performance, by removing the need to do a linear lookup for nested views. It's not native to LeanBack. Here, the view we create is just a simple text view and we set a couple of properties to customize how it looks. Next stop is onBindViewHolder. This method receives the view that we created earlier as well as an object that should be bound to it. Bound by the way, is a fancy way of saying that we should populate the fields of the view, using properties of this item. The parameter always comes in as an object, so we cast it to the appropriate type. Here, we know that this will be a string. We also know the view we created above is a text view, so we make that cast, as well. And now we can set the text to match the string we passed in. Now we have a presenter that creates cards and three strings that should become cards. To tie these together, we use a LeanBack class called an ObjectAdapter. If you recall from our previous lesson, an ObjectAdapter's job is to connect the model with the view. When we create our adapter, we give it our presenter in the constructor. Next we add objects to it. At run time, the ObjectAdapter will use our presenter to create views for these objects. Now we want our category name and our three cards to be displayed as a single row. To do so, we'll use a list row class. This is built into LeanBack, and it's just a holder for a category name and the ObjectAdapter that will create cards for that category. And remember at this point, the object adapter contains both the presenter to create cards and the data that should become cards. Now that we have a list row, we'll need a presenter for it. Scrolling up a bit, you can see one that LeanBack provides for us, called a ListRowPresenter. Just like we used a presenter to create views for strings, this presenter will create a view that contains multiple views and line them up in a row. We need a second ObjectAdapter to hold this presenter and we create it here. We add our list row to that adapter and then we set it as the main adapter for the browse fragment. That's the Model Presenter View pattern in action. Once you understand it, you're well on your way to understanding the LeanBack library. Here's a quick quiz. An ObjectAdapter uses a blank to display objects from the blank. An ObjectAdapter uses a Presenter to create views for objects from the Model. If you need more details, have another look through this lesson, and we'll also cover these concepts again later in the course. In this lesson, we'll use the same pattern to add cards from movies from a database. Cards can be created to display any type of data you want. After all, they're just views. For example, to display a movie card, you could create a view with an image view for art work, and a text view for the title. Here, we'll actually use an image card view, which is provided by the Leanback library. First, let's take a quick look at what our movies database looks like. I'm back inside the LoadRows method now, and I'm going to uncomment the line that loads our dummy database. By the way, our dummies database is actually going to pull images over the web, so you'll need an internet connection from here on out. Let's check out the Movie class. Basically a Movie is a simple container class with a title and a description, and a couple of images that we'll get to later. The setup movies method creates five movies with this data. Now we'll create cards to display these. Back inside the low rows method, I'm going to undo the comments I did earlier. And by the way, the UI that we're going to create will contain rows of five videos, organized randomly and repeated. First, we'll create a presenter that we can use to create views for our movies. Let's take a look inside this class. Scrolling down a bit, as before there's two methods we care about, onCreateViewHolder and onBindViewHolder. This time instead of a text view, onCreateViewHolder creates a view using a ImageCardView. This is a class provided by Leanback and it's a card with a picture and some text. Whereas, previously we passed a string to our presenter, here we pass a movie object. We cast the parameter to the appropriate type, and this time, we set multiple fields on the view. In this loop, we create a rows of cards. In a real app, these would be organized in some sensible way. Here, we just randomly create rows of five cards and assign them a category name. As before, we need to create an object adapter that will hold the presenter that's used to create views for movies, as well as the movies themselves. Here's where we add movies from our database to it. And now we create the header item that will be used for the category name. Then we wrap the header and our list of five movies into a list row. And we add that list row to the object adaptor we previously created, that has our list row presenter. Run your project at this point and see what you get. Not. The next piece we'll add to our app is a Leanback details fragment. When the user selects one of our movies, we'd like to bring it front and center. And we'll use the details fragment to do just this. The details fragment has three sections. The first is an overview row, which contains text fields like a title, an author, and a description. Below that is an actions row. In our sample app, we've used the said actions to watch a trailer, and to buy and rent movies. These actions of course, are fully customizable. Finally, if you scroll down you can see a list of related movies. Pretty cool, right? The details fragment works in a similar way as the browse fragment in the previous lesson, it provides the smooth animations and most of the functionality behind the scenes, all we have to do is add our content to it. The pattern is the same as in our previous lesson. First, we create a details activity class. And it's only job will be to apply a layout that contains the details fragment. As before, this layout does little else except specify our details fragment. But before we see how that works, let's look at how we can start it up. Here I am back in the browse fragment. When the user selects a movie, we'd like to show our details fragment. And to do so, we're going to add an ItemViewClickListener to the browse fragment. In the on activity created method, uncomment set up event listeners. And let's see how that method works. Let's look inside the ItemViewClickListener. We have two different types of cards in our display, and this listener contains logic to handle each. The important part is that if a movie's clicked, we fire an intent to start up our details activity class, and we include the selected movie as an extra, so we can retrieve it later from our details fragment. The second listener on the browse fragment is used to detect when a user selects a movie. These names can be a little bit confusing, so by select, I actually mean that certain card has focus. A neat trick is that inside this listener we can update the background image of the browse fragment to reflect the selected movie. To do so, return to on activity created and uncomment prepared background manager, and setupEventListeners. Let's run the project now and see what we get. Here, you can see the background's changing to reflect the currently selected card. Pretty cool, right? I won't go into the logic of how the BackgroundManager, works but there is one note. Notice that if I scroll very quickly, the background doesn't immediately change. Instead we wait for about a second after a card has been selected before adjusting the background. This provides a much smoother user experience. Now let's see how to set up the details fragment. The pattern to set up this class is pretty much the same as the browse fragment. I'll focus just on the new concepts, and you can find more details in the notes. We'll start by populating the DetailsOverviewRow, which is a class provided by Leanback. And this is going to contain the title, the author, and the description. First, we'll pass it a reference to the selected movie. The DetailsOverviewRow will store this as an object that we can access later. Next we add actions. The actions class is also provided by Leanback. To respond when one of our action buttons is clicked, we'll setup all callback later on. First, it's important to understand the DetailsOverviewRow will be presented by DetailsOverviewRowPresenter. Aren't Java naming conventions just great? They're so concise. This presenter already knows how to layout our actions row, because that's built in to Leanback. But we need to add our callbacks to it. Here's where we do that, and take a look at how these are handled. The most important action starts at the video playing activity via the play back overlay fragment. This is also provided by pingback, and we'll see how it works later in our lesson on the Universal Media Player. Last, although our presenter knows how to present actions, we have to tell it how to display our movie. To do so, we create a DetailsDescriptionPresenter. This is a very simple class, which binds our movie data to the details fragment. One last new concept. Our details fragment uses two different presenters, one for the overview row, and another for the related videos row. And by the way, if you look through the code, the related videos row is built identically to how a row of videos was created in the browse fragment, by using list rows and a ListRowPresenter. We'll need a way to collect these two presenters into a single object adapter before passing them to the browse fragment. And to do so, we'll use a ClassPresenterSelector. This is a simple class that associates presenters with the classes they should be used to present, like this. Once we created an ObjectAdapter with our class presenter, we can add different types of objects to it as long as it knows how to present these classes. And finally, we can set this ObjectAdapter as the main adapter for the details fragment. Let's run the project now and see what we get. Watch what happens if you select an action now. Here the playback fragment appears. Pretty cool, right? Now, I know there was a lot of code here and few new concepts. But if you think about how much functionality you got in return, I hope you'll agree that it's a good trade-off. And one last tip, here we use the Leanback support library to build a catalog browser. But you could easily use it for something else other than displaying movies. For example, you might have cards that just play weather information, as in the Sunshine app. All right. Now let's see how TV support was added to the Universal Media Player. On the GitHub page, you can see a complete dif of the code changes that were made. Of course, there's a link to this in the instructor notes so there's no need to memorize anything. While you're going through this, pay special attention to the manifest. This dif is pretty good, but a few changes were actually committed before this dif was made. So it's probably about 90-95% complete, but it doesn't quite get everything. Right now what I'd like to do is walk through the essential pieces in Android studio. Here we are inside the manifest, the first step is to update the manifest to include leanback support, being careful not to require it since we're running multiple devices. And because we intend to run it on our TV, we need to set the touchscreen to not required. Now let's create an activity to run on the TV which we specify using the leanback launcher category. Here's our main activity for the TV. As before, our plan is to use the leanback's browse fragment as the main UI, so the first step is to create a layout for it. Also as before, the layout does little else but hold the browse fragment. If we ran the project to this point without including any other code we would see an empty browse fragment. Notice that right now we only have a mobile configuration. Launching that will cause the phone UI to appear on the TV. Let's see what happens. Here's the UAMP mobile UI on the TV emulator. This is in fact proof that Android TV is just Android. Now you can still run the TV UI by exiting UAMP and launching it again from the TV home screen. And here's the empty browse fragment. Or you can create a new launch configuration for the TV that specifies the right activity to run from Android Studio. And here's how you do that. Under Configurations, select Edit. Then add a new one, for an Android Application. Specify the module, and choose the TV activity. Oops, didn't give it a name. Then hit OK. Now when you run that launch configuration, the right UI will come up. Now it's time to populate the browse fragment, and to do so, we'll use the media browser you saw in the Android auto lesson. Scrolling up a bit, I want to point you to this comment. The details of the media browser aren't important, beyond that it provides a tree structure of media items you can explore. A media item can be playable, like a song, or browsable, like a playlist. UAMP is hardcoded to expect the three levels of media items that you see here. The first level is a category of categories, like genre which will go here on the header items. The next category is music style, like rock or jazz. They'll go here on the rows fragment. Finally, we have actual songs which will be displayed on a separate UI. First, let's create the header items. We do so inside the media browser callback. This gives us a list of media items which can be playable or browsable, and they contain metadata about a song or category. At this first level, we'll create a header item for each of the media items, and the pattern is exactly the same as you've seen in previous lessons. Next if the item is browsable, we'll explore the music hierarchy deeper and create a row for its contents. Here the browsable items are going to be music styles, like rock and jazz. You can see how the second level is explored inside the row subscription callback. Also, as before, we'll need the presenter to display these items. Here presenter can take either a media item or a queue item as input. And if you scroll down a bit you can see logic to set the description and to fetch the album artwork. At this point, if we run UAMP again, let's see what we get. Here you can see the header items and the row items we can drill down into further using a separate UI. Back in the browse fragment, we set up event listeners to respond when one of our row items is selected. These will open up a vertical grid fragment and this is just another leanback class that works a whole lot like the browse fragment. It just happens to layout its content a bit differently. Inside the vertical grid fragment you can see we also use the media browser subscription call backs to populate the UI. The principals are identical to the material we've already covered and here's how they look in action. When one of the songs is selected on the vertical grid fragment, we'll want to start a playback activity. Notice that we actually kick off playback here using the media controller. Although, of course, we could do so instead inside a playback overlay fragment. Now, let's have a look at the playback overlay fragment, another leanback class that provides playback controls. The media controller will handle the actual playback. All we have to do inside this class is map the UI control actions to commands for the media controller, and we do so using this event listener. Here you can see how we forward an action on the UI to the media controller. And here's how that looks in action. At this point you have everything you need to know to build a great leanback application. Let's say the user presses the Home button on the remote control while playing back media. On the left, we see something called a Now Playing card and is useful for keeping media playing in the background while the user navigates elsewhere on their TV. Notice that the Now Playing card appears front and center right here in the recommendations row. And if the user would like to jump back into their content, they can simply select it. For the Now Playing card to work, you must use a MediaSession to play your media. We've already seen how the MediaSession worked earlier in the Android Auto section of the class. Here I am inside the Playback Activity section of our sample. The Now Playing card will appear automatically as soon as you set the MediaSession to active. So this is a feature you get nearly for free. There's just one thing you have to do, and that's to keep the Metadata of the currently playing media item current. You can see how this is done inside the updateMetadata method. Once this is done, the contents of the Now Playing card will be updated automatically using this information. So remember to call this method whenever a new media item is played. And here's a quick tip. Remember to release the MediaSession object when playback stops. And that's it. You can see a couple of more details in the instructor notes. The Recommendations Row is a central feature of the Android TV home screen. You can place recommendations here to prominently feature content, and to enable your users to effortlessly start playback. Notice that when the recommendation part is selected the background image of the home screen changes to reflect it. In code, recs work a lot like notifications. Basically, you issue a recommendation by creating a notification. Cool, right? As apps issue notifications, they're collected by the Notification Manager. The Leanback Launcher then displays these notifications as recommendation cards in the recommendations row on the Android TV home screen. To create a good user experience, Android TV looks at all the available recs and ranks them using an internal algorithm. The more often a user picks one of your recommendations, the more prominently they'll be displayed in the future. Also, it's best practice not to have more than two or three recommendations active at a single time. Let's take a look at how this works in code. To make issuing recommendations easier, the Leanback sample provides the recommendation builder class. This class uses the builder design pattern to construct recommendations and if you're curious check out the instructor notes. You can reuse this in your own code. Let's take a look at how it works for completeness. First we'll issue notifications using notification compat which you saw on the wearables section. This class is included with the Android V4 support library. Lets take a look inside the build method to see how we construct our recommendation notification. Scrolling down a bit you'll see here's where we start construction our notification. You might be aware that notifications have four available styles BigPicture, BigText, Inbox, and Media. Here we always use BigPicture, so we can set a background image for the card. Details on each style are in the notes in you're curious. Scrolling down a bit more to indicate this notification is actually a recommendation. We set a category like this. We need to set both a Large and Small icon. The SmallIcon is drawable resource for a badge icon on the card, and the LargeIcon is used as the card background image. We setLocalOnly to true to indicate this notification is for the current device only. To learn more about the other fields, check out the link in the instructor notes. Also notice that when a recommendation card is selected, the background image of the Android TV home screen changes to reflect it. The image we used for the background image is a large bitmap. It's too big for a notification. Instead, we'll use a content URI to provide its location. But first, to serve this image, we'll use a sample content provider and implement only its open file method. We'll do this right inside the recommendation builder class and here's how our bare bones content provider looks. The only method with implementation is Open File. A ParcelFileDescriptor by the way, is used to send a file descriptor across different file processes. In this case, from our sample app to the Leanback Launcher app. Moving back to the build method, here's how we get a background image. Notice by the way that this method is also called insider content provider to resolve the background image from an ID. At this point, we can create a content URI that indicates the background image. We put this notification into the bundle using the Extra_Background_Image_URI key. Finally, we'll set a pending intent to take users straight to the details fragment when they select our recommendation. If you prefer, you can also jump directly into playback. To see how this works we'll need to take a look at another class called UpdateRecommendationService. We'll use this class to keep our recommendations current by updating them from time to time. And performing small job regularly sounds like a perfect job for an intent service. We'll actually start this service when an Android TV boots up and we'll see how to do that shortly. First, let's cover what it does. You can extend intent service to create your own recommendation service. When you need to refresh your recommendations, invoke the service by sending an intent. Inside this method, we find items to recommend from our Media Library. This is just dummy code. And of course you replace it with something smarter. Once we have items to recommend, here's where we'll use our RecommendationBuilder to construct the notifications. After they're constructed, we'll post the notifications by calling the notification manager's notifying method. This part is important. The first parameter is the notification ID, and you'll need to keep track of this. It's required when we'd like to update or dismiss an existing recommendation. Later on, if you'd like to remove a recommendation, you can do so by calling the cancel method on the notification manager with a notifications ID. Here's a gotcha. If you want to update a recommendation, don't cancel and resend it. This could cause the position of your recommendation to change on the recommendations row and that could annoy users. Instead update your recommendation. To do so, simply create a new notification and issue it by calling the notification manager, dot notify method, with the same notification ID as the recommendation you'd like to replace. Earlier, we said we'd add a pending intent to our notifications to take the user directly to our content. We construct that intent using this build pending intent method. This is pretty standard. The most important part is to keep track of the movie and the notification id by putting them in the bundle. There's one last thing to do here. Our Movie Details fragment is the fragment that's actually going to receive this intent. We need to insure that it responds properly when it receives the pending intent as opposed to being started up directly from the Android TV home screen, or from a search activity. To do so, we check for the pending intent inside the onCreate method. Now we have our intent service ready to go, but how can we start it up? One option is just to update your recommendations when users are using your app. Alternatively, you can use an AlarmManager to always update them in the background. That's our approach here and we'll kick off that AlarmManager when the TV boots up. Let's head to the BootupActivity to see how this is accomplished. And by the way, this class really should have been named BootupReceiver. This is a typo and, hopefully, we'll have it fixed by the time you take this class. To start it when an Android TV boots up, we'll declared this intent filter. Inside boot up activity we created an alarm manager with a pending intent to update our recommendations. And here's a performance tip. When you set alarms, it's important to use set in exact repeating instead of set repeating. The former lets Android adjust your delivery time slightly to keep the device in sleep mode for as long as possible. This saves power and is especially important on mobile devices. Although it's still counts on Android TV. If you're like to learn more about Android Performance Patterns, check out the Android Performance Patterns course linked in the instructor notes. And that's all there is to it. Remember that high quality recommendations will lead to more clicks. And those will in turn lead to your recommendations being featured more prominently. Also, please keep your recommendations family safe. This is True. Always be sure to update a recommendation using the same ID as the previous one. If instead you create a new recommendation, this could cause the order of your cards on the recommendations row to shift around and might annoy users. Search is a big part of Android TV. Let's see it in action, cat videos. Neat right. You can use search to expose content from your app so users can get to it straight from the home screen. Here's how search works. Android TV shows search results from Google, as well as locally installed searchable apps. To make our apps searchable, we implement a content provider to provide search results for a query. Android TV combines these results into a search results page. Users now have one click access to launch content you return, using a pending intent you provide. For this to work, we'll have to map our data into a cursor format that's expected by the system. If you've previously worked with search on Android, for example, on a mobile device, then you should know that Android TV uses exactly the same search interface that you might have already built for another form factor. In this section I'll describe the key steps involved, and you can check out the instructor notes for more detail. Before we go any further, we should talk about privacy. Protecting user privacy is key, and many users consider their activities on TV, including searches, to be private information. Here are two tips, never send personal information over the network. For example, if a user is searching for a pizza restaurant near them, you don't need to send their user ID to the server, along with their zip code. Now in the rare case that you have to send personal info, you should never log it. And if for some reason you do have to log it, then you should protect that data extremely carefully and erase it as soon as possible. Also be sure to provide users with an easy way to clear their search history. The first step is to define an activity to respond to the search request. Here we are inside of our manifest. The first step will be to define an activity that will be launched when the user selects one of our results from the TV home screen. We declare this activity to accept the action.SEARCH intent using this intent-filter. We also have to specify the search configuration to use using this meta data element and this XML. You can find this data inside of searchable.xml, and see the instructor notes for a description of the fields. Now, when our MovieDetailsActivity is launched, we need to resolve the search data and display the corresponding movie. You can see how we do that inside the checkGlobalSearchIntent method. Here we check to see if the intent action corresponds to global search, and if so, we pull the selected movie id out of the intent. Next we need to define a content provider which returns search results to the Android TV search dialogue. We only need to implement a part of it, and the necessary columns are already defined by the SDK. And, you can find a step by step guide for building a ContentProvider in the instructor notes or the Android Fundamentals course. First, let's define a ContentProvider in the manifest. Here we'll use the VideoContentProvider. Also make sure that exported is set to true so Android TV global search can use the results. You should see the instructor results for more info about securing your ContentProvider. Here we are inside our VideoContentProvider, and the first step is to implement the query method. Now, usually this takes five arguments. Our provider though, only needs to provide search results, and the only argument we need to handle is selectionArgs. This contains the search terms from the user. Because of the way we configured out searchable.xml, it will only contain a single element which represents the entire search string. The last thing we need to do to finish our search provider, is to pack the search results into the cursor format that's expected by Android TV. The data fields expected are represented as columns of a SQL like database. And regardless of your data's original format, you must map your data fields to these columns. You can find a description of all of these in the instructor notes. To polish your search results, you can also add a Watch Action Link. As you can see here, this search result actually came from Google. But when a use clicks this button, it will launch your app. In order for this to work, these three field return by your search data much match those from Google. When these three fields match, Android TV will automatically attach the Watch Action Link to the search result card. So it's in your favor to provide accurate metadata with your search results. There's one last thing you can do to polish your app, and here's in app search. Notice that here we've added the search button to the browse fragment. At this point we've already got our ContentProvider working, and all we have to do is add a fragment provided by the Leanback Support Library. That fragment is the search fragment, and you can see details on how to use it in the instructor notes. The pattern is exactly the same as we've seen previously in this course. I hope you enjoyed our lesson. Once you can use Leanback, you're well on your way to building for the big screen. We have tons of great resources to help you develop, and I can't wait to see your app. Google Cast is a technology that lets you connect your application to the TV. The biggest and most beautiful screen in the home. In this lesson, we'll learn how to add Cast Support to your existing Android app. We'll cover the basics of sender receiver topology, and show them in action by adding Cast Support to the Universal Media Player. Here's how UAMP looks when we start. And here it is after we've added Cast. Now you'll be able to do this. That's how easy it is to Cast your app to the big screen. Now as Timothy mentioned, the average TV viewer spends about three hours a day in front of their television. Google Cast is an opportunity for you to capture that time and engage with your users in new and exciting ways. Most people think of Chrome Cast when they hear the word Cast. But Google Cast, as a technology, is much more than that. Chrome Cast is a small USB powered dongle that plugs into the back of the HDMI port on your TV. Google Cast itself is a connecting technology. Using the SDK, you can integrate a Cast device to become part of your application's output area. And if you'd like, you can control the Cast by sharing it with multiple people. Chrome Cast is just one product that supports the Cast protocol. There are many others as well, including Android TV, which is Cast built in, as well as audio speakers from LG and Sony. Now as a developer, you don't need to be aware of these different device types. Once you cast enable your app, it will work with all of them. When you start working with Cast, you typically have an existing application you've already built and would like to extend. You integrate Google Cast into your app using the Cast SDK. And Wii of SDK is available for Android, iOS, as well web apps running on Chrome. The first thing to understand about Google Cast are the concepts of a sender and a receiver. When you integrate the Cast SDK into your existing app, it becomes what we call the sender. Sender applications send, or cast, content to cast devices either on the local network or to cast devices running in guest mode that can be detected based on proximity. What you cast from your application needs to be received by something, and that's what we call the receiver. Receiver's are small HDML five applications that run on a cast enabled device, like a Chrome Cast. Here's a quick overview of how the cast process works. First, you begin with an existing app. Then you integrate the Cast SDK to enable discovery of devices. The SDK detects Cast devices in the background, and when one's discovered the Cast icon becomes visible. Clicking that Cast icon shows a menu of the available Cast devices. And selecting one will start the Cast. The Cast icon then changes colors to show that it's connected. At this point, the sender becomes the remote control. Note that here, we've connected to the Cast device before we've started playback. But it's also possible to connect after playback's already begun. When the Cast begins, the Cast device loads the receiver app from a URL. Here we can see the receiver's been loaded onto the Cast device. Now receiver apps run independently, so you can tell it to load things like media streams from a URL. When you do that, the Cast device pulls the content directly from the Cloud, and this is great because the user can use their mobile phone for other things, like checking e-mail or sending texts, while the Cast is in progress. To control the Cast, you can send messages from the sender to the receiver to do things like adjust the volume, start playing new media, or even send custom messages, for instance, to enable a thumbs up or a thumbs down button. And we'll see how to do that later in the course. Receiver applications running on Cast devices are just web applications. They're written in HTML, CSS, and JavaScript, just like any other webpage. The Cast device itself runs a special version of Chrome, and you can think of that as having a single tab. It's optimized for video playback, and typically, you'll have a single video tag that represents the onscreen area where the media is to be played. For developers who want to get started quickly, you don't have to worry about writing a receiver application at all. Google provides a couple of sample receiver apps that you can load onto your Cast device during development. And you can even use these in production if you'd like, for playing back simple media. The one we're looking at here is called the Styled Media Receiver, and we'll get to that later in the course. Google's also built a whole bunch of sample sender apps for Android, iOS, and Chrome. You can build and run these on your devices to see how the SDK hooks into an existing application, as well as how to handle different Cast scenarios, like DRM playback. There are quite a few of these samples, so I'll point you in the right direction to the best ones later in the course. Finally, it's essential to keep in mind that we're designing for the living room. We want the user experience to be familiar and enjoyable. To help you build the best possible Cast experience, Google provides a set of user experience guidelines and a design checklist. We'll cover these later in the course, and they're optional, but highly recommended. Let's recap with a quick quiz. Sender apps can run on iOS, Android and Chrome for all platforms. Cast Receiver applications are not pre-installed on Cast devices but instead they're pulled from a URL. And although Senders could transmit video content directly to a Cast device, this is almost never done for efficiency reasons. You don't want to tie up the Sender and you want users to be able to use their phone for other things like checking e-mail while the Cast is going on. The best place to start learning Cast is our collection of sample apps. We have everything from a hello Cast example to a complete video player. Hello Cast is a great learning tool. It includes a one-file sender app, it's easy to understand. And once installed, it lets you do this. Hello world. Next step is our reference application, Cast videos. This complies with the design checklist, and it's by far your best bet to use as a starting point for your integration. Source code is available for Android, iOS, and Chrome. It includes a reference receiver app as well, and we also have samples to help you with specific tasks like DRM. Now before we get into the details there's a couple of topics we should discuss first. A receiver app is launched when a sender tells a cast device, like a Chrome Cast, that it wants to launch a particular receiver. It does this by specifying an application ID. All receiver apps have a unique application ID, and the sender must provide this during its request to the cast device. The cast device then looks up that application ID using Google service, and then downloads the URL that corresponds to the receiver app. It then loads it onto the cast device. When you start adding cast functionality into your application, the first thing you'll need to do is get an application ID, and you'll do this by visiting the Cast Developer Console. This is a self service online console where you can publish your own receiver apps so they can be downloaded by any cast device. This is actually pretty important and great for developers, because it means you can self-manage your Google Cast receiver releases. To do so, you'll first need to register as a Cast developer, and pay a small $5 registration fee to verify your account. This is just a one-time thing. Once you've registered, you'll be greeted by the developer console where you can register your receiver applications, as well as register your Cast devices. It's important to register your cast devices, so they can be used for debugging. And the main reason is so they can download a non-published receiver app. Once you've registered your cast device, it can take about 15 minutes for those changes to propagate out, and you'll need to reboot your cast device at that time. I'd recommend going ahead and registering your cast device for development now. This is important to do, because cast functionality is unfortunately not supported by the emulators. So we need to use a real Android device like a phone or a tablet. .And a real cast device like a Chromecast for development anti-bugging. These are some of the recently published receiver apps at chromecast.com/apps. When you publish your receiver, it can take about six hours for those changes to propagate to all cast devices. And, although the user experience guidelines we mentioned earlier are optional, they are required if you'd like to be featured on this web site. Developing is optional for this class, and if you'd like, you can follow along without doing any on your own. But if you do intend to write cast applications, then I'd recommend going ahead now and creating an account at Cast Developer Console and registering your device. Casting relies on smoothly coordinating two or more screens. The users should never feel like they're doing work. Successful cast apps have a familiar UI and controls that minimize the user's learning curve. The receiver UI should be smooth and cinematic and feel as uncomputerlike as possible. To help you build the best possible cast experience, Google provides a visual design checklist. Although following this checklist is optional to publish your app, it's highly recommended and required if you'd like your app to be featured at chromecast.com/apps. To make it easier to build a UI that follows these guidelines, Google provides the Cast Companion Library for Android. This contains ready built features and components you can reuse in your app, also samples are available for all platforms supported by Cast. We'll cover both of these later in the course. For now, let's take a look at the key points from the checklist. Once your app is Cast enabled, you'll want to let your users know. A great way to do this is to show a one time Cast Introduction Screen. This is useful both to alert experienced users to the new capabilities and also to teach novices. You can find a link to this particular screen, which you can reuse in your app, in the instruction notes. Next, it's important that users can start and control casts at any time. We recommend that the cast icon should always be visible in the upper right corner of your app whenever a cast receiver is available. When a receiver is not available, don't show the icon at all. And the color of the icon should change to reflect the connected state. Users should be able to easily control the Cast device at any time and from any place in your app. It's important to always show playback controls when you're casting. These can either be full screen, or you can show a persistent mini-controller when the user is navigating elsewhere in your app. It's also important to show playback controls when your device is locked, and here's how it looks on Android 5.0 Lollipop. The task companion library makes doing this very easy on any version of Android, although the appearance might differ slightly from version to version. It's also important to show playback controls via notification when your app is out of focus. If you use the task companion library, you can also startup the notification's service, so this notification will show even if your app is killed by the operating system, so the user can always control the cast. Also, make sure that users can always disconnect from the device, and they can bring up this disk menu from the cast menu. And keep in mind that if multiple users are connected, only stop the actual cast when the last user disconnects. In the event the connection to the cast device is lost, automatically reconnect users when they're in your app. Connections can drop for a couple of reasons, for example the WiFi could go out of range or the users battery could die. Also keep in mind that if your app is killed without gracefully exiting the cast session and the user restarts your app, you can try and rejoin the existing session if its still running, and the cast companion library will make this task very easy for you. Now a user can start casting in one of two ways. They can connect to the cast device and then begin playing media, or they can start playing media and then connect to the cast device. Either way, once the user starts casting, whatever they play in their act should automatically be cast to the tv. At this point, the phone is going to allow the users to browse and control the cast while the tv displays the streaming content. Remember the sender always shows the action and the receiver always shows the state. It's very important that your Receiver UI should never appear touchable, and you should definitely not re-use an existing phone or tablet UI. If you do so this is going to confuse your users and make them very unhappy. On your Receiver UI it's important to keep the users' content front and center. If they pause playback be sure to fade away your controls after about five seconds so they can see the full screen. Let's recap with a quick quiz. The design guidelines are not mandatory, but they're highly recommended. And for a good cast experience, it's super important that users can always control the cast, even if their phone is locked, and even if your app has been closed. In which case you should show a notification. Now let's talk about how Receiver Applications work. First, recall that receivers are written in HTML five and JavaScript. They're loaded onto a cast device via URL, via an application ID provided by the sender. Senders can control the receiver by sending them messages, for example, to play or pause media. You can send custom messages as well. There are three types of receiver applications. Two of which require you to register an application ID on the cast developer console. And one of which does not. Which one you choose depends on the type of application you're building, and how much customization you'd like to do. The Default Media Receiver plays a wide variety of simple media types. No styling or customization is possible though, and there's no way to modify the receiver's behavior. Next up is the styled media receiver. This is also a prebuilt receiver that's hosted by Google. Likewise, it's designed for streaming audio and video content, and works with all the same types supported by the default media receiver. This time though, you can customize it's appearance by supplying a CSS file during registration on the developer console. You can use this to add your own colors and branding, and the styled media receiver is fully compliant with the user experience guidelines. This is your best bet, if your app doesn't need to utilize advanced capabilities like DRM. This style media receiver also has a splash and loading screen that you can customize. Now, as I said, you can customize the style of the media receiver by providing a CSS file, when you register on the cast developer console. There's five elements you can customize, such as the background, the logo, and a splash screen. You can also add a watermark when your media's playing, and you can checkout the link in the instructor notes for more details, and for requirements on the different types of images you can use. Now Custom Receivers are the way to go, if you want to have full control over all aspects of the behavior of your application. There are main use cases if you want to play d or m content with dynamic licenses, or if you need authentication. Custom receivers also make sense if you're building a data centric app, like a game. To help you build these, we've open sourced a reference custom receiver that's equivalent to the default media receiver. You can find a link to that in the instruction notes, and many Google cast featured apps use this as the starting point for their integration. Now custom receivers is a HTML five app that you host on your own servers, and they must be implemented using the JavaScript receiver API. You can use this API to hook into the various events that are fired within the cast receiver message system. You can also use the cast media player library to build media applications that use a variety of techniques, like adaptive streaming, HLS, MPEG DASH, smooth streaming, and more. I've switched over to Android Studio, and I've brought up the sample receiver from the hello cast text example. We'll explore this example in a little bit more detail soon, and it's a great teaching tool. The reason is, is that it has a one file sender app and a very simple receiver. I just want to note right here, that you should not use this as a reference point for your integration. Instead, you can use the cast video sample, which we'll also get into later. The receiver app is located within the receiver folder. And if you don't see this by default, change this display here to be project instead of Android. Your receiver app should access the Google Cast receiver API using this reference. Don't self host this file because it's updated periodically. You can then get an instance of the castReceiverManager and use it to implement call backs to handle various events. There are a number of these, like onSenderConnected or onSenderDisconnected. And you can find details for all of them in the notes. You can also debug your receiver using the Chrome developer tools, by connecting your browser to your receivers IP address on port 9222. Before you can connect, your Cast device must be running your receiver app. And your Cast device must be registered for debugging on the cast developer console. The debugger's quite powerful, and it supports full dom manipulation as well as JavaScript console support. Let's recap with a quick quiz about receiver types, and an optional learning exercise. The best way to verify that you've got your cast device set up properly with the developer console, is to set up your own custom receiver app, for the hello cast example that you can find here. What I did on my own when I first started using cast, was just basically copying it and changing the background color, but making sure that I could get it up and running on my cast device with its own application ID. To support DRM, you'd have to use a custom receiver. And I hope you found this exercise all right. You can see some troubleshooting tips in the instructor notes. Now let's take a look at the life of a Cast sender application. First off you should know the Cast SDK uses an asynchronous callback design to inform the sender applications of events, like connects and disconnects to the Cast device, as well as whether commands were received. For example, let's say a Sender app sends a message to a receiver. To do so, you'll implement a couple of callbacks provided by the SDK so your app can be informed of whether it was received. We'll go into each of the steps of a sender life cycle by looking at the CastHelloText example which you can find in the instructor notes. The reason I like this sample as a teaching tool is because it contains a one file sender app inside of main activity. The only other files we need to look at are manifest and our menu XML. And again, you can find the corresponding receiver app inside the receiver folder. Now off the bat, you should know that some of the code we'll cover is going to be simplified by the CastCompanionLibrary which we'll get to in the next section of the course. But it's important to see how it works from the ground up when you get started. Also, just a reminder, don't use this project as the starting point for your integration. Use Cast videos instead. Let's start with the manifest. The minimum Android SDK version supported by Cast is 9, which is Gingerbread, or version 2.3. Next the applications theme needs to be correctly set based on the minimum Android SDK version. For example you might need to use a variant of Theme.AppCompat. Believe it or not that's it for the manifest. Of course in a production app with the CastCompanionLibrary you'll need a couple of permissions. And we'll get to those later on. Now on to the sender application inside of main activity. Our first job is to initialize the Cast API, and to do so we'll obtain an instance of the media router. You'll need to hold on to this instance for the lifetime of you sender application. And a good place to do it is inside your onCreate. At this point, the user has launched her sender application and we've initialized the API. Now it's time to discover available Cast devices. Behind the scenes the media router detects Cast devices, both of the local network and by proximity. We'll need to filter those Cast devices for just ones that can run your application, and to do so we'll use the MediaRouteSelector. This is important because if you have a video playing application, you wouldn't want to show the user an option to Cast to a stereo. Our application ID by the way is defined in our strings.xml file, but here it's displayed by Android Studio for convenience. And remember this ID corresponds to the customer receiver app already published for this application. But in our own app of course we have our own. Next up we'll need a MediaRouterCallback. This is used to inform our sender app when the user selects a device they want to cast to. And we'll see how they do that with Cast menu in just a sec. First notice that our activity extends ActionBarActivity. There are a couple of different ways to add the Cast button but the easiest is to use the MediaRouter ActionBar provider. Here's how we add the Cast button to the ActionBarCompat using the MediaRouterAction bar provider in our menu XML. And inside onCreateOptionsMenu, we assign the MediaRouteSelector to the MediaRouteActionProvider in the ActionBar menu. Now to trigger the discovery of Cast devices, we add the MediaRouterCallback to the MediaRouter instance. Note that we're doing this inside of our activities onStart method. To conserve battery power, we remove the callback when our activity has stopped. At this point, the user can select a Cast device using the menu in their app. Once they do, our sender application's informed via the MediaRouterCallback. And you can see that happening here inside the onRouteSelected method. Now it's time to launch the receiver app. To do so, we'll need an instance of the Google API client, which contains the Cast APIs. We use the connection callbacks to be notified whether or not the connection was successful. And once the connection is confirmed, our sender application can launch the receiver app by specifying the application ID we found on the Cast developer console. At this time we'll set a couple of callbacks. And if you see how far to the right we're getting indented, you'll know this is never a good sign. Fortunately this task is greatly simplified by the CastCompanionLibrary we'll get to in a sec. Now that the receiver's launched it's time to control it by sending messages. To do so, we'll need to set up a custom message channel. The sender can use this channel to send string messages to the receiver. Each custom channel is defined by unique namespace. It's possible to create multiple channels, just make sure they all have unique names. There's some rules about this namespace that you can find in the instructor notes. Once the sender application is connected to the receiver the custom channel can be set up. And as usual we create some callbacks being informed if there are messages we've received. Again this is simplified by the CCL. Now that our channel's set up, the sender application can use it to send string messages to the receiver. A good trick here, if you need to send a complex message, is to simply encode it in JSON, and then decode it on the receiver side. And the response from the receiver is then captured using the callback we set up. Now it's time to handle disconnecting from the receiver, when the user selects this option on the Cast menu. You can see how that's done inside our teardown method. That was a quick tour of the basics of the sender lifestyle. Next up we'll learn about the CastCompanionLibrary or CCL for short, which greatly simplifies working with Cast on Android. Here's an optional learning exercise you can do if you really want to learn how to write Cast applications. Now you know the basics of senders and receivers. The best next step is to try and write your own sender. What I'd recommend doing is creating a new Android Studio project with a blank activity for a phone or a tablet. Next, add simple Cast support to it so you can send a message to the custom receiver we already have from our HelloCast example. To help you with this, you can find links to documentation in the instructor notes, and if you can do this successfully, you should feel really good. By far the best way to build a sender application for Android is to use the cast companion library, or CCL for short. CCL provides a collection of ready to use components that provide features and behaviors recommended by the US guidelines. CCL primarily helps you build media centric apps, but it's also helpful for data centric use cases as well. In this lesson, we'll take a look at the major features and in the next, we'll see how it's used in practice. Now as you can see, CCL does quite a bit for you. The main features include a ready to go mini-controller and a full screen player. Also included are notifications to provide player controls when your app is closed or out of focus. Lock screen controls, so the user can maintain control even if their phone is locked. Closed caption support and WiFi reconnection support in case the connection drops. You control the CCL through the VideoCastManager. This is a singleton and maintains the state of the application, and keeps track of the connectivity to the cast device behind the scenes. It also keeps track of the playback status of the receiver for instance, if it's playing or paused. Now for data centric app you'd use its analogue which is actually called the data cast manager instead of the video cast manager. Before you can use it you have to initialize it as we're doing here. When you initialize it, you choose which features you want enabled. The first argument is your application context and the second is the application ID of your receiver. We'll get the third and fourth arguments later. At this time you also select the types of features you'd like enabled, here we're enabling quite a bit, notifications, lock screen and so forth. When you use the CCL you want to create an instance of the video cast manager early, typically inside your oncreate method of your activity. Before you do so though it's a good idea to call this method to check whether Google Play Services is installed and updated. If it's not, the CCL will automatically show the user dialog that will direct them to take the appropriate action. The third argument when we initialize the CCL is for a player control activity. If you leave it null, CCL will provide one for you. This player shows automatically during media playback, and provides controls to play, pause, and seek. You can also start this activity manually if you like, using the cast manager instance, like this. This argument by the way is an instance of media info and that's the class used by cast to keep track of metadata for a video or song. Check out a link to that in the instructor notes. Now the fourth argument is a name space of a custom message channel if you'd like one created between your sender and your receiver. Once the channel is created you can easily send messages with just a single message call. CCL also provides call back to inform you whether a message was received and whether there was an error. And, if you have a data centric app, the DataCastManager makes it easy for you to add a number of channels, each with their own name space. One of the ready to use components CCL provides is the mini-controller. This is a small controller that can be added to the layout XML of your different activities. It's purpose is to allow the user to browse to different pages of your app while still being able to control the cast. If you use this component, in addition to the XML, you'll need to register it with the Video Cast Manager inside your activities oncreate method. Also remember to unregister in ondestroy. Behind the scenes, the Cast Manager handles everything else including updating its metadata, its playback state, as well as hiding it when no media is playing and for details on all of this, check out the instructor notes. Another great ready to go component is notifications. CCL shows these automatically when your app is in the background or killed. All you have to do is indicate you want this feature when you initialize the video cast manager. Then, you need to keep CCL informed when your app is active or not, and you do this by incrementing or decrementing the UI counter inside your onStart and onStop method. Another excellent ready to go feature are lock screen controls. When you enable this feature when you initialize the video cast manager, a play pause button will automatically go on Android devices running Jellybean or higher. And as a added bonus, CCL automatically provides the ability for users to control the cast device's system volume, even if your application is in the background and you're screen is locked by using the hardware controls on your phone. CCL makes reconnecting to a running cast sessions easy, for instance if the WiFi drops. To do so, it starts up a reconnection service behind the scenes, and you have to indicate you want this functionality both when you initialize the video Cast manager, and also by adding these lines to your Android manifest. You need these permissions as well. Then, inside your on-create method, indicate that you want to reconnect to an existing session if possible. In this parameter here is a time out in seconds. In addition to helping you out of the wi-fi drops, the reconnection services is also great if your app is killed before you can gracefully exit the cache section. If the user then restarts your app, you can automatically rejoin the existing session. That's the CCL in a nutshell, we have an extensive PDF guide you can find in the structure notes that will provide details on how to use all of these features in your app. And again, if you're building or extending an Android app to work with Cast the CCL is definitely your best way to go. Another great resource is the Cast video sample. This contains a handy guide on downloading and installing the CCL and connecting it to another project inside Android Studio. All right, in this lesson, we're going to add Cast Support to the Universal Media Player. We'll start with the version of UAMP that's missing a few key pieces like the Cast button, and we'll add them back in one by one. Specifically, we've removed the CCL, the Cast button, as well as the logic to play media. But most of the functionality to handle playback state has been left intact. For this lesson, you're welcome to download and follow along in code if you like, or you can just watch to get the gist of it. I just downloaded the zip from the link in the instructor notes. This will be a great point to ensure that your Android device is connected to the same WiFi network as your cast device. Notice there are a couple of layers in this project. That's because UAMP uses the CCL to make things easier. So the first thing we'll need to do is download and build it. We're going to build the CCL as a standalone library that we'll include as an application dependency. First, we need to clone the CCL from the main GitHub repository. To do so, open up a terminal and use the command shown here. Next, we need to build the library, and we use Gradle to do so. First to make sure that your Android Home and Java Home environmental variables are set. You can see how they look on my MAC using the Echo Command and take a look at the instructor notes to see what to do if they're not set on yours. Now go ahead and build gradle using the command. You'll see a whole bunch of output and at the end you'll see BUILD SUCCESSFUL if everything ran okay. Here I've cleared the screen to get rid of some noise. And if you have a look at the build output's AAR directory you'll see the debug and a release version of the CastCompanionLibrary. The next step is to copy these over to our UAMP source trees, so we can link them to our application. The directory you want to copy them to is mobile/Libs. Now if we go back to Android Studio, we'll see them appear in our project. The next step is to open up build.gradle and add the CCL as a dependency. Here I've copied and pasted this line, and notice that we're including the debugged version for now. In the future you can include the release version. Now go ahead and re-sync the project. And if you run into a re-sync error like I have, just make sure that the CastCompanionLibrary is actually named the way you included it. Here we are inside UAMPApplication.java. The CCL uses a singleton object to manage it's internal state. Let's go ahead and initialize it. Notice also that while we initialize it we can enable some features. For instance, automatic reconnection. Now UAMP does this inside its application class, but I'd actually recommend doing so inside one of your activities. The reason is that objects you initialize within your application class are actually global. And so if you're creating things that you don't necessarily need here later in you application you're wasting memory. Lazy initialization is usually preferred. Notice also that we've enabled the debugging feature. And for this lesson, we're using an already registered styled media receiver that's hosted by Google. Here I am in our menu main.xml. One of the user experience principles is making sure the cast button is always visible to the users. They can control playback. Its good practice to add the cast button to every activity in your application. To do so we need to add a media route action provider item in the menu for each activity and we'll do that by editing the menumain.xml file. I'll just scroll over so you can see the whole thing. This step assumes of course that your activity is a sub class of of action bar activity. Now lets head over to ActionBarCastActivity and lets take a look at the on create method. Now that the CCL is all set up, any activity can call VideoCastManager.getInstance. This instance provides all the capabilities we need to manage the life cycle of CAST in our app. It's good practice to first make sure that Google Play Services is set up and enabled. If not, the VideoCastManager will prompt the user to do so. And, when our activity starts up, we check to see if we can reconnect to an existing Cast session. Next add the following line the on create options menu. You'll want to do this in all your activities and it will use the cast manager to add the cast button for us. CCL keeps track of how many UI activities are running and if at least one is active it will look for cast devices on the local network. Inside on resume of your activity, add the following snippet to tell CCL that you're active. Likewise, inside on pause, add this snippet to decrement the UI counter so CCL doesn't waste energy looking for cast devices when your activities aren't running. At this point, if you run your app, you'll see the cast button appear. The next step is to create a callback so we can update RUI when the receiver state changes. For example, when it starts or stops playing media. CCL provides a wide variety of callbacks you can use to be informed of lifecycle events. As you can see here, there's a ton of them. To make our lives easier, CCL provides the VideoCastConsumerImpl. This is a simple no-op implementation of all the callbacks. So you can extend it and just override the ones you're interested in. Here you can see we've implemented a method that's triggered when a cast device is discovered on the local network. We've also added logic to show a first time usage screen to the user. We need to register our callback when the activity becomes active, and likewise remove it when we're inactive. At this point we've set up the CCL to inform us about any events that are coming from the cast session. Now let's see how to actually trigger the playback of some media. To see how that's done, let's head over to the CastPlayback class. This is an implementation of a simple interface that contains playback logic for different devices. Inside the play method, the first thing we'll have our sender do is tell the receiver to load media. The CCL uses the media info class to hold meta data about the media to play. For example, its title and its URL. Next we use the CastManager LoadMedia method to start playback on the receiver. Since we've already added a callback, we'll be informed of the events returned from the CCL, when the receiver state changes. And here's where we are informed of that state change inside the video cast consumer. That's adding Cast support in a nutshell. As an exercise, you can continue to play with adding Cast support to UAMP or you can check out one of the many examples on the Cast Developers homepage. Great job. You made it to the end of the lesson. As you've seen, it's easy to add Cast support to your existing application. Now you can bring your app to the living room and interact with your users in new and exiting ways. I can't wait to see your app on the big screen. Happy Casting. The average TV viewer spends over three hours a day in front of their TV set. You would expect by now we would have solved the problem of providing a simple interface to find and engage with our favorite content. Watching TV should be easy, but way too often it's not. In this lesson, you'll learn all about Google Cast, Android TV, and how to use them to give your users a great experience that they deserve, with your content. Okay. Let's start by looking at what each of these platforms offer. Google Cast provides a simple sender receiver typology which allows developers to create multi device experiences integrated with their existing apps on Android, iOS, and the web. Android TV simply takes Android and brings it into the living room. This makes it a deal for extending your existing Android app to work in the living room whether or not the user has their phone on them. You can integrate your service or app with Google Cast, Android TV, or even better both. Where to start and how to go further depends on the type of service and what platforms are already integrated. In many case you can then extend to cover multiple platforms for the greatest impact and best user experience regardless of what the user has driving their TV. The most common use case I hear talked about is streaming media. Whether your app is on Android, iOS or web your first step in building for the living room is adding in Google Cast functionality. Now this will ensure your users will be able to stream on all Google Cast ready devices including chrome cast and Android TV. Once you've finished adding Google Cast support you can then extend your Android app using your lean back library. Allowing your users to launch and control the app from their Android TV. Beyond this common use case, it really depends what platform you're already working on and therefore the platform from which you're extending. If you've already got an Android app, it can most easily be extended with a lean back library to work on Android TV. In fact, many Android apps for the phone, tablet, and TV will all be bundled in the same app package, which simplifies the Google Play experience for users. In essence, the TV is just another form factor for your app. If you've got a web app, it can most easily be extended to the living room by using the Google Cast platform. The Chrome API takes care of rendering the app and then streaming that content to the Cast device. If you've already got an iOS app, it can also easily be extended to the living room by using the Google Cast SDK for iOS. For developers, if you've already built an Android app for your phone, you're just a few enhancements away from it working on your tv as well. Let's start with the Creative Vision for Android TV. Which will help you understand the difference between building on something like a phone and building for the TV. Now, sum this up quickly. You want to create an experience that's Casual, Cinematic and Simple. By Casual we mean that it should be easy going, TV is not work. Picture yourself hanging out with your friends in the living room watching a movie, playing a game, or just listening to some music. For example, you don't want to have to pick up the remote every five minutes to play the next video, the TV should just handle this for you. In the Cinematic App, the interface should just fade away, putting the content front and center. Tell your story with pictures and sound instead of text and menus. And finally, everything should be so simple that it feels magical. Minimize steps, avoid text entry, just get the user to their content. Android TV gives you the tools to easily extend your existing app or game, or create new ones just for the big screen. The most important thing to remember here is that it's android. It's the same platform you already used to build your phone apps. In fact, most of that code can be used for the TV app as well. It can even be in the same APK. Here are the differences. First, you've got to declare the activity that's intended to run on the TV. This is done on the manifest. Second, you want to update your UI to use the Leanback Support Library. It is not strictly required, and if you like making things hard on yourself, then don't use it. For the rest of us, interface widgets such as BrowseFragment, DetailsFragment, and SearchFragment make building a great TV UI fast and easy. The third and final step is to make your app and content available to the user. In the first step, you will have declared the Leanback launcher intent-filter so that your app shows up in the apps row on the home screen. To have it show up in the games row instead, you just need to set is game to true in the manifest. There's also the opportunity for your content to appear in the recommendations row or through search. The recommendations row is an essential feature of the home screen. It gives users quick access to dynamic and relevant content. It's essentially a bunch of cards that represent a system or App Action, notification, activity, or piece of media. We'll show you a little later how to get your content shown here. Search is important too, because users often have specific content in mind and browsing for that content is usually not the fastest or easiest way to find it. However, you can add a search action in your app, which you should do, but why stop there? We'll also show you how to get your content in Android TV's global search as well. That's it, three major steps to getting the great experience for your content on Android TV. Declare your activity, use the Leanback library to make everything look and feel as it should on the TV, and then integrate with recommendations and search. Google cast is all about enabling multi-screen experiences for the living room. It uses a sender/receiver topology that works like this. You already have an app on Android, iOS, or the web. That's your sender. You start by integrating the cast SDK to enable discovery of cast devices on the local network. When your app finds a nearby device, it makes the cast icon visible to your users. This nearby cast device could be a Chromecast or even an AndroidTV. It's the receiver. The user will touch the cast icon on the sender and select the cast device to send or cast stuff to the big screen. What's actually happening here is the sender's telling the cast device to load a particular receiver application. The Cast device then pulls that in from the web since it's just an HTML 5 app written in HTML, CSS, and JavaScript. It could be a simple media receiver that plays a stream directly from the cloud, or a custom receiver such as a game. You can also send messages from your app to the receiver app to control things like play, pause, volume and more. And that's it. A quick and simple overview of Google Cast. We'll cover more detail in the Google Cast STK and receiver apps a little later on. You may have run out of popcorn during this introduction to the platforms and principals of developing for the living room. If so, grab some more because we're just getting to the good part. Next up, Josh will lead us through the details of both Google Cast, and Android TV. Thanks Timothy. Its such a cool experience to see your app running on the big screen, and in these lessons I'll show you exactly how its done. Okay Google, send a message to Wayne. I'll be at our building in 20 minutes, can you get Josh and meet me there? Okay Google, start a run. [SOUND] Here's the message. I'll be at our building in 20 minutes, can you get Josh and meet me there? Reply. How about we make it five minutes? [SOUND] Got it. Here's your message. How about we make it five minutes? Do you want to send it? Cat videos. [MUSIC] I'll see you in 20 minutes, Timothy. [MUSIC] I'll see you in five minutes, Wayne. [LAUGH] Okay. One, two. Welcome to Ubiquitous Computing. I'm Timothy Jordan, a Developer Advocate at Google. And I'm Wayne Pakowski, also a Developer Advocate at Google. And I'm Josh Gordon, and you guessed it, also a Developer Advocate at Google. As you'll learn in this course, Ubiquitous Computing is simply a fancy phrase for technology that's accessible to the user, whenever and wherever they want or need it. Perhaps, they're outside going for a run. Or having some friends over to watch cat videos and play games. Or maybe, it's time for a road trip. Your users can be in any of those situations, and you don't want them to have to think about what device they need to get to your service. No, you want an experience that presents itself to the user wherever it makes sense. By building an app that runs on whatever devices they happen to have nearby. In this course, we'll teach you about the existing platforms and tools that can make your app available on a variety of form factors. But before we get started, I'll explain just a little bit more about what it means for your app to be ubiquitous. The traditional approach to supporting different devices is to make an app for each device and platform as they become available and adopted by users. This has typically meant a new project for each and every device developed and supported independently. For most of us, we actually think of the experience on any of these devices as part of a single whole experience for the user with our service. Approaching each as a separate project is well, it's just the wrong way to go about it. Instead you're going to want to think about having a view into your service from every device that makes sense. Now this is really the definition of ubiquitous computing where your service is available anywhere and everywhere. Each view into your service will differ slightly depending on the features available, and what the user needs in that context, but taken as a whole these views lead to a single seamless experience for the user across their life. We'll be teaching you how to use Google's platforms and tools to build these kinds of experiences for your users, while staying relevant and unobtrusive. Now keep in mind this is not an introductory course. We're going to assume that you're already familiar with the Java programming language, and that you already know the Android platform. If you're unsure if you're ready for this course, we recommend you start with Developing Android Apps. Also make sure to check out the instructor notes for other resources that will introduce you to Android and Java. Let's get started! One of the best parts of being a developer advocate is the opportunity to talk to developers and designers around the world in building amazing things. And we talk about their existing integrations, ways to make them more performant, how to take advantage of new services and increasingly, how to extend them to new form factors and more parts of users lives. And in these conversations, I hear a lot of great ideas. Some of these ideas can be characterized like this. Incredibly cool [LAUGH] while also not useful at all. And herein lies a central issue with building for new form factors. Often our imaginations run so far to the science fiction of our art, that we forget to make sure we're solving actual user problems and then test to make sure our solution is a good one. That's why we're going to take a few moments right now to discuss the big picture of design for new form factors as a baseline for our more detailed design discussions later in the course. Now let's start here, with the desktop computer many of us have spent countless hours working with to build and use software. It looks conventional and familiar only because we've each spent so much time with it. But something you'll notice about all these devices when you look at them without anything else in the picture. It's hard to see the human. Now one of my favorite thought experiments is by the VR pioneer, Jaron Lanier. You may have heard this before, so just bear with me for a moment. What if aliens came to Earth and people were nowhere to be found, and what if these aliens tried to figure out what we were like based on our computers. What would they think? DI mean do we have a hundred and two fingers and one big eye? Now if they look to the hammer, he'd make more sense. You can almost see the arm that would wield this tool and hammer in nails. But this computer? I mean, just look at that keyboard, it's not really built for us, in fact, it's not even built for us to type fast. Back when this keyboard layout was designed, it was intended to keep us from typing so fast, that these tight bars would get caught together and screw everything up. But this, is what evolved into the modern keyboard. Now this is a very physical difference between a person and a machine. Let's look at a less physical difference that's just as dangerous. Now what's wrong with this picture? In many ways, this is our relationship to technology today, it gives us a lot of value. However, we're adapting our lifestyle to the technology that assists us, and there's a heavy cost, distraction. We pay that cost frequently. But what if we didn't have to? These are some ways that you can use the technology we discussed in this course. And what I love about each of these examples, is that you can hardly see the technology. Some people think that the future of computing is keeping you immersed, and keeping you away from you life. But as it turns out, with ubiquitous computing, less is more. It's where we, as developers and designers, put the user before the technology. As we remove the abstract barriers, such as what functionality resides on what device. Our users can focus more on what they're doing, and less on how to start doing it. Now, this simple core philosophy to ubiquitous computing is what you see over and over again in terms like seamless, simple, microinteraction, and glanceable. And it's not new. It's the idea that success isn't measured by how long the user is engaged with the interface, but how quickly we can get them to what they need. Now there's a website that's been around for just over 15 years, that was built with that philosophy in mind. Google. This kind of user interaction is in our DNA. Back at a time when the search engine philosophy was to keep the user on the page as long as possible, Google went the other direction. And as it turns out, that's exactly what users wanted. And that's why it made sense for us to recognize that as a core philosophy across all these new form factors.