In this spreadsheet, and then copy and paste the data into a new spreadsheet and then try and find the average number of Facebook friends. Hm, okay, link to Udasion's Facebook friends, equals, I can't type anything. Okay, oh copy and paste the data into your own spreadsheet. How do I do that? Now from Google drive, looks like I have to sign in, I can go to create spreadsheet. Let's call this the same thing that says here, this way I can reference back to this in the future. Okay, so I have to Copy and Paste the data, edit, Copy's not there, I'm going to copy it by brute force. Force command C. Maybe I'll just try copying the data. Command c, command v, yay it worked. What's something else I could've done? File, maybe download as Microsoft Excel. Hey maybe this'll work. It did! I have an exact copy of the data and I can do the exact same thing in Microsoft Excel. Well, I think I'll use Google Docs for now. Do-do-do-do-do. Oh no, I accidentally clicked on it and it's gone, but here it is in my drive. From your Google account, you can see all the options here at the top. And here's my spreadsheet. Let's get to know this spreadsheet. This is cell A1, this is cell A2. Each cell has a name just like you and me, except we usually don't have numbers in our names. There's no comma or special characters in the name of the cell, and it's always the letter first and then the number. What do you think this cell is? Let's put a header on this data so we know what it is. Insert row above. This is the header, and is not part of the data, it just tells us what we're measuring. So that means, the data is from cell A2 to A28. Now I wonder what cool stuff we could do with this data? How about data, sort, range. Oh, data has header row. I almost said data has header row. Sort by number of Facebook friends, indeed. What else could we do? What's this? That looks like sigma sum. Here's a bunch of functions, let's add them. It automatically put the formula here, we could've written that ourselves though and not been so lazy. Actually, let's do some formulas. Let's calculate the average. Equals. Average A2, because that's where our data starts, all the way to A28. Cool. And then we can see here, in the formula bar, what we just typed. Let's find the deviation from the mean. There's no formula for that, you just have to intuitively know what it is. Deviation, meaning how far something differs. From the mean, how far each value differs from the mean. How do we find the difference? By subtracting. And how do we start our formulas? With an equals. A2 minus, and now we always want this value to be constant. So instead of just writing E1, we're going to put a dollar sign in between the letter and the number. That'll keep it constant when we copy and paste this for the other cells. So that makes sense, right? 0 minus 584, blah-blah-blah should be negative 584, blah-blah-blah. So now, there are a few things we can do. We can go to this corner here where there's a plus sign, then drag it down, and then we get each deviation from the mean. In fact, if we click on it, we see in the formula bar this is A10 here minus the mean, which is E1. Another thing we could do is click on this cell, push command c, or, if you're on a PC, Ctrl+C, go to cell A2, push command-down, go to cell B28, push command-shift-up. And then that area is highlighted. And then push command v. What's the sum of the deviations? Equals, sum B2:B28, it's like 0. It should always be about 0. Let's name this column, Deviations. Xi minus x-bar, and x-bar is the mean. Okay, let's do squared deviations. That's pretty self explanatory, it's just the deviation squared, equals B2 squared. Now again, command c, go to cell B2, command down. Go to cell C28, command shift-up, command v. I'm telling you, if you get used to those keyboard shortcuts, you'll be able to do things way fast, especially if you have a list of 500 data values. It might be easier for short data sets to just drag this down, but for large data sets, these keyboard shortcuts will make your life a lot easier. So now we have our squared deviations. Let's find the sum of squares. Equals, sum, C2;C28. Error, I must have typed something wrong. Let's look and see what I did. Oh, there's a comma right there, let's delete it. Now we get the sum of squares, or SS. Let's find the variance. That's simply the sum of squares divided by n, equals C30 divided by, and it looks like there are 27 values. This goes until C28, but there's a header here. So it's not using one of the cells. So there are only 27 values. Cool. So this is the variance. Should probably write this down, just so you remember. Oh, and look, it says all changes saved in drive. Every change you make is automatically saved in drive, that is so awesome. I love Google Docs. Notice that for variance, we divided by n, but if we were treating this as a sample, we would divide by n minus 1. Okay, let's find the standard deviation, equals square root of C31. So here is the standard deviation. Which remember, it's represented by that Greek symbol Sigma. What if someone had a lot of Facebook friends? Like 100,000. How would that change our mean standard deviation and variant? Let's say that instead of having 408 Facebook friends, he has 100,000. Ready for this? Whoa! Everything changed automatically. The deviations are huge, because the mean is huge, and the square deviations are huge, and everything's huge. That's so cool. And then you don't have to do anything different. What if everybody had zero Facebook friends? Then everything's just zero. Are spreadsheets totally awesome or what? Hi and welcome to Descriptive Statistics. Whether you're a financial advisor, sports coach, or the everyday person, statistics allows you to summarize data and shapes how we make decisions. In the first half of our statistics series, you will learn how to compute values from data by hand, and by using Google spreadsheets. By the end of this course, you'll have the tools necessary to gather, organize, compute and visualize information from a data-set. And for the final project, you'll use the tools you have learned to compute statistics hidden within a deck of cards. So I'll let Katie Corminick take over from here so you can get started with the class. Let's jump right into lesson one with an example. Let's say you have a big exam tomorrow and your memory needs to be as sharp as possible. What would you do to prepare? Well, let's say I asked this question to a bunch of people and 42% said they would get a good workout the night before, 28% said they would eat a good dinner. 16% said they would stay up studying. 13% said they'd get a good night's sleep. And the remaining 1% said something else. Would you believe these survey results? Before deciding whether or not you would, what would you want to know? How many people I surveyed, who I surveyed, or maybe, how the survey was conducted. Check all that apply. You would want to know all of these since they all influence the validity of our results. A good sample size, a representative sample and a sound methodology are all really important in a good research study. When you read and analyze research, you should always be aware of these factors. That way, you can make good decisions based off of valid research. Let's assume that to get a good grade, we need a good memory. That means that we have to do everything we can to boost our memory. Therefore, we might look at a bunch of different factors that influence memory. If we're doing a real research study, we want to make sure that these factors really do positively influence memory. But we can't analyze the relationship between these and memory without having a clear definition and way of measuring memory. So, how would you measure memory? There is no right or wrong answer. The purpose of this is just to get you thinking. How would you define happiness? Being at peace. The ability to , I guess live a fulfilled life. It's more what you feel. What you can bring to others and not so much what you see. They kind of bounce. Their eyes get all happy. Their ears kind of go up. They get all loose, like their body language is real loose. and yeah, their tails wag, like he's happy right now. How would you measure it? Oh, I don't know, I, I don't measure nothin' really. I just I'm happy. I think I'd probably measure it by the number of times I smile every day. I have no idea. I'd have to think about that for a sec. Their, their attitude. Their, just the way they are personally. If I was a researcher and I wanted to know how well you remembered something how could I do that? If you're talking about like say, scientific experimentation. To have a control of some sort, you could film the experience that a person has and then interview that person. maybe have multiple people to interview, so you get different points of view for the same event. And see how that differs. How would you define memory? , It's an accumulation of information, data. far more important is how you apply that, how you use it? The ability to retrieve information from your past. I have to think about it. Things I bothered to remember for Later use. How would you define itchiness? Discomfort, yeah just general discomfort. Something that's irritating you, and you have to scratch it. The intensity of the itch would match the time length of scratching. With the amount that they move around. You could ask a very forward question, like is there something you need me to scratch? Right now. Well that sounds a little personal. Very close friend, maybe you could do that. So kind of like happin ess if you had to measure how itchy your dog was right now, how would you do that? How would you quantify it? I'm going to I'd, I'd definitely go with, like, how often they were scratching, or how often they were doing sort of a full body shake. How would you measure stress? I think you could measure stress by, obviously, there's, I'm sure medical ways that you could measure stress. Yeah, the frown on my face, on my, on my mouth I mean. Heartbeats probably. The blood rushing to the head. Clearly, there isn't just one way to measure things like memory, happiness, guilt, love, and these are called constructs. It's very difficult sometimes to define and measure them, and so there are a lot of different ways that we can do that. BBC came up with one way of measuring memory by creating a test on how well you can remember faces. Click the link at the right, called BBC Face Memory Test, to do this memory test. Remember, your scores and write them down. And again, this isn't graded. We're just going to do this because this is a formal way of measuring and testing memory. Great, so you should have gotten both a recognition score and a temporal memory score. The recognition score measures how well you recognize faces that you saw before and the temporal memory score measures how well you remember when you saw each face. We're going to use your data to analyze the construct memory. Click the link at the right called BBC Scores to enter your scores from the memory test. And don't worry, we don't really care what your scores are and we still think you're awesome. If you got 68%, for example, just type 68. You'll also notice that we ask how long you slept last night. So please enter that as well. Throughout this course, we're going to be sharing a lot of data that we collect with you. We're going to be sharing a lot of data with you. Some of it will be data that we collect, maybe from you guys, and some of it might be data that we find. Either way we're going to try to make it really fun and interesting and you'll learn a lot in the process. We think that the easiest way to share this data with you is with a Google account. Count. If you haven't already, pause this video now, and create your Google account so we can share this data with you. This'll be really important throughout the entire course. So now, we're going to take a quiz. And this is a little tricky. But, of course, the whole point is to help you out and get you thinking about measurement. So, how did BBC measure memory? Note that we're not asking about factors that contributed to your score, we're asking specifically how did BBC precisely measure memory. Was it that types of faces you remember? The percent of faces correctly recognized and placed from parts 1 and 2? Knowing whether you saw the face first or second? Knowing whether or not the face was there? Or the number of faces you remember. So the correct answer is this one. The percent of faces correctly recognized and placed from parts I and II. So this is very precise, the percent of faces. The others contribute to your score so if you remember specific types of faces, that helps you to get a higher percentage. Or knowing whether you saw the face first or second? That also helps you get a better score, but that's not how it was measured. Knowing whether or not the face was there? Yes, that contributes to your score. And the tricky one, the number of faces you remember. If the score were given to you as a number For example, you remembered, I dunno 11 out of 12, that would be the way that BBC measured it but your score at the end was a percent. So this is how they measure memory. And it's important to be very precise. And if you got that wrong, don't worry. This was a tricky question and it's more just to get you thinking. Now, when we pick a way of measuring it, we also kind of have an operational definition. So in the BBC study, the operational definition of a good memory is when the percentage of faces correctly recognized and placed is higher. So once we have an operational definition, we're able to measure constructs in the real world. Now, as we said before, constructs are very difficult to define and measure. And everyone might have their own definition and way of measuring it. So now, we're going to take a quiz to see how well you understand constructs. Choose all that you think are constructs Gallons of gasoline is not a construct, because we already have a way of measuring it in gallons. Intelligence is a construct. How do we measure and define intelligence? We have the IQ test, or maybe we could measure intelligence with grades. But grades may also help us measure something like how hard we're working. Like effort. Effort is a construct. We might measure effort by how many minutes we spend working on homework or by grades, or GPA. Age is also a construct. If I had put age in years, I would have defined age as the number of years you've been living but some people might look at age as how mature you are. Here I didn't put a way of measuring it, and age therefore is kind of tricky, would be considered a construct in this case. So maturity, age in years, and wisdom are all three different ways you could possibly measure age. Hunger is also a construct. Hunger could be measured by how often your tummy grumbles or by how deficient in nutrients your body is. Annual salary in US dollars, is not a construct. That's because we put in US dollars. And therefore we have a way of measuring your annual salary. And finally itchiness is a construct. How would you define and measure itchiness? I'll let you ponder that one yourself. We have to think carefully about how to define and measure constructs. The descriptions for constructs that we settle on, and that allow us to measure them, are called our operational definition. So try and match each construct here with a possible operational definition. Now these aren't necessarily the operational definitions for each of these constructs. These are just examples of how we could measure them. Good job trying to match each construct with each operational definition. For depression, someone's score on the Beck's Depression Inventory, could be a way of measuring and defining depression. For hunger, grams of food consumed. For stress, levels of cortisol, which is the stress hormone. For anger, this could either be number of profanities uttered per minute or even ratio of minutes spent smiling to minutes not smiling. The difference is that ratio of minutes spent smiling to minutes not smiling would be a better measure of happiness. Because if it's greater than one then you're spending more time smiling. So the higher this ratio, the happier you would be. So this would be a better measure of happiness. So for anger, a better operational definition would be the number of profanities uttered per minute. Some operational definitions aren't so good, but this is just an example. Health, could be resting heart rate. Obesity, body mass index. Effort, minutes spent studying for an exam. And brand loyalty, the number of products purchased per year from a particular brand. So these are all operational definitions which allow you to measure each of these constructs. And after being able to measure these constructs, we can analyze them. Data is the most essential part of statistics. Without data we couldn't do anything. So now we have data on how many hours you slept, your recognition score and your temporal memory score, and we have data values for each one of you who entered these scores. So the first person slept 7 hours, got a 91% recognition score and an 86% temporal memory score. So this row of data is for one person. The next person got 6.5 hours of sleep, a 95% recognition score and a 78 percent temporal memory score. And remember, that you can see this actual data if you click on the BBC results link. Anyway, so we have columns of data, and each row corresponds to one person. Hours slept, recognition score, and temporal memory score are called variables, because they vary across individuals. So now we're returning to the question. You have an exam tomorrow. Should you get a good night's sleep tonight? Does it matter? What if we looked at our data and noticed that everyone who got less than six hours of sleep also got a temporal memory score less than 70%. And that everyone who got greater than six hours of sleep got a temporal memory score greater than 70%. So that would mean their scores are either here Or here. Which of the following conclusions is the most likely? The less sleep you have, the better your memory, the more sleep you have, the better your memory, people always get the same memory score no matter how long you slept, or there's no relationship between sleep and memory. The answer, if this were true, is that the more you sleep, the better your memory. People who didn't sleep for very long got lower scores. People that slept for longer, more than 6 hours, got better scores. So, if all the data followed this pattern, we could reasonably conclude that the more you sleep, the better your memory tends to be. Besides sleep, other things influence memory. This is supposed to be a brain. So, what else do you think could have influenced your memory? The time of day at which you took the test, the number of palm trees in Hawaii, whether or not you're stressed, the price of milk, whether or not you took the five-minute breaks that the test recommended, the mass of Pluto, your age and finally, not paying attention when the phases are shown. Check all that apply. So definitely, the time of day that you took the test could have influenced your memory. If you're taking the test late at night, you might be really tired, or maybe your brain just doesn't work in the morning, like one of my friends. He can't think at all in the morning. That isn't to say that it definitely did influence your memory, but it could have. The number of palm trees in Hawaii isn't very plausible to have influenced your memory. Your stress level definitely could have influenced your memory. If you're really stressed, maybe you don't remember as well. The price of milk, not so plausible. The five minute breaks? Absolutely. If you didn't take the five minute breaks, my hunch is that it might be more difficult to remember which face was from part 1 or part 2. The Mass of Pluto, not so much. Your age? Definitely. And finally, not paying attention, definitely could have influenced your memory. And the BBC study concluded that age and time of day were the two biggest influencers on how well people performed on this memory test, if you didn't get a chance to see the BBC test results. So, we're looking at a bunch of different things that could have influenced your memory, including sleep. We make hypotheses all the time, everyday. So as a reminder, we are specifically concerned about whether or not sleep influences memory. But we've identified all these other variables that could influence memory. Now it's time for a quiz. So, would you trust this data more If you knew that everyone had taken this test at the same time of day, meaning the same time of day for everyone's respective time zones. So, for example, someone in India took it at 5 o'clock their time, same as someone in California who took it at 5 o'clock their time. Yes or no? Yes, we would trust the results more because we're holding one of these factors constant. When more factors are held constant, the test is more reliable because these conditions are the same for all people that took the test. We always have to think about extraneous factors, things that can impact the outcome that we may not have thought about. These extraneous factors can impact the outcome, in this case, our memory. These extraneous factors are often called lurking variables. They just lurk around waiting to ambush your results. It's really difficult to account for every possible extraneous factor. But this is something to be aware of, not only as you're going through this course, but when you analyze data in your daily lives. You may have seen after taking the BBC Face Memory Test that the average recognition score of everyone they studied was 92%, and the average temporal memory score was 68%. Do you think that the average scores for this class will be exactly the same as these averages? Yes or no? Now, the answer is no. Within the population of everyone who took the BBC test, you guys are a sample. There are a lot of different samples we could take of all the people that took the test. We can take big samples, and small samples. And, while your sample could get the same averages as those from the entire population. There will also be samples that don't have the same averages. The average memory score of the population is called the population parameter and it's denoted by mu. It's a funny little Greek symbol. And the average for the sample is called a sample statistic and we generally denote it by x bar. An x with a bar on top. Now, I want to stress, please don't be nervous about symbols. I know a lot of people get pretty nervous when they see symbols that they haven't seen before. But in mathematics, we really need symbols to illustrate our points. Think of it like, when you're playing a musical instrument, and you have sheep Sheet music. How else are we going to make sure that people in future generations learn these classical melodies? Sheet music is the best way to do that. And same with, in mathematics when we use symbols. We need to be able to articulate are thoughts to other people all around the world. And that's one things that's really cool about mathematics it's a universal language. So don't worry, you'll soon become familiar with these symbols because we're going to use them throughout this course. Now we can use this sample statistic to approximate the population parameter. But generally, they won't be exactly the same. The difference between these two is called the sampling error. Which means we can make educates guesses about population parameters using sample statistics, but we probably won't be 100% accurate. That's why it's really important to have a good sample that can better predict the population parameter. So going back the quiz, no. We don't think that the average scores for this class will be exactly the same. But it will probably come close, and we're going to be able to see that later. So, we are going to spend a little more time with population parameters and samples statistics. Let's say that we have these memory scores. And, what if I pick the sample from this population that consisted of these scores. Now, just a point of clarification, populations are generally much bigger than this. But we're just going to pretend that this is the population, and we are choosing these three scores for a sample. Will the average of the sample be greater than, equal to, or less than the population average? What do you think? So without even needing to calculate anything, you can see that we chose the biggest scores to be in the sample. All the other scores are a lot lower. And so, therefore we automatically know that the average of the sample is going to be bigger than the population parameter. So this is just an example where sometimes we'll take a sample, and it won't accurately estimate the population parameter. So now we're going to have another quiz. What will make the sample statistic, x bar, closer to the population parameter mu? A smaller sample or a bigger sample? What do you think? So the answer is a bigger sample. If we chose this one also, and let's say these two, so we double our sample size. Then you see we have this small value 32%. And that's going to drag down the sample statistic X bar, the average. So, this will be closer to the population average mu. And of course, ideally, we'd want the entire population, but often times that's really hard to do, because the population might be so big. That's when we need a sample. But a bigger sample will better approximate the population parameter. Another example where samples can be misleading is predicting the outcome of the presidential election. For this last Presidential election with President Obama versus Mitt Romney, a lot of polls were given out which each came up with its own result. Some of them predicted that Romney was going to win, which as we know didn't happen. While each sample may not perfectly predict population parameters. The sample statistic can give us an interval in which the population parameter lies, as long as the sample is random and as unbiased as possible. We'll go more into depth about samples later. The earliest known academic rating about the meaning of random was by John Venn in 1888 and his book called the logic of chance. He said, Perhaps the best typical example that we can give of the scientific meaning of random distribution is afforded by the arrangement of the drops of rain in a shower. No one can give a guess whereabouts at any instant a drop will fall, but we know that if we put out a sheet of paper it will gradually become uniformly spotted over. And if we were to mark out any two equal areas on the paper, these would gradually tend to be struck equally often. Therefore, when we have a random sample, each subject has an equal chance of being selected. And then our sample is more likely to approximate the population. Now, let's say we have a random sample of ten people who each told us how many hours they slept the night before and their temporal memory score on the BBC test. Is there a relationship between these two variables? Well, it's very difficult to judge relationships based on just lists of data. So let's visualize it. This scatter plot visualizes the data in the table. Each point represents one row of the table. Here you can see that the hours slept was about five and the temporal memory score looked to be about 55. That must correspond to this row here, 556. You can see that hours slept is on the X axis and temporal memory score is on the Y axis. We call the variable on the X axis the independent variable or the predictor variable. And we call the variable on the Y axis the dependent variable, or the outcome. We're trying to predict temporal memory score, using hours slept. Now that you've sen this data visualized, what can we say about the relationship between hours slept and temporal memory score? This time this question might be easier to answer. When we visualize data, it's usually a lot easier to draw conclusions. And we can see a clear upward trend in the relationship between hours slept and temporal memory. Based on this data, we can say that in general, the more you sleep the better your temporal memory score. This isn't to say though that sleep causes a higher temporal memory score. This is just the trend that we can see from this data. We don't really know if a higher temporal memory score translates to a better test result. So we can't say this second option. This third option is the opposite of the relationship we see here. And we definitely do see an upward trend in this relationship. We'll get more into visualizing data later. But for now, the point is, data comes in all forms. As raw data, or visualized, or in certain numbers that summarize the data. We should make good decisions on what methods we should use to draw sound conclusions from our data. Well, I showed my friend this data, and now he thinks if he goes to bed early, his memory will definitely be better tomorrow. Is this neccessarily true? No, this is not necessarily true. Look at these 2 people. They slept different amounts of sleep, 6 hours and 8 hours but got the same temporal memory score about 70. Remember our discussion of lurking variables or extraneous factors? We haven't taken all of these into account. In other words, we haven't controlled for lurking variables. Sleep could influence memory, and by this trend, it looks like it probably does. But, many other factors can also influence memory, and this varies by person. There's a relationship here between hours slept and temporal memory score, but that doesn't mean that hours slept causes a higher temporal memory score. In other words, correlation does not prove causation. Now this is a really important concept, and we're going to try to drive this point home. Thomas Friedman said in his book, The Lexus and the Olive Tree, that no two countries with a McDonald's have ever gone to war since opening the McDonald's. This is called the Golden Arches Theory of Conflict Prevention. Let's just assume that after hearing Thomas Friedman's observation Policy makers concluded that if every country built at least one McDonald's, we would attain world peace. Well, what could be a plausible explanation for Friedman's observation, and which would lend for different policy conclusions? Let's ask some people what they think. Did you know that no two countries with a McDonald's, they've never gone to war with each other? Did you know that? No. No, I didn't know that. , Yeah, because everyone's way too lazy after they get fat. China and the US haven't gone to war because there's a McDonald's in China and then there's a McDonald's here. Really? No I didn't know that. Two countries? Yeah, did you know? No, I didn't know that. Wouldn't surprise me. Is that really true? It's true. Nobody had heard of the golden arches theory of conflict prevention before, but I put them on the spot and asked them for a possible explanation. Why do you think that is? Because that good food, tastes good. Globalization of the economy. Once you have a McDonald's in your country, your motivation just goes way down. To have a McDonald's you've reached a certain status, I guess. , And that makes you more first world. I don't know. Maybe countries that bring in McDonald's accept Americans a little more freely. I don't know. This correlation between McDonald's and peace can only lead to some interesting policy conclusions. So I asked another question. So do you think that if every country in the world had a McDonald's, there'd be world peace? Absolutely not. I don't think so because there's different, different, different ways that everybody think. I wish everybody agreed to do it, but I don't think so. I'm skeptical that it's true, actually. It's called the golden arches theory of conflict resolution. Okay. Correlation does not mean causation, so. Words of wisdom. , That would be nice wouldn't it? Guess I'd go to McDonald's to get that Big Mac, that smile on your face. You don't have to about anything. Mcdonald's originated here, right? So, I mean , I just don't think it's in their best interest to be putting up McDonalds's in places that they're at war with. I mean, that, the, it stretched too far. There's way to correlate that data. I think there probably is something more driving the conflict than not having a McDonald's. Well, what do you think? Which of the following could be a plausible explanation for Friedman's observation, and which would lend for a different policy conclusion? One option is this is completely plausible. McDonald's makes people happy and happy people don't go to war, or maybe countries with McDonald's spent too much of their money in order to open the McDonald's and now they can't afford to go to war. Or maybe citizens of countries with McDonald's are too unhealthy to go to war. Or maybe countries with McDonald's are more open to globalization and foreign investments and less inclined to go to war with other such open countries. It sounds logical that happy people don't go to war, but we don't know that McDonald's makes people happy, for one thing, and even if it does, many other factors could make people unhappy again. For this one, I'm thinking that countries that have McDonald's tend to be more economically developed. And therefore have more money, so it's implausible they wouldn't be able to afford to go to war. That's my own hunch, but I'm pretty sure this is implausible. For this one, we don't know anything about how many people eat at McDonalds and how often, and we don't even know for sure if eating McDonalds would make people too unhealthy to engage in a war. This one seems to be the most plausible option. Countries with McDonald's are more open to globalization, and less inclined to go to war. But I have to admit, that would be pretty awesome if we could obtain world peace just by building a McDonald's in every country. If that were true, I would definitely advocate for that. So now you've seen that even if we can observe a pattern between two variables, we always have to consider lurking variables. Referring back to our sleep data we see that there's a relationship between hours slept and temporal memory but we can't necessarily be confident that sleep causes better memory. What if we wanted to prove that sleep causes better memory? Well, if we want to show relationships only as with scatter plots, we can do observational studies where we just take note of already existing data or we can do surveys where people are the main subjects and we ask them questions that we're interested in knowing the answers to. But if we want to show causation that one particular variable causes another then we'd have to a controlled experiment. Throughout this course, you'll see a lot of examples of controlled experiments. And we're also going to ask you a lot of survey questions and then we can analyze the data you give us. Let's take a closer look at surveys Surveys are used a lot in social and behavioral science. In fact, I used the education longitudinal study for my masters thesis. I looked at a lot of different factors, that might influence student effort in math class. What do you think are some benefits of using a survey to do research? It's one of the easiest ways to get info on a population. They are relatively inexpensive, they can be conducted remotely, or anyone can access and analyze survey results. What do you think? Surveys are one of the easiest ways to get info on a population. Sometimes it is really difficult to conduct the survey, but surveys are often a lot easier than doing, say, a controlled experiment. They're also relatively inexpensive. They mostly just require time to implement, and for the respondents to write their answers. Surveys can also be conducted remotely. You might have gotten a survey in the mail at one point. And finally, this is my favorite thing about surveys. Anyone can access and analyze survey results if the survey owner is willing to give out that data. But assuming they are, the survey results are timeless. The National Center for Education Statistics created this Education Longitudinal Study and thanks to them, I was able to look at factors that influence effort including how much students enjoy school, or how much students value school, or their relationship with their teachers, their gender, and a bunch of other things. it was really fun and interesting to analyze this data. Looking at someone else's survey results can be a lot easier than conducting your own survey. But just like with any research, surveys are prone to all kinds of problems. What do you think are some downsides to surveys? Untruthful responses, biased responses, respondents not understanding the question, or respondents refusing to answer? There can be more than one downside. All of these are down sides to surveys. We can't always be sure that respondents are answering truthfully. Even if surveys are anonymous, people don't always give the truth for some reason. It's probably a psychological thing, and oftentimes, without realizing it, we give biased answers. Everyone has their own bias based on where they grew up, the experiences that they went through, their beliefs and values. When respondents don't understand the questions, we get what's called response bias. That's why it's very important for those who write the survey to be very precise in their wording. And finally, some people just refuse to answer, they don't feel comfortable answering that or they're incapable of doing so. There might be a certain group of people who don't answer the questions, and therefore, the data that we get from the survey doesn't accurately represent them. When certain people refuse to answer, this is called non-response bias. Remember when we talked about contructs? Surveys are often used to analyze constructs. And like I told you, I analyzed the construct effort. As you've seen, there isn't just one definition of a construct, there are many. Therefore, it's crucial to use as subjective measurements as possible. Surveys really have to be carefully thought out and worded. Besides surveys controlled experiments are another important type of research. Imagine that the researchers are testing the effects of some sleep medication on a bunch of different people and that these people are random sample. This is as oppose to the convenient sample in which people who are convenient to find are used in the study. Everyone receives a pill, but for some, the pill has medication that supposedly helps you sleep, and for others it's inactive. The pill looks the same for everybody, and the sample is randomly assigned to one of the two types of pills. What do you think the purpose was for giving some of the people in inactive pill? To make sure there are no side effects of the active pill, to have a comparison group to those who took the active pill or to see if the inactive pill can help people sleep.Which do you think the purpose was? This answer choice is correct. There are 2 groups of people, and so that way researchers can compare how well the active pill helped people sleep compared to the inactive pill. In this particular experiment, we can't really test for side effects of the active pill, and also in this experiment, researchers aren't really concerned if the inactive pill can help people sleep. To figure this out they would have to compare the effects of the inactive pill to something else. But the purpose is to find out how well the active pill will help people sleep. Furthermore, participants are not told whether or not they received the active pill or the inactive pill. Why do you think this is? Is it because deception is important in all good research because they may not participate if they knew they were receiving a drug? To make them all believe that they are receiving medication? Or they may not participate if they knew they weren't receiving a drug? The researchers want all participants to believe that they're taking the medication. This prevents participants from being biased concerning the effectiveness of the drug. If they all believe that they're taking the medication, then we're controlling for this bias. Not letting the participants know which treatment they received is called blinding. Researchers use these techniques when they suspect that knowing which treatment you're receiving might influence your behavior. The inactive pill in this case is called a placebo. Placebos are fake treatments so that the control group is unaware that they are being treated differently. When I say they're treated differently, I mean they're not receiving the medication The researcher's didn't tell the participants which pill they received, because of something called the placebo effect. People who know they're taking a placebo, might subconsciously think that the placebo's not doing anything. And this will influence the results when they report how well they slept. Getting back to the quiz answers. This first option is untrue. Researchers only use deception when they suspect that knowing the actual intent of a study may influence the participants behavior. And in that case information is kept hidden until the end of the study. These two options are also not true. Participants in experiments need to provide their informed consent before the research can expose them to any treatment. Letting participants know that they may end up in the control group is part of the informed consent To continue with our story, these participants are randomly assigned to either the control group, where they take the placebo pill, or the experimental group, where they take the active pill. They take the pill at the same time of day, on the same day, and the same place. And then, they all go to bed in a sleep lab at the same time. While sleeping, researchers observe the particpant's sleep patterns and rate their quality of sleep. Do you think these researchers should or should not know which treatment participants received? Why or why not? Yes, because their ratings will depend on which treatment the people received. No, to help maintain participant confidentiality. Yes, because the researchers ratings will be more accurate. Or no, because if they know, their judgements may be biased. What do you think? In this case, it's better if researchers do not know which treatment the participants received. This is called a double blind experiment, because neither the participants nor the researchers studying them know which treatment they received. Single blind is when only the participants are unaware of their treatment condition. And the reason this experiment is double blind is because if the researcher knows which treatment each person received, that may impact their judgements on how well the active pill works versus the Placebo. It may subconsciously cause the researchers to think that the people who took the active pill are sleeping better, and this might consciously or unconsciously alter their measurements. Let's take one more quick quiz to wrap up our experimental vignette. What are all the factors we controlled for in this experiment? You may have to replay a few of the videos if you don't remember, but that's okay. Check all that apply. We controlled for the time at which participants took the pill, the place at which they slept, because they all slept in the sleep lab, and what the pill looks like? It looks the same for both the active and the placebo. We did not however control for gender and age. We can, however, minimize the impact of these variables on the results. Let me elaborate on this a little bit. Let's say that only females were assigned to the active pill. And only males were assigned to the inactive pill. Then, if people who took the active pill slept better. We wouldn't know if it was due to the medication in the pill or maybe women just sleep better than men. In this particular experiment, we are not controlling for these demographic variables, but we can minimize their impact on our results through random assignment. This is why participants should be randomly assigned to treatment, meaning everyone has an equal chance of being assigned to either of the two treatments. In theory, randomization makes the two groups similar. On average, for example, the groups should be similar in age, the number of males and females, their sleep habits, et cetera. And that's why it's good to have a big sample size. Randomization works best with larger groups. We'll come back to the benefits of larger samples later in this class. Now one more thing before we end this story. We started this module with the question, If you're preparing for your exam tomorrow, should you stay up studying or should you go to sleep? Researchers have demonstrated using both observational and experimental studies, that sleep is important for memory. Getting a good night's sleep will likely pay off more than a couple of extra hours of studying. On the right you'll find a link to an interesting story, of you'd like to learn more about this topic. I wish I had have known that in college. You may have already thought of this, but another thing we could do is two memory tests on each person, one where they don't get much sleep, and one where they get a lot of sleep. Then we can compare the results between each person's test. If we did this, what would we be controlling for? Would we be controlling for differences in memory capabilities due to gender, or differing amounts of sleep amongst individuals? Or the variation in people's individual memory capabilities? Or time of day at which subjects took the memory test? We're not controlling for differences in memory capabilities due to gender in this particular test. To do this, we could divide the group into males and females, and then see how sleep influences memory in both groups. We want the amount of sleep to vary amongst individuals, so that we can see how their memory is related to that. We are controlling for variation in people's individual memory capabilities,, because by using this same person twice, we're doing what's called a within-subject design. That way we can see how sleep influences memory for this same person. And by using this same person twice, we're keeping a lot of other things constant, including this variation in people's individual memory capabilities. We're also not controlling for the time of day in this study, but this would be a good thing to add on to the within-subject study. We could tell our subjects to sleep a lot, then take the memory test at the same time the next day as everyone else, like 3 o'clock. Then again, we can tell people to sleep a little, and then take the memory test at the same time of day, again at 3 o'clock. Now you've seen observational studies where we correlated sleep and memory, and you've learned about surveys where we asked people questions. And you've learned about experiments which are our best bet for proving causality. For the most part in this course, we're going to focus on observational studies. That's because we can't exactly do a survey or experiment online. But we do think it's important for you to know the different kinds of studies that we use with statistics. Here at Udacity, we did our own little observational study which we're going to share with you so that you can draw your own conclusions. By now, you know my hand pretty well. You see it all the time when I'm writing out the lessons. So now, you're going to use what you know about my hand, to know a little more about me. There's a link at the right called height and hand length with real data on hand length and height of the people here at Udacity. Let's bring this data up. If we highlight the data, and then go up here to insert chart. Go to Charts, and select Scatter. And we need to click here on Scatter chart, and then click Insert. After you click Insert a scatter plot should pop up that visualizes this data. You should see a clear relationship between height and hand length. Based on this scatter plot, if I tell you my hand is 6.75 inches long How tall do you think I am? Write your answer in inches, since that's how we measured hand length. Just take a guess. This doesn't affect your grade. The purpose of this is for you to use patterns to make educated guesses. For the answer, we decided on a certain interval in which my height is feasible. If you guessed within this interval, congratulations. It seems like you understand the pattern shown in the scatter plot pretty well. What conclusions can we draw from this data, combined with knowing how long my hand is? If I want to be taller, I should let my nails grow longer. Taller people have longer hands. There's no way I'm taller than 7 feet. People who have longer hands tend to be taller. Well, this first one doesn't work. It's not really feasible, because your nails grow independent of how long your hand is. We also can't assert this second one. We can't simply say that taller people have longer hands. In fact, if we look at the data, these two people look like they have about the same height. But they have very different hand lengths, therefore we can't say with certainty that taller people automatically have longer hands. There's just a trend in that direction. Also if we look at this trend, we see that people that have about my hand length, which is 6.75. Tend to have heights around this area. Seven feet is 84 inches, which is way up here. Therefore, it's very unlikely that I'm taller than 7 feet. But I could be. Remember, this relationship shows a correlation, not a causation. therefore we can't draw this conclusion, however we can say with certainty that people who have longer hands tend to be taller One really cool thing about this course is that you're potentially taking it with thousands of other students, and these students come from all over the world. Well, maybe not from Antarctica. This is a really cool thing about education today, these massive open online courses that anyone can take for free. Udacity students are super diverse. What if we want to know who takes these classes? For now, let's just look at a sample of size 50. I'll give you four seconds to look at this table and tell me which country most students in this sample are from. Well, most of the students are from China, but it was probably really difficult for you to tell from only having the table for 4 seconds. You were probably able to glance at the table and see a lot of China written there and so, you might have had a sense of which country occurred most frequently. Your brain naturally does this, but with statistics We need to be able to formalize this procedure in an easy way. That's okay if you didn't get it right this time. The whole point was to show you that by just having the table, it's difficult to make quick conclusions. So what's a better way? You're going to create what we call a frequency table. In other words, we count the frequency of occurrences from each country. One way that I would do this is to tally it. Starting from here, we have 1 US, 1 Canada, 1 China, US, India, England, etc. And then, convert each tally to the number. So, go ahead and fill in each of these frequencies. These are the frequencies you should have got in and notice that they all add to 50, the total number of observations in our sample. Using a frequency table, we can easily see that most students are from China. It was probably tedious to do this by hand and I apologize, but sometimes when you do it by hand, you really understand how to do it. We do, however, have technology that can tabulate this. And so, we're going to make use of technology throughout this course. But every once in a while, we'll have you do something by hand. So instead of drawing conclusions off of this table with a multitude of unorganized data, we're going to use this one, which is more organized and easier to understand. Using this table, how many students are from the U.S.? How many students are from the U.S., China, or Pakistan, and which two countries have the fewest students? Well, we can easily see that ten students are from the US. To find out how many students are from the US, China, or Pakistan, we'll just add 10 plus 1 plus 12, which is 23. And finally, the two countries with the fewest students look like they're Pakistan and Sweden. See how easy it is to quickly analyze data when it's organized in a coherent way? By doing so, we can quickly describe data, see patterns, and make decisions. Now, we might not just be interested in the absolute numbers, but instead, how these numbers relate to each other. In other words, we might want to know the proportion of students from each country, understanding how many students from each country make up the whole. This is called the relative frquency. For example, 2 out of 50 students in this sample are from Canada, 2 divided by 50 is 0.04. 0.04 is the proportion of students from Canada. Try calculating proportions yourself for China, Germany, and Japan. You should have calculated these for your proportions, then you can see that relative to Canada for example, China has a lot more. You could also see this with absolute frequencies, but relative frequencies also give you a sense of how much of the whole they comprise. If we had include Kyrgyzstan in our example, Kyrgyzstan would have had a proportion of zero, since 0 of these 50 students are from Kyrgyzstan. If instead of looking at countries, we had looked at planets, we would have found that all of our students are from earth, so earth would have a proportion of one. In other words, 50 out of 50 of our students are from earth and 50 divided by 50 is 1. This might give you a hint for the next quiz. All proportions are always between or equal to what and what? All proportions will always be between or equal to 0 and 1. For the most part proportions will be between them but they could equal them, like in the case with Earth. If we were looking at the proportion of students from planet Earth the proportion would be 50 out of 50 which equals 1, and in the case of Kiergistan, 0 out of 50 equals 0. For any frequency table, the relative frequencies, when written as proportions, should add to. This should add to 1. That means we've accounted for every observation. Now, let's analyze this frequency table. What proportion of students are from the US? What proportion of students are from India? There is a greater proportion of students from which one, Europe or Asia? We see from the table that a proportion of 0.2 of the students are from the US. And we see that 0.16 of students are from India. Hopefully, you've figured this one out. But if not, it's okay. We see that all the European countries are England, Germany and Sweden. And all the Asian countries are China, India, Japan and Pakistan. So, it looks like the total proportion of students from Europe is 0.04 plus 0.06 plus 0.02, so this is 0.12. And the proportion of students from Asia is 0.24 plus 0.16 plus 0.16 plus 0.02, this is 0.58. So, it looks like there's a greater proportion of students from Asia. Another way to show relative frequencies is with percentages. Percentages are really nice because I, and many other people maybe including you, don't like to work with decimals and fractions. If we use percentages for relative frequencies, we can work with whole numbers. A percentage is basically a proportion Except we multiply it by 100 and we call it a percent. So here I converted the proportion of students from Canada to a percentage. Why don't you try a few? Change the proportion for China, Germany, and Japan from a proportion to a percentage. You should have gotten these percents, China, 24%, Germany, 6%, and Japan, 16%. Now before we move on, what values do percentages range from? Percentages range from 0% to 100%, just like proportions, how they range from 0 to 1. And again, all percentages should add to 100. So, if you added all of these, we should get that our total is 50 here, since we have 50 in our sample, 1 and 100%. Now, there's a lot of data here. And we can further simplify this data by just looking at continents. So we have North America, Asia and Europe. Use this table on country data to fill out this table for continent data. Use percentages here. To do this, all we do is add the frequencies in each country belonging to the different continents. There are 2 students from Canada, 3 from Mexico and 10 from the US. So if we add them, we get 15 students from North America. And similarly, 4% are from Canada, 6% from Mexico and 20% from the US. So if we add them, we get 30%. We have 12 students from China, 8 from India, 8 from Japan and 1 from Pakistan. So if we add them, we get 29 students from Asia. 24 plus 16 plus 16 plus 2 is 58. And finally, we have 2 from England, 3 from Germany and 1 from Sweden, making 6 from Europe. 4% plus 6% plus 2% is 12%. So now we have even more simplified data. While this is interesting and provides new information, now we can't know how many people are from individual countries just by looking at this table. Creating this table was convienent because it gave us fewer categories to look at. Three categories in this case. But we lost information in the process. In other words, information about specific countries. When creating frequency distribution tables, there can sometimes be a trade off between convenience and informativeness. But there's no one right way of presenting the data. It all depends on what kind of question you want to answer. In this case, if you want to know how many students are from North America, organizing it like this is best. But if you want to know how many students are from India, or other specific countries, organizing it like this is best. The key is to know how you're going to organize your data, based on the questions that you want to answer. So now you've seen how diverse Udacity students are. They come from all over the world, I think, over 146 countries actually. What else do we want to know about Udacity students? Ron, Shawn, and I think that people should be life-long learners, and a lot of our students value learning beyond high school and college. And then other students want to get head start on things that they wont learn for another few years. As a result, we get students as young as 13 and as old as 85. Just like how we analyzed countries students are from, how about we analyze their ages? So here's a sample of student ages, and again we have 50. How can we analize this data the same way we analyzed the country data? So instead of having country in the frequency data, we have age. How can we create this table? How many rows would you have? Would you need 50, one for each student in the sample, or maybe 66 rows, one for each age from 10 years to 75 years? 8 to 10 rows because that's just easiest to understand? It depends on how you group the data, or 2 rows, one for over 50 years old and one for under 50 years old. We could really group this data any way we want. In fact, we could only have two rows, one for the number of students younger than 50 years old, and one for the number greater, but we don't have to. We could even have one row for each age. So here, we could have 10, 11, 12, and then we would count to see how many students are aged 10, how many students are aged 11. But this is not the most convenient way. That's because, the frequency would most likely be 1 for all of them, maybe 2 for a few. So, instead of doing that, how about we choose an interval for each row? For example, 0 to 19 and then 20 to 39, then we can count how many students are between ages 0 and 19, between 20 and 39, etc. This is called the interval or bin or bucket. For the most part, we'll either call it an interval or the bin and in this case the bin size is 20. That's because it includes 0. So let's create a frequency chart with bin size 20. What's the frequency of students who are between 20 and 39 years old? One way to do it, would be to look up here at the original table and count the number of students who are either 20, 21, 22, all the way to 39. But that would be really time consuming. Instead, we know that all of these should add to 50 because that's the total number of students in our sample. 19 plus 5 plus 5, is 29, and 50 minus 29 is 21. So in other words, we did 50 minus 19 plus 5 plus 5 in parentheses. Given a messy set of data, we can easily visualize it using a frequency table. In this case, we've made four bins of size 20, and we can count how many students fall within each of these intervals. But now, we've going to go into even further ways of visualizing this data. You're going to create a graph. Here, I'm drawing the x-axis, which is age, and here I'm drawing the y-axis, which is frequency. You're going to make a bar graph for this data by clicking where the heights of each bar should be. So, we'll have four bars, now click where the height of the first bar should be, the second, the third and the forth. There are 19 people aged zero to 19, which is about here. And there are 21 people aged 20 to 39, which is about here. And then, five people between 40 and 59, and five people between 60 and 79. That means our bar graph will look like this. So, we have the axis. Frequency is always on the y-axis. And then, the variable is on the x-axis. In this case, it's age. If you recall, sometimes the independent variable is also called the predictor variable. And the dependent variable, which is on the y-axis, is also called the outcome. But not in this case. This is a special type of graph called a histogram, and the variable on the y-axis is just a frequency. Here, at the intersection of the axes is the origin. It's Cartesian coordinates are 0,0 because from here, we've gone zero to the right and zero up. Each axis goes from low to high. So, we start at zero and work our way up to higher values. Same with here on the y-axis. So now we have a histogram for our sample of 50 Udacity students. You just created a histragram with bin size 20, but we can create a histagram with any bin size, or also called the interval size. This is interactivate histagram software, which I really like. You can just input the data here, and then you can see how the shape of the histagram changes when you change the bin size. So here, we're getting bigger and bigger bins. And you see that there are more observations now within each interval. If we keep making the bin size bigger and bigger, eventually it gets kind of hard to see the shape of the histogram because the bin sizes are too big. So here, the bin size is about 23. And if we keep going, of course, eventually, we get way too much data in each bin, and the shape of the histogram isn't even recognizable. So, let's get a smaller bin size. Here, you can kind of see better the shape of the histogram, that it peaks at around age 20. And so, it's really up to you to decide what bin size you want, and it all depends on what questions you want to answer. How much detail you want. because at some point, the bin size is probably too small, but at some point the bin size is probably too big. In this software, you can also see that the sample size is 50. You can see the average. And we're going to get to these statistics later. The link to this software is on the right, so you can click on it and input the data. We shared this data with you in a Google spreadsheet. And then, you can see for yourself how the shape of the histogram changes when you change the bin size. Now you've seen many different ways that histograms can be visualized using different bin sizes. Which of these two histograms has a smaller bin size? Remember that the bin size is the interval in which you're counting the frequency. So, in this histogram, the interval is 10, and in this histogram the interval is 20. Therefore, this histogram has a smaller bin size. Here's the same data on the Udacity students ages, but with a smaller bin size than the one you just created by hand. What is the bin size here? This one was probably pretty tricky to figure out, but you should have gotten that the bin size is 5. Now approximately, what is the most frequent age? Well, you can see from the histogram that the most frequent age is between 20 minus 2.5 and 20 plus 2.5. In other words, from 17.5 to 22.5. The tallest bar includes ages 18, 19, 20, 21 and 22, those are within this interval. And you can see that the size is 5. So, if you said anything between 18 and 22 here, you're correct. Now, what proportion of students are over 60 years old? Well, you see that no one is around the age of 60. People are either below age 60 or above age 60, and there are two bars showing the people that are above 60. We see four people are between 62.5 and 67.5, and then one person is between 72.5 and 77.5. So in total, there are five people who are over age 60. 5 out of 50 is .1. Okay, So what percentage of students are under 60 years of age? Remember, this is now percentage, not proportion. Well, we've found before that 5 out of 50 students were greater than 60 years old. And this is the same as 10%. That must mean that 90% of students are younger than 60 years of age. Remember how all percentages will add to 100? Okay. So, what if we wanted to know how many students are younger than 20? Can we answer this from the graph? Yes or no? Unfortunately, no. When we create this histogram and we choose our bin size we sometimes sacrifice detail for convenience. It's very convenient to look at this histogram and see the shape, because we can't know all the little details just from this. So now, you've seen how we can use visualizations to easily understand the population of Udacity students, in this case, their ages. We can do the same with the data on where students are from. So, let's create that real quick using the frequency table that you created for continents. Again, click which radio button corresponds to the height of each bar. Well, 6% are from Europe, which is about here. 15 percent are from North America, which is about here. And 29 percent are from Asia, which is about here. Therefore, the columns should look like this. What's the difference between these 2 graphs? Just think about it and write your answer here. The answer that you give isn't going to affect your grade but take a look at these and really try and think about it. The differences between these 2 graphs is really important and we want you to come up with some ideas on your own. While this one is called a histogram, this one is called a bar graph. The spaces in between the columns here mean that each of these is one distinct category. Europe is completely distinct from North America, completely distinct from Asia and we can't mix them. But with age, we can choose any interval or bin size. And so, that's the next really important difference. You can change the bin size for age data but not for the country data. We could make our bins from 0 to 80, in which case, all the students would be in that one bin. But we can't do the same for this unless we make it world, or something different, we could divide it into hemispheres but we're still more restricted with this type of data then with the age data. And finally, the order of the continents doesn't really matter. However, age is just in one order, from least to greatest. With the bar graph, we might choose to put it into a particular order depending on what question we're trying to answer. Such as where are most students from? Then, we might put it in a order from least to greatest or greatest to least. Or, we might choose to alphabetize it so that viewers can easily find what continent or country or whatever they're looking for on the x-axis. And finally, the shape of a histogram is very important. Whereas, the shape of a bar graph is arbitrary, depending on how you choose to order the categories on the x-axis. One thing to remember is that with a histogram, the variable on the x-axis is numerical and quantitative. Whereas with a bar graph, the variable on the x-axis is often cateogoriacal or qualitative. There's one more important point we need to make about bar graphs. This is the way we've been viewing it so far, but here's another way. Does this accurately represent the frequencies? What about now? Now we've labeled the axes, and you can see that all we've done is look at this part of the bar graph. However, this way of depicting the graph can be very misleading. And there are graphs everywhere that are misleading. So, it's really important to look over at the y-axis and make note of the values. We're going to go deeper into understanding the shapes of distributions. Interactivate has its own data sets that you can play around with, so let's check out Body Fat percentage of 252 Men. Here, you can see that it looks like it has on peak, pretty much right in the middle. And if we change the shape of the distribution, you can see it always peaks about here. At maybe 21% body fat. Again, sometimes when the bin size is too small, you can see it's really, really small Then, there's a little too much detail. And when the interval size is too big, there's not enough detail. This is probably a pretty good bin size around this area. Let's stop at about here with the bin size 2.662. Now, let's take a quiz. What can be said about men's body fat percentage using this histogram? Could you say that most scores fall at around 20%? The shape is roughly symmetrical, meaning if you fold it, that it'll kind of overlay on each other, or that the most common body fat percentage is 35%. Most scores fall in the middle of the distribution. That there are more scores between 15 and 25 than between 35 and 50. Can we say there are more scores between 0 and 10 than between 18 and 24? Or, finally, can we say that relatively fewer men have a body fat percentage above 35 or below 5? This data, is what's called normally distributed. That's very important. A normal distribution has one peak right here, called the mode. And it looks like most scores fall at around 20%. And in fact, when you looked at the interactivate software in the last video, when I showed you the different bin sizes for this data, it said that the mean was 19.151, which is about 20%. The shape is roughly symmetrical. You can see that it follows a distribution that looks kind of like this. And, if you were to fold it across this line, it would pretty much overlap right on top of each other. Now 35 percent is out about here, so we cannot say that the most common body fat percent is 35%. We can, however, say that most scores fall in the middle of the distribution. Here's the middle, and you can see that 64 approximately men fall right here in the middle with these 2 bins. Are there more scores between 15 and 25 than 35 and 50. 15 and 25 covers this middle area here, but 35 and 50 goes from here out to about here. So yes, we can say there are more scores between 15 and 25 than 35 to 50. How about between 0 and 10? Doesn't look like there are many scores, whereas between 18 and 24, that covers pretty much this whole section, so no we cannot say this. And finally, do relatively fewer men have a body fat percentage above 35, or below 5%? Yes, only a few men have that body fat percentage. As you saw before, the distribution of ages of students taking Udacity courses is positively skewed, with small students being younger than older. Here's another example of a positively skewed distribution. Where you can see that most values are on the left side than on the right. This is a distribution of income in the United States in 2005. You can see here is says 98% of households have a household income of less than 250,000. And it looks like the highest frequency of household income is even less than 25,000 a year. What can we say about this distribution? Can we say that it's symmetrical? The most common household income is less than 25,000 per year. Most households make less than 100,000. Or finally, no households have an income greater than 250,000. This distribution is not symmetrical, it's skewed, With most incomes falling toward the left of the distribution. The most common household income is here, which is less than $25,000 per year. We can also say that most households make less than 100,000. If you see here, it says 95% of households make less than 166,000, and 80% of households make less than 91,705. So, somewhere between 80% and 95% of households make less than $100,000, which is most of them. We cannot say, though, that no households have an income greater than $250,000. 98% have less than $250,000, which must mean that 2% of households have an income greater than $250,000. Let's say you're a college freshman and you're choosing a major. You're debating between Nursing and Geography. So, you decide that you're going to look at how much Nursing majors make as opposed to Geography majors. Fortunately, these days, data is often easily accessible. You find that the frequency distributions of Nursing majors' salaries and Geography majors' salaries looks like this. So, let's say that these histograms were created from the data from everyone who either majored in Nursing or Geography, and the x-axis represents their annual income in thousands. From these distributions, approximately what income do most Nursing majors make, and approximately what income do most Geography majors make? Judging by these distributions, it looks like most Nursing majors make somewhere in between 50,000 and 60,000 a year. So, if you said anything between these two numbers, then you got it right. But for most Geography majors, it looks like anything between 40,000 and maybe 55,000. So here, we're focusing on the center of the distribution, where most of the scores lie. And this center is a little fatter. You can use distributions like this to guess how much you might make if you were a Nursing major or Geography major. In the last quiz we determined an interval estimate that you're likely to make with either major. So, in the case of nursing majors, between 50,000 and 60,000 and in the case of geography majors, between 40,000 and 55,000. But ideally, we want one number that describes the entire data set, this allows us to quickly summarize all of our data. Here's an analogy. Let's say you're describing someone to a friend, there are a few words that you could use to describe that someone like, tall, dark and handsome. Sometimes, 3 words will be accurate descriptions, but let's say you can only choose one, cus you want to quickly describe this person. Which word would you choose? The same with data, and just like there are multiple words that could describe a single person, there may be multiple numbers used to describe the whole data set. How would you choose one number or at least a very small range of numbers that accurately represents the typical salary of Nursing or Geography majors? Would you choose the value at which the frequency is the highest? The value at which frequency is lowest? The value right in the middle of the distribution? The biggest value on the x-axis or the average? Choose all that you think would work. There's more than one correct answer. The value at which the frequency is highest is called the mode. And this certainly works in describing the distribution. In the case of a histogram, the mode would be this bin right here, and this bin right here. The most common value is the mode. And, in this case, we can't see individual values, but we can see that most of the values rest within this bin. So, the mode occurs here, and here. The value in the middle of the distribution is called the median. And this would also work. We'll go more into depth later. And finally, the average is a statistic that rests at a specific spot in the middle of the distribution. We're going to elaborate on this in this lesson. The value where the frequency is the lowest, in this case, it's right here. And it looks to be about 72 or 75. And this most definitely would not represent the whole distribution where there are a lot of scores here, more towards 50. And the biggest value on the x-axis, again, that's here at about 75. Here, probably the biggest value is somewhere around 90. And that doesn't represent the whole distribution so let's ignore these two. But we know that the mode, median, and average can all help describe the distributions. They each have strengths and weaknesses, and we'll explore all of these throughout this lesson. So we're going to take a quick quiz on the mode, and it'll take 2 seconds. Remember, that the mode occurs with the highest frequency. So let's say we have this simple data set. What is the mode? In other words, what's the number that occurs the most? Write your answer in the box. Well, you see that each of these occurs once, but 5 occurs twice. Therefore, 5 is the mode. But in this case, we have a lot of data values, hundreds of thousands. So, what is the mode in the case of these histograms? Is it a single number that occurred with the highest frequency? Or, is a range that occurred with the highest frequency? In this case, the mode is a range that occurred with the highest frequency, because we can't see the individual values, but we can see which bin has the highest frequency. So I'm going to give you a hypothetical example, and ask you to tell me the mode. Most people live to be about 60 or more. So let's say that this is 60 here. But unfortunately, some people reach the end of their time when they're younger. So where would you say the mode is? Would you say it's here, here, at 60? Or up here? We can see that the frequency is highest in this bin, so the mode is here. Now let's say that you live in a city where it rains a lot, all day every day. So, in this case, we're not going to have frequency here, we're going to have precipitation. And it rains about the same amount, and the average rainfall is about the same every month. So, we can just summarize our distribution with a line, and then we have what we call a uniform distribution. So where does the mode occur here, or, is there no mode? Well, it turns out that some distributions like this one have no mode. This is called a uniform distribution. However, some distributions have multiple modes. Distributions are considered multi-modal when there are two or more distinct, clear trends. So, what so you think about this distribution? Would the mode be here? Now remember, there could be more than one mode in this case, so you can check more than one button. Now, I've said before that data tells a story. And in the case of foot size, there are two modes, here at 7, and here at 9. Why is this? Because a lot of females have size 7 foot, and a lot of males have size 9, making this a bi-modal distribution. Now you're going to take a little true or false quiz that might be a little tricky, and it'll get you thinking more about the mode. Do you think this is true or false? Can the mode be used to describe any type of data, numerical or categorical? True or false that all scores in the in the data set affect the mode. Or, if we have a population, and we take a lot of samples from this population, and we find the mode in each sample, the mode'll be about the same. True or false? And finally, there is an equation for the mode. Is that true or false? This one is true, the mode can be used to describe any type of data, numerical or categorical. You saw in the last quiz that male versus female is categorical and the mode was male, however, all scores in the data set don't affect the mode, necessarily. Take this data set. The mode is 22 because it occurs 3 times and the rest of them occur once. However, if we take away the value 14, the mode will still be 22. Now, let's say we add an extreme value like 1 million, the mode will still be 22. So, we cannot say that all scores in the data set will impact the mode, this 1 is also false, if we take a bunch of samples from a population the mode is likely to change from sample to sample, lets illustrate this idea with this applet, its really cool. Tthis is a histogram distribution, this is called the bell shape curve and lets pick a random 5 sample that this software will generate for us So just look at these top two. You see that the mode is here, which is kind of at the left of this distribution. Let's do it again. Now the mode's here at the right of the distribution. If we do it again, the modes again, at the kind of far left of the distribution. Now here, there's not even a mode. So you ca see that if we take samples, the mode is somewhat arbitrary and it won't necessarily be similar to other modes and it also won't necessarily represent the population. Another thing about the mode is that it changes as you change the bin size. So, if we make the bin size smaller, looks like the mode is going to the left. Here it's at about 470. Here it looks like it starts at about 450. And the mode changes as we change the bin size. If we make the bin size really big, then the mode occurs in this interval, between 346 and 514 point 74. So the mode really depends on how you present the data. Because this third point is false, we can't really use the mode to learn about our population. And this is our goal in this course, inferential statistics. But still, the mode is important to take into account when describing a data set. And finally, this last one is false. While there's a procedure to find the mode, where you look at all the data values, you see which one occurs the most, or you look at the histogram and you see which bin has the highest frequency, we can't describe the mode with an equation, and this is why we often use the mean, or the average. Unlike the mode, the mean take all values into account because we add them all up, then we divide by how many values there are. And just to remember, we pretty much always write these calculations as decimals when we don't get whole numbers. So, I'm going to calculate the mean for Nursing. And remember, mean is often represented by x bar. So, the mean is 58,350 plus all the rest of the values in the column divided by 5, because there are 5 values. And when we do that, we get that the mean salary for Nursing, from these values at least, is 58,948. So now, find the mean salary for the five Geography majors. I'll call this x bar sub geo, for Geography. Write your answer in the box. You should get that the average salary for the geography majors in this sample is 47,718. Based on these statistics, if your only objective is to make more money, then you might choose to be a Nursing major. Now, what describes what you just did to calculate the average of the Geography salaries? Did you do this? Or did you do this? I'm purposely not explaining these in full because I want you to really look at it and think about what they mean. Or did you do this? Where sum means you add them up. Or did you do this? There's more than one answer here. So, check all that you think accurately describe calculating the mean of the Geography majors. This first option is simply adding up each of these salaries, and that's it. So, this doesn't accurately describe the procedure for finding the average. This one, however, is adding them all up, and then dividing by 5, which is the total number of values, which is the definition of the average. This one doesn't really make any sense. Summing up all of these and then multiplying by 5. And then, this one also says what you did. You took the sum of this salary of Geography majors and then you divided by the number of geography majors, which was 5. Now, which of these two procedures would work with any dataset? In other words, what if we had 100 geography majors? Which of these would work better in calculating x bar go, the mean? This one. This equation is restricted by 5 because we're not always going to have 5 here. This could be any number, and if we had 1,000 values, we most certainly would not want to write them all out. Instead, we can just call it the sum of all the salaries, divided by the number of Geography majors that exist in the dataset. So, this will work for any dataset. And, in fact, we can simplify this even further by replacing this with a special symbol and replacing these with something shorter. The symbol we'll use for sum, is this, sigma. And instead of writing out the whole salary of Geography majors, we'll just calls this x. And instead of writing out number of Geography majors, we'll just call this n, for number. So now, we have a really nice formula for the mean, x bar equals the sum of x divided by n. Isn't that pretty? Now remember, this is for a sample, and you saw in Lesson 1 that the sample statistic is denoted by x bar. For a population, we denote it by mu. And again, we can say it's the sum of x divided by big N, because this is the number in the entire population, whereas, this is the number in the sample. Now, this might look Greek to you, well, it is. Sigma is the Greek letter capital S, which makes sense because it means sum. When you see sigma and then x, this is then telling you to add everything that comes after the sigma. And actually, sometimes, there'll be a little i here. And this is going from i equals 1 to n, meaning that we sum x1 plus x2 all the way to xn and then divide by the total number. So, in this case this is x1, this is x2, x3, x4, and x5. And n is 5. So, this is telling us that we sum everything from x1 all the way to xn. Now, don't be intimidated by these symbols. In fact, I'm hoping that you're going to get more and more comfortable with using symbols throughout the course. So that, by the end, none of this seems intimidating. The reason I want you to feel comfortable using symbols is because they're very important and they'll help you with your mathematical thinking in every class. Symbols are like the alphabet. The alphabet is basically symbols and we use these symbols to tell a story. We've used them to learn about people from hundreds of years ago, who have written their thoughts, in these symbols. Now, it's the same with Math. Except, since Math is a different kind of language, we have different symbols. And Math is a universal language. These symbols are used across the world, which is really awesome. Mathematics is a way of thinking, and it has evolved over thousands of years. And the way we symbolize these thoughts has evolved with it. This statistical notation is shorthand instructions that tell us what to do. Now that you've had practice calculating the mean, before we move on, we're going to go over some properties of the mean. We're going to do this with a quiz, and this'll be a very challenging question that requires you to think about what we've discussed and remember how you calculated the mean. If you can get the answer to this, I'll be super happy. And, if not, don't worry. Again, this is a very challenging conceptual question. So what are properties of the mean? Check all that apply. All scores in the distribution affect the mean. The mean can be described with a formula. Many samples from the same population will have similar means. The mean of a sample can be used to make inferences about the population it came from, or the mean will change if we add an extreme value to the data set. All scores in the distribution affect the mean. Think of the mean like a teeter-totter. Where should the pivot go so that it balances? These rocks or balls represent the salaries of the Geography majors. Now, if this were a teeter-totter, where should the pivot go so that it balances? That's basically the mean, which in this data set of the salaries of Geography majors is 47,718, like you calculated. Now, if we were to add another ball, say here, then the teeter-totter would tip, therefore, the pivot would have to move this way so that it keeps balancing. So, you can see that all scores affect the mean. You've also seen that the mean can be described with a formula. We describe the sample mean as x bar, which is the sum of the values divided by the number of values. And the population mean, sum of the values, divided by the number in the population. This is also true that many samples from the same population will have similar means. For now, I'll just show you this applet again where if we pick a sample of size 5, we get that the mean is roughly in the middle. And if we do it again, again, we get that the mean is roughly in the middle. Let's do it a few more times. Now, we've calculated ten simulated means from this distribution and they're all roughly similar. This is actually a very important and awesome concept. And we're going to go into this in a lot more depth later in the course. Also, the mean of a sample can be used to make inferences about the population it came from. This relates to this last property of the mean because you saw that all the samples are pretty similar and that they roughly approximated the population. It's only the last one that's not a property of the mean. If we bring up the pivot example again, let's say, we add a stone way out here. Is this going to change the mean? You betcha. It's going to have to go to the left somewhere so that this will still balance. Let's explore this point a little more. With the starting salaries of the geography majors that you calculated before, what would the new mean be if we added a person who has a salary of $500,000? Calculate the new mean and write it in the box. Well now you should get that with these six data points, the new mean is 123,098. Based on this data, if your friend wanted to be a geography major, and asked you how much they could expect to make, would you tell them that they could expect to make 123,098, or would this be misleading? I think we can say with confidence that this average is misleading. That's because, if you look at the data, all of these guys are less than 123,098, in fact, significantly less. The average can be misleading when we have outliers. Values that are unexpectedly different from the other observed values. Outliers create skewed distributions by pulling the mean toward the outlier. This makes the mean a lot less representative of the middle of the data. Now, if we don't look at these, you've seen that the average salary of Geography majors is about 48,000. However, in the mid 1980s, the average starting salary of Geography students at the University of North Carolina was well over 100,000. Why was this? Well, maybe UNC had a really good Geography program or UNC's career services program found top-tier jobs for students, or maybe salaries in North Carolina were just higher in the 1980s. Actually, it's because Michael Jordan graduated from there and his starting salary was over $500,000. So, you saw that the mode isn't influenced by an outlier at all, but the mean is influenced a lot. We need something in between. This is where the median comes in, which is the middle of the data. What do we mean by this? If we look at the middle number in this data, so let's cross off the top and bottom and let's cross off the next top and bottom, and we're left with the middle. The middle number in each of these data don't really represent the data. In fact, they're the smallest value in each of these columns. So, what do we need to do to make the median a useful statistic that can actually represent the data? Calculate the average? Put the data in order? Eliminate outliers? Or eliminate data values that repeat? We have to put the data in order. So now that you know how to calculate the median, find the median for nursing and geography major's salaries. Enter your answers here. We can put them either in order from least to greatest, or greatest to least. It doesn't matter, we'll get the same results. So, let's just do least to greatest because that's more typical. So, for Nursing, the lowest salary is 44,640. Next is 56,380. Then, 58,350. Then, 63,120. Then finally, 72,250. So, let's cross off the first and last. And again, the second to first and second to last, and we're left with this one in the middle. So, this is the median, 58,350. Let's do the same for the Geography majors. First, we have 38,150. Then, we have 41,290. Then, 48,670. Then, 53,160. And finally, 57,320. So, again we'll cross off first and last. Second to first, second to last, and we're left with one in the middle. Note that the median splits the data in half. So we have the same amount of data points on the left as on the right. Now, let's include the geography major, Michael Jordan, who makes $500,000 a year. Where do you think the median is? Do you think it's 48,670? 53,160? Anywhere in between 48,670 and 53,160? Or exactly in between these two? Now I haven't taught you this yet, but if you guessed exactly in between these two, then you guessed correctly. Good job! If you put this data in order, you have 38,150, 41,290, 48,670, 53,160, 57,320, and 500,000. And then if we cancel out the first and last, second to first, second to last. We can't cross out these two, then we're left with nothing. Or we could include both of them, but we can't have two medians. So in that case we take the number exactly in the middle of these two. What is this number? Put your answer here. The number right in between these two numbers, smack dab in the middle, is 50,915. You may have realized this, but this is the average of those two middle numbers. We always take the average of the two middle numbers when finding the median of data sets with an even number of scores. So, in this case, there are 6 scores. You've done a lot so far and let's just summarize what happened when we added this outlier to the dataset. The mean changed from 47,718 when we just looked at these to 123,098 when we looked at all of them. However, the median only changed from 48,670 for that, to 50,915. This tendency of the median is called robust. In everyday language, robust means strong and sturdy, which makes sense for its statistical definition. It's not affected much by departures from the norm. So now, you've learned about the mean, the median, and the mode. These are all measures of center because they typically describe the center of the distribution. As you've seen though sometimes, the mean doesn't describe the center because of an outlier. Sometimes, the mode doesn't describe the center and the median doesn't take every data point into account. However, in the case of the starting salaries from the University of North Carolina, the median might have been a better statistic to determine what starting salary you would have made as a Geography major in the 1980s. The median is always the best measure of central tendency when dealing with highly skewed distributions. So let's see how measures of center change when we have different distributions. On this positively skewed distribution, which do you think is true? Do you think the mean is less than the median, which is less than the mode? Or do you think the mean is the greatest, and the median is the smallest? Or do you think the mode is less than the median, is less than the mean? Or do you think the median is the greatest and the mode is the smallest? Now remember, on the x axis, we go from least to greatest. So, on this distribution, think about where the mean, median and mode would lie on the x axis, and choose which one of these you think is true. This is a really difficult question that requires you to think about everything we've talked about so far, how the median and mode are not affected as much by outliers as the mean, how the mode is not affected by outliers at all, and how the median will barely change if an outlier is added. Give it your best shot. In this case, the mode is less than the median, is less than the mean. If you got this, great job. This was a really difficult question, so don't beat yourself up if you didn't get it. Let's analyze this. We can tell that the mode is here, in this bin. Now, since this distribution is skewed, these values out here will pull the mean more than the median. So, that means the mean will be maybe somewhere around there, but the median should be less. What about in this distribution? Which is biggest, the mean, the median, or mode? This time, decide what symbol should go in between these three. Should it be a less than sign, greater than sign, or equal to sign for each? There should be an equal to sign here, because the mean equals the median equals the mode. We can see that the mode occurs here, in the bin where the frequency is highest. But also, you can see that the distribution is symmetrical. Therefore, the mean and the median will both occur pretty much right in the center. This data is normally distributed. Not only can we use measures of center to make decisions, but we can also use them for comparisons. Take, for example, Facebook friends. How many Facebook friends do you have? Click the link at the poll that's titled, How many Facebook friends do you have? If you don't have Facebook, enter 0 for your number of friends. After this, you can click the Facebook friends results link and compare yourself to others. According to an article on Mashable, the average Facebook user has 229 friends. 22% of these friends are from high school, 12% are co-workers, and 9% from college. In 2008, the average Facebook user was 33 years old. But in 2010, the average user was 38 years old. And one more fun fact, 52% of facebook users visit Facebook daily. So, these are a lot of averages concerning Facebook usage. If you want to compare yourself to the average more, Zeebly.com slash socialme has some really fun statistics. They'll prepare a personalized report that analyzes your Facebook usage. So, if you haven't already, enter the number of Facebook friends you have by clicking the link at the right. We're going to analyze this data later. Now there's one more thing you have to do before we end this lesson. As I said before, it's really important to know how to use technology. And that's why we had you create a Google account, so that we can easily share real data with you to analyze. I asked all my co-workers here at Udacity how many Facebook friends they have, and then I put this data for you in a spreadsheet. Open this spreadsheet, and then copy and paste the data into a new spreadsheet. And then try and find the average number of Facebook friends. I'll help you get started. In any of the blank cells, we always start our calclulations with an equals. There is already a built in function called average that calculates the average. See you can write this and the spread sheet software will recognize that you are going to take the average. Then in parenthesis write your start cell which is the letter and then the number that corresponds to the location of that cell. For example c32. Then you'll put a colon because you want to include all the cells in between the start cell and the end cell and then finish it off with your end parentheses and then push enter or return. This should give you the average. Once you find it using the spread sheet tell us what you found. What's the average number of Facebook friends that my co-workers at Udacity have? So, here's the data and you see that people entered this data on January 2nd, and that the data goes all the way down to cell B28, right here, and it starts at cell B2. Now, we can calculate the average anywhere in this white space. So, let's pick this one, looks like a good cell, good old cell D4. So, we'll write equals average of cell B2 all the way to B28. Now, we'll just make sure that all of the data is highlighted with this dotted green line. That way, we know that we are accounting for all of these data. And then we push Enter. So, you should have gotten that the average Udacian, at least in this sample, has 585 Facebook friends. Another way to do that is to sum cells B2 to B28 and then divide by how many people there are. So, there are 27 people and we get the same thing because remember, what we just did here when we took the sum of all the data values then divided by n, that's the formula for the mean, for any dataset. And that's what you worked with earlier in this lesson. So, spreadsheets are not only useful, but they're a great way to look at the same concepts in a different way. What if we wanted to find the median number of friends using this spreadsheet? Remember, that first we want to order the date. So, there's a very useful tool called Sort which is in pretty much all spreadsheet software. In Google spreadsheets, the way you do it is by first highlighting the data. And I'm going to show you a keyboard shortcut really quick. From cell A2, hit Shift and then Right arrow key, so it highlights cell A2 and B2. And then, Command, Shift, Down arrow key. And then, this highlights all the data. It's easier to do it this way than by simply dragging your cursor all the way down to the bottom. This is just a quick and easy way to highlight all the data. So now, click on Data, and we want to sort by column B because that's the data we're interested in. We're not really interested in the time stamp. So we'll click Sort Range, and then we should get a box here saying, Sort Range By, and we'll say column B. And in general, we want to sort from least to greatest. So, I'm going to stop here and let you try and experiment with this. And then after you sort the data in the spreadsheet, try finding the median Let's do this thing. So, we'll go to cell A2, Command, Shift, Right arrow key, Command, Shift Down arrow key, highlighting all the data. Data, Sort range, Sort by column B. And even though it says A to Z, that also applies for numbers, from least to greatest. Ascending rather than descending, and then Sort. And now, we have all our data sorted. Note that it starts in cell B2 and goes to B28. Meaning, there are 27 values that we're interested in. And that means our median is going to be this one, 479. With 479, we have 13 values greater, and that's because we're going from cell 16, B16 until B28. So, we can find that number of cells by just doing 28 minus 16 plus 1, that gives us 13. And there are 13 values below 479. And likewise, we can do 14 minus 2 plus 1, which gives us 13. So, here's our median in the 14th place. It says 15 here, but that's because the data starts at cell 2. So, this is the 14th value. After sorting our 27 data values, how do we know that the median was the 14th value besides by simply counting? Is there a way we can describe the median value in symbols? Let's start first by going over some symbolic notation that we briefly mentioned earlier. Remember that these are the rankings, these are not the actual data values, but we can denote the actual value in terms of its rank. So, the value in the first rank would be X sub 1, then X sub 2 for the second rank. X sub 3 for the third, and in the case of Udasiyans Facebook friends, X sub 14 is the median. And remember that any value we can generically denote X sub i where X sub i is the ith value. Yes, we do say ith. So what about for any data set with n values? The median will be different depending on whether or not n is even or odd. Now, this is going to be a really tricky quiz. Which formulas describe the median after sorting the data? There's only one answer for n even and one answer for n odd. Now, really quick, this might look daunting, but just remember that this subscript part corresponds to the rank. So, for example, say there are four values in the data set, n divided by 2 means 4 divided by 2 which is 2. So this would mean X sub 2, which is the second value might be the median if n is 4. I'm not saying that's right, I'm just giving you an example. So give this your best short. And again, this should really help get you used to using symbols. Let's go over each of these individually. And let's say that n is 4. So, we have four values here in order, from least to greatest. So if n is 4, the rank of this value is 1, and the rank of this value is 4. And the actual values are these. So, for this one, n divided by 2 would be 4 divided by 2, which is 2. X sub 2 is this one. Is this the median? No, because since it's an even data set, the median is going to be the average of these 2. But now, we know that this data value is Xn divided by 2. The rank n minus 1 divided by 2 would be 4 minus 1, which is 3. And then divided by 2, which is 1.5. And that doesn't exist. We don't have any with a rank of a decimal value, only integer value. What about this one? We have X sub n plus 2 plus X sub n plus 2 plus 1. So, if this is the rank n divided by 2, n divided by 2 plus 1 is 3. And then, if we divide it by 2, we get the average of these two values. So here, we have our answer. And you see that this value is this third one. Using this logic, we can extend these formulas for any even numbered data set. Okay, let's say that n is 5. So, we have 5 values in order. Can we divide 5 by 2? No. If n is odd, then we can't have a decimal rank. What about 5 plus 1 divided by 2? Well, that's 6 divided by 2. And 6 divided by 2 is 3. So that means we would get the value in the third rank. And that is the median. Let's still explore the others. If we have 5 minus 1 divided by 2, that gives us 4 divided by 2, which is 2. And the value in this second rank is not our median. And finally, we have 5 divided by 2, which is 2.5, plus 1, which is 3.5. And again, we don't have a value that's ranked 3.5, only integer values. This is probably kind of confusing. So before we finish this solution, let's just apply this one to the Udacion's Facebook friend data set that we had before. Remember, we had 27 values. So, we want the value in the n plus 1 over 2 place. So if we plug n in, then we get 27 plus 1 divided by 2, which is 28 divided by 2, which is the 14th place. And then, when we looked for that value in the 14th way, we found that the median was 479. Hopefully, that makes some sense and that it's getting you more familiar with using symbolic notation. Good Job. So now, you've learned about three measures of center, the mean, median, and mode. Now, for your final quiz, we're going to solidify the ideas that we talked about in this lesson with the pros and cons of each. Which of these have simple equations or will always change if any data value changes? Which of these are not affected by change in bin size? Which of these are not affected severely by outliers? And finally, which of these is easy to find on a histogram? Remember, that bin size refers to when we're creating the histogram of the data. So, check which ones apply to each of these measures of center, and that's your final quiz. So, the mean does have a simple equation. And if you remember, it's the sum of x divided by n. The mean will also always change if any data value changes. Median will sometimes change, and mode will also sometimes change. The mean is not affected by change bin size, it will always be the same, no matter how we visualize the data with a histogram, and the same with the median. But you've seen that the mode does change, it becomes a new interval if we change the bin size. You saw in the case of the Michael Jordan example that the mean is affected by outliers, but the median not as much, so not severely affected, same with the mode. And finally, it is kind of hard to find the mean and median on the histogram. But you always know which one the mode is, because it's the highest frequency. So, congratulations. You finished a pretty difficult lesson. From here on out, you're going to do more and more calculations, but gradually we're going to ease into doing these procedures. So keep at it, you're doing great. Let's say you're debating whether or not to get a social networking account. So you look at distributions of salaries of a sample of 1,000 people with social networking accounts and a sample of 1,000 people from the general population, which may include people that also have social networking accounts. If this is distribution 1 and this is distribution 2, fill in the blanks here with either the less than sign, greater than sign, or equals sign. What do you think? Well, it looks like the mean, median and mode are about the same. You can see that the mode occurs here in this bin and same with this bin, which is 40,000 to 50,000. And that the mean is probably about here at 50,000. And the median is about in the middle as well, because the distribution is pretty symmetrical for both. So then what's the difference between these two distributions, and did they help you in your decision as to whether you should get a social networking account? What do you think? Should you get a social networking account, if you're just looking at how to increase your salary? There is no wrong answer. We just want to know what you think, based on these distributions. Write your answer here. So if the mean, median and mode are the same for both of these distributions, what's the difference between these two? There might be more than one answer. Are the salaries of people with social networking accounts more consistent? Or maybe the salaries of the general public are more consistent. Or is it that people with the highest salaries don't have social networking accounts? People with the lowest salaries have social networking accounts? Or finally, the distribution of salaries of the general public sample is more spread out. Check all that you think we can say about these distributions. Well, it looks like the general population is pretty spread out. It ranges from really low salary to really high salary, so we can automatically say this one. That means also that people with the social networking accounts, their salaries are more consistent because it's closer together. And then, we can say that the salaries of the general public are more consistent. We actually cannot say this one. Because this is the general population, and it could include people that have social networking accounts. So, these high salaries up here, we don't know if they have social networking accounts or not. And we also can't see this fourth one for the same reason. So we can see that this distribution is more spread out than this one. How can we quantify this spreadoutness? How about we look at the maximum value and the minimum value and find the distance between them? Let's say here that the maximum value is 78,600 and that the minimum value is 21,180. And then let's say here that the maximum value is 116,020 and that the minimum value here is 7,350. What's the range for this distribution, or the maximum value minus the minimum value? And what's the range for this distribution? Here, the range is 78,600 minus 21,180 which is 57,420. Here, the range is 116,020 minus 7,350, which is a lot more than the other distribution. So this is one way of measuring how spread out a distribution is. The range is easy to compute and understand, and provides a quick familiar picture of how spread out your data is. However, as we've seen before, convenience comes out of price. Range changes when we add new data to the data set always, sometimes, or never? Range sometimes changes when we add new data to the data set. We could add a lot of data values here in the middle and it would really change the shape of our distribution. But it wouldn't change the range. Same here. So instead of a 187, we could have 190. We could keep adding values to the data set and the range would never change. But if we added a value out here higher than the maximum value or out here or even down here. It would change the range of our data set. We lose detail with the range because it's only based on two scores. And these scores are the most extreme in the distribution. These extreme values are likely not very representative of the rest of the distribution. Let's say that we include Mark Zuckerberg in our sample of people with a social networking account. I don't know his salary, but let's suppose it's like 10 million. Now what's the range of this distribution? Well now the range is 10 million minus 21,180. This makes the range 9,978,820. That's a lot bigger than this range, just because we added one extra datum. Mark Zuckerburg is an outlier, and this significantly increases the range of our data. In general, outliers increase the variability, but way too much when we use the range as our measure. Therefore, the range may not accurately represent the variability of data, especially in the presence of outliers. One way statisticians deal with outliers is by cutting off the upper and lower tails of the distribution. So we're going to cut off Mark Zuckerberg. Then we only consider the data values in the middle. What do we mean by cut off the tails? Traditionally, statisticians cut the lower 25 percent and the upper 25 percent. So let's say you have eight data values. We're going to cut off the bottom two, or the bottom 25 percent, and the top two, the top 25 percent. So all we care about are, are these values in the middle. So with this distribution do you think the first quartile will be here at 25000? Or will it be to the left or to the right? This might be a kind of tough question. But try your best. And keep in mind that we're trying to divide the data into four equal groups.And the first quartile divdes the first group from the other three. If we look at this graph, it looks like 50,000 splits the data in half. So, we have about 500 values on this side and 500 on this side. 500 split in half is 250. So, we know that the first quartile has to occur when there are 250 values, Which will be somewhere here. This is to the right of 25,000. Here are samples of size 10 from each of these distributions. So I'm going to find the first quartile, which we'll henceforth call Q1 for the general population. And I'm also going to find the third quartile, which we'll call Q3. First, let's split the data in half. So we have five values above, and five values below. Then we find, essentially, the median of this half, which is 38,801. So this is Q1, the value of the first quartile. And then we find the median of the second half, which is 56,863. So this is Q3 and the median we'll call Q2. And our new range, then, will be the distance between Q1 and Q3. So 18,062, then, is our new range, when we cut off the upper and lower tails. Or the upper and lower 25%. So now, how 'bout you do the same for this one. Cut off the upper and lower 25%. This one is Mark Zuckerberg. And then let's find the range between Q1 and Q3. Put your answer here in the box. So again we'll split the data in half, so five below, five above. And then we'll find the median of the first half, which is 49,191, so this is Q1. If we find the median of the second half, we get 54,135 or Q3. The difference between them if 4,944. This is a lot more representative of the variability of this distribution than if we included Mark Zuckerberg. When you fouind Q3 minus Q1 you were calculating what we call the interquartile range abbreviated IQR. I defnitelly don'talways want to write that out. Iqr is a lot easier. So now you're going to take a quiz on the interquartile range. True of false? About 50% of the data falls within the IQR? The IQR is affected by every value in the data set. And finally, the IQR is not affected by outliers. So based on what you've learned about the inter quartile range, give this your best shot. This one is true, especially for large data sets. In each of these samples, our sample size is only 10. So here we don't exactly have 50 percent of the data, but it's pretty close. And this will more and more closely approximate 50 percent as the data set gets larger. Think of it this way. We split the data in half. We split it in half. And we split it in half here. If this is 25 percent, this is another 25 percent, another 25 percent, and another. Then here between Q1 and Q3, we'll have about 50 percent. This one's false. We could make this one a lot bigger, and the IQR would still be the same. However, we could change this if we made it, say, 59,000. Then the IQR would get a little bigger. If we made 54,135 bigger than 60,181, then this would be the new Q3. So sometimes certain values can influence the IQR, but it's not affected by every value in the data set. And the same concept applies here. We have this outlier, but even if this were something closer to the data set, like 63,000, the IQR would still stay the same. So this one's true. So what even is an outlier? Given this data, where do you think an outlier would be? So let's just look at this data for now. What values do you think are outliers for this data set? 60,000, 80,000, 100,000 or 200,000? So just looking at this data, which do you think are pretty far from the data set as a whole? This is just to get you thinking. Maybe none of these are outliers. Because see there's this really extreme data value, so if that's part of the data set, then maybe these are not outliers. That's why we have a way to statistically calculate whether a value is an outlier. A value is considered an outlier if it is less than the first quartile minus 1.5 times the IQR. Or if it's greater than Q3 plus 1.5 times the IQR. So here you've found that the inter-quartile range was 4,944. And you know that Q1 is 49,191 and Q3 is 54,135. So, using this universally accepted definition of outliers. Statisticaly decide which of these are considered outliers. Well, let's calculate the range for values that are not considered outliers. So Q1 is 49,191, and the interquartile range is 4,944. So that means that Q1 minus 1.5 times the IQR is 49,191 minus 1.5 times 4944. And this is 41,775, so anything less than this number is considered an outlier. We also know that Q3 is 54,135 so Q3 plus 1.5 times the IQR Is 54,135 plus 1.5 times 4,944, and this is 61,551. So anything between these numbers is not an outlier. Anything less than this is an outlier, and anything greater than this is an outlier. So that means that statistically these values are outliers, but not $60,000. We visualize quartiles and outliers with bo plots also called box and whisker plots. It looks like this, where this line represents the min. This line repressents the locatin of Q1. This line represents of Q2 or the median. This line repsresents the locatin of Q3. And this line is the max, and then, outliers are represented by dots in their respective location. Boxplots can also be drawn sideways like this, with the minimum value, the median, Q1 and Q3, and then the maximum value, with dots that represent the outliers. Given that information, which of these boxplots do you think corresponds to these distributions? This one with and without Mark Zuckerberg, and this general population distribution. And just a hint, this dot right here is an outlier. So, type A, B, or C into each of these red boxes, corresponding to these boxplots. Well with Zuckerberg, there is an outlier here, so C must be the box plot that corresponds to this distribution with Mark Zuckerberg, and without Mark Zuckerberg, it'll look pretty much the same. The interquartile range as you saw before will remain the same, as will the min and maximum value, not including Mark Zuckerberg. In this one, since it's more spread out and it has a larger interquartile range. Corresponds to this dot plot. Of course, these values would have to be exactly placed on a number line for it to be an accurate box plot. But in general, this is how the box plots will look differently from each other for different distributions. So you've seen then that we can visualize data with histograms. You learned that a few lessons ago. And we can also visualize data with box plots, which shows specifically the median, the interquartile range, the min and max value. And remember that the distance between these is the range. So now I'm going to ask you a conceptual question. Will the mean always be between Q1 and Q3? Yes or no? The answer is no, although usually it is, especially for symmetrical distributions like these, the mean will generally be in between q1 and q3 and close to the median if it's perfectly symmetrical. But take this data set for example. It has this outlier 90. Q1 is 1 because here's the median with six data values on either side And if we split this half of the data in half, the average of 1 and 1 is just 1, and if we split this data in half, we get that q 3 is 3, but if we calculate the mean, we get that the mean is 8.62, which is a lot more than q 3. You saw though that neither the range nor the IQR take all the data into account. We could have two totally different data sets with the same IQR. Like these. A normal, a bimodal, and a uniform distribution. So the IQR doesn't tell us as much as we would like to know about the dataset. Remember how we used all of our data when calculating the mean because remember with the mean we took the sum of all the values and then divided by the number of values. So that used every data value. What if we were to take a similar approach when calculating our variability? In other words, we need one number that decribes the spread of data that takes all the data into account. Let's go through some scenarios. Here are three options for measuring the variability using all data values. One option is to find the average distance between any two data values. So, for example, if you had a bunch of data values, you find the distance between each one from each other, and then you take the average. Or find the average distance between each data value and either the min or the max. So the distance between these two, these two, these two, et cetera. Or you could use the min. So the distance between those two and all of them, and then find the average. Or find the average distance between each data value and the mean. So if this is the mean, find the average distance between each one of them and that point. Which of these do you think would be best? Let's go through each of these options. Let's say you had just three data values. We'd find the distance between this and this. And then, we'd find the distance between this and this. And then, we'd find the distance between this and this, and the distance between this one and this one. And finally, the distance between this one and this one. The distance between this one and this one. So we've calculated 6 distances. Now, imagine if we were to add another data set. Then, we would calculate 6 more distances. The distance between this and this, this and this, and those two, and then the distance from this point to all the others. Now imagine if we had 1,000 data values, which is very typical for a data set. This one just isn't feasible. There are way too many things to calculate. Okay, so what if we found the average distance between each data value and neither of them max or the min? If we add four data values and we chose the min, we would only have to find three distances. Well, I guess four actually including the min itself. But, what if we had an outlier? What if we had a value all the way out here or even further? Then, the distance of each from the outlier would be really far. And it wouldn't matter how closely grouped these were. One outlier would cause a huge variability. So, this one doesn't work either. However, the mean is a really good reference point because it's in the middle of the data. And the mean takes every data value into account. So this is our best bet. This is the sample from our general population, which you analyzed before when you found the core tiles. To calculate our variability in the way we discussed from the last slide, we first need to calculate the mean. So find the mean. The mean is $52,793.80. And if you remember, we did that by adding each of these up. And since there are 10 values, we divide the sum by 10. To remind you of the way we put this in symbols, we summed the first through the tenth, and then we divided by the number of values, and then we get the mean. So now let's calculate the deviation from mean by subtracting the mean from each of these values. So, the first one here would be 33,219 minus $52,793.80. Just to practice, enter a few deviations from the mean. Remember, some of these might be negative. Here are the deviations from the mean. It's always important to maintain consistency when you do your calculations. So if you subtract the mean from each of these values, in other words, this value minus the mean, you should do this with all your calculations rather than going the opposite way, x bar minus xi. So now calculate the average deviation. This is essentially just calculating the average of these numbers. Put your answer here. You should have found that the average deviation is 0. If you use technology, you might have found that it's something like this, which basically means times 10 to the negative 12th, which brings this decimal point to the left 12 spaces. This is practically 0. Once you knew the mean, what equation describes what you did to find the averrage deviation? This is a difficult question, especially because we are just barely dipping our toes in the water when it comes to using formulas, but that's why I'm asking quizzes like these, so that you get more and more comfortable using formulas and understanding what they mean. So give it your best shot. Well this one would just be one value, where we take one of the data values, subtract the mean, and divide by 10. But we used all of these values, so it can't be this one. This one doesn't really make sense, we wouldn't add the mean to one of the values and then divide it by 10. This one makes a little more sense because we have this summation sign, sigma sum, but we're not adding up the Xi's and dividing by 10. That is the mean, but we are adding up the deviations from the mean, which are each of these, and then dividing by 10. So this is the formula that describes what we just did. Good job. But now we have problem. We got 0 for our average deviation, and that's can't be a good measure of how spread out these are. I would think that, if we had the same data point over and over again that the spread of this would be 0. But here we have a lot of data points all over the number line and there's got to be some kind of spread there. So this can't be our measure of spread, if we're going to get 0. The problem is that the negatives are cancelling out the positives. When measuring spread, we don't care if our data values are above or below the mean. We just care how far they are from the mean. So how can we make sure that the positive and negative deviations don't cancel each other out? Choose all that you think could work. Ignore the negative sign, at least for those with negative deviations. Multiply each deviation by 2. Square each deviation. In other words, multiply each deviation by itself, or is there nothing we can do? But first, this is very tricky, please think hard about this because measuring spread is crucial to statistics. Good luck. This might be a bit surprising but sometimes in math we can just ignore certain things. If we ignore the negative sign, what we're really doing is taking the absolute value of these. So we're just looking at the positive distance of each value from the mean. If we multiply each deviation by 2, we'll still have negative deviations, or negative double deviations now, and positive double deviations, and the negatives and positives will still cancel out. However, we can square each deviation. A negative times a negative is a positive. If we multiply this number by itself, we'll get a positive number. And a positive times a positive is also a positive. So we'll end up with some positive square deviations. There's always something we can do, never give up. So let's do our first idea which is to ignore the negative signs and make each deviation positive. We can write this as the absolute value of each value minus the mean. Convert each to a positive. So basically, all of these are just going to be positive, and the already positive ones are just going to stay positive. Now calculate the average absolute deviation. Put your answer here. If we take the average of these numbers, we get $13,543.56. Now what formula describes what you just did? More than one of these formulas can describe what you just did to find the average absolute deviation. This one is really tricky and the purpose is to get you really used to formulas. This one means that we're first summing up all the values, and then we're taking the absolute value. Since these are all positive, taking the absolute value won't do anything. It'll already be positive, but this says nothing about the deviations. In fact, this will just be the mean again. This one says that you're finding the absolute deviation. So you're taking a value minus the mean, which are these deviations. But then taking the absolute value of them, here, adding them all up, with this sigma, and then dividing by n, the total number. Which in this case is 10. So this one does describe what you did. This one says that you're finding each deviation, which are here. You're taking the absolute value, which are here. You're dividing each of these by n, which in this case is ten. And then you're adding all of them up, which will give you the same thing as if you first added all of these up, and then divided by 10. So this one also describes what you did. This one, looks like it's going the opposite direction, so instead we're taking the mean and then subtracting each of these values. So we would get the opposite of each of these. We would get positive 19,574. This would also be positive, but these would be negative. But then it says we take the absolute value, which will lead us right back to this column here. Because it doesn't matter if these are negative or positive. Then we divide by n, and we get the same thing as this. This one says that we're just using one data value. We don't know which one it is, so tracking the mean, taking the absolute value, and then dividing by n, which doesn't give us what we want. This was very tricky, but the more you work with formulas, the more comfortable you'll get with them. And don't worry, by the end of this course, you're really going to understand all of these symbols and formulas that we're using. And it'll really help you in expressing mathematical ideas. Another way to get rid of these negative values is to square each one. In other words, we multiply each value by itself. So go ahead and do that here. Now I admit, I did not enjoy writing all of these out. In fact, I never want to write these out again. Hopefully, this will get you really excited about using spreadsheets. Because that's going to make your life so much easier, and mine. Because then I won't have to write these out for you. So now we want to find the average square deviation. If we add all these, what formula describes this procedure? Only one is correct this time. And I know this is tricky, but the purpose is to really get you familiar with using formulas and understanding how they explain a certain procedure. Well this one means that you're squaring each of these values, and then adding them up. Which isn't what we're doing here. Here we're adding up the squared deviations. For this one you really have to be careful about having parentheses or no parentheses. Because this means that we only sum up the values, all of these, and then we subtract the squared mean, which isn't what we did. Again, here parenthesis are important. This means that we sum up all the values, then we subtract the mean, and then we square the result. That's not what we did. This one doesn't have a summation sign, which means that we take some arbitrary value, unspecified. So like $38,801, for example. And then we subtract the mean. And then we square the results. And that's again, not what we did. But this one says that you take this, which is each value, minus the mean, to get the deviation. You square each deviation, and then you add them all up. So this is the correct formula for adding these up. This is called SS, for sum of squares. So now if we wanted to take the average, since we already added them up, all we would do is divide by 10, which is the number of values. So calculate the average squared deviation, and put it here. I promise this will be the last tedious thing you do by hand. The average score deviation is 291,622,740. There's a really special name for this. It's called the variance. How can we put this variance in words? Mean of squared deviations? The sum of squared deviations divided by n? Or mean squared deviation squared? There may be more than one correct answer. The purpose of this quiz is to make you better understand what the variance is, and it helps to put it in words. This first option, mean of squared deviations. Each of these is a squared deviation. And then we took the mean, which is adding them all up and then dividing by 10. So this is a way to put it in words. Sum of squared deviations means we, sum up all these, and then we divide by N, which is exactly what we did. The mean squared deviation squared, would be finding the mean of this, which is the variance, but then squaring this number. So this one doesn't work. Let's visualize what we did here. We have a bunch of data values and we found the mean, let's say it's here. Then we found the distance between each value and the mean and then squared them. This is essentially the area of every square whose side length is the distance between each value and the mean. So we all of these squares and the sum of squares is basically the sum of the area of each square where each distance is Xi minus X-bar and each area is Xi minus X-bar squared. So then we find the average square. So the average square might be something like this size, let's say. In this case, the dimensions are in dollars. So the area of this square would have units of dollars squared. So, how do we convert this back to just dollars? Take the square root, divide by 2, or subtract one dollar sign? We take the square root. If we know the area of this square, then if we take the square root, we end up with just the length of one of the sides. Since it's a square, all the side lengths are the same. So, for example, if the area of this square is 49, then each side length would be 7. That side length of the square is the standard way of measuring the variability. And it's called the standard deviation. So to recap what we did, we found the deviation of each value from the mean. Here's the deviaton. And then we squared each deviation, which is the same as the area of these squares. Then we found the average area of the squares. And then we took the square root of this area to just find the length of the side, which is the standard deviation. The standard deviation is the most common measure of spread, and the symbol is the lower case Greek sigma. Remember that the upper case sigma is the sigma sum sign. Recall that the variance is basically the area of this average square deviation. So when we take the square root of the variance, we get the standard deviation. So our last step to finding the standard deviation of this data set is to take the square root of the variance. So take the square root of this, and enter your value here. We get approximately 17,077. So remember, this was the data set from the general population distribution. Now let's find the standard deviation of our sample of social networkers, and compare it to this standard deviation. Notice how when we did this, we were very organized. We found the mean, we found the deviations from mean, we squared each deviation, we took the average And then we took the square root, to get the standard deviation. So finally, find the standard deviation, of this sample, of size 10, and of people with the social network. We're going to ignore Mark Zuckerberg, he's no longer part of our sample. I recommend you organize it like we did before, where you have a column of the deviations, and a column of the squared deviations. This is going to take you awhile, especially because it's probably your first time calculating the standard deviation by hand. So take your time, because the important thing is that you really understand how to do it, and what it is. And when you're done, put your answer here. I'm going to show you the answer to this, by doing it in a spreadsheet. You could do all of these calculations, without any technology. And if you did that, that's really great. It's always really good practice to calculate things without using any technology, but just for the purposes of figuring out the standard deviation, technology can really speed along this process. So I'm not going to use any shortcuts here. We're going to do it exactly as we would if we didn't have technology. So first we need to take the avearage of all of these, we're going to sum them up. So we're taking the sume of all of these. I could also just write equals a1 plus a2 plus a3 all the way to a10. And we'd get the same thing. But why would we do that if we can simply write this. Now to take the average all we do is divide by the number of values which is 10. So that's just going to be 51,511.1. Alternatively, the nice thing about technology is we can just do this. Take the sum, and divide by the total number. Do it all in 1 step. So now we have the average. Now we're going to subtract the average from each 1 of these values. Not the opposite, where we subtract each of these values from the mean. That's an important distinction. In this case, it doesn't matter as much but in other statistical concepts that's an important distinction to make. So we'll write equals A1 minus. The mean. So I'm subtracting the mean from each of these values. Now I could just do the same thing here and write equals a2 minus the mean but that would be tedious. We can just drag this down. When you do that, remember that there has to be a little plus sign there. That means you're successfully dragging it down. If you went like this, it won't do anything. It'll just highlight the boxes. So here, we have the deviations from the mean. Here, in the next column We're going to square each deviation. Equals b1 squared. And again, we're going to drag it down. So we have the squared deviations for each of these values. Now remember that the variance is the average squared deviation. So we could just write. Average of c1 to c10. But I want to make sure we go through all the other steps in between. So let's again practice calculating the average just for clarity's sake. So the variants then would be the sum of c1 to c10. Remember that's how you start out taking the average, and then divide by 10. So here's the variance, and then the standard deviation is simply the square root of the variance. So we'll write equals SQRT. That's the shortcut for square root. And then we can just see C13. So we know that the standard deviation is 6557.16 approximately. Now I want to point out something really important before we finish this solution video. Here I simply said equals square root of this cell C13 Teen. Whereas here, I wrote out the whole average. The reason for that is because say I had but this all here, A13. Then, when we drag it down, we don't get the right deviations. And we can double click on it, and see what it did. Here, it took A4 minus A16, whereas here it took A1 minus A13, which is what we wanted. But we want it to always stay A13, which is why we have to make sure this is a constant. And the way to make sure it's a constant is by just writing it. Notice also that all of these values changed when these values changed because all these values are dependent of these values So when we change it back we should again get the correct standard deviation. What's a way to put the standard deviation in words? Just to warn you, this is going to be a really tricky quiz. Maybe read them out loud slowly. You'll have to think about each one, but this will really help you understand what the standard deviation is. Are you ready? Here are your options. More than one of these could be correct. I put parentheses to specify the procedure. So, for example, with this one here, I could have put parentheses around just some of absolute deviations, which would mean you find each absolute deviation, you sum them up, and then you square it. But instead I put the parentheses here, which means you take the absolute deviation, you square each one, and then you take the sum. This one's a little more complicated. This basically means that you sum each square deviation, then you divide it by n, and then you take the square root. So this will be very tricky. Go through each one of them and try and see if it'll give you the same thing as the standard deviation formula here. Good luck. Let's go through each one of these. With this first one, we take the square root of the average square deviation. Well, the deviation is x i minus x bar. The square deviation is then just x i minus x bar squared, and the average is summing each squared deviation up and then dividing by n. And this is our formula for standard deviation. The average square deviation is basically this inside the square root sign, but then instead of taking the square root, we square it. This is not the standard deviation. The sum of square deviations, well these are the square deviations so the sum is just this. Remember that this is also called sum of squares. Sum of absolute deviations squared, the absolute deviation is xi minus x bar with the absolute values, and if we square them that's the same as if we didn't take the absolute value. And then if we take the sum, we get the sum of squares again, so these two are equivalent, and they're not the standard deviation. Finally, we have the sum of squared deviations, divided by n, and then the square root of that. So that is also our formula for standard deviation. Good job. I know this was really hard. Now that you've calculated the standard deviation by hand, we can finally do it with spreadsheets. And that's what we're going to do from now on. Open the spreadsheet called Sample Social Networkers Salary. The link should be at the right. This has 100 data values and you're going to calculate the standard deviation. You'll have to copy this into your own spreadsheet. To guide you, I've listed instructions, and they have numbers that signify the order in which you should do them. So first find the average of these values. Then you find the deviation. Then you square each deviation. Then you find the average squared deviation. And then finally, you take the square root. And this is the standard deviation. Once you've calculated the standard deviation, enter it here. So first let's find the average of all these values. You can see that the values go all the way to 101. Since there's not a value in cell a1, there are 100 values. So here, we'll write equals average, a2, through a101. That gives us the average. Now here, we type a2 minus whatever we got for the average, which is our deviation. So equals, a2 minus 50,586.36. I usually like to round it to two decimal places. Make sure that you don't put in any commas. Now, we drag this down, all the way to cell b101. And now we have each deviation from the mean, so remember, this is xi minus x bar. Now we're going to square each deviation. Equals, b2 squared. And again, we're going to drag this all the way down. Now we can find the average of the squared deviation, which is the variance. So we write equals average c2 colon c101. So now we have the variance. And finaly, we take the square root of the variance to find the standard deviation. Equals SQRT of cell f1. And you always kjow if you are using the right cell, if there is a dotted line highlighting it. So here it is. You should have gotten approximately 10,657 for your standard deviation. Due to rounding, there is a range for answers that were correct in this quiz. Good job! This is really tough stuff. And actually, this is probably one of the most complicated calculations that you'll have to do throughout the rest of this course. Now you saw that it's pretty complicated to calculate the standard deviation, but what's so great about it? Why don't we just find the average absolute deviation? Why do we have to square each deviation, find the average square, and then take the square root? Doesn't it seem like a lot of extra complicated steps? Well actually, the standard deviation is really cool, and it helps us a lot when we do statistical analysis. It turns out that with a normal distribution, and if you remember, that's where the mean equals the median equals the mode, right in the center of the distribution, which is symmetrical, the standard deviation has great properties. Approximately 68 percent of the data falls within 1 standard deviation of the mean. So here's 1 standard deviation on each side, and I'm using the lower case sigma to represent standard deviation. So 68 percent of the data falls between this value, which is x bar minus 1 standard deviation, and this value, which is x bar plus 1 standard deviation. And 95 percent of the data falls within 2 standard deviations. So this value here is x bar, the mean, plus 2 standard deviations, and this value is x bar minus 2 standard deviations. So 95 percent of the data will be between this value and this value. In fact, we can approximate how much data will lie in between any number of standard deviations from the mean. We know this because some poor mathematician calculated this for us and put it all into a really nice table. We're going to use this table a lot in the course, but not yet. So now we're going to take a quiz. If the mean is 60 and the standard deviation is 13, what are the values one standard deviation below the mean, two standard deviations above the mean, and what's the value x bar minus two standard deviations? Well if the mean is 60 and the standard deviation is 13, one standard deviation below the mean would be 60 minus 13. 60 minus 13 is 47. So here's 60, the mean, and we go back 13, and this value is 47. Two standard deviations above the mean would be 86, because we go up 13 for one standard deviation and another 13 for two standard deviations. And finally, the mean minus two standard deviations would be 60 minus 13 minus 13, which is 34. So let's say we have this population here. I calculated that the mean is 18.97, approximately, and standard deviation is 5.99. If you want you can go ahead and see if you get those same parameters. By the way, I randomly generated these numbers using R. So let's take a sample. Let's say this is our first sample. It includes 18, 20, 23, 18, 21, 15, 17, 22, and 21. So there are 9 in this sample. Calculate the standard deviation of this sample the way that you've learned. First you need to take the average of these, which is 19.4. Remember that the sample average is denoted by x bar. Let's round to two decimal places. Now we subtract the mean from each value in our data. Then we square this deviation and then add them up. This sum of squares is 54.22. After that, we divide by the sample size, which is 9, and then we take the square root. We get then that the standard deviation of this sample is 2.45. Whoa, that's a lot smaller than what we got for the population standard deviation, which is 5.99. In general, samples underestimate the amount of variability in a population, because samples tend to be values in the middle of the population. Especially in a normal distribution, most of the values are centered here in the middle. So when we take samples from it, most of our values are going to be around here, since most of the values are in this area. Therefore the variability in this sample will be less than the variability of the entire population. To correct for this, we use something called Bessel's correction, where instead of dividing by n, we divide by n minus 1. Same within the variance. So what will dividing by n minus 1 do to the original standard deviation and variance? Will it make them bigger, or will it make them smaller? Well, when we subtract 1, we have a smaller number now in the denominator. And when we divide by a smaller number, we get a bigger number. You saw that we denoted this corrected standard deviation by lower case s. When we have a sample, this is what we use to approximate the true population standard deviation sigma. We call this the sample standard deviation. But don't get confused with the actual standard deviation of a small data set. For example, let's say we have this data set where n equals 5. To find the standard deviation of this, we would find the mean, which is 3, take each squared deviation from the mean. Add up the square deviations, which if also called the sum of squares, and divide by 5 or n. That's the standard deviation of these five values. But if we're using this to approximate a larger population that this sample came from, then we would divide by n minus 1. So, then this would be 4 in the denominator if we're trying to estimate the standard deviation of the population. So, don't get confused between these two. But it is kind of confusing because this, when we use this formula with n minus 1 in the denominator, it's called the sample standard deviation and it's denoted by lower case s. So, just remember if you're given a sample and you need to estimate the population standard deviation, you use this formula with n minus 1 in the denominator. Making the whole standard deviation slightly bigger. That's a better estimate then of the true population standard deviation sigma. But if you have a data set and you need to find the standard deviation of that data set, not approximate a population standard deviation, then you divide by n. Here's another way to look at it. Let's say you have a bag of jellybeans. There's only one licorice, but there are four strawberries and four blueberries. There's also only one cherry and two lime, or lemon, I don't know, you decide. If we take a sample of, say, four Jelly Bellies, most likely, we're not going to get the licorice one. Say we just get these in our sample. This sample doesn't show the whole range of Jelly Belly flavors that we have, including cherry and licorice. So our sample underestimates the variability in our Jelly Belly population. Hopefully, this example lends a little more insight into why we divide by n minus 1 when calculating the standard deviation of a sample. But please let's discuss it in the forums. There, we could go into a lot more depth. For the purposes of this class though, as long as you have a basic intuitive understanding of the difference between sample standard deviation and population standard deviation, then you'll be fine. You've seen how important it is to visualize data sets with histograms, in order to analyze the shape. We want to analyze the shape so that we can think critically about the mean, median and mode to describe the data set. In a skewed distribution, the mean, median and mode differ from each other. And the median might be more useful than the mean in a lot of ways. In a normal distribution The mean, median, and mode are approximately equal. What else is important about knowing the shape of a distribution? Let's explore this question with a story. I've played chess my whole life. I learned when I was four, and started competing in tournaments when I was seven. There are three things I could tell you about my chess abilities. The first is my rating in chess, which all competitive players have, my ratings 1800. Another thing I could tell you, is that among competitive American chess players, I come in 8,110th place, this is base on rating. And the third, is that I rank higher than 88% of American competitive chess players. Which one of these gives you the best sense, of how good I am at chess? For most of you, you won't know what a rating is, but if you do, pretend like you don't and you're hearing this term for the first time. So for the purposes of the quiz, if you knew nothing about chess, What would be the best metric to tell you how good I am? Well I think this one is the best metric, for determining how good I am. Because most people, when I tell them that I'm rated 1800, they don't really know what that means. Because we don't know the scale, what's the lowest rating and what's the highest rating, how many people are rated about 1800, how many people are rated 1,000? This number alone doesn't give us much information. Same with this number. We know that approximately 8,000 people are probably better than me but how many people even play chess? However, the percentage tells us a lot. And that's why the shape of a distribution is even more important than simply figuring out whether the mean, median or mode are the best metrics. Therefore, another really important thing about the shape of distributions. And what we're interested in throughout the rest of this course is the proportion of data values, less than or greater than a certain value in the data set. So if I tell you that my rating is 1800, you really don't know what that means until I tell you the shape of the distribution of ratings. And you can see the proportion less. Well, if we're interested in the proportion less than what should we do to our histogram. Use absolute frequencies or use relative frequencies? We should use relative frequencies and convert all absolute frequencies to a propotion. Let's use another example. On average, people have 190 Facebook friends. Let's say the distribution of their sample looks like this. First, convert each frequency to a relative frequency and plot the relative frequency graph. Click the radio buttons that correspond to the height of each bar if we convert to proportions. You see that the relative frequency distribution looks almost exactly the same as the absolute frequency distribution. Using the relative frequency distribution you just made, what proportion of values, or number of Facebook friends, are between 170 and 210? If you look at the histogram, the two highest bars in the center are between 170 and 210. And the proportions there are 0.237 and 0.223. So if we add them, we get 0.46. This is relatively simple, but in real life, it's often difficult to answer the questions we want. What proportion of values are between 180 and 200? You might have seen that both 180 and 200 fall in the middle of bins and so therefore, we can't tell for certain what proportion is between these two numbers. Remember, a problem with histograms which you learned in lesson two. We sacrifice detail for convenience. Because of these bins, we can't determine the proportional values less than or greater than certain numbers. We want to know things like this, though, to see how certain scores compare to other scores in the distribution. So what will allow us more detail? Adding more values to the data set, increasing the bin size, or a smaller bin size? A smaller bin size will allow us more detail. For example let's cut the bin size in half. The bin size is now 10 instead of 20 resulting in twice as many columns or intervals. So there are now twice as many numbers for which we know exactly the proportion less than or greater than those values. But still, we don't know how many values are less than any of the numbers in between the balance of each bin. For example, we wouldn't be able to tell their proportion less than 175. Ideally, we would want a smaller bin size as possible. In fact, infinitely small. But look out what happens with this data when we decrease the bin size. In other words, we increase the number of bins. Here, we have 50 intervals. Here, a 100 intervals. And notice that the frequency on the y-axis is getting smaller. Here's 200 intervals. 500 intervals. 1,000 intervals. And now, you see that the frequency is 1 for a lot of bins. 5,000 intervals. 30,000 intervals. And now at 150,000 intervals, the frequency for each bin is either 0 or 1. That's because the size of each bin is so small so there is only 1 value in many of the bins or just 0 values. So eventually, if we keep decreasing the bin size we loose the shape of our distribution. Try it yourself. I shared a histogram link that illustrates this concept really well. Use the data set of physics test scores and change the bin size to the smallest possible bin and see what happens to the frequency. So we're in a predicament. We want a small bin size to have as much detail as possible about the location of data values, relative to the rest of the distribution. But eventually, we start losing the shape of the distribution. But if we have a large bin size, then we can't tell the proportion less than any data value. We're going to handle this conundrum using a theoretical model for our distributions. This model is a smooth curve that uses relative frequencies. This is a theoretically continuous distribution that can be described with an equation. This simple feature, an equation, allows us to calculate the proportion between any two values on the x axis. We're not going to discuss or analyze this equation in this class, but we can definitely go in depth into forms if you'd like. Now what will be the area under this curve? This is a really tricky problem. Just remember that when we had a histogram, what did all the relative frequencies add to? The area under this curve, is 1. Remember that when we have the proportions, they all add to 1. Likewise, the area under the curve, is equal to the sum of all the frequencies in all the bins, which should be 1. For the most part, we're going to focus on normally distributed data. There are many kinds of normal distributions. Stretched, skinny, or maybe somewhere in the middle. But the area underneath will always be 1, or 100%. You've also seen before that in a normally distributed data set, the mean, median, and mode are approximately equal. In our theoretical model, they're exactly equal. Our theoretical model is perfectly symmetrical. This doesn't usually happen in real life. These models approximate our real distributions. But we can usually get pretty good approximations. And in our theoretical model, most of the data is in the middle clustered around the mean, median and mode. You might recall from the last lesson that approximately 68% lie within one standard deviation of the mean. And 95% fall within 2 standard deviations of the mean. The location of particular values on the X axis is often described in terms of standard deviations. Here you can see this is the mean, this is the mean plus one standard deviation, this is the mean plus two standard deviations, and likewise, this is the mean minus one standard deviation and minus two standard deviations. Whatever score we might have, we can convert it to a value equal to the number of standard deviations away from the mean. Let's call it z. By converting scores in a normal distribution to this special number z. We can know the percent less than or greater than that value. For example, if we find that a value is one standard deviation away from the mean. Then no matter what normal distribution we have. We know that approximately 84 percent are less than that value. In the next lesson, you'll learn how to calculate the proportions less than or greater than certain values in a normal distribution. What's an example in which we might want to know the proportion less than or greater than a particular number? Let's illustrate this with another story. Katie, I'm so unpopular. Don't worry, I am too. I only have 63 Facebook friends. Yeah, well I only have 54 Twitter followers. Well, the average number of Facebook friends is 190, so I have 127 friends less than average. That's a lot. Well, the average number of Twitter followers is 208. So I have 154 fewer Twitter followers. Ok. Well, let's look at the ratios. I have 33% of the average number of Facebook friends. Yeah, but even looking at it that way, I still only have 25% of the average number of Twitter followers. I am totally unpopular. I know what to do. Let's ask Chris. He has way more Facebook friends than average. Well, a better way to look at popularity, would be to look at the distributions. The distribution of facebook friends and Twitter followers are normal. The standard deviation of Twitter followers is 60, but the standard deviation of Facebook friends is only 35. So, the number of standard deviations away from the mean is definitely a better way to look at unpopularity. So, looking at these distributions how many standard deviations is my number of Facebook friends away from the mean number of Facebook friends. Remember that I have 63 facebook friends. The mean number of facebook friends is 190 and the standard deviation is 36. So how many standard deviations away am I? The distance that I am away from the mean is 127. This means that 127 divided by 36 will give us the number of standard deviations that 63 is from the mean. This is 3.53. So I am 3.53 standard deviations below the mean, in terms of the number of friends I have. What about for Andy? How many standard deviations is Andy's number of Twitter followers below the mean number of Twitter followers? Well, he's 154 followers less than average, and if the standard deviation is 60, then the number of standard deviations that fits into 154 is about 2.57. So Andy is about 2.57 standard deviations below the mean. Now if Andy only uses Twitter and I only use Facebook, can we necessarily say that Andy is more unpopular than me? Note that in this simple example, our operational definition of popularity or unpopular is the number if Facebook friends or Twitter followers, why yes or why no? No, we can't necassarily say Andy's more unpopular, even though he has less Twitter followers than I have Facebook friends. Because look at the distributions, they're both different and we can compare them by looking at them on the scale. In other words, in terms of their unique standard deviations. This is called standardizing the distributions, using 0 as our reference point. When we standardize Andy's and my individual scores, I am further from the mean. Our standardized scores show us the proportion that have a lower or higher score in that distribution. In the distribution of Facebook friends, there's a greater proportion of people with more friends than me, than the proportion of people with more Twitter followers than Andy. So basically I'm more unpopular. What formula describes what you did. To find the number of standard deviations each values from the mean. Let's go over a values X. So in the case of me, X was 63. In the case of Andy X was 54. Was it Mu minus X. Or did we do X minus Mu. X minus Mu times sigma. X minus Mu divided sigma. Mu minus X times sigma. Or did we do mu minus x divided by sigma? Now remember that in the last two quizzes you got a positive number. So we're just looking for the formula that describes the positive distance that we were from the mean. Let's go through each one of these before we find our answer. Mu minus x will give us a positive number because mu is larger than x in both cases, so that's the first thing we did. But that doesn't tell us how many standard deviations away from the mean x is, x minus mu will give us the opposite of this one. Which is basically the negative distance, x minus mu times sigma doesn't really make sense. That would be multiplying thins distance by this standard deviation, x minus mu divided by sigma is closer to what we did, but this will give us the negative distance. However, later you'll see that this equation is very important. And again, we didn't multiply by sigma. So, this is the correct answer to find just how far in terms of standard deviations x is from the mean. We found the distance which is mu minus X, and then we divide it by the standard deviation. However, we're not just concerned with how far values are from the mean, we're also concerned with whether or not they're below the mean or above the mean. When you standardize any score on the x-axis, we get the z score. We already called it z before. And we're always going to subtract the mean from the x value, and then divide by the standard deviation. So that way if we have a value less than the mean, we're going to get a negative z score. The z score is basically the number of standard deviations any value is away from the mean. Therefore, we can convert any value in a normal distribution to a z score. When we do this, we standardize the distribution. We can start with any normal distribution, and then standardize it. So let's again, refer back to our normal distribution. The mean is 190, this is the actual mean of Facebook friends I've looked at up, but let's pretend that this standard deviation is 36 like it was in our example were Andy and I were arguing our unpopularity. So using this information click the link that we share and tell us how many Facebook friends you have And then calculate your z score. Later we're going to, to analyze that data and see if we get a normal distribution. So we'll be able to tell if some z score values don't match up. Do your best when you calculate your z score because it's good practice, and we're going to use z scores the rest of the class. If you don't have Facebook just enter 0, but be careful. This does not mean that your z score is going to be 0. You'll have to calculate that and figure out what it is. And I will know if you didn't calculate it correctly. Just to get you thinking about z-scores. What does a negative z-score mean? Does that mean the original value is negative? The original value is less than the mean? The original value is less than zero? Or, the original value minus the mean is negative? Check all that apply. Since z equals the original value minus the mean, divided by standard deviation. In other words, the number of standard deviations that the original value is from the mean. Then a negative z score means that, the mean is greater than x. That must mean that x is somewhere down here. So that doesn't necessarily mean that the original value is negative, though it could be. It does mean that the original value is less than the mean. The original value is less than zero, it basically says the same thing as this first one, that the original value is negative. In this one, the original value x minus the mean is negative, must also be true. Because for the z score to be negative, then this is going to be negative, because the standard deviation is always positive. Just to review, remember when we calculated standard deviation, we squared all the deviations. So all the deviations are then positive, because if you square anything negative or positive, you get a positive number. And then we just took the average squared deviation, which is still positive, and then we took the square root, which is still positive. So remember that this standard deviation will always be positive. Which means that if the z score is negative, then this numerator must be negative. And that must mean that x is less than mu. Here's another quiz. If we standardize a distribution by converting every value to a z-score, what will be the new mean of this standardized distribution? This is a pretty difficult question, so if you get it, then you should definitely brag about it in the forums. As a hint, just think again about how we calculate z-scores. Any value on the axis minus the mean divided by the standard deviation. And think also, in the previous videos, we showed how we take any normal distribution and then we standardize it. So you might want to replay those videos, if you don't know the answer right now, because those might help you figure out what the new mean will be. So remember, we're taking this distribution. We're shifting it all the way over to zero because we're subtracting the mean. So basically, if we have a normal distribution out here with a mean of 100, if we subtract the mean, we shift this distribution to the left 100. And then, the new mean is 0. What if we add a distribution with a mean of, say negative 30? If we subtract negative 30, then we're essentially adding 30. And we shift the distribution right, again, centering at 0. Another way to look at it is, what's going to be the new z-score of the mean? Let's say that the mean is x. The z-score of the mean is basically how many standard deviations away from the mean is the mean. Well, that's 0 instead of x, we have mu. Mu minus mu divided by standard deviation is 0 divided by the standard deviation, which is just 0. So, that's another way to look at it. And now here's one more tough conceptual question. After we standardize this distribution, what will be the new standard deviation of this standardized distribution? Now remember, when we calculate the z score of any value in the distribution, we first subtract the mean which shifts the distribution without changing the shape so that zero is now the mean. And when we divide by the standard deviation, we then change the shape. Let's look at it this way. We have any distribution, with mean, mu, and standard deviation, sigma. Which basically means that sigma is one standard deviation away from the mean. After we standardize this distribution, what is going to be the z-score of sigma? Well, remember when we subtract mu, we shift it so that mu is now zero. So now, the z-score of sigma is going to be sigma minus zero divided by sigma, which is sigma divided by sigma, which is 1. So, the z-score of any value, that's one standard deviation away from the mean, will then be 1 after we standardize it. Which means that the new standard deviation of this normalized distribution, or standard distribution, is 1. Just to recap, when we have any normal distribution, we can standardize it by first subtracting the mean, shifting it to 0. And then dividing by the standard deviation, which makes the standard deviation 1. This is called the standard normal distribution with mean 0 and standard deviation 1. So here, the z score is going to be negative 1, and two standard deviations away, a negative 2. So now, every value in the data set is written in terms of the standard deviations it is from the mean. Let's say that Chris is super popular. The number of Facebook friends he has is 2.5 standard deviations above the mean. So, in other words, he has more friends than about 99% of people. If the true standard deviation of our original data set is still 36 and the original mean is still 190, then how many Facebook friends does Chris have? So here's Chris, with more friends than approximately 99% of people, 2.5 standard deviations above the mean. Well if the standard deviation is 36, then what's 2.5 standard deviations? 2.5 Standard deviations, is just 36 times 2.5. Which is 90, so Chris has 90 more friends than the mean, and the mean is 190. So 190 plus 90 is 280. So Chris has 280 Facebook friends. Another way to look at it is to use the equation. You know that the z score, which for Chris is 2.5, is the original value Minus the mean divide by the standard deviation. So if we plug in the values that we know, 2 point 5 which is z is x, his number of Facebook friends, minus the mean divided by the standard deviation. And if you solve algebraically by cross multiplying and then adding 190, you get the number of friends that Chris has, denoted by x. Which is again, 280. So this time, instead of converting a value to its z score, we converted the z score to the real world value. Actually, we can create any normal distribution with any normal distribution. Sorry if that sounds confusing. Basically, we can take any normal distribution, convert it to the standard normal distribution and then scale it any way we want. So, for example, let's say we want to give everyone a popularity score from 0 to a 100, with 50 being the mean. And let's say we want the standard deviation to be 10. We start with our original Facebook distribution, where the mean is 190 and the standard deviation is 36. So let's say that someone has 210 Facebook friends. Let's first convert this to a Z score. What's that going to be and where does it fall in this distribution? So if we convert to the z score, we get 210 minus 190 divided by 36. So the number of standard deviations away that 210 is from the mean, which is 190, and we get about 0.56. That is about here. So now we want to convert it to a new popularity score with mean 50 and standard deviation ten. So try doing this yourself. What's going to be this person's popularity score if they have 210 Facebook friends? And remember again, our operational definition of popularity is the number of Facebook friends. Though I'm sure this isn't necessarily true. Well, we have that the z-score is 0.56 and we want to find out what value corresponds to this in a normal distribution with mean of 50 and a standard deviation of 10. So here, we have our setup, so all we do is multiply 0.56 by 10 and then add 50. So this gives us 55.6, so basically, what we did is we took a pretty spread out distribution with mean 190 and standard deviation 36. We converted it to the standard normal distribution with mean of 0 and standard deviation 1. And then, we converted that to a normal distribution with mean of 50 and standard deviation 10. So the score for each of these distributions corresponds with the same percentage less than that score. So a score of 210, here, the proportion less is exactly the same as a score of 0.56 here. And that's exactly the same as the proportion less than a score of 55.6 in this distribution. Hopefully, that makes sense, and if it doesn't, don't worry. It's going to make more and more sense as you get used to working with normal distribution. And in the next lesson, you're going to learn how to calculate the percentages less than or greater than certain values on the x-axis. Good job. You've seen that in the normal distribution, by knowing where on the x axis the value falls, in terms of standard deviation, we can determine the percentage less than or greater than any value. In this lesson you are going to calculate these percentages. Remember that we used theoretical curve to model the data and the area under this curve is one, that's because it models with relative frequency of a distribution of data in terms of proportions. This curve is called the probability density function which we often abbreviate PDF. Now why is this called the probability density function? Well, let's say we're taking a random sample of Udacity students. Here's a histogram representing Udacity students' ages, and the sample size is 7,901. This is actual data. Let's model it with this probability density function. Now remember when we model it, the absolute frequency turns into relative frequency, and here I've put probability and you'll see why. We know from our sample that about 70% of Udacity students are aged 30 or less. Now, let's say we randomly choose any Udacity student by giving each one of you, and all other Udacity students, a unique number. And then we randomly pick a number. Then what's the probability that the person that we choose will be age 30 or less. Write your answer as a proportion. We know that 70% of Udacity students are aged 30 or less. That means that the probability of selecting a random student aged 30 or less is 70% or 0.7. That's why this is called the probability density function. The area under this curve represents the probability. See how we can visually model statistical concepts? It's so cool! We can do the same with normal distributions, which are modeled by a special probability density function. We're not going to go over the equation for this probability density function in this course. But if you want, you can easily look it up and see what it is. And that might be pretty cool for some of you who want a little more information. But basically, since we have this theoretical curve, we can model it with an equation. And using this equation, we can use Calculus with a curve. But, we don't need to use Calculus, because someone else already did. And then, they created a special table, so we can always figure out the area under the curve between any two value. We're going to use this table later. First, let's make sure we're all up to speed on the normal probability density function and the area underneath. First, the tails never actually touch the x-axis, they get closer and closer to the x-axis. So, the x-axis is a horizontal asymptote. The reason the tails of this thoretical model don't touch the x-axis is basically because we can never be 100% sure of anything. In other words, we could have a value way out here, really far from the mean, like five standard deviations away. But the probability of getting this value or lower is very small. And it's equal to the area under the curve. So, if we could zoom in, we would see this tail get closer and closer to the x-axis but never touching. And then, the area in between the tail and the x-axis, all the way to negative infinity is the probability of getting this value or lower. We'll go more into depth in that in a second. And similarly, we could get a value way out here. But the probability is very small. So basically, what you have to remember is that if we have a certain value, let's just call it x for now. That the area under the curve from negative infinity to x is equal to the probability of randomly selecting a subject in our sample less than x. And this is equal to the proportion in the sample or population with scores less than x. If this probability is 80%, then we say x is the 80th percentile. If this probability is 90%, we say x is the 90th percentile, etc. If this is a little confusing, don't worry. That's the whole point of this lesson. You're going to get really comfortable with using the probability density functions and analyzing this area, and finding this area. Now, what's the probability, for example, of randomly selecting a subject that is greater than five standard deviations above the mean? Would the probability be approximately 0.01? And notice that these are in proportions 0.3, 0.8, or 0.99. Well, this is so far above the mean that it has a very low probability. Probably even less than 0.01 or 1%, but this is the closest. What's the probability of randomly selecting a subject that is less than 5 standard deviations above the mean? 0.01, 0.3, 0.8, or 0.99. Now, the probability is almost certain that we're going to select a subject less than this number. In fact, according to our relative frequency distribution, most of our values are going to be centered around the mean. And remember, that's the case in a normal distribution. In other distributions it might be different. Therefore the probability is about .99 or 99%. Remember that we can have all sorts of normal distributions wide or skinny but the total area underneath the density curve will always be one and for normal distributions approximately 68% is within one center deviation of the mean and 95% is within two standard deviations of the mean. So, what proportion of data values is either below two standard deviations or above two standard deviations from the mean? And this is for a normal distribution. Write your answer as a proportion. Remember that the whole area under the curve in terms of proportions is 1. And therefore, if 95% or 0.95 is between negative 2 and 2. Remember, these are the z scores. And same with this skinny normal distribution. 95% is between two standard deviations below, and two standard deviations above the mean. So, that must mean that the remaining proportions here, are 1 minus .095, which is 0.05. So, a proportion of 0.05, or 5%, is either below 2 or above 2 standard deviations. That means then that since this data is symmetrical, 2.5% is on this tail and 2.5% is on this tail, same here. You don't have to memorize this. If you know that 68% is within one standard deviation, and 95% is within two, you can always know the percentage, or the proportion in between any two integer standard deviations for the mean. We're going to practice that now. Lets refer back to our distribution of Facebook friends from the last lesson. Assuming that the distribution is normal and the average person has 190 Facebook friends, and the standard deviation is 36 Facebook friends, then approximately what proportion of people have less than 154 Facebook friends? 154 is one standard diviation below the mean. So we're trying to find out what proportion of people are in this green region. Well, let's start with what we know. We know that 34% is between one standard devaition below the mean and the mean. And 34% is between the mean and one standard deviation above the mean. The mean because that makes 68% here. And we also know that 95% is between two center deviations below the mean and two above the mean, 95 minus 68 is 27 so 27% is distributed between this area and this area meaning that 13.5% is here. And 13.5% is here, And you already found that, that leaves 5% to be distributed between the two tails, below two center deviations below the mean and above two center deviations above the mean. So we know here, this is 2.5% and this is 2.5%. Now, we pretty much have our answer. 13.5% plus 2.5% is 16%, so that's a proportion of 0.16. Okay, what about the percentage of people who have more than 262 Facebook friends. Fill in the number that should go here. Well we know that 5% is either less than 2 standard deviations below the mean or greater than 2 standard deviations above the mean. And since the distribution is symmetrical then that means 2.5% is here and 2.5% is here. So our answer is 2.5% since 262 is the value 2 standard deviations above the mean. Let's take another quiz. Approximately what proportion of people have between 118 and 226 Facebook friends? So we're looking for the proportion between 118 and 226. And using this same logic as before, we know 68 % is here, within one standard deviation, and 13.5 % is here. Just between these two standard deviations. So the proportion is 0.815. Sorry if I keep bouncing around between proportions and percentages. I just find it easier to explain using percentages, but I'd like you to put your calculations in terms of proportions because that's generally the way in which we write probabilities and also later in this lesson we are going to use a tool that only uses proportions. So, that's why I am asking you to calculate it in terms of proportions. What about less than 240 Facebook friends? Put in approximate proportion. Well, now I'm being tricky because 240 is in here and that's not going to be an integer standard deviation away from the mean. It's like 1.5 standard deviations from the mean approximately. So then, how are we suppose to calculate the area under this curve between negative infinity and this non-integer number standard deviation Deviations from the mean. Well, we can guess that it'll be between approximately 84 percent and 97.5 percent. 'Because we know that this is 68 percent between here and here. 13.5 percent here, and 2.5 % here, so there'll be approximately 84 % of values between negative infinity and 226. Another way to look at it is since it's symmetrical, you know that 50% is less than the mean, and you know that 34 % is between the mean and one standard deviation. So 84 % have less than 226 Facebook friends. If we add this section which is 13.5 % then we know that 97.5 % Are between negative infinity and 262. So, lets' call the proportion p. We know that p has to be greater than 0.84, and less than 0.975. But we're going to use a tool that will help us figure out, for any value, what proportion will this be. Remember I said earlier that if we have the equation of the probability density function we can use calculus to find the area under the curve between any two values or between negative infinity in any value. Mathematicians have put these values in a table. Here is the top of this table, it gives you a little diagram saying that when we have the z score, the values in this table tell us the proportion less than that z score, in the standard normal curve. So this table is for the standard normal distribution, and remember that's when the mean is zero and the standard deviation is one. This way, after standardizing any normal distribution. In other words after calculating the z scores for the values that we're interested in, we can use this table to approximate the proportion less than that particular z square. Here, the tenths place runs vertically and the hundredths place runs horizontally. So if we have of a z score of say, negative 2.75. Remember, that would then be less than the mean. Negative 2.7 is here, and 0.05 is here. So if we follow over, these cells meet at .003. So the proportion less than negative 2.75 is .003. Or .3%. So again that's the probability that we get less than negative 2.75. And that's the proportion of objects in the sample that have a score less than negative 2.75. There's a link to the z table that we shared and so use that link when you use the z table which you are going to use in the next quiz. So now I'm going to give you a touigh question. Which ironically is the same exact question I asked you before. But this time use the Z table to approximate the proportion of people who have less than 240 Facebook friends. So this might be tricky, but spend some time on it, and I think you can do it. And remember, whenever you are doing math, think about it logically. 240 is about here. So should your z score be negative or positive? Should it be between zero and one, or between one and two? You may have figured out we first need to convert 240 to the z score. In other words we want to find out how many standard deviations away from the mean, 190, 240 is. So it's going to be one point something. So first we take the distance between 190 and 240, and then we divide by the standard deviation, to find out how many standard deviations fit into that distance. So we get about 1.39. We have to round to the hundredths place, because the z table we have doesn't get more precise than that. So our z score is 1.39, so let's bring up the table. First, we find 1.3. And we find 0.09, which is the last column. And we find that the area, less than a standard deviation of 1.39 is 0.9177. So this means that approximately 91.77% of people will have less than 240 Facebook friends. And remember, this is all assuming that this is a normal distribution with mean 190 and standard deviation 36. If your a Udacity student you can earn karma points by posting questions in the forums or answering others questions. If someone likes your question or your answer you can earn 15 karma points. If the person who asked a question accepts your answer you can earn 25 karma points. And if you accept some of the answer you can earn 5 points. However, you can also lose karma points by being downloaded by having a flagged post or if your flagged post is deleted. Most of these students are pretty awesome, see they are all in the positives and if you have a certain number of karma points you can even edit other people's posts or delete others posts. The karma point system is explained Udacity FAQ section. I put the link at the bottom. Well, I put together a data set for you guys to analyze, which is the average number of Karma points per post. This data set has 200 students. Click the link I shared to access this data. We're going to start the same way we would when given any data set. What's the mean number of Karma points per post? If you can't remember how to calculate this using a spreadsheet, feel free to replay some of the videos from lesson three. Here's the spreadsheet with the karma points per post. If we go to the bottom, you see that we have a sample of size 200. So in any of the white space we can write equals average a 1 colon a 200. You should have gotten that your average is about 12.96. If it rounded up to the nearest integer value then that's fine. Now the next thing we're going to do is calculate this standard deviation. But this time, since we're trying to estimate population parameters, you're going to use the formula for the sample standard deviation. Remember that this is just like the population standard deviation except we divide by n minus 1. This is Becel's correction to make the standard deviation a little bit bigger, since the standard deviation of the sample tends to underestimate the variability. So go ahead and try and calculate the sample standard deviation. And this will be a good review for you. This one probably took you a little longer to figure out, and that's okay. So, you remember that the first thing we have to do is find the deviation from the mean for each of these values. So, let's do it in cell B1. We found that the mean was approximately 12.96. So, I'll write equals A1 minus 12.96. And that will give us the deviation of the first value from the mean. Now, we'll copy this so that we don't have to write it in every cell. And then, if you go to cell A1 and push Cmd+Down, at least on a Mac keyboard, and then we'll go to cell B200 and then push Cmd+Shift+Up Arrow key, then you should have the whole column B highlighted. Now, write Cmd+V and then you have all of the deviations for each of those values. Something else that you may have done in which I taught you before, was going to this corner until you see the plus and then dragging it down. And if you did that, that's perfectly fine. These are just some keyboard shortcuts that make our life a little easier. So, in column C now, let's square each of the deviations, so we'll write equals B1 carrot 2, which means squared, and then again, we'll Copy cell C1, go to cell B1, push Cmd+Down Arrow key, to go to the bottom. Go to cell C200, push Cmd+Shift+Up Arrow key, so that we have all of column C highlighted, and then, Cmd+V. And now, we have all of our squared deviations. So, what's the next thing we do? We have to sum them up. So, let's do that in cell D1, equals sum of C1 colon C200. So, we added up our score deviations and now this time, and this is where the sample standard deviation differs from the population standard deviation, we'll divide this by n minus 1, which is 199 equals D1 divided by 199. And then, our last step is to take the square root, equal sqrt of D2. So, we get that our sample standard deviation is approximately 4.76, and this is our best estimate of the population standard deviation. So now we have that our population parameters are about 13 for the mean and 4.8 for the standard deviation. And remember the units are average karma points per post. Using these rounded values, let's draw our distribution. We're going to say that it's normal so here's our theoretical probability density function. And here where the mean, equals the median, equals the mode, we have 13. Let's say here's one standard deviation above the mean, and here's standard deviation less than the mean. And here's two standard deviations. If our standard deviation is 4.8, then what should our values be here, here, here, and here? Well, since this value is one standard deviation above the mean, 13 plus 4.8 is 17.8. If we add another 4.8, we get 22.6. And similarly, if we subtract 4.8 from 13, we get 8.2. And then subtract another 4.8 to get 3.4. So here's our normal, unstandardized distribution. So now, let's answer some questions about this distribution. What proportion of students have gotten less than five points per post on average? Use the Z table to figure this out. First of all, where does 5 fall on our distribution? Here, in between 3.4 and 8.2. How many standard deviations below the mean is that? Looks like about 1.5 or somewhere in between 1 and 2. So that's important. To keep in mind when we use the z table. So what we need to do first is figure out exactly how many standard deviations 5 is from 13. How far is 5 from 13? It's 8 away. And how many standard deviations of 4.8 fit into 8? This is negative 1.67 if we round to the nearest hundredths place. And remember, it's negative because 5 is less than the mean. So now we can bring up our z table. So here's the z table. And I shared it with you in the link at the bottom. So we have negative 1.6 here. And for negative 1.67, we find where this row and this column, intersect. And that's here at 0.0475. So if we look at our distrubution, then the proprotion less than 5 is 0.0475. That means that a little less than 5% of students have gotten less than 5 points per post on average. Here's another one. What proportion of students have gotten more than 20 points per post on average? This question's a little trickier than the last one. Here's a hint, just remember that when you use the Z table it gives you the proportion less than a certain value, not the proportion greater. If you get this, I'm going to be super happy. Just like we did last time, let's find where 20 falls on this distribution. It's here between 7.8, 1 standard deviation away from the mean, and 22.6, so the z score will be somewhere between 1 and 2. Let's find out what it is exactly. So we find the distance from the mean, which is now going to be positive and divide by the standard deviation. We get that 20 is 1.46 standard deviations away from the mean. Now, let's bring up our z table. So like I said before, if we have our z square, we'll find the proportion less than that z square. So let's start with that. The proportion less than 1.46 is .9279, here at 1.4. And then the top of this column is 0.06. So if the proportion less than 20 is 0.9279, then the area above it must be 1 minus 0.9279. Because remember, the total area under the curve will always be 1. This is .0721, so about 7.21 percent of students have gotten more than 20 points per post on average. If you've gotten more than 20 points per post on average, then you're in the 93rd percentile of students, congratulations. Before we move on, I'm going to show you one more way to look at this. Since the data is. Symmetrical. Then not means that amount greater than 1.46 is equal to the amount less than negative 1.46. That means that we don't necessarily have to calculate the proportion less than 1.46 and then subtract it from 1. We could just find the proportion less than negative 1.46. And we find that the proportion less than negative 1.46 is the same thing that we calculated before, 0.0721. What about the proportion in between scores? What proportion of students have gotten between 10 and 16 points per post on average? Again, you'll use the same techniques that we've used in the last two quizzes. So let's start again by looking at where ten and 16 fall in the distribution. Ten is three points less than the mean, and 16 is three points greater than the mean. So, if this is the mean, 13, this is ten, and here is 16. We're looking for the area in between those two. How might we figure this out? Well, what we'll do is, we'll find the area under 16, and then we'll subtract the area less than ten. So what's the area less than 16? First, let's convert 16 to a z-score, 16 minus 13 divided by 4.8. So this is 0.625. Since ten is equidistant from 13 as 16 is, then the z-score for 10 is going to be negative 0.625. You can go ahead and calculate that if you want. Sometimes that helps things make more sense. So let's find the area less than a z-score of 0.625. We'll find 0.6 in the vertical column and we'll have to round to the hundredths place, since that's as detailed as the z-table gets. So we'll round to 0.63, so that's this column and we should get 0.7357. And then, we'll subtract the area less than ten, which has a z-score of negative 0.625. So here is negative 0.6 and here is the column for 0.03, so the area less than negative 0.63 is 0.2643. Notice that since these are the same values, just one is positive and one is negative, then the area less than 0.625 plus the are less than 0.625 adds to one. So basically, this area here, plus this area here equals one. That's because the area here is equal to the area here. So, if you add 0.7357 and 0.2643, you should get one. So that's just a property of the symmetry of the normal distribution. So if we subtract them, we get 0.4714, and that's the area here in between 10 and 16. Now we're going to ask one more quiz question to finish off this lesson. How many karma points per post on average would you need to be in the top 5% of Udacity students? This one's a really tricky one. And it requires going backwards. So instead of trying to find the proportion less than or greater than a certain value, we're trying to find what value will give us this proportion. Try and figure it out yourself. This might take you some time. But I think you can do it. So here's our distribution. And we want to find what value is here such that the percent here is 5. That's the same as a proportion of 0.05. Well, first, what z-score could go here? We'll use the z-table to figure that out. This time we're going to look in the center of the z table, and we're looking for 0.05. Here we have 0.0505 and 0.495. That corresponds to a z score of negative 1.64 and negative 1.65. Since 0.05 is right in the middle of them, then let's use the z-score that's right in the middle of those. So negative 1.645. Note that we found the z-score that corresponds to 0.05 being less than that. So this z-score is just the positive value of this. Positive 1.645. Since the normal distribution is symmetric, then 0.05 would be greater than the positive of that z-score. So this is the z-score that we want. And now let's convert this to karma points. 1.645 equals the number of karma points minus the mean. Divided by the standard deviation. And now we can solve for x by multiplying 1.645 by 4.8, and adding 13. This is about 20.9. So if you've got a non average, 20.9 karma points per post. Then you've gotten more karma points per post than 95% of students. Great job! This is really tough stuff and as long as you understand the properties of the normal distribution and you understand what we're doing rather than memorizing it you'll be fine. Below I'll provide a link to an app the allows you to visualize the area under the curve less than any Z score. So check it out if you want, and that might help better understand this whole process. But you'll get better and better at it as we go through this class. Good job! Now you've seen that by knowing the mean and standard deviation of a normally distributed population, we can compare any value in that population to the rest of the population by determining the percent less than or the percent greater than that value. But what about for a sample, how can we compare a particular sample in a population to other samples in that population? By finding the mean of this sample, by finding the means of other samples in this population, or maybe by comparing the mean of this sample to the other samples. Check all that apply. The correct answer is all of them. You saw that measures of center can describe a group of data. Well if we're trying to compare samples, we can use the measures of center of that sample, and specifically the mean, to compare them. Let's go more into this. Let's take a simple example that will be analogous to having a population with samples. Let's say you're gambling in Vegas. You're playing a gambling game where you roll a tetrahedral die. You roll the die twice, and then, take the average of the two rolls. To win, the average has to be at least three. What's the probability that the average of your two rolls will be at least three? Just take a guess. To help us really understand this example let's make a tetrahedral die. There, I have a tetrahedral dice. So this vertex is number 2. This vertex here is number 1. Here's number 4. And here's number 3. The numbers 1, 2, 3, 4 are analogous to our population. So when we roll the die, we can get either a 1, 2, 3, or 4. What's the mean. Well, 1 plus 2 plus 3 plus 4 is 10. And 10 divided by 4 is 2.5. So the mean is 2.5, represented by Mu. This is called the expected value. Even though we don't really expect to get 2.5, because it's not even an option. But since that's the mean of our population, we expect to get somewhere around 2.5 if we take a sample from this population. So just to remind you, we want to get at least an average of 3 if we're going to win in our Vegas gambling game. So let's look at all the possible outcomes we could get if we roll the die twice. We could get 1 and then 2, so the mean is 1.5. Or we could get 1 and 1, so the mean is 1. Or we could get 3 and then 4, so the mean is 3.5. There are a lot of different possibilities. These are analogous to our samples within the population. Out of our three rolls, we only won one time, because we only got one average that's 3 or greater. So how many total possibilities, or samples of size 2, can we select from this population? Give it your best guess. We can have 16 samples of size two. We could get any of these or then two and then one. So here are all the samples we can possibly get from this population. So now go ahead and find the mean of each example. So if we take the mean of each sample. For the first sample mean, we get 1. For the second mean, we get 1.5. And for the third sample mean, we get 2, and etcetera. So now, what's the mean of the sample means? In other words, if we roll the tetrahedral die twice, what do we expect to get for the average roll? That'll be the mean of the sample means. Well, if you add all of these up, 1 plus 1.5 plus 2 plus 2.5, all the way to 4. You get that the mean of the sample means is 2.5. Let's symbolize it by large M. Now, copy and paste the sample means from the instructor notes to the link that's also in instructor notes. This is Wolfram Alpha, and it's a really cool site. Enter those values here and click this icon. This will analyze all the data that you put in here. In our case, that's the mean of all possible samples of size 2. Look at the histogram that visualizes the frequency of means. This visualizes the distribution of the sample means, and this is called the sampling distribution. What's the shape of this sampling distribution? Uniform, bimodal, normal, or skewed? It's normal and that's not just a coincidence. So here you can see that the mean with the highest frequency is 2.5. This histogram might be a little confusing since it's discrete. But if you look back at your sample means, you see that you get four samples that have a mean of 2.5. Ideally, 2.5 would be right here in the center. But you see that it's more rare to get a sample mean of 1 or 4. So, what's the probability then of having the average of your two rolls be three or more? Write your answer as a proportion. Well you see that there are 3 samples of size 2 that will have an average of 3, and 2 samples of size 2 that will have an average of 3.5, and 1 sample of size 2 that will have an average of 4. So 3 plus 2 plus 1 divided by the total number of possible means, which is 16, will give us the probability and this is .375. We can easily calculate the probability for discrete samples like this one in a discrete population, but what about in real life, with huge populations? We can't possibly calculate the mean of every possible sample of size n and even if we could, we wouldn't want to. We ended up with 16 samples when we just had a population of size 4. What if we had a population of 350 million? Which is often the case. Well, you've already seen that the distribution of sample means is normal. And we know that the mean of all these sample means is the population mean. Remember, the population mean here was 2.5. And the mean of all possible sample means is also 2.5. So, if we have a random sample and we take the mean of it, how can we know where it falls on this distribution of sample means. We're missing one important thing. Do we need to know the total number in the population? The standard deviation of the distribution of sample means? Which is this here. Or, the total number of possible samples? Hopefully, you've remembered from the last lessons, that in order to compare where a value falls on a distribution, we need to know the mean and the standard deviation of that distribution. Okay. So, for a population of size 350 million, how would we find the standard deviation of the distribution of sample means? Well, let's start with what we know from our simple example so far. Let's calculate the population standard deviation sigma, and then, let's also calculate the standard deviation of all our sample means, let's call this SE. All the sample means are also written at the bottom and in iInstructor notes if you want to refer to those. So calculate the standard deviation of this population and the standard deviation of all the sample means, where the sample is size 2. How about we do this in a spreadsheet. So first let's find the population standard deviation. We already know that the mean is 2.5. So we'll say, equals A1 minus 2.5, and we'll square it. So those are the squared deviations. Command c, command down, command shift up, highlighting these four cells. Command v and we have all of our squared deviations. Now to take the standard deviation we take the average of cell B1:B4. So that's the variance. And then we take the square root of the variance to get the standard deviation. So we get 1.118 approximately. Okay, now let's do the same thing with our sample means. So we already know that the mean of all our sample means is 2.5. So here we'll write equals A6 minus 2.5 squared. So we get our first squared deviation. And now we'll go to that cell, push command c, go to cell A6, command down, go to cell B21, command shift up, highlighting all these cells and then command v. And now we get all of our squared deviations, now we'll find the average squared deviation or the variants. B6 through B21. And then we'll take the square root, to find the standard deviation. So the standard deviation of our sampling distribution, or the distribution of sample means, of size n equals 2, is about 0.79. We're going to call this SE. So now we have our population parameters and our parameters for the distribution of sample means. Do you think that this standard deviation for the population and the standard deviation for the distribution of sample means have any relationship to each other? Actually they do, and let's see how. Well, I'm going to show you something really awesome. This is going to change your life. First, tell me this. What is the ratio of the population standard deviation sigma to the standard deviation of all the sample means? In other words, what's sigma divided by SE and do you know what this number is? After you calculate it, really think about it, and try and figure out this question. Well lets bring back our spreadsheet. Here we'll write equals c 2, which is sigma divided by c 7, and we get 1.414. So that's the ratio between them and now what is this number? Do you recognize it? I wouldn't be suprised if you didn't, but this number is the square root of 2 and what was 2, 2 was our sample size. Is that not totally cool? I would pause this video now and think about this a little bit, because it's definitely a very difficult conceptual idea. So now let's recap. You just saw that the population standard deviation, divided by the standard deviation of the distribution of sample means, also known as the sampling distribution, is equal to the square root of the sample size. And in this case, the sample size was two, cause we rolled the tetrahedral die. Two times, so if we know this, the what is SE? The standard deviation of our distribution of sample means. Is SE sigma, square root n, sigma squared divided by n or sigma divided by square root n. Well, let's multiply both sides of the equation by the standard error. So then on this side of the equation, standard error cancels out. And then let's multiply both sides of the equation by 1 over root n. So over here, the root n divided by root n, cancels out. So on this side, we're left with sigma divided by root n, and on this side we're left with SE. So we get that SE is sigma divided by root n. Now remember what we are originally trying to find. We're trying to find where on the distribution of sample means a particular sample will lie. Not just for a simple population like this one, but for a huge population. And now we can do that. Because now we know that the distribution of means, where every mean is the mean of a sample of size n. This distribution has a standard deviation equal to the population standard deviation divided by the square root of n. This is called the central limit theorem. And it not only holds true for these simple populations but for any population. Because of the central limit theorem, we can have a population of any shape. And then let's say we draw a sample from it and calculate the mean, and then we draw another sample from it and calculate the mean. And we keep doing this, say a 100 times. Assuming the sample size is large enough, if we plot the distribution of means, we're going to get something that's relatively normal. With a standard deviation equal to the population standard deviation divided by the square root of the sample size. And we've been calling it SE so far. And that's because this is called the standard error. This is super cool, but I also understand that it's also super complicated. So we're going to go through a few more ways of looking at this, using applets and demonstrations. And then finally at the end of this lesson, go over an example where we would actually use this in real life. Let's first play around with this really cool applet. Click the link below to open it, you need to have Java installed on your computer to view it. Now, what happens if you roll one die at least 100 times. To get a feel for it, you can try rolling one die, one time. Try rolling it again, and then, you'll see the distribution start to build up. Then, you can change this part here, so that you're rolling one die, 100 times and it'll plot all of the values that the die lands on, or the simulated die. So, what happens you roll it at least 100 times? What does the distribution look like? Well let's bring it up, lets roll one dice one time, we get a five, let's roll it again, we got a three. You can keep rolling it and we see what we get, we don't want to do this 100 times, so let's just change this and select 100. Now we'll roll one dice, 100 times. So by chance, by simulation, we get a lot more 6s than 5s. Let's try it again. Our distribution is looking pretty uniform. Let's try a thousand. Here it looks even more uniform, so the more you roll the die. One two three four five and six all have an equal chance of getting selected and so our distribution is uniform. In this case our sample size was one, so we don't really have sample means right now. So essentially, the standard error is just the population standard deviation divided by the square root of 1, which is just the population standard deviation. So when we're taking samples of size 1, we're just drawing members of the actual population. And so then when we graph our distribution of sample means then we're really just graphing our actual population so we'll get a distribution that's approximately equal to the distribution of the population, which is uniform since we just have 1, 2, 3, 4, 5, and 6. Well, what happens when we roll two dice at least 100 times, and then we took the average of the two rolls? This is similar to what we did with the tetrahedral die. So if we calculate at least 100 averages from rolling two dice, what's going to be the distribution of these averages? If you use the app and then you select two dice, you might notice that this scale goes from two to twelve and that's because this app sums up the two rolls. But this is still the same concept because when you take the average, you sum up the two rolls and then you just divide by two. So, the distribution will look exactly the same pretty much as what is shown here. Except the scale will just be these numbers divided by two. But the important thing is that the shape of the distribution will be the same. So let's roll 2 dice once. So we get that the sum is 9, which means that the average would be 4.5. If we roll again we would get that the average is 3 and the sum is 6. And we can keep doing this a number of times and you see that the more we roll the dice and plot the distribution of the sums or the averages, it looks more and more normal. Play around with this applet because it's a really good learning tool. Now it's time for a tricky question. What are the mean and standard deviation of this sampling distribution? As a hint, remember what the population was. So in other words, if we take all possible samples of size 2 from this population, and then plotted the mean of each sample, which is this distributionm what will be the standard deviation? Well first we need to find the population standard deviation, which is about 1.7078. And remember, our sample size is 2. So according to the center limit theorem, this standard deviation of this distribution Is sigma divided by route N which is one point seven zero seven eight, divided by the square of two and this is about one point two zero seven six. It's okay if you round it. And you can even find that yourself by brute force since the population only consists of six values. So you could take all samples, one, one, one two, one three, one four, one five, one six, two one, two, two, two three, two four, blah, blah, blah and then you can find the mean of each sample of size two and then find the standard deviation. Of that data set, consisting of all the means, and you'll get this number. But, we already know, because of the central limit theorem, we can find this number just by dividing the population center deviation by root n. Okay, now what if we rolled five dice and then plotted the average of the rolls? And then repeated this at least 100 times. So here, let's change it to five dice, and you can pick 100, or 1000, or 10,000 Anything greater than 100. Do you think this distribution of new sample means with n equals 5 be skinnier or wider than this distribution when n equals 2? You can take a guess or you can use the applet to see what happens. You might have guessed that it's going to be skinnier, because if n increases from 2 to 5, the denominator of this standard deviation of the distribution is going to be bigger. Meaning that this standard deviation or this standard error is going to be smaller, that means that the distribution of sample mean is going to be skinnier. But it's also great if you use the applet, because that's a really good visualization tool. Let's do that. So we'll roll five dice, and then take the average of all the numbers that show up at the top. And then we'll do this at least 100 times. Let's do it a 1000 times. And you see we have a skinnier normal distribution. Again let's compare this to if we only rolled two dice. Then we get a much wider distribution. So now, what's the standard error of this distribution if we could take all possible samples of size 5 from this population? What will be the standard deviation of this sampling distribution? Well, you already calculated the population standard deviation. And the standard deviation of this sampling distribution will be the population standard deviation, divided by root n, which is 1.7078, divided by the square root of 5 this time, which is .76. This is less than the standard error we got before. So let's take a quiz to make sure you really understand how the size of the samples change the shape of the distribution. As the sample size increases, does the standard error increase or decrease? As the sample size increases, we have a more accurate representation of our population parameters. In general, we want bigger samples. And since the standard error tells us the interval in which the population mean is most likely going to lie, the standard error decreases. It's okay if this is somewhat confusing right now. I promise that you're going to get it. So, as n gets bigger, the entire denominator gets bigger, and then the quotient gets smaller. So now one more quiz. As n increases, the shape of the sampling distribution gets wider or skinnier? It gets skinnier. When the sample size is small, the standard error is large, and the sampling distribution is more spread out. When the sample size is large, the standard error is smaller and the sampling distribution is narrower. So you that as n increases, we have a smaller standard error and we have a smaller interval in which we're pretty sure. That the population mean lies. Remember that the mean of the sampling distribution is the same as the population mean. Specifically, we have to quadruple n to achieve half the measurement error. Let's explore this, let's say that instead of a sample size of n, we use 4 n, so we quadruple our sample size. This is the same as sigma divided by root 4 times root n, due to the properties of the radicals and the square root of 4 is 2. So this is the same as one half. Times our original, standard error. So if we quadruple n, then we have half the measurement error. This isn't particularly important, but it's good to make a note of. So now you've experimented with the die applet. And you've seen how the sampling distribution changes when we have larger sample sizes. But try one more applet. This time we have an actual population distribution. Rather than just the values 1, 2, 3, 4, 5, and 6. Like we did with the dice. So this time, you can start with a parent population being normally distributed. Or you can start with a uniform distribution for the population, or skewed, or custom. You can draw a bimodal distribution, and then you simulate taking for example, five values from this population, finding the mean. And graphing it. So I clicked animated here, and it found five random values from this population, and then plotted the sample mean. Let's do that same thing 1,000 times. So here's the distribution of sample means, and you can even calculate the standard deviation of this distribution, because here we have the population parameters. So the standard deviation of this distribution should be 7.88, the population standard deviation divided by the square root of 5. Try it out, do you get 3.54? You probably get something pretty close, and that might be because this standard deviation is treating this as a sample, so it's using Bessel's correction. That means that this standard deviation would be slightly bigger than if we were calculating it. Treating this as a population. But that's besides the point. Play around with this. This will really help you get to know sampling distributions. And we're going to use sampling distributions throughout the rest of the course. The important thing to take away here is that you can have any population of any shape. And then, if you plot the distribution of sample means, you'll get a normal distribution. And it will be more an more normal as n increases. Remember though if n is 1 then the distribution of means is just going to look like the population distribution. We don't just need to use computer applets to demonstrate the central limit theorem we can use real materials, too. Professors Laraway and Rogers prepared the following example for you. Check it out. What we have here today is a sweet demonstration of the sampling distribution of the mean, otherwise known as the distribution of sample means. We've laid out 48 plates each with a bag of M&Ms on it. We're going to dump out these M&Ms, there's Dr. Rogers doing so, across all 48 plates. This is a very tedious process so we sped things up a little bit to make it go faster. Once we have all the M&M's dumped out we're going to separate them by color. We are interested in the blue M&M's. Of course, you could pick any color if you wanted. You could use orange or green or yellow, but we've chosen blue. Once we've separated out the blue M&M's we're going to count them. There I am helping along, Dr. Rogers got tired of me sitting over in the corner just watching, so he recruited my help. Once we have the blue M&Ms sorted, we're going to be interested in the average number of blue M&Ms per bag, or per plate in this case. And what we're going to do then is take random sample of n equals 5 and compute an average. Once we've done that we're going to do that 50 times, taking 50 random samples, each with n equals 5 plates. Then we're going to construct a histogram. That histogram will be the distribution of sample means. Remember that the central limit theorem says, that the distribution of sample means will be approximately normal, and we'll have an expected value of the mean equal to the population mean. Now the true population mean we've computed by taking all blue M&Ms across all plates and divided them by 48, the number of plates. Remember the central limit theorem also says that the standard deviation of the distribution of sample means is equal to sigma the population standard deviation divided by the square root of the sample size. Let's see how the data turned out Okay. Let's look at the data and compare the two distributions. The population mean, in other words the average number of blue M&M's across all of our bags, was 11.25. The expected value, in other words the mean of all the means we took from our 50 random samples. Is 11.08. Notice those are pretty close. They're not exactly the same, but they are not far off. The population standard deviation for all 48 plates or bags was 3.49. Hi, it's me again. Let's recap Professor Rogers and Professor Laraways experiment, and then predict what happened before they tell us their result. So they took a bag of M&Ms and they calculated the number of blue M&Ms in that bag. So this is the first bag and they put it on a plate labelled plate number 1 and n1 represents the number of blue M&Ms in that bag or now on that plate and they did that with the second bag and counted the number of blue M&Ms. And they did that up to number 48, and they have 48 bags, and they dumped each bag onto one plate. So here's our population, the number of blue M&Ms on each plate. They found that the mean is 11.25 blue M&M's, and the standard deviation is 3.49. From this population, professors Laraway, and Rodgers, randomly selected five plates. And then they counted the number of blue M&M's divided by 5, and got the average number of blue M&M's. And then they did this 50 times. Can you believe it? So then they got 50 sample means. So now let's take a quick quiz. According to the Central Limit Theorem, approximately what should be the mean and standard deviations of this distribution of sample means? So write your answer here. When you calculate these, remember you're calculating the theoretical probability, which is based on using every possible sample of size 5 from this population. But here they're using only 50 samples of size 5. There are a lot more, but we can't expect professors Rogers and Laraway to do that. That would really take forever. So calculate the theoretical mean and standard error based on the central limit theorem. Good job. So the mean should be the same as the population mean. And the standard error should be the population standard deviation divided by the square root of the sample size. Which is about 1.56. So remember, since these are the theoretical mean and standard error based on all possible samples of size five from this population, they probably won't get exactly the same standard error as what we found. But they should come pretty close. So let's see what they got. The standard error of the mean computed using this formula, derived from the central limit theorem is sigma 3.49 and divided by the square root of our sample size 5. So that's 3.49 Divided by the square root of 5 or n, that comes out to be roughly 1.56. If we took the true standard error of the mean, in other words, computed the actual standard deviation for all of the sample means we just took, that value is 1.57. Very, very close to that given by the formula. In checking the distribution of sample means The sampling distribution of the mean here was approximately normal; not perfectly normal, but pretty close. So all in all, we see the central limit theorem works. It provides us a useful tool that we'll use for the rest of the semester. So, hurray for the central limit theorem! As we briefly showed before, when finding the probability of rolling an average of at least three with the tetrahedral die, the central limit theorem is not only awesome, but important, because it allows us to know where any sample mean falls on the distribution of sample means. In the example of the tetrahedral die, we wanted to know the probability of getting at least a three, for an average, if we rolled it twice. And we found that when we looked at the histogram, rolling at least a 3 was 6 out of 16. And now, we're extending this concept to populations. So, if we have the distribution of sample means where the samples can be any size. Where does a particular sample mean of that same size fall on the distribution? If we know where it falls on the distribution, then we can decide if this sample is typical or if something weird is going on. So, let's use another example. Klout score is the single number that, that measure your influence across the Internet primarily the social media. I'm Chief Scientist at Klout, I run the data science team as well as the data infrastructure team oversee all the data driven products that come, you know kind of lead the team behind the score. And you created the whole infrastructure, and the algorithm behind the Klout score, right? I create the infrastructure and the science behind the Klout score. We grade everyone's influence on the Internet from a scale from 1 to 99. It's a single score that you can see as your credit score for you online reputation or you know, a single score that you can measure influence with. Klout score is calculate by incorporating all your data from various social networks, ranging from Twitter to Facebook, to LinkedIn. And for wach network, I would collect about a hundred signals. Number of retweets you get per twwet, number of likes you get per post on Facebook stuff like that are all the signals that we look at overall we consifer four hundred signals per person. And we generally scores for 400 million and more users at everyday. Klout score follows this by model distribution, where you have this very big peak of users who are in this 40 to 60s on this side and also the users who are quality content producers. And you also have this very nice peak of users on the other side who are the consumers of those content. And if you, you know, plot the score, and the number of users with each score buckets from this side to this side. You know, this is zero, and that's 99. And this, you know, the y axis is the number of users who are in this buckets. You can see a very nice by model distribution there. The average cost score is lower than the median because they are proportionally more people with a lower score, so they drag the average down. So in such a distribution, taking a look at the median makes more sense. Being able to beat forty is actually a pretty good sign of being very influential on the social media. So, the first thing we're going to do, is analyze this data. This is the biggest data set you've worked with so far, so don't be intimidated. So, open the spreadsheet of Klout scores and calculate the mean and standard deviation. Treat it as a population rather than a sample. They're our Klout scores. So first let's calculate the mean. Push Cmd down arrow key and then you see that there are 1,048 values. And make sure we don't have a header row, which we don't. So we know that n is 1,048. So anywhere really, we can write equals, average, A1, colon A1048. So we get that the mean is 37.72 approximately. Let's calculate the standard deviation. We calculated our mean in cell D1048. Cell rate in cell B1, equals A1 minus $D $1048, which keeps the value in cell D1048 constant. Okay, the we'll go to cell B1 hit Cmd+C to copy it, go to cell A1, Cmd down arrow key go to cell B1048, Cmd+Shift up arrow key. So all of column B is highlighted, Cmd+B. Now we have all our deviations. And actually what we could do is instead, we can just calculate the square deviations. So let's put parenthesis around the deviation and then square it. Now let's copy this and paste it. Now we have all our squared deviations in column B. So we calculate the variance by taking the average of this, equals average B1:B1048. So that's our variance and then our standard deviation is the square root, equals square root of D1. So, here's the distribution of Klout data and you can see it's bimodal, like the chief scientist described. The mean is about here at 37.72. So now, let's say we're able to take all possible samples of size 35 and calculate the mean for each one of them. And then, graph this distribution of sample means. So, this distribution should be normal, if you remember from previously. What will be the mean of this distribution? The mean should be the same as the population mean. So, about 37.72. What about the standard deviation of this distribution? The standard deviation is the population standard deviation divided by the square root of n, where n is the size of each sample that we're taking. So this is 16.04 divided by the square root of 35, which is about 2.71. Good job. So now which one of these distributions is the correct sampling distribution for the population of klout scores. Is it this one, this one, or this one? Since the mean is supposed to be the same as the population mean, then it will be one of these two. So, which one of these looks to have a standard deviation of about 2.71? For this wider one, the standard deviation looks like it's at least 5. So, this is the correct sampling distribution. So, why is knowing the shape of the sampling distribution important? Well, first of all, let's say we want to increase our klout score. Brans use cloud to reach out to the influencers for word of mass marketing . So for example Audi has this test drive combined and they are offered to influencers in San Francisco. They basically invited all those opinion leaders in different vectors of the industry to come in and test drive one of this newest model, and get a sense of how they feel about this new model. That's what Albie did, and Virgin America for example offered this VIP servers for those, who are even just account user, when they come to the airport terminals, the envidos influences over to their VIP lounge, just for all the pros they have there. Because you know, they think having a car score is a good indicator of you being. Influentia Savio was this social media thing that's coming up. So, and people has different ways of using car scores across different domains. I've also heard of some dating sites using cars score as a signal to match people up. Come on, the other ways that you can think of often top of your head. Some CM malls chip marking offers is Klout to just filter their candidates with. Because a lot of the bigger, more traditional companies, when they try to adapt to their social media era, they're trying to hire someone with social media background. To be able to help with their branding and, and, and the picture of their company. And for those jobs having a Klout is a must. I've heard. I think Barrack Obama, the President, has the highest class score of 99. And San Francisco Giants since just recently after they won the World's Series, they had a share that same score for a month. Wow, and Justin Bieber? Justin Bieber has a very high class score of 92. And a lot of celebrities and media celebrities are a whiz in their range. And but he's one of, definitely one of the highest Klout score. So, influential people have high Klout scores like the president and Justin Bieber. Now, let's say we looked up the Klout scores of 35 people who use the app Bieber Tweeter, which automatically retweets Justin Bieber whenever he tweets anything. If you don't know, a tweet is a message that people write on the social networking site Twitter. And the Klout score is partly based off of people's Twitter activity. So, let's say that the average Klout score of these people is 40. Where does this mean fall on the distribution of sample means for other samples of size 35? So, just to recap, you found that the standard deviation of this distribution is 2.71. So, how many standard deviations above the mean of this distribution does this mean lie? So where does 40 fall on this distribution? Looks like it's above the mean, and the mean is 37.72, so we're finding how many standard deviations 40 is from 37.72. So 40 minus the mean of the sampling distribution, divided by 2.71, which is about 0.84. Good job. You just calculated the z-score for this particular sample, using the mean and standard deviation of the sampling distribution. And remember that the standard deviation of the sampling distribution is called the standard error. So now what's the probability of randomly drawing a sample of size 35 with a mean of at least 40? As a hint, use the z table. We know the z score, so we can find the probability of getting anything less than that z score using our z table. So here's 0.8 and here's the column where it intersects 0.04. So the probability of getting less than a mean of 40 is 0.7995. So the probability of getting greater than a mean of 40 is 1 minus 0.7995, which is only about 0.2. So it's somewhat unlikely to have randomly drawn a sample from the whole Klout population with a mean of 40. If this mean wasn't selected by chance, then it's possible that this app, the Bieber Tweeter, which automatically re-tweets Justin Bieber, could have played a role in increasing these people's Klout scores. But how unlikely is a probability of 0.2? There's no set threshold. But in the next lesson, we're going to show you how statisticians have formally decided whether or not something is likely or unlikely. So, can we say that the Bieber Tweeter caused these higher Klout scores? No, we can't. All we know is that the Bieber Tweeters tend to have higher Klout scores on average. Maybe the Bieber Tweeter caused this, but maybe it's just that users with higher Klout tended to be the ones who used the app. Moving on, what if instead of having a sample of size 35, our sample had 250 people in it. And they also have the mean Klout score of 40. Now, what's the likelihood of randomly selecting from this Klout population? A sample of size 250, whose mean is at least 40. First let's find the standard error. Again, the standard error will be the population standard deviation, divided by the square root of the sample size, which is now 250. So now, the denominator is bigger which means that the standard error will be smaller. And indeed it's 1.01. So, let's write it over here. So, this sampling distribution is going to be a lot skinnier. So, where would a mean of 40 from a sample of size 250 fall on the distribution of means from all other samples of size 250? 40 is still above the mean of sample means, so the z squared will still be positive, but it's going to be a lot more because the standard deviation is less. So, we have 40 minus 37.72, divided by 1.01. Smaller denominator, bigger quotient. So, this gives us 2.26, approximately. So, now that we know the z-score, what's the probability of randomly selecting a sample with a mean of at least 40? Let's bring the z table back again. So now our z score is 2.26. So here's 2.2, and here is where it intersects 0.06. So the probability of getting a mean less than 40 is 0.9881. The probability of getting at least 40 is 1 minus 0.9881, which is a far lower probability. So we have this really skinny sampling distribution and the probability of getting greater than 40 is very small, 0.0119. So getting a sample of size 250 with a mean of 40, is pretty unlikely to occur by chance. In this case, this shows some kind of relationship, probably, between the Bieber Tweeter, and Klout score. Great job so far, this is pretty complicated stuff. To wrap up this lesson we're going to do a little experiment and it's going to be awesome if you guys participate. Together we're all going to generate sampling distributions of size 1, 5, and t0. So here's what we're going to do. Pick a number, any number between 1 and 1048. Click the link to access the Klout score data, and then find the Klout score in the row of the number you picked. So if you picked the number 786, for example, go to row 786, find what Klout score that is, then write it down. Next,pick 5 numbers between 1 and 1048, find the Klout scores in these rows, take the average, and then write this number down. Finally, repeat this process with ten numbers. So you should get three numbers. Write these numbers in the Google form, with the link below. So essentially, we're going to get a bunch of means, depending on how many of you guys participate. So let's say 1000 of you participate. Then we'll have 1000 Klout scores. Which we'll then create a distribution for. And this should look like the population distribution. And we'll also have 1000 means, of samples of size five. And we'll have 1000 means from samples of size 10. Then we'll be able to graph each of these sampling distributions and see if it turns out how we theoretically would have expected. So time for your last quiz of this lesson. Is this going to be awesome? You bet it's going to be awesome. I'm so excited to see how this turns out. In the meantime, feel free to check out the results in that Google form and try to analyze it yourself. See if we get some sampling distributions that look like they should, theoretically. Great job, you guys. Great job completing Descriptive Statistics. I really hope you've enjoyed this experience. Having taken this course, you should now know how to summarize data by measuring its center and its variance. You should be familiar with how to use Google Spreadsheets to handle data sets as well as how to visualize data. And finally, you should have introductory knowledge of probability and how it can be applied to normalized data. In the final project, you will conduct an experiment using a deck of cards. You will sample and describe your own data using visualizations and descriptive statistics. Now once you completed the final project, you'll be ready to move on to Inferential Statistics, where you will learn how to make predictions using data. There's a link to this course in the instructor's notes box. We really look forward to seeing you there. A researcher found a relationship between the number of hours spent playing violent video games per week and the level of aggression shown by adolescent males. This is measured by the aggression scale, which is someone's self-reported measure of aggressive behavior. More hours spent playing corresponded with a higher aggression score. Given this information, what can we say is true? Lets say we randomly assigned people with insomnia to one of two treatment conditions. In one condition, participants received 20 milligrams of ambien, and in the other condition patients received a placebo pill. The participants did not know which they received. After taking their pills, the participants slept in a sleep laboratory to help control for lurking variables. The next morning, they rated their quality of sleep on a scale of 1 to 10, with 1 being very poor quality of sleep and 10 being excellent. They reported this scale to a psychologist who also did not know which treatment each person was assigned to. It turns out that the people in the ambien group reported a higher quality of sleep with an average of 8, whereas the people in the placebo group reported an average of 5. What can we say is true about this scenario? Select all that apply. A researcher finds that students who listen to classical music in elementary school tend to have better grades in high school. There was no random assignment to conditions or manipulation of an independent variable. In other words, they didn't just look at a bunch of random students. We might not even know which students they looked at. But then the researcher recommends that children should listen to classical music, because that will cause them to perform better in school. What's wrong with this logic? Select all that apply. This scatter plot here shows the relationship between the number of caffeinated drinks people consume, and the hours that they sleep. Each of these dots represents one person. So, for example, this person consumed about 3 caffeinated drinks and only slept about 5 hours. This data was collected from a survey of 9 individuals. From this graph, what can you conclude? At an elementary school, there are a lot of students for which Spanish is their first language. Now let's say there are two ways of teaching English to them. One way is to only teach them in English. Another way is to teach them in both English and Spanish. So researchers randomly assigned students to one of these two groups. One student were assigned to either being taught in English only or in both Spanish and English, it stayed that way. After a while, the researchers gave each student a reading comprehension test. On the day of testing for the English only group, the temperature was 72 degrees Fahrenheit. But for the bilingual group the temperature was 100 degrees Fahrenheit, which is pretty hot. When the scores were calculated, the English only group had an average of 70%, while the bilingual group had an average of 55%. This is an African pouched rat. They're particularly good at detecting TNT and landmines. They're trained using something called tea eggs, which have TNT in them. TNT is an explosive found in many land mines. If the African pouched rats were able to find the tea eggs. They were given a food reward. But if they scratched near tea eggs that did not contain TNT, then they did not receive a food reward. Scratching near the tea eggs was considered the indicator response. The researchers found that the training was very successful in teaching the rats to detect real landmines in the field. This was measured by the number of meters of land cleared by explosives. And by the number of landmines that they found. Based on this study, what can we say is true? Have you ever had the winter blues? Where you're just depressed during the winter? For some people, this is an actual recognized disorder called seasonal affective disorder. This applies to any season. Throughout most of the year, people with SAD have normal mental health, but experience depression during certain seasons, even, Even spring or summer. Researchers tried to figure out how to treat SAD. 95 people were randomly assigned to one of three conditions. Bright light therapy which is where participants were exposed to 10,000 lux for 30 minutes every morning From 6 o'clock until 6:30 a.m. Lux is the international standard measurement of illuminance. The second group received dawn simulation, which was a dawn signal in which people were exposed to lux that peaked at 250. And this lasted 1.5 hours, and the third was a placebo condition, which was just 1.5 hours of a dim red light at only .5 lux. This lasted from 4:30AM until 6:00AM. The number of people in each group was as follows, 33 in the bright light, 31 in the dawn simulation, and 31 in the placebo. After six weeks, all the subjects were blindly rated by a psychiatrist. So the psychiatrist did not know which treatment each person was exposed to. The psychiatrist uses the structured interview guide for the Hamilton Depression ration for seasonal affective disorder. This measures a person's level of this type of depression on a scale of 0 to 89, with higher numbers indicating greater severity of depression. The researchers found that dawn simulation was associated with greater recovery. That is compared to the placebo group and the bright light therapy group. During a one day blood drive, 35 people donated blood at a mobile donation center. Here are the blood types of the 35 people. Using this table that shows the blood type of each person that donated blood, fill in this table. F represents the frequency, so the number of people of each blood type, p represents the proportion of people with that frequency, and the percent is, well, the percentage. This little symbol here, you're going to get pretty used to it. It's called sigma and it means the sum. So in total, you know that the sum here should be 35 and the proportions should all add to 1, just as the percentages should add to 100. Hopefully you calculcated these numbers in the last question. Now, let's answer some questions about this table. This grouped frequency table is based on the birth year reported by students taking Udacity courses. This is actual data. You're going to answer a few questions about this table.