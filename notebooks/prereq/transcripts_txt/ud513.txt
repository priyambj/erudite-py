Congratulations. You've learned a lot of tools to solve computational problems. Now we're going to dive into some of the most well-known computational problems, and see how we can use our tools to solve them. Along the way, we'll look at a few famous algorithms and concepts that show up often on technical interviews. You might not get asked specifically to solve one of these problems in an interview. But it's common to be presented with a problem that boils down to one of these. Make sure you have a crystal clear understanding of the root problem. And stay vigilant for instances of these cases masked as something else. Hopefully, you've got used to straight forward titles in computer science by now. If so, you might be able to guess that the Shortest Path Problem is all about finding the shortest path in a graph. Early on, we talked about weighted edges, where the edges in a graph have some numeric value associated with them. The shortest path is the one where the sum of the edges is as small as possible. If we had an unweighted graph, the shortest path would just be the one with the fewest number of edges. The nature of the solution to this problem changes a lot depending on the type of graph. For example, the solution to the Shortest Path Problem for an unweighted graph is actually just a breath first search, with the BFS, you start at one node and visit the closest nodes first, slowly moving out to more distant nodes until you find the one that you were looking for. Again, when there's no weights, you just want to find the path with the least number of edges. One solution to the shortest path problem for weighted undirected graphs is called Dijkstra's Algorithm. Let's say we're trying to find the shortest path from here to here. We begin by giving all vertices a distance value. A distance is the sum of edge weights on a path between our starting point and whatever vertex we're on. At the end of the algorithm, this distance will be the distance of the shortest path. The distance value we start with is infinity. This is a placeholder value that will update whenever we discover a node and have an actual distance to store. The node we're starting with will have a distance of zero. A common implementation of Dijkstra's uses a min priority queue, where the element with a minimum priority, or minimum distance in our case, can be removed efficiently. We store all of our nodes in the priority queue and use extract min to take out the minimum element, the only one with a distance of zero. >From our starting node we have several options. We will follow each edge and update the node on the other side with a distance value, which is just the weight of the edge. Now we are faced with a choice. Which node should we visit? We'll always pick the node with the smallest distance value, which means we run extract min on the queue. Because we always pick the node with the lowest distance, Dijkstra's is often called a greedy algorithm. The philosophy for this class of algorithms is pick whatever option looks best at the moment, hence the name greedy. We repeat the process visiting all adjacent nodes that are still in the queue and updating their distance values if we can decrease it at all. We keep going, extracting the minimum from our queue and exploring adjacent elements, until the node we're looking for has been extracted from the queue or everything else has a distance of infinity. Which means the path we're looking for doesn't exist. The basic runtime of Dijkstra's is the number of vertices squared. Since in the worst case, we visit every node in the graph once or twice and every time we visit we need to search through the queue to find the minimum element. There are a lot of optimizations for Dijkstra's, if the priority queue is implemented really efficiently, the runtime looks more like this. [BLANK_AUDIO] The so-called Knapsack Problem is another famous thought exercise in computer science. Here we have a theoretical knapsack with a limited weight capacity, and more items that can possibly fit in it. Each item has a weight and a value. The question here is, how can I optimize the total value of items in my knapsack, given the weight constraint? We'll focus on the 0-1 knapsack problem, where you only have one of each object and you must either take or leave a whole object. In some other variants, you can take a fraction of an object. When I heard about this problem, my first thought was, just put the objects with the highest values in first. but what if putting the two elements with the highest values hit the weight limit, but putting all the other elements in together would actually fit and have a higher value? There aren't a lot of actual knapsacks in computer science, but this problem describes an optimization issue that crops up often. Before devising the best solution, particularly if you're stuck, it can be helpful to think of the solution that's slow but straightforward. Here we could try every combination of objects and just pick the one that's best, also called the brute force solution. The runtime of the solution is on the order of 2 raised to the n, where n is the number of objects. There are actually 2 to the n possible combinations. There's a precise mathematical reason for this, but you could also think of one combination as a string of length n, where each spot has a 1 if it's in the knapsack or a 0 if it's left out. That means this is an exponential time algorithm, since the n is an exponent. We would prefer a polynomial time algorithm where the n is multiplied by something else or has a constant number for an exponent, which is going to be much faster for large numbers. Let's talk about a smarter approach. Our first goal doesn't need to be maximize the value for the largest weight possible. What if we tried to maximize the value for the smallest weights possible, then kept adding them together until we had our maximum weight. Let's look at an example. We start by creating an array, which we'll use to store the maximum possible value for every weight up into our weight limit. We assume that the weights are all positive integers, so the indices in the array represent those weights. We initialize everything to zero for now. These are objects that we're trying to fit in a knapsack with a weight limit of six. We take out the object with the weight of two. We can update the value at index two to the value of the object, then we'll update everything after it as well with the same value. Even if our knapsack can hold six, we've only seen one object so far, so we need to base that best value off one object. We look at the next object. Again we can't change anything until index five. The value of this object is bigger than the max and it takes up the whole weight. So we replace the max value for weights five and six. For the last object, the first thing we could possibly change is index four. This value is less than that one so we leave it alone. We look at index five and compare it to our value plus the value at index one. Since the old value is larger, we leave it as is. At index six we add our object value to the value at index two and end up with a bigger value, so we replace it. And we've solved the problem. This may seem unnecessary with an example this small, but imagine how much time we'll save with lots of objects. Here, we're taking advantage of the table to store precomputed maximum values. So we only need to do the work once to get those numbers. We go through every object and check if it can increase the maximum value of every possible weight up to our maximum weight. Thus, the runtime is n times W, where W is the weight limit of our knapsack and n is the number of elements. This is a pseudo polynomial time solution. A true polynomial runtime wouldn't have a variable besides n. I reiterate, polynomial time algorithms are much faster than exponential time algorithms for big numbers. So the solution here is generally faster. The knapsack problem shows the beauty of a technique called dynamic programming. With dynamic programming, you can make a really complicated problem like this one run much faster by breaking it into subproblems. Here, the subproblem was finding the answer for a smaller weight. You begin by solving for something like a base case, a subproblem so small that the answer is very simple or trivial to compute. We started with one object. With only one thing to consider, finding the maximum possible value for any weight was simple. Another common feature of a dynamic programming solution is a lookup table that stores solutions to subproblems. We stored the maximum values for different weight limits in our lookup table. Dynamic programming solutions take advantage of these two things, solving the problem for a trivial case and storing the solution in a lookup table, by using them to slowly add complexity to a problem. Another feature of a dynamic programming solution is an equation used at each step as you add complexity. For us, the equation looks something like this. The equation often combines some values previously computed in the lookup table, sometimes with each other and sometimes with a new value you introduce like the value of whatever object you're looking at. We used the values already stored in the table as we added new objects, a technique often called memoization. So we ultimately didn't need to recompute them every time. This is the hidden power of dynamic programming. You compute and save solutions to smaller problems. Then you don't need to calculate them again for more complex problems. It might seem like a simple idea, but it can have a really powerful effect on efficiency if it's done well. One of the most useful skills to have when going into a technical interview is the ability to spot a problem that has a dynamic programming solution. It didn't come naturally to me at first, but after practicing by trying to solve a bunch of dynamic programming problems on my own, I started to get the hang of it. One thing you need to look out for is a complicated problem that seems to require trying every combination to find the solution. You need to ask yourself, can I break this problem into subproblems? If the answer is yes, then you've got a problem with a dynamic programming solution. The Traveling Salesman Problem or TSP as it's called, is one of the most famous problems in computer science. Envision a graph where all of the nodes are cities and all the edges are roads between them. TSP asks, what's the fastest way for our salesman to travel between every city and return home? At its essence, this is an optimization problem. When you get rid of the analogy and, I guess, the hat, we have a complicated graph and we're looking for the optimal route. TSP is used in everything from microchip design to DNA sequencing. Let's take a look and see how it's done. The traveling salesman problem falls into a special class of problems called NP-Hard. NP-Hard problems don't have a known algorithm that can solve them in polynomial time. As I mentioned during the discussion of the Knapsack Problem, polynomial time algorithms have a run time where n has a constant exponent like n squared and we want algorithms that can solve problems in polynomial time or they're often too slow to be useful. The Knapsack problem is actually also an NP-Hard problem. The solution we looked at was in Pseudo-Polynomial time and no true polynomial time solution is known. Since the problem is so difficult, there are two classes of algorithms considered solutions. The first are exact algorithms, which don't happen in polynomial time but will get us the correct answer. The second are Approximation Algorithms, which don't always find the exact optimal solution but generally find a near optimal solution. They tend to run in a more reasonable amount of time, and several are even polynomial time. The Brute Force solution to TSP has the same philosophy as the one for Knapsack. Find every possible combination and pick the best one, but it takes significantly longer. It's in factorial time, so n multiplied by every integer between it and one. There are also dynamic programming solutions for TSP. The most famous one is the Held-Karp Algorithm. However, even the dynamic programming solutions are slow. The runtime for this particular solution is an exponential time. Even though the approximation algorithms don't find the exact optimal path, there is a lot of active research into them for TSP. One of the most famous, called Christofides algorithm, works by turning a graph into a tree, where the starting note is the root creating and traversing through it in a particularly intelligent way. The algorithm guarantees that the path it produces will be at most 50% longer than the shortest route. There have been some slight improvements on this first specific cases of TSP but generally it's considered to be the best there is. TSP is an interesting case study for this type of difficult problem, particularly, since it has so many applications and is so widely studied. While you likely won't get asked about it directly in an interview, it's good to know that problems like this exist and that it is possible to get an interview problem with no efficient solution. You don't need to worry too much about the exact meaning of NP-Hard or similar classes, but it's good to remember that even the brightest minds can't solve every problem efficiently. This new project that I'm in charge of really has me stumped. Do you mind if I think out loud for a bit Winston? Thanks. I have a ton of data about websites and I want to be able to search through them by which pages have links to each other, there's an issue though. I can't get a clear mental picture of what they would look like and I don't know which data type is best for modeling connections between web pages. [BLANK_AUDIO] Do you think I should use a graph? What's that? [BLANK_AUDIO] Right. That's a lot of technical mumbo jumbo. But I gather a graph is a data structure designed to show relationships between objects. Sounds great. I can't wait to learn more. Okay, so what does a graph look like? Wait. No. I've been conditioned to think of a graph as a chart that shows trends, like a line graph, or a graph of a function. But that's not what we're talking about here. Let me try again. Our definition of a graph looks more like this. The purpose of a graph is to show how different things are connected to one another. It's also sometimes called a network. A graph is similar to a tree in many ways. For starters, these are called nodes or vertices, and these lines over here are called edges. In fact, a tree is just a more specific type of graph. It's just like a binary tree is a type of tree, and a BST is a type of binary tree. Here's an example of a valid graph, where you can really start anywhere and you'll follow some path back to the start. Since cycles like this one are possible, graphs don't really have a root node like trees do. Graphs are really vaguely defined, but we should keep it that way. As you've probably come to expect, we need to keep it vague so our definition will encompass all of the different use cases for our graph. For example, I would love to say that nodes are the parts of graphs that store data, and edges are just the connections between them. But, edges can actually store data too. Since graphs can seem really abstract at the start, let's make them more concrete with some examples. Let's imagine this graph as a social network. This node can be me. I can draw the rest of the graph like this. Here, it's clear that nodes are people, but the edges between the nodes could be describing any connection between us. Maybe it's people who've met each other. Here, two notes that are connected by an edge, sometimes called adjacent, have met each other in person. This could be a graph of people who've lived in the same city at the same time, or people who've worked on a project together. The nodes could contain any amount of data, like each person's full name, where they live now, their birthday, etc. As I said before, edges could contain data, too. Edges often contain data about the strength of a connection. For example, these edges could show how many times in the past week we talked to each other. Okay, let's think about a new example. Let's say this graph is my travel plans for a big trip. I'm going to fly from San Francisco to Tokyo, then Kuala Lumpur, then Delhi, then back to KL, Tokyo, and SF. The edges could contain no information, or they could contain how long the flights are. The information I decide to store will depend on what I'm using this graph for. Graphs have some additional properties that make them useful for modeling various types of data. Edges can have a direction, meaning the relationship between two nodes only applies one way and not the other. Directed graph is the term for a graph where the edges have a sense of direction. For example, if you're using a graph to represent travel plans, a directed edge can be used to show which city you start in and which you travel to. Often one node will be the start and the other the end of the relationship or action taking place. You might describe the edge of a graph as the first node, a verb, the word two, and the other node. Here, it's, From SF, travel to Tokyo. If you make a round trip and travel through the same two cities going in opposite directions, you might have two edges between the same two nodes, one representing each direction you travel in. An undirected graph has edges with no sense of direction. For example, if you have a graph of people and edges are drawn between two people who have met before, directed edges might be unnecessary. I previously mentioned that graphs can have cycles but trees can't. A cycle happens in a graph when you can start at one node and follow edges all the way back to that node. Cycles and graphs can be really dangerous when you're describing algorithms. You might read some code to travel through the nodes in your graph. But if you have a cycle, you might just go through the same sequence again and again and again in an infinite loop. You often need to make sure the graph you're taking in as input is acyclic, meaning it has no cycles. One type that shows up often is a dag, or directed, acyclic graph. It's really just what it sounds like, a directed graph with no cycles. Another important property of graphs is connectivity. That term has a specific meaning in the study of graphs called Graph Theory but let's define what it means for a graph to be connected first. A disconnected graph has some vertex that can't be reached by the other vertices. A disconnected graph might have one vertex off to the side with no edges. It could also have two so-called connected components, which are connected graphs on their own but have no connection between them. Thus, a connected graph has no disconnected vertices. There's also a metric used to describe a graph as a whole called connectivity. Let's look at two different graphs and say they're each describing a different group of friends. When looking at these two graphs, the one on the right looks like a much stronger friend group. There are overall less people but more connections between them. In the left group I could remove one connection and the whole group would be destroyed. This is the principle behind connectivity, which measures the minimum number of elements that need to be removed for a graph to become disconnected. Depending on the information in the graph, you can sometimes use connectivity to answer the question which graph is stronger. Graphs can be functionally the same but built in a handful of different ways. If you're using an object oriented language you could create vertex and edge objects and give them each properties like name, strength, ID number, etc. A vertex could have a list of edges it's connected to and vice versa. However, operations that traverse graphs can be more inconvenient if you need to search through vertex and edge objects. There are several ways to represent connections on simple graphs that only use lists. One example is an edge list, which is precisely what you'd think, it's simply a list of edges. The edges themselves are represented by a list of two elements. Those elements are normally numbers that correspond to ID numbers for vertices. So, in the end, this list is just showing two nodes that have an edge between them. And an edge list is the list that encompasses all the smaller lists. Because our edge list contains a list of other lists, it's sometimes called a two dimensional list, or a 2D list. If you had a list of lists of lists, that would be called a 3D list, and so on. An adjacency list is another way to represent edges in a graph. In an adjacency list, your vertices normally have an ID number that corresponds to the index in an array. In your array, each space is used to store a list of nodes that the node without ID is adjacent to. For example, the opening at index 0 represents a vertex with an idea of 0. That vertex shares an edge with one node, so a reference to that node is stored in the first spot in the array. Again, our adjacency list is two dimensional, since it's a list that contains other lists. Lastly, an adjacency matrix can be used. In computer science, a matrix is essentially a 2D array, but the lists inside are all the same length. A matrix can also be called a rectangular array. When you look at it, it looks like a rectangle. The adjacency matrix is actually really similar to the adjacency list. The indices in the outer array still represent nodes with those IDs. And the list inside still represents which nodes are adjacent. However, these inner lists represent information in a slightly different way. Before, the list contained just the IDs of adjacent nodes. Now the inner list has one slot for every node in the array, where node IDs map to array indices. If there's an edge between these two nodes, a 1 goes into the array. If there's no edge, a 0 is present. You might notice that the diagonal or the place on every line where the row number equals the column number, is always 0. This value would only be 1 if there is an edge that started and ended with the same node. But we don't have any of those in our graph. You should also note that a single edge shows up twice in the matrix. For example, this edge shows up when the row is 0, and the column is 1, and also, when the row is 1, and the column is 0. Which method of representation you use depends on what makes the most sense for you and what operations you'll be performing the most often. If you're looking at node degree or the number of edges connected to a particular node, the adjacency list will probably be the fastest. As you've seen, graphs are great for modelling connections between elements. That's wonderful on its own, but graphs are really powerful because they're so easy to traverse based on connections. Graph traversal is a lot like tree traversal. Again, a tree is just a specific type of graph, so that makes sense. We have two basic methods for traversal. A depth first search, where we follow one path as far as it'll go, and a breadth first search, where we look at all the nodes adjacent to one before moving on to the next level. These traversals techniques often show up bundled into more advanced graph algorithms. So make sure you understand them before you move on. Depth first search, or DFS on graphs, operates under the same principles as DFS on trees. Keep in mind that a graph search and a graph traversal are roughly the same thing. In a traversal, you look at every element, and in a search, you just stop traversing when you find the element you are looking for. Unlike a tree, there's no root in a graph, so there's no obvious place to start. You can begin with any node. First mark the node you selected as seen. A common implementation of DFS uses a stack. So we can store the node we just saw on the stack. Next, you pick an edge, follow it and mark that node as seen, and add it to the stack. As long as there are more edges and more unseen nodes, you keep repeating those steps. When you do hit a node that you've seen before, just go back to the previous node and try another edge. If you run out of edges with new nodes, you pop the current node from the stack and go back to the one before it, which is just the next one on the stack. You continue this approach until you've popped everything off the stack or you find the node you were originally looking for. There's another common implementation of DFS that uses recursion and no stack. As you can imagine, you just repeat the same process of picking an edge and marking a node as seen until you run out of new nodes to explore. That becomes the base case, and you move back to the last level of recursion, which happens to be the previous node in the search. In this algorithm, we're explicitly visiting every edge and vertex once. Thus, the runtime is often written as this, which reads the number of edges plus the number of vertices. You might notice that we actually visit every edge twice, once to explore it and once traveling back through it. The runtime is technically two E, but the approximated version remains the same. The number of edges by itself actually summarizes the runtime pretty well, but the V is added to account for the time taken to look up a vertex. That exact time varies depending on your choice of data structure, so be careful to base it on your implementation when asked. [BLANK_AUDIO] BFS, or breadth-first search, is actually quite similar. You're still visiting every edge and marking off every node. However, here you search every edge of one node before continuing on through the graph. Again, we start with the first node and mark it as seen, then visit a node adjacent to it. Once we mark that node, we can add it to a queue. Remember the difference between a queue and a stack, like we used for DFS. For a queue, we'll remove the first element we put in it. But for a stack, we remove the most recently added element. We go back to that first node and visit everything adjacent to it, marking each as seen and adding them to a queue. When we've run out of edges, we can just DQ a node from the queue and use that as our next starting place. We look at every node adjacent to that one, adding each one to the stack until we've exhausted our options. It's important to note that when we DQ, we're getting a node adjacent to the one that we started with. You can kind of envision a BFS as creating a tree out of a graph. The node that we started with becomes the root, the group of adjacent nodes is the next level in the tree. We need to continue adding nodes one level at a time. We finish off one level entirely before moving on to the next. And soon, we're done with our BFS. The efficiency is still the number of edges plus the number of vertices. Again, we're visiting every edge and vertex during our traversal. There are a couple particularly notable types of paths and graphs. Here, a path is just a specific route you take in a traversal or search. For example, you could create a path that travels through every edge in a graph at least once. That path is called an Eulerian path. Yes, it's pronounced Eulerian after the famous mathematician Euler. In a basic Eulerian path, you start at one node, traverse through all edges and might end up at a different node. In an Eulerian cycle, you must traverse every edge only once and end up at the same node that you started with. It turns out that not every graph is capable of having an Eulerian path. In this graph, no matter where you start, you'll get stuck on one of these outer nodes and won't be able to reach the other nodes without traveling over an edge you've already seen. There's a nice trick here. Our good friend Euler said that graphs can only have Eulerian cycles if all vertices have an even degree or an even number of edges connected to them. Eulerian paths are a little bit more lenient. So it's okay for your graph to have two nodes with an odd degree as long as they're the start and end of the path. Let's look at a quick algorithm for finding Eulerian cycles. You can start at any vertex and follow edges until you return back to that vertex. If you didn't encounter every edge, you can start from an unseen edge connected to a node you've already visited. Again, you create a path through those unseen edges. You can continue this process until you've seen every edge in the graph once. Then you can simply add the paths together, combining them at the nodes they have in common. This algorithm is really efficient. It takes big O of the number of edges since it visits every edge once, which is the best we could hope for. A Hamiltonian path is another type of path that must go through every vertex once. Similarly, a Hamiltonian cycle will start and end with the same vertex. Trying to decide whether a graph has a Hamiltonian path is a famous issue in computer science called the Hamiltonian path problems. We don't quite have the tools to talk about it yet, but we'll talk about similarly famous problems very soon. Hey, there. Welcome to Algorithms and Data Structures for Technical Interviewing. My name is Brin and I'll be your guide and mentor. My job is to teach you how to invent efficient solutions to unsolved problems using algorithms and data structures. A skill you need both in a job and to get a job. Algorithm is a fancy word for something straightforward, a program that solves a problem. At the start of this course, algorithms might seem a bit like a magic trick. [SOUND] You see the input to the trick, and you see the output from the trick. But everything that happens in the middle seems pretty mystical. My goal is to teach you the steps involved with the trick and to explain what resources you'll need to pull off the trick. In this analogy, the resources are often data structures or a common pattern in the problem. After I've given you all the tools you'll need, we'll do a little practice to make sure you're ready to pull off the tricks and technical interviews. However it's not my intention to discuss every little detail like the laws of physics or what each tool is made out of at the molecular level. You'll be able to apply algorithms to problems you'll face at work or in an interview. But not use them at the level of a mathematician. In a real technical interview you just need to show that you can successfully perform the trick and that you understand the concepts behind it. I'm also not trying to explain every concept that could potentially show up in an interview. I've designed this course to explain topics that show up most often, but you should definitely use this course as a starting place and continue to explore on your own. I should also note that I'm not expecting any prior exposure to algorithms or data structures. Just having the will to learn. You should have some familiarity with our programming language and have written some working code with it before if you want to understand the practice problems that go along with the videos, but soon I'll make sure we're on the same page about exactly what that means. In this course, the videos will all contain explanations in pseudocode. But the text will be full of language specific notes and examples. I want to briefly go over some of the basics you should already know before we get started with the new material. If any of these things don't sound familiar to you, please spend some time reading about them or checking out other Udacity courses. If you already know everything just skim through the information and move on to the next lesson. Efficiency is perhaps the most important concept I'll teach you in this course. So make sure you understand it before moving on. Efficiency, also called complexity, is how well you're using your computer's resources to get a particular job done. You can normally think about it in terms of space and time. How long does your code take to run, and how much storage space do you need? Let's think about efficiency in terms of cutting hair. Normally, your hair would be snipped off in clumps, and you would just compare some strands you cut to a group you're about to cut, to approximate where your next cut goes. However, you could cut one strand of hair at a time and compare each strand to a ruler or measuring tape, to make sure the exact same amount gets cut every single time. That's ridiculous, though. It would take a crazy amount of time, and it really doesn't matter if hair is exactly, precisely cut. If you could use less time to get the job done pretty well, than you're good. Often, the tradeoffs for algorithm efficiency are similar. You could take a slow, methodical approach, but there's often a faster method that accomplishes the same goal by reducing repetition. Efficiency can rely heavily on your creativity and your ability to get the most done with minimal resources, but there are a lot of known tips and tricks that you'll be expected to know in a technical interview. An interviewer will expect you to know some common data structures and the efficiency of doing basic things with them, as well as some knowledge of specific well known algorithms. In this case, an algorithm is just a series of steps for solving a problem. One of my favorite moments in my programming career was the first time I used an algorithm to improve my code. I had a function that took hours to run on this huge dataset. But after applying an algorithm, it only took seconds. Writing really efficient code can come off with another tradeoff between time efficiency and space efficiency. What if your code could run in just a fraction of the time by storing temporary values in a new data structure? For that decision, you would need to weigh whether the time of the person using your code or the amount of memory needed is more important. By the end of this course, you'll have most of the tools and knowledge you need to make efficient code. Great, we've learned so much about efficiency. When you look at two pieces of code you can maybe tell which one is going to be faster and which one is going to be slower, but how do we say definitively which one is faster and which one is slower, and how do we communicate that with our coworkers and the people around us? We can describe efficiency of code with something called Big O Notation. The name is odd if you're hearing it for the first time but it turns out to be a very straightforward name. Every time you use this notation you literally write a capital letter O, parentheses and some algebraic expression inside the parentheses. This is the algebraic expression is always going to be a mathematical function of the variable n, and such as some of these examples. As you can see, these all contain algebraic expressions with the letter n. The only exception is this guy right here. Yes the number one still counts. It's just another way to rate 0n + 1, so this is a value we can still use. So what does the algebraic expression mean in this case? Well n represents the length of an input to your function. Let's say you received a secret coded message from one of your friends and a cipher that tells you which letter maps to what. You want to automatically decode the coded messages with the cipher your friend gave you, so you write some pseudocode to figure out how you can do that. You take a string input, you iterate through it and change each letter based on the cipher, and then you output a new message. The pseudocode shows you what it might actually look like. Okay. So to get a sense of the time efficiency, we're just going to count up the number of lines. Creating an output string and returning an output string each only need to happen once every time the function is run. To represent that, we can go ahead and add a 2 to our efficiency. Now both lines inside the for loop have to get run for every letter in the input. For those we can add a 2n since 2 is the number of letters in our input string and we have two lines that need to get run for each one. Hopefully, the way we count up lines and turn it into something in Big O notation is starting to make sense to you. Let's take a look at some examples. If our input string had ten letters, the calculation would look something like this and we would end up with the value 22. To get the actual efficiency, you can multiply 22 times the amount of time it takes for your computer to run one line of of code. If the input string was 1 million you would have to multiply 2 million times the amount of time it takes for your computer to do a computation. There you go, looks like you've got the basics down. I need to reiterate that those were just the basics. There are a lot of complications that we're going to address now. First of all, in the previous example, we should think about the for loop. Does the for the Itself count for something? In order for the loop to work, you need to do a computation each time to get the next letter in the string. Since this needs to happen one time for every input letter, we can just add 1 to the value before n, and now we end up with 3n + 2. It turns out that it's actually really hard to predict how many computations this pseudocode is going to take. For example, a lower level language like C would break down the process a lot more and it would take a lot more lines of code, but they would be doing less work in the background. The Python version of this might be about these many lines of code. But it would be doing a lot more in the background because it's a higher level language. For example, let's look at this line. This line could take a different number of computations based on the data structure the cipher is using. Maybe we store data in a list, and we need to check each letter against our current letter to solve the code. That could be as many as 26 checks for each letter in our input string. So ultimately our efficiency calculation might look a little more like this. Let's go back to those example input strings we talked about before. Now if we start out with a 10 letter input string, we end up with 292 computations. If we start out with 1 million letters in our input string, we end up with 29 million steps to go through. Because of our choice of data structure, we're doing a lot more computations than we might if we chose it a little bit smarter. Later, we'll talk a little bit more about the efficiency of different types of data structures. But for now hopefully you realize the importance of thinking about efficiency before writing code. Since the amount of steps can vary widely based on the specific implementations, we normally use approximations when talking about efficiency in big O notation. Since the efficiency of the pseudo code we wrote before could end up looking like any of these and even more, we would say that the efficiency is actually about O(n). If you're writing code for production you definitely care about the number of computations and want to minimize it. However, in an interview, your interviewer doesn't care that you can do that exactly, but just wants to know that you are thinking about efficiency and can calculate it if you need to. By approximating, we're really saying some number of computations must be performed for each letter in the input. There's actually more things you should consider when using big O notation. If you noticed before I said you'll need at least 26 checks to get through every letter of the alphabet. In reality it's unlikely that you'll have to go through all 26 letters each time. When we're discussing efficiency we often talk about it in terms of the worst case scenario, where we would have to check all 26 letters each time. We focus on worst case because it puts an upper bound on the amount of time our code is going to take. You could also talk about efficiency in terms of the average case or even best case. So if we're talking about letters in the alphabet, the worst case means we would have to go through all 26 letters. However the best case would be that we find what we're looking for on the very first try, but on average, we're going to fall somewhere in the middle, around 13. In a perfect world, iterating through a list of letters is going to take an average of 13 times. Sometimes it will be a little more, sometimes it will be a little less, but it's fair to say it will be about 13. If we're going to try and calculate the average case efficiency, it's going to look a little more like this, in the end will get 16n+2. Now let's go back to our point about approximations. If we consider the average case the worst case or the best case, it's also going to result in O(n). In an interview you might be tempted to combine the average case, the worst case, the best case, and just give an answer like this. However the best case, the worst case, and the average case might not always approximate this nicely. So you really should specify to your interviewer which case you're talking about. We've been talking a lot about time efficiency but you can actually use the same notation to refer to space efficiency too. For example, let's say you needed to copy over your input string three times in your code for some reason. Then the space efficiency would look something like this. Space efficiency is asked less frequently in interviews, but it's definitely something you should consider because it will come up from time to time. Now we're going to talk about collections. There's almost no need for a formal definition here, since a collection is, as the name implies, just a group of things. You could have a collection of books, a collection of Udacity employees, or even a collection of desk animal things that I talk to when I'm stuck on programming projects and the coaches aren't around. Let's make a few observations here. Collections don't have a particular order. You could shuffle some elements around and it wouldn't make a huge difference. Similarly, we couldn't say, give me the third element in this collection, since there's no inherent order. Collections also don't need to have the same type of object. Everything in the room could be in the, around the dusty office collection, even though there are books and people and animal things. A collection on its own really isn't something you can use in a programming language. However, there are many data structures that are extensions of collections. So, they just add more rules to the ones that already apply for collections. Let's take a look at some of those now A list has all the properties of a collection. So it has a group of things but the objects have an order. A shopping list, well not really something programmers do with a lot, happens to have a lot of the same properties as the list we use. There are things that might not have the same type of object, but they're still related to everything else on the list. The order doesn't mean much. It's just the order that I ran out of each item and there's no fixed length. I could just keep adding things to the end, or adding and removing from the middle and it doesn't really affect anything. Again, what exactly the implementation looks like for adding elements will change based on the language and the type of data structure. We're still talking theoretically here. So don't sweat the details too much yet. In this lesson we'll explore several types of lists. Different programming languages treat them differently. Sometimes a list is incorporated as a core feature of a programming language, but what that looks like can vary wildly. The videos here will talk about the theoretical concepts that could apply to any language, while the text will explain the language-specific implementations. Arrays are perhaps the most common implementations of lists. In many programming languages, the ability to create an array is built in as a core feature. You may have seen or heard of them before but it's important to have a more formal definition of what an array is so we can compare them to other data structures a bit later. As I mentioned before an array is a list with a few added rules so we all ready know that it has some things in some order. In some languages you can only have objects with the same types in the same array. And in some languages your array can contain different types so we can't use that as a rule to define arrays. It would also be nice to say that arrays have a set size that you determine right when you create them. Again, this is only true in some programming languages so we need to avoid adding that to our definition. Defining the thing that different languages call arrays is actually pretty hard. There is one big feature that differentiates arrays from lists. Each array has a location called an index. An index is just the number associated with that place in the array. It would make sense to number these boxes from one to five. But of course it can't be that simple. Normally, an index starts at zero so instead of referring to these boxes as one to five, we usually say it's 0 to 4. Having indices can make using arrays a great or sometimes terrible choice for the code you're working on. If you need to access a certain location in the middle frequently, using an array can be a great choice. You just need to keep track of how long the array is. Calculate the middle element and check the object in the box with that index. Insertion and deletion can be really messy with arrays though. Inserting in the end is often easy but it can be hard if the array has a set size and you've already filled it up. Insertion is difficult if you want to put an element in the middle of the array. If you want to do a normal insert you'll need to move everything after the inserted element back into different boxes with a different index. The operation as a whole is pretty inefficient since you need to move every element behind the one you're inserting back in the array. In the worst case this operation takes linear time or a big O of n. Deletion causes a similar problem. If you delete an element you have an empty box. You can't just look at the index and say that this is the fourth element anymore since there's an empty box before it. Again all of this can change based on the way a particular language implements an array but it's important to consider when talking about arrays abstractly. Hopefully, arrays are starting to make sense to you. Let's quickly look at them in another way. You can think of an array kind of like this. It's a set of boxes where each has an address called an index. This paper chain is more similar to a construct called a linked list. A linked list is an extension of a list, but it's definitely not an array. There are still some things that have order, but there are no indices. Instead, a linked list is characterized, well, by its links. Each element has some notion of what the next element is since it's connected to it, but not necessarily how long the list is or where it is in the list. An array is different. There is nothing in one element of the array that says here's your next element. You know what the next element is by what the next index is. My first thought when I heard this was why would anybody ever want to use a linked list, an array gives you more information since you already know where the next element falls in the array. However, as we discussed before, adding and removing elements from an array can be really complicated. Adding and removing elements from a linked list turns out to be so easy by comparison. You can actually just take an element out or add one in. There's only one real quick consideration here, but we'll talk about that in a minute. In higher level programming languages there often isn't a distinction between linked lists and arrays. There's just a list that has the properties of both. However questions about these two data structures are fairly common in interviews, so it's important to know the difference. Again, the main distinction is that each element stores different information. In both cases a single element will store a value. Or the actual information. So if you have an array or linked list of numbers, the value will be a single number. If you have an array or linked list of string values, then each value will contain a string. In this case we're just going to stick with numbers. Now, in both cases we store one other type of information. But that type is different. In an array we would store a number as an index. You can get the next element by querying the array for the element at index one in this case. Let's clear this out and focus. Just some linked lists for now. In a linked list, we store a reference to the next element in the list. In many languages this will look like assigning the actual next element as a property of this element. Way down at the hardware level your element actually has some space dedicated for it in memory. These are representing the memory locations. So in this case, the next component is actually storing the memory address of the next element, while this element is storing null because it doesn't point to anything else. The nice thing about the system is that it's pretty easy to insert and delete elements. Adding an element is going to look something like this, we really just need to change the next reference to point to the new object and done. There's a quick trick you need to remember though if you delete the next reference and replace it with a new object. You'll lose your reference to this object, you should always assign your next pointer for this element before you assign your next pointer for this element, so you don't lose your reference to this one down here. Now everything looks great. Note that insertion takes constant time in this case since you're just shifting around pointers and not iterating over every element in the list. Removing an element is going to look pretty similar, so I'm not even going to go over that now. There's also something called a doubly linked list where you have pointers to the next element and the previous element. The rules are pretty similar for a doubly linked list, except you can traverse the list in both directions now. Again, you need to be careful not to lose references when adding or removing elements but it's easier than performing these operations on an array. Literally every time I hear about stacks, I can only think about a stack of pancakes, chocolate chip, of course. Or maybe I'm just always thinking about pancakes? Not actually sure. Stacks are also list-based data structures, so they have a bit of a different flair than arrays and linked lists. The main idea is a stack is like a stack of objects in real life. You keep putting elements on top and you have easy access to remove or look at the top element. Even if the pancake on the bottom looks like it has the most chocolate chips I can't get to it easily. I need to make my way through everything above it to reach it. I also know that it was the earliest thing put on the stack, back when I thought I would have chocolate chips to spare. Stacks can be really useful when you only care about the most recent elements or the order in which you see and save elements actually matters. Maybe you have a page like a news feed. You'll need to access the more recent elements more quickly and more frequently, but you may need to show all of the elements when the user scrolls down. There is some specific terminology associated with stacks. When you add an element to a stack, the operation is called push instead of insert. When you take an element off of the stack, the operation is called a pop instead of remove. Remember all you need here is to look at the top element of the stack. So the operation should be constant time for both operations. Now here's a possibly confusing point because a stack is pretty abstract, it could actually be implemented with another data type. What each element looks like and how there’s connected aren't actually specified, just the methods for adding and removing elements. For example you could use a link list to implement a stack. You would keep track of the front of a single link list and just keep adding elements on top as you went. I also want to note that you might see the notation L.I.F.O. associated with stacks. This acronym means Last In, First Out. All it's saying is the last element you put on the stack when you use push, is always the first one you get out when you use pop. It's just a fancy way of saying something you hopefully already know about Stacks. Imagine a line of people all waiting to get their hands on the best chocolate pancakes in existence. The way this works is the person who's been in the line the longest gets her pancakes and gets out first. People can join on this end, but only people on this end get their pancakes and leave. You could perhaps call this a first in, first out structure or a queue. A queue is kind of the opposite of a stack in this way. In a stack the most recently added element comes out first, but here the oldest element comes out first. Otherwise queues are actually pretty similar to stacks. The first element in a queue, or the oldest element in the queue, is called the head. The last element in the queue, or the newest element in the queue, is called the tail. When you add an element to the tail, the operation is called enqueue. When you remove the head element, the operation is called dequeue. There's also an operation called peek, where you look at the head element but you don't actually remove it. Again, you can implement this data structure with a linked list, where you also save references to the head and tail so you can look them both up in constant time. There are actually two special types of queues that show up a lot. A dequeue or double-ended queue is a queue that goes both ways. You can enqueue or dequeue from either end. If you think about it, a deck is kind of a generalized version of both stacks and queues. Since you could represent either of them with it, you could treat it like a stack and add and remove elements from the same end, or you could treat it like a queue and add elements on one end and remove them from the other. Let's examine a priority queue now. In a priority queue you assign each element a numerical priority when you insert it into the queue. When you dequeue, you remove the element with the highest priority. This doesn't really follow the rules of a normal queue, where you remove the oldest element first. However, if the elements have the same priority the oldest element is the one that gets dequeued first. Let's talk about maps. Hm, this wasn't exactly the kind of map I wanted to talk about, but it will do for now. The defining characteristic of a map is its key value structure. You can look up where something is by its key and then get whatever is stored at the value. This is actually pretty similar to our normal definition of a map. We can look up something by its name and then go to a location to get the value. For example, let's say I wanted to find Winston. I can look him up by his key or his name to figure out where he is and then go get him over here. Maps are also called dictionaries for a similar reason. If you think of a word as a key and the definition as a value, it's pretty much the same thing. Time to add another type of data structure to the mix. Before we dive into maps, I want to introduce one new concept quickly. A set is another one of these broad, abstract concepts. It's comparable to a list. It's also vaguely a collection of things, but with one big difference. A list has some kind of ordering for its elements. A set doesn't have that but instead doesn't allow for repeated elements. You can think of a set kind of like a bag. You can reach in and pull something out, but you'll never know what order you're getting the elements out in. A map is a set-based data structure, kind of like an array is a list-based data structure. The keys in a map are a set. I know that's a lot of new terminology, but let's look at exactly what that means. The keys of a map, like the words in a dictionary, need to be unique. You can have several definitions for the same word stored in the value, but if you had the same word in the dictionary many times, you would randomly pick out a definition when you were first looking. Thus, each key only exists once in a map. And a group of these unique keys without any order is called a set. So what are maps useful for, other than dictionaries? You can use a map for a lot of things that have unique names. For example, maybe you have a bunch of information about Udacity courses. You could store each course ID as a key and use that to look up more detailed information as values. Hi. I'm back again. Just wanted to stress the importance of the material you're about to learn. Using a data structure that employs a hash function, allows you to do look-ups in constant time. Let that sink in for a second. With everything you've learned so far, you need to do look-ups in linear time. In a list or a set, you need to look through every element to find the one you're looking for. Stacks and cues let you look up the oldest, or newest elements, immediately. And priority queues will let you find the highest priority element quickly, but if you want any other element, you've got to do a linear time search. The ability to do constant time look-ups, will make almost any algorithm you write, instantly faster. Everyone here at Udacity was really excited, that you'd be learning such a crucial topic today. They all wanted to be here to talk about it. But they couldn't all abandon their work, so they decided to send a representative, whoever was the most excited. Let's see who they sent. Starbucks says that this concept shows up constantly in technical interviews. You can almost always optimize an answer, or improve an answer with it, or reduce a complex question, to just use a hash function. Also, he saw a squirrel this morning. Okay, so how do these magic hash functions work? The purpose of a hash function is to transform some value into one that can be stored and retrieved easily. You give it some value, it converts the value based on some formula, and spits out a coded version of the value that's often the index in an array. Let's go through an example to clarify this process. Let's say you're running some big event. Maybe it's for people who like a certain type of music. Everybody who wants to come to the event needs to buy tickets in advance. All of those tickets are randomly numbered, and those numbers are embedded in bar codes on physical tickets that people hand you when they first arrive. You need to be able to look up ticket numbers fast when people check in at the event. Storing the ticket number in one of those incredible hash functions sounds pretty great to you. You have all of your numbers, now you just need to come up with a hash function to convert them to hash values so they can be stored easily. One common pattern in hash functions is to take the last few digits of a big number, divide it by some consistent number, and using the remainder from that division to find a place to store that number in a array. In this case, the remainder turned directly into the index of an array. Why does this strategy work? Well, if the numbers we had assigned to each ticket were 0 to 100, we could easily store it all in an array and look them up instantly by the index. When numbers are big and random like the ones we started with, we need some way to convert those numbers into array indices quickly. That's how the constant time lookup works. You give your number to a hash function, which spits out a hash code that turns into the index of an array. You can go to your array and get your original value in constant time since an array look up with an index happens in constant time. Hm, why use the last few digits of a big number? In most cases the last few digits are the most random. If you're assigning your numbers in numerical order The first few digits really wouldn't be random. They're more likely to start with a 1, a 2, a 3, etc, than later numbers like 7, 8, or 9. As it turns out there's a flaw in our elegant system. There are times when a hash function will spit out the same hash value for two different inputs. This situation is termed a Collision. There are two main ways to fix a collision. The first is to change the value in your hash function, or to change the hash function completely, so you have more than enough slots to store all of your potential values. You can also keep your original hash function but change the structure of your array. Instead of storing one hash value in each slot, you can store some type of lists that contains all values hashed at that spot. These lists are generally called buckets in this context. Rather than storing one value in each slot, you can store multiple values or a collection in each bucket, but are either of these approaches actually helpful? For the first one, you can maintain constant time look up but by using a bigger number in your hash function, you're going to require a lot more space to store your values. Also, if you do this reactively and change the value in your hash function every time you have a collision, moving all of your data to a new array is going to definitely increase the complexity in terms of both size and time. With the bucket approach, you still need to iterate through some collection, though a shorter one, every time you're looking for something. Hash functions have a constant lookup time in the average case, but because of the bucket system, you could end up storing every value in one bucket and then you're still essentially just iterating through a list. In that worse case, this actually turns into O(m). When done well, hashing is really fast and can save you a ton of time, but all of these things are very real concerns. There's no one perfect way to design a hash function. You need to consider all of these things and build a system that makes the most sense for your data and your limitations. Often, you'll have to choose between making a hash function that spreads out your values nicely but uses a lot of space and one that uses less buckets but might have to do some searching within each bucket. Ideally, you would have one to three elements stored in each bucket. So you can design a hash function with that in mind. You can also do something creative like using a second hash function inside of a large bucket to split up those elements even more. That would work particularly well if you know you're going to have well spread out data but end up having a few really large buckets. Hashing questions are popular because there's often not a perfect solution. You're expected to talk about the upsides and the downsides of whatever approach you choose to use. So, just do your best to optimize your hash function and make sure you're communicating with your interviewer well. And finally, we reach the intersection of two very important topics, maps and hashing. Hash maps are one of the main places you'll see hash function show up. In the previous example we were just using a hash function to find a place to store keys. However, maps have keys and values. You can use the keys as inputs to a hash function, then store the key value pair in the bucket of the hash value produced by the function. Again since you know the keys in a map are unique since they belong to a set you could use a hash function to give them each their own unique buckets. You can also design a hash function to allow for collisions. In an interview, you'll often be asked to create a hash table to show that you understand hashing. Also that you know the upsides and downsides of designing a hash function. However hash maps are very useful to integrate into algorithms. Constant time lookups can really speed up your code. Keep it in mind and always think if it'll work first when thinking through data structures. The real beauty of the system is that you can use it with string keys too. You just need to come up with some hash function that converts letters into numbers. Individual letters can be pretty easily converted into ASCII values and many languages already have functions built in that do that. We can combine the ASCII values with a formula to get a unique hash for each letter. So what should our hash function look like? Again, there are some trade-offs here. Do we want every word in it's own bucket? Are you okay with collisions but want a relatively simple hash function? If you have 30 or less words, you can probably just use the ASCII value for the first letter of a string as a hash value. The standard hash code function for string keys in Java a large hash table over having any collisions. The formula looks something like this. For example, let's say, we're going to hash the word UDACITY and we're starting with the first two letters of the string, UD. We can plug these ASCII values into the equation to get a hash value that is unique to our string. Why does this work? Well by multiplying the ASCII value for each letter by a power of some number like 31, we can guarantee that every number representation or hash value will be unique to that string. A hash function like that would be great for a dictionary where we need unique buckets for each string. However strings with just three or four letters already have huge hash values. The tradeoff is really important here. As long as you have the space for it, a unique hash value can be really useful. Lastly, why is the number 31 used here? The earliest hash functions took advantage of some properties of the number 31 and research showed that it was a great choice for this kind of string hashing. However, now there are more complex hash functions that have been discovered. So thirty one is more of a convention than the best value for every situation. Remember that designing a solution for your keys is the most important thing. Don't get too bogged down in all of these conventions. Okay, now that we have some tools, we can dive into basic algorithms. Let's say you have an array sorted in numerical order and you want to see if a number that you have exists in this array. If you start at the front and check every number in the array, the time could be O(n) if your number is really big. The same happens if you start at the other end. You could have a really small number and it might end up taking linear time in worst case. However, we could do a cool trick here that takes advantage of the fact that you have a sorted array. Rather than starting at the end, let's say you started in the middle of the array. You could say, is my number bigger or smaller than the one in the middle. Since my number is bigger, I know that it'll have to be in the second half of the array. Now I don't even have to search the first half of the array. The best thing about this strategy is that you can make the same assumption again, just on the second half of the array. Check the middle element and then move forward from there. My number is still bigger than the middle of this array, so now I only have to check the last element. And now we're at the end. We've checked 19 and 29, two numbers side by side, and we still don't see 25. Thus, we can conclude 25 does not exist in our array. Guess what? You just learned an algorithm. An algorithm is really just a high level description of a trick for solving a problem. This one happens to be called binary search. Let's start talking about the efficiency of this algorithm with a new example. Here we have an array with eight elements and we're going to try and find the number ten. Okay, so we'll want to start in the middle. This problem is a little bit weird because there's an even number of elements in this array. So we could either start with four or five really. When you're designing your algorithm, you need to decide from the start whether you're going to err on the lower side or the higher side when you hit this weird case of having no real middle. I'm going to go ahead and err on the lower side and start with the number four. Just like before, we'll check whether our number is bigger or smaller than the one in the middle. And since our number is bigger, now we only need to look at the second half of the array. Again, we have an even number of elements in our array, so we're going to pick the number on the left of the middle and start from there. Okay, since ten is greater than six, yet again we get to cut our array in half and deal with just the top. Yet again we have an even number of elements, so we're going to err on the lower side and check there first. Since ten is greater than our middle element seven, we're left with just the last element now. And now that we've checked the highest element in the array and our number is still bigger than it, we know that this number doesn't exist and we can move on. Since the time efficiency is really just the number of steps you're going to need, keeping track of each iteration is going to help us figure it out. In the past, we had to step through every single element, but here we're cutting the array in half and only considering some elements. So the efficiency isn't going to end up being as big as O(n). I've created a table so we can take a good look at the array size versus the number of iterations of our algorithm. As we just saw, we had to go through our algorithm four times for an array size of eight. Again, we're talking about worst case here. I could have tried to put a number in between all of these places and on the outsides to figure out what the worst case was. Just to save your time, I did that before recording this video. In the process I also discovered a trick here. If you want to make sure you're checking for the worst case, you can pick an element that's bigger than anything else in the array. And when you hit this weird middle case, always favor the lower side. You should prove this to yourself so you know that it works, but just trust me on it for now. Okay, so for an array size of eight, we have worst case for iterations. We can take the results from the first example too. There it took us three steps to find something in an array of seven. Hm, let's start thinking about different array sizes now. If you had an array size of one and you were looking for an element, say 30, it would always only take you one step to see whether that element was in your array or not. Great, that means we can add one to our table. Let's say we have an array with two elements in it and we're looking for this number, 23. Again, I'm taking advantage of that trick. I picked a number bigger than the ones that were in the array. And when I have that weird case, I'm always going to favor the lower side. And when my array has an even number of elements, I'm going to pick the number that's on the left of the middle. Since 23 is greater than 18, I can check 21 now. In just two steps I have my answer, that 23 doesn't exist in this array. Great, we can add two to the table now. In the interest of time, I'm not going to go through all of these examples. You should definitely do them yourself though to prove that these results are true. Okay, we're starting to get a picture of our efficiency here. I notice that four is half of eight, so maybe our efficiency looks something like this. Again, keep in mind that I'm approximating here. Instead of counting the number of overall steps in the algorithm, I'm just counting the number of times I'll need to run the algorithm, in this case four when my n is eight. Actually I don't know that this is true since half of two isn't two, and half of one isn't one. That's too bad. I really feel like two should go in this efficiency somewhere since I have to cut the array in half every single time I run the algorithm. Let me think about exactly what that means for a second. If I cut the array in half every time I go through an iteration, that means every time I double the number of elements, I need to do an extra iteration. Actually, now that I think about it, that actually looks true with the results of my table. Every time my array doubles in size, it takes an extra iteration of the algorithm to get through it. Actually when I represent these as exponents, I notice that the power on top of the two is always one more than the number of iterations it takes. So maybe my efficiency looks more like this. The number of overall iterations is going to increase every time the exponent on the power of two increases, and of course it's only off by one. How do I get some number or expression that represents this, the power of two exponent? So this is what I'm starting with, the number of elements in my array, and I want to end up with this number, the exponent. Well, I remember some weird math that I used to do a long time ago. There was this thing called a logarithm that let me express this kind of function in a different way. Instead of saying this, I could say log base 2 of 8 = 3, and it means the same thing. Okay, so this looks a little bit nicer. I want to make a point to note that you don't need to know a ton about how logarithms work to understand this. If you just understand the equations I showed you before, you should be fine. So we could actually use another approximation to make this look even nicer. As I mentioned before, adding a constant or adding one doesn't really change my efficiency very much. Also, I don't really need to say that this is log base 2. In computer science it's actually pretty safe to assume that your logs are in base 2. We often do things like cut arrays in half or use binary. So our logs are normally going to be in base 2 instead of the typical base 10. So this is the efficiency that we end up with. A little later on we'll have some more tools. So it'll be easier for me to show you visually why this efficiency makes sense. There's one thing I want to make very clear to you all right now. When I was learning about efficiency, it often seemed like people could just jump to this conclusion. They could just look at the original problem and say, of course this is log n. However, after years of being a developer, I learned that that really wasn't the case. I found that there are three main ways that people can get to this efficiency. One of them is just by knowing what binary search is, having seen it before, and being able to just memorize the efficiency and spit it out when you see it again. I highly recommend you just memorize the efficiency of basic algorithms for your interviews. It'll make you seem really smart if you can just spit out the efficiency from the start. However, if you're presented with a new problem and you don't know the efficiency of the algorithm already, you need to be able to solve it on your own. One way you can do that is with proofs. I personally don't like proofs and have never used them in an interview or seen other people use them in interviews. If you want to learn how to make proofs, you can look it up online and go read research papers and see other people step through that. However, my goal is to teach you the practical approach to this. When you haven't seen the algorithm already, and you don't have time to whip up a proof out of nowhere, making a results table like this is actually one of the best things you can do. Creating a results table helps you notice patterns and helps you start thinking about efficiency in terms of array size versus the number of iterations of your algorithm. When I first started learning about efficiency, I needed tables like this to see those patterns. However, after using tables for a while, I started to be able to see the patterns on my own without them. Now when I interview, I know what types of patterns are common like changes in the powers of two. And now I solve problems way quicker and there are very few problems that I can't at least approach and try to solve. The worst thing you can do in an interview is say I've never seen this algorithm before or I don't know how to write a proof for it. If you're down on your luck and you really don't know what the answer is, please make a results table and try to figure it out. Your commitment to solving the problem is really going to impress your interviewer. Recursion is maybe one of the most confusing subjects in computer science. If you get stuck at any point, keep studying the examples and try to find some more examples until it sticks for you. The idea here is to create a function that calls itself. Here is a really basic example in pseudo code. This code doesn't do anything interesting, but it shows the main parts of a recursive function. First of all, a recursive function needs to call itself at some point. Second, a recursive function needs a base case. You can think of a recursive function kind of like a while loop in some regards. You keep going through some code again and again and again until you hit some exit condition. In a recursive function, the base case is like the exit condition. It lets you know when you need to stop. Without a nice base case like this, you could get stuck in something called infinite recursion. That means your code keeps calling itself and executing again, and again, and again, and again, and again. But because you never tell it when to stop, it just keeps going infinitely. The third thing a recursive function needs to do is alter the input parameter at some point. If you aren't altering the input parameter or giving yourself some condition for changing every time you iterate, you'll never really know when to stop. And again, you end up in the same bad place with infinite recursion. Okay, now we're going to go through an example. This is the code that we're starting with, and on this side, I'll write out each major line that we execute. So we're going to start by calling the function with an input value of 2. We'll start with this if clause. Since our input value is 2, and that's greater than 0, we move on to the else part. We start by evaluating this before we assign anything to the variable output. Okay, so input- 1, that's just 2- 1. So we're going to call this function again, but instead with an input value of 1 and not 2. So this is the really confusing part. Your computer knows that you still haven't finished that initial call with the input value of 2. You're just jumping into this part without finishing. In its temporary memory, it will remember that. However, it essentially makes a copy of all of this initial code, and you have to do it again, but this time with an input value of 1. I put an indent here to signal that we haven't actually finished this initial call yet, but we're just doing a call inside of that right now. Okay, so let's do this again. This time, the input value is 1, but that's still greater than 0, so we can skip down to the else block. The same thing happens a second time. Before we assign anything to output, we have to do this recursive statement. This time, the input value's 1, so 1- 1 is 0. And we have to call this recursive block again, but with a value of 0, instead of 1 or 2. Again, I indented to indicate that we still haven't forgotten about the fact that we never solved this and returned anything, but we do need to move on and start executing again. All right, let's do this again. So this time, we actually have something interesting here. The input value is 0, so we never get to execute any of this else block. We're just staying up here. We return 0, so we see it pop up over here. Now that we've actually returned something from one of our calls, we can forget about this recursive with an input of 0 and go back to that recursive with an input of 1 that we were looking at a minute ago. All right, so we return to having an input of 1. We evaluated this statement to 0, as I indicated over here. Again, remember, we called the recursive block, we return 0. So now we say that this whole thing evaluates to 0, and we can assign 0 to output, great. Now that call we did where the input was 1 actually has a return value here, which is 0. So now we can kind of forget about this recursive block and move back to that original call, where our input was 2. Okay, so back to the original call, this call, which used to be recursive with an input of 1, got evaluated to 0. So now we can say 0 is equal to output. And again, we can return 0, and now even our initial call is done. Great, so the initial call returns 0. Recursion can be really confusing, because you start out with just this one method, but you don't even know how many times you'll end up calling it. When I'm trying to think through how a recursive function works, I'll often write out a table or a list of steps like this, so I can keep track of my values every time I hit the recursive block again. I always used to think of it like my computer was just going on tangents. I started with this initial function, and then I kept calling something else, and doing something else, and doing something else, and doing something else. And eventually, I would get back to the thing I was talking about originally. Remember, your code is going to go through these lines of code in order. It's never going to assign this output any number until you've hit the recursive block as many times as you need. Remember to be really careful when you're writing recursive code. If your base case doesn't make sense, you could end up doing infinite recursion. Let's say we change this base case just a little bit. All I did was get rid of the less than or equal to and replaced it with just equal to. Now this code is going to run fine if we give it an input value of 2 or 1 or 4 or even 1 million. But if the input value was actually less than 0, this code's never going to return. If you start out with -2, you'll skip the if block, and you'll just keep subtracting 1. Next time, your input's going to be -3. Then you'll subtract 1, -4. Subtract 1, -5, and so forth. If I use less than though, if my number initially is -2 or -50 or -1 million, I'll stop as soon as I hit the base case. Remember to think about all the different input values your recursive statement could have. Lastly, if you're really not sure what your recursive block is doing, you can use print statements wherever necessary. By printing out intermediate values, you'll get a sense of exactly what your code is doing at exactly which steps. Let's move on to sorting algorithms. In a search algorithm you have some kind of list, and you're just checking out each element. However, in a sorting algorithm you're changing the order of elements in an array. Let's say you wanted to sort these guys by height. You have some decisions to make first. Should I put the shortest element on this end or on this end? And what algorithm should I use to sort? You could just compare every element to every other element until you have everything in order. This is called the naive approach since there is no real trick or cool algorithm involved there. The naive approach is normally the first thing your mind skips to when you see a new problem. It makes you think I know I could do it this way and everything's going to be right, but there's probably a smarter way to do this. Sorting is kind of a weird topic. You'll rarely be given a problem that can't be solved with one of the classic sorting algorithms we go through here. As far as interviews are concerned, it's important to understand how sorting algorithms work and have their run times memorized so you can answer complexity problems really fast. Lastly, you should make sure you've considered whether your sorting algorithm is in place or not. An in place sorting algorithm just rearranges the elements in the data structure they're already in without needing to copy everything to a new data structure. These algorithms have low space complexity because you don't need to recreate the data structure. Often you'll have that classic tradeoff when you use less space or use less time. It doesn't really matter for arrays with ten numbers, but if you have millions or billions of data points it makes a huge difference. It'll never hurt to mention that in an interview. You can pick one way or the other, but say why you're doing so. The more reasons you can provide for your choices, the smarter you seem. Bubble sort, or sinking sort, is a naive approach, like I described before. You will go through your array comparing elements side by side and switching them whenever necessary. Let me illustrate with a quick example. We'll start at the beginning and we'll compare the first two elements. If the first element is bigger than the second one, we'll switch them. Great, we've made one switch, so now we can move on over to the next two elements. Looks like we need to switch here as well. All right, now we can move along to the next two elements. And again, looks like our first element is bigger than our second one, so we'll need to switch. Okay, now we can compare the last two elements in the array. And once again, the first element is bigger than the second one, so we'll need to switch. All right, now we're done with this round. Okay, now that we've passed through the array one time, we should count up the number of comparisons we needed. It took us four comparisons to get through five elements. I think it's safe to say that our runtime for this step was n-1 since the array size was five. So with our n-1 comparisons, what actually changed? The array is still not actually sorted, but it looks a little bit better now. The largest element, 8, moved up to the end of the array where it belongs. This is actually why this method is called bubble sort. In each iteration the largest element in the array will bubble on up to the top. So by that logic, in the next iteration the number 7 should bubble up to right below 8. Let's repeat the process. Again, we're going to start at the bottom and work our way up to the top. Looks like we need to switch on our first step. Let's move to step two now. It looks like we actually don't need to flip these two for the first time. This first element is actually less than that second element, so we have nothing to do, awesome. We do need to flip 7 and 0 though. And the last two elements are in order, so no work here. Again, we had to do four steps, which is still n-1. Okay, now we're going to have to do two more rounds of this, but I don't want to bore you with the details. Let me show you what the next iteration of comparisons looks like. I did another iteration, but not much has changed. I just had to flip the 3 and the 0 to look like this. Again, the biggest element left, 3, bubbled up to the place it belongs, but we still have work to do. And again, note that this took four steps, so n-1 steps. We only have one iteration left. Let me show you what that looks like. This time we only had to flip the 0 and the 1, and we're done. Great, so what's the efficiency of this algorithm? Well, we had to go through pretty much the whole array a lot of times. Here's roughly what we did. Overall we had to do four or n -1 iterations, and at each step we did n- 1 comparisons. So n- 1 comparisons for and n- 1 iterations gives us something like this. We can use a little algebra to figure out what this simplifies to. And we get something that looks like this. Again, don't worry too much if you can't do the algebra. The minus one isn't changing much for us so you could always just do n times n equals n squared. Thus, we really don't need this minus -2n+1 bit. We can get rid of it and that leaves us with this is efficiency big o of n squared. Actually there's a way we could have saved ourselves some time with this algorithm. If you noticed each time we went through the array we compared the last few values even though we really didn't need to after a while. It made sense to compare 7 and 8 when we were bubbling up 7. But when we were bubbling up some of the lower numbers this comparison didn't really change anything. The most common version of Bubble Sort actually leaves out this step every time of comparing the last few when you really don't need to do that. It assumes that after the first iteration you know that eight is in the right place. So you don't need to check it against anything else later. Again this will save us some time but it's not going to change our overall run time of big O of n squared. I have a couple quick notes for you okay. So we understand by the worst case and the average case are going to be big O of n squared. However, the best case is actually going to be big O of n. The best case would either be that the array was already sorted in which we can just go through it once see that sorted and not have to do anything else or if there's only one number that needs to be bubbled up to a new place in the array. And one last thing, we didn't have to use anything extra to do our sort. We had no extra arrays, no extra data structures, nothing like that. Bubble Sort is a great example of an in-place sorting algorithm, so the space complexity is constant. Meaning we didn't need any extra arrays or data structures in the whole process. Let's talk about another popular sorting algorithm called Merge Sort. The overall idea is that you split a huge array down as much as possible and then over time build it back up doing your comparisons and sorting at each step along the way. The general idea of breaking up an array sorting all the parts of it and then building it back up again is called Divide and Conquer. Several efficient sorting algorithms have this principle at their core. Okay, let's see how merge sort works with an example. First we break up the array into a bunch of arrays with one element each, now we need to start building it back up again. You may wonder why the one leftover element goes at the beginning and not the end. Don't worry too much about details like that. They’ll differ depending on the implementation. However the step should only have arrays of size two and size one left. First we compare these two elements to see which one is smaller. Since one is smaller, it'll go first in our new array and since 3 is bigger it'll go second. Now we move on and do this with the next pair of arrays. Again 7 is bigger than 0. So in our new array we put 0 in the first spot and 7 in the second spot. And now we do this one last time comparing the last two elements 10 and 2 and putting them in order in a new array. Great that was easy enough. Now the last thing we want to do is keep track of the number of comparisons we did so we can come back to it when we do our efficiency calculation overall this step took 3 comparisons each one in green. Okay, we're running out of space, so I'm going to clear everything off except this bottom row and move it on up. Now we're going to combine the first two arrays and the last two arrays and create an array of three and an array of four. Now these comparisons are a little more complicated. Here we're comparing an array with one element to an array with two elements. We know that the array on the right is already sorted so the first element of our new array is either going to come from here or the first element of this array. It looks like one is smaller so that will be our first element. Now we can move on to the next element in this array and compare it with this one. Since 3 is smaller than 8 we'll put 3 in this box. Now that we're done with this array we can just take anything that's left in the other one and tack it on the end here. Great, let's move on to building this array. So we're going to start out comparing the first two elements of these arrays, that's 0 and 2. Since 0 is less than 2, 0 becomes the first element in our new array. Okay, so now we move on to the next element in this array which is 7 and compare that to 2. 2 is less than 7 so we put 2 in the new array. We repeat again checking 7 against 10 and fill them in at the end of the array accordingly. How many comparisons did we do in the step? Well, if you count up all the purple comparisons that leaves us with 5. Okay we're almost there. We just need to go through one more round to get our completely sorted array. This is going to start just like it did in the last round. We start off by comparing the first two elements of the two arrays that we have, so 1 and 0. Since 0 is smaller it ends up going in this array and we get rid of it in the old array. We move on comparing 1 and 2 and end up with one being smaller. So 1 fills in the spot. We compare again with 2 and 3 and again with 3 and 7. And again with 8 and 7. And finally we're done. We have our newly sorted array. We need to keep track of the number of comparisons we did in this step. So I'll write 6 comparisons off to the side here. So how many overall comparisons was that? Well, let's try to find a pattern here. It looks like at each step, we did one less comparison than the array we were building. So when we were building an array of seven, we only did six comparisons. When we're building an array of three, we did two comparisons, an array of two, one comparison, and an array of four, we did three comparisons. I'm going to go ahead and call the length of each array m just to make it a little easier for me. So if m is the size of the array that we're building, the number of comparisons is always going to be one less than that. It's going to be hard to get an exact number of comparisons here. However, we have a good friend that we can count on, approximation. Since we know that every iteration just cycles through the same seven elements again and again, we know that the size of our arrays, or m, are always going to add up to seven at the end. So again, here we have an array of seven. Here we have an array of four and an array of three, which adds up to seven. And here, one, two, two, and two adds up to seven. Since we're just doing divide and conquer, splitting up the array at each step, we can say that approximately we're going to do seven comparisons at each step. Again, seven is an upper bound. We're never going to do more than seven comparisons, but at many steps we get close to seven. So to get the run time of a sorting algorithm, you can normally multiply the number of overall iterations by the number of comparisons at each iteration. In bubble sort, we were doing n comparisons and n steps. We just proved that we're doing at most n comparisons at every step. But how many steps overall are we doing? Okay, so to sort an array of seven, we had to do three steps, one, two, and three. Also I should note some of the sub-processes that happened here. In order to sort this array of two, we only had to do one step. When we were sorting an array of three or an array of four, we needed to do two steps. We can use a nice table to keep track of all of that. Again, so when our array size was seven, we had to do three different steps. When our array was three or four numbers large, we had to do two steps. When our array was two large, we had to do one step. And if we just had one element, we don't need to do any comparisons, so no steps. I'm going to go ahead and fill in a few numbers here just for the sake of time. You can assume that I did these calculations on my own and I encourage you to do them as well. Okay, so this might look a little bit familiar to you. Remember in binary search when we had a similar table and the number of iterations incremented at every power of two. Well, we actually have something pretty similar here. Instead of incrementing at the powers of two, this time we're incrementing one after every power of two. [BLANK_AUDIO] So, in the binary search video, I proved why the number of iterations would be equal to log(n). So if you need a refresher, you can go back there. Again, this time is a little different because it increments after the power of two instead of at the power of two. Yet again, we're kind of relying on approximation here. We don't exactly care where the change happens as long as we capture the interval at which the change is happening. So in summary, we're doing roughly n comparisons for log(n) steps. That makes an overall complexity of nlog(n). This is definitely better than the n squared we got in bubble sort. In bubble sort, we had n times n or n squared as our efficiency. log(n) is generally going to be less than n, but it's definitely never going to be greater than n. So we can say that the efficiency of merge sort is better than the efficiency of bubble sort. However, the space efficiency of merge sort is actually worse than bubble sort. In bubble sort, we were sorting in place, so we didn't need to use any extra arrays. Here we frequently copied our values into new arrays. You can say that the auxiliary space or the extra space we used was linear. This complexity assumes that we got rid of arrays after we were done using them. At each step, we were copying values into new arrays, so we needed new arrays at some point. However, we could say that we were getting rid of the old ones whenever we were done. So we're not using a ton of new arrays every time. We really only need two different arrays at every step, the array that our numbers were in and the new array that we're copying the values into. In many cases quicksort is one of the most efficient sorting algorithms. To do a quicksort you essentially pick one of the values in the array at random. Move all values larger than it above it and all values below it, lower than it. The value that you pick initially is called a pivot. You continue on recursively, picking a pivot in the upper and lower sections of the array, sorting them similarly until the whole array is sorted. Okay, let's watch this in action. First we need to select a pivot. The convention is to pick the last element as your pivot. So let's go ahead and do that. Our first element is larger than our pivot. So we need to move it behind the pivot. However, we're doing an in-place sort. So we need to move the element in front of the pivot to make room. So in this step we're moving 8 to the last block, 2 to the second to last block, and 10 all the way up to the first block. This seems a little counterintuitive. You would think that we would move 10 back and not forward. Just hold on for now, we'll see exactly why this works soon. So we did our shift and now we can make another comparison. Now we get to take a look at 10. Since 10 is bigger than 2 we need to, again, move 10 back. Move 2 up and move whatever's in front of 2 up as well. Great, one more swap down. Finally, we have 0 at the front of our array, which is actually smaller than our pivot of 2. That's pretty great, but now we just move to the second element and do the same process with that one. Since 3 is greater than 2 we'll need to shift around everything again. Awesome, this is looking so much better. Since 7 is still bigger than 2, we need to do another comparison, though. Here's the swap we need to make. And this is what our array looks like now. This is definitely good. 2 is in the right place. We've checked all the elements before it and we're sure they're less than it. And we've put all of these elements after it that are all definitely bigger than it. Because we're sure everything in the array below 2 is actually less than 2 and everything above 2 is greater than 2, we know that 2 is in the exact right place. I'm going to color code it so we know that we don't need to move it again. At this point we normally do the same process for everything smaller than the pivot and everything bigger than the pivot. Let's start with the smaller part first. We select our pivot and compare to everything before. Since 0 is actually less than 1, there's nothing we need to do. And with just one comparison, we know that both of these are in the right place and we can move on. Now in the second half of the array, we select a pivot. Since 7 and 3 are both smaller than the pivot, we won't need to move them. However, 8 is bigger than the pivot. So we'll need to do a swap here. Now that our swap's complete we can see that 8 and 10 are actually in their eventual positions. Everything less than 8 was already below 8 so we didn't need to move it at all. Okay, now we only have two elements left that maybe need to be sorted. Again, all we have to do here is assign 3 as a pivot and compare. We'll see that these two need to be switched. Do that quickly and then our whole array is done. As it turns, out the efficiency of quick sort is actually pretty complicated. First, let's just focus on the worst case. What would that look like? The magic of this algorithm is that it cuts the number of comparisons you need to do. By splitting the array in half, pretty much every time. So, the worst case for us would be if we couldn't split the array in half and had to do all of the comparisons every time. You'll end up doing all of the comparisons if the pivots are actually, already in the right place. Since 13 is already the biggest element, we'll end up comparing it to everything else on the first go. And realizing that it doesn't need to move. This is a lot of comparisons to do at once, but the real problem happens when the next value is also the largest. Again, we end up comparing to everything smaller than it and we're not saving any steps. Hopefully, the number of comparisons here reminds you of bubble sort. Remember in Bubble sort, we would have to compare each element to the one next to it, again and again and again and again. Eventually, we could leave off the ones that were at the end, because we knew they were in the right place. The worst case of Quick Sort is exactly the same which means that the worst case of Quick Sort is actually big O of n squared. For something called a Quick Sort, that's a really terrible efficiency. However, Quick Sort is useful for two main reasons. First of all, the average and best case complexity are actually nlogin. In a good case, the pivot will move down to the middle and we'll get to divide the array in half every time. With our pivot in the middle, we can look at the other halves of the array and move their pivots to the middle too. Since these pivots are sorted, we know that everything else is sorted, so we're done really quickly. Here, because we're cutting the array in half every time, it'll end up looking a lot like merge sort. So again, that's why are efficiency is at log n. The average case is actually going to look a lot like this. We'll pick a random number, it'll move close to the middle and so on and so on. However, if we know we're going to be getting a raise that are near sorted. We don't want to use Quick Sort, since that will end up being the worse case every time. The second point is that you can do some optimizations with Quick Sort that will make it run faster. For example, when you split your array, you can configure your program such that it runs both halves at the same time. It will end up using the same amount of computing power, but it will eat up less time. Also, rather than selecting the last element as a pivot. You could look at the last few elements and select the median of them as the pivot. Selecting the median will give you a better sense of what's in the middle of the array overall. So, you have a better chance of moving your element in the middle and having that best case scenario. Also this version of Quick Sort is in place, so we aren't using any extra space. Our space complexity is constant. Now that you understand algorithms and data structures, it's time to think about the actual interview. For this lesson, we're joined by a very special guest to help us to do just that. Welcome, Horatio. Hello, I'm Horatio, and I'm an Android coach here at Udacity. Not too long ago, I was in a similar situation to the one you may be in right now. I never took a class in college about data structures and algorithms. But I just graduated from a UDacity Nanodegree, was preparing for technical interviews. Learning about data structures and algorithms made me feel more confident about technical interviews. But I soon realized what is also very important is the actual process for answering an algorithm question in an interview. Definitely. When an interviewer asks you a question, your ability to get the right answer is just a small part of what they're evaluating. They also want to see your approach to solving problems and your communication skills. In this lesson, you'll be shown interviews, and then we'll break them down at each step to help you understand exactly what the interviewee is doing right and wrong. Then you will be able to implement the interviewees' skills while avoiding their mistakes. The path to a successful algorithm's interview involves a seven-step process, which we'll go over in further detail throughout the lesson. The steps are Clarifying the Question, Generating Inputs and Outputs, Generating Test Cases, Brainstorming, Runtime Analysis, Coding, and Debugging. So let's get right into it with our interview. Thank you for joining us today. Thanks It's a pleasure to be here. Let's get started. So given a 2D matrix, m, of just 0s and 1s, count the number of islands in the matrix. In this case an island is just a group of 1s or a 1 by itself. Okay, so we're given a 2D matrix. Which would look something like this. [BLANK_AUDIO] So what we're to do is we're to find the number of islands. Where an island is a group of 1s connected. But if 1s are connected diagonally, does that constitute an island? No, the 1s need to be connected either horizontally or vertically to count as an island. Okay, so what we're really supposed to do is find the number of islands in this matrix. Where an island is 1s connected horizontally and vertically. That's exactly right. Cool. All right, how do you think our interview we did for the first portion of the interview? Well I think he didn't do too bad. You clarified the question to make sure that he's solving the right problem. Right. This is a very popular interview question, so there are different variations. It's important to make sure you're solving the right variation. Some interviewers may consider 1s connected diagonally to be an island. In general, clarifying a problem is crucial to being a good software developer. You need to prove to your interviewer that you won't dive head first into a problem and potentially waste time writing code that doesn't solve the initial issue. So I'm taking in a matrix containing only 0s and 1s. [BLANK_AUDIO] And what I'm returning is an integer that represents the number of violence in our matrix. [BLANK_AUDIO] Yeah that's perfect really clear. All right thanks. So that was a short but important portion of the interview. Can you tell me what the gentleman was doing? Well, it looks like he's confirming the signature of the algorithm he would be writing. This step is important because now he knows what's given in the problem. And what the result needs to be. This is going to be really useful information once we get into the next step. In a way, he's also confirming the question again to make sure he's on the right track. Okay, so we can also take in an input of a non object and maybe even an empty matrix. Is that right? Right. Hm, so we have our regular matrix so that we're taking in, but it contains zeroes and ones, nothing else, right? That's true. So this is also another input that we can have. So here we started thinking through tests and edge cases for inputs into the algorithm. As you may have noticed, when he was creating edge cases, he thought of the possible weird inputs that he would have to handle. Right, if there were other values that could have been used in his matrix, he would have to account for those. It's always a good idea to think about how your code handles null and empty inputs, since not addressing them could cause your code to crash. You know what is great about this step in the approach, is I have a feeling it might help in future steps. Let's see. Okay. So what I'm thinking here looking our possible inputs. If we were to take none or an empty matrix as an input. We would return zero. Because for sure there would be no islands in this case. But we could also have this as an input. So what I'm thinking here, is we start at the first element. That's a one. So, I'll probably want some sort of island counter variable and I'll initialize that there and increment it by one. And then I get to the zero. And that's not part of any islands. So, I don't need to do anything there and I get to this next element right here. So, that's actually part of the same island. So I need to keep track of what ones are part of the same island. Maybe what I could do is I could look at the elements on top, and on bottom, and then check if they're part of the same island. And then keep track of it somehow. Yeah that sthird approach but if you're looking at that first element for example there is another one below it but there's nothing above it. So what would you do in that case?? All right because since there is not a one above that we'd probably get some sort of runtime error. So first I would want to make sure to check that there's actually an element there before I check if it's a one or a zero. So I'm going to need to keep track of this somehow. Maybe this is some sort of data structure. Yeah, does this approach or the problem or anything you've been walking through your remind you of anything? Yeah, since this matrix can sort of be like represented as a graph. This might be a breath first search problem. So what I could do is when I find the first one, I can look at the elements around it and I can add those elements to Q. Check if those are ones and then if they're are ones I could set them to another number so they're marked as visited. And then I could keep going. Yeah that sounds really great. So it looks like his test cases came in handy when brainstorming his solution. Sure did. He was able to use test cases to inch his way towards developing a solution. Something else I noticed was that he had great communication with the interviewer. He took the interviewer's cues to guide him towards the correct solution. Yeah a big misconception that future interviewees can have is they think that their interviewer isn't on their side. But really the interviewer is there to help you and being able to take in their feedback demonstrates your teamwork skills. Not to mention it helps you solve the problem. Sometimes in an interviewee might get so nervous that he or she just completely ignores the interviewer's suggestions. This can be a big turn off for the interviewers. So what if the interviewee was still stuck after looking at his test cases and edge cases? Well, one thing the interviewee mentioned was that this problem reminded him of the breath for a search algorithm. Often if an interviewees is stuck on a certain portion of the problem they can begin to brainstorm algorithms and data structures. The interviewee may have noticed that a graph can be represented as a matrix and that a breath first search algorithm would fit here. Okay, so when I'm looking at every element in this matrix, I'm looking at least once. So I'm thinking the runtime will be n times m, where n is the number of rows and m is the number of columns. Yeah, that sounds like a good analysis. Well, this seems like the most optimal solution. So I think I'm going to jump in and start coding now Awesome let's do it. Hear the interviewee analyze the steps to his algorithm and calculated the Big O runtime. Exactly. This is an important step because he may realize that his solution is not optimal while analyzing run time but in this case it actually was optimal. So I'm going to call my main function Island Counter and I'm taking in the matrix here. Now what I first want to account for is the none. So if my matrix is equal to none or if it is equal to the anti matrix. [BLANK_AUDIO] In this case I just want to return zero right way. [BLANK_AUDIO] Now if this is not the case, I'm just going to initialize my islands variable to 0, which is the number of islands. And I'm going to want to do two for loops to loop through this. So I want to know the number of rows and columns. [BLANK_AUDIO] So this is actually going to be vice versa here. Okay, so I'm going to write my for loop. [BLANK_AUDIO] Now when I get here, I want to check if the element is equal to one. [BLANK_AUDIO] And if it is, I'm going to increment my islands count. [BLANK_AUDIO] So now what I need to do here, is I need to do the breadth first search I was talking about before. So I'm just going to have a helper method to do that. [BLANK_AUDIO] Call it find points of island. [BLANK_AUDIO] So I'm going to go ahead and define that down here. OK. [BLANK_AUDIO] So the first thing I want to do is I want to initialize a queue. [BLANK_AUDIO] Why did you choose a queue? Since we're using a breadth first search here, I want to be able to use a queue so that I can get the first element in the-. When I add an element to the queue, I want to be able to get the first element that I added. Yeah, that makes perfect sense. And I actually need to import this method here and this is from the collections framework. [BLANK_AUDIO] That's a good catch. [BLANK_AUDIO] And then the first thing I'm going to do is I'm going to append my Indexes here so I know the locations. So I'm going to pin these to my queue. [BLANK_AUDIO] And now I'm just doing a breadth first search here, so while it's empty, we're going to keep searching for other things around. [BLANK_AUDIO] So while the length of the queue [BLANK_AUDIO] Does not equal 0. [BLANK_AUDIO] So what I want to do here is I want to look at the elements around it. So you mentioned before that I needed to check if they were actually there. So I'm going to write a helper method to do that, too. [BLANK_AUDIO] Okay, so now that I'm checking if this is empty, I want to do something with this element here. But it looks like I'm running out of space, so I'm just going to draw a little arrow over here. And just cut this little section off here that I'm going to be using to finish this method here. So what I want to do is I want to be able to get this element here from-, let me make sure to go up a little bit higher. So I want to pop this element off the cue. And then I want to be able to access these two variables, so I'm just going to set them to a new one. [BLANK_AUDIO] Okay. So now that I'm here, I want to check if this element is equal to one. [BLANK_AUDIO] And if it is, I want to set it to two as I mentioned before. [BLANK_AUDIO] So I'm going to use a helper method right here to check if these elements that are next to it, actually next to it. And then if they are, I'm going to add it to the queue. So, I'm going to call that- [BLANK_AUDIO] Set up more space here. So this method is going to be called append [BLANK_AUDIO] If. And I'm passing in the queue. And I also need [BLANK_AUDIO] The row, column, and both of these coordinates here. Okay. So what I want to check here is if the x value is greater than or equal to 0, or-. I mean not or. And. [BLANK_AUDIO] It is less than the number of columns, and if y is greater than or equal to 0, and if y is less than the number of rows. And then if all those conditions hold true, what I'm going to want to do is append this to the queue. [BLANK_AUDIO] there, and so I'm going to want to append this right here. And just to make sure I'm appending it the right way so I access the first element, I want it right here. So my helper method's going to work. So after I do all of that there, method here. I'm running out of space right here. So all this information here is just basically I'm calling this method on all the elements around it. So I'm going to append [BLANK_AUDIO] If. And so what I need to do is pass the queue. Rows, columns, and what I'm going to do is x plus 1 and y. So it's going to be the elements to the right of it. [BLANK_AUDIO] And then this is going to be the element below it. [BLANK_AUDIO] So left. [BLANK_AUDIO] And on top. That looks like it. Well, that went pretty smoothly. Yes it did. After going through all the necessary steps before coding the solution, the actual coding is a lot easier than if he were to have just jumped in and coded from the beginning. The interviewee also made sure to let the interviewer know what he was doing when he was coding. But he didn't read out all of his code as he was coding. We often see interviewees just reading out their code as they code which can get annoying and does not really build understanding with the interviewer. Yep. This interview, we focused more on explaining the purpose of the code while he was writing. By talking out loud, you give your interviewer the chance to jump in and help if your logic is wrong. You also proved to your interviewer that you're actually thinking through all of your code. If you just write code you might be spitting out a memorized answer. Okay, so I'm looking here to see if I have any errors. It looks like I forgot braces here for dequeue. So I'm calling that method correctly now. So now I'm just going to go ahead and go through my test cases that I mentioned before. I'm just going to put them in this little box here and we had our None, our empty brackets. So we go through these, zero returns zero immediately, so that's working, and what about when we actually have a working test case? That goes through most of this code here. [BLANK_AUDIO] Okay, so I go through, so that one's a 0. Checking the length. So, this would be set to 3. And this would be 2. So, going through every element. So, I find my first one. I'm incrementing my island count, so now that's going to be equal to one. And I got my FindPartsOfIsland method here, creating my q, appending those values and then I have my breadth first search here that I'm going through. And so for this instance, there is no one to the right or on the bottom of it, so I won't have to do this, and then I go to the next one. Sent zero, so we don't do anything, we just go through the loop. One, okay, so I'm going to increment my island count again it's now going to be two. So then I go through that and then I find my parts of island, so there actually are ones here, so I'm going to go ahead and add those, then I add those there. And that looks all good, and so this one, this one actually get set to 2 there. And this is going to get set to 2 and now I'm adding both of these to the queue to look at them. And we're not appending any this up front, I'm only appending that to the queue. So I'm looking through there, there's nothing else and so basically it's going to be setting these to 2, and that's, and then I'd probably just visit every element in the array. There's no more ones, so that would be end of it. But it looks like I forgot to return the number of islands here. [BLANK_AUDIO] So it looks like it works. [BLANK_AUDIO] Well it looks like all of his code worked. Not only is this fellow handsome but he sure is intelligent. The debugging process is often overlooked but a very important part of the algorithm interview. Not only does this show the interviewer that the candidate checks their code, it may even help the candidate find mistakes that they missed the first time they were coding their solution. It's easy to miss a semicolon here and there if a candidate were coding in some language like Java, or forget to return a result. Well that was an awesome interview. In the interview we followed the seven step process, and made sure to not skip a step. If you're enrolled in Nanodegree, make sure to check out our career resource center. Where you can find further resources to help you prepare for your interviews. Don't forget about the career projects we offer in the Nanodegree. Where students can get their resume reviewed, their LinkedIn polished, their cover letter critiqued, their GitHub graded, and their interviews improved. You've done a fantastic job making it through our interview course. But the more practice you have the better chance you'll perform better and get the job. You really should be proud of all you've accomplished so far. Make sure to approach your interviews with a positive mindset, and don't give up. If there's one thing that you take away from this course, it should be that data structures have rather uncreative names. Trees are data structures that, well, look like trees. A tree starts from a place called a root and you add data to it called branches. A big tree will fan out with lots of different branches in lots of different directions, just like a real tree. There are plenty of other tree terms related to these data structures. They have leaves, a collection of trees is called a forest, etc. Try to keep a straight face as we dive in and see what trees are all about. Trees have a lot of properties that make them useful. However, a tree is really just an extension of a linked list. A linked list has one element at the front and a next pointer to another element, and so on. A tree is similar. The first element is called the root. Instead of having just one next element, a tree can have several. A linked list is often drawn horizontally, but a tree is normally drawn vertically like this. Just like a linked list, each element on a tree contains some data. The individual elements in a tree that contain values are often called nodes. Let's add a few more constraints on what we call a tree. First of all, a tree must be completely connected. That means, if you're starting from the root, there must be some way to reach every node in the tree. Here is an example of unconnected nodes. So this definitely isn't a tree. Next, there must not be any cycles in the tree. A cycle occurs when there's a way for you to encounter the same node twice. For example, in this tree there is a connection back to the root. If I start at the root, then I'll get back to the root. The same goes for this tree. If you ignore the arrows, you can still go in a circle. This counts as a cycle. So beware. Okay, I'm going to throw some terms at you, get ready. You can describe a tree in terms of levels, or how many connections it takes to reach the root plus one. The root is level 1, and nodes directly connected to it are on level 2. Nodes in a tree are often described as having a parent child relationship. In this system, a node at a lower level is a parent, and the node connected to it at a higher level is a child. A node in the middle can be both a parent and a child, it depends what it's being compared to. Keep in mind though this isn't a real family tree where children all have two parents. Here, children are only allowed to have one parent. If a parent has multiple children, those children are considered siblings of each other. You can talk about ancestry in a similar way. A node at a lower level can be called an ancestor of a node at a higher level which is the descendant. The nodes at the end that don't have any children are called leaves or external nodes. A parent node is called an internal node. You can call connections edges, and a group of connections taken together as a path. The height of a node is the number of edges between it and the furthest leaf on the tree. A leaf has a height of zero but the parent of a leaf has a height of one. The height of a tree overall is just the height of the root node. On the flip side, the depth of a node is the number of edges to the root. Height and depth should move inversely. If a node is closer to a leaf then it's further from the root. [SOUND] Glad that's over with. Time to learn some more about trees. When we looked at list data structures, traversal was pretty straightforward. We just needed to go through the list in order to make sure we looked at every element. Trees aren't linear, so there's no clear way to traverse through everything. Suppose we start at the root, do we go left first or right first? Should we completely traverse one subtree or one section of the tree including a parent and all of its descendants or traverse everything at the same level first? Traversal in a tree is a bit more complicated but it's equally important. We can't search or sort elements unless we have a way to make sure we can visit all elements first. There are two different broad approaches to treat traversal. One is called depth-first search or DFS for short. In DFS, the philosophy is if there are children nodes to explore, exploring them is definitely the priority. We'll take a more in-depth look at what that means in a minute. The alternative is called breadth-first search or BFS. In BFS, the priority is visiting every node on the same level we're currently on before visiting child nodes. BFS and DFS are kind of vaguely defined since we can apply their principles but actually traverse the tree in several different ways. For trees, a level order traversal is a BFS with a more exact algorithm to implement. A level order traversal is exactly what it sounds like. Start at the root then visit its children on the second level then all of their children on the third level until you've visited every single leaf. By convention, we start on the left most side of the level and move right. This traversal is definitely a BFS. We're visiting all nodes at the same level before moving on to child nodes. There are several different approaches to DFS and trees. First, we'll talk about pre-order traversals. To me, the pre says, check off a node as you see it before you traverse any further in the tree. There are other methods of traversal in which you check off nodes after you've seen their children. So this rule is really important to keep in mind. We start at the route and immediately check off that we've seen it. Next, we pick one of the children. Normally the left one by convention, and check it off too. We would continue traversing down the left most nodes until we had a leaf. We check off the leaf and from there go back up to the parent. Now we can traverse to the right child and check it off too. We're done with this subtree, so we can travel back up to the root and look at its right child. We do the same steps until we've seen everything. Cool, were done! This might seem intuitive, but it'll make more sense once we compare it against a different type of traversal like an in-order traversal. The trick here is that we're moving through the nodes in the same order, since this is still a DFS and we need to explore all children first. However, this time we're going to check off the nodes in a different order. We're only going to check off a node when we've seen its left child and come back to it. All right, let's see what this looks like. Again, we start at the root, since we haven't seen the left child yet. We have to keep traversing down until we hit a leaf. We check off the leaf and move up to the parent. Because we've seen the left child now we can actually check off the parent this time. We move on to this right node, which has no children, so we can check it off too. We go back up to the root and repeat all of this on the right side until we're done. Why is this called in-order? Well, we pretty much went through the nodes in order from the left most to the right. We had to move up and down a lot, but if you just look strictly at left to right, we actually did go through everything in order. Lastly, we have the post-order traversal. This time we won't be able to check off a node until we've seen all of its descendants or we visited both of its children and returned. Similar steps, we begin at the root, don't check it off, but continue down to a leaf. We check off the leaf and move to the parent. This time we don't check off the parent though, we just move on to the right node. Once we've checked off the right child we can go back up to the parent and finally check it off too. Again, we'll skip over the root node and just move all the way down to the right. Once everything there is good we can move back up to the root and get it. Binary trees are simply trees for where parents have at most two children. This means nodes can have zero, one, or two children. Those children might even be null, but that's okay. Let's see what search looks like in a binary tree. We could start off by using any of the traversal algorithms to go through the tree. Because there's no real order to the elements, I need to go through every single element in the tree if the value I'm looking for doesn't exist. There are no cool tricks here, so we're stuck with a linear time search. A delete operation often starts out with a search since you need to find the node you want to delete. If you're deleting a leaf, you can simply delete it and move on. However, if you delete an internal node, you'll suddenly have a gap in the tree. If the node you deleted only has one child, you can actually just take it out move the child up and attach it to the old node's parent. If you're trying to get rid of a node that has two children, you have a few options. You could promote the child up just like you did before. Of course, it can't always be that easy. What if both children also have two children? In the worst case, you'll need to keep traversing down the sub tree until you hit a leaf. Since there is no real order requirement here, you can just put the leaf where your deleted node was without a problem. Since there's a search involved and some additional work to shift around the elements after deletion, the run time is linear. Inserting an element into our tree, when it has no order is relatively easy. We'll just talker node onto another node. Maybe it's a leaf or maybe it's a parent with only one child. We only need to make sure that we're being the two children rule. We're given the root at the beginning and we'll need to keep moving down the tree until we find an open spot. How long will it take to find an open spot? The worst case is that we travel down the longest path until we find the farthest leaf. In that case we'll travel through the number of nodes equal to the height of the tree. But what is the height of a binary tree? Like we did with our sorting algorithms, let's look at some examples and reason through it. Here are two different trees. We can call these perfect trees, since every node, except the leaves on the last level, has two children. It looks like this tree with 3 nodes has 2 levels, and this one has 7 nodes and 1, 2, 3 levels. As the tree grows bigger each new level has the capacity to hold a number of nodes equivalent to a power of 2. Mull over that for a second. Each node can have 2 children. So each new level can have twice as many nodes as the one before it. Since we've gone back to talking about powers of 2, your mind should jump to log (n). Does that apply here? Let's take a look. We see the same pattern that we saw in binary search. The number of levels increases near a power of two, and now, we have a good visualization to understand why. At every level, we're adding roughly twice as many elements. Every time we need to add a new level or in a binary search, divider our array another time, it's because we're giving ourselves the space to allow for about twice as many elements. Remember that we're adding a power of two at each level. So when we count up the nodes overall it won't be exactly a power of two. Having three levels doesn't mean an overall node count of eight. But it does mean that the fourth level, itself, will have eight elements in it. Trees inherently aren't really organized. When we use a tree, we know what the overall structure looks like, but we don't know where a specific element will be. It turns out that we can add some rules about the ordering of elements to our trees to accomplish certain tasks really fast. In the coming videos, we'll talk about some common tree types with specific rules regarding the ordering of elements. As we go, don't let yourself get so caught up in understanding the types of trees that you forget the big picture. When would you want to use these structures? What tasks would they speed up? What are the pros and cons of each approach? There's a more specific type of binary tree called a binary search tree. A binary search tree, or BST, is definitely a binary tree. That is, every parent node has at most two children. But just as an array is a type of list, a BST is just a type of binary tree. There's a specific rule for how the values associated with each node are arranged. BSTs are sorted so every value on the left of a particular node is smaller than it and every value on the right of a particular node is larger than it. Because BSTs have this structure, we can do operations like search, insert, and delete pretty quickly. Let's say we want to find 7. We would start at the root. We see 7 is greater than 5 so we go right next. Since our next element is 8, we know our next move is left. And in just a few steps we found it. You may have noticed that we didn't need to search every element to figure out where the one I was looking for belonged. We just had to look at one value in each level of the tree, and then we can make a decision with just a comparison to my element. That means the run time of a search on a BST is just the height of the tree, which we proved was log(n) when we learned about inserting. Inserting in a binary tree is pretty much the same process. You start at the top and you can make quick decisions about where to look at each step by comparing to the element you want to add. Eventually you'll hit that open spot in the tree. As long as you compared your element correctly at each step, you can put your new element there and not violate the core BST properties. Deleting is yet again a bit complicated. However, it's complicated in the same way that it was for the generic binary tree, so those solutions for different cases still apply. Binary search trees are nice to look at when they're full. But there's no rule that states that BSTs need to look that way. This is a completely legitimate BST, but it looks very different. There are two or fewer children for every parent. And everything on the right is bigger than its parent. We call this an unbalanced binary tree since the distribution of nodes is skewed to the right side. A weird structure like this could start at the root, but it could also take place in one of the subtrees. You can also consider this to be the worst case scenario for a BST. When the tree looks like this, search, insert and delete all actually take linear time in the worst-case. Here, the worst-case would be searching through every element from the root to the leaf. Thus, the average case for these operations is big O(n). And the worst-case for all of them is O(n). A heap is another specific type of tree with some of its own additional rules. In a heap, elements arranged in increasing or decreasing order such that the root element is either the maximum or minimum value in the tree. There are two different types of heaps. Max heaps and min heaps that capture those two situations. In a max heap, a parent must always have a greater value than its child so the root ends up being the biggest element. The opposite is true of min heaps, a parent has a lower value than its child so the root is the minimum element. Heaps don't need to be binary trees, so parents can have any number of children. Operations like search, insert and delete can vary a lot based on the type of heap we're discussing. Here, I'm going to focus on a max binary heap. We're going to keep the two children rule and the root will be the maximum element. In addition, a binary heap must be a complete tree, meaning all levels except the last one are completely full. If the last level isn't totally full, values are added from left to right. The right most leaf will be empty until the whole row has been filled. In this heap, a function that gets the maximum value, also called peek, happens in constant time. Let's see what search looks like. Should we start our search by going to the left or the right? In a BST, we knew which direction to go at each step by doing comparisons. Here there's no guarantee either way. Thus, searching ends up being a linear time operation since normally, we can't rely on tricks and we'll end up searching the entire tree. One thing to note is that we can actually use the maxi properties to our advantage in a search. For example, we can quit our search immediately if the element we're searching for is bigger than the root. In general, if our node value is bigger than the one we're comparing to, we don't need to check anything in its sub-tree since we know that it's the biggest. The worst case remains but in the average case, we don't actually need to search every element. The average case is closer to n over two but that's still approximated to linear time. Next let's try inserting an element. We could take the approach we used with BST's, start at the root then move down the tree one level at a time and do comparisons until we find an open spot. However, if our element is bigger than most of the parent nodes or even the root we'll need to shuffle the tree around a lot. As such, we take a different approach to inserting here. We can stick the new element in the next open spot in the tree. Then we heapify. Heapify is the operation in which we reorder the tree based on the heap property. Since we care that our parent element is bigger than its child, we just need to keep comparing our new element with its parent and swapping them when the child is bigger. We can take a similar approach to an extract operation where the root is removed from the tree. We stick the rightmost leaf in the root spot then just compare it to its children and swap where necessary. The runtime of insert and delete, a more general case of extract ends up being O of log in the worst case. Ultimately the worst case would involve moving an element all the way up or down the tree and would roughly be as many operations as the height of the tree. Another thing I want to discuss is the implementation of a heap. Though heaps are represented as trees, they are actually often stored as arrays. How is that possible? Well, since we know how many children each parent has, two, and thus how many nodes will be at each level, we can use a little math to figure out where the next node will fall in the array and then traverse the tree. Let's convert this sorted array into a tree. We know that the first element is going to be the root since it's the biggest. The next two elements in the array are the children of the root. By convention I insert them from left to right. However, that doesn't really matter as long as they're on the second level. Again, each level on the tree is twice as big as the one before it, so we know the next level has four elements. We can do this programmatically by tracking the size of each level in a variable, then doubling it each time we move to a new level. And everything left over fills in the left side of the last level. Now you're done. Just a quick note. Not every array can be represented as a heap. This one could because it was sorted in descending order. In general, the numbers need to be in an order that will make sense on a heap. Storing our data in an array can save us some space. If we use an array, we just need to store the node value and index in the array slot. However, if we created some kind of node object for a tree, each would need pointers to children and parents, so we need to store values and a bunch of pointers for every element. Since the array saves us the pointers, it saves us space. In the tree world, a balanced tree has nodes condensed to only a few levels, while an unbalanced tree has nodes spread out among many levels. The most extreme type of unbalanced tree is really just a linked list, where every node has only one child. By that definition, a self-balancing tree is one that tries to minimize the number of levels that it uses. It does some algorithm during insertion and deletion to keep itself balanced, and the nodes themselves might have some additional properties. The most common example is a red-black tree, which is an extension of a binary search tree. As is painfully common in computer science, the name turns out to be very straightforward. In this type of tree, nodes are assigned an additional color property where the values must be either red or black. The use of color as red or black is just a convention. You really just need a way to distinguish between two different types of nodes. The second property of a red-black tree is the existence of null leaf nodes. Every node in your tree that doesn't otherwise have two leaves must have null children. As you can see, all null leaf nodes must be colored black. Next, if a node is red, both of its children must be black. There's an additional optional rule, the root node must be black. Lastly, for the rule that makes these trees actually useful, every path from a node to its descendant null nodes must contain the same number of black nodes. We'll see these rules in action when we talk about inserting. For now, know that these rules and the last one in particular assure that the tree never gets too unbalanced. Inserting a new node is where the magic happens. There are several different states of the tree and the node you're inserting that require different courses of action. Remember that your resulting tree needs to follow both the red-black tree and BST rules. There's a lot of information here. Your first goal should be to watch and get a sense of how red-black trees work. You can rewatch the video later to actually memorize the details. One overall rule of insertion is that you should try to insert a node as a red node, which from here on out I'll represent as both red and square to help with differentiation, and then change its color as needed. The first situation is when you're inserting the first node into the tree. Since it's the root, you can change the color to black if you're adhering to the root must be black rule. Otherwise you'll have nothing to do. If the new parent node is black, you don't need to do any work. Since you're adding a red node, you haven't upset the balance of black nodes in any path or violated any of the other rules. Now if the parent is red, there are several cases with more complicated solutions. If the parent and its sibling are both red, then they should be changed to black. And their parent, the grandparent, let's say, of the node you're inserting, becomes red. We switch the node colors in this way to maintain the number of black nodes in a given path. Look through all the paths in this tree. The number of black nodes in each path is still two. The biggest problem here is that we could have violated another property by changing the grandparent. We can just treat the grandparent as a newly inserted node and change it or its ancestors according to the same cases and rules. Here we just treat it like Case 1 and change it back to black since we still want the route to be black. In cases 4 and 5, your node's parent is red and its sibling is black. In both, you'll need to perform a rotation. In a rotation, you shift a group of nodes around in a way that changes the structure of the tree, but not the order of the nodes. Keep in mind that this is still a BST, so, we need to keep our elements in strict order. In case 4, our red node and its red parent are not on the same side of their parents. Our node is a right child and its parent is a left child. Here we will perform a left rotation since the nodes shift one place to the left while maintaining their order. At this point, we have a set up that looks exactly like case 5, where both the red node and its red parent are on the same side of their parents. Here, that's the left. We'll do a right rotation here, but this time, involving the grandparent and both of its children. We need to swap the colors of these two nodes as well. Ta-da! We have rearranged the nodes without changing the number of black nodes in any path. Those are all of the cases that could arise in insertion. Again, we just needed to do some clever rearranging to keep our red black tree and BST properties satisfied. In doing the rotations, we kept any one sub-tree from getting much larger than the others. Insert, like search and delete, is log of n in the average and worst cases. BSTs were o of n in the worst case because they could be unbalanced. Because we're careful about staying balanced here, the run time won't be that large.