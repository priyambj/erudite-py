For this problem, suppose an eavesdropper, Eve, knows Alice is sending Bob a message m, which is in the set {00, 01, 10, 11} using a one-time pad where the key, k, is a perfectly random key unknown to Eve and the distribution of the messages is uniform--each message is equally likely What is the conditional probability that m = 01 given that Eve intercepts the ciphertext 01. Give your answer to two decimal places. For this question, we have the same information that we had in the first problem. Additionally, suppose Eve learns that Alice generated the key k using a biased random number generator that outputs 0 with a 52% probability. What is the conditional probability now that m = 1 given that Eve intercepts the ciphertext 01? Submit your answer with four decimal places. This question asks about perfect ciphers. Assume that m1 and m2 are messages selected from M, the set of all possible messages. You should not assume that all messages are equally likely. c is a ciphertext selected randomly from the set of all possible ciphertexts. k1 and k2 are keys selected randomly from K, the set of keys. The question asks to check all statements that are true. If a cipher has fewer keys then possible messages, it is not a perfect cipher. AES is a perfect cipher. If the probability of m1 encrypted using the key k1 equals c is greater than the probability of m2 encrypted using the key k1 equals c, then E--the encryption function--must not be a perfect cipher. The last statement--if E is ap perfect cipher, then the probability of m1 encrypted using k1 equals c is equal to the probability of m1 encrypted using key k2 is equal to c. This next question is about an application of symmetric ciphers. To exploit the BEAST vulnerability in HTTPS, suppose an attacker has intercepted the ciphertext, using 4-bit blocks. C0, the first block of the intercepted ciphertext, is 1000 as the result of encrypting m0, the first block in the message. C1 is the result of encrypting the second block of the message, and C2 the result of encrypting the third block of the message. Our attacker wants to find if m1, the second block in the intercepted message is equal to 1010. The question is what should the attacker use as the target value for m0', which is the first block and the next response. Your answer should be 4 bits long. This question is about key distribution. Suppose Alice and Bob execute the Diffie-Helman protocol but pick a value for g that is not a primitive root of q. Which of the following could happen? Alice and Bob could produce different keys. The generated key would be more vulnerable to an eavesdropper. The number of possible keys that could be generated would be smaller than q. Or an in-the-middle attacker would be more powerful than if g is a primitive root. Check all that apply. For this problem, let M be a two-digit number. The value of M is encrypted using RSA and Alice's private key using no padding-- like this--given the values of Alice's public key and C the ciphertext in the instructor's comments below, what is M? We would like to add digital signatures to the course accomplishment certificates that would allow someone who knows KU,CA, the public key of our certificate authority, to validate that a certificate was generated by Udacity and earned by the person whose name is in the certificate. Assume m is the certificate string. For example, "Certificate of accomplishment with highest distinction earned by Alice Hacker in CS587." Assume E is a strong public key encryption algorithm, H is a cryptographic hash function, and r si a random nonce. Which of these schemes would allow Udacity to generate unforgable, verifiable, signed certificates. Check all that are valid. For this problem, we ask about the missing step in garbled circuits as covered in Unit 7. At the end of the garbled circuit protocol, the evaluator has the output wire labels x1 through xn. The generator knows the corresponding semantic values. For notation x10 is the wire label corresponding to a semantic value of 0 for wire 1 and x11 is the wire label corresponding to semantic value of 1 for wire 1. Suppose the generator want to reveal the result to the evaluator without revealing any other wire labels. Which of these five options would allow the evaluator to correctly determine the semantic value of the were label x1 and not reveal the other output wire label? Check all that apply. The bitcoin protocol relied on using proofs of work to provide scarcity in producing new blocks. For this question you are asked to order a list of tasks by how much expected work they require. Assume that H is a strong cryptographic hash function that produces 128 output bits from any length input. Comuting H(x) takes 1 unit of time. E indicates RSA encryption. K sub U is a known public key. But the corresponding private key is not known. Computing E(x) takes takes 1000 units of time. There are no memory limits, but the task has no access to precomputed values. Using those assumptions, order these by how much work they prove from the least expected work to the most expected work. For example, enter cebad if you think c requires the least amount of work and d the most amount of expected work. I here with Carson Knowle from Security Research Labs in Berlin. Carson has done a lot of interesting work on breaking real-world cryptosystems, including the GSM cryptosystem that's used to protect many mobile calls and SMS messages. He's going to give us a little demo of that and then explain how it works. I'd like to start with a motivating example of where this has been applied, cracking mobile phones--the 2G part of the phones we still use. Every phone today is still a 2G phone in some way, even though 3G and 4G now have been added to it. Breaking this, of course, allows you to snoop in on messages, phone calls, may allow you to put charges on somebody else's phones, and may allow you to track them, for instance, right? You can certainly get in a lot of trouble for misusing those powers. Yes, and we certainly want to stay away from that today, which is why we're only breaking this phone, my phone. The demo I want to show is intercepting a text message sent to this phone. It's something that is delivered to the phone in an encrypted form-- an encryption function from 20 years ago, but some of these are still solild. DES, for instance--TripleDES anyway. However, this one turns out to be weak and how weak we'll show now. For this I need a way of intercepting the phone communication with another phone. Of course, every phone can record phone traffic. Usually, they'll record traffic sent to their own number. If you slightly reprogram it, it'll just record traffic sent to any number or whatever number you specify. To stay in the legal side, we'll make it just intercept traffic off this one other phone today. This and a little bit of theory that you'll learn today is all that's needed to intercept a text message. Okay. You see these blue lines? These are all attempts of the cell to reach phones for whatever reason. Those are--we don't want to intercept them. Now you see the red lines. This is when it started sending data to the right phone. The one where we gave it-- So you set this up only to look at data that's sent to that particular number. &gt;&gt;Right. We could have put any number or just left it out to intercept it. That would put us on the wrong side of the law. You see after just over 2 seconds, it found the correct key. It also would show the from number. We blanked this out here. It did come from England though. Interesting service you're using there. And, of course, the texter of the SMS--the actual secret. All of this can be cracked in less than 3 seconds, apparently. While we are on this data, let me briefly show you what actually was exchanged between the phones, which will be interesting later to see why this was crackable. In particular, we'll ask the question, why was this predictable in parts. Some of the messages that you're intercepting you need to be able to guess what they are. That's ciphering, which is the old word for starred encryption. Then everything below here is encrypted. Now, a lot of this shouldn't be encrypted, at least it's not secret. For instance, the GPRS data connection list switched off. While you're receiving an SMS, apparently this phone is not supposed to also do data connections for just 1 second. Then the cell introduces itself, saying I am the cell that uses all of these frequencies. It did that as well before. You see up here the exact same message but unencrypted. Here's the borderline. &gt;&gt;So these messages are very predictable. Very predictable given one's encrypted and one's not. Then the phone responds, hey, I measured our communication, and I measured you to be this far away. This is for tweaking certain parameters--also not very secret, because probably the measurement report didn't change much from before the encryption. He goes on to exchange a few empty packets. In JSM you have to say something in certain time slots, and if you have nothing to say then you say you have nothing to say. Exactly. These are just empty. Again it measures. Again, empty, empty, empty, empty. And again, the cell introduces itself and again it measures. And again an empty and an empty frame. And only now the actual SMS is exchanged. The from number with the text--I won't go into it so we don't see the from number. The phone acknowledges the message as being received. These are the only two messages that I actually really needed. Only one of them should really be encrypted. This cell, again, introduces itself as if the connection wasn't over yet. And again, the phone measures. There's a couple more empty messages. The cell says, "Please stop using this channel. We are done." Just in case the phone hasn't gone away yet, sends more empty encrypted messages. Now, this is all the decrypted traffic after you've decrypted it, but before decrypting it you've got a pretty good guess of what was in it other than that one frame with the message. Precisely, each predictable message gives us some attack surface that we can exploit using methods that we want to discuss next. Great, well, let's hear about how you actually explain that. So in this real-world example of phone numbers and people, for instance if we didn't have the phone book yet we could start with every phone number and call every person and ask for their names. You'd go through a brute-force phase that may take a couple of months but once your're done with this you have this beautiful phone book or code book as they call it in Cryptography that allows you to do the second and third and forth and infinite number of look-ups in basically zero time. So that's the basic idea of an attack that then uses very little time in fact zero encryption functions are needed after the pre-computation phase but that invests storage, in this case the storage to store every possibility to achieve the faster computation. And that's the basic scheme that in this case is not practical because this amount of storage would equal Petabytes in fact hundreds of Petabytes and that storage is available to no hacker in the world. Google may have this much, but nobody wants to break GSM phone calls for cheap. So we need improvements on this scheme. Luckily there are many possible choices between this one extreme, the code book that uses a lot of storage and very little time and the other extreme, brute force that uses a lot of time but very little storage. And those we want to discuss next. Some trade-off points are achieved by a technique called Time Memory Trade-Offs. And that works as follows: We again start with a table of many of these keys. And for each of them again, apply the encryption function but not just once but several times. And of the resulting data set there of course will be of the same amount of samples again we want to have the same coverage. We throw away most everything, these things in the middle and only keep the first and the last column. Now this should be equivalent in coverage to a data set that in the simplest scheme took a lot more storage capacity, and here's how. You're looking up a message found in the last column, then clearly it works as before. You find the encrypted message in this last column and you look up the key through following the chain that led up to this message. So, after finding the message in the last column, you go to the matching entry of the first column and compute the encryption function, in this case twice. and that's the key you wanted. However, how do you find now a message that is in the middle that you didn't in fact store on the hard disk. For this you'll have to apply an encryption function first and then find it. This parameter three is configurable, of course. And to correct something of the size of the GSM 64-bit key we would want to choose a parameter more on the magnitude of a million. So this table becomes a million columns wide of which you only stored the first and the last. Now if this before took Petabytes and now you only have to store a millionth of it you're down to manageable Terabytes. And that's exactly where we want to be to launch this attack on today's computers. There is a large amount of academic work to solve these practicality problems two of which we want to look into to solve, exactly these two problems now. The first one is called Distinguished Points. Distinguished points work off this principle. You again start with a large list of keys and for each of these your start computing encryption functions, other than before though you're not computing a certain number of encryption functions but rather you're computing until you hit a certain distinguishing criteria. Often times the number of zero bits at the end of your output. Each of these computations are terminated very quickly or terminate very late so they vary in length, the chains. The number of zero bits, however, determines the average length. So instead of a square data set, so to say where you have a stable height and a stable width you get more a zigzag data set where each line is of different lengths but on average you will have the same coverage. Now this solves the hard disk bottleneck because you don't have to go through your computation and chainlink by chainlink look up on the hard disk. You know that nothing that doesn't have the right number of zeros at the end will ever be stored on the hard disk, so you keep computing up to let's say on average a million and only then you have to do one hard disk look-up. So this scheme is clearly superior to the first one in that it requires the same number of encryption functions it requires the same amount of storage but it only requires a single hard disk look-up. So really now you are at seconds Terabytes and for the hard disk for milliseconds. In solving the second bottleneck, the pre-computation expense that's a little bit more tricky and there's a technique needed that gives this whole project it's name, so-called Rainbow Tables. Rainbow tables look as follows: again, start with a long list of keys and you do the encryption function for each of these keys as before, and you go through all the colors of the rainbow in this case a million different colors with a million different encryption functions before terminating your chain. Whether or not you will need a high coverage very much depends on your attack scenario, of course. In GSM, for instance, as we saw before, there's a lot of different packets that are attackable so maybe your coverage for each single one of them can be very low and, in fact, in the project that we computed, the coverage is much smaller than 1%. That's still enough to track each transaction with a probability much above 80%. So, the Rainbow Table helped us to some degree but we actually mixed it with Distinguished Points to kind of mix the benefits of both. [Carson] If we treat an encryption function--as the one in GSM--as something that takes a message, produces an encrypted message, and it consumes a key to perform that. There are different ways in which you can make this resistant against time-memory trails. Three different mitigations. First, you can make the message unpredictable. Sounds obvious. Why would you encrypt something predictable? But a lot of systems do just encrypt a whole stream of data, some of which is encrypted. Something is worth protecting. GSM mixes the two. [Dave] All of these attacks you know some--because of the GSM protocol-- there are some messages that are very easy to guess. &gt;&gt; [Carson] Exactly. [Dave] And you know what they are and can you those in this attack. To exchange a text message, for instance, between the base station and your phone, somewhere between 20 and 40 messages are exchanged, only one of which has the actual text message content. Another one may have some unpredictable set of information. All the rest is just management information--encrypted though-- and that management information is usually-- [Dave] Does it make sense as one solution to not encrypt the management information. [Carson] Clear, yeah. There's a standard extension that suggests exactly that. But basically the whole ecosystem would have to change to accommodate for that. Currently, it's just the encryption is switched on at some point and everything thereafter is encrypted, be it predictable or not. There are measures, though, to make predictable messages more randomized to create some wiggle room for how you send a message. That's currently being deployed--not in all networks, though. I've not seen--one of the two American networks I don't think has it. You need to know the base station is understanding if you change the protocol like that. The base station would have to change to go all the way of making all messages unpredictable, but even changing the base station controllers, bigger installations that control dozens of base stations that control dozens of base stations would do the job, and those are usually software updateable, because they have to accommodate for usually faster connections all the time. Now, more secure connections that could that, too, hasn't been seen too much yet. We do track that on gsmmap.org. The second way, of course, of making this system uncorrectable is to decrease the key size sufficiently so that none of these tradeoffs leads to practical values anymore. That is used, for instance, in 3G and 4G networks most of the time. Then a third way of protecting against these types of attacks that assume, if you predict a message you have a direct mapping between message out and key, is to introduce an auxiliary input. It's usually called either a "nonce" for random number or initialization vector. GSM, in fact, does have an initialization vector, so you would think that it breaks the correlation between key and message. Now you'd have to compute your table separately for every nonce. This is sort of like adding SALT to passwords. Exactly. This is the exact same system as adding SALT. This has been included in GSM. However, the nonce is only effective if the attacker has to go for the key to break the system. There are different encryption functions that are attackable with time-memory trade-offs in different ways, in particular for block ciphers. This protection measure works out of the box, because the block cipher consumes the key, crunches it in complex ways and outputs the message. Unlike those, stream ciphers take the key, do once more computation, do the same computation again, do the same computation again-- in the case of GSM 100 times--and then outputs the message. It becomes a function that's consistent over very many smaller functions and the nonce is, of course, only input at the very beginning. To make this now resistant against time-memory tradeoff their intermediate states--those results of the first round, second round, and so forth to the 100th round--have to be a non-attackable time-memory tradeoff. That sometimes is achieved by a large, internal state of the function. For instance, the GPRS functions or etch function--2.5G networks-- they also consume a 64-bit key, but they have an internal state of at least 96 bits. That is a protection, because now the attacker has to go for the key after all. However, if you use a small internal state, then the attacker can go to attack that internal state and GSM falls exactly in this category, and then the countermeasure doesn't work anymore. So the internal state is still 64 bits. &gt;&gt;Exactly. &gt;&gt;Combining the key and the nonce. Yeah. Message in, the key, and the message out as well as the internal state-- everything is 64 bits in GSM. Of course, adding a large internal state doesn't come at a great cost. It takes a few more flipflops and hyper-implementation or slightly larger words in the software implementation. This didn't prevent the attack that you described, but the attack you showed us would actually be better than this. To do the n lookups would have taken hours. You were able to show it working in a manner of seconds or minutes. To explain why JSM is more vulnerable, we have to go a little bit into this discussion we started earlier on the hundred little functions instead of one big function-- what makes this a stream cipher. This stream cipher consumes in the beginning both a key and IV, as we've seen, but does a computation 100 times to produce an internal state that is then mapped onto the output that's then XORed with the message is basically the output message. Interesting is this part, though. Each of these links is such a small computation that you can easily reverse it. It is nonlinear, meaning you cannot really reverse it without 100% certainty, but it's such a small computation that you can just kind of brute force the input space. [Dave] Okay. &gt;&gt;[Carson] Right? So little-- [Carson] But the input space is small. [Carson] Yeah, just a few bits change in each of these steps, so you can just go through all the possibilities of input space and see which ones map back to your output. Right? Now, you may end up with several, so it's not always that only one of the inputs would have produced. That's exactly the nonlinear property. However, this may look as a complication at first but does, in fact, help the hacker because each of these also generates a key for some given IV that you assume. Well, at least some. Some may be dead ends. But at least in a lot of cases you can track one point that you store in your table back to different keys. &gt;&gt;[Dave] I see. Different starting key IV pairs lead to the same output. Exactly, and on average, for each value that you store in the table you have 13 keys that you find every time. Only one of them will be the correct one. Sometimes even all 13 are wrong. &gt;&gt;[Dave] I see. [Carson] But if you do find a key it will just be one of them. However, storing this point in the table is equivalent to storing 13 points in this table. [Dave] Right, and now each lookup is the equivalent of doing 13 lookups. [Carson] No, the lookup is actually the same effort, but the storage is decreased. This is basically an inherit encryption function for rainbow tables that this system comes with. We are 13 times more efficient with this encryption function than with a generic 64-bit encryption function. Thirteen times and such a tradeoff with all the different tweaks is the difference between hours and seconds. [Dave] You're shrinking the table by a factor of 10, which is making all the lookups less expensive. &gt;&gt;[Carson] Exactly. Basically we're having 13 times more storage, and the crate of storage is quadratic. We're more efficient as an approximation by this factor, and that's the difference between seconds and hours. That's why SMS can be snooped in seconds. Interviewer: So this is really interesting and very powerful, but it raises a lot of questions that certainly deal with intercepting people's cellular communication could be used for evil. Presumably you're not using it for evil. So what's the motivation for doing this and and kind of how can you do this in an ethical way? Carson: We're clearly not doing this to do evil ourselves. However, we do this because it is already used to do evil by others. For at least 10 years industrial-scale GSM cracking equipment has been available and there's been very little talk about the system so far. So this is an attempt to shine that light onto evil that is ongoing spying on citizens, spying on host countries from embassies spying in war zones on civilian populations that we want to uncover and the result of this discussion that we've been having very publicly was to GSM networks over the past two years is hopefully that these networks now implement protections against what they perceive mostly as a publicity threat. Because we are not actually doing anything evil but we say we could. It is the ultimate convincing argument that you can intercept the phones of even the phone company's executives. So the ethical part of hacking includes both not doing evil but convincing everybody to do the counter, in this case deploy technical countermeasures or at least to warn customers that cellphones are not as secure as companies would like them to be. It always hard to say what would have happened had we not done this but it may be time coincidental, it may be because of this research networks are starting to upgrade or have been upgraded these past years. More in newer networks, that is everywhere outside of the western world but also in Europe and hopefully the US soon will start rolling out countermeasures. Often times they find that these countermeasures are little more than configuration changes, software patches, things that are overdue for many years. Technology twenty years old that has been upgraded many times adding MMS and fast-internet connections and visual voice mail and all these things but the security hasn't been patched, even once. Same works for the T-Mobile network here. Same works against every network in Europe right now pretty much every network outside of Europe. A few have been patched now. The most secure network that we have seen recently was in Egypt. Given that it's now not just defending your customers from ongoing evil but also a possible publicity threat you're avoiding there's a lot to gain for the networks and at the same it doesn't cost them very much. So it just took a few research years to create the information base for them to act upon. Interviewer: Thanks very much, Carson, this has been really interesting and very cool what you showed us, thanks. Carson: Thank you. Welcome to Applied Cryptography. Cryptography is a branch of mathematics and computer science--that's a lot of fun--it's about using secrets to solve problems. In this class we're going to learn about some of the foundations of cryptography and we're also going to learn to use cryptography to solve problems in computing, like how to send messages securely, how to manage accounts on websites, and how to do things like perform computation where you can keep your data secret and still get the result of a function that depends on your data and some other data. The name cryptography has two parts: crypto comes from the Greek root for secret--to hide-- and cryptography's all about using secrets. The second part comes from the root meaning writing-- this is the same graphy that appears in telegraphy or photography. What we do in this course, and what we do in cryptography involves a lot more than just secret writing. It's really everything to do with secrets. So a better name for the course would be cryptology. Cryptography is used so frequently in practice, though, that it makes more sense to use the name cryptography, even though our concern is with the much larger goal of understanding how to use secrets in general. And cryptology is the science of secrets which is really what this course is all about. So that's enough to answer the first quiz, and the first quiz will check that you understand the definition of cryptology. So the question is, which of these involve cryptology? Check all of the answers that do. Opening a door--and I mean a traditional door, using a traditional key. Playing a game of poker. Logging into your account at Udacity, or doing a search using google.com, and I should be a little more specific for this last one, this assumes you're doing the search today, and you're already logged in to one of your Google accounts. The answer is all of these involve cryptography, and the reason for that is they all involve secrets. Let's go through each in turn. So the first is opening a door with a mechanical key. What's the secret there? Well, let's look at a key. So, the secret is actually the shape of the key. Now that I've shown you this key, there's no secret any more. If you can look at the ridges on the key, well, you know the secret. You can make a copy of the key and open whatever door this opens-- hopefully it's not something very important-- since I've just shown the key in a public video, and it's actually possible to do this from a fairly large distance. There was a group of students at University of California, San Diego, that showed that from taking a picture several hundred feet away you could learn enough about a key to create a replica that would open the same door. Playing poker certainly involves secrets. It wouldn't be much of a game if everyone could see everyone else's cards. Logging into your Udacity account involves entering an email and password, and I'm not going to actually type my password now, because--one of the things I hope you'll start to understand from this class is-- even if you don't see it because of the stars here, the fact that you can hear the audio of my keyboard would give you a pretty big clue about what my password is and the number of characters. And--unlike the key that I showed you-- this protects something really important so I'm not going to expose my password. We will talk, in one of the future units, about how to manage passwords correctly. This involves a lot of encryption--your password is a secret-- but you also want to manage passwords-- and check them--in a way that doesn't require storing those secrets. You don't want to store passwords in a way that could expose them easily, and we'll talk about that more in unit 2. The final option--doing a search using google.com-- is an interesting one. This actually changed about a year ago-- whether or not that involves encryption-- and, if you try it today, you'll note when you enter google.com what happens is, you actually get redirected to https://google.com and--you can see the lock in the browser-- this means it's using a protocol called TLS to encrypt all the traffic between you and Google. So--both the keywords that you're typing are not exposed on the internet-- as well as the search results, and we'll talk more about that in a later unit in the class that involves both asymmetric and symmetric encryption. There's lots of interesting things about the protocol to protect web traffic, that we'll talk about later in the class. So the key point is that secrets are all around us, that they're very useful for solving problems, you're using cryptology all the time, every day. The main things we're going to learn in this class: the first two units, we'll focus on symmetric cryptography and it's applications. Symmetric cryptography means both parties have the same key. So if we have two people: Alice and Bob-- in cryptography we almost always talk about Alice and Bob-- it's not clear why, but, they're useful people to talk about and they apparently have lots of secret messages to send between each other-- and they want to send messages. They want to know that they couldn't be intercepted by some eavesdropper listening on the channel. In symmetric cryptography we assume that they both start knowing the same key, and they can use that same key for both encryption and decryption, we'll see more of what that means soon in this unit. What we'll do in units 3 and 4 is introduce asymmetric cryptography, as well as some applications, and asymmetric cryptography is also known as public key cryptography. And the key difference between symmetric and asymmetric cryptography is that the keys used to encrypt and decrypt can be different. So, our message gets encrypted, it means there's some function that takes a message and a key produces some encrypted message--which we call ciphertext. In symmetric cryptography, the decryption uses the same key. In asymmetric cryptography, the decryption uses a different key, and if the keys are different and unrelated, that means you can reveal one of the keys without revealing the other key, and that's the power that asymmetric encryption gives us, and we'll see lots of interesting ways to use that. And in units 5 and 6, we'll look at protocols that use both symmetric and asymmetric cryptography to solve interesting problems. The reason most interesting protocols combine both of these is because asymmetric cryptography tends to be very expensive-- you need big keys, you need lots of mathematics-- to do asymmetric cryptography--lots of computation. Whereas symmetric cryptography can be very efficient. So that's why most interesting protocols-- like the one we saw for accessing a secure website-- involve both symmetric and asymmetric cryptography. We'll get into both the mathematics and the practice of implementing systems using cryptography, but there's one thing I want to really emphasize, and make sure that you understand. Implementing your own cryptography is very challenging, it's something that is very easy to do incorrectly. One example of that to keep in mind-- and this is an important thing-- so we often think about programs as black boxes with inputs and outputs, and our encryption functions will usually take a key in a message and produce some ciphertext, and the implementations we'll show in this class will be functionally correct, they'll produce the correct outputs. They won't be secure, and the reason they won't be secure is things aren't really black boxes when we use them. There's lots of other things that someone could observe about this. They could observe how long it takes. And when you can observe properties of a function--other than their functional behavior-- other than just their inputs and outputs-- that's called a side channel. An example of a side channel is timing. So, the time it takes to run this function could vary, it could depend on the message or the key. Most of the code that we're going to write in this class will have that property. We're not going to be adding complexity to our code to worry about things like side channels, but in terms of implementing things correctly, that's really important, and there are lots of ways that code could have side channels. It could affect what's in the cache of the processor, and that could be visible in some way, it could actually affect how much power your processor uses, and people have shown ways to break smart cards by measuring the power consumption as they do encryption and that gives you some insight into what the key is. So, this is just an example of the kinds of things that it's very hard to get right in implementing cryptography. These are things that, if you were building cryptography for any important use, you'd have to worry about. These are things that we are not going to worry about in most of the code that we write in this class so you shouldn't view these implementations as secure implementations. The only reason to implement your own crypto is for fun and learning. And that's what we hope you'll get from this class by doing it, but if you actually want to use cryptography to protect anything important, well-- you should use a library implementation that's been carefully vetted, that's been written in a way to worry about these kinds of issues like timing side channels, and that other people have looked at carefully enough to have some higher level of confidence that it's correct and secure. So I want to make sure everyone was paying careful attention to that last point, and we'll have a quiz: Should you use any code from this course to protect nuclear launch codes? The answers are no--check the correct answer. So I hope everyone got this quiz correctly. If you didn't, you'll be hearing from our lawyers soon. So the first main topic we'll introduce is symmetric cryptosystems. Symmetric means both encryption and decryption are done with the same key. I want to introduce some terminology that we'll use throughout the course. So we can think of encryption as a function, and it takes as input a plain text--this is the unencrypted message-- it outputs a ciphertext, and then our goal is to be able to send that ciphertext over an insecure channel. This could be a wireless network, this could be the internet, this could be a courier carrying your message, any channel where we can't trust the channel to be secure. We hope what comes out of the channel is the same ciphertext we put into it. Then that goes into a decryption function, and what comes out is the message that we sent it. Our cast of characters--we often talk about Alice sending the message, Bob receiving it. We also have malicious characters who might be listening in on this channel. One of those is called Eve--for eavesdropper--and she has really big ears and can hear the messages sent between Alice and Bob, but she only hears what's sent over the insecure channel, she doesn't have access to the plain text--that's the input to the encryption function-- or the plain text that comes out of the decryption function at the other end. But she can hear the ciphertext that's sent over the insecure channel. To think about this a little more precisely, the plain text is some message, and it's selected from a set of messages. So, M is the set of all possible messages, and M could be finite--for a given length, M is finite-- and we'll often be thinking about fixed-length messages, so there's a finite set of them, and M is some message selected from that set. The ciphertext is C--selected from the set of all possible ciphertexts, and the encryption and decryption functions are functions. The encryption function--as shown here-- is a function that goes from an element of M to an element of C. The decryption function goes from an element of C--a ciphertext--to an element of M. In order for Bob to receive the same message as the one Alice encrypted, we need this property--that the D function is an inverse of the M function. So we need it to be the case for all messages. The result of decryption the encryption of that message is the same as the message we started with. We'll talk about that a little more later, before I go on, I want to mention one very important principle in cryptography, which is known as Kerckhoff's Principle, and it goes back to a book that Kerckhoff wrote back in 1883, and the way Kerckhoff stated it was this--this was in a book he wrote in 1883 about military cryptography [speaking French], and I apologize for my French, but this loosely translates as, "The cipher must not depend on secrecy of the mechanism. It must not matter if it falls in the hands of the enemy." So, the reason for that--and it's important-- what he's focusing on is the mechanism here. So, if you think about these two functions we need-- we need encryption and decryption. Coming up with new functions--or new mechanisms-- in Kerckhoff's day these were mostly mechanical devices that you needed as part of the encryption process-- well, that's a lot of work. You don't want to have to keep inventing new encryption and decryption functions, and deciding they're secure and distributing them. What you want is some way that, even if your encryption and decryption functions are known, your cipher can still be secure, and the way to achieve that is to use a key. So instead of just having the message as input to the encryption function, the other input to the encryption is a key, and if it's a symmetric cryptosystem, that same key is the key that is needed in the decryption function as well. If the security relies only on keeping the key secret, well, then, we can make our encryption and decryption functions public, we can analyze them, we can put a lot of work into developing good encryption and decryption functions, and then the security depends only on keeping the key secret. If we think our key has been exposed, well, then we just need to come up with a new key, but we can keep using the same functions. If the function turns out to have a weakness in it, well, that's a much more serious problem. Then we need to develop a new encryption function and argue that that function is secure. So, this quiz is to check that you understand Kerckhoff's Principle and understand the terms that we've defined. So the question is: which parts of a cryptosystem must be kept secret? The choices are: Alice, the algorithm used for encryption, the algorithm used for decryption, the keys, and the ciphertext. Check all the parts that must be kept secret. The answer is: just the keys. This is the point of Kerckhoff's Principle, that we don't need to keep the encryption algorithm or the decryption algorithm secret, we want to make those public so they are carefully tested and understood, and analyzed by many people. We wanted only to keep this key secret. The point of the encryption system is to allow us to send the ciphertext over an insecure channel, so that's no longer going to be secret, and Alice is just a silly answer. So now that we have the key, we have three main things: we have a message in the spaces of all messages, we have a ciphertext in the spaces of all ciphertexts, and we have a key in the space of all possible keys. And we want our encryption function--we'll take a message and a key-- and map that to a ciphertext, and our decryption function will take a ciphertext and a key and map that to a message. And to be correct, we need to obtain the same message after decryption, we need to know--for all messages and keys--we have the property-- the decrypting--using that key, and I'm going to use the notation where I put the key as a subscript to the decryption function, instead of as an input, but that means the same thing as saying there are two inputs to decryption, one is the key, one is to input ciphertext, and the input ciphertext is the result of encrypting using that same key--that message--and we want to know for correctness that that value must always be the message that was input to the encryption function. Correctness, of course, is not enough. We also want security, and our security property, in an ideal sense, would be that the ciphertext reveals nothing at all about the key or the message. We're going to formalize this a little later, this unit. For now, let's think about that property informally, and what I'm going to ask you is a quiz, to see whether given functions satisfy our correctness property. So now it's time for a quiz to see that you understand the correctness property for a symmetric cipher. Which of the functions below satisfy the correctness property for a symmetric cipher? And we'll assume both the message space--the set of all possible messages-- and the key space--the set of all possible keys--are the natural numbers, so the whole numbers starting from 1. So, each choice is a pair of functions--the encryption function, and the decryption function. For the first choice, the encryption function adds the message and the key. For this decryption function, we subtract from the ciphertext, the key. For the second choice, the encryption function is the identity. The decryption function is the identity on the ciphertext. For the third choice, I am using the modulo operator, x modulo y is the remainder you get when you divide x by y, and modulo is a very useful operation in cryptography. We'll see that quite a bit later on this unit, as well as in later units. The answer is: the first two of these do satisfy the correctness property, the third one does not. And to understand why--well, let's remember what the correctness property is. That's that for any message, and any key, we have the property that when we encrypt the message with that key, and decrypt it with the same key, we get the message back. And this is what we need for our encryption to be decryptable. We need these functions to be inverses. So, for the first choice, the encryption function is M + K, the decryption function takes that input--which is now called C-- and subtracts K from it using the properties of arithmetic while this is indeed equal to M. For the second choice, the encryption function is the identity, that gives us M back. Certainly this does not provide the security properties that we need, we haven't hidden anything about the message. And then the decryption function is also the identity. Takes in M, gives us M back. So this satisfies correctness, certainly does not satisfy security. The third property doesn't satisfy the correctness property. And the easy way to see that is to observe that the output of this is a smaller set than the number of messages. So there--for some choices of message and key, this maps to the same value, and an example of that would be, if we choose K = 2, if M = 4, then the encryption of that message is equal to zero, because the remainder of dividing 4 by 2 is zero. But if we also choose to message 6, well then, the encryption with that same key of 6 is zero. Since two messages mapped to the same ciphertext, there's no way they could decrypt correctly, they can't both decrypt to different values, because decryption is just a function, it takes that input, it's going to produce the same output in both cases, in this case it's going to produce zero, it's not going to produce the message we need. So this is neither secure nor correct. But once it's not correct we know it's not useful. The first one, as it is, is not secure. But it's actually pretty close to being secure. And if we combine the first one with a modulo operator, and we never reuse keys, we'll end up with something that's perfectly secure. And that's what I'm going to talk about next. I'll introduce the first cipher that we'll talk about, and it's actually a cipher that is perfectly secure. So I'm going to introduce a cipher called the One-Time Pad, and this is actually a perfect cipher, and we'll define more precisely what that means, but it's a cipher that reveals no information at all about the key or the message, and it's actually the only one. And--you might be wondering--well, if we're starting right at the beginning of the class with a perfect cipher, what's there left to do? It's perfect, but it's very impractical. And the reason it's very impractical is because it requires huge keys. So to understand the One-Time Pad, the first thing we need to understand is the XOR function--which is a name for the exclusive or-- and it's often written as a plus with a circle around it. The truth table for XOR is this: the value of A XOR B is true if one of A or B--but only one of them--is true. So that's why we end up with this truth table. XOR has lots of useful properties that make it the favorite function of every cryptographer, and I'll see if you can figure out one of these on your own. So the question is: for any values X and Y, what is the value of x XOR y XOR x? So here are your choices: 0, x, y, or it depends on x. So the answer is always y, and this is why this is such a useful function in cryptography--if we XOR in the value of x twice, they cancel out. So the general property for any value x--x XOR x-- is equal to 0. We can see that from the truth table. The value of x could either be 0--and 0 XOR 0 is 0--or 1, and the value of 1 XOR 1 is 0, and that means that x XOR x is always 0. XOR is also associative and commutative, so that means if we have x XOR y XOR x that's equivalent to x XOR x XOR y which is also equivalent to y. And I've written this using x's and y's--if we change the names of our letters to M and C it becomes really clear why this is useful in cryptography. If we call x the key, then we have the key XOR the message--that's going to be the ciphertext-- and then the ciphertext XOR the key again gives us back the message. And what I've described is exactly the One-Time Pad. The key aspect is--why it's called One-Time--is that we can only use the key once. So if we do this one bit at a time, we need a new key for each bit. If we think of doing it for a longer message, well then we need a key long enough that we're XORing each bit with one bit of the key. So let's define that a little more precisely. So we're going to find our set of messages-- is strings of 0s and 1s--so we'll use bits and some fixed length. So n is number that gives us the maximum length of a message. Our message is selected from all binary strings of length n. Our key is also selected from the set of all binary strings of length n. And then to do encryption--our encryption function-- we're going to think of the message as being this sequence of bits and the key is also a sequence of bits. The result of our encryption is the ciphertext, which is a sequence of bits. So, length n, where the value of each ciphertext bit is equal to the XOR of the corresponding message bit and the corresponding key bit. So let's try an example. And for this example, I'm going to give you the ciphertext and the key and the message. So suppose our message is the string 'CS,' but our message space is in bits. Well, the first thing we need to do is to convert those strings to bits and we can do that in Python by using ord, that takes a one character string and turns it into a decimal number. And then we need to convert that decimal number into bits. Into a binary number. And we need to do this for each character in the string. We're going to convert it to a character, convert that to bits, and I'll show you the code for doing that, we'll leave the more interesting code for you to write. Here we're converting to bits. This is a fairly straightforward, but not the shortest way to do this. We're going to make an array of bits as our result for any decimal number if it's divisible by 2, we want to have a 0 at the beginning of a result. If it's not divisible by 2, that's going to be a 1. And then we divide the number by 2 as we go forward. So that's going to fill up all the places. We want our bits to be particular lengths, so we have a padding, and for all the characters, we'll use 7 bits. So we're going to pad the result with leading zeros until we get to that size. We can see this--so if we do ord we see that the number corresponding to the letter C is 67. If we convert that to bits--and we'll use 7 as our padding-- that gives us enough for 128 different values which is enough for the ASCII character values that we get back from ord. We can see those bits as a list, and we can see that a little more easily as a string using the display bits procedure that just turned that into a string. So now we want to convert more than one character. To do that, we have a string to bits procedure that goes through all the characters in the string, converting each one to bits using convert to bits, and concatenating those all together to the result. So now we can do string to bits. For our two-letter string, and now we get 14 bits as a result. So if that's our message, then the value of M is what we got there. So this is our message. There are 14 bits, n is 14. That means--to encrypt this using a One-Time Pad, we need a key that also has 14 bits. So let's pick our key, and--we're just going to make up a random key now. Actually finding random values is very important in cryptography, and we'll talk about that in a later unit, but for now let's just make one up. So suppose this is our key. Then the ciphertext is just the result of XOR in each message bit with the corresponding key bit. So that's our ciphertext. So the question is, as an interceptor, you saw just this ciphertext, you don't know anything about the message or the key, and you're going to guess possible key values to try to figure out what the message is. And what key value would you guess that would mislead you to think that the message was actually BS instead of CS? So the key point to notice here-- and this is why the One-Time Pad provides perfect secrecy-- is that for any given ciphertext we can produce any message we want by picking different keys. That means that if we just have the ciphertext, we haven't learned anything at all about the message. The way to produce the message 'BS' from this ciphertext-- well, we just need to change this first letter to be one below where C was. So that means we want to change this bit to be a 0 instead of a 1 in the output. That means we need to flip one bit in the key. So we'll need to change this key bit to be a 1 instead of a 0. Everything else will be the same. This will change the output letter by one. So if you guessed this key instead of the correct key, we get what looks like a fairly reasonable message out but it would be one off from the one that was there. And--in fact, we could get any possible message we want by guessing different keys. So this is what's called a One-Time Pad, and that notion of a One-Time Pad goes back until at least 1882. It was discovered quite recently that it was known about this early. There were One-Time Pads used in World War I, and used in many other contexts since then. The notion of the One-Time Pad being known this early was only discovered last year by Steven Bellovin. What made a One-Time Pad really interesting was the analysis that Claude Shannon did. Claude Shannon was the father of information theory. He did some of the first theoretical work in cryptography. During World War II, he studied communication, and he also studied theory of secret systems, and he wrote this paper, which was the first paper that really understood in a theoretical way what it means for a cipher to be good, and this paper was written during World War II. It was classified and released in 1949. So what we're going to look at next is how you formally reason about a cipher being secure and why the One-Time Pad has all the security properties that make it a perfect cipher. We saw earlier how we could prove correctness, and correctness is easy to show. Proving security is really hard. This property of the one-time pad that we can change the key and that leads to a different message, is the reason why it is secure. But first I want to talk about how to prove security in general. Then we'll come back to the problem of how do we prove that the one-time pad is actually a perfect cipher. How can we argue that a cipher is secure? This is a really hard problem. I'm going to start with a quiz, and it will definitely require some guesswork. See if you think it through before I explain the answer. I'm going to give you give possible ways someone might argue that a cipher is secure. For this quiz, your goal is to order the arguments below by how effective they are in convincing someone that a cipher is secure. Use 1 to indicate the best argument and 4 to indicate the worst argument. The choices are many very smart people were highly-motivated to break the cipher but were not able to. That means it must be secure. The second choice is there are 834 quadrillion possible keys, so it must be secure. The third choice here is a mathematical proof that's been accepted by experts that shows the cipher is secure. The forth choice is here is a strong argument why breaking the cipher is at least as hard as some problem we already believe is hard. This is a bit of a unfair question, since we haven't talked about this yet. I think it's useful for you to think about it on your own before I talk about it. See if you can figure out which of these arguments are most effective. Here what I think the best answer is. The best possibility is that we do have a mathematical proof that follows standard rules of deduction that shows that the cipher is secure. There are very few ciphers that we can actually get that for. One actually is the one-time pad. We'll see later this unit how to prove in a mathematical sense that it has a very strong security property. The next best one is the last one, which is to show in a formal way why this cipher is at least as hard as some other problem. We'll talk about reduction proofs more later in this course. The basic premise here is that there is some other problem that we have good reasons to believe is always already hard. Then we're going to show that if we could break the cipher, we could solve that other problem that we already have good reasons to believe is hard. The next best is this one-- that many smart highly-motivated people tried to break but couldn't. This is often the best we can do. For the best symmetric ciphers that are in use today this is really the reason that they're argued secure. There may be formal arguments that show why they resist particular attacks, and that's part of smart people trying to break ciphers, knowing all the known best-case attacks and trying them against the cipher and seeing that the cipher resists them. But ultimately the best we can do is show that we think it's secure because it has these properties, and lots of smart people weren't able to break it. But that's not a very satisfying way to know it's secure. We're certainly much rather have the strong mathematical proof. The worst possible argument here is the key-space argument. This one is often made incorrectly. The number of keys gives you an upper bound on the difficulty to break the cipher, because at worst the attacker could try all the keys. That's not true for the one-time pad, as we saw. Trying keys gives you perfectly sensible messages. You'll eventually see all possible messages. You can't know which key is correct. For ciphers where the key space is smaller than the message space, you could try keys and have a good likelihood if the key leads to a sensible message, that that's the right message. This gives you an upper bound on how hard it is to break a cipher. It doesn't give you a lower bound. The fact that you have a large number of possible keys doesn't mean the cipher is secure. We can always add to the key space without increasing the difficulty in breaking a cipher. We'll see many ciphers with very large key spaces that are completely insecure. Our best possible argument is to have a mathematical proof. That's what Claude Shannon was able to do for the one-time pad. It's really a fairly unusual case where we can get a mathematical proof that's that strong. Before getting onto the proof that the one-time pad is secure, I want to do a brief of probability. If you feel very confident that you understand probability well, it's fine to skip this section, but I think for many students it will be helpful to review probability. For some this will be new. It's not necessary to have a lot of background in probability entering this class, but we will certainly be using it in crytography. To talk about probability we need to think of sets of outcomes. We'll use omega to represent the set of all possible outcomes. This is sometimes known as a probability space. For a simple example, if we think about flipping a coin, omega is the set of the outcomes we could get. We could get to land heads, and we'll call that H, or we could get it to land tails, and we'll call that T. If our probability space has a uniform distribution, that means each outcome has equal probability. We can write that as a probability using P as our probability function. P is a function that takes as its input some outcome, and it maps that to a real number between 0 and 1. Zero would mean that event never happens. One would mean that even always happens. If we assume a fair coin with a uniform distribution, then the probability that we get heads is equal to 1/2, and the probability that we get tails is also equal to 1/2. That's how we think of mathematical coins. We hope to have mathematical coins to use in our cryptosystems. Real coins aren't so perfect. Let's assume instead that there are three possible outcomes. Now our probability space will be heads, tails, and edge. The edge outcome does not happen frequently. We'll make the probability distribution. The probability of heads is 0.4999. The probability of tails--we'll assume our coin is still fair, that the probability of tails is equal to the probability of heads, which is 0.4999. Is that enough to determine what is the probability of edge, of outcome E? I've given you two of the probabilities. Can you determine what is the probability of the third possible outcome, which is landing on the edge denoted by E. The answer is 0.00002, and the reason for that is the sum of all the probabilities of all the events in the probability space must be 1. We're certain that we get 1 of these events everytime we draw from the probability space. I'm not going to try flipping the coin enough times to see if I can actually make it land on its edge. The actual probability of landing on the edge is probably much lower than that. The next term I want to define is an event. You might think of landing on the edge or landing on tails being an event. We can define an event a bit more broadly that it's a subset of outcomes from the distribution. So an example of an event would be landing on heads, and landing on heads as an event is just a set of the outcome {H}. We could define then valid for a valid coin toss, and that's the set of outcomes {H, T}. The probability of an event is just the sum of the probabilities of all of the outcomes in that subset. So the probabililty of an event A is the sum of all the outcomes in A of the probability of that outcome. So that should be enough to figure out, what is the probability of a valid toss? I'll remind you that we defined the probability of heads as 0.49999, and the probability of tails as 0.49999. So what's the probability of a valid coin toss? The answer is 0.99998. There are 2 ways to solve this. One is to sum these two. Not too hard to do. The other way to solve it is to use the complementary event property, which makes the math a little easier in this case. That property is that we know the sum of all possible events has to be 1. Instead of computing P valid by adding the probabilities of heads and tails, we can compute it by subtracking from 1 the probability of events not invalid, which is the probability of E, which is 0.00002. That would get us the same answer. The property that we're using to do this is the complementary even property. What it states is that for any event A, the probability of that event and the probability of the complement of that event-- that is of that not happening--is equal to 1. What not-A here means is that elements in the universe minus the elements in A. This is set subtraction. We can think of that like this. This is our universe if we have some event A all outcomes that are not part of a must be the complement of A. That's what you get by removing A from the probability set. The next concept I want to talk about is conditional probability. This is the one we're going to need to use to prove that the one-time pad is perfectly secure. Here's the definition of conditional probability. If we have two events--we'll call them A and B-- and they're in the same probability space, the conditional probability of B given that A occured is written like this: it's the probability of B given A--so we use the bar to indicate conditional probability--and it's defined by this formula. It's the probability of A intersected with B divided by the probability of A. To get an intuition for that, let's look at these sets. Here's our universe omega. That's all events. Our question is given that we know A occurred, what's the probability of A. The fact that A occurred means the rest of our probability set doesn't matter. We're only considering the outcomes where A occurred. That's the set for A. We want to know what was the probability that B occurred. Those are the elements in this intersection--the times when B did occur divided by the size of the sets A, which is the probability of A. That's the intuition behind this formula. Let's see if it makes sense for our example as well. The question is given that we have a valid coin toss, what's the probability that it's heads. I'll remind you the model we had. It said the probability of heads is 0.49999 with four 9s. The probability of tails is 0.49999. The probabiilty of edge is 1 minus the sum of those, which is 0.00002. We define valid as the outcomes where it lands on heads or tails not on the edge. Given that you know a coin toss is valid, what's the probability that the outcome is heads? We can compute this using the formula for conditional probability. In this case what A is is the probability the coin toss is valid. A is valid, and we know that the probability of A is equal to 0.99998. What B is is the probability that it's heads. So the probability of B is the probability of H, which is 0.49999. Now we just have to plug these into the formula. But what we need to use the formula is the probability of A intersect B. And what A is the probability of A intersect B. What A is is valid. Instead of heads and tails, what B is is heads. If we intersect heads and tails with heads, we get tails. We know the probability of tails is 0.49999. That means using the formula we have the probability of tails, which is 0.49999, which is the probability of A intersect B. We're dividing that by the probability of A, which is a valid event which is 0.99998. We get 0.5. I should note that this is not the case for real coin tosses. There is no physical coin ever manufactured that has exactly chances of landing on both sides. In fact, with real coin tosses, at least with American currency, there is a much higher percentage--much higher meaning close to 51% rather than 50%--that the coin lands on the same side that it started on. When we talk about mathematical coin tosses, we're going to assume that there is no edge case and that it's equally likely that we have a uniform distribution and there are only 2 outcomes. When we talk about mathematical coin tosses, we're going to assume that we have a uniform distribution, and there are only two outcomes. Now we're ready to define what a perfect cipher is and to see why the one-time pad satisfies this property. The notion that we want a perfect cipher to mean is that if an attacker intercepts a message they receive the cipher text that provides them with no additional information at all about what the plain text was. We have a message. It's being encrypted with a key. The attacker is intercepting that cipher text as it's sent over the insecure channel. What we want to know is that an attacker who sees just the cipher text learns nothing about the message. If you followed the definition of conditional probability, you should be able to decide how to define that formally. We'll make that a quiz. The question is which of these is the property that we want in order for a cipher to be perfect? That is, to have this property that the cipher text reveals no information about the plain text. I'm going to introduce some notation. Our message is selected from the set of all possible messages. We have some other message we'll call m<i>, also selected from M.</i> We don't know if M is equal to m<i>.</i> Both of these we could think of being drawn from the set of messages. We have some key drawn from the set of possible keys. For this quiz your goal is to understand how we can formally define what a perfect cipher is. This is the scenario: we have an attacker whose heard of cipher text. We want to know that by hearing just the cipher text the attacker has learned nothing new about the message. We've introduced some notation here. We have a set of possible messages--the uppercase M. We can select lowercase message m<i>. Those are both messages selected from M.</i> The attacker's goal would be to tell is the intercepted cipher text the same as message m<i>.</i> That's the attacker's guess. We have encryption using some key selected from the set of all possible keys. We don't know what the key is. Here are the possible choices. The answer is the second one. Here we're using conditional probability. The event that we saw was the cipher text. That's the encryption of m. What we want to know whether the message is equal to m<i>.</i> Both of those are drawn from the set of all possible messages. In order for the attacker to know nothing is the same as the probability that the message is m without knowing the cipher text. The first answer would be correct if, a priori, the attacker knew nothing about the messages. All the attacker knows is each message is equally likely. In that case, the probability that the message is m would be 1 over the number of possible messages not depending on the cipher text. The problem is the attacker might know something more. They might know that some messages are more likely than others. In most realistic scenarios this is the case. The attacker knows that the message is likely to be a valid sentence in English. Very few possible bit sequences correspond to that, so all messages are not equally likely. That's why choice 1 is not the definition we want. We want choice 2 where whatever the attacker already knew about the probability of the message as m is not affected in any way by seeing the cipher text. That's our definition of a perfect cipher. Now the question is can we prove that the one-time pad has this property. Our goal is to show that a one-time pad is a perfect cipher, and now that we have the definition of a perfect cipher, we should be able to argue that formally. This was our definition. Since our definition uses the conditional probability, we should also remember the definition of conditional probability, which is the probability of some event A conditioned on event B happening is equal to the probability of A intersect B divided by the probability of B. To show that the one-time pad is a perfect cipher, we just need to calculate this where A will be this event and B will be this event. We need to know the probability of B, which is the probability that some message with some key encrypts to C. Let's compute that first. Then we'll need to compute the probability of A intersect B. We'll have a little quiz. Given any message and any cipher text C, and we're using a one-time pad, how many different keys are there that encrypt that message to that cipher text? The answer is there is always exactly 1. For any message ciphertext pair, there is one key that maps that message to that ciphertext. We can see that thinking about the nature of the one-time pad. Suppose we just have 1-bit messages. We have messages 0 and 1. Then the key 0 will map 0 to 0. The key 1 will map 1 to 0. The key 0 will map 1 to 1. This works for any length message and any length key. Note that the length of the key and the length of the message scale the same way. If we had 2-bit messages there would be four different keys that would map each message to each different ciphertext. If we had 2-bit messages, there would also be one key that maps each message to each different ciphertext. For example, the key that maps 00 to 10 would be the key 11. That means we can compute this probability, summing over all the messages summing over all the keys where we've got the probability that that key encrypts that message to the given ciphertext. We need to divide this by the space, which is the size of the messages times the size of the keys. Now whats the value of the summation? We know for a given message there is exactly one key that maps it to that ciphertext. This is one, and we're summing over all the messages, summing the value of 1. That means we're going to get the number of messages divided by the number of messages times the number of keys. That's equal to 1 over the number of keys. That's the probability of event B, which is probability that some message encrypts to some key. Notice this is computed over all the messages. That's over the distribution. But what we need to do now is consider this particular message M<i>.</i> We want to compute the probability of the intersection of those events, that the selected message is M and the message encrypts to C. I'm going to make that a quiz and see if you can figure out what that is yourself. Here are your choices. See if you can figure out what the probability of M equals M intersected with the encryption of M equals C. The answer is this one. It's the probability that the message is M divided by the number of keys. To see why let's think about our probability space. We have all the possible messages being ordered. We have message 0, message 1, all the possible messages. There is some message that we've selected that's message M8. We don't want to assume that the messages are uniformly distributed. The probability that the message is M is not necessarily 1 divided by the number of messages. If the messages were uniformly distributed, then this answer would also be correct. But they're not. We don't want to assume that. Maybe the attacker has some prior knowledge about the distribution of the messages. That's normally the case. Even if we don't know anything about the particular messages, maybe we know it's a message in English. Most random strings of text are not messages in English. The other dimension in our probability space is the choice of the key. There are K possible keys. What we want to know is what is the intersection of these two events. The M even is this line in our probability space. Each one of these keys maps exactly one message to C. We saw that before, that there is one key that maps each message to C. We can think of that as being a diagonal line through our probability space. This is the line where the encryption of Ki and Mi is equal to C. Depending on what C is, it might not be a diagonal line. But we can think of it that way, that there's one key that maps each message to each cipher text. That means there's exactly one point here where those two intersect. If the distributions were normal, that would indeed by the correct answer. It's one point and our space is the size of M times the size of K. But the distribution isn't normal. We don't want to make any assumption about the distribution of M. For the keys we do want to assume that the distribution is normal, that each key is equally likely. The keys are chosen perfectly at random. We have the probability of picking M<i>,</i> and we're multiplying that by the probability of picking this intersecting key. We'll call that K<i>.</i> But because the keys are uniformly distributed, we know that this probability is 1/K-- 1 over the size of K. We don't know that about the messages. We leave that probability as it is. That's why this is the answer to the probability that the message M intersected with the encryption of the message is C. Now let's go back to our definition. We showed earlier that this was 1/K--1 over the size of K. Now we've shown that the probability of M equals M is equal to the probability the message is M divided by the K. We've shown these two things. Now we just need to plug them into our conditional probability formula. We're going to have this on top, and we're going to divide that by the probability of B, which is 1 over K. The "over Ks" will cancel out, leaving us with the probability that the message is equal to M<i>.</i> That's exactly our definition of a perfect cipher. We can conclude that the one-time pad is a perfect cipher. It exactly satisfies the definition of a perfect cipher where the cipher text reveals nothing at all about the key. You might think the class should be over. We've achieved our goal of perfect secrecy using a cipher that was invented over 100 years ago and is actually provably perfectly secret. We're not going quite done yet. There are some pretty serious problems with the one-time pad. One problem is that it's malleable. What malleable means is if Alice sends her ciphertext to Bob, and our evil interceptor--this times it's not just an eavesdropped, it's an eavesdropper with a hammer. If our interceptor has control over the network, and instead of just being an eavesdropper can be an active attacker. An active attacker means they can actually change messages on the network. The message that arrives at Bob is not C. It's C'. Because at the perfect cipher, the attacker can't learn anything new about the message from C, but she could modify it. Maybe she had a pretty good guess. Maybe there was a number in the message somewhere or something that she wanted to change. Well, she could flip the bits at that part of the message, change the ciphertext that Bob receives to C. It would decrypt, and with the one-time pad encryption and decryption are the same function. The decryption of C' would be M'. The attacker can actually control the difference between M and M', because the way the one-time pad works is just XOR. The attacker could decide whatever difference she wants to introduce in the message. This is a dangerous property for a cipher to have. Another big problem with the one-time pad is that it's very impractical. The real reason that it's impractical is because the keys have to be as long as the messages, and we can never reuse the key. The is this property that the number of possible keys is equal to the number of possible messages. Maybe what we should try to do is to find a more practical perfect cipher. Unfortunately, Claude Shannon proved that that's not possible. That's what we're going to look at next is why this property that the key space has to be at least as big as the message space i s a requirement for a cipher being perfect. The theorem essentially says is that if a cipher is perfect, it must have this property. It must be impractical in the sense that the number of possible keys must be at least as big as the number of possible messages. We saw for the one-time pad that they were equal. We can always remove possible messages from the message set so there's no problem with adding more keys. The problem is we need to have at least as many keys as messages. Let's prove this property. We're going to prove it by contradiction. We're going to start by assuming that we do have a perfect cipher that does not satisfy this property. Suppose we do have some perfect cipher--we'll call it "E"-- where the number of possible messages is greater than the number of possible keys. There is some ciphertext--call it c0 element of the set of possible ciphertexts. Let's assume that that ciphertext is possible. We know that there must be such a cipher text. There is some key that encrypts some message to c0. The probability that a message and key get encrypted to this ciphertext is greater than 0. We know that such a ciphertext must exist. Now we have a our ciphertext c0. Let's try decrypting that. We'll decrypt it with all keys in the key space. We haven't mentioned what the decryption function is. For the one-time pad it's exactly the same as the encryption function. For this we don't want to assume this. I could be any function. We'll assume there is some function D that is our decryption function. Since our cipher is correct--in order to be perfect it has to both be correct and perfectly secure. That means the decryption function must have the property that if we decrypt a message encrypted with the same key, we always get the same message out. We don't need to know anything else about the decryption function than that. We shouldn't assume anything, because all we're trying to do is show that there is no possible choice for E that is both a correct encryption function and a perfect cipher. Now what happens when we decrypt c0 with all possible keys? Well, we're going to look at the set of messages that we can produce. We'll call M0 the set of messages that we get by unioning over all possible keys the result of decrypting the cipher text c0. This models what an attacker would do. They've intercepted some ciphertext. They're trying all possible keys. This is what a brute-force attacker would do. Looking at all the possible messages that can be produced. Now I want you to think about which of these statements are true. Check all the statements that must be true. This is a fairly tricky question. The first one is actually not true. The reason it's not true is there's no guarantee that the same cipher text with different keys doesn't decrypt to the same message. We know that the size of M0 is no bigger than the size of K, because we constructed by unioning the decryption, which produces just one result over all possible keys. We know that it's no bigger than the number of possible keys. We don't know that it's actually equal to that. That would require another assumption about the decryption function. The other two properties we do know are correct. We know that M0 has fewer elements than M. This is the case because of this assumption. We said that the space of possible messages is greater than the space of possible keys. If that's the case, combined with the size of M0 being less than or equal to K, then we know that the size of M0 must be less than the size of M. The third property is also true. This is the key property that should give us insight why it can't be a perfect cipher. There exists some message in M that can't be in M0, and that follows directly from this property. If one set is bigger than another set, there must be some element in the bigger set that's not in the smaller one. How do we go from there to prove that it's not a perfect cipher? Let's remember our definition of a perfect cipher, and that was that the probability that the message is some particular message given the ciphertext without knowing anything other than that ciphertext is equal to the probability that M was the message beforehand. That probably hasn't changed. In this case, if we have intercepted C0. This was our ciphertext for the example. Because of this property, we know that the probability that the message is M is actually 0, because that's not in the set of possible decryptions of C0 over all possible keys. But we said that M was a message in M that initially had some non-zero probabilty, which is not the case that these two are equal, because this was greater than 0. We've contradicted the requirement for our perfect cipher. The only assumption that went into this was this one. We assumed that there existed a perfect cipher where the number of messages is greater than the number of keys. We've reached a contradiction. This proves that there exists no perfect ciphers where the number of messages is greater than the number of keys. This is bad news. Every cipher that's practical must be imperfect. This means that given some ciphertext that an attacker intercepts, even if the attacker knows nothing about the key, they can eliminate some of the possible messages. This means that when people attempt to use ciphers similar to a one-time pad in practice, they tend to actually not work very well. The reason for this is that they reuse keys, because the number of messages they want to encrypt is greater than the number of keys they have. Next we'll look at an interesting historical example of this. I want to show you an example of the kinds of things that go wrong when people attempt to use one-time pads in practice. This is a story that is very important to the history of computing as well as the history of the world. It's how the Allies at Bletchley Park were able to break the Lorenz cipher, which was a cipher used by the Nazis to communicate between the capitals of the cities of Europe. [Vernam's "One-Time" Pad (1919)] The story goes back at least to the 1919 patent that Gilbert Vernam, who was an engineering working at Bell Labs, got on a one-time pad system. It shouldn't really be called a one-time pad. It was a re-use pad system. The idea behind this is somewhat clear from the patent, especially if we rotate it so we can see it more clearly. One thing you can see is that there are five wires going into it. This is the output from reading a tape that encodes the message. The key was also on a paper tape. The encoding used just five bits. This was the Baudot code. With five bits we can encode 32 different symbols. That's enough for the alphabet and a few punctuation marks. You can see the letters were encoded. There's the strip down the middle that was used to align the tape. There are five bits to encode each letter. Managing paper tapes like this was difficult, and the keys needed to be distributed on paper tapes. If the keys were constructed in a perfectly random way and you had enough key that you never needed to reuse it, this would actually be a perfect cipher. This would be a perfectly good one-time pad. The problem is distributing large paper tapes is pretty difficult, especially if you're trying to do this in war time. The machine that was built based around the same ideas, the Lorenz Cipher Machine, where instead of having a paper tape with the key you had a machine that would attempt to produce a good sequence of key bits. Of course, it's impossible for a machine to produce a perfectly random sequence. The structure of the machine determines properties of the sequence that's produced. The machine would generate a key sequence based on its configuration. That configuration is the initial settings of all the rotors and other parameters that we'll talk about soon. If there are two machines that start in the same configuration, say one in Berlin and one in Paris, then they produce the same key. Their encryption is just the message XOR'd with the key that produces the ciphertext that's sent over radio wireless, received at the other end, which puts it into a machine starting with the same configuration. That means that ciphertext XOR'd with the key generated by this machine, starting from the same configuration will decrypt to receive the message. The machine was designed to produce a large number of possible sequences. There are a lot of complex operations, which we'll talk about a little next, but the key would not repeat for 10^19 letters. Given that this number was larger than the amount of text written by humans, at least at the time, this lead the Nazis using it to believe that it provided the highest security they needed. They also were confident that because these machines operated in capitals-- they weren't like the enigma machines, which operated in the field-- that they never lost one of these machines. The Allies never actually had access to the actual machine to try to figure out how it worked. These links started to be used in May 1941. In August 1941, the Allies had a big break. An operating was sending a message starting with the initial configuration. These configurations came from code books. The recipients at the other end needed to know that the configuration for that day was. But these could be distributed securely and were much smaller than needing to distribute the key sequence. Once you had the starting configuration, you could transmit an arbitrarily large message if this assumption that the key was completely unpredictable was true. In this case the operator sent the message. The receiver did not receive it. The receiver said, "Garbled transmission, please resend." So the annoyed operator had to send the message again for the insecure channel, and this time it was received okay. If this is exactly what happened, it would have been fine. These would be exactly the same. We restarted the machine in the same configuration. But what happened was a little different. The operator got a little lazy the second time. The operator was annoyed having to retransmit the message, so instead of sending exactly the same message sent a slightly different message. That meant that there was a slightly different ciphertext than we had the previous time. Both of these messages were intercepted. That means the Allies now had access to both C and C'. They didn't know anything about the key. They didn't know anything about the original messages. Well, maybe they knew a little bit about the original message. That's what's going to help them given these two ciphertexts to figure out both the key, and, once they have the key, the structure of the Lorenz. What is the result of XORing these two messages? That means for each bit, we go through the bits to the messages XORing each bit. I want to introduce a new variable here. The key that the machine generates we'll call K. That's the key that the machine generates starting from configuration C0. Which one of these is the value that the interceptor would get by XORing the two intercepted ciphertexts. The answer is M XOR M'. The reason for this is because each cipher text is the result of XORing a message with a key. C is equal to M XOR K. C' is equal to M' XOR K. Note that the K is the same in both cases. If we XOR C and C', that's going to be M XOR K XOR M' XOR K. The really useful thing that happens if you're trying to break this cipher is these two Ks will cancel out. That's why you end up with M XOR M'. That doesn't tell right away what the message is. If the messages were exactly the same M XOR M' would just be zero, and this wouldn't tell us anything. The ciphertext XORed will be zero, but they weren't exactly the same. When the Allies XORed these ciphertexts they didn't get all zeros. The reason for that was because of the abbreviations that were used in the transmission. An example would be if the original message has Spruchnummber, meaning serial number, maybe in the retransmission it became an abbreviation Spruchnr. That would mean the place were the letters start to be different we start to see differences between the ciphertext and the ciphertext prime. Unlike the key which was pretty close to random, there were a lot of things you could guess about the messages. The Allies had a pretty good guess that the message was in German. The Allies could start trying. Let's say they guess the message. We'll guess it's M<i>.</i> Then we could try seeing what we get when we do M XORed with a C XORed with a C'. We can make some guess for the message. Try XORing it the intercepted ciphertext. That should give us back the other message, and we'll look if that looks like a possible message. This is definitely a painstaking process but something that can be done. Once you get the two messages by this guessing process, then it's easy to find the key. Once you have the two messages through this guessing, you had the ciphertext intercepted. Now you know the key. Learning the messages might be useful. By the time you've done all this work maybe it's too late for the message to be useful, but learning the key could be really useful. This was done by Col. John Tiltman who took these two intercepts and determined the key that produced them. They had enough content here that there were 4000 letters in the intercept, so they had a 4000-long key. This key was given to Bill Tutte. This was his first assignment at Bletchley Park. He was to try to make sense of these key. After about 6 months of work, he was able to determine the machine structure that produced that key, and this is what it is. You have five lines going in. This is the five bits in the Baudot code for encoding each character. The rest of the machine is to produce the key bits. There is an XOR with each of these wheels. These wheels will rotate with each character. Then there is another set of wheels that sometimes rotate and sometimes don't. We have these two M wheels. Based on the result of XORing the M wheels, either all the S wheels rotate one position for that character or none of them do. This was really the biggest weakness in Lorenz cipher--the S wheels all moved at once. This is the structure of the machine. Each letter of the message would be divided into 5 bits, and those would be XOR'd with the value coming from the corresponding K wheel. These wheels would rotate with each letter, and at each position they either had a 0 or a 1, depending on whether there was a pin in that position or not. Each wheel had a different size. The last one had 23. The first one had 41. They were different-sized wheels. This would also be XOR'd with the result from the S wheels, which worked similarly. They also had positions that were either 0s or 1s around the wheels. The different was the K wheels turned every character. The S wheels only turned conditionally on the result of 2 other wheels, which were the M wheels. The M1 wheel would turn every time. The M2 wheel would rotate depending on the value of the M1 wheel and depending on the XOR of those, either all the S wheels would rotate by 1 or none of them would. Then the result of all these XORs is the cipher text. The is similar to the idea behind a one-time pat. We're XORing message with key. But the key is not a perfectly random sequence. It's the key generated by this machine. To break the cipher, once you knew the structure of the machine, that's not enough. You need to know the initial configuration. Here's the intercepted message that was intercepted over the radio. We can think of all the characters in the message. The important thing to remember about the Lorenz cipher is that the message encoding each character--each character is encoded into 5 bits, using the Baudot code. The cipher text is in these same sequences where each cipher text bit corresponds to one bit of that letter. The next one corresponds to the next bit of that letter. We can think of the cipher text being broken into channels corresponding to each part of the letter. That means each channel would be the sequences that repeat every 5. This would be part of the first letter, part of the first letter. This would be the second part of the first letter. This would be the second part of the second letter. And this would be the third part of the first letter. This would be the third part of the second letter and so on. We can think of each of these as a separate channel. What we're going to do is use a new notation. We're going to subscript Z by channel and the letter for that channel. Zc sub i is the ith letter for channel c. In terms of this mapping, if we look at channel 0, that's going to be Z0, Z5, Z10. We can just break the cipher text up into channels. The key reason for doing this is because of this weakness that was noticed in the cipher. The key weakness is that all these S wheels move in turn. Either when we advance one position they're the same for all the channels or they advance by one for all the channels. The value of each of these depends on the message. It also depends on the outputs of the K wheels, and it is also XOR'd without outputs of the S wheels. By separating it into those three pieces, we're going to be able to take advantage of the properties that they have. The key insight is that the S wheels don't always turn. If we look at subsequent characters, there's a good chance that the S wheels have not changed. We're going to define Z sub c, i as the difference between 2 subsequent characters in the cipher text for that channel. That's XOR'd with Zc of i plus 1. Now because this is for that channel, these are five-characters apart in the intercepted cipher text, but they're adjacent for that channel. What happens when we look at these values for two different channels? We're going to look at channel 0, and we're going to XOR that with the delta value for channel 1. Plugging in the definitions, that's just the result of Z XORing all of these values. Where this becomes valuable is because we can break these down into the three parts. Let's break them own into parts, separating the M, the K, and the S parts that combined into this cipher text. What we get is these three things. For the message bits we have the XORs of all the message bits for the two channels adjacent and the key bits XOR'd with the message bits. Then we have this for the S bits as well. I want to ask a brief quiz here. For each of these combinations--this is m, K, and S. Supposing we could separate each of those, the question is which of these inequalities are likely to be true. Each one we're asking whether the probability of that part being 0 is greater than 1/2. If it was a uniform distribution, the probability would be equal to 1/2. If it's not uniformly distributed, then the probability could be less than or greater than 1/2. I'm asking here are there any of these that we can be confident-- or at least have a good likelihood --that the probability of the delta for the component is equal to zero. Let me remind you what the components are. The message is text in a language, and in this case it's in the language German. These are the K wheels. Each wheel rotates with every character. They have pins around the wheels deciding if it's a 0 or a 1. These are the S wheels. The S wheels rotate only when the output of the M wheels is a 1. Sometimes they don't rotate at all. Sometimes they all rotate at the same time. The answer is actually both the first and the third are likely to be significantly greater than 1/2. The second is not. Assuming the keys have as many 0s and 1s as the good key rotor should be designed to have, this is likely to be equal to 1/2. There is no reason to expect it to be greater than 1/2 any more likely than it'd be less than 1/2. We're XORing two consecutive key bits. The other two are different from 1/2. This is what gives us the opportunity and what gave Bletchley Park the insight needed to break the cipher. For the third one, this followed from the structure of the machine. When the S wheels advance this probability is about 1/2, but when they don't advance S is always 0. This means the probability that S is 0 is significantly greater than 1/2. It turns out for the structure of the Lorenz machine it's about 0.73. To know that you'd have to look in more detail at the structure of the M wheels to know how frequently the S wheels advance. When they don't advance we know the result is 0. When they do advance about half the time the result will be 0. Getting the first one right required a little more linguistic insight. The reason this is greater than 1/2 depends on subsequent message letters. If adjacent letters in the message are the same, that ensures that M is equal to 0. It turns out that this is a property that many languages have. You can see that English has it. We have repeated letters here. We also have some repeated letters in "wheels." It's more like that you would expect by just the normal letter distribution for subsequent letters to be identical. It turns out that this is a property of German that about 3.3% of digraphs, meaning two adjacent letters are the same letter. That means the probability that the messages are equal-- well, they could be equal for lots of other reasons, but this bias is towards being more likely to be zero than non-zero. It turns out that that's 0.61 probability for German. Both of these are numbers that you didn't have enough information to guess on your own. You would need to have a analysis of German text to know that this is the probability here, and you would need a lot more details on the M wheels to be able to get that. Don't feel bad if you didn't get this quiz correct, although the structure of the machine should have been enough to guess that this is greater than 1/2, and if you're familiar with German or could guess that it has properties similar to English, you might have been able to get this one as well. Now we've got this great property. We've got two things that are significantly different from 1/2. Can we use that to break the cipher? Now the question is assuming that the probability of K is 0 is still 1/2, does knowing that these probabilities for M and S being greater than 1/2 help us? Remember that our ciphertext values that we're intercepting are the XOR of M, K, and S. The choices are, yes, that it definitely helps us. It doesn't necessarily help us, but if we knew something more about K maybe it would help us. It doesn't necessarily help us, but if we knew something more about M maybe it would help us. The answer is it only helps us if we know something else about K. The reason for this is if K is uniformly distributed, and we're XORing it with these other values, whatever patterns that M and S have are lost when they get XORed with K. This is the whole point of the one-time pad that we can XOR a random key with a message and hide all the statistical properties in the message. This is not the case that it helps us by itself. If this was all we knew this is a potentially interesting property, but we need to know something more in order to use this to help break the cipher. Knowing something more about M would be great. That's the message we're trying to decrypt, but that's separate from being able to use this property in a useful way. What we really need to do is know something more about K. What more do we know about K? Here's our Lorenz machine, and we have the K wheels. That's what produces K, and each of these wheels might be a different size. The first one has 41 positions that are all set to either 0s or 1s. The second one has 31. If we're looking at Z for two channels--remember what we were producing here, looking at the XOR of Z from channel 0 XORed with Z from channel 1. Let's focus just on these two channels--the outputs. The question is how many different configurations are there for the first two K wheels, the ones for channel 0 and channel 1. The numbers here are the size of those wheels. [Evans] The answer is there are 41 different positions for the first wheel, and we multiply that by 31 different positions for the second K wheel. And so that means that every 1271 letters those wheels would repeat, and there are only 1271 different possible settings for the K wheels. Remember that based on the intercepted messages they had learned enough about the structure of the machine to know what the bits are around these wheels. So it's only necessary to figure out the right configuration. So certainly today, being able to try 1200-some possibilities would be trivial with a modern computer, and you might wonder why when they encountered this at Bletchley Park they didn't just go down to the store and buy a computer to solve it, but this was 1941. Computers didn't exist yet. They had to invent one first. And in fact, arguably, the first computer was invented to solve exactly this problem. Before we get to that, I want to make sure that we actually have enough of an advantage here that trying these configurations will help. So let's look at the probability that if we guess the right key we'll be able to tell that we got the right message. [Evans] So if you recall the separate channels, we divided the cipher into the message part, the key part, and the part from the S wheels, and now we've concluded that if we can-- we're going to try 1271 possible settings. So for 1 of those possible settings we're going to know all the key bits, and that means if we guess the right setting the key will go to 0. So now all that's left is the other 2 parts. This is what we had before. We knew that the probability of delta M being zero was greater than a half, and it was actually about 0.61 for German because of the likelihood of message letters being repeated. And we knew that the probability of delta S being 0 was about 0.73, and that's because of the structure of the machine-- that the S wheels don't advance every step. Only when the M wheels are on a 1 do the S wheels advance. And so now we've said if we guess right, that means the probability that the key bits are 0 goes to 1. And it'll be 1 when we guess right. When we don't guess right it'll be . So in order to tell if we guess right, we need to know that the probability of the cipher text bits when we guess right, the probability of the delta Zs being 0 will be different from . So the question is, what is the probability delta Z is 0 when we guess right? And remember that what delta Z is is the XOR of delta M, delta K, and delta S. So to solve this you need to think about all the different things that would make delta Z equal to 0 and compute their probability. [Evans] The answer is 0.55. The key is 0. We can cancel that out. So we're left with delta M XORed with delta S. That could be 0 either if delta M is equal to 0 and delta S is equal to 0, then the XOR of 0 and 0 would be 0. The probability that delta M is 0--well, we know that. That's 0.61, and we're going to multiply that by the probability of delta S is 0, which we know is 0.73. But that's not the only way delta Z could be 0. The other way delta Z could be 0 is if both of these are 1. So it's the probability they're both 0 plus the probability they're both 1, and these probabilities are 1 minus the probabilities that we have before us, so it's (1 - 0.61) (1 - 0.73). And if you calculate all that, you get 0.5506. That's probably a little more precise than it should be, especially because this value is just a guess. We'd have to do a much more detailed analysis of German to know whether that probability is really 0.61, and we'd have to know more about the particular messages that might be encrypted. But this value is pretty far away from a half. Any advantage that's that large, if we have a lot of text we're going to see that pretty clearly. So if we have enough text, we can count the number of positions where delta Z is equal to 0. If it's close to half of them, then it was a wrong guess. If it's close to 0.55 of them, then we have a good likelihood that that was the right guess. So now all we have to do is feed in the intercepted messages. Our guesses for the starting position of the 2 keys, we need to compute a big summation of these values, of the delta Z values, with those keys. And if it's close to the length of the message divided by 2, that means it was a bad guess. We weren't able to cancel out the key. If it's close to 0.55 times the size of Z, then it's a good key. It's likely that that's a good guess. And then we should be able to use those key guesses to find out what the actual message was, to decrypt the cipher text. So this is exactly the problem that what is arguably the first electronic digital computer was built to solve. So with this advantage there's a good likelihood that you would be able to know when you guess the right key. You need to try all the configurations of K0 and K1, and for each one of those configurations you have to compute this double delta. What we're computing for each configuration is this double delta, the XOR of 2 deltas, and that involves computing all these XORs. We need the XORs of the keys XORed with the messages and the S wheels. But remember what we're doing is guessing that this is 0. We don't have any way to predict those S values. We're producing the key values, and we're XORing those key values with the intercepted cipher text. So we need to do these XORs, XORing out the key and XORing the key with the value of the cipher text. So for each character we're doing 7 XORs and we're counting the number of times that's equal to 0. So multiplying all those together, we know the total number of XORs we need to do, and we get about 44.5 million. That's the maximum number that we might need to do. If we're lucky, we might guess the right configuration right away, and we could know that that's the right configuration by getting the high number of 0s out. If we're unlucky, we might need all 1271. Normally, we should expect to need about half of that. So maybe on average we would need about 25 million XORs to find the configuration, the correct value of X K1 and K0 for 1 cipher text. Once we've got K1 and K0, we can do similar things to find K2, K3, and K4, and then we can decrypt the whole message. So today, a modern processor runs at 2 gigahertz, so you're doing 2 billion operations per second, and 1 operation could include many XORs, possibly 64 XORs or 32 XORs depending on your processor. So to do that on a modern processor would take a fraction of a millisecond. To do that in 1941 was a major technical challenge. Computers didn't exist yet, but this was the main impetus for building what was arguably the first electronic programmable digital computer. The first machine that was built to do this was called the Heath Robinson. It was named after this British cartoonist who drew cartoons of crazy machines to do operations. This one was for peeling potatoes. The timeline of this is quite interesting. The first operator mistake--the re-transmission with abbreviations-- happened in August 1941. That was intercepted. Over the next year that was enough to learn from the two intercepted messages the structure of the machine and then to understand it well enough to develop this technique--the double-Delta technique for finding the correct configuration and to break messages. By December 1942 they decided to build a machine and put the resources necessary to design and build the machine. It was requested to be done by June of 1943. It was actually delivered 3 months early. By April 1943 they had the first machine delivered and operational. It could process messages about 2000 characters per second. It was able to do 7 XOR operations at 20 km per hour. This was not quite fast enough. A lot of the value in decrypting messages, especially in war time, is being able to decrypt them before what the message is describing actually happen. They built a faster machine. The faster machine was called "Colossus." This was operational by January 1944, and the big change that made Colossus much faster and more useful than the Heath Robinson machines was that they replaced the configurations, which were previously on a tape, with an electronic keytext generator. The logic is doing all the XORs and counting the number that are zero. Then it's printing out those tallies so you can go back and find the right configuration. The ciphertext is still on a paper tape. This is spinning through about 50 km an hour. It was a pretty impressive sight when it was operating. Processing 5000 characters per second. What made this arguably the first computer was because of electronic keytext this logic was a little bit programmable. It didn't do always exactly the same thing. There were ways to program it to do slightly different operations depending on what the analysts through was the most useful thing to do with that intercepted cipher text. This is arguably the first programmable digital electronic computer. It wasn't fully programmable. It wasn't a general purpose computer. But it could be programmed to change its behavior slightly. You can see what this looked like during World War II. There are very few pictures of it. This was a very secretive operation. The machines were actually all destroyed after the war. A lot of the value in breaking a cipher is to make sure that no one else knows you've broken it. This is different from academic cryptography where people like to publicized when they break ciphers. In military cryptography the whole value of breaking a cipher is your enemy keeps using it. You want to keep it very secretive that you've done that. A lot of the work on Colossus was not declassified until the mid-90s. Today if you visit Bletchley Park you can see a replica, a rebuilt version of Colossus. This is my picture of it from 2004. It does look very similar to the black and white picture from World War II. Unfortunately, they don't operate it frequently, so you won't get to see the tape spinning through at 50 km an hour and all the crunching going on unless you're very lucky to go one of the few days a year that they might actually operate it. These Colossus machines had a huge impact on World War II. By the end of the war there were 10 of them continuously operating at Bletchley Park decrypting all the traffic that they could intercept. They decoded over 63 million letters of messages in Axis communications. Among the things they learned from them were the troop locations of Axis on D-Day. This was very helpful for planning the operation. Why did I tell you this story? First of all it is an important story in the history of both the world as well as computing, but this isn't meant to be a history class, although I certainly like to have historical excursions when I can. It's also very relevant to modern cryptanalysis. Modern ciphers are much better than the Lorenz cipher, and the main reason for that is we can use computing power to do the encryption. But the basic ideas are actually quite similar. The goal of the cipher is hide all the statistical properties that are inherent in the message, and they're present in the key--at least the generated key. The actual key we hope is perfectly random. Finding perfect randomness and getting it in computing is quite challenging. But let's assume we have a perfectly random key. What we learned from the perfect cipher analysis is that the key must be shorter than the message. That means in order for the cipher to work, we need to generate more key bits than we actually have. Even if the original key--and you can think of that in the case of Colossus as the configuration of the machine that comes from some code book that could be perfectly randomly generated and shared between endpoints. That is only the starting configuration. There is some larger key that has to be generated to produce the cipher text. So both the key and the message have some statistical properties. The goal of cipher is to hide all those properties. The goal of the analyst is to find statistical properties in the cipher text and then to use those to break the key or the message. In the case of Bletchley Park breaking the Lorenz cipher, they found statistical properties. When you looked across channels at subsequent letters, there were some statistical properties that were not hidden by the cipher. That was because of a mechanical weakness that all the S wheels either all moved or didn't move. That meant that instead of having the long period with no repetition of 19 million letters that the users of those Lorenz machines thought it had, the Allies could break that down and find a pattern that was only 12,071 letters long with much fewer configurations to try to have a good guess of the cipher with many fewer guesses needed. In a modern cipher we think of that as a mathematical weakness. There's some problem is the mathematics of the cipher that leaves some statistical property that a cryptanalyst could exploit. The other relevance to modern cryptanalysis is it's really lots of hard work to do that. I find it quite amazing what Bletchley Park was able to do with Colossus. It still took 6 months of effort looking at those two messages to figure out the key and interpret the key structure from that. That requires an awful lot of trial and error and a lot of creativity but also a lot of tedious work. In modern cryptanalysis we try to do as much of the tedious work as possible by computers, but there is still lots of hard work that goes into breaking a cipher. Motivation certainly helps a lot for that. In the case of Bletchley Park, the fear that your country was under attack is a pretty strong motivation. We won't talk in much detail about modern symmetric ciphers in this course. There are a couple reasons for this. One is that I want to get on to uses of ciphers, which I think is more important and more interesting for most people. Very few people today need to implement a cipher. You really should use library implementations of ciphers in any serious application. It's certainly useful to understand more about what's going on beneath the scenes. But even few people should be designing new ciphers. You would have to have very unique requirements to think that you're better off designing a new cipher yourself than using a standard well-accepted carefully analyzed cipher. Most of the time we've used ciphers as black boxes that are taking in messages and keys and outputing ciphertext and assuming that they have the properties that we need. Certainly we've proven that no cipher really does. We've shown that the only way for a cipher to be perfect is if the key space is at least as big as the message space. That's impractical for any useful use. We can group modern ciphers into two types. There are stream ciphers and block ciphers. The different is that with a stream cipher we've got a stream of data, and our cipher can encrypt small chunks at a time. You can think of the data streaming through the cipher encrypting encrypting usually at the level of one byte at a time, whereas with the block cipher we think of our data in larger chunks, and the cipher encrypts a block at a time. Usually a block size is at least 64 bits and can be longer up to 128 or 256 bits. These are sort of really the same thing. The only difference is changing the block size. If the block size is small enough, it would become a stream cipher. If the block size is large enough, we can think of it as a block cipher. But there are enough differences in the way you build ciphers that different ciphers are designed for each purpose. The most important block cipher today is known as AES, and this stands for the Advanced Encryption Standard. AES is a block cipher. It works on blocks of 128 bits. AES is the result of the competition that was run by the United States National Institute of Standards in Technology, more commonly known as NIST. This competition was started in 1997 to find a cipher to replace DES, which was the Data Encryption Standard, which had been a standard for the previous 30 years. That contest ran from 1997. They had a very open process. This is very unlike the process that was used to select DES as the previous recommended cipher. This lead to 15 submissions for round 1. Some of these were actually completely broken. Others were rejected for other reasons. This was narrowed down to five finalists, none of which were seriously broken, and one winner, which was selected. The main criteria for selecting the winner of AES were the security of the ciphers, and this is really the hardest thing to measure. We've seen that other than the one-time pad provable security is not achievable for most ciphers and not achievable for any cipher that was a candidate for AES. The way this was measured was trying to estimate the security. The main metric for measuring security was looking at the actual number of rounds in the cipher and dividing it by the minimum number of rounds that were breakable in some sense. Breakable here is very much the academic of breakable. You didn't need to be able to extract a message or a key. Anything that showed you could reduce the search space even a little bit would be enough to show that it was breakable for that number of rounds. The other properties were easier to compare and measure, which were speed-- implementing it both in hardware and in software--and simplicity. Simplicity is usually against security. To have higher security we want more confusion. We want to do more transformations to the data. That goes against simplicity. Simplicity aids the analysis to make it more clear whether the cipher is secure or not. The winner of the AES competition was a cipher known as Rijndael, which was developed by two Belgian cryptographers. The good thing about it winning AES is now we don't have to figure out how to pronounce it correctly in Belgian. It's now called AES. The best known current break against AES is very theoretical, so with 128-bit keys, that would be a brute-force attack that tries every possible key. It would require on average 2^127 attempts. You're expected within half the 128-bit keys. You've got 2^128 total keys. You expect to find a break after testing half of them So they expect it costs a brute-force attack 2^127. The best known attack removes less than 1 bit of security. That's the best known today. And 126 bits of security is well beyond what's practical to imagine implementing even with a huge amount of computing resources. [Evans] We're not going to get into the details of how AES works, but I want to talk a little bit about the main components of it. There are 2 main things that go into AES and go into almost all modern block ciphers. They all involve XORs--we saw that in the one-time pad-- and they're XORing some round key which is generated by a key schedule, some process of generating new keys for each round. And then what's going into that, there are 2 main operations. One is shifts, so permuting bits, and there would be a map that would move bits around. This is better than just doing only XORs because we're moving data around instead of just XORing and changing it. The other thing that's really important for a cipher to be hard to cryptanalyze is to have some nonlinearity, something that is very difficult to analyze and mixes up data in a way that is nonlinear. This is done basically by having lookup tables. So what's called an S-box is something that takes in 8 bits-- in this case it could have different numbers going in-- and basically has a lookup table. So that's going to have 256 entries mapping each set of 8 bits to some other set of 8 bits. And designing that lookup table is a challenge. We want the lookups to be as nonlinear as possible and make sure there are no patterns in the data in this table. And so the way AES works is combining shifts and S-boxes with XOR to scramble up the data. And it's going through multiple rounds, so we'll take the outputs of this, put them back through a series of shifts and S-boxes again, and keep doing that. The number of rounds depends on the key size. So for the smallest key, for the 128-bit key, which is the smallest key size for AES, we would do 10 rounds going through the cycle, getting the output cipher text for that block. So the details are definitely more complicated than this, and getting them right is very hard. There's lots of great resources that explain AES in detail, though. For our purposes we're going to think of it as a black box-- that we can use encryption in our protocols. We're going to assume it has the properties that we want it to have and not need to look in more detail at how to actually implement that in a modern cipher. [Evans] So we've reached the end of Unit 1. Let me remind you what we've covered. We covered symmetric cryptosystems and introduced the terminology and definitions we need to talk about encryption. In particular, we know what it means for a cryptosystem to be correct-- that encryption and decryption are indeed inverses. Defining security is a much trickier thing, and we talked about ways we could define security for a symmetric cryptosystem. We introduced the one-time pad, which is a very simple but important cryptosystem, and it's all based on using the XOR operation. As long as we have a perfectly random key that's as long as the message, the one-time pad gives us perfect security. We looked at a formal way to define what a perfect cipher means and prove that the one-time pad has that property. We also saw that in order to be perfect, a cipher has to be impractical-- that the number of keys has to exceed the number of messages, and that means that every cipher that's used in practice is potentially breakable. We saw one very interesting example of that with how the Allies at Bletchley Park were able to break the Lorenz cipher. We talked about modern symmetric ciphers which take advantage of computing power, following many of the same principles of the historical mechanical ciphers like Lorenz but using modern computing power and new ideas about how to scramble data to produce much more confusion and make things much more challenging for cryptanalysis even when attackers have access to the huge amounts of computing power available today. I hope you enjoyed Unit 1 and have a good understanding of some of the theory behind symmetric ciphers and how they're constructed. In Unit 2 we'll look at how to actually use symmetric ciphers to solve problems like sending a message securely between 2 parties over an insecure channel like the Internet and being able to use symmetric ciphers to play games online and to do important things like managing passwords. Hope to see everyone back for Unit 2. So in unit one, we learn about symmetric ciphers. In unit two, we're going to learn about how to apply symmetric ciphers to solve problems. So for the rest of this course, we are mostly going to view ciphers as black boxes. They provide two functions: encrypt and decrypt. Encrypt takes in the message from some message base and produces a cipher text. And, it also takes in a key from some key space. Decrypt is the inverse. It takes in a cypher text and a message. And if it takes in the same key, it will produce the same message that we got. And our correctness property is that the decryption with the same key of a message encrypted with that key gives us the same message. For the rest of this unit we are going to assume that we have some function that provides encryption and decryption. AS is a good choice for most applications. And we're going to look at how to use these functions to solve problems. The first thing we're going to look at though is how we actually get this key. All of our assumptions about security depend on this key. And we have two key assumptions. The first is that the "k" is selected randomly, and uniformly. This means each key is equally likely to be selected, and there's no predictability about what the key is.The other big assumption about the key is that it can be kept secret. The adversary can't learn the key. But, it can be shared between the two end points. This is a big challenge. We're not going to look at that yet this unit. We'll get to that later. This is the big problem of key distribution which we'll talk about starting in unit 3. Now I want to talk about this first problem though. that we need to select a random key. So if we're going to generate random keys, we need some understanding of what randomness is. This is a very deep, philosophical question, and I'm not much of a philosopher, so let's try a quiz instead. So the question is which of these sequences is the most random? So here are the sequences. I won't try to read them, but see if you can figure out which one of these is the most random. And I should warn you in advance this is a bit of a trick question, if you haven't already decided that. So the correct answer--other than refusing to answer the question, which would be a very correct way to answer such a question-- the correct answer is the third choice. And there's some ways to know which ones are not correct. I'll show you soon how each of these sequences was generated. Well, if you look at the first one, you see there's a sequence of 0-1-1-0-1-1 that keeps repeating. Anything with a repeated pattern like that is definitely not random. Now, the caveat is certainly if you generated a long enough random sequence, eventually it would contain this pattern inside. The other one that is probably the trickiest one-- that when humans generate random sequences they look sort of like this. The property that this sequence has is there's never more than three repetitions in a row. And when humans are asked to generate random sequences, they usually don' t have long runs of repetition, but in real random sequences there are. You can see there's a sequence of five 0s here. There's a sequence of four 1s. There's lots of repetition. In this there's never more than two in a row, so this is not random. The other two look potentially random. You'd have to analyze them a little more carefully. This one has more structure. This one is actually random. Let me show you--and I have to say, this one is actually close to random. And let me show you how each of these was generated. Here's how I generated each of those sequences. So the first sequence was generated by generate<u>sequence.</u> I'll show you what generate<u>sequence does next.</u> But for each position in the sequence, the procedure that generates the output generates 0 if the position is divisible by 3, otherwise it generates 1. And we do that for the length of the sequence, which is defined as 88. Display-bits turns a list of bits into a string. All this code will be available linked from the course. So that is what generated the first sequence. And the code for generate<u>sequence is here.</u> It just maps the function onto the range from 0 to n minus 1. So for that length, we're seeing the output of the function at every position. So that's the first one--and clearly non-random. The second one is taking a string, converting it into bits, and displaying those bits. And the string is this great quote from John von Neumann. And what the quote says is that "anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin." But, of course, we do it anyway. So definitely, that's not random. It's generated from a very specific string. If you knew the string and you knew what these procedures do, you could generate that sequence. And the sequence that it generates is the one that as the second choice here. It looks sort of random, depending on how carefully you look at the patterns, but it's definitely not random, because it comes from this quote from John von Neumann. The third one is actually designed to be random. So we're using the Random Choice Method. That's provided in the PyCrypto library that I'm using to generate random numbers. It claims to generate fairly good cryptographic random numbers. So this third sequence at least is generated in a way that is designed to be random. The fourth one is generated using this procedure called generate<u>fake<u>random,</u></u> which starts by making random choices, but is designed to trick humans by not having repetitions greater than 2. And I should change my comment. I had originally done it with >3, and that looks to most humans very random. And I know it was a trick question, but I thought it would be a little too tricky-- including >3. But this is a good test for humans. And one of the experiments people sometimes do is ask humans to come up with a sequence of random numbers-- that they think are random--from their own head, and they come up with sequences that are easily recognizable as non-random because they don't like to have too much repetition. That question assumes some notion of randomness, but is a very fuzzy concept to define. What I think is the most useful definition of randomness is the one that Andrey Kolmogorov came up with, which is based on Kolmogorov Complexity. This is an idea that was developed by the Russian Andrey Kolmogorov as well as by the Argentinian Gregory Chaitin. The idea is a way of measuring the complexity of some sequence. And it's defined as the length of the shortest possible description of that sequence. For this to be well-defined, we need to understand more precisely what a description is. One way to do this would be to say it's a turning machine and decide on some formal way for writing down a turning machine. But we could use any method of describing algorithms that we want. It could be a Python program. Whatever we select as our description language, Kolmogorov Complexity is well-defined, and as long as that description language is powerful enough to describe any algorithm, it's a reasonable way to define complexity. And we can use that to define a notion of randomness. Some sequence is random if the shortest way to describe that sequence is the length of the sequence plus some constant. That means, as we make the sequence longer, the description gets longer at the same rate. This matches the notion of randomness that we used informally in the quiz. Then if there's a short program that can produce the sequence, that means it's not random. It has some structure to it, and the program shows us what that structure is. If there isn't any short program that can describe that sequence, well, that's an indication that the sequence is random-- that there's no simpler way to understand that sequence other than to see the whole sequence. So this seems like a useful notion for understanding randomness. We're going to ask a quiz to see if it seems like a useful notion form measuring randomness. So the question is for a given sequence S, is there a way to compute the Kolmogorov Complexity of S? The choices are that we can. That is the length of S plus some constant. So the second choice is, yes, we can. It's impractical to do this, but here's an algorithm for doing it. We can start by initializing n to 1. We can have a loop that keeps going until we find the correct value of n, which is the Kolmogorov Complexity of the string S. And we're going to do that by looping through all the programs of length n. This is a big step, but it's finite, so, in theory, that would eventually finish. And for each program, we're going to execute that program and see if it produces S as its output. If it does, that's the result that we want. And the third choice is no, it's theoretically impossible. The answer is that it's theoretically impossible to compute this for an arbitrary sequence. The first answer is not correct. If S is truly random, this would be correct. But if S is not truly random, there might be some shorter way to compute it. So this gives the maximum value of the Kolmogorov Complexity of sequence length S. And an example of such a program would just be this Python program, that would be the program "print" + S. That would print out the sequence S. It's length would be the length of S plus the 5 characters for the print plus 1 for the space. But that doesn't prove that there isn't a shorter program that can produce S. The trick in the second answer is this part. This assumes that we can execute P to completion. In order to do that, we would need to solve the halting problem. We would need to know that P will either never finish or know how long to wait to keep running P. So we can't actually do this step. This might run forever; this loop may never finish. So we would never learn the result. So that's why this doesn't work. This is not enough to prove that it's impossible, because maybe there's some other way to do it. And I'm not going to go through how we actually prove that the Kolmogorov Complexity is uncomputable, but I'm going to show you a paradox that will give you a little bit of an idea why that is. The paradox is known as the Berry Paradox. What the paradox asks is, "What is the smallest natural number--" and by that I mean the set of numbers 1, 2, & 3-- sometimes we'll design it starting from 0-- the paradox would work just as well either way-- "that cannot be described in eleven words?" So this seems like a reasonable question. We can define the set of numbers that cannot be described in eleven words. It seems like a perfectly reasonable description of a set. It's also a case that any set of natural numbers has the smallest element. The numbers are well-ordered, so whatever that set is, there must be some smallest number. So that means there should be an answer to this question-- that there is a smallest number that cannot be described in eleven words. And I'm going to describe it for you. Here's my description. And my description has 1-2-3-10-11 words. So I just described the smallest natural number that cannot be described in eleven words, but I've described it in eleven words. So that suggests that no such number exists, but that contradicts the properties we had before. That's why this is a paradox. This is certainly not a proof that the Kolmogorov Complexity is undecidable, But it should give you some sense of how we might prove that. So, Kolmogorov Complexity gives us an interesting way to think about randomness. It's certainly a useful notion in many areas of computer science. But, it's not going to give us a practical way to tell if the sequence is random. It won't even give us a theoretical way to do that because it's uncomputable. The next thing we might think about trying is statistical tests. And, this in fact often used as part of an argument that something produces random numbers. It should be part of that argument, but it can never be the whole argument. The only thing we can do with a statistical test is show that something is non-random. We can't ever prove that it is random. We can always find some non-random sequence that satisfies all of our statistical tests. So, we're left with our best argument is how the sequence is actually generated. If we know how the sequence is produced, we can have some confidence that it satisfies the randomness requirements we have, at least for generating a good key. And, the real requirement there is unpredictability. We want to know if our sequence is a sequence of numbers like this; where each xi is under the reign of 0 to 2 to n-1 That even an attacker who's seen the first m numbers in the sequence can only guess the next one with the same probability they would have if they were just guessing by chance, only 1 out of 2, 2 to the n, the size of the elements in the sequence. So, that's the property we want. If we're going to get that property--and we can't do it by using Kolmogorov complexity because that's uncomputable and we can't do it with statistical tests because they can only show non-randomness-- the way we're going to try to achieve that is by generating the sequence in a way that produces values that are unpredictable to the attacker. And Quantum Mechanics provides a notion of random events --that there are events in the universe that are inherently random-- and we can count things like radioactive decay with a Geiger counter and use that to generate randomness from physical events. Thermal noise is an easier thing to measure in most circumstances. If you can measure that precisely enough--it also depends on Quantum Mechanics-- at some level and produces randomness. And many modern processors have a way of generating a small amount of randomness by measuring thermal noise in the processor. Whether it's really physically random depends on a lot of other things. You can also look at things that actually happen, and think that they are random. Maybe if they're key presses or user actions-- maybe those are random. An example of this is when we generate a new key using GPG, it will ask you to generate --when you start to generate a key--it says we need lots of random bytes and you can perform some type of action like moving the mouse using the disc to help generate more randomness for it. And humans aren't good at doing random stuff When we move the mouse, we're probably moving it in a pattern-- When we type on the keyboard, maybe we're doing things that are not very random. So unless you're generating your randomness from quantum physics, there's always some question whether it's really random enough. Or whether you can predict the particular motions I took. And certainly given that this has been recorded and released, the fake key that I generated for Alyssa B. Hacker should not be used for any secure purpose. So this approach of waiting for physically random events is OK for GPG, maybe, because someone using it is probably patient enough to sit around for a while, doing random stuff as well as a human can to generate a key. This would not work very well when you need more randomness more quickly. And this happens every time you do a web transaction. Every time someone does a secure web session, any time you see the lock key in your browser, there's a protocol going on called TLS. We'll talk about that more in a later unit. But one thing that that requires is a new random key. for each secure web session. I don't think many people would tolerate being asked to move around their mouse and do strange things to generate enough randomness in the hopes that that key is secure every time you connect to a website. So we need something better. We need a way to take a little bit of physical randomness, and that's usually known as the seed-- that's the initial state, and that's the input to what's known as a pseudo-random number generator. And that produces a long sequence--that is longer than the amount of physical randomness we started with--of random bits. So that's our goal--to take a small amount of physical randomness --some source of entropy that we can use as a seed-- have some function that will compute from that seed a long sequence of apparently random bits. So for this quiz, I want to see if you can figure out a sensible way to do this. So we've got some pool of randomness... and one approach would be to extract a seed from that pool, use our Encryption function, we'll make the Seed the Message, we'll select the Key as Zero, we'll get an output from that, and we'll make a copy of the output. That's going to be our first random value. Then we'll encrypt again, using the previous Output as the new Input, using the key Zero again and extracting the next random number. So the second option: we're going to extract the seed from our random pool, we'll use that as the key to encrypt and for the first random output, we'll get the result of encrypting Zero with that key. For the next one, we'll get the result of encrypting One with that key. We'll keep using the same key, encrypting and getting a new random value as our output. And for our third option, we'll extract new values from the random pool each time, we'll encrypt a message, selected from the random pool, with the key selected from the random pool, to get X<u>0.</u> To get the next random number, we'll extract a new message from the random pool, and a new key, and do that encryption. So which one of these options makes the most sense for a pseudo-random number generator? And let's assume our encryption function behaves well--it provides a mapping between keys and message that's hard to invert if you don't have the key. And that we have a random pool that provides a limited, but good, source of randomness. The best option is the second one. The problem with the first option is that it doesn't provide the unpredictability property that we need. If an attacker knows X_0, they can easily compute X_1. And that's a property our pseudo-random number generator needs to have. The third option seems reasonable. It requires a lot of randomness in the pool. But if we have that much randomness in the pool--to extract a new random value for each random output-- we should just use that. If we do have enough randomness for this there's no reason for all these other steps. We should just extract something for the random pool each time. We're not able to do this--we're assuming we don't have enough randomness to do that. So we're using things that aren't actually random here if we're extracting them from the random pool more quickly than we're actually able to produce new randomness here. It's doing a lot of extra work, but it's eventually starrting to use values that are not random as the inputs to our encrypt. And if those values are predictable, well then the outputs become predictable, too. So that doesn't work--so the best solution is this middle one where we're extracting the seed once, we're reusing that seed, and we're encrypting a sequence of values which is--can just be a counter-- and each time using the output of that encryption. So this is a good start for a basic design for a pseudo-random number generator. We have some pool of randomness, we extract a seed from that pool, we use that seed as the key to our encryption algorithm, and as the messages, we use a counter. So suppose we use AES 128--so that means the key size and the block size are 128 bits, and so each time we do this, we get 128 bits out. which we use as our random values. And this counter will go up to (2^128) - 1, and then go back to 0. So now I have a quiz about how well this works. So the question is: does this produce a sequence that appears random? And let's say... for the first 2^70 outputs--so certainly if we have more than-- 2^128 outputs, well, these countervalues repeat. So that would be non-random. The choices are: "Yes," "No," because it repeats values too frequently, or "No," because it repeats values too infrequently. The answer is no. It repeats values too infrequently. This may seem a little surprising. If it was random, well the probability of two consecutive values being equal should be to the negative 128, a very low probability, but not zero. If this is an encryption cipher, well it satisfies the property of invertibility. So any value thats different here encrypts to a different value. So that would mean that for our actual pseudo random number generator, this probability is actually equal to zero. And thats different from what it would be in a real random sequence. This probability is low enough that we still wouldnt expect a repetition for a long time, but we would expect to have one within the first two to the 70 outputs. And we will talk soon about the birthday paradox to see why we would expect one this soon. Still a pretty big number, but it would be enough to distinguish this from pure randomness. So to fix that, there is a couple of things we could do. One is to occasionally change the key and after some number of outputs we can take one output, instead of using it as an output we use it as the new key. So even if we dont have any more randomness, we will still change the key; that will affect this probability, it will no longer be zero, we will start seeing values that have some probability of repeating what we saw with the previous key. So well get a new key every few million outputs. The other concern is whether this pool of randomness is good enough? On Unix machines there is a pool of randomness stored in /dev/random and it is collecting events that are believed to be random and out of the control of the attacker. These could be user actions like key presses; these could be things that you collect over the network. There is some risk that these could be compromised, that some of them could be controlled or at least predictable to the attacker. So we want to have multiple pools of randomness and combine them in a way that makes it very difficult for an attacker to control this seed. And this is essentially what the Fortuna Pseudo Random Number Generator does and thats one of the more popular widely used ones. It does use this routine to avoid this apparently non-random property and it uses multiple pools of randomness selected in a way that makes it very hard for an attacker to control what the seed is, even if the attacker can control one or a few of the pools of randomness. So the key point is if you're generating keys, it's really important to use good randomness to generate keys in a way that makes them unpredictable. And there was a study recently that looked at public keys. We'll talk about asymmetric crypto in a future unit. But the problem in generating those keys was not using good randomness. And this was widely reported in the press--often in a somewhat misleading way-- talking about the encryption system being broken, which it wasn't. All that was broken was--if you pick bad keys, the encryption doesn't work well. So you should be careful not to use the built-in random generator in your programming language. It probably is not cryptographically secure. There are good random number generators provided by cryptography libraries--like the Pi-Crypto library that we'll use for some of the code in this class that implements the Fortuna random number generator. So now that we have an understanding of symmetric ciphers, and we know how to generate a random key, we're going to talk about an application, which is to store a file securely. And our goal here is that we have--this is the computer-- it may look more like a stapler, but it is intended to be a computer. And we have on that computer, we have some file-- what we'd like to do is store that data on the computer, know that if we leave the computer in a coffee shop and someone captures the computer, they won't be able to read the contents in that file this is to prevent the kind of incident one hears about fairly frequently in the news, where someone has a list of Social Security Numbers for everyone at the DMV, at the drivers' licensing office, and they leave that on their laptop and it gets stolen and then you've got a big worry about all that personal information being lost. Probably that kind of file shouldn't be stored on someone's laptop in the first place, but our goal is to be able to store files that contain sensitive information and know that even if the computer that they're stored on is stolen, the contents of the file will still be secure. Now we don't want to just throw the file away, we want the owner of the file to still be able to read the file. And so we're going to assume that there's some key that the owner of the file can store in some other place that's secure. We'll talk a little bit later--how we might want to do that. But let's assume there's a key that can be used to decrypt the file. So here's the straightforward way to do this: we're going to take our file-- we'll call it m--and we'll divide it into blocks and the block size will depend on the cipher we're using. Let's assume we're using a cipher with 128-bit block size, which is the size that AES uses. Then what we'll store is the ciphertext, and each ciphertext block is the result of encrypting using the key k, the corresponding message block. So how well does this work? Let's try a quiz. So the question is: Assuming that E has perfect secrecy-- we know this is not true, because we're reusing the key to encrypt multiple messages and Claude Shannon showed that this would only be possible in the case where the number of possible keys must exceed, or be equal to, the number of possible messages. And that's not the case here. But let's assume that for now. What can an attacker learn if they capture the laptop and acquire C? Check all the choices that are true--the choices are: nothing, the length of m, the value of k, which blocks in m are equal. The answer is the attacker can learn both the length of m and which blocks in the message are equal. The attacker learns the length because the length of this ciphertext is actually equal to the lenght of m. The output for encryption functions is the same length as its input so we know that these lengths are equal. It's assumed that file contents m were an even number of blocks, since we're dividing m into If it's not, we'll have to do something about that. And we'll get to that soon, but the solution to that is to add padding. The other issue, which is a more serious one in this case is the attacker learns which blocks in m are equal. This may not sound like such a big problem, if we're thinking of the blocks as 128 random bits, maybe the probability that two blocks are equal is actually very low. We'll see later--this class is not quite as low as you might guess but the message is not random bits. The message, or the file, if we're thinking of these as 8-bit characters, well, 128 bits is only 16 characters. If they're Unicode characters, well then we have more than one byte per character. This could get down to a pretty small number of characters. There are certainly lots of sequences in many files that would be 16 characters long. that could be repeated. So this is a pretty large amount of information to leak. We want to find a way to encrypt our file that doesn't do that. Ways of using ciphers like this are called Modes of Operation. The one that I just described is known as Electronic Codebook Mode. And the reason for that is that you can think of having a book that for each input message, 128 bits of input, there are (2^128) - 1 inputs You would be able to look up the value of encrypting that message. So you can have a codebook that gives you for each input the 128-bit output that corresponds to that input. And this would be a really big book if we assume we could get about 10,000 entries on a page, and each page weighs five grams, well, this would be about 1.7 * 10^32 kg to carry around. Adn that's just for one key. So it's really not practical to think of this as being a physical codebook today, but this is really the same thing that early codebooks did. They just provided one-to-one mapping between inputs and keys. And that's exactly how we're using AES here is to provide that mapping, and we're using the same mapping every time we need to encrypt the same value, we're getting the same output. This is the problem that we mentioned earlier, that it doesn't hide repetition in the message. Another problem it has is that someone could scramble-- an attacker could move blocks around--could replace blocks--could change things-- and it would still decrypt to a perfectly valid message, just with the blocks in a different order. So those are problems with the Electronic Codebook Mode We're going to look at some alternatives that avoid some of those problems. One way to avoid some of those problems is to use Cipher Block Chaining. The idea here is that we use the ciphertext from the previous block to impact the next block. So here's what this looks like. So we're still going to break our message into blocks. So this is the idea of Cipher Block Chaining. We're going to take each message block, encrypt it, with our encryption function, although let's assume it's still AES, using the same key, so we're using the same key for each block. We're going to get as output a cipher text. Instead of doing each block independently, though, and having the codebook property, for the second block, we're going to take the ciphertext that came out for the first block, EXOR that with the message block, and then make that the input term of the encryption function. So this keeps going. This means, as a result, in CBC or Cipher Block Chaining mode, the (i)th encrypted block is the result of encrypting the EXOR of the (i)th message block with the (i-1)th encryption block. We have a little bit of an issue with the first one. The first one, well there's no 0th block. If we just did what was shown here, well then we'd still have a problem that we would see repetition every time the first block in a file is the same as the first block in another file, encrypted with the same key. We'd get the same C<u>0 out.</u> So we don't want that. We're going to add what's called an "initialization vector," and we'll EXOR that with the first message. That keeps things consistent --each message is being EXOR'd with something before it is encrypted-- and this might worry us--that we're adding more secrets-- we want to minimize the number of secrets--to be as few as possible-- the IV doesn't really need to be kept secret. Note that we're still encrypting this output. It's helpful to not reuse an IV, though. So it's OK to make the IV something unsecret, as long as it's not always the same. So the question is: suppose that Alice forgets the value that she used for the initialization vector--she encrypted some file, she has the ciphertext and she has the key. Can she still recover n, even though she forgot the IV? --the Initial Value-- So the answers are: "No," she can't recover any of them; "Almost," that she can recover all of them except for the very first block; "Almost," that she can recover all of them except for the first and the second block; or that she can only recover the very last block of n. The answer is the second choice--that you can recover almost the full message-- everything except for the very first block-- that the point of the initialization vector is just to hide repetition among encryptions that would appear just looking at the first block. And the reason for this--we can look at how the encryption mode behaves-- We saw that for all of the blocks except for the first one, the value of C<u>i</u> is the encryption of the value m<u>i--include my key there--and we saw for</u> the way the encryption mode works, C<u>i is equal to the encryption</u> using the key K of M<u>i EXOR C<u>(i -1). The exception to that is block C<u>0.</u></u></u> Where that's the value of encrypting m<u>0 EXOR'd with IV.</u> So we didn't explain how to do decryption. But from the way the encryption was, you should be able to figure that out. We can look at this backwards--so in order to get the last message block-- well, what we need to do is decrypt using key K, and input to decrypt is this last ciphertext block. So we're going backwards-- we're decrypting. We don't have the message block yet. To get the message block, We need to do the EXOR to get the message block N - 1 and so that means we're EXORing that with the ciphertext value of the previous block, which we already have. Remember we have--to decrypt, we start wtih all the ciphertext blocks. So this is how we decrypted the last block, but each block is the same. To get message block i, we need to decrypt ciphertext block i, and EXOR that with the previous ciphertext block. So we can do that for all the blocks, except for--we have this exception for the last one. The encryption for the last one used this IV-- to get the last message block, what we need to do is decrypt the last ciphertext block--or the first ciphertext block--we're going backwards now. And then EXOR that result with the IV. So if we lose the IV but don't lose the key, and don't lose the ciphertext, we've lost just the first block. And if the IV was selected perfectly at random, well, we have no information at all about the first block. Because whatever we get out of this decryption is EXOR'd with that IV to get the message. So if we have no information about the IV, we have no information about the first message block. But we can decrypt all the other blocks. The next mode of operation I want to talk about is called CTR mode, which stands for "Counter." We have a message divided into blocks, just as before. We'll have our encryption function, just as before, and we can think of it as AES or any other block cipher. And that takes a key as input--we'll use the same key--but instead of just having a message block go in, what we're going to do instead is have a counter: some value that cycles through the natural numbers. That's going to be our input message, so we'll get, out of that, some cipher text. And what we're going to do now--well, so far none of this has anything to do with the message. Right--we've just encrypted the counting values from Zero to n - 1. What we're going to do is EXOR those--EXOR the outputs, here-- with the message, so the message box goes into these EXORs and what comes out is the cipher text box. If we did it just like this, we wouldn't have quite as much security as we would like. We'd be vulnerable to attacks that search the space of counters. We'd also be vulnerable because we're using the same sequence of counters for every file that we encrypt with the same key. So the solution to this is similar to what we did with the initialization vectors in the previous mode. What we're going to do is add a nonce. We'll append the nonce with the countervalue. And a nonce is very similar to a key. A nonce is a one-time, unpredictable value. Unlike a key, it doesn't need to be kept secret. The point of the nonce is to make sure every time we use counter mode with the same key we get different blocks out. So as an example with AES, if we have 128 bits as the block size, we might have a 64-bit nonce and a 64-bit counter. So let me summarize these two modes. So we saw Cipher Block Chaining mode, and we saw Counter mode, and CBC mode--the "i"th block of the ciphertext, is a result of encrypting using the key. The "i"th block of the message with the previous ciphertext block-- and we need a slightly special case for zero, which would use, instead of the -1 ciphertext block, which doesn't exist, would use an initialization vector. With Counter mode, the "i"th ciphertext block is the result of encrypting the value of "i"--that's our counter-- with some nonce, and I'm writing this as concatenation-- so we have 64 bits here pasted next to those 64 bits for the counter and the nonce, and that is EXOR'd with the corresponding message block. To do decryption with Counter mode, well the "i"th message block is the "i"th ciphertext block, EXOR'd with this same value, which, as we know the key, we can decrypt. So that's how decryption is done. With CBC mode, the "i"th message block is a result of decrypting the "i"th ciphertext block and EXORing that with the previous ciphertext block. Or in the case of the very first block, EXORing that with the IV. So now it's time for a quiz to see if you understand the differences between CBC mode and Counter mode. So the question is: If Alice wants to quickly encrypt a large file and she's got a multi-core machine, or she's got access to a large distributed computing resource, which of these two modes will allow her to encrypt the file more quickly? The choices are that it doesn't matter--that CBC mode would be faster or that CTR mode would be faster. The answer is the Counter mode would be faster. The reason for this is with CBC mode, there's a data dependency. Then we can't start doing this encryption until we know the value of the previous ciphertext block so we can't run all these encryptions in parallel. That's one major drawback of CBC mode. With Counter mode, well, we can actually compute all these encryptions before we even know the message. These values and the expensive operation here is not the EXORs, EXORs are very cheap. Encryption is expensive. So we could actually pre-compute, for a given nonce, all these encrypted counters, And then once we know the contents of the file, we can do those EXORs, also in parallel, but those are very quick. So all of these encrypted blocks could be computed in parallel using many processors. So that's a big advantage of Counter mode over CBC mode. The last mode of operation I'm going to talk about is called Cipher Feedback Mode--also known as CFB-- There are many different modes of operation. We won't talk about them all. But the ones that I've talked about will give you a good sense of the modes of operation work. This one's a bit different from the others that we've seen so far. It does use some encryption function as a black box, like the others, We'll call the input to that encryption the X-values. So X<u>0, X<u>1, X<u>2... are the inputs</u></u></u> of successive encryptions. So the first one will be an initialization vector Similarly to how we've used that in other modes of operation. And that would be in the input to encrypt. There's also a key. The output of encrypt is some encrypted block. We'll use n as the encryption size. So for AES, we'll assume n is 128 bits. Whatever the block size is--so that size of input block and the size of output from encrypt is 128 bits. This cipher has an additional parameter, which we'll call S, and S is how we're going to divide the output. We're going to take the first S bits of the output and those will be EXOR'd with that message b lock producing the output cipher text. This looks very similar to CFB, except for we haven't used the entire output here The other thing that we're going to do is we're going to use the cipher text to update the X-value--so we're going to take these S-bits we're going to put them into the next X-value and we're going to move the old value S-bits to the left. So that means we're going to be taking the n - s bits that are the right part of the previous value of X<u>0 and we're going to be moving those into here.</u> Everything else proceeds the same way--with--we encrypt the X-value-- we get our output block--we take the left-most S bits of it we EXOR that with a message, we get our ciphertext block. and this keeps going. So we can describe that process first we'll describe what happens with X-values So value X<u>i is the result of taking the previous value of X<u>i</u></u> so that's value X<u>i - 1, so the left value of X<u>i.</u></u> Taking from position n - S to the end, I'm going to use Pythonic notation for this-- we're taking from position S to the end of the previous value of X and we're concatenating that with the value of the previous ciphertext. So this is to find--as recurrence--we need to find the initial value and that was given by the IV-- the Initialization Vector--so that's how we update those values-- how we compute the ciphertext values--we compute the ciphertext values by taking the outputs here--that's the result of encrypting using key K--the X-value for that position-- And we're going to take just positions up to S, and then we're going to EXOR that with the message. So this is how we compute the ciphertext blocks in the Cipher Feedback Mode. The important thing that you should notice here is that there's this additional parameter S, and what S means is that size of the message block. And the value of S should be less than the value of N --that's the normal block size of the cipher--otherwise, we wouldn't have any input left--it would end up being a different mode. Now we'll have a couple of quizzes to see that you understand Cipher Feedback Mode. The first question is, how does one decrypt a message encrypted using Cyper Feedback Mode? Here are the choices. The first choice is we have to go through the blocks backwards, XORing out the cipher text from the results of encrypting the X values, and we can compute the X values in reverse order like this. The other choice is we do the decryption going forward, XORing out the cipher text with encrypted Xi values and updating them as we did in the encryption mode. A third choice is the same as the second choice except instead of using the encryption function, we're using the decryption function, which is the inverse of the encryption function. The answer is the second choice. The first choice doesn't make any sense at all. The difference between the second choice and the third choice is whether we use the encryption function or decryption. The reason that we're using encryption function just like we did in encrypting is because when we're decrypting we have these cipher text, and we're trying to get the messages, so we're just trying to go through these XORs in the reverse direction. That means we need the same exact thing that we had in encryption to XOR out, which is the result of encrypting these Xi values. For this quiz, I'm going to list some properties, and you should check the box for the ones that are true for the given operation mode. So which modes require that E--the encrypt function that we've been using in the black box-- is invertible? Check this box if Cipher Block Chaining mode requires this, and check this box if Cipher Feedback Mode requires this. Which of them require the Initialization Vector to be kept secret? Can use small message blocks? Which can provide strong protection against tampering, so Alice would be able to know if someone had changed values of parts of her file? And the final one--that the final cipher text depends on all of the blocks in the message. Here are the answers. CBC does require the encryption function to be invertible because to do decryption we need to use the inverse of the encryption function. CFB does not, as we saw both encryption and decryption use the encryption function in the forward direction. This has advantages--that means there are more possible functions that we could use for this. We haven't seen any yet, but soon we'll talk about hash functions, which could be used as the encryption function for cipher feedback mode. Neither of them require the IV to be kept secret. One way to see this is if you look at the structure of the ciphers, they're using the IV as though it was ciphertext block -1. All the other ciphertext blocks are visible to the attacker. So if you think about how it's used, it's used just like another ciphertext block. There's no security required in keeping the IV secret. What's important about the IV for both of these is that it's not reused, that a unique IV is used to avoid the possibility of the same block encrypted the same way. The big advantage of cipher feedback mode over cipher-block chaining mode is this ability to use small message blocks. We can select the value of s and only encrypt the message in chunks of size s. This means we can use this mode to turn a block cipher into a stream cipher, where we're encrypting messages 1 bite at a time, if we wanted to do that. It's not necessarily the best way to design a stream cipher because we're doing a lot more work than might be necessary. This one is maybe a little unfair to ask since we didn't talk about this, but an important point to make is that neither of these, as we've described them, provide any strong protection against tampering. An attacker can modify. The blocks can move blocks around. The decryption may or may not look like a valid decryption depending on what the contents are, but there's no way to easily detect that there's been tampering with either of these modes. We'll talk about in a future class ways to provide message authenication that would make it so you can detect when tampering happens. The final property is that the final ciphertext blocks depend on all of the message blocks. This is actually true for both. This turns out to be a property that's very useful for what I'm going to talk about next, which is cryptographic hash functions. I'm going to motivate this with a somewhat silly example, but something that will illustrate many of the properties that are important for cryptographic protocols. As usual, we have Alice and Bob. She's Alice, in case you cannot recognize her, and we have Bob. Alice and Bob want to decide whose turn it is to charge the Robo-dog. If you can't recognize my drawing, this is indeed a Robo-dog. But they're not in the same place. They're trying to do this over the telephone. They would normally do this by tossing a coin. So they decide, let's try to toss a coin over the telephone. So this is the first protocol they try. Before describing the protocol, I want to define more carefully what I mean by a protocol. We've actually seen a few already, but I haven't defined what a protocol means. A protocol involves 2 or more parties, and what it is, it's a precisely defined sequence of steps, and what each step can involve is some computation as well as communication. Communication can involve sending data between the parties in the protocol. One way to think of this--it's really the same thing we mean by a procedure in computing. That's something that we can define precisely enough that we can follow it mechanically, but unlike a procedure, which is thought of normally as being followed by 1 processor or at least a set of processors controlled by the same party. With a protocol, we have 2 or more participants involved, so that's why it involves both computation as well as communication. In this class, we're mostly concerned with cryptographic protocols, which means 1 more thing--that it involves secrets. The usual reason we want to involve secrets is because we want it to be a security protocol. A security protocol means that it provides some guarantees, even if some of the participants cheat, and that means they don't follow the steps as specified. So now let's describe our first protocol for this, and this is intended as a cryptographic security protocol. It's participants are 2 parties: Alice and Bob. The Robo-dog that needs charging is certainly involved in this process, but is not an active participant in the protocol. Here are the steps. The protocol starts by Alice initiating it. She initiates the protocol by sending Bob a message that says, "I'll toss the coin, you call it." Bob calls the coin and so he sends a message back to Alice that says, "I call tails." At this point, Alice tosses the coin, and she responds to Bob that sorry-- with the results of the toin coss. In this case, she'll respond, "Sorry, it landed on the edge," and it's Bob's turn to walk the Robo-dog. So I think I won't insult your intelligence by asking a quiz about whether this protocol works well. If it was a nonsecurity protocol, and Alice and Bob trusted each other completely, maybe it would work. Bob can trust Alice to actually toss a coin and tell Bob the real result. If it's a security protocol, it doesn't work at all. If Bob doesn't trust Alice, Alice decides the result whether Bob wins or loses. So we need something better than this if we want to deal with mutually distrusting parties. We'll call this protocol 2. In this protocol, Alice will start by doing 3 things. She'll pick a value for x. There are 2 possible values--0 and 1. We can think of 0 as representing heads and 1 representing tails. She'll also pick some key of 1th n, so this is going to be a random key. N is a security parameter--the higher the value of n, the larger space of possible keys, the more secure the protocol will be. Let's assume we're going to use AES, and we'll make n 128. Then Alice will compute a message, which is the result of encrypting using the key the value of x. She'll send to Bob, the value of m. Bob receives the value of m--we'll keep track of it--and picks a guess. Bob will generate, however he wants, a guess valued at either 0 or 1. That's like calling the coin toss. Bob will send that guess back to Alice. Alice knows the values that she picked for x, k, and m as well as the value Bob picked for the guess. At this point, Alice sends the key to Bob, and Bob can compute the result of decrypting with the key--the message. This is the message that Alice sent to Bob in step 1. At this stage, Bob can check whether or not he won the coin toss, comparing the value of t with the value of his guess. If they're equal, that means Bob guessed correctly and Alice should charge the Robo-dog. If they're not equal, then it's up to Bob to charge the Robo-dog. Alice needs to know the winner too. She could do that. She could easily do that by checking if the guess is equal to x. She could also do the decryption because she has the key as well. So at the end of the protocol, both participants agree on the result of the coin toss. Now the question is, is this fair? So the question is, how hard is it for Alice to cheat using Robo-dog Protocol 2? And by cheat here, we mean gain an advantage-- be able to make the coin toss unfair in her favor. Here's the choices. There's only 1 correct answer. It's the easiest thing for Alice to do that would be enough to make the coin toss unfair for Bob. The answer is the 4th choice. If she can find 2 keys where either the encryption of 0 with key 0 is equal to the encryption of 1 with key 1 or the opposite, then at least half the time she has an advantage. So in this case, if she sends this as the message, if Bob guesses 0, she can win by revealing K1. If Bob guesses 1, she can win by revealing K0, so she's guaranteed to win. If she finds a key pair like this, well, then she can do the same thing. She can reveal this is the message and then based on Bob's guess, reveal wither K1 or K0. Either of these would also allow her to cheat if she could find a key that has this property-- that for whatever key she happened to pick, encrypt 0 to the same thing that key encrypted 1 to, well, that would allow her to cheat just as well. But this is a harder thing to do than finding 2 keys that have this property. We might not know enough yet to be able to be sure that this is harder than that, but we know it certainly is no more difficult and this could be easier, and we'll see that, in fact, it is easier. The second one would also be enough, but this is the much stronger property to find a key pair that has both of those properties. How hard this is depends on lots of things. The first thing we have to answer a little more is what encryption means here. For the protocol, we have a value x, which is just 1 bit, and I've been writing the message as a result of encrypting x with some key. But if we're using a block cipher, well, that assumes that the input is 128 bits. The key is also 128 bits, and AES does operate with different size keys, but the minimum key size is 128 bits. The ciphertext out is 128 bits. So in this case, we only have 1 bit. So the question is, what should we do about this? I'll give you 3 choices. We could use the Cipher Feedback Mode and set s to 1, that allows us to encrypt 1 bit at a time. We could use the ECB mode. That's just using AES passing in the input, and we could pad this input x by adding 127 random bits to the end of it, or we could use ECB mode, padding the input with 127 0 bits added after x. For the protocol that I've described, which one of these would be the best choice? The answer is, the only reasonable choice is the third one. The reason for that is, either of the first 2 make it very easy for Alice to cheat. For the first choice, we're only looking at the first bit out of the encryption, so if she tries a few different keys, she is likely to find ones that produce outputs that differ in the first bit. Very easy to find one with this property. For the second one, it's not easy to find exactly this property. Finding this equality would be just as hard as it is with the third property, but the problem is Bob does not know the 127 bits that are added. So Bob is only looking at the first character here and finding 2 keys that have this property would be very easy. So in this case, the only 1 that makes sense is this one. When we're thinking about encrypting files and other things, this isn't quite the right answer for padding in most cases. This solution only works when Bob knows the actual size of the input and agrees with Alice in advance to know that all padded bits should be 0's. When you don't know the size, this doesn't quite work because if the message ends with 0 bits, and we start the padding with 0 bits, we don't know where the actual end was. So the usual solution to this, and there are several possible solutions, is to start the padding with a 1 bit and then all 0's to the end of the message. That means if the message actually ended here, if this was the end of the block, if the message was an even number of blocks, we'd need an extra block of just padding to indicate that we got to the end. In this case, since Alice and Bob both know that there's only 1 bit for the coin toss, as long as they agree in advance that the padding will be all 0 bits, that would be okay. This approach works for the coin tossing problem, but there are lots of other situations where we need to commit to values, and we need to commit to values much longer than just 1 bit. For that, we need something a little more versatile, but with the same idea. What we need for this is what's called a cryptographic hash function. It's a function that has this property that it takes some large value as an input and outputs a small value. This is similar to a regular hash function like we used in a hash table, has the property that maps values from a large input domain. And by large here, we usually mean infinite, such as all possible strings of any length to a small fixed output size. It should also have the property that it's well distributed. That for any given input, the probability that that input maps to a particular value is close to 1 over the size of the table, and we'll use N to represent the output size. So we'll say that outputs are in the range from 0 of 10-1. And so if it has this well-distributive property, this works great for a hash table. This is not enough for cryptographic committments. If we just have these 2 properties, well, it might be easy to find other x values that map to the same thing. We saw that one of the things that we need for a cryptographic hash function is to make it hard for Alice to be able to do that, otherwise, she could find 2 inputs and cheat in the coin tossing game. This first property of mapping a large input domain to a small fixed output, we'll call compression. What we need for a cryptographic hash function is 3 additional properties. The properties are pre-image resistance, which means given the output of the hash function, it's hard to find the input that produced it. This is a form of "one-way ness". The other 2 properties we need concern the collision resistance. The weakest form of this is known as weak collision resistance. What weak collision resistance means is that if we're given some hash value, it's hard to find any input that hashes to the same result. These are sort of similar. This one says it's hard to find the actual value of x that was used. This one says it's hard to find any x prime value that hashes to that value. If we have weak collision resistance that would imply we have a form of pre-image resistance as well. The final property is strong collision resistance, which is exactly the one we saw that we needed for the coin tossing game. Strong collision resistance requires that it's hard to find any pair, x and y, such that the hash of x is equal to the hash of y. So have we seen anything that can do this? And the question is, which of these is almost a good cryptographic hash function? I'm using almost here in somewhat of a fuzzy way, but I want you to select the one that seems closest to being a good cryptographic hash function. None of them is a perfect cryptographic hash function, and we'll discuss that more after the quiz. So here are the choices. Use cipher-block chaining mode to encrypt x, and then take the last output block as the value of the hash. The second choice is to use counter mode to encrypt x, and then take the last output block. The third choice is to use electronic code block mode to encrypt x, and then take all of the output blocks and XOR them all together and use the result as the hash value. The fourth choice is to use counter mode to encrypt x, XOR together all that, put blocks, and that's the hash value. Which one of these makes the most sense to provide the cryptographic hash function properties that we need? The answer is the first one. The property that we need is to provide this collision resistance property. All of these provide the compression needed. We're taking a large input x that could be any size, turning it into the size of 1 block. The other 3 don't provide the collision resistance we need. So with counter mode, the value of the last output block is the encryption of the last block in the message XORed with the counter value and the nonce. That doesn't depend on any other blocks in the message. It only depends on the last block. It depends on the length--the number of blocks before that. But if we want to find that pair of values, x and y that hash to the same value, well, in this case that's easy. We can change any of the previous blocks. For the other 2, it's a little less clear to see that. The ouptut does depend on all of the input because we're XORing all those inputs into the ouput, but there are lots of things we could do that would still allow us to find collisions. One example with ECB mode--well, we can just flip the messages. If we swapped the first block of the message with the second block of the message, the XOR of all the output blocks will still be the same since with ECB mode these will encrypt to the same thing. With counter mode, this swap is not quite as simple. We'd have to adjust what's in the block to also adjust the change in the counter, but we could produce things that hash to the same value. So none of these would work. The first one is actually pretty close to what traditional hash functions used, and it's a construction known as the Merkle-Dangard Construction, which is quite similar to CBC mode encryption. Since it's a hash, we don't need a secret key. We can use the same key for each steps. We could select the key being 0. There are some subtleties to make this work as a hash function, and in fact, there's a lot of controversy today about how well hash functions work. The ones that were considered the standard, until recently, was a hash function known as SHA-1. This was a standard accepted by NIST. People have found ways to find collisions in SHA-1. There's an ongoing competition to select a new standard hash to find a replacement-- to find a hash function that is closer to achieving these properties, and it's expected that the winner will be announced in 2012. There are 5 finalists currently under consideration. We're not going to look any more in detail at how to construct a modern hash function. Instead we're going to assume that we have an ideal one. We're going to assume that we have what we know is a random oracle, which is an ideal cryptographic hash function. And this random oracle assumption means that we can have an ideal cryptographic hash function that match any input to H with a uniform distribution. And, an attacker trying to find collisions can do no better than a brute force search on the size of H. So, that's what we will assume we have. Before we go forward with that assumption, I want to ask a quiz--whether random oracles actually exist. The answers are: Yes, they do, and we know of one; maybe they exist but we don't know of one yet; or, no, it's impossible to actually build a random log. The answer is no, that it is actually impossible to construct one. This function must be deterministic, so it produces the same output for the same input. We want it to produce uniform distribution so that means it needs to add randomness to what comes in, but that's impossible. Since it's deterministic, the only randomness it could use is what's provided by x. So there's no way to amplify that to provide more randomness in the output. So, this is actually impossible. We're going to assume they exist anyway. And the assumption is that, even though it's impossible in theory to do this, in practice we can build hash functions that provide mappings that are computationally very difficult and indistinguishable from uniform by an adversary with a reasonable amount of computational resources. So now we'll assume that we do have such a function. We'll assume that we have a function H that acts like a random oracle that provides the properties of an ideal cryptographic hash function. Let's try our coin-tossing protocol again. So, here's our new protocol design. Alice will pick a number, 0 or 1, representing heads or tails. She'll compute using our ideal cryptographic hash function-- the hash of x--and she'll send m to Bob. Bob will make a guess, send that guess back to Alice. Then Alice will send her claimed value of x back to Bob. Bob can check if the hash of x equals m. If that's not the case, then Bob suspects that Alice has cheated. If x is equal to g, Bob wins. So, do we like this protocol? We'll assume that H is an ideal hash function, So which one of the parties, if any--or both--can cheat with this new protocol that we've defined? So the answer is that Bob can cheat. And the reason that Bob can cheat-- well, remember this hash function is not encryption. There are no secrets that go into it. It's only providing this commitment to the input, and so Bob can easily cheat in this case. He can compute by himself the hash of zero, the hash of one, check them, whether they equal m, and then instead of guessing, he can pick whichever one did. So, with this protocol Bob can always win. I'm going to leave it as an open discussion problem for you to figure out how to fix this protocol. And there are definitely lots of different ways to do this. I hope there will be some interesting discussion about it on the forums. The next thing we're going to look at is how big does the hash output need to be to provide the properties we need? So first let's just consider a weak collision resistance. So the question is assuming that our attacker has just enough computing power to perform 2 to the 62 hashes, how many bits should our ideal hash function--and we're assuming now that we have an ideal hash function that provides a perfect uniform distribution on the outputs--to provide weak collision resistance? And to formalize what we mean by providing that, let's assume that we're satisfied as long as the attacker's success probability of finding some pre-image that hashes to our value H is less than one-half. Of course, for most applications we want this probability to be much less than one-half. To answer, give the number of bits in the output of H. So the answer is we need 63 bits to ensure this probability is less than one-half. And the way to think about this is assuming the uniform distribution of our ideal hash function, every time the probability that one guess maps to H is 1 over the number of bits, or 2 to the negative b. We'll use b to represent the number of bits. So with our random oracle model, for any given guess the probability that it hashes to a particular value is 2 to the negative b. The probability a guess is bad is 1 minus that. And now over a series of guess, the probability that they're all bad is that raised to the number of guesses power. And we've said that k is equal to the number of guesses here, which we said was 2 to the 62, and we can solve 1 minus 2 to the negative 62 to the 2 to the 62. That's equal to approximately 0.63. There are lots of ways to solve this. You could try computing this in Python. These numbers get pretty big. The easiest way to solve this is to just plug this into Wolfram-Alpha. There are also, certainly, mathematical things you could do to get this equation in a simpler form, but since we just want the answer, it's easier to just try plugging in some numbers. If we increase the number of bits by 1, that means the probability for each guess being bad increases to 1 minus 2 to the negative 63, and that turns out to be 0.39. That means 63 is the fewest number of bits to provide the attacker with less than a 50% chance of finding a pre-image that maps to the same hash value in 2 to the 62 guesses. [Evans] The key point is that this was for weak collision resistance. For weak collision resistance, our assumption, an attacker needs to do about 2 to the b work where b is the number of hash bits to have a good chance of finding a collision. Strong collision resistance is harder than weak. To obtain strong collision resistance is actually much harder, and we'll see that we need about twice as many bits for that-- that the attacker's effort is more like 2 to the b over 2, so we'll need about twice as many output bits in our hash function to provide this. The reason for this is because of what's known as the birthday paradox. It's called a paradox. It's not really a paradox. It's a surprise to people who don't follow the mathematics on this, but there's nothing paradoxical about it. It's not like the Berry paradox that leads to a contradiction. The way this is usually framed is you have some group of k people-- let's say it's a kindergarten classroom-- and you want to know what the probability that 2 people have the same birthday. The reason this is called a paradox is the answer is actually much higher than most people expect. So let's compute this. We'll assume that there are no leap years and that birthdays are uniformly distributed. We assume the same property for our outputs of our ideal hash function. So we'll compute the complement probability that there are no duplicates, and then the probability that there is at least 1 duplicate is 1 minus this. And the way to think about that is we can go through the people in the class. We can assign some birthday to each one. So we'll assign b0 to the first one, b1 to the second one, keep assigning birthdays. There are lots of different ways we could do that. In order to assign birthdays with no duplicates, then there's a limited number of ways that we can pick any of 365 days for the first birthday. For the second one, if we want no duplicates, we can't use whatever day we pick for the first one. So it will be that times 364 times 363 and so forth, and that's the number of ways to assign birthdays with no duplication. We're trying to compute the probability, so we're going to divide that by the number of ways to assign with duplication, which is just 365 choices for each person. So in general, this first value is n factorial divided by n minus k factorial, and the bottom result is just n to the k. N is the number of possible days or the number of possible hash outputs. K is the number of trials. And so our probability that there are duplicates is just 1 minus this. This is what we need for strong collision resistance. It's higher than what we needed for weak collision resistance, which was 1 minus 1 minus 1 over n to the k. So to actually compute this, these numbers would get really big if we tried to actually compute the factorials. Instead we need to use approximations to do this. But to give you some idea what the results are like, if we have the duplicate birthday question where we have 365 days and 20 people, the probability that there's at least 1 duplicate exceeds 0.4. If we're thinking about hash collisions, if we only had a 64-bit hash and our attacker was much weaker than the one we hypothesized-- let's say they can only do 2 to the 32--that's already almost a 40% chance of success. This success goes up quite quickly. If the attacker can do 2 to the 34 hashes, then the success probability is very close to 1. So the conclusion is that given an ideal hash function with N possible outputs, an attacker needs about N guesses to find an input that hashes to a particular value but only needs about the square root of N guesses to find a pair that collide. This assumes the attacker can store all those hash values as they try the guesses and compare it to all the previous ones. This is the reason why hash functions need to have large outputs. SHA-1, which was a widely used hash function, used 160 bits in its output. This was actually broken. There is a way to find collisions using SHA-1 with only 2 to the 51 operations-- much fewer than the 2 to the 80 that one would expect from this square root estimate, and that's because of mathematical weaknesses in the way SHA-1 works. SHA-2, the output can be either 256 or 512 bits. As long as there aren't mathematical weaknesses in the cipher, if it was a really ideal hash function, this would be big enough to defeat any realistic attacker, but there are beginning to be suggestions that there may be ways to break this. No practical breaks have been found yet. And SHA-3, the winner is expected to be announced later this year. So for the rest of this class we're going to assume that we do have an ideal hash function and that it has enough outputs to provide strong collision resistance against a motivated attacker. [Evans] The last thing we'll do this unit is look at an application that combines many of the things we've seen so far. Our goal is to figure out how to use passwords to do authentication. We're going to assume that we have some file or database to keep track of users. And for each user we want to keep track of something that will allow us to authenticate that user. In UNIX, this file is stored in /etc/passwd. So our goal is that even if some malicious insider or someone who can compromise our system can get access to the database or the password file, they won't be able to impersonate users. They won't be able to log in by just being able to read this file. If they can modify this file, that's another story. But let's assume that they can't modify it, they can just read it. This is not a completely unrealistic model. This is actually the model that motivated the design of the UNIX password system. There are lots of people that need to read this password file but only very privileged applications that need to write to it. So the really bad idea number 1 is to just keep the passwords in clear text. There actually are web applications that do that. Any web application that actually does that is doing some things very, very wrong. First of all, it's wrong to send a password in email, especially one that was created by a user, but it also means that they're actually storing your password in a way that they can recover it. And that probably means that anyone else who has access to their database can also recover it. So we want to do something better. We want to provide this property that even if someone can get access to the database and read all the user and password information, they can't break into your account. So that was really bad idea number 0: keeping the passwords in clear text. Now we're going to move on to really bad idea number 1, which sounds a little bit more plausible but is not a good way to store passwords. So here is the plan. The server operator will generate some key. Let's assume the server can generate a good random key. And for each password we're going to store the encrypted value of that password, encrypting using the key using CFB mode. That's cipher feedback mode with message block size 8, so that will allow us to use this for any size string. 8 is the number of bits per character. That could be 7 or 8 depending on how we encode the characters. The question is, why is this a really bad idea? Here are all the possible choices. Select all the ones that apply. That it reveals the length of the password; that the encrypted text reveals when 2 users have the same password; that it requires use of E, an encryption function, which is slower than using a hash function; that if the key is compromised, that reveals all of the passwords. [Evans] The answer is all except for the third are true. The third is not true because encryption is generally faster than hashing. These 3 are really big problems with this scheme. Let's talk about the fourth one first. We need this key to decrypt. The program that's running on the server that needs to check passwords will need this key all the time, so chances are if the password file is compromised, the key is also compromised because it's available in memory, it's stored in this program, or it's readily accessible because we need it every time we check a password. The solution to this is that we don't actually need to invert the password to check it's correct. All we need to do is recompute from the entered password some function and check that with what's stored. So there's no reason to have a key here. The first reason is also true--that because we're encrypting the password, the output that's the stored password will reveal the length. The reason that's a bad idea, well, it reveals information about the password. It also helps an attacker out--tells them which passwords are short, and that helps the attacker know which ones to try and break. The easier way to fix this is to use a hash function as well. If we use a hash function, the size of output doesn't depend on the size of the input. So no matter how long the password is, we'll have the same number of output bits based on our hash function. The second one is a little more subtle. If we just use the hash function in a straightforward way, we won't save this problem. This problem is pretty important. If you look at typical passwords, many are the same. The most popular password, at least according to an analysis of leaked passwords, is 123456-- if that's your password, you should probably change it-- out of the 32 million passwords that were leaked. So an analysis of the 32 million passwords that were leaked by RockYou.com-- and they do not store passwords in an encrypted format, so all the actual passwords were easily revealed-- the most popular password was 123456. Over 290,000 people used that for their password. That's almost a 10th of a percent. So if this is your password, you should probably change it. Once you start looking at a set of a few thousand popular passwords, you're covering a large percentage of users. So this first approach fails on many accounts. [Evans] So now we'll look at a slightly better idea. This was actually what was done by early versions of UNIX. The idea was to store for each user the result of encrypting using the user's password as the key the value 0. So this means there's no key to keep secret on the server. Someone who compromises the password file knows that the value being encrypted is 0 but would need to test possible passwords to find the one that matches. To make this harder, you wouldn't just run encryption once; you would run it many times. If you encrypt twice, this doubles the work to check a password. It also doubles the attacker's work for each password to guess. Since the attacker has to guess many more passwords than just the one that was entered, this scales the attacker's work more than it scales the checking work. And the way this worked in UNIX, it was doing this 25 times. So the value that was stored for each user was the result of encrypting using the password as the key, using 0 as the initial value, but looping it around 25 times. There are some problems with the way this was done in early UNIX systems. The encryption function was DES, the Data Encryption Standard, which used 56-bit keys, and the key was generated from the password by just cutting it off. Take the first 56 bits. So with the 56-bit key, the first 56 bits of the user's password are used. If this was encoded as 7 bit ASCII, that was only 8 letters. It was actually a little bit less than that because DES placed some restrictions on the key structure. So if users selected their passwords using only the uppercase, lowercase, and digits, this corresponds to 62 possibilities. Most users would not pick uniformly from those 62 possible characters, but if they did, there are 62 to the 8 possible passwords. This is less than 2 to the 48. For someone with modern computing resources, doing a search on a space like this is very feasible. The reality is even worse than this-- that a motivated attacker can pre-compute a dictionary, pre-compute this value, for a set of common words w, store those pre-computed values, and then every new password file that's compromised check all the passwords against this list and have a good likelihood of finding some accounts that can be broken into. So how can we make dictionary attacks harder? One idea would be to train users to pick better passwords. Users are pretty hard to train. This usually means coerce instead--force users to pick a password that satisfies some properties. It could be a minimum length, a mix of upper and lowercase letters and special characters. Many websites do this. It usually just leads to annoyance and doesn't necessarily lead to better passwords. You can certainly come up with bad passwords that satisfy any set of rules that someone forces you to use. So another solution would be to protect the encrypted passwords to make sure that an attacker or an insider can't get access to them. This would certainly help, would prevent the dictionary attack, because they wouldn't have access to the encrypted passwords. They could still try a small number through the login interface. If the passwords are bad enough, the dictionary attacker will still succeed but only breaking a small number of accounts. The third approach is to add salt. This is the one I'll talk about next. [Evans] So with a salted password scheme what we'll store in the password file, we'll have the users, we'll have an extra column that we'll call the salt, and then we'll store the encrypted password. What the salt is is random bits. They don't need to be kept secret. For the UNIX scheme there were 12 random bits. And they're different for each user. That's why they're stored in the table. So for user Alice let's say we have the salt 011010001111, which is an apparently random sequence. What we'll store as the encrypted password is the result of hashing the salt concatenated with Alice's password. And there are different ways of doing this. Some hash functions can be modified to behave differently based on another parameter. But if it's a simple hash function that just takes an input, we can make the input the salt concatenated with the password. And then for Bob we'll do the same thing, but we'll have a different sequence of random bits. And so here what we're storing is the result of hashing Bob's salt, which is this selection of random bits, concatenated with Bob's password. This means as long as the salts are different, even if the passwords are the same, the encrypted passwords will be different. And this is very similar to what standard UNIX systems did where H is actually using DES encryption 25 times using the salt to modify how the encryption happens and the password as the key and the initial input as 0. So to see if you understand the impact of salting, I have 2 quizzes. The question is, how much harder does adding salt make it for an attacker who compromises a password file who just wants to learn Alice's password? The possible answers are not much harder at all; about twice as hard as it would be without the salt; and about 2 to the 12, which is 4096, times harder than it would be without the salt. [Evans] The answer is not much harder at all. All the attacker has to do-- The attacker has compromised the password file. That contains the line giving the salt for Alice and her encrypted password. To break the password, the attacker needs to try possible passwords concatenated with that salt and find one that matches the hash value. This seems like the salting doesn't help us at all, but let's change the question a little bit. [Evans] So now the question is, how much harder does the addition of salt make it for an attacker who wants to carry out offline dictionary attacks? That means the attacker can pre-compute a dictionary of encrypted passwords and then for each collected password file wants to find any passwords that match the values found in that dictionary. [Evans] Now the answer is the third one-- that it makes it about 4000 times harder. So to compute a dictionary that would be effective against all the different salts, the attacker really needs to pre-compute that dictionary for all the different salt values to be able to look for passwords that match. So salting adds a lot of value for very little cost. We just need some extra bits that don't need to be kept secret that are stored in our password file. [Evans] This helps a little bit but doesn't avoid many of the problems with passwords. One big problem with passwords is that they're reused. There are lots of ways to reuse passwords. Some of this is using the same password across many sites. That's not what I'm talking about here. I'm talking about the point that every time Alice logs in she enters the same exact password. The password is the same until she does something to change it. So she's typing the same password many times. This means if there is something logging what she types, it will learn her password if she types it in an Internet cafe or somewhere where it's visible to someone looking over her shoulder-- a shoulder surfer. It's also the case that her device that she enters her password in would start to have smudges where she types her password. This is a particular problem for short PINs on smartphones that are entered so many times that finger smudges start to give an idea of what the password might be. All of these problems stem from using the same password every time she logs in. So we're going to talk about one way to avoid that, which is to use a hash chain. Hash chains have lots of interesting applications. In this case, we're going to use a hash chain to make it so we still have the nice property that we had with the password file where the server stores no secrets. It doesn't matter if all the data on the server is compromised. That still wouldn't give someone the ability to log in as Alice. [Evans] So here's the idea of how to do this. We're going to start by selecting some secret, and then we're going to compute the hash of that secret, and then we're going to do that again. We're going to compute the hash that we got for the first time as the input, and we're going to keep computing hashes here. This could be done anywhere. I've written it on the server side. The important point at the end of this-- so what Alice gets is the hash of s, the hash of the hash of s, the hash of the hash of the hash of s and so forth. Of course, if she just has s, she could compute all of these herself. The server only stores 1 thing. The only thing the server stores is the last value in this hash chain. So let's suppose we did this 100 times. This is the value the server would store, and that's the only value stored by the server. So now the protocol to log in will be Alice will send the 99th value in this hash chain. So what the server will do is compute the hash of p, check if it's equal to the stored value x. If it is, it will allow Alice to log in and will also update the value of x. The new value of x will be the value Alice sent as p. So that's how the first login works. Now the question is, the next time Alice logs in, what does she need to send? Here's the choices. Different number of times of doing the hash starting from the secret s. [Evans] The answer is Alice needs to send the value of hashing s 98 times. The hash chain is going backwards. We can only verify hashes in 1 direction. The hash is hard to compute in 1 direction. That's the valuable property the hash function gives us. And so we have to go backwards if we want to use it for authentication. Here we're using it to authenticate Alice. If someone just knows x, if someone intercepts p, knows the previous password value, they could compute any of these other values. Those are easy to compute once you have p. This was p, this one is just computing the hash of p, and this one is computing the hash of the hash of p. The only one that would be hard to compute is this one. The server can check that that's correct using the same process. At this point, the value of x is hash 99 of s. So when the value of p that's sent is hash to the 98th power of s, doing hash 98 times, then this equation will be true only if the value sent was correct. So what I've described is what's known as the S/Key password system. The way S/Key would work, the server would generate the hash chain. Let's say there are 100 entries. Alice would print these out in a list, and they would be turned into strings that are easier to type than pure bit sequences. The server would store the last entry in that hash chain and nothing else. And so what's stored in the server could not be used to log in. The list that Alice has could be used, and I should correct this that if Alice starts with H 100 as the first thing in her list, what the server should actually store would be H 101. This has a pretty big downside--that it requires Alice to carry around with her a list of passwords, and instead of remembering something that becomes easier and easier to type, she'd have to look at that list, type the password correctly, cross that one off, and use the next one next time. And at some point she's going to run out. She's going to need to get on a secure connection again to generate a new hash chain to be able to keep doing this. [Evans] There's an interesting story about using hash chains for hotel doors. I'm not positive if this story is true, but I've heard it from fairly reputable sources and it seems too good not to be true. So if you think about a hotel, you've got rooms-- let's say this is Room 387--and you've got a check-in desk that may not be connected by any wire to the room. What you'd like to happen is when a new visitor checks in-- let's say it's Alice--the desk at the check-in can give her a new key, and that key will allow her to open the door. There's no connection between the check-in desk and the door, and we want to know that Alice will be able to enter the room. Whoever stayed in the room previously--let's say that was Bob-- shouldn't be able to enter the room anymore. At the point after Alice checks in and enters the room, Bob should no longer be able to enter it. We'd like to have all this happen without needing any coordination between the desk and the door. One way to do that would be to use a hash chain. What the door will do is when it reads a card-- and I'll write this in sort of Python-ish code-- the door will have some stored value x. What the door will do is check if the value of hashing that key is equal to x, the stored value. If it is, the door will unlock and everything is good. We open the door with the correct key. There's another situation that we need to test for, and that's if the hash of the hash of the key is equal to x. If the hash of the hash of the key--if we're thinking about a hash chain, that would be the next key in the hash chain. If that's the case, we also want to unlock the door, but we want to update the value that's stored as x. We'll update that value to be the hash of the current key, hash of k. So that's our read protocol. What this means is that if Bob is using the room first, his key is k1, and his key will have the value some number of hashes on some secret, and the value of x would be m + 1 of those hashes. And Bob keeps opening the door, going through this path through the code. There wouldn't really be a Python interpreter in the door. This could be done by something much simpler than having a full Python interpreter. Then when it comes time for Alice to open the door, Alice's key would have the value 1 fewer hash of s. That means that the value of the hash of the hash of Alice's key is equal to this value, hashing m + 1 times starting from s, and that means Alice would open the door. It would change the value of x, so now it no longer works for Bob. But the next person who enters, it will work, and then it will stop working for Alice. So what the check-in desk needs to do, it needs to keep track of some secret, and it needs to keep track of the number of the guest. The initial value stored as x needs to be set, but once those are done, there's no coordination needed. Every time a new guest arrives, they'll be given a key, which is the result of hashing n times starting from s, and the value of n would be decreased by 1. So this is our design for allowing secure doors with a sequence of users. So to see that you understand it and understand hash chains well, we have a quiz. What would cause problems with this design? If a criminal could extract the stored value from the lock; if a guest checks in to the hotel but never actually enters her room; or if a guest enters the room many times. [Evans] The answer is the second choice. If a guest checks in but never enters the room, here's what goes wrong. If Alice never enters the room, then the next guest--who's very tall-- who checks in, she'll get a key that is the next value here, which is going to be hashing m - 2 times, but the value of the stored x is still hash of m + 1, and that means the value of hashing of the hashing this new key will only be the hash of m, which won't match the value of x. So the new guest won't be able to open the door. The hotel operator would need to do something to reset the value of n or resynchronize the door and the check-in. And apparently, the hotels that adopted this way of doing keying never imagined the possibility that a guest would check in but never actually enter their room with their key. Apparently, this happens more often than you might think, though. [Evans] So rather than speculate on why that is, this brings us to the end of Unit 2. Our main focus for Unit 2 was how to use symmetric encryption to solve problems. For the first part we focused on this key, the need to generate a random key, which is a very hard problem, and we saw that we could use physical randomness if we had enough available to do this, but there's no way mechanically to produce a perfectly random key. But we could use a pseudo-random number generator built using encryption to take a small amount of random data and amplify that to produce more pseudo-random data. We saw how to use a symmetric cipher to take a small amount of random data and produce a sequence of values that appear to be pseudo-random. We also looked at the problem of how to encrypt a large file or a large message, and that brought us to look at modes of operation for using symmetric ciphers. We talked about the cipher block chaining mode, the counter mode, and the cipher feedback mode, all of which have different advantages and disadvantages. We also looked at how to do fair coin tosses remotely, and that led us to the need for cryptographic hash functions. We saw how to use those to check user passwords in a way that doesn't require us to keep any secrets on the server, and we also saw how to use hash chains to make it so those passwords never needed to be reused. The big problem we haven't addressed yet is how to establish a shared key. If we want to use a symmetric cipher to allow 2 parties to communicate, they have to agree upon the shared key beforehand. If you think about most ways we want to communicate today, that's very difficult. If you visit a secure website, you don't have a shared secret with them to begin with, but you want to start communicating securely with them despite not having a shared secret. And for most uses of cryptography today this is a really big problem. We can't assume that we have a shared secret with every website that we want to use. We need some way to establish a shared secret without having one to begin with. That's the main topic of Unit 3. I hope to see you there. Welcome to Unit 3. In Units 1 and 2, we learned about symmetric ciphers and how to use them. Symmetric ciphers assume that both parties have the same key. The important property that makes them symmetric is that the same key is used for encrypting and decrypting. In some scenarios, this is okay. When we talked about using symmetric ciphers to encrypt a file, we could assume the same party is encrypting and decrypting the file, so they can create and keep the key locally. But when we talk about using symmetric ciphers for most applications that involve more than 1 party, this is a big problem. If Alice and Bob want to talk together, they have to first agree on the secret key. There has to be a way for Alice and Bob to communicate that key without exposing it. They can't send it over the insecure channel. If they could send a secret key over an insecure channel, well, they wouldn't need encryption at all. In the early days, this was done by having a secret code book that was physically distributed to the end points that were communicating. With Colossus, this was the code book that had the keys for each day, and that was physically distributed to the capitols where the Lorenz machines were. With Enigma, this was physically distributed to all the ships, and captains were under very strict orders to destroy those code books any time their ship was in danger of being captured. This was the most important thing to destroy on the ship to make sure the enemy didn't capture the code book. For most applications, especially any civil application in the day of internet, distributing a physical code book or having 2 parties meet in a secure place to exchange a key is not very practical. What we want is some way to establish a secure key or some way to enable 2 parties to communicate that doesn't require that, so that's the main topic of this unit. We're going to look at different ways that we could enable this kind of communication without starting from advance where Alice and Bob have a shared key. The first approach I'll talk about--I'll call it Solution #0 because it's really not even approaching a solution-- is to have pairwise shared keys. And the idea here is for everyone who ever might want to communicate to get together and form keys between them, so let's say this is Alice, Bob, Colleen, Diana, someone whose name starts with E, and someone whose name starts with F. And Alice would set up a pairwise shared key, secretly communicating with each of the other people. Bob has a shared key with Alice. Bob would find a shared key with all of the other people that he might want to communicate with, and Colleen needs to find a shared key with the remaining people, and so forth. And you would have a key between every pair of people, and so whoever wants to communicate, well, if it's between Colleen and Edgar, they would have a shared key that they've previously established to communicate. This might work for a small number of people. If we have a large number of people, this gets to be ridiculously expensive, so we'll have a short warmup quiz to see how expensive. The question is how many keys would we need if there was a group of 100 people, and any of pair of them should be able to communicate secretly with each other? The answer is 4851, so suppose we have 100 people, and I'm not going to try and draw 100 people, but here we have N people. In order to be able to communicate with everyone else, the first person will need a shared key with every other person, so that's going to be N-1 key shared between Alice and the N-1 other people. The second person already has a shared key with Alice but needs a shared key with everyone else. That is N-2 additional keys. The third person that has keys shared with Alice and Bob needs a key with everyone else, and this continues until the next to last person, who needs a key shared with the last person. That means we need the summation from N-1 down to 1, or we can think of that going forward 1+2+...up to N-1, and that simplifies to N-1 times N-2 divided by 2. For 100 people, this would be 99 times 98 divided by 2, which is 4851, which might not sound too bad, but 100 people is a pretty small number. If we think of doing this for the billion or so people on the internet, then we would need around 10 to the 18 keys. This isn't really a solution, both because of the huge number of keys that are needed and the need to establish all of these pairwise keys. We haven't really solved our problem. I'm going to discuss one other non-solution. We'll call this one #0B, which is to use a trusted third party. Here is the idea, that we have some very, very trustworthy place, and we'll make it green. People trust things that are green. We have this very trusty place called TrustyKeys, and TrustyKeys has a shared secret with each individual in the network. Now if Alice and Bob want to communicate, the protocol is Alice sends a message to TrustyKeys that says she wants to communicate with Bob. That's step 1 of the protocol. What TrustyKeys will do is generate some new key, some new secret key, and we'll call that KAB because it's for Alice to communicate with Bob. In step 3, TrustyKeys will send the new key to both Alice and Bob, and it will send it encrypted using the key that's already shared between those 2 parties. What it sends to Bob is the encryption using KB, the shared key between Bob and TrustyKeys of something that says Alice's name concatenated with the actual key. And what gets sent to Alice is encrypted with KA, the shared key between Alice and TrustyKeys and the name Bob along with KAB. At this point, both Alice and Bob know KAB and can communicate securely using KAB to encrypt messages between them. The quiz, to see if you understand third party key distribution, what could go wrong with this protocol? The choices are TrustyKeys can read all the messages that happen between A and B over their encrypted channel. TrustyKeys can impersonate any customer that shares the key with TrustyKeys, and some evil party C could tamper with messages in step 3 to steal the key AB and set up a different key between Alice and Bob and then steal their traffic. The answer is definitely the first two, maybe the third. In the first case, TrustyKeys generated Key AB. That's the key used to protect these messages, so any message over this channel that TrustyKeys can intercept it can read. TrustyKeys can impersonate any customer because it has a key shared with them. It can generate a fake channel, make it seem like Alice is talking to Colleen but pretend to be Colleen in that communication. And finally, if there's a malicious party C, well, maybe they could tamper with messages along these channels. I didn't provide enough details on what we're using for encryption here and what mode of operation is used and other details to know whether or not that's possible. Let's assume we could solve this problem. These other two are really big problems, and there's no way to solve these if we're using a third party to generate our keys. We also haven't solved the key distribution problem because we still needed to set up all these shared secret keys between TrustyKeys and every other party. We haven't really solved our problem. This would only work if everyone completely trusted TrustyKeys, which seems unlikely, even with the green box, and if there was a secure way for everyone to establish a shared secret key with TrustyKeys. The next thing I'm going to talk about is an idea that Ralph Merkle came up with in 1974. It's not a practical idea at all, but it's a really clever idea, and I think it's a fun thing to talk about before we get into the more realistic solutions that are used today. The idea behind Merkle's puzzles is that you have the 2 parties that want to share a key, and we'll call them Alice and Bob, as usual. First they have to agree on some parameters. They agree on an encryption function. They agree on security parameters s, n, N. The basic idea is Alice is going to create many puzzles, and then Bob will randomly pick one of the puzzles to solve, and part of solving that puzzle will give Bob the number of the puzzle and a secret. And what he'll send back to Alice is the number of the puzzle, and Alice will know the corresponding secret, and they'll use that as the key. Let me go through this in a little more detail, but that's the high level idea, and the reason that this gives you a way to establish a key is because Bob is randomly picking one puzzle to solve, whereas an attacker would have to solve many puzzles before they could get the one that Bob actually solved. Here's how the steps work in a little more detail. In order to create puzzles--so Alice creates a list of N secrets. Each one of these is a random number s bytes long. Then she creates a set of messages, also N messages, and each message will be encrypted using the encryption function that they agreed on using a key that's the corresponding secret concatenated with enough zeros to make the appropriate key length. If this was a 128-byte AES key, the secret size might be, say, 40 bytes, and then we would add zeros to fill out the rest of the key. And then what we'll encrypt using that key is a message that gives the identity of the puzzle, which will be that index, and the message will give the identity of the puzzle, and we'll include a string before it so it's clear that that's the right message. If we just included a number, if there were enough numbers we might accidentally decrypt one to that. That's what we encode for each message here. After this, Alice shuffles the messages, and then she sends all N messages to Bob. Bob randomly picks one of those, then does a brute-force key search where Bob is trying to decrypt using a guess key concatenated with N - s zeros, the message that Bob randomly selected from those puzzles. Eventually he's going to find one of these decrypts to puzzle followed by a number. At this point, Bob knows the guess, so he knows the secret that was used for that, and he knows the puzzle number. If we wanted a larger key, which we probably do, what we should include in the puzzle instead of just the i we should include a key, and this means the key can be of any length. This would be some new random key associated with that puzzle. Alice would keep track of those keys, and so for each puzzle, when Bob decrypts it, he acquires both the puzzle number and the key number, sends the number of the puzzle back to Alice. Alice can look up in her keys and figure out which key was the one in the puzzle Bob decrypted. Assuming encryption and the random number generation work perfectly, the question is how much more work does an eavesdropper have to do than Bob has to do? And an eavesdropper hears all the messages between Alice and Bob, wants to determine the key that they'll use to communicate. Here are the choices. It's impossible for the attacker to find the key. The attacker would need to use N times as much work as Bob to determine the key. The attacker would expect to need N/2 times as much work as Bob, or the attacker would be expected to need twice as much work as Bob. And the value of N is the number of puzzles that Alice creates and sends to Bob. The answer is that the attacker would expect to need N/2 times as much. The reason for this is because of this step. Bob randomly picks one of the puzzles and solves just that puzzle. Once Bob has solved it, he sends back the number of the puzzle that he solved, which was part of the puzzle to Alice, and that establishes the shared key. An attacker can hear that number but doesn't know which of the encrypted messages corresponds to that. The attacker would need to try and break all of the encrypted messages and would be expected to find the one that matches the one that Bob picked after trying about half of them. This is a neat idea. That N/2 advantage is really not enough. For that to be significant, we would have to have a really large number of puzzles. That involves a lot of work for Alice. Alice still needs to create all those puzzles. And it requires a lot of bandwidth. We'd have to send all those puzzles across the network, so a very impractical idea. The first real solution to this problem, and one that is frequently still used today, is called Diffie-Hellman Key Exchange. It was first published by Whitfield Diffie and Martin Hellman in a very famous 1976 paper. It was actually discovered a few years earlier than that by Malcolm Williamson, who was working at the British government communication headquarters. This is the successor to Bletchley Park. It was kept classified, so Williamson didn't get credit for this, and it wasn't known that it had been discovered earlier there until quite recently. And this idea came from a paper that Diffie and Hellman published in 1976. I mentioned Shannon's paper as one of the 3 most important papers in cryptography. This is one of the other 3. The third one--and I'm not trying to put these 3 in order, just chronologically-- is the RSA paper that we'll talk about a little later. But this paper really introduced the idea of public-key cryptography. It didn't provide a solution. It provided a solution to part of the problem. They started with this introduction about we're on the brink of a revolution in cryptography and posed the question of can we build cryptographic tools that are the equivalent to a written signature? There are lots of other kinds of applications they talked about in the paper as well as the idea of basing crypto systems on known hard problems or believed to be hard problems. The one solution that is in the paper that we're going to talk about today is this Diffie-Hellman key exchange protocol, and it's actually quite a simple protocol. It's still very widely used, and it allows 2 parties without any prior agreement to be able to establish a shared secret key. Before describing it mathematically, I'm going to describe it as an analogy to paint mixing, and this analogy comes from Simon Singh from "The Code Book," which is a really fun book about the history of cryptography. We have our usual suspects, Alice and Bob, that want to establish a shared key, and they're going to do this by first picking a public color. Let's say they both like yellow. And they'll start with cans of identical yellow paint. Both Alice and Bob start with a can of yellow paint. Then each party will pick their own secret color. Let's say Alice picks a greenish color, and Bob picks a purplish color. Then what they do is mix their secret colors with the public color. They're going to mix those two, and the green and yellow will make some ugly green color, and Bob will do the same thing, mixing the yellow and purple to get some new ugly color. At this stage, they change colors. Alice gives Bob the ugly green color that she got by mixing her secret color with yellow. Bob gives Alice the ugly pinkish color he got by mixing yellow with his secret color. Once they've received the other party's colors, they mix their secret color with the other party's mixed color. At this stage, both parties should have the same color. Alice mixed her green with the pink that came from yellow and Bob's secret color, and that will give some new color probably uglier than the grey that I'm drawing, but Bob will get the same thing because Bob is mixing the yellow, Alice's secret color, and Bob's secret color and getting the same color. This gets them the same color. They've mixed the same 3 colors together. It would be secure if someone who knew yellow and could intercept these 2 transmissions couldn't guess the secret colors. This assumes that paint unmixing is impossible. Probably not a very good assumption, and certainly not a practical way to actually do this. What we want is to do the same thing with mathematics. This is also the main operation we used in symmetric ciphers. XOR = + mod 2. To do key agreement, though, plus is not going to be the main operation. The main operation we'll use is multiplication. To do this with math, the main operation we need is multiplication. There's a few key properties of multiplication that we'll take advantage of. The first is that it's commutative. That means that a times b is always equal to b times a. This is true on integers. It's also true using any modulus. This property also holds for powers of powers. That means for any integers, a to the b based to the c power, well, that's equivalent to a to the b times a to the b times a to the b c times, which is equivalent to a to the b times c. I'm just adding powers when we multiply here, which is also equivalent to a to the c times b. We know that from the commutative property of multiplication, which is also equivalent to a to the c to the b. Using the same property here, a to the c, a to the c b times. These things are all equivalent, and they're also equivalent with modular multiplication as well. That's the main property the Diffie-Hellman Key Exchange takes advantage of, and I'm going to show you the protocol next, and then we'll talk about why it's secure and some of the details in actually implementing it. Here's the idea of the protocol. First they agree on 2 shared values. The first is q, some large prime number, and the second is g, and g is a primitive root of q. What it means to be a primitive root is that for all numbers in the group Zq, that is the numbers 1, 2, through q-1, we can generate all those numbers by raising g to some integer power. If q is prime, it must have a primitive root, and there are ways to find these primitive roots. We could think of a brute force way of trying numbers until we find one. That would be very expensive for a large prime number, but there are more efficient ways to find them, which we won't talk about. That's what they start with, those 2 things. And then here's how the protocol works. Alice will select a large random number, and Bob will also select his own large random number. This is like selecting the secret paint colors. Then Alice will compute a value we'll call yA, and she'll compute that by raising g to the xA power and doing this modulo q. Bob will do the same thing but with his secret power xB. He'll raise g to the xB power modulo q. They'll exchange these values. Alice sends yA to Bob. Bob sends yB to Alice. And then Alice will compute a key that will be shared between Alice and Bob, and she'll compute that by raising the yB value that she received from Bob to the xA power and do that all modulo q. If this is a good key distribution protocol, then there should be a way for Bob to compute the same key. I'll see if you can figure that out yourself by making that a quiz. These are the possible choices. Which one of these should Bob compute to obtain the same key as Alice did here? The answer is the first one. Bob should compute yA to the xB power modulo q. The second one would compute the same thing. This is in fact exactly what Alice computed, but Bob can't do this because he doesn't know xA. The third one wouldn't compute the same key, so the correctness property is that Alice and Bob obtain the same key, and we can show this by just plugging in the values. The key Alice computed was yB to the xA. The value of yB is g to the xB, so that's equivalent to g to the xB xA mod q. The key that Bob would compute--and we'll write that as key BA since we haven't yet shown that they're equivalent using this equation. Well, yA is g to the xA, so this is g to the xA xB mod q. And we already showed that powers of powers are commutative, so these two are equivalent. The next important property is that it's actually secure, and we're going to focus first on secure against a passive eavesdropper, so this is an adversary that can listen in on all the messages but can't modify them. That means the adversary will be able to get the public values q and g. We'll also be able to hear all the messages transmitted, so we'll be able to learn yA and yB, and our goal is to be confident that the adversary would not be able to learn k AB. The question is which of these would be most convincing in proving that the Diffie-Hellman protocol is secure? The first is that showing that the number of possible xA values to guess is too high. The second is showing that an attacker who could solve a known hard problem would be able to compute the key. And the third is showing that an attacker who can compute the key would be able to solve some known hard problem. The correct answer is the third choice. What we'd really like to be able to show is that we can reduce some known hard problem to the Diffie-Hellman problem, and the important thing is that this would show that anyone who can solve the Diffie-Hellman problem would also be able to solve some problem that we already know is hard. Showing that the problem is hard just shows that if we have a large enough instance of it, it can't be solved with a certain amount of computational resources, and we hope that by picking a large enough instance we can make the work for the attacker far beyond what any attacker could afford. That does depend on this property, so it depends on the number of values being high. That itself is not a useful proof of security. What we need to know is that there's no easier way to solve the problem than trying all the possible values. The hard problem that is closely related to the Diffie-Hellman security property is the discrete log problem. Discrete logs are like continuous logs but over a discrete group. So continuous log if we have a to the x equals b, and we know a and b, we can solve for x. That's the log base a of b, and they're well know efficient ways to compute these logarithms. One of the earliest use of computers was to compute these tables of logarithms. With discrete numbers, this gets much more interesting. So now we have a to the x equals b, modulo sum value n, and our goal is to solve for x, which is the discrete log base a of b, and this turns out to be, as far as everyone can tell, a very hard problem when n is a large prime number. It's not clear that discrete log always exists, and for certain choices of a, b, and n, it would not exists, but if we choose n as a large prime number and a as a generator, well then by definition, it must exist. What it means for our number to be a generator is that if we raise g to each power, what we get is the permutation of the numbers in the group Zn. So as a little demonstration, certainly not a proof, here's a code that produces the permutation for given some generator and some modulus, raises the generator to every power between 1 and the modulus minus 1. So we can try that with a fairly small prime number so you can see the results. We'll use 277 as our prime number and 5 as a generator for 277. One could check that in a root force way just to show that it all produces all the numbers, and we'll see that in the output for generator permutation. These are the results and you can see the first 1 is 5, that's 5 to the 1; 25 is 5 to the 2; 125 is 5 to the 3. The next one is 71 because 5 to the 4 mod to 77 is 71, and if we look at all the numbers here, it would be a permutation on the numbers from 1 to 276. Other than the early ones, it would be fairly hard to predict where a number is in this sequence. You could certainly compute the whole sequence to find it. The question the discrete log is asking is given a number, can you figure out where it would be in this sequence or can you figure out the power that you need to raise the generator to find it, and the claim is that that's hard to do. Showing this sequence really is not enough to convince you that that's hard to do, and there's no proof that it's hard. The reason people believe it's hard is that many smart people have tried to find good ways of doing this, and none of the solutions rendered polynomial time that the fastest known solutions are exponential. That means essentially that the only way to solve this is to try all possible powers until you find the one that works. You can do a little better than that by trying powers in a clever way, and you can exclude some of the powers more quickly, but you can't do any better than doing this exponential search, which is exponential in the size of n so this is something we have to be careful about when we talk about hard problems. When we say it's exponential, well it's not exponential in the value of n. It's linear in the value of n. We just need to try n operations, but the magnitude of n grows as 2 to the number of bits needed to break down n. So as long as that's the best solution to discrete log, then for very large n, it is intractable no matter how many computer resources you have, you can't do this exponential search. You can't find the value of x that's the discrete log of b, base a, mod n. So as long as no one can find a fast way to solve the discrete log problem, as long as n is large and is an arbitrary instance of this problem, we think that it should be hard to compute x given a and b and the modulus. So for this quiz, we will assume that we have and advisory that's passive so all it can do is ease drop on the messages, but they also have access to a powerful computer resource, they have a procedure dlog that is a fast procedure for computing discrete logs that works on any inputs, and they have modular exponentiation, a fast procedure that outputs base to the power mod modules. And now the question is can they break a Diffie-Hellman key? So we're assuming that they're passive attackers, so they've eased dropped on all the messages between Alice and Bob, so they have all these values that were sent over the secure channel, and the possible answers are no that it's impossible with no more resources or information, or yes there is a way to do it, and here's the way that she would compute that. The answer is the second choice. If it were possible to compute discrete logs quickly, and our adversary had a way to do that, well then, they could compute the discrete log of the intercepted value y-A, the g, and the q values, which were also intercepted. Those ones were public, and the result of this would be the x-A value that was Alice's secret, and then the adversary can compute the key the same way that Alice would. This was the only secret value, and if you had the discrete log function, you could compute that secret value. The second answer doesn't compute the right thing. It's computing the straight log of y-B, which would be the y-A value. That's not the value that is necessary to compute the key. The third value doesn't use discrete log. If we could actually compute the key this way, well then the protocol would be completely unsecure because it turns out that modular exponentiation is a function that we can compute efficiently, and it's necessary that we can compute modular exponentiation efficiently because otherwise, the legitimate participates in the protocol wouldn't be able to determine their keys. So we will look at that soon. Before doing that, I want to emphasize that this is not a proof that breaking discrete log is hard. So if we were able to assume the discrete log is hard, then we could prove that Diffie-Hellman is secure if we could show that breaking Diffie-Hellman implies that one can also solve the discrete log problem. No one has been able to prove that. Instead the security of Diffie-Hellman relies on a stronger assumption-- what's known as the Decisional Diffie-Hellman Assumption. This is a little bit circular that we're basing the security argument for Diffie-Hellman on an assumption that was invented because this is what Diffie-Hellman relies on. That seems a bit circular, and indeed it is. But it is a useful way of crystallizing what we really rely on to know that the Diffie-Hellman protocol is secure. And what we're relying on is this property that G to the XY mod Q, and this is how we're computing the key when the XY values-- one is a secret value, and one is the value that was sent in the previous message-- and the assumption is that this value, which is G to the XY mod Q, and that's the value that's computed as the key in the Diffie-Hellman protocol, is indistinguishable from random even to an attacker who knows G, G to the X, G to the Y, and Q-- these are the messages that would be intercepted. So if this assumption is true, that would be enough to prove that the Diffie-Hellman protocol is secure, and that an attacker who eavesdrops on all the messages can't compute the key because what the assumption is stating is that there is no way that they could tell that that value, which is the correct key, is any different from a random number even when they have access to this information. So this is an assumption, and it's even worse than that-- it's known that it's not true for certain values. We can show that it is distinguishable from random. So the security of the Diffie-Hellman protocol would follow from this assumption if we knew this assumption were true. We don't, but people have been trying for a long time to break Diffie-Hellman, and so far it seems to be adequately secure. That is certainly not enough to guarantee that it always will be. Next we're going to talk about some issues in actually implementing Diffie-Hellman. The first issue is that we need to do fast modular exponentiation. One of the steps in the protocol is to compute the generator to the XA power modulo Q where XA has to be some large random number. And it has to be large and random because if an attacker can guess XA well that would break the security of the protocol. So we need to know that this is possible-- that we can do exponentiation more quickly than we can do discrete log. So we're going to explore this with a quiz. So how many multiplications are needed in order to compute 2 raised to the 20th power? The straightforward way to do this would be to multiply 2 times 2 twenty times. For the quiz, I want you to answer the minimum number of multiplications needed to compute this. And I should point out that we're starting assuming that we don't know anything. Obviously, if you already know the value of 2 to the 20, you can do it without any multiplications. So the answer is we don't need 20 multiplications; we only need 4. And the property we're taking advantage of is that X to the 2A is equal to X to the A squared. That means we can break 2 to the 20 down into parts-- it's equal to 2 to the 10th squared which is equal to 2 to the 5 squared squared which is equal to--now 2 to the 5 is not divisible by 2-- so that's equal to 2 to the 4 times 2 squared and squared. And so the number of multiplications-- well what we're computing is each square involves a multiplication-- so there's 1 multiplication there, there's 1 multiplication there, there's 1 multiplication there, and there's 1 multiplication there-- so 4 total multiplications. So this does not scale exponentially in the size of the power. So this means computing A to the N power requires on the order of log N multiplications. In terms of the size of N, that means it's linear in terms of the number of bits needed to represent the power. So for the programming quiz your goal is to define a fast modular exponentiation procedure-- call it mod underscore EXP--that takes as inputs A, B, and Q, and returns the value of A raised to the B power mod Q, and its running time should be linear in the size-- meaning the number of bits to represent the input B. So to do this we're taking advantage of the property that A to the 2B is equal to A to the B squared. So let's first define a procedure to do squaring, and then we'll define our modular exponentiation procedure. And the easiest way to do this is to do it recursively. So our base case is when B is zero. Anything raised to the zero power is defined as having the value of 1. If B is not zero, well then we want to check if it is divisible by 2. If it is divisible by 2 we can do this transformation. Then the result is the square of the modular exponentiation of A to the B divided by 2. If B is not divisible by 2, well then it's of the form 2N plus 1 for some N. So if we break off the 1, we will end up with something divisible by 2. So that means we can return the result of multiplying A times the result of modular exponentiation A raised to the B minus 1 power. Now this is just fast exponentiation. We haven't yet taken the modulo out. If we didn't care how big numbers got, we could do that all at the end. But we want to do that as we go. We're going to add the mod operator to do these modulo Q. And that will give us the result of raising A to the B power modulo Q using a number of multiplications that's close to the log of the value of B. The next issue in implementing Diffie-Hellman I want to talk about is this problem of finding a large prime. The protocol assumes that we can start with this value Q, which is a large prime number. It doesn't have to be kept secret, but it should be different. And the reason we want it to be different is if there was only 1 large prime number, and everyone used the same one, then an attacker could do a lot of pre-computation based on that number and have a better chance of breaking a Diffie-Hellman protocol. So both protocols require a way to generate large prime numbers. For Diffie-Hellman you might think, well maybe we could have some large computing resource do that-- not every participant needs to do this themselves. For RSA, which we'll talk about in the next unit, every participant needs to generate their own prime numbers to be able to generate their own key. So we'll talk now about how to do that. So both of these crypto systems depend on finding large prime numbers. The first question we should ask is are there enough prime numbers that we can always find a new one when we need one? The good news is that Euclid proved, over 2500 years ago, that there are infinitely many primes, and it's a very elegant and beautiful proof. So let's look at how Euclid proved this. We'll start by assuming that there are a limited number of primes. That would mean we could define a set consisting of all of the prime numbers and know that it's a finite set with N primes in it. The elements would be 2, 3, 5, and so forth and there would be some highest prime, which we don't know the value of. So then we'll compute the product of all those primes and add 1. So let's have a quiz. Is the value that we computed--is P a prime number? So the possible answers are: No, it's definitely not prime; maybe, we don't know enough to determine that; or yes it is definitely prime. The answer is, given the assumptions here, P must not be prime. And the reason for that is P is greater than Pn. And we said this set includes all prime numbers-- that's the assumption we started with for this proof, and this number's the product of many numbers-- all of these are positive, adding 1 to it. So the value of P must be greater that P sub N and that means, according to our assumption about the limited set of all primes, P must not be prime. So since P is not a prime, that means it must be a composite-- which means it must be the product of some prime number and some other integer. So that means we can write P as some prime, and we've said all the primes are in this set, so it's something selected from that set, multiplied by some other number--we don't know what Q is other than it must be an integer. So now we have P, which we computed as the product of all these primes plus 1 which is equal to some prime from that set times Q. So now that we have this, what should we do to get a contradiction and prove that there's no maximum prime number? So the possibilities are to: divide by Q; to divide by P i; or try proof by intimidation. This is an especially effective proof technique when we're proving something that Euclid already proved. The answer is we don't need to resort to proof by intimidation in this case. We can prove this by dividing by P i. So what happens when we divide by P i? Well the left side--we have this product of primes. So we're going to still have this product of primes, now removing P i from the product. plus the 1 which is now divided by P i and we divided P i out of P i times Q so we have Q here. This is our contradiction. Q is an integer. This is an integer. P i cannot be 1 because it's a prime. It must be some integer 2 or greater. So that means that this is not a whole number. So there's no way we could get an integer as Q by adding something that is not an integer to an integer. Thus we've shown that there are infinitely many primes by contradicting our assumption that there was a limited set of primes. This isn't quite enough to be happy that we can generate large prime numbers when we need them. We need to know how dense the primes are and how to find a large prime. It turns out that the density of primes is such that the number of primes below some number X is proportional to X divided by the natural log of X. So this is log base E. And I won't attempt to prove this. I'll resort to proof by intimidation. This was conjectured by Gauss and then Legendre and proven later. And that means that the probability--if we pick some random number, the probability that that number is prime is approximately 1 over the natural log of X. So the question is: how many guesses do we expect to need to find a prime number that's around 100 decimal digits long? And in computing this, you should assume that this probability that a random X is prime is equal to 1 over the natural log of X even though this is an approximation. So the answer is we only need about 115 guesses. So that shouldn't take too long, as long as there's a quick way to tell if a number is prime. And to compute that result, we computed the log of 10 to the 100. You could do that in Python by using the math library and computing the log of 10 to the power of 100 where the base of the log is math.e-- that's the constant for the natural log. That would give you a result about 230. We'll divide that by 2 to get the expected number of guesses we need. So that suggests a strategy for finding a large prime-- which is to keep trying guesses until we find one that's prime. If it's not a prime, then we add 1 and we try again. Because of what we know about the number of primes being finite and the density of primes, we know that after a reasonable number of iterations this will actually find a prime and return that value. We could do a little better if we assume we start with an odd number and we'll assert that X is not divisible by 2 since that is very obvious that it would be prime and increment by 2 here. What's missing is we need some way to do this "is prime" test. So here is a naive way to test to the finest prime. We're going to go through the numbers from 2 to X minus 1, checking whether X is divisible by each of those. If it is divisible by any of those, that would mean that it is a composite number and we should return False. If we try them all and it's not divisible by any of those, well then it's prime. So the question is: Would this work? The possible answers are: Yes, this would work fine. No, it's too slow because the running time is exponential in the value of X. No because it's too slow because the running time is exponential in the size of X. And no, because there's a silly bug in my Python code. I hope the correct answer is the third choice-- that the running time is exponential in the size of X. We're looping through the values from 2 to X. So if X is N bits, that's going to require 2 to the N iterations, and for each one we're computing this modulo. So this is not going to work if X is large. And in order to use primes in cryptography, we need to find large primes. So we need a faster prime test. We need a faster test for primes. What we're going to use is a probabilistic test. That means if some number x passes the test, the probability that x's composite is less than some value. We're going to use probabilities like 2^(-128). This is certainly low enough for most uses in crytography. One way to think about that is if the key size is 128 bits, we already have that probability that someone would randomly guess that key correctly. The main basis of probabilistic primality tests is properties of prime numbers. One useful theorem about prime numbers is Fermat's Little Theorem, which states that if p is prime, if we select some number a between 1 and p and raise a^(p-1) power--modulo p--the result must always be 1. Maybe we could try lots of a's. If this equation always holds that a^(p-1) is congruent to 1 mod p, that would mean that p is probably prime. The problem is that there are some composite numbers that are known as Carmichael numbers where this test also holds for most a values. Indeed, it holds for all the a values that are relatively prime with p. Unless we try all the a numbers between 1 and p there is a high probability that this will always hold. If we need to try them all, this test isn't fast enough. It's still going to be exponential in the size of p. The good news is there is a faster test, which is known as the Rabin-Miller test. Sometimes it's known as the Miller-Rabin test. It was discovered independently by both Miller and Rabin. The main idea was originally proposed by Gary Miller in 1976. He provided the deterministic test based on the assumption that Riemann hypothesis was true. Michael Rabin, who won the Turing award in 1976, probably did some of his more important work after winning the Turing award, which is quite unusual, proposed this in 1980, and that's the one that we'll talk about. Here is how this works, and I'm not going to cover the mathematics behind it, but just enough to be able to implement the test. We're going to start with our guess n, which must be odd. Obviously, if it's even it's not prime. Because it's odd, that means we can write it as a multiple of 2 + 1. That means we can break it into 2^t s + 1. Next we want to choose some random a value in this range from 1 to n -1. If n is prime, we know that either a^s is equal to 1 mod n or we know that a^(s(2^j)) is equal to n - 1 mod n for some j. The j values are in the range from 0 to t -1, which is the power here. That's the big advantage that we only need to try a small number of values. If we don't find any value that satisfies this, we know it's composite. The important property that this test has that's different from the Fermat test is that the probability that a composite number of passes is always less than some constant. In this case it is less than one quarter. There are no bad composite numbers like the Carmichael numbers. If we choose a randomly, for any composite number the probability that the test is less than one-quarter. Now for a quiz. If we want the probability that our guess n to be composite is less than 2^(-128), how many times should we run the test each time picking different, randomly selected a value? The answer is 64, so each trial the probability is 1/4. We need 64 trials--each as independent-- to have the probability reach 2^(-128). Let's restate our protocol. Alice starts by generating a large prime-- we've seen how she can use that using the Rabin-Miller primality test-- finding a primitive root--we haven't seen how to compute that, but there are efficient ways to do it-- and selecting a random secret in the range 1 to the value of her prime-- -1. Then she computes the value of raising g to that power mod q, sends that to Bob, and I'm showing in this version of the program I'm sending bot the prime and the primitive root. Those could've been agreed on earlier in public. Nothing secret about them. Then Bob computes his value, selecting his random secret and computing yb. At the end they can both compute the same key, raising their respective values to the appropriate powers. So we've assumed so far an eavesdropper--a passive attacker-- who can hear all the messages on this channel but can't disrupt the channel. What happens if we have an active attacker? An active attacker can change messages on this channel. instead of just intercepting them and listening to them--eavesdropping on the message-- they can intercept and change the messages. Is this protocol secure against an active attacker who can modify messages in transit? The answer is most definitely not. This protocol depends on the integrity of the messages received. One easy way to see how it can fail is if an active attacker can change the value of ya to 1 and change the value of yb to 1, then Alice and Bob will still agree on a key but that key would be 1 raised to their secret power, which is still 1. It would be the key value 1, which would be known to the eaves dropper and make all messages encrypted using that key vulnerable. The attacker could also intercept that protocol here and separately execute the protocol with each party. This would make Alice think that she has a secure key with Bob, but it's actually a secure key shared with an attacker in the middle. It would make Bob that that he has a secure key with Alice, but it's actually a key shared with an attacker in the middle. That means the attacker in the middle could take a message that Alice encrypts with this key that's actually shared with the attacker in the middle, can decrypt that message, can then re-encrypt that message using the key that the attacker shared with Bob and send the result to Bob. Bob will decrypt it, thinking it's a good message from Alice. This is a very dangerous attack. Diffie-Hellman can only be used in places where either the integrity of the channel is guaranteed or there is a way for Bob to find out Alice's ya value and know that it's really Alice. It could also be useful in the case where there's some trusted directory that has the y values. Bob could look up Alice's ya value, know the q and the g values, and then Bob would know that he's communicating with Alice if he trusts this directory. One way to provide that is using a certificate authority, which we'll talk about in unit 5. This brings us to the end of unit 3. As long as we have a channel with integrity, we have a solution to our key distribution problem. We can use the Diffie-Hellman protocol where at the end of the protocol we've established a shared secret. Once we're established that shared key we can use it to send messages, using our symmetric encryption operations from the previous two units. We still haven't solved many of the important problems that Diffie and Hellman identified in this 1976 paper. The one that I highlighted at the beginning was digital signatures. We can't do digital signatures this way because the key is shared between Alice and Bob. That couldn't be used to sign something since either Alice or Bob would be able to make that signature. To have digital signatures, what we need is asymmetric crypotography. That's the main topic for Unit 4. Hope to see you there. Welcome to unit 4. Unit 4 is all about asymmetric cryptosystems. We started talking in unit 3 about the paper that Diffie and Hellman wrote in 1976 that introduced the idea of an asymmetric cryptosystem. They didn't figure out how to build one quite yet. They did figure out that key exchange protocol that we focused on in unit 3. What we'll do in unit 4 is talk about the first successful asymmetric cryptosystem, which is called "RSA," which is named for its inventors-- Ron Rivest, Adi Shamir, and Leonard Adleman. We'll look at how asymmetric cryptosystems can be used that we'll see how RSA is constructed, and we'll go in a fair bit of detail of why RSA works and why people believe it is secure. Then we'll look at some of the problems that happen when people use RSA in practice. Finally, we'll start talking about applications of asymmetric cryptosystems. We won't get too far in applications in this unit, but that'll be the main focus of unit 5. All the cryptosystems we've talked about so far have been symmetrical. What that means is that the key used for encryption, and the key used for decryption are the same. This lead to the problem that we talked about in the previous unit about how do you distribute this key between two parties, but even if you can solve that problem using the same key for encryption as you do for decryption limits what you can do with a cryptosystem. With an asymmetric cryptosystem we have different keys for encrypting and decrypting. We'll call one of those keys ku--the u stands for public-- and the other key kr--the r stands for private. Unfortunately, both public and private start with the same letter p. That's why we use u and r to distinguish the two keys. Public is a bit of a misnomer here. What we really mean is this key does not need to be kept secret. It's still very important that it's generated in a particular way and it's associated with the right identity. Before we get into the scheme for actually building a cryptosystem like this, and until one is actually built, it seems like this should be impossible. If we reveal the way we do encryption, that seems like we also reveal the inverse. The way to do decryption. For the cipher to be correct, decryption must be the inverse of encryption. For most functions when you reveal how you compute the function in one direction, that also reveals how to compute the inverse. Now let's look at a couple applications, if we could build a cryptosystem like this. First we'll look at a standard use of it to send private messages. This is what our goal was initially for the symmetric cryptosystems. That assumed that we had a shared key to begin with. Now let's assume that we don't. Let's assume Alice wants to send a private message to Bob. She doesn't have a secure channel, and she doesn't have a key shared with Bob, but she does know Bob's public key. We'll call that KUB. Bob knows his private key that corresponds to the public key that Alice as well as anyone else who wants to know knows is associated with Bob. Now Alice can send a message to Bob, encrypting it with Bob's public key knowing that the only one who can decrypt it is someone who knows Bob's private key, which should be only known to Bob. Bob decrypts the message using his private key. Now we'll have a quiz to see that everyone is understanding asymmetric cryptosystems. What correctness property does this rely on? Here are the three choices. Pick the one that this private messaging system is relying about the E and the D functions. The answer is it's relying on the third property that decrypting with the private key is the inverse of encrypting with the public key. The second property turns out to be one that's also very useful It's not necessary for this use of asymmetric cryptosystems, but it is necessary for other uses. Many asymmetric cryptosystems, including RSA also provide this property. An important application of that is to do digital signatures. We think of physical signatures as authenticating documents. When something is signed, that means that the signer is responsible for the message in the document. Physical signatures don't actually work very well for this purpose. Someone can cut and paste a physical signature or maybe they can modify the document after it's signed. There's nothing that really guarantees that the signature and the document are matched. What we want with digital signatures is something much stronger than that. We want something that shows that the person who signed it agreed to the message, and the message can't be changed without also breaking the signature. How can we do that with asymmetric cryptography? Here is our goal: we want Alice to be able to sign a message, transmit that message to Bob, Bob should be able to read the message. and know that it could only have come from Alice. The question is what should we use for the respective keys k1 and k2 in order to provide that digital signature where the message that Bob receives proves that it was signed by Alice? Here are the choices. We've used for k1 Alice's public key and k2 Alice's private key. For the second choice we'll use Bob's public key for k1 and Bob's private key for k2. For the third choice we'll use Alice private key for k1 and Alice's public key for k2. The answer is the third choice. The first choice doesn't work, because it assumes Bob knows Alice's private key. If Bob knows Alice's private key, then it's not Alice's private key anymore. The second choice doesn't work because the message is being assigned using Bob's public key. There's nothing that proves that it came from Alice. In the third choice, the message is being encrypted using Alice's private key, which only Alice should have. Anyone who has Alice's public key, which includes Bob, can then decrypt the message. Then if it decrypts to a reasonable message, they know that it came from Alice. The correctness of this depends on the inverse property that we saw on the previous quiz. That in order for signatures to work, we need to be able to do encryption and decryption in the reverse order and have them still be inverses using the appropriate private key private key for decryption and public key for encryption. There are lots and lots of other interesting applications of asymmetric cryptography. We'll see some of those later in unit 5, but for now I want to focus on how to actually build an asymmetric cryptosystem. What we want to build is what's known as a "trap door" one-way function. A one-way function would be a function that is easy to compute in one direction and hard to compute in another direction. With asymmetric crypto we need to reveal this function. We want the reverse to still be hard, but we want some way to be able to do the reverse easily if we know some secret. That's our trap door. We want to be able to--if you have some secret key, you can do the inverse. If you don't, you can't. That's what makes an asymmetric cryptosystem. It's hard to do in the reverse direction unless you have this extra key. But revealing the easy way to do the forward direction does not reveal the easy way to do the reverse direction. Diffe and Hellman envisioned such a cryptosystem in the 1976 paper that we talked about last unit, but they didn't devise a function that had this property. The first cryptosystem to successfully have this property, is the RSA cryptosystem. That'll be the focus for the rest of this unit. The RSA comes from the initials of its three inventors-- Ronald Rivest, Adi Shamir, and Len Adleman. One thing you'll notice in cryptography is many of the most important ideas are named after groups of people. We have Diffie-Hellman. We have RSA. We have Needham-Schroeder. We'll see lots of other examples. This is different from what we tend to see in mathematics where most of the big ideas have just one name. Cryptography is a very collaborative process. The way Rivest, Shamir, and Adleman worked together-- Rivest and Shamir came up with lots of ideas, trying to come up with ways to provide this public-key property, and Adleman would find the flaws in them. They kept trying this and eventually came up with the one idea that Adleman couldn't find any flaws in, and that's the one that we know of today as RSA. RSA was the first successful public-key encryption system. It's also one of the most widely used crypto systems and continues to be the most widely used public-key system today. The way it works is actually quite simple. We have the public key as a pair of two numbers, e and n, where n is the product of two primes. The secret key is a pair of two numbers. The n is the modulus, which is the same in both the public and the private keys. The difference is in the public key we have e. In the secret key we have a secret number d. To perform encryption of some message m, we raise m to the e power mod n. To perform decryption using the secret key, we have some ciphertext, which we raise to the d power mod n. That means encryption and decryption are the same functions, just with different powers that we're using, depending on whether we're using the public key or the secret key. RSA is a very important cryptosystem, but it's simple enough that we can fit the whole thing on one screen or even on a license plate. Unfortunately, in the state of Virginia, the license plates don't allow you to have exponents in them. If you live somewhere where you're DMV allows you to have exponents in license plates, that's great. You can get a much better version of this than I have. Actually understanding why it works and the argument that it's secure, require understanding some more things, and in particular, the we choose the values of e, d, n. In fact, we have to be careful how we choose the message as well. To check that you understand how the crypto system works, we have a quiz. Suppose n has chosen to be 6371. That's 277 times 23, the two prime numbers. And I should emphasize that this is not a secure choice. For RSA to have any security, p and q have to be very large prime numbers, large meaning several hundred digits long, but to keep things simple, suppose we pick our n value as 6371. What is the maximum value of our message? The answer is 6370. In order for this to work, well, we need a mapping. We need encryption to be invertible. Given that the output is mod n, we have output values that the possible values here would be from 0 to n - 1. We need to know that this mapping maps each message to a unique cipher text, otherwise they wouldn't be invertible. If 2 messages map to the same value, then we wouldn't know which to decrypt to. That would definitely be the case if we have more than the modulus number of values. That would mean that we've wrapped around and we've definitely used one at least twice. But as long as m and n are relatively prime, we should generate all the different values, so we can use each of these as different messages, but we should be very careful. What if the value m is 0? If m is 0 no matter what the exponent is, we still get 0 as our result. That's not a very good encryption function if the result doesn't depend on the key. The same thing if m is 1. We still get 1 as the output, and in fact, it's dangerous for any small value to use this encryption. And one reason you can see that--and we'll talk more about this later-- is the key is public. We're assuming that the adversary knows e, so if there's only a small possible set of m values, the adversary can just try them all and see which one maps, so it's very dangerous to use a small m as the input message for RSA. We'll talk more about that later, but first I want to talk about why RSA is correct and then why we think it's secure, at least when it's used in a very careful way. First we're going to talk about correctness. The question is what property do we need in order for RSA to be invertible? And let me remind you the encryption and decryption functions. Here are the possible choices. Which one of these do we need in order for the RSA encryption system to provide the correctness property that we can invert messages encrypted with a public key by using the private key? The answer is the second choice. What we want to know is that we can get the message back, so to get the message back. We started by encrypting the message. Then to decrypt it, now this is what goes in as the cipher text, so we're going to raise this to the d power mod n. And then if you remember when we looked at Diffie-Hellman, we have this rule for combining powers of powers, that this is equivalent to m to the ed mod n, and we want that to be equivalent to the message. If we divide both sides by m, this becomes 1. This becomes m to the ed - 1. We've reduced the power by 1 because dividing by m. This is the property we want. If we had this property, all messages would decrypt as one. That wouldn't be very useful. This property is always true but not useful, and this property is unlikely to be true. That means our goal is to select values for e, d, and n that satisfy for all m values, that m to the ed - 1 is congruent to 1 mod n. That's our goal. If we have that, we have the correctness property we need for RSA. The main theorem that we're going to use to get that was devised by Euler, and the theorem is that if a and n are relatively prime, then a raised to the totient of n power is 1 mod n. I'll talk soon about what the totient means. If we can obtain this, then what we want to do is set ed - 1 is equal to the totient of n. Then we would have exactly the correctness property we need with the assumption that m and n are relatively prime. This is the totient function, and what the totient function means is the number of positive integers that are less than n and are relatively prime to n. We'll have a quick quiz to see that you understand what the totient means, and the question is what is the value of the totient of 277? And I'll point out that 277 is my favorite prime number. The answer is 276, and I hope you didn't work this out by counting them all. There's an easy way to solve the value of the totient when n is prime, and that is if n is prime, that means that all of the positive integers less than n are relatively prime to n, so the totient of n is n-1 for prime number n. Let's try a tougher one. What is (15)? The answer is 8. The easy way to see that is to observe that 15 is 3 5. That's a composite number that can be broken down into two primes. Then if we look at the numbers from 1 to 14-- these are all the positive integers less than n-- we want to figure out how many of these are relatively prime. The multiples of 3 are not. You can cross all those out. true for the multiples of 5 as well. The multiples of 5 are not. That leaves us with eight numbers. This works in general. If we know that n is the product of two primes. Then we could compute (n). It's the number of integers less than n, which is also pq - 1. That would be all the integers. Then we subtract out all the multiples of p that are less than n. Since n is pq, there are q - 1 of those. Then we want to subtract all the multiples of q less than n, which is again pq, so there are p - 1 of those. If we do the algebra, we get pq - (p + q) +1, which we could also write as (p -1)(q - 1). Since p and q are prime--and this property depended on q being prime. Otherwise, some of these multiples might have collided. Since they are prime, we know they didn't. That means that we know that (n) is equal (p)(q). This is going to turn out very useful for RSA. The reason for that is if we know the factors of n, we have an easy way to compute the value of (n). But if we don't know the values of p and q, it appears to be hard to compute the value of (pq). That's the crux of what the security of RSA relies on, and we'll talk more about that later. For now we're still focused on correctness. Remember what we wanted to prove was that a^(n) is congruent to 1 mod n. Now you should understand what the totient function means here. First we're going to look at a different theorem, which we've already used, that's very similar. This is Fermat's Little Theorem, which is a^(n - 1) is congruent to 1 mod n as long as n is prime and a is not divisible by n. We used this in the previous lecture when we talked about finding large primes. This was the basis of the primality test. We didn't prove it, though, and it's actually fairly straightforward to prove. So let's see if we can prove that Fermat was correct. The question is what is the value of this set where we're taking a mod n, 2a mod n, 3a mod n, 4a mod n, all the way up to (n - 1)a mod n. You have four choices. The answer is the first choice. We know that this is the case because this set has n - 1 elements. We also know that there are no duplicates in this set, because we know that n is prime and a is not divisible by n. That means that this will generate the numbers 1 through n - 1 in some order, some permutation of those, but since it's a set, it's the same as the set 1 through n -1. This turns out to be a useful property for proving Fermat's Little Theorem, because we can set these two things as equal and multiply all the elements in those sets. Since we know they're the same set, we know that their products are also equal. This product is (n - 1)!. Since the sets contain the same elements, we know their products also must be equal. So this is the product of the first set. This is the product of the second set. It's (n - 1)!, because it's multiplying all those numbers up to n - 1, and mod n--those must be equal. We took the mod n out of each of these terms, but that's fine. Then we can simplify this, taking out the a's. If we take out all the a's what we have is (1 2 3 ... n - 1)a still equal to (n - 1)! mod n. Now we can separate those, taking out all the numbers at all the a's, so we'll have n - 1 terms. We have n^(n - 1) here times all the numbers is still equal to (n - 1)! This is the same as (n - 1)!. Now we can remove these terms, and we're left with exactly what we wanted-- that a^(n - 1)! is congruent to 1 mod n. This is pretty close to what we want, but we needed for Euler's theorem and what we'll need for the RSA correctness proof is this property with (n). That's different from what we have here. Let's look at the two cases. The first case is where n is prime. When n is prime, (n) is equal to n - 1, so we're done. We have exactly what we need for that from Fermat's Little Theorem. Case two is where n is not prime, but we know that a and n are relatively prime. In this case, we know since n is not prime, there are some numbers that are not relatively prime to n. Let's put those in a set. We'll call it R. We're going to multiply that set by a mod n to get a new set we'll call S. Now I have a quiz about R and S. Here are the choices. Check all of the statements that are true. The answer is the first and the third are true. For the first one we know that the size of r is equal to (n). That's the way the totient is defined. It's the number of positive integers less than n that are relatively prime to n. That's exactly how we define r. The second one is not true. In order for this not to be true, it would mean that there is some element where a xi mod n is equal to axj mod n, but that's not the case, because a and n are relatively prime. We know that these values must all be different. The only way these could be equal is if xi is equal to xj. That means that the sets are the same size, and this set contains numbers only up to n - 1. This set also contains numbers from 1 to n - 1. That's why we know that s actually contains the same elements as r. It must be a permutation of r. Now we can use a similar idea to what we used in the proof for Fermat's little theorem. We can take the product of these two sets. Since we know the contain the same elements, we know their products are also equal. Here is what we get: the (R) is equal to the xi's all multiplied together, which is equal to the (S), which is equal to these values all multiplied together. We also know from this property that the number of terms is (n). That's true on both sides. That means we can separate out the a's from the x's. We're going to have a^(n) times all the x's--still equal to this product. Now we can do the division. Removing the x's from both sides, we end up with exactly what we need, which is that 1 is congruent to a^(n) mod n for any a and n where a and n are relatively prime. Now let's recap RSA and see how things fit together. We have our modulus n, which is the product of 2 primes. We have our encryption function, which raises m to the e power, and we have our decryption function, which raise the ciphertext to the d power. For the invertibility property, we need to know that m^(ed - 1) = 1 mod n. What we know from Euler is that that's true for a and n relatively prime. That means if we can pick e and d such that ed - 1 is equal to (n), then we're real close to having the correctness property we need. To see how close we are, let's ask a quiz whether we can determine if m and n are relatively prime. That's the property we need to be able to use Euler's theorem. The answer is not necessarily. We can't guarantee that m and n are relatively prime. What we know is that n is pq, which are prime, and we know that m is less than n. But it's possible that m is some multiple of p or some multiple of q. Those could still be less than n if c1 is less than q or c2 is less than p, but that would mean that m and n are not relatively prime. That means that we can't use Euler's theorem directly. We'd have to deal with these special cases. That's a little kink in our correctness proof. I'm not going to go through those details. They're not too interesting, but we're real close, and we're going to assume that we can deal with this detail, and now we have the invertibility property that we need for RSA to be correct. One useful thing to notice here is that this works in both directions. What we wanted for signatures was invertibility where we do decryption first and then encryption.. That's equal to c^de mod n, which is also congruent to c mod n. We'll have correctness in both directions. We have one big issue left to resolve, which is how do we pick e and d to make this property true? For correctness what we need to know is that ed - 1 is equal to (n), and we know since n is the product of the primes p and q that this is equal to (p-1)(q-1). The quiz is which one of these should be selected randomly-- either e, d, or it doesn't matter which one? The answer is that it'd better be d. The reason for that is that's what the private key is. The private key includes the secret d. The public key includes e. If we want the private key to be secret and hard for an adversary to guess, it's got to include something that is unpredictable. That will be the value of d. N, again, is part of the public key, so that doesn't provide any security for the private key. Notice from this that since e is public, it's okay if e is computed in a predictable way. In fact, e is often chosen to be a small value. We're going to start by picking a random d that has to be relatively prime to (n). We can pick random numbers and test that this property holds and pick again if it doesn't. Since d is relatively prime to the totient of n, that means it has a multiplicative inverse in that group. So we can find some value e such that d is equal to 1 mod (n). There exists some value e such that de is congruent to 1 mod (n). Now the question is given d and the totient of n is it hard to compute e? The choices are, yes, it'd better be hard. Otherwise RSA would be insecure. No, it's easy. We can use the extended Euclidean algorithm to do this. Or three--no, it's easy. We can just divide the (n) by d. The answer is the second choice. The extended Euclidean algorithm does exactly this. It will give us that multiplicative inverse. We won't go through that in detail now. Maybe it'll be a good question for a homework problem. I'm going to ask a slight variation on this question. This time you're given e and n and the question is is it hard to compute d. The answers are yes, otherwise it would mean RSA is insecure or no we can just compute the totient of n on the multiplicative inverse using using the extended Euclidean algorithm. The answer is "yes." At least we hope it's hard to compute d. Otherwise, RSA would be insecure. This, of course, is only true if n is big enough and n is a product of two primes. The reason the no answer doesn't work is because actually doing the computation of the totient is difficult. If we don't know how to turn n into a factor of primes, then it's hard to compute this. We finished showing the correctness property for RSA. Now we're getting to the question of how can we claim RSA is secure. This claim that it is secure because otherwise it would be insecure is not a very convincing proof. That's what we're going to look at next. Now we're going to look at whether RSA has the security properties we need. We've seen that it has the correctness property, that encryption with a public key and decryption with a private key are indeed inverses. But we want to know also the most important property-- that it's difficult for an attacker who doesn't have access to the private key to perform the decryption. This is the property that we need that given e and n, which is the public key, it's hard for an attacker to find d. We actually need stronger properties than just this. We want to also know that the attacker can't learn anything about the message. This is not strong enough by itself to know that an attacker can't learn anything about the message. In fact, we'll see there are cases where an attacker could learn something about the message without learning d soon. The first thing we know is that this would be easy for someone who knows the factors p and q--the two large primes that we multiplied to get n. We know that because such an attacker could compute the multiplicative inverse of e mod the totient of n. If you know the factors of n, you know the totient, because that would be the totient of p times the totient of q, which are both primes. So easily solved. Our security argument relies on two things. The first is that showing that all ways of breaking RSA would allow some easy way to factor n. If we could use that way of breaking RSA to factor n, the we could always use that to factor large numbers. That would contradict our second claim that factoring large numbers constructed by multiplying two large primes is hard. We're going to show the first thing first--that other ways of breaking RSA, other ways of finding d, would allow us to factor n. Then we're going to argue from experience and historical effort that factoring seems to be hard. The first question is whether it's easier to compute the totient of n than it is to factor n. Our goal is to show that that's not the case. What should we do to show that? Here are the choices. Give p and q, show that it's hard to compute the totient of n. Given the totient of n, show that there is no easy way to compute p and q. Or given the totient of n, show that there is an easy way to compute p and q. The answer is we want to do the third thing. We want to show some way that if we had the totient of n we could also factor n. That would show that finding the totient is not easier than finding the factors. Of the other choices, the first one is actually not true. If we have p and q, it's actually easy to compute the totient of n, and it's easy because we know the totient of n is (p - 1)(q - 1). The second answer is not true, but if it were, it wouldn't help us prove that RSA is hard. It would just show that there might be an easier way to compute the totient of n than there is to factor. This would be damaging for our security proof. Fortunately, at least for the security of RSA, the third one is true, and next we'll show why. Remember that the totient of n, where n is the product of two primes, is the totient of those two primes. Multiplying this out, we get pq - (p + q) + 1. Pq here is the same as n. Now we know that p + q is equal to n - (n) - 1. Now we're our goal is to show that if we know (n) we can easily find p. If we can find p, we can easily find q. What we want to do is find an equation for p - q that only uses n. Then we'd be able to add these two, get rid of the q and be able to solve for p as the sum of these two equations divided by 2. That's our goal to get a solution for p - q that doesn't involve n. Let's see if you can figure that out yourself. I will give you a hint, which is to think about squaring p - q. The answer is the first choice. We'll go through the steps next. The third choice is clearly useless, since it's still in terms of p and q, and it's also incorrect. The second choice would be useful if it was correct, but it's just not correct. The hint was to try squaring p - q. We get this. Pq here is equal to n, so we can simplify this to p^2 + q^2 - 2n. We've still got these terms of p^2 and q^2. If we want to get an equation all in terms of n, we need to get rid of those. But we also know was p + q is. Let's try squaring that. When we square p + q, we get p^2 + 2pq + q^2, which--substituting the n for pq--is equal to p^2 + q^2 + 2n. This looks good. If we subtract these two, we get rid of p^2 and q^2. Now, remember, our goal is to get an equation that doesn't involve p or q, but we have this property from the totient that we know was p + q is in terms of just n. We're going to substitute that in. Then we can just take the square root, which is how we ended up with this equation here, which was the answer to the quiz. Now we know the value of p + q in terms of an equation only using n. We could have these two get the value of 2p, divide that by 2, and get the value of p. Once we have the value of p, we factor it is, because we can divide n by p and easily get the value of q. So our nice little box here turns into our proof is finished. I should emphasize that there is no difficult computation to do here. We're doing subtraction, squaring, and multiplication. The square root is a discrete square root, but discrete square roots are not difficult. We know that this result will be an integer, because the value of p must be an integer. The last thing that we need to show is there isn't an easier way to compute d than finding the factors of n. This follows from what we just showed-- that if we know the totient, we could easily find the factors, because the correctness of RSA depends on this property. That means that there is some integer k such that k (n) is equal to ed - 1, which means that we already know the value of e. We're assuming now that we figure out a way to compute d. If we can solve this, then we know a multiple of the totient. Once we know a multiple of the totient, it's easy to find the factors p and q. If there were some easier way to find d than factoring the modulus, that would provide an easy way to factor. We finished showing that at least all the obvious mathematical ways of breaking RSA would easily allow us to factor n. This certainly doesn't cover issues in implementation or issues in weak choices of messages or keys, but assuming all of those things are good then we've shown that all the obvious mathematical ways to break RSA are equivalent to factoring n. That means if factoring is hard, breaking RSA would be hard. That's the second part of this claim--that factoring is, indeed, hard. One way to convince people something is hard is to post a challenge, provide some motivation, and see if anyone can solve it. This particular challenge was posted in Martin Gardner's Scientific American column in an article that he wrote in August 1977 that was introducing RSA to a wide audience. The technical paper on RSA was published shortly after that in 1978. As part of this article, he asked for Ron Rivest to come up with a challenge. This was the challenge that was published in this article. Given a public key and an e--and here we have n as a 129-digit number-- and the intercepted cipher text, figure out what the message was. That can be done by factoring n. Ron Rivest estimated that factoring n would take about 40 quadrillion years. It turns out this was a little bit off. The actual time was 17--without the quadrillion--years. It was shown by a team lead by Derek Atkins in 1994 that these two numbers multiplied together to produce the n in the challenge. For the quiz, the question is what does this mean-- the fact that Derek Atkins and his team were able to break n into these two factors. This is kind of a silly quiz, but I hope you'll think about it anyway. Some of the answers are more serious than others. It could mean that RSA is fundamentally broken. It could mean that the magic words are "squeamish ossifrage." It could mean that RSA keys need to be longer than 129 for good security. Or it could mean that time flies when you're factoring, The best answer is the third one. It definitely raises some concern that the time to factor is much less than what was estimated here but maybe that just wasn't a very good estimate to begin with. It doesn't mean that RSA is fundamentally broken. If we make our keys large enough, unless we understand more about the cost effector, maybe that's still good enough. So we'll talk about that next. It is the case that the message could be decrypted. Once you have these factors, it's easy to compute d. The actual message was "The magic words are squeamish ossifrage." This gets back to the fundamental question we have of how hard is factoring. We saw that it was possible to factor 129 decimal digits in 1994. The computing resources to do this were about 1600 machines collaborating on the internet. Certainly that's much less than many people have access to today. There are larger numbers that have been broken since. The largest challenge that's been broken so far was RSA-768, which was a challenge. Those are bits instead of decimal digits. That's equivalent to 232 decimal digits. That was done in 2009 using about 2000 years of computational power. If we want to know that RSA is secure, we need to understand how the cost of factoring depends on the size of the numbers that we need to factor. We'd like to know that if we pick a large enough key, even an adversary with a large amount of computational power-- and as the years go on adversaries will have more and more power-- still won't be able to factor the number and break the RSA. We don't know any way to actually prove a problem like this is fundamentally hard. The best we can do is look at the best-known algorithms and have some confidence because people worked very hard to find better algorithms-- and progress has been slow--that, barring some very unexpected breakthrough, it won't bet much faster. This is quite unsatisfying, but it's the best we can do. To talk about this, I want to measure the size of the input. That's the number of bits--we'll use "b" for that--in the modulus. We could try a brute force approach, which would go through all the numbers from 2 up to the square root of n, checking whether each one is a factor. If it finds a factor, then it returns that. Let's be optimistic and assume, unrealistically, that we could do this factoring test which could be done by finding the greatest common divisor of the two numbers in constant time. Then we're going to need to go through this loop square root of n times, which means our running time will be linear in the square root of n, but b is the log of n. Our running time will be exponential in b/2. This clearly is not going to work for large b. But this is not the best-known factoring algorithm. The fastest known factoring algorithm as of 2012 when I'm recording this is what's known as the General Number Field Sieve. This is a bit faster than the brute force, but it's still essentially requires trying all possibilities. Its running time is exponential in b^1/3 log b^2/3, which is still much worse than being polynomial. One important caveat--this is the best known factoring algorithm assuming a classical computer. If you have a large quantum computer, which no one does yet, there's a faster algorithm, which is known as Shor's algorithm, which was created by Peter Shor in 1994. That actually has a running time that's polynomial in the number of bits. This development, which we won't go into more depth in this class, is both why there's a tremendous amount of practical interest as well as theoretical interest in quantum computing. It seems to allow for some problems that seem to be hard a way to compute them more efficiently. This means that factoring is in a complexity class called "bounded error quantum polynomial time"--known as BQP. What's unknown is whether factoring is in the class NP-Hard, which are the hardest problems that can be solved by a nondeterministic turing machine in polynomial time. If you haven't had a theory of computation class, I'm not going to explain this now, but I hope some of you have had it and will be able to answer the next quiz, which is if it's proven that factoring is NP-hard, what would it mean? The answer is it would mean that NP is a subset of BQP. It wouldn't mean that RSA is secure. In fact, we know because of Shor's algorithm, if someone can build a sufficiently large quantum computer, they would be able to break RSA. But it would increase our confidence, because it would put factoring in the class of problems that are believed to be hard to solve in polynomial time as long as P is not equal to NP. If it's also showing that P is not equal to NP, then this would mean that there is not any polynomial time solution for factoring with a classical computer. That would greatly increase our confidence that RSA is secure. To see why this is the case, we can think about how complexity classes as sets of problems. That makes the green circle the class NP. We have the class BQP, which is not known whether it's a subset of NP. We have the problem factoring, and we know factoring is somewhere in this circle. The reason we know factoring is NP is we have solutions that solve it in non-deterministic polynomial time. Even the brute force solution, if we could execute all those passed at once, would solve it in polynomial time. The other way to see that it's an NP is there's an easy way to check if the answer is correct. That's just multiplication. That's enough to know that factoring is definitely within the NP circle. We also know that it's within BQP. If it's known to be NP hard, that would mean it's as hard as the hardest problem in NP. It's also known to be in BQP, so that would mean that we'd have to squish-- everything in the circle would now be inside BQP, because there is no problem that would be harder than factoring in NP. I hope those of you who haven't yet studied the theory of computation will forgive the full excursion, but this is a really important point to understand what the security of RSA relies on--these assumptions about problems being hard. We finished the theoretical argument for the security of RSA. Now I'm going to talk about some ways that things can go wrong when RSA is used in practice. Our security argument assumed that m was a large random number. What happens if m is small? Assume n is some large unfactored number. RSA-1024 is one of the unsolved RSA challenges. We'll pick e as 17, and e is often a small number. It doesn't inherently become insecure when we pick a small e. The advantage to picking a small e is that it makes encryption fast. You've intercepted the ciphertext below. You'll be able to get that number in the Python ID, so you don't have to try to copy it correctly. The question is what is m? Can you determine it from that number? Here's how we can solve this. We have our cipher text c. We have our public key component e. The key point here is that when m is small, if m^e is less than n, then we never wrapped around the modulus. That means decryption is as easy as finding these eth of the cipher text. What we're looking for is the eth root of c. We can compute that in Python using the pow function. So 1 divided by 17 will give us the 17th root of c. We get the message was 7. This means decrypting is very easy if m^e is less than n. This is also true if m^e is less than some multiple of n. We can try just adding n to the cipher text and taking the eth root. If we ever get an integer result, that's a good indication that that was the message. Suppose we want to send a small number-- say it's the day of the year for some very secret event like the next Udacity party-- using RSA. Can we do it? Which of these solutions would work? We could pick a large value x, and then send a pair where x is in the clear text, and we're sending the encryption of n + x encrypted with Alice's public key. We could pick a large enough value of e so that m^e is always much greater than n. We could pick a large random value x and send m + x encrypted with Alice's public key. Or none of these. The answer is that none of these solve our problem. The third one would be secure, but would be useless, because there was no way from this to learn the value of M unless you know x, and there's no way to know x. The second one doesn't solve our problem, and it's not even possible. M could be 1. There is no value of e we could pick that would make m^e when m is 1 exceed n. So that doesn't work. The first one is the most plausible, but it also doesn't work. Let me show you why the first one doesn't work. Even though the previous attack of just taking the eth root would not work on this. There's another attack that does, which is just to try all the messages. We can try for each message, and in this case the messages were the numbers from 1-365 whatever the set of messages is. As long as that's small, we can try them all, and we can try for each of those messages we're going to try encrypting using the public key. That message--see if it matches the cipher text. In this case it's not that message that we would try encrypting. We would try encrypting x + m, since we know the value of x. We find the one that matches. Then we know what the message was. Without randomizing the encryption system, this is very dangerous. If the set of message is small in a public key system, that's always dangerous because the public key is known. We can always try encryption in the forward direction and see which one matches the intercepted cipher text. If we're going to use RSA in practice, we need a solution to that, because we can't assume the messages are large and random and from some large set. Our goal is to avoid the message guessing attack. The key idea is just to add some random padding to make the message large and unpredictable. There are lots of different ways to do this. One is the public key cryptography standard number 1, which is to replace the original message with 0 padding followed by a 10, followed by some random bits, followed by a bite of 0s, followed by the original message. This uses at least 64 random bits. Depending on the length of the message and the size of n, it may use more. This prevents the small message-space attack, since even if the set of possible messages is fairly small, an attacker needs to try all possible choices for the random bits, which is at last 2^64 of them in order to test those messages. There's a better way to do this, which is known as optimal asymmetric encryption padding--OAEP. I won't go into the details of that, but the main idea is to XOR the message with the output of a cryptographic hash function that takes in a random value. But the recipient can still decrypt the message, because they can obtain the random value and XOR out the result of the cryptographic hash. The last thing we'll do in this unit is talk about how to use RSA to solve the problem of signing a document. This is what we started with as one of the key motivations for asymmetric cryptography. This would be the straightforward way to do this, that Alice would take the document, encrypt it using her private key. That produces a ciphertext, which is really the sign document,. Anyone who has her public key, including Bob, can now use the encryption using her public key on that signed document and obtain the document and verify--because this document was decrypted using Alice's public key-- that only Alice could have created it. The problem with this approach is RSA is very expensive. We don't want to use it on large documents. It costs about 1000 times more as much computing power to do one RSA encryption as it does to do symmetric encryption. That means we don't want to encrypt the whole document like this. We need to do something else. The question is how should Alice sign a large document m? We'll assume that Alice has a public key known to anyone who wants to read the document, a private key known only to Alice, an implementation of RSA, and H-- a secure cryptographic hash function. Which one of these options makes the most sense? The answer is the third choice. We're only looking for signatures here, so we don't need to encrypt the document. We don't care about confidentiality. We can send the document in clear, but what we want to send along with it is something that proves that it's the document that Alice intended. To do that we need to do something that uses Alice's private key. That's these two options. If we use the public key--well, anyone can do that. The public key could be known to anyone else. We're assuming that the private key is only known to Alice. The only one who could compute these two things would be Alice Then we have a choice of which one of these two things is better. If we believe we can have one-way hash functions that have the collision resistance properties that we talked about. Then this is much better, because the output of the hash function is small fixed-size value. It's only for a given security level. It might be 256 bits. We can encrypt that much more cheaply than if we had to encrypt the whole document using RSA. That's why this is the best choice, and lot's of protocols are based on this kind of solution where we use RSA to encrypt something small, which could be a hash value or it could be an encryption key. Then we use that with symmetric crypto for the rest of the message. I hope you've enjoyed our introduction to RSA and asymmetric cryptography. It's really a very powerful idea. In this unit, we introduced the idea of using asymmetric cryptography both for privacy and for signatures. There are lots more that we can do with asymmetric cryptography. We'll talk about some of that in the next unit. We talked about the RSA cryptosystem, which is probably still the most widely used public key cryptosystem. There are billions of transactions going on every day using RSA. Nearly every time you use secure website, it's very likely that RSA is being used. We'll talk about the protocol for that next unit. We argue that RSA was correct, that it was invertible and had all the properties that we needed to be able to encrypt and decrypt messages. Its correctness depended on theorems that go back thousands of years from Euclid--there are more recent ones, but still many hundreds of years old from Euler and Fermat. It's putting those things together in the right way that lead to this solution that enables most of modern ecommerce. We also argued that RSA is probably secure--at least for the time being. That depends on the hardness of factoring. Then we talked about some issues in using RSA in practice, the dangers of encrypting small messages or messages from a small set of known messages, and solutions to that based on using random padding. There's one issue that we haven't covered yet. That's how does Bob actually know Alice's public key. If they can get together in a room, maybe she could give it to him. That's not usually possible, so this is a hard problem. Until we have a solution for that, we're going to have to take away Bob's smiley face. He's still a little bit frowny. Until we have a solution to this problem, we don't have a good way to use asymmetric cryptography in practice, because it relies on being able to get all these public keys. That's one of the things we'll talk about next unit-- how to build a public key infrastructure so Bob can learn Alice's public key as well as lots more interesting applications of asymmetric cryptography. Hope to see you back for unit 5. Welcome to Unit 5. The main topic for Unit 5 is cryptographic protocols. We're going to look at ways to use the things that we've seen in the previous 4 units to solve problems. So we'll be using symmetric encryption, we'll be using cryptographic hash functions, and we'll be using asymmetric encryption to solve problems. And the main problem we're going to solve is how to authenticate between a client and a server. Any time we talk about cryptographic protocols, we have to think about what our threat model is. If we want to argue that a protocol is secure, we need to understand the threat model, which means knowing the capabilities of the adversary. In order to argue the protocol is secure, we need to argue that an adversary with only those capabilities will not be able to break the protocol. So what are the kinds of things we need to assume? The first main assumption is that the adversary has limited computational power. And that generally means that we're assuming that our encryption primitives work and the attacker who intercepts a message encrypted with some key k is not able to decrypt it unless they know k or have some other advantage for decrypting that message. We also assume that hash functions have the properties they should-- that it's preimage resistant, so an adversary who has the hash of x cannot figure out what x was, and that they also have strong collision resistance-- that an adversary can't find 2 values that hash to the same output. We might also make assumptions about what the attacker can do to the network. If the attacker is passive, that would mean that they can only eavesdrop. They can listen in on messages on the network, but they can't modify them and they can't inject their own messages into the network. A more powerful adversary would be an active attacker. An active attacker controls the network. They can modify data and messages. They can replay messages. That means they can record messages on the network and then at some later time replay a message that they heard previously. They can also do attacks like we saw against Diffie-Hellman where they act as a middleman intercepting traffic between 2 parties and replacing it with their own traffic. So let's have a quiz to see if you understand threat models. Which of these threat models would be a good model for an adversary who controls a router on the Internet? The answer is this would be an attacker who has limited computational power but does have active capabilities. Since they control the network, they can modify messages going through that router, they could replay messages, they could act as a middle attacker. They have lots of things they can do other than just intercept and try to analyze intercepted messages. For most of what we're going to do, we're going to assume this threat model. We're going to assume an attacker who does have limited computational power. That means we'll assume that encryption and hash functions work the way they're supposed to. Certainly that's not always true. They could have weaknesses that an attacker could exploit. But when we design protocols, we usually don't need to pick a particular encryption algorithm. We're going to assume that we have one that either is a symmetric or an asymmetric encryption function that has the properties it should have. And we're going to assume the attacker is active-- that they can intercept messages, they can try to analyze what they hear, but they can also modify messages and replay messages and do anything they could do to disrupt the protocol by changing things on the network. So that's the main threat model that our protocols are designed to resist. So what we're going to do the rest of this unit is look at 3 particular protocols. We're going to look at the encrypted key exchange protocol, we're going to look at SSH for setting up a secure shell, and we'll look at TLS, which is the most widely used cryptographic protocol today. That's the protocol that's used every time someone connects to a secure website using HTTPS. All these have some things in common. They all have the goal of authenticating a client and a server. This could be in either direction or in both directions. It could be to allow a client to know that they're talking to the right server. It could be to allow a server to know that they're talking to the right client. It could be to mutually authenticate both parties. And they all involve a mix of asymmetric and symmetric techniques. What I hope you get from seeing the 3 different protocols is a sense of some of the things that are the same-- that there are techniques that all the protocols have in common at some level, but there are also differences between the protocols, the things that led to those decisions and why the protocols have different properties. We'll start by talking about encrypted key exchange. The idea for this protocol comes from Steve Bellovin and Michael Merritt back in 1992. There have been many, many variations on this protocol, many of which are still in use today. This protocol starts from Diffie-Hellman, which you should remember from Unit 3, where each party starts, they have agreed on the generator and some modulus. The client picks a random XA value, raises g to that power, sends the result to the server. The server picks a random XB value, raises g to that power, and sends the result to the client. They can both now compute the same key by raising the value they received to the power of their random value. The problem with Diffie-Hellman for authentication is if there's an active attacker, that attacker can change the values, send something different, set up a shared key independently with each party, and act as a middle attacker intercepting all the traffic because the attacker knows the keys on these 2 separate channels. The idea of encrypted key exchange is to combine this with symmetric encryption to allow the client and server to authenticate each other even if there is a middle attacker. Here's how it works. I assume there's some password that at the beginning of the protocol is known to both the client and the server. Then the way this protocol works, the message that the client will send to the server instead of just being the message that would be sent in Diffie-Hellman is now that message encrypted using the password. So this is symmetric encryption using p, the message that would have been the message in the Diffie-Hellman protocol. And in addition to that, we also need to send the name of the client. So let's say that's Alice. When the server receives this, it can decrypt it and knows this password, and so it obtains the value that would have been sent in the Diffie-Hellman protocol. That means it can compute a key using its secret value XB, combining that with the decrypted result here. As in the Diffie-Hellman protocol, it also sends a message back to the client. In Diffie-Hellman that would just be g to the XB mod q where XB is the secret value selected by the server. Now we're going to combine that with symmetric encryption, encrypting that with the password. Alice decrypts that message, so she can also obtain the value here and obtain the key using the normal Diffie-Hellman technique. So now we'll have a quiz to see if you understand the EKE protocol. The question is, which of these are drawbacks of using the protocol as we've described it here to authenticate a user--that would be Alice on this side-- to a website--that would be the server on this side. The first possibility is that it's vulnerable to offline dictionary attacks. This is where an attacker goes through a dictionary of common passwords trying to guess the password of a user. The second choice is that it requires servers to store the passwords in clear text. And as we discussed in Unit 2, this is never a good idea. The third choice is it's vulnerable to meet in-the-middle attacks just like the Diffie-Hellman protocol was. The answer is only the second one is true. The first answer is not true. In fact, the EKE protocol is designed primarily with this goal of not being vulnerable to offline dictionary attacks. The reason for that is if you look at the 2 messages that are sent, the value that's encrypted with a password is the result of a Diffie-Hellman exponentiation. If we believe the Diffie-Hellman assumption, that's indistinguishable from random. An attacker who intercepts these messages could try all possible passwords in the dictionary. Even if one of those is correct, the attacker couldn't tell which. The attacker obtains the encrypted messages, which are these Diffie-Hellman exponentials. From those there's no way to obtain the key, so there's no way to know if the guess was correct, and it should be indistinguishable from any other guess. So that's a good property. It also satisfies this other property that it is actually not vulnerable to the meet-in-the-middle attack that Diffie-Hellman was vulnerable to because in this case if the attacker tries to relay something without knowing the password, that will lead to some other key being computed not known to the attacker, and the attacker won't be able to set up the middle attack. The main drawback of this protocol is it does require the server to keep the password. In order to decrypt this message encrypted with a password, the server needs to use the password to do decryption. So this means the server would need to store all the passwords in clear text. This is highly undesirable. If the server was compromised, all the passwords would be lost. I have another question about this protocol. The way that I've described it here, does it actually provide authentication? The possible answers are no, neither party is authenticated to the other one; yes, that it authenticates the client to the server but not the server to the client; yes, it authenticates the server to the client but not the client to the server; or yes, it authenticates both parties to each other. The answer is actually no. We haven't done enough yet to provide authentication. And as I've described it, it's not the full EKE protocol. The reason that it's not is the way to authenticate depends on proving that you have the same key. To authenticate, what Alice needs to prove to the server is that it knows the password, p, and the server needs to prove to Alice that it knows that password, p. Our assumption is that the password is shared between the client and the server-- in this case, Alice and the server-- and the way to authenticate is to prove knowledge of that password. In this case, that's not done--that we've just established a key. That doesn't prove anything. This message could have been anything. The fact that Alice can decrypt it using the password doesn't prove that Bob knew the password. To do that, we need to add an extra challenge to this. What we want to prove is that both parties obtained the same key. For the server to obtain the key, it needed to be able to decrypt this message using the password. And so what we're going to add to the message is instead of just sending this value, we're going to also send a challenge, and that challenge is going to be encrypted using the key. The challenge is just some random value selected by the server. And now Alice needs to be able to obtain the right key. That proves that she knew the password. And using that key, she can decrypt this message and obtain r. This effectively demonstrates to Alice that she's talking to the right server-- at least a server that knew her password-- because the key that's produced here could only be produced correctly if the server was able to decrypt the message that Alice sent encrypted with that password. This hasn't yet proven anything to the server. So to finish the protocol, Alice has to send a response back to the server that proves that she was able to obtain r. So Alice could do this by proving that she learned r. She could send the message encrypting with the key r back to the server. Does this work? The answer is no, this doesn't quite work. The reason it doesn't work is this message encrypting r with a key, that's exactly what the server sent. So this certainly doesn't prove knowledge of k because this could just be reflected back to the server. So we need to do something a little different. Instead of encrypting just r, we need to encrypt something else that proves knowledge of k and r but couldn't be generated just by replaying this. And so one way to do that would be to add another nonce. We're going to concatenate a new nonce here and encrypt the concatenation of those 2. The server can decrypt this, extract r, check that it matches, and extract that value, and the server could send that value back encrypted with k. At this stage both the server and the client have proved knowledge of the password, they've established a shared secret that they can use for further communication, and they've done this in a way so that even if there's an active attacker intercepting and modifying all these messages, if the attacker doesn't know the password, p, they can't trick the client and server into believing that they're talking with each other or establishing a key that's different from the one that they would establish. The next protocol I'm going to talk about is SSH, which is the secure shell protocol. The problem SSH is trying to solve is also a client server authentication problem. We have a client who wants to log into the server, and to log into the server the server asks for the password, and the client enters it. What we want to know is that when clients are connecting to the server that they're connecting to the server they think they are and that an attacker who can intercept and modify this traffic won't be able to trick the user into giving its password to a different server or thinking it's interacting with a different server. This protocol is also based on Diffie-Hellman and uses aspects of both symmetric and asymmetric cryptography. The first step is very similar to Diffie-Hellman. In fact, it's the same. The client picks a large random number and sends to the server g to that power mod p. G is the generator and p is the modulus just like in Diffie-Hellman. The server picks its own large random value--we'll call that xs-- and computes ys, which is g raised to that power. So far this is the same as the Diffie-Hellman protocol. Then the server computes the key, which is yc--the value transmitted here-- raised to the xs power. So far this is all the same as what's done in Diffie-Hellman. We just changed the names of the variables to match the client and server. The next step is where things get interesting. What the server will compute is a hash. We'll use some cryptographic has function. The input to the hash function--first there will be some parameters that identify the protocol. Those will be concatenated with the public key of the server. We're assuming at the start of the protocol the server has some public-private key pair. What will be included in this hash is that public key--public key of the server, the value of yc that was sent by the client-- that verifies that it's part of the same session and prevents replay attacks, because now that value was determined by the client, the value of ys--this is the normal Diffie-Hellman response, and the key. Note that this is all in a one-way hash. Someone who intercepts that hash won't be able to learn anything about the inputs. Someone who knows the inputs would be able to verify that the hash is correct. So what the server sends is the value of its public key, the value of ys, and the hash signed with the servers private key. We finger that as sending the hash along with the hash encrypted with the private key. That's what it means to do a signature in an asymmetric cryptosystem. For this quiz, I want to see that you understand the SSH protocol. At this stage, the client and the server have a shared key. The question is what should the client do to verify that the server is indeed the server that the client expects it is talking with. Check all of the choices that are necessary to verify this. The answer is the first two. The first one is checking the signature. We're checking the signature here by decrypting using the public key that verifies that this message was created by someone who knows the private key so we verify the signature, and you might think that would be enough. We verify the signature, we obtain the public key, and we obtain the value that we need to produce our Diffie-Hellman key. The reason that's not enough is because we're also worried about replay attacks, and that's why we need to do the second check. We're going to recompute this hash. Note that the hash is one way. We can't use the hash to learn the key, but we can compute the key like this for the client and then check that the key and the hash matches that. By computing the hash, we know that there is no replay attack because these values are fresh. If there was a replay attack and a different hash value was replayed, then this hash wouldn't match, so we need both of those. The third check doesn't make sense, and this uses Yc as well as Xs, which is the secret value of the server. If that was revealed, that would break the protocol. The next question is how well does this is actually work for providing our goal of authenticating the server to the client in cases where the client does not know this public key value before the protocol. We note that in this step the server sends the public key to the client, and that's the key that is used to verify the signature. Here are the possible answers. Think about what we will actually get from this protocol. The best answer is that it does not provide any authentication. It does establish a shared key, but it's a shared key with some unknown entity. It's may be really providing no benefit, but I would say that at least establishing the shared key means now you've got a channel with some entity that's encrypted, and if someone is eavesdropping on that channel-- you're only worried about a passive adversary-- they might be able to eavesdrop on that channel, but you do have a secure key established. You don't know that it's with the right server. If you there's an active adversary, this definitely provides no benefit, because the active adversary can do the attack in the middle, impersonate the server, establish the key, and that's the one who has the shared key instead of the server that you intend to talk to. If it's a passive adversary, that assumes the message actually does get to the right place, so it provides some security against the passive adversary, who can't intercept or inject new messages. It allows you to establish that key, but doesn't provide any authentication. I think the 3rd answer is the best. One could make arguments for either the 2nd or the 4th. Don't be upset if you had good arguments for those, and I should emphasize that this establishes a secured shared key only against a passive adversary even without worrying about the authentication since an active adversary can interfere with the key that is established. For this protocol to provide a meaningful benefit, it's necessary that the client actually knows this KUs value or has a way to check it before accepting that this is the server that she intends to talk with. Let me show you how that actually works when we use SSH. Here I'm using SSH to log into udacity.com. What I see is a message that says the authenticity of the host can't be established. That means I don't already know a public key for the host. We can see because I don't already have a public key for udacity.com-- that it cannot establish the authenticity of the host-- it's showing me this finger print, which is a way for it to be more easy for a human to view the key, rather than seeing all the bites in the key. If I was very paranoid and careful, what I would do is call up an administrator that I trust at Udacity and say is this the finger print for our public key. If I'm less paranoid or more impatient, then I could connect, and once I connect that public key is going to be stored. It's added to the list of hosts that I have. I'm not going to actually continue connecting, but now I have a secure channel encrypted with some key that's been agreed to using the SSH protocol to send my password to the Udacity host knowing that it couldn't be intercepted. But this only has value if I really knew that this was the public key. Otherwise, I could be talking to anyone. What's happened as a result of this is now that key has been added to my collection of keys. Now if I look in the file. ssh/known_hosts, I'll see that I have udacity.com here. What's been stored is the IP address I was talking to as well as this is the public key that's stored for udacity.com. If I try it again, you'll see that it does connect. It has a secure channel now. It's verified it by checking that the public key that the server sent matched this one. What's more fun is if I modify this file. I'm going to make a little change to the public key. Let's change that 4 to a 3. Now let's try connecting again. What happens now is we get a very large yelling message with lots of capital letters, saying that the host identification has changed, that the host I'm trying to connect to sent back a public key that's different from the one I've stored. It means that someone could be doing something very nasty. In this case it means that I just modified the key file, and they're talking about possibly being eavesdropped on. It's really not the case that it would be eavesdropping. It must be an attack in the middle. This is a bit of poor wording in the error message. It's possible the key changed or something else happened. So they don't allow you to continue with the connection at this point. You could modify the file to create a new key, but it's worried enough that something has gone wrong that it won't let you connect. The next protocol I'm going to talk about is called TLS, which stands for Transport Layer Security. It's also sometimes referred to by it's old name, which was SSL, which was Secure Sockets Layer. This is the protocol that was invented by Netscape in 1995 with the goal of enabling clients, meaning web browsers and web servers, to be able to communicate securely. This is really the essential thing for creating ecommerce. It's the protocol that, among other things, allows people to send credit card numbers over the Internet as well as other personal information with some confidence that it's going only to the intended destination. It consists of two main parts. The first is the Handshake Protocol, and that's used to authenticate a server to a client. It can be in both directions. It can also be used to authenticate clients to servers. The way it's used on the web, this rarely happens. This would require clients to have public keys that would be known to the servers. The other outcomes of the Handshake Protocol are agreement on the cryptographic protocols to use. TLS is a very complex protocol that allows many different encryption algorithms to be used for different parts of it. Part of the handshake is to agree on which one is to use. And the final thing is the essential outcome for communication to establish a shared session key--a key shared between the server and the client. The second part of the TLS protocol is the TLS record protocol. That's done after the handshake to enable secured communication between the client and the server using the session key that they established from the handshake. The question for this quiz is which one of these two protocols is likely to involve asymmetric cryptography? I haven't described the protocols yet, but based on what they do, you should be able to figure out which one is likely to use asymmetric cryptography. The answer is the Handshake Protocol That needs to use asymmetric cryptography to do authentication. If we want to do authentication without starting from a shared key-- which, if were talking about a web browser and a client, we dont have-- that always require some kind of asymmetric cryptography. Thats what the Handshake Protocol uses. It uses a combination of asymmetric and symmetric crypto. The record protocol only uses symmetric encryption. The reason for that is that weve established a session key by the end of the handshake. Now we can use symmetric encryption which is much faster for encrypting all the content of pages. First we're going to look at a protocol that's not quite TLS. This'll give you a basic idea of how TLS works, but there is some vulnerability in the protocol, which we'll talk about fixing next. Here's our client, which is typically a web browser, and a our server, which is a web server. The first step in the protocol is the client connects to the server, sending a message "hello" --without the exclamation point. It also sends information about what ciphers it is able to use. Different versions of browsers will have different ciphers implemented. The server and the client need to agree on a particular server The client will also send a list of the ciphers and the hash functions that it has implemented. Different browsers will have different ciphers implemented. It's up to the client and server to agree on the one to use. In the second step, the server responds. That response includes several things. It will pick the cipher and the hash function to use. Those'll be selected from the list that the client sent based on the ones the server can use. It should pick the strongest ones that are acceptable to both. It also sends a certificate. What that certificate is is something that gives the public key of the server to the client in a way that the client can trust it. What's in the certificate is the domain, the name of the server, as well as its public key. There is some other information like expiration times--these certificates expire. The important thing about the certificate is that it's signed by a certificate authority. We'll talk more about certificates later, but the important thing the certificate does is give the client access to the servers public key in a way that the client can trust it. The next step is for the client to verify the certificate. Since the certificate is signed by the private key of some certificate authority, that means the client needs the corresponding public key to verify the signature. The client also extracts the public key from that certificate. The next thing the client will do is select a random number-- some random value that will be used to generate the session key. In the third step the client wants to send back that random value to the server, but in a way that's secure. Let's make that a quiz to see if you can figure out how to do that. The question is how should the client securely send the random value to the server. Check the best answer from the list below. The answer is the third choice--the client should send the random value encrypted using the server's public key. Both of the first two options assume that the client and server already share a symmetric key. If this was the case, they wouldnt need this whole protocol. They could use that symmetric key right away to start communicating, and the fact they are able to communicate using that key verifies that they have authenticated each other. This solution does not work well for web applications. There's no way for me to easily establish a shared symmetric key with Amazon before the protocol begins. The fourth choice would be useful for integrity. If the server knew the clients public key, then it could verify that this random value came from that client, but this would expose the random value to anyone else who intercepted the message and also knows the client's public key, and it provides no authentication that the server is who the client expects. The right answer is the third one--we want to send the random value encrypted so it cant be intercepted, but that it can be decrypted by the server, and the server has the corresponding private key to decrypt this. Now, once the server receives this message, it can decrypt it using its private key. We'll use the value of r as the session key. After this, both the client and server have the shared session key and can communicate. Now, we have a secured channel between the client and the server encrypted with the symmetric key k. Now I want to ask a few questions about this protocol to see that you can understand some of the things that could go wrong with this simplified version of TLS. Here are some possibilities and keep in mind the threat model that we introduced earlier. We're assuming an active attacker with limited computational power. Check all the things that we should be worried about going wrong the way the protocol is specified here. I think the best answer is the first two but not the third. Whether the first could happen depends on what we mean by hijacking a session. An attacker in the middle could certainly replace this with some other message-- instead of sending the r value selected by the client, sending some other value-- and that would make the server pick some different key, and then the messages that the server sends would be encrypted with that key. That means the attacker could pick in our prime value that the attacker knows and decrypt the messages from the server. It wouldnt allow the attacker to hijack the entire session though, because the client messages would still be ecrypted with k, not known to the attacker based on this r value, and the attacker doesnt have the way to decrypt this, just the way to change the key the server picks, but thats a pretty serious problem if the server is sending messages that the attacker can intercept and decrypt. This is a problem, and what wed like to do is to change it so the key incorporates more values that couldnt be interfered with by the attacker. Well talk about how to do that next. First, let me talk about the other answer that is true. The attacker could easily force the client and server to use a different cipher. In step 2, the server responds with the selected cipher and hash. This is an non-encrypted message. An attacker who intercepts it could change the ciphers to something weaker. If the client supports the weaker cipher, the client has no way to know that the server supports a stronger cipher and will settle for the weaker one selected by the attacker. Well look at how to fix both of these problems next. The challenge is we need to fix them where these steps in the protocol dont have a shared secret yet. We need to agree on the cipher and the hash function before we actually establish the shared key. These are two pretty fundamental problems. That's not going to work, so we need to make some improvements here. The way to solve this problem is to include more randomness in the key here, so an attacker who can replace this won't be able to trick the parties into agreeing to a different key, and then they can verify that they've agreed on the same key. What we're going to do is add extra random nonsense to these steps. In addition to the messages, the client will generate a random value--we'll call that rc-- that will be added to this message. The server will generate a random value rs that will be added to this message. This value will still be created. This is what's call the premaster secret. What we're going to send here is going to include a little extra information. It's still going to include r--the premaster secret. We need to share that with the server. We're going to pad this. This will make search attacks more difficult, prevent some of the weaknesses. If this is a message, happens to be a small number. We're going to use the PKCS protocol that we talked about in Unit 4 to use RSA more securely. If this were some other cipher, we could use a different way of padding it. We're also going to add to r something about the client version, and this will be useful for verifying the previous steps have not been tampered with. Instead of making the key just r, the key is going to combine all the randomness that we've used so far. The way we'll compute the key, which is now called the master secret, is by using a pseudo-random function--this is like a hash function-- where what it takes as inputs are the premaster secret--that's r-- and it also takes a label, which is just identifying this as the master secret, which gets combined with the value of rc--the client's randomness--and the value of rs-- the servers randomness. What H is is a way of using a combination of hash functions where it takes a key as well as the value that you're hashing. What it produces is 384 bits, which gets divided into three 128-bit keys. One of these will be used as the key for symmetric encryption, and that's using an algorithm like RC4, typically. One of these will become the IV, and that's the initialization vector which we need for CBC-- cipher block chaining mode--that we'll use. We'll talk about the protocol, how it uses that, later. The other one would be a key that's used for a keyed hash. We've previously talked about hash functions that didn't use a key. The important property of the hash function is it gives us a one-way mapping. A keyed hash function--that mapping depends on the key. One way to think about that, which is not quite the way these work, but it's really concatenating that key with the value we're hashing. If it's a strong cryptographic hash function, that should be effectively equivalent to a keyed hash function. This change means if an active attacker can interfere with this message, they still can't control what gets computed here. This depends on the randomness used in the previous two steps. What happens before the channel is used for anything secure is the client and server need to verify that they got the same key. We add a step here to finish the handshake. The way that step works is we encrypt the finished message, using the key extracted from the master secret. That means if any of these values were tampered with that key would be different, the handshake would never be finished, and there would never be any secure communication sent using that key, because both parties verify the handshake before continuing. This fixes the first problem. Now there's no way for the middle attacker to hijack the session. If they tamper with this message, it will be detected when the handshake finish is done using a key that depended on all these values. Depending on the random values here also prevents replay attacks. Since new random values are used for each protocol execution, there is no way to replay it. The key depends on these values that were used previously. The second problem--the decision about what ciphers and hash functions to use--is based on these clear text messages. An attacker who tampers with these can make the client and server use some very weak cipher and then be able to break messages encrypted without cipher. This was really a problem because US Export Control Law is meant that some of these ciphers were very weak. It was necessary to support ciphers that use only a 40-bit key to satisfy US laws. That meant the encryption was weak enough that it could easily be broken by brute force. The solution to this is to make sure that these messages are also authenticated. That cant be done at the beginning because there is no key established yet, but that can be done at later steps. Let's recap the TLS Handshake. There's an important part of it that we haven't focused on yet. The first step is the hello from the client to server. The second step, the server responds--both of these messages include new randomness, and part of this response is the certificate. In the third step, the client sends a new random value encrypted with the server's public key. At the end of this process, the client and the server have agreed on the master secret, which is used to derive the keys for communication. So the key question and the goal of the protocol is for the client to know that it's communicating with the correct server, and that that master secret is only shared with the server it intends to communicate with. The question is how can the client trust that it's communicating with the server it intends to communicate with. Here are the choices, select the best answer, and I should warn you this is a bit of an unfair question since we haven't talked about this yet, but I think you'll know enough to figure out the answer based on what you've seen so far. The correct answer is the third choice. What the client needs to do to know that it's talking with the correct server-- it needs to know that it's talking with server S and knows that that's a public key owned by S. If that's correct, then S owns the corresponding private key and only that server can decrypt this message and obtain the right session key. This is the problem the certificate is designed to solve. Number one would work if we had a way to always know the public key beforehand. That would be great. We wouldn't need any other solution. This is not going to work for websites. This would only work if we could pre-load the public key of all the websites we might ever communicate with into the browser. That's not realistic. We need some other way of getting new public keys for new sites as we visit them. Verifying the certificate using KUS doesn't make any sense, because KUS was provided by the server, so if we use the server's public key to verify the certificate, that would be a self-signed certificate. It wouldn't prove anything since the signature is being verified with the key provided by the person claiming the signature. That's why we need the third solution, which uses some other key that the client already trusts to verify the certificate and then the information in the certificate to know that it's the right server and to know the server's public key. Before looking at how that works, I want to demonstrate what happens when we connect using TLS in the browser. Different browsers deal with TLS in different ways. If we connect to https://www.google.com that's using SSL to connect, we can see in the address bar it's indicating that it's a secured connection. If we click on that, we can see more information about that. We are connected to the site google.com, unknown who it's run by, and it is verified by a certificate provided by Thawte Consulting Ltd, and we can click on this to get more information about that key. This has the information about the certificate, and we can click on the certificate to see what it was. It says our communication between the sites is now being encrypted. As the result of the Handshake Protocol, the client and the server have agreed on the key, and now all the messages between our client and the server are encrypted using that key. If we look at the certificate, we'll see more information about how that was established, and we have an SSL server certificate. It indicates who it was issued to, so it was issued to www.google.com. That matches the name of the site we connected to, and it explains who issued the certificate, and it has fingerprints for the certificate. We can also see that it has an expiration date. Certificates don't last forever. That's an important property. If the certificate lasted forever, we'd have to have a very strong encryption on it. By making it expire, the key license needed for the certificate can be predicted based on expected computing power until the expiration time. We can see more if we look in the details. This is where we get into the real issue of how the certificates work-- that there is a chain of authorities that validate the certificate. We'll talk about that more soon and then look at this again. What we need in a certificate is something that communicates the public key of the server to the client in a way that the client can trust. Here's one way to do that. We'll make the certificate include the name of the domain and its public key. Let's assume we're using RSA, so this would be an RSA key. That's the public key, and that means that the server knows the corresponding private key. This is encrypted using a private key of some certificate authority that the client has to trust. The question is how should the client verify the certificate? Here are the possible answers. Select the best one. The answer is the second one. The first one doesn't make sense. Since there is nothing to match KUs against, this is what the certificate is protecting. It's ensuring that the client learns the right public key for the server. The second one is our goal. We want to know that the certificate matches the domain that we're connecting with. Then we can trust that that's actually a public key associated with that domain. The other two could possibly work, but they would require an extra round of communication. They'd require the certificate authority being online. If we could obtain the key directly from the certificate authority, we wouldn't need this step. We'd talk to them in the first place. The appeal of sending the certificate is we don't need the certificate authority to be online or add an extra communication to another place when we connect to the server. The server can store it's own certificate and send it to clients when they need it. Now let's look more closely on what's actually in a certificate, and we can double click in our browser, depending on the browser. You're either clicking on the key or clicking on the name, and we can see that our certificate is telling us we're connected to google.com, and the connection is encrypted. When we look at more information, we can see more about the certificate, and we can view the certificate. Let's look at the details of the certificate, and we can see many things here. We'll look at several of them. The first is the hierarchy. We'll talk about that later. That's what give us confidence in the certificate. Let's look more closely what's in the certificate, and I've zoomed the window to make it easier to see. The fields of the certificate are the contents, so you can see it's got a version. It's got an issuer. Issuer indicates who issued the certificate. In this case, Thawte, which is now owned by VeriSign. It's got an expiration time. It's got the time it was valid from until the time it was valid to. Then it has the subject, which gives us the name of the owner of the certificate, which is google.com, and then we can see the public key information. It says the algorithm of the public key is, which is using RSA Encryption, and we talked about PKCS in unit 4 That's the way of padding to provide security when using RSA, and we can see the public key. We can see the modulus is 1024 bits. That's long enough to be secure with the recommended RSA key sizes today, and then we can see the exponent, and we see that it's a small exponent. This is a popular exponent. Many keys use this. And we talked about, when we look at RSA, that it's okay that the public key has a small exponent. What we need is to know that the exponent for the secret key is hard to determine, so that must be a large number, but the public key can have a small exponent. We've seen some parts of the certificate. Just to make sure everyone is paying attention, have we seen anything convincing yet in what we've seen of the certificate so far? The answer is no. Everything we've seen so far any attacker could have generated. If our communication was actually be intercepted, an attacker could generate a certificate that says all of these things. But instead of having Google's real public key, we have a public key generated by the attacker. Just looking at this provides no value yet. What we need is a signature. That's the important part of the key. We can see that this certificate was signed using a signature algorithm. This is PKCS#1 using RSA encryption-- that's the padding scheme we talked about in Unit 4--and SHA-1 is the hash function. Then we can see the value of the certificate, and this is it. We have 1024 bits, and this is what we need to use to convince ourselves that this is a valid certificate. Of course, this is actually work that the browsers is doing. We're really trusting the browser to implement this checking correctly. What is in this signature? It is the result of encrypting the hash of the certificate contents, using the private key of the issuer. The signature algorithm tells us what we're using for the encryption and the hashing functions. Both of these are popular but perhaps somewhat questionable choices. SHA-1 is known to have some weaknesses that may make it possible for attackers to find hash collisions. For this quiz, I want you to decide whether or not the certificate contents that is used as the input to the hash here should include the signature itself. The answer is, no, it's not possible to include the signature in the hash itself. The reason for this is if that was included, we don't know what it is until we've computed this. After we compute this, well, if we computed it again with the new result, That would change it every time. Unless we had a very weak hash function and a very weak encryption function, there is no way to sensibly include the signature in these contents, but the good news is we don't need to. The reason we don't need to is this is what we're using to verify the rest of the certificate. The important part is that it's encrypted using the private key of the issuer. In order to trust the certificate, the client needs to validate that signature. To do that it needs to know the corresponding public key. It needs to know that public key that can be used to decrypt this message. If it decrypts it, it can check that this hash value is the same as what it computes when it hashes the certificate content itself. That is the real problem that public key infrastructure needs to solve. We need ways of distributing these public keys. That's called public key infrastructure, also known as PKI. We need a way to securely know the public key of the issuer. Once we know that we can use the certificate to learn the public key of the website we visited. How do we know that public key? Let's look back at the certificate that we got from Google. What we see at the top of it is the certificate hierarchy. We see that we have the google.com certificate, and that's the one we looked at and saw that it had this public key. That was signed by an issuer, and that issuer was Thawte, and we can click on that. Now we can see the certificate from Thawte that was used to sign this certificate. We have this certificate from Thawte. It was issued by VeriSign. We can check that it's valid. It's valid until 2014. That certificate also has a public key. It's got a subject identifying Thawte Consulting that generated the key. It's got a public key. It's also got an RSA key with PKCS. We can see that public key. That's the public key that we can use to validate the certificate that RSA provided. We can use that to decrypt the signed hash to validate that certificate. This would only be useful if we knew that we could trust this public key. How do we trust this public key? Well, it's got a certificate. It's certificate was issued by VeriSign. We can find VeriSign's public key to verify this one. That's the top of the certificate hierarchy here. We have a certificate from VeriSign, and if we look at that one-- well, it came from VeriSign, and it's got a public key, also an RSA key and this one. You'll notice that all of them use 65,537 as their exponent. The moduli are all different. If they weren't different, that would mean they were all using the same public key, which would be a pretty serious problem. If we look at this key, we can see that it's expiration date is all the way up to 2028. It goes back to 1996. This is a very long-lived key. This is the key that is the root of our certificate hierarchy. Here's what we have. We have a certificate that was sent by Google. It was signed using a private key owned by Thawte Consulting. To validate that we need the corresponding public key, which we get from a certificate that was signed by VeriSign. To verify that, we need the public key for VeriSign. We have that from a certificate, which says it's VeriSign's. How can we trust VeriSign's certificate or do we have to keep going on forever? I'm going to claim that the answer is the third one. You could certainly argue that there is a better case for the first one, but that would be pretty disappointing. The reason that we trust this key is not because it's signed by VeriSign. In fact, it actually is signed by VeriSign. When we look at that certificate, we see that it is signed. It was issued by VeriSign, and it does have a certificate signature. We could verify it against this public key, but that would be verifying it against the same key that the certificate describes. That's what called a self-signed certificate. This doesn't convince you of anything. Anyone could generate it like that. In fact, you can do that. You can generate one yourself. If you're using a Mac, you have open SSL built-in and use that to generate a self-signed certificate. But it doesn't prove anything. The best answer here is that we trust it because we got it from some trusted source, and we protected it and we know that it's valid. How that works in a web browser is there is a set of certificates that are built in. You can download more, but you have to be careful. If you download them, you need to know that they're secure. If you look at your browser settings, you can see all the certificates that are built in. There are actually quite a lot of them. Not all of these were always built in, so all the ones that are listed as built in were built in. They came with the browser, and we can see that there is one for VeriSign-- actually several for VeriSign--that were built in. These are trusted only because we think we got the browser from a trusted source, and it had these certificates built in. We can look at one of these certificates. You'll notice that their rated as different classes. What it means is mostly that someone was willing to pay more to VeriSign to get a higher certificate. If you pay more, they do a little more identity checking to say that you're probably the one who's asking for the certificate. But all the certificate proves is that VeriSign granted it to you and decided that the person that they granted a certificate that said "I'm google.com" probably had some affiliation with Google. The higher the class the more you have to pay and the more work VeriSign does to validate that. If we look at one of these certificates, we can see the root certificate that's built in. This is the root of trust. We're trusting this, because it came in the browser. You'll notice that these have quite long expiration dates. Since these are used to sign all these other certificates, if they expire, all the certificates they sign would break. That raises the question of what happens when there's a bad certificate. There have been several well-publicized incidents where VeriSign or other certificate authorities accidentally granted a certificate to someone who wasn't actually representing the company that they asked for a certificate from. Revocation lists are one way to deal with that. You can have a list of--well, these are signed certificates that aren't actually valid. If those are kept up to date, and the browser always checks them, then you could have a certificate be revoked even though it was signed. This is a pretty painful solution to this problem, though. It requires always updating this list. It means that the time between updates you're vulnerable if there is a bad certificate and requires a lot of extra work for the browser to validate a certificate. The best solution, of course, would be never to grant a certificate without validating that identity. This change can be arbitrarily long. As long as all the keys along the chain are consistent and chained together in the right way, as long as we start with a root that we trust already, we can validate certificates signed by any of the parties along the chain. We could, in theory, use Google to validate a new certificate and add another layer to the chain. What I want to do in the forums is encourage some open discussion, encourage you to find interesting certificate chains on the web, see what the longest one you can find is, and post those in the forum as well as any interesting things you find along these paths. Here's the essence of the TLS handshake. To recap, the client initiates with the server. We're leaving out all the details about selecting the ciphers and the extra randomness, which is important for security. The server sends back a certificate. The client validates the certificate using a chain of certificates going back to some root certificate that the client trusts, selects a random r and then encrypts that using the public key of the server. The server can decrypt it, because it knows the corresponding private key. Then they finish the handshake. Now the client and server can communicate using those shared symmetric keys. The protocol to do that is the TLS record protocol. I may have made the mis-impression earlier that once you've established symmetric keys communication is easy and there's nothing left to work out. That's not the case. We're going to look at the TLS protocol next, and indeed it does have some vulnerabilities. At the end of the TLS handshake protocol, the server and the client have agreed on three keys, which they've extracted from the master secret-- a key for a symmetric encryption function, an initialization vector for that encryption, and a key for a hash function. The goal now is to protect the traffic between the server and the client. This will be in the form of requests for webpages. A typical HTTP request is to get some webpage. Then the server will send a response. I should reiterate that TLS protects many kinds of traffic. The protocol is not specific to HTTP. What I'm really talking about here is HTTPS, but the record protocol is the same regardless of how we're actually requesting the traffic. So this response is the contents of some webpage, which can be quite long. This could be many kilobytes of data. We need a way to encrypt that response and send it back to the client. The responses will call M. We want both confidentiality and integrity checking, so we're also going to include a MAC, using the hash function, so that's going to be HMAC(M). This uses kh to key the hash function for HMAC. This is similar to what we talked about in Unit 2 where we're hashing the message, but because it's a keyed hash function, it depends on the key as well. Finally, we're going to have some padding to fill up the block size. Now we want to send this whole response--the result of concatenating all of these--over the secure channel. We'll call this R. The way this is done with the TLS record protocol is to use CBC-- that's cipher block chaining mode of operation and some encryption function. We talked about CBC mode in Unit 2. As a reminder, this is the way CBC works. There's an initialization vector. We break the message into blocks. We'll call those blocks m0, m1, up to m(m - 1). The first message block is XORed with initialization vector. The output of that is encrypted using a key. The key that's used is a key that resulted from the handshake protocol. This results in the first ciphertext block. That becomes the input to the next XOR. That's the way the first response is done, but in a session there might be multiple responses. When the next response is done, we don't want to do the whole handshake protocol again. What happens in the next response, whatever the next message block is-- that's going to be m'0--will be encrypted using CBC mode again that will produce the ciphertext block at the beginning of the next message. What we need is an IV here. We don't want to use the same IV again. That would make it clear if m0 equals m0'. The approach that is taken by TLS is to use the last ciphertext block of the previous message as the IV for the first block of the next message. This effectively makes it seem like it's one long message. It seems like this is a good idea. It's certainly better than reusing the original IV. It's better than having to do the handshake protocol all over again. The handshake is quite expensive. It involves asymmetric cryptographic operations, which are very expensive compared to the symmetric ones we're using here. This message does raise an interesting vulnerability. Let's see if you can figure out what it is using a quiz. The question is suppose an adversary intercepts the first message. The adversary learns all the encrypted blocks. It doesn't learn anything else. It doesn't know the k value of the IV value. The handshake worked fine. The question is whether the adversary can learn whether some block is equal to some guess for that block. To make this more concrete, suppose the blocks are all 8 bits. The adversary has intercepted all the ciphertext blocks, but I'll only tell you three of them. C3 was 10101010. C4 was 01010101. C(n-1)--that's the last ciphertext block--was 11110000, and what the adversary wants to learn is if the value of m4 was equal to all zeroes. To do this, the adversary can set the value of m0', figure out how to make the server give a particular response, and examine the first ciphertext block. The answer is 01011010. Let me explain why. Remember that we're using CBC mode encryption. The value of ciphertext block i is the result of encrypting, using the key. The value of ciphertext block i -1 XORed with message block i. For the first block of the second message, that's going to be c0'. It's the result of encrypting the value of the previous block, and this was the initialization vector for the first block. For this one, it's the last block in the previous message. That was the block we call c(n-1). That's XORed with message block 0. Here is the danger. The adversary actually knows this value. That means the adversary can pick a message value so the value of c0' reveals something. That means it has to be an input to the encryption. We're assuming the adversary can't break the encryption. But it has to be an input to the encryption the adversary already knows what the expected result is. The adversary knows the encryptions of all the blocks in the previous message, and we know from the way CBC worked, if we want to learn the value of m4-- m4 was encrypted in the original message using the value here. This was ciphertext 3--the previous ciphertext block-- and that was encrypted and it produces output--ciphertext block 4. That means if we can feed into this result, that means if we can make the value of this equal to the result of c3 XORed with m4--we don't know m4, but we can make it our guess for m4. We're going to XOR it with our guess for m*--m4. If that's the input to the encryption function, then we'll be able to look at the output and see if it matches c4. To make that the input, we have this IV, which is really the last block in the previous message. We can just XOR that out. What we're going to pass in is the guess XORed with the known value of c3 XORed with this value. With the values I've chosen, c3 is this, m* is all zeros, so we don't need to worry about that in the XOR, and c(n-1)--the last ciphertext block--is all the 1s. That's going to flip the first 5 bits. That's why we get this value. Let's see what happens when that value is used for m0. What we're going to get for ciphertext block 0 is the result of encrypting the known value of ciphertext block n -1 with this result, which is c3. Now this is XORed so that same values cancel out. That means the result is the result of encrypting c3 XORed with m*. That's the same as we were doing here. That's encrypting m4 XORed with c3. We know what c3 is. We could construct this message to pass in. We don't know what m4 is, but we know if this result is the same, then this should be the same as ciphertext block 4. That would give us the answer to the question was m4 equal to m*. Of course, we can generalize this. If we can pick the value of m0' to be the XOR of the last ciphertext block and the one before the one we want to guess and our guess, that means what we learn if the ciphertext block that results from the new message is equal to ciphertext block i, then we know that block mi is equal to m*. How serious is this attack? If the block size is small we could guess all of these blocks. It turns out though the block size isn't that small. The parameters we saw were using RC4. RC4 is a symmetric encryption algorithm. It at least loosely stands for "Ron's Code 4." That's not what it officially stands for. But that's what most people think of it as, and this is Ron Rivest--the "R" in RSA who invented the symmetric encryption algorithm that's known as RC4. The block size typically used in TLS is 64 bits. TLS could also be using AES with a block size of 128 bits. That would mean the attack is only useful if the attacker can guess those 64 bits in a useful way and has a way to control the message. That seems like it might not be much of a threat, and this attack was actually known. It was identified by Gregory Bard in 2004 and viewed as not too serious of a problem with TLS, because it does give an attacker an advantage but these blocks are big enough that ability to do this--and if you can only guess one more message, if you can only control the server, you don't have too much power. It assumes a quite powerful attacker that has an ability to control the messages the server is going to send. It turns out that there are very serious implications of this attack. They depend on being able to inject JavaScript code, so there are ways that an attacker might be able to control the responses. Using JavaScript requests, if the attacker controls JavaScript on the page that the victim is using TLS to request, that JavaScript can make repeated requests to the root server. Those are still going to be part of the encrypted session, but the attacker can control what those requests are and perhaps can design requests that the server will respond to in a particular way, giving the attacker control over the first block in the next message. The other thing the attacker could do is figure out a way to know many of the bits. Suppose the attacker knows 58 bits, and there's one byte. But that byte contains one character of an authentication token that could be part of a cookie, say. It turns out that there is an attack that exploits this. It has the rather clever acronym B.E.A.S.T.--Browser Exploit Against SSL/TLS. This found ways to use this cryptographic weakness. It turns out if you can inject JavaScript into the page, you can control these requests enough to actually use this. This was demonstrated quite recently in 2011, showing a way that you could use this and this technique of controlling many of the bits in the first block to break an authentication token for a PayPal cookie. This is quite a powerful attacker. If you only need to guess 8 bits at a time, you expect to only need 128 guesses for each byte. I want to talk about one more vulnerability in TLS. That's an information leak. Now we'll assume that the encryption works fine. The handshake is good. All of these messages are encrypted. But the attacker can listen in on this channel. Assuming the attacker can't break any of the encryption but can observe the message on this channel, what does the attacker learn? The answer is many, many things that we should be worried about. The attacker might not learn the exact size, because of padding on the request up to the block size. The same thing for the responses. The attacker also learns the pattern. This reveals a lot about a webpage. Because of optimizations and HTTP, there are often multiple responses to one request. This could be because the large page is broken into many responses. It could be because there are many images on the page, and they respond, and although they would normally require separate requests, because of caching they could be sent before those requests. These patterns are very distinguishing. Different webpages will have different sizes of response as well as a different pattern. An example of where this is particularly dangerous in modern web applications--we're using HTTPS connected to Google. Now when I type a letter, I get a response. It's filling up the guesses of what I'm going to type next. The size of this response depends on the letter than I typed. If I type another letter, I get a different response. You can see that udacity is the most popular word starting with "ud," at least according to when I'm using Google. I don't know if that's true for everyone. But that size differs, and if I type another letter I get a different response. The size of these responses depend on the letters I'm typing. The length of these words will effect the size of the response. You can build a model that would identify what someone is typing based on the sizes of these responses. There is a paper by a group from Microsoft Research that shows some of these vulnerabilities and how much information that an attacker can learn from that. There's another paper by Peter Chapman and myself, and you may remember Peter if you took CS101. He was the TA for that. It looked at ways of measuring this and understanding how much information is really leaking. This is a serious attack. It's something that even though the encryption is there, that an attacker can learn a lot about what's going on on a webpage. This brings us to the conclusion of Unit 5. We've seen several examples of authentication protocols and focused on TLS as the most widely-used protocol that's used for every secure web session. The first thing I hope you take away from this is that designing cryptographic protocols is really hard. The details matter and the way things are implemented matters. TLS is a protocol that's built on SSL that's been around for over 17 years. It still has new vulnerabilities being discovered and people learning new ways to exploit them. The other key lesson is attacks get better, they never get worse, even if the initial attack seems to be only theoretical and not be easy to exploit, there are often ways that clever people will figure out ways to exploit those attacks. In Unit 6, we'll look at other things you can do with cryptographic protocols. This unit focused all on authentication and communication, but there are lots and lots of other things we can do with cryptography. Some of those are quite interesting, and I hope you'l be back for Unit 6 to learn about them. So welcome to Unit 6. In Unit 6 we're going to look at ways to use cryptography to solve a wide range of problems. In Unit 5 we focused all on using cryptography to solve the traditional problem of authentication and then communication with secrecy and integrity. What we're going to look at in Unit 6 is different kinds of problems we can solve using cryptography. The first thing we'll look at is Anonymous Communication, and we'll learn about how to use a chain of asymmetric encryption to enable two parties to communicate over a network without anyone knowing that they're even talking with each other. Then we'll look at how to do voting, and there are lots of issues with voting--we won't get into all of them-- but we'll look at the issue of can you provide an accurate tally-- know that each vote is counted without revealing who voted for whom. This will also be done using a chain of asymmetric encryptions but with some added features to ensure that the vote tally is correct. Then we'll look at ways to provide digital cash-- a way to represent and transfer value similar to paper cash that can be done all with cryptography. This will involve introducing some new techniques. We'll look at a centralized way of doing that which can be done using a new technique called blind signatures. And we'll also look at a decentralized way of doing that that doesn't require any trusted authority but uses proofs of work to create value. And this is what the bitcoin network does. This is just a small sampling of the kinds of things that you can do using cryptography. All of these can be used just using the tools we've seen in this class. And we'll see ways to use asymmetric cryptography in many different ways in these examples. We'll also see ways to use symmetric cryptography, including hash functions, as well as one-time pads. Last unit we talked about HTTPS-- how to use the transport layer security protocols to first do a handshake, to agree on our secret key, and then to have a secure channel between the client and server. What these messages are we just drew as arrows, but what they really are-- if we think Internet, well, their message is going through routers on the Internet. There may be many hops between the client and the server. Along these hops will go packets. Because they're using TLS, part of the packet is the encrypted message, but another part of the packet is the routing information. This is necessary so R1 knows the direction to send the message. It needs to know that the goal is to reach the server. Any eavesdropper who can see one of these messages, and it could be the first one from the client to the first router or maybe she is intercepting a message on some later routing path. She can learn that the client and the server are talking to each other. This is a form of traffic analysis where the important property we want to hide is not the contents of the message, which if HTTPS is working correctly, are encrypted and cannot be understood by Eve. But what we really want to hide is the fact that this client is talking to the server or that Alice is talking to Bob. The mere presence of communication between two parties is often enough to cause problems. An example would be if the client is a dissident in some oppressive country and is connecting to server, it's used by dissidents to communicate or a server in another country that's viewed as a threat. If we want to prevent this kind of traffic analysis, we want to prevent adversaries who can observe messages on the routers from being able to know which parties are communicating, we need to do something else. Let's think of a simple answer first. Suppose I want to send a message to Alice. If I want confidentiality, I can put that in an envelope, seal the envelope, and write that it's for Alice. To make sure my seal is good, I'll make sure that no one else can open this. I could give the message to Alice. Now, the problem is if Bob is jealous and the mere fact that I'm communicating with Alice would cause problems I can't just hand the message to Alice. That would show that we're communicating. What I'm going to do instead is use Coleen. I'm going to give the message to Coleen and ask Colleen to give the message to Alice. I will have a new envelope. I'll give this to Coleen. What I'll put in that envelope is the message I want to give to Alice. This also has a very secure seal, so no one else will open the message, and I give it to Coleen. If someone sees me giving a message to Coleen, well, there's no problem there. Coleen would open the message and see that it's got a message to give to Alice. Then she could give the message to Alice. This works a little bit, and Coleen could open the message and give it to Alice. How well does this scheme work? The question is which of these properties are necessary for this scheme that I just described to successfully hide the fact that I'm communicating with Alice? Requires complete trust in Coleen? It require that Bob cannot see when I talk with Coleen? It requires that Bob must not be able to see when Coleen talks with Alice? It requires that Bob must not be able to see both communications when I talk with Coleen and when Coleen talks with Alice? The answer is it requires me to completely trust Coleen. Coleen knows that I'm giving a message to her, and she can open that message and know that it goes to Alice. It doesn't require the first one. If Bob see me talking with Coleen, presumably that's not a problem. It doesn't require the second if Bob sees Coleen talking with Alice. That's also not a problem. If Bob sees both of those and sees them in quick succession, especially if he sees the envelopes being handed on, then he's got a good reason to suspect that I'm talking with Alice. So we need both of these properties. We need to know that Coleen can be trusted, and we need to know that Bob can't see both of these transactions. Those are pretty high security assumptions. We don't want to have to trust Coleen completely, and we don't want to rely on the fact that Bob can't see these two conversations. The way to make it more secure is to add a third person-- let's say we know Edgar as well. Now instead of giving the message directly to Coleen, what I'm going to do is take the envelope for Coleen, put that inside an envelope for Edgar. We need to fold things a little to make them fit. Now I'm going to seal this envelope with a very secure seal that only Edgar can open. Now I'll give this to envelope to Edgar. Edgar will open it, see that it has an message for Coleen. Edgar will give the message to Coleen. Coleen will open that, see that it has a message for Alice, give the message to Alice. Alice will open the message and see that she shouldn't eat the onion. This has more protection. Now Coleen doesn't know that I'm communicating with Alice. Because the message is going like this: It's going first to Edgar, then to Coleen, then to Alice. Neither Edgar, nor Coleen knows that I'm communicating with Alice. Edgar knows that I'm sending a message through him that goes to Coleen. Coleen knows that she's receiving a message from Edgar, and it's going to Alice. Neither one knows both endpoints. If we add enough hops, this can start to be quite secure. This is actually called onion routing. That's why the message is to not eat the onion. The name comes from the idea that an onion has all these layers. As you peel off the layers, you get deeper into the onion. Doing this with envelopes would be pretty painful. We want to do this with math. How do we do this with cryptography. Now Dave wants to communicate with Alice without anyone being able to know they're communicating, and we have a set of routers. These are like our friends in the example of the envelopes. Let's say I know about five routers. We'll call them R1, R2, R3, R4, and R5. The routers are all connected. They can all talk to each other. They can all talk to Alice, and they can all talk to me. We're assuming we have a fully connected network here. I haven't drawn all the connections, because that would get kind of messy. We'll assume we have a fully-connected network here, and we don't need to have a physically fully-connected network. There could be other nodes in between these. We're assuming we have secure channels between each of the routers, and we can set up those channels using TLS, so we have these secure channels between all of the routers and between all the participants, including myself and Alice. We'll select some random sequence of the routers. Each of these i values is one of the values that identifies a router, and I want to send a message into this network. Let's assume for now that the last router that I select is R4. I want it to be the case that R4 sends the final message to Alice that Alice can read. The question is what message should R4 receive? The answers are encrypted with Alice's public key. The message to Alice in clear concatenated with the message-- this is the message Dave wants to send to Alice encrypted using the public key of R4. Encrypted using the public key of R4, the message to Alice concatenated with the message encrypted with Alice's public key. Encrypted using the private key of Bob, the message to Alice concatenated with the message encrypted with the public key of R4. Or the message to Alice concatenated with the message encrypted with the public key of Alice all encrypted with the public key of Bob. The answer is the second one. R4 is receiving this. So it'd better be something encrypted with a key that R4 can decrypt. R4 should not be able to decrypt something encrypted with the public key of Alice. R4 could decrypt these two because R4 could know the public key of Bob, but then anyone could also decrypt these two. So we want it encrypted in a way that only R4 can open. That's like the envelope with a seal that only a particular person should be able to open. The content needs to be visible to R4 where to send the message next, which is what the To Alice part does, but invisible to R4 what the actual content is. Alice looks different from the other routers the way I'm writing in here. For this to be secure, there shouldn't be any real distinction between R4 and Alice. That's hard to get in a network like this. So it probably is the case that R4 actually learns that Alice is the endpoint, but because the message was received from the previous one in this chain doesn't learn that it was Bob talking with Alice. So the question is, what message should Bob send to R2--the first link in the chain? Here are the possible answers. Check the one that would be best. The third one is the correct answer. The first one wouldn't work. R2 couldn't decrypt this message and wouldn't know where to send the next hop. This one would work for the first hop. R2 would decrypt it. Note to send it on to R5, but would then when R5 receives it, it wouldn't be able to decrypt this message and know where to send it. It's also unnecessary to have this extra decryption using KUR2. This outer method has already encrypted using that public key. Anyone who can decrypt this message can also decrypt that one, so there's no value added by having this encryption. The third one is the best answer. Here we sending the message encrypted using R2's public key, so R2 can decrypt it. It has the next routing hop in plaintext in the message that R2 can decrypt. Then it has the rest of it encrypted using R5's public key, so R5 will be able to decrypt the next hop and learn that that should be sent on to R4. The problem with the fourth choice--it would work for this hop, but then when R5 decrypts it, it wouldn't know what the next hop is. The final option wouldn't work, because R2 would get the message to send to R5 and send the message here--which is M4. That's encrypted using R4's public key. R5 wouldn't know where to send the message. The only one that works is the third choice. In general, for onion routing to work, we need each message in the chain to be the message that's sent to the router Rik. It's going to be a message encrypted with that router's public key. It will have as its contents something explaining the next destination as well as the message that should go to that next destination. We can wrap as many layers as we want. The more layers there are, the more hops we go through, the less risk there is that all of the links in the chain can collude or that a party can observe all of the communication and learn what's being communicated. Now the question is suppose an eavesdropper can listen in on this link and can listen in on this link, so it has a way to listen on the outcoming links from me and the incoming links to Alice, can they determine that we're communicating? No. Yes, if there's no similar traffic on the network. Yes, if there is a way to correlate message 1 and message 2. And yes, if the adversary can see all the traffic on the entire network. I think the best answers are the second and third. No is hardly ever a good answer to a question about is there any possible way to break something. The way the adversary would learn that Dave and Alice are communicating is if there's a way to match up the messages at the end points. This would be the case if there was no other traffic that looks similar on the network. These messages aren't the same. Remember that they're encrypted in different ways. This message will not look similar to message 1, but they'll both be encrypted messages. They might be similar because of the length. There might be things the adversary can do to adjust the timing. Remember this is not a direct link, so the adversary-- if there are many messages going between Alice and Bob-- could introduce delays in the messages, and see the same pattern of delays on the upcoming messages. That would work if there weren't any unpredictable latencies inside the network. If there is any way for the adversary to correlate those incoming and outgoing messages, then the adversary would know these two parties are communicating. If the adversary can see all the traffic on the network, this would be a very powerful adversary. That's not enough unless they can correlate it. Even if an adversary could see all the messages coming into R2, if there are many other messages coming into R2 at the same time, and many other messages going out of R2, if the adversary can't connect the ones coming in with the ones coming out, then they wouldnt be able to follow the chain. In order for this to be secure, we need to know that there's a lot of other traffic on the network that looks similar. You'd like to know that between all of these routing points, there is always messages going. They look similar lengths, similar timings. There is no way or correlate the messages that come in to one router with the messages that go out from another one. One of the things that you need to do to make a network like this secure, is to make sure that there is enough traffic. You can inject a lot of fake traffic. That's expensive. You're wasting bandwidth on things that don't go. You've also got to be careful to make the fake traffic look like real traffic. You can't just send random packets. If there aren't correlations between those packets and if you're not sending a full flow, then that could be detected by the adversary. So there are lots of hard problems to solve to make this secure. There's a very successful project that provides this as a service on the Internet. It's called Tor. You can download it as an extension to your web browser. It provides a way to connect to a website without revealing to the website where you're connecting from. You need to get a response as well, and so that means in addition to selecting the route for reaching the website, you need a route for returning.o You don't want to include your IP address. That would reveal your location in the response, but you want to get the responses. What this does is select routes in both directions. It selects a random set of routers to reach the website that you want to communicate with as well as a way for that to send a response along another random path. So one more question about this which is an important question about whether or not this can work effectively. And the question is, how does a client who wants to use Tor to communicate anonymously know that set of routers as well as the public keys to select a path? In order to send messages, the first step is to select a random set of routers. So here are some possibilities. Select the one that you think would work best. So the closest thing to a correct answer here is to download the set of routers from a trusted directory. This has some pretty serious drawbacks though. The other choice is, well, asking Alice. If Alice actually knew everything, it could work. But not everyone can ask Alice. Using SSL certificates would be a good way to get the public keys starting from some root certificate authority to validate the public keys of the routers. And that could be used as part of it. That doesn't tell you what the routers are though. We need some way to identify the actual routers as well as their public keys. We could send messages to the random nodes. This would work if a large fraction of the nodes on the Internet were Tor routers. We could also require that every router implements Tor. This would be great, but it's not very practical. So what actually happens is that there's a list of known routers on the network that can be downloaded from a trusted directory. There's a big problem with this in that someone who wants to censor anonymous communication which a regime that someone is using Tor to get around might well want to do can also find these nodes, and the censor can download the trusted directory as well and block all traffic to those nodes. So this is a big challenge for networks like Tor to figure out ways to distribute the routers that is accessible and can be used by people that want to use it to communicate anonymously but isn't visible to a censor that would want to block all of those routers. There's no known right solution to this, but I would encourage an open discussion on the forum about ideas you might have how to solve this. The next topic I want to talk about is voting. Voting involves lots of cryptographic issues and there are many interesting security properties that a voting system should provide. One property that many voting systems desire to provide is anonymity that it shouldnt be possible for an adversary to know who someone voted for. The other property a voting system should have is that the count is verifiable. These are shorter than opposition. Verifying the count would be easy as each voter would be willing to publicly declare their vote and someone could look at all of those votes. But if you want to keep the votes anonymous and have a notion of private ballot, well then verifying the count becomes more challenging. Another property some voting systems want to provide is coercion-resistance. What this really means is that a voter can't prove who they voted for if votes are secret and there is no way to prove for whom one votes, well then, there is no way to coerce the voter into voting a particular way. And then we have all the usual goals of reliability and security. These things are hard to achieve. There are good ways to achieve them with paper ballots if we have trusted ballot boxes guarded by trusted election officials and if we have enough trusted third-parties, well maybe we can trust them to report the right tally. Were going to look at one interesting cryptographic way to do voting, but I want to emphasize that this is not a solution to the real problems that are necessary to running election. But it solves this issue of can you keep votes anonymous and still have a way to verify that the count is correct. Here's the idea: you collect the votes from the voters. Those are inputs to a random permutation. Let's say this is controlled by the Apple Party, and what the permutation does is randomly scramble the order of the votes. Now the position of the votes that came in doesn't match the position of the votes that came out. We'll also do some encryption, and we'll get to that in the next question part. Those are passed along to the next party. Let's call that the Blue Party. The Blue Party also scrambles the votes in some random permutation, and then those are passed along to the next one and we'll call this one the Crazy Party. Our parties start with the letters A, B, and C, and the Crazy Party does the same thing. It selects some other random permutation where it randomly selects a permutation, and it scrambles the positions of the votes. In order for this to work, we want at the end of this to know what the actual votes are. The question is what value should we use for x to enable this chain. Our security assumption is that the Apple, the Blue, and the Crazy Party all hate each other. They wouldn't possibly collude, so they can be trusted not to collude with the other parties. But otherwise they can't be trusted, they'll want to try to change the election, change the votes, and do everything they can. The question is which one of these would be the best choice for the xi values? Neither of these are the correct choice. Neither of these will work securely. We're going to have to do some other things, but which one of these is the best starting choice? The answer is the second one. We need to be able to unravel the layers of encryption as we pass the vote through the network. So that means we need the last step to be encrypted with the public key of C so C can decrypt and then get the actual vote. The previous step should be encrypted with the public key of V, and the first step should be encrypted with the public key of A. This idea is called the Mixnet. It was invented by David Chaum. Any youll notice that this is quite similar to onion routing and in fact onion routing was based on this idea. There is a couple issues we have to work out though. This kind of encryption won't work very well. There are some big problems with the way weve described it so far. So supposed that we do this and we encrypt this way, how well would this actually work? If we encrypt to vote as described where youre going to use the layered public key encryption and the vote is the name of the party, the voter wants to vote for and you can assume that the encryption is done using RSA where the public keys are well-known to everyone, the private keys are kept secret to their correct owners. And there are no vulnerabilities in RSA. The answer is it doesnt work very well. So the reason for this is that anyone observing these messages given that there is only a small set while you can compute this value given the public keys of C, B and A, you can compute the value of Xi for the three possible votes, match that up to the incoming votes, know exactly what they are so there is no anonymity for the voters, so either A or an eavesdropper could do this. The second choice is a little more ambiguous. See you could output any votes at once, there is nothing that verifies the votes that C outputs. But in this case because the encryption is broken, someone looking at that votes that come in knows what the votes are already and knows if C cheats but theres no point to the Mixnet if this is what we do. So we need to do something different; this by itself is not going to work. What we need to do is add some randomization to our encryption. So the first idea to solve this is just to replace the vote with the vote concatenated with some random value selected by the voter and kept secret. So the question is, does this work? And we're only worried about protecting the voter's identity here. We're not yet considering the problem of can C fabricate votes at the end. So the answers are: Yes. Only if the eavesdropper cannot intercept messages on both this link and this link, only if the interceptor can't intercept all the messages along the whole chain, and only if the eavesdropper doesn't collude with C. The answer is the fourth choice. That this works at least against the attack that was described as long as the random value is kept secret. If C decrypts the message while C has the private key here also, well learn the random value and can use that in collaboration with eavesdropper who heard these messages now to figure out which voter voted for which candidate. So this is a weak system. So this solution doesnt work at least not it for a threat model includes an eavesdropper colluding with party C or C having the ability to eavesdrop on this link. So this isn't good enough, but we can carry that solution through and add random nonces to each of these layers, so that's what we're going to do. The inner message will be the vote concatenated with a random nonce encrypted with C's public key. The next layer will be that message concatenated with a new nonce and encrypted with B's public key. And the outer layer will be encrypted with A's public key will be this message concatenated with the third nonce. So this is the message the voter sends with that voter's vote. Party A will decrypt the outer layer, randomly permute, and send the result to B and will decrypt B's layer, randomly permute, and send the result to C and will decrypt the final vote. And the first step to validating votes instead of just publishing the v values, we're going to require that C also publish the r0 values. So the final tally will be a list of votes and their associated nonces. This means the voter can check that the nonce that the voter used is in that list. So if we use this protocol, which of the three parties-- Apple, Blue, or Crazy - would you rather be? It looks like Crazy definitely has the most power here. Crazy publishes the final votes. Crazy could decide not to include votes that aren't for the rate selection. Crazy could tamper with the votes. The one validation we have is the voters can check that their vote is in that list. So Crazy can't add extra votes if the number of votes in the beginning is known. Crazy could replace some votes and change those votes. And there's no way with this scheme yet for a voter to prove that Crazy cheated. If the voter can prove that Crazy cheated, well then maybe it's not such an advantage to be Crazy. So is there a way for the voter to prove that their vote was not included? If what C publishes at the end as the tally is pairs of votes and the associated r0 nonces, how can a voter prove their vote is not included or was corrupted in the final tally? The answer is it could reveal all three of their nonces and show that they compute the correct xi value. Assuming the encryption function has the properties it should, it should be difficult to find r0, r1, and r2 values that encrypt in the same way with a different vote. Note that this requires the property of the encryption function that it's hard to do this by cheating by finding a different vote with different nonce values that encrypt to the same thing. This is actually a required property of encryption. As long as the encryption is invertible, this would be impossible. If we could find two such values, well then we couldn't decrypt and get the same x or y out correctly that went in. That's why this works--if we're willing to reveal these values and show that they produce xi that proves that someone somewhere along the chain cheated if the vote is not recorded in the final tally. The big drawback of this is it requires that the voter reveals her vote in order to show that her vote was not included in the tally. So the other approach for solving this is what's known as an auditing mix net, and the idea here is that each participant in the mix net can audit some of the outputs of the next step. So to do that, we need to provide extra inputs. So instead of the voter just providing the vote as encrypted to party A, they'll provide it to party B as well. All of the incoming votes go to the next two steps. And now B going to audit A and it does that by picking some random set of inputs. So let's say it picks this one and it picks this one, and it picks some of the ones in here. So that means it's looking at some A's ouputs, and we'll call these y values. So this would be y2, this would be y something else, and this would be y of n-1. All right. And this would be y of n. So for each of the n inputs, Bob picks some number to audit. And what B does is ask A to prove that y2 is a valid input. In order to prove this, what does A need to provide? What must A provide to B in order to prove that y2 is a valid output from A's mix net? The answer is the third choice, and it's really only necessary to provide the nonce, but it's helpful to provide the k value as well. What B needs to check--once this nonce is provided B can check that the incription using A's public key of the output, which is the y2 value, with the nonce added--that was the part that Alice removed. What Bob needs to do is check that that output is equal to x of k. And that value was provided directly to B, because all of the inputs from two links ago, went into this mix as well as the outputs from A. This proves to B that for that particular output that B asked for, that that corresponds to one of the inputs to A. We can do the same thing all the way down the chain. That means that A will provide its outputs both to B and C. C will be able to verify that B did the mix correctly by picking some random ones, and having B prove that they're correct. And at the final stage, B will provide its outputs to C as well as some validator who can validate that C did the correct thing. This audits one output. How many should we actually audit this way? Usually when people talk about auditing, the more is always better. The more auditing you can do--other than the cost of it-- the more you have a likelihood of catching cheating. But in this case, that's not the case. We have to be careful. If we audited all the outputs of the mix net, well then I would reveal exactly what the permutation was. We have a tradeoff here between voter anonymity and catching cheating of the mixes. If we just audited one output, the chances of catching cheating would be too low. If there are N votes and mix net can chew on half of them, we then be one half probability of getting caught. This is much too low. It's probably not necessary to cheat on half the votes. The last step can see what the actual votes are and decide which ones to cheat on. Cheating on a much smaller fashion than one half and getting caught with the much less probability than one half would make that risk worthwhile in many situations. Suppose there are 100 votes and 20 are audited. What's the probability that a mixer can cheat on four of the votes without getting caught? The answer is 0.4. The way to think about this is thinking about how to choose the votes. We have a hundred votes to begin with. There are four that the mixer cheats on, and the auditor is sampling 20 votes in this space. As long as the auditor picks one of the votes that was cheated on then the mixer would be detected. We can compute this by dividing the number of ways to choose without picking any of the cheated votes--that's choosing out of the 96 noncheating votes-- divided by the total number of ways of choosing the votes. To solve this, we just need to compute 96 choose 20 divided by 100 choose 20. Since the orders were cancelled out, they're 20 elements each time this the same as computing 96 times 95 down to 77 divided by 100 down to 81. We had cancelled things out and compute this by hand. We could just compute it all in Python. Here's a procedure to compute the product of the list remembering to multiply by 1 point to turn this into a floating point, so we get accurate division. When we run that, we get the result 0.4 with some more. We might like a higher probability of catching cheaters if we think cheating on four votes is enough to change the election. We'd like to audit more votes. We could do that. That will increase the probability of catching a cheating mixer but will also increase the probability that privacy is lost--that a vote can be traced through the mix net. Here's the scenario I have in mind. We have three mixers. At each stage, we validate 1/5 of the votes, and we have to audit the last one. We don't have another mixer here. We need some designated output auditor that receives these outputs as well as these and audits the last one. That's our three-step mix net. At the end of this, some votes are revealed. The question is if an eavesdropper, who can observe messages along all these channels and observe all of them, can determine which voter cast this vote. The answer is 0.008. At each step, there's a 1/5 probability of seeing that particular vote at both stages and knowing how it was mixed. In order to trace the vote all the way back, we need to see that same vote each time. That will be 1/5^3rd which is 1/125 which is 0.008. So that means there's a tradeoff here, if you do more auditing, if you increase this from 1/5 to a higher value, that would increase the probability that a given voter could be identified. It will also decrease the probability that the mix nets could cheat without getting caught. So here, we have a tradeoff between the privacy of the voters and the likelihood of detecting cheating. The next major topic we're going to talk about is digital cash. What we want to do is find some way to use numbers to simulate something that would be similar--maybe better, maybe worse--than physical cash. What are the properties that physical cash has--at least the properties we might want it to have? We'll have to have a universal recognized value-- that means everyone agrees that it's worth something. It should be easy to transfer between two individuals-- that means you can transfer it without having to talk to a bank or some trusted 3rd party. Some might argue against us including some governments, but many individuals would like their cash to be anonymous and untraceable. Not all government wants this but let's assume that as an individual that we do want our cash to be anonymous and untraceable. We'd like it to be light and portable--easy to carry. We'd like it to be hard to copy or forge. If someone can counterfeit the money, well, it probably won't have recognized value for very long. Assuming that we do want these five properties, which of these does US paper currency satisfy well? I apologize for specifying US, I know many of you are not in the US and have different paper currencies. You can answer this for your currency. I'm going to answer this for what I think about US paper cash. It's definitely a subjective question, but we'll talk about what I think the right answer is. US cash definitely has universally recognized value. At least in the US, essentially everyone accepts it, but even in other parts of the world, many people accept US cash. It is easy to transfer. You don't need to go to a bank or even interact with a bank in order to be able to transfer cash between individuals. It's sort of anonymous and untraceable--and I didn't check this one. It's definitely not designed to be untraceable. Each bill has a unique number in it, and you could trace that number. It's untraceable in practice, because people don't tend to record those bill numbers, but it's not untraceable technically. It's certainly the case when banks are robbed, and there's a sequence of bills that were marked in the same order, that's one of the ways those bills might be detected. It's not light and portable. The largest currency in the US is a $100.00 bill. That weighs the same as a $1.00 bill, which weighs about a third of a gram. That means if you can carry 1 kilogram, that's only 2740 bills, which would be over a quarter million dollars-- not a bad amount of money for an individual to carry around, but it certainly could become a factor if you're talking about large corporate transactions where 1 kilogram weighs a quarter of a million dollars. This is one of many reasons why very few large cash transactions are done and you see scenes in movies were someone needs a whole briefcase full of money in order to do things. The last question is whether it's hard to copy or forge, and I think the answer to this is also really no. It's just paper. The reason that it works is that the penalties for getting caught copying or forging are high. The real answer is that it is hard to copy or forge in large quantities without getting caught. Enforcement of counterfeiting is taken very seriously. It's done by the Secret Service that's also responsible for protecting the president. The only people who can get away with forging US currency in large quantities, are hostile countries where US law doesn't reach. When we think about the money is it's really a promise from a trusted bank. If the promise it says if someone brings this money to the bank, the bank will exchange it for something of real value. Today, we think of the paper itself as having real value. But in the early days, what the paper meant was you could go to the bank and exchange it for something that everyone would recognize as real value, say a bar of gold--almost as valuable as a gold star. If that's what money means, well then we should be able to do the same thing with cryptography. Let's see how that would work. Alice would go to Indivisible Prime Bank, which everyone knows is very trustworthy, and she would give to the bank a $100 in some other currency, let's assume the bank to take US dollars. The bank will read the message that says, "The bank owes the bearer of this note $100." The bank will send Alice a signed version of that message. The message, along with its signature, which is signed by the bank using the bank's private key, signing a hash of that message. This signature proves that it is a valid IOU from this bank, and now Alice has something representing currency. Now Alice could spend the currency. She could buy some tacos from Bob's Taco Emporium. Give him the currency. Pretty expensive tacos. And Bob could take the note--this signed message from the bank-- and give it to the bank and ask the bank to deposit into his account. Now the question is how well would the scheme that I've described work? Let's assume that everyone in the universe trusts Indivisible Primes and knows that bank's public key. So if this is true, then it does have universal recognized value. Everyone can check that signature and know it's an IOU from IP Bank. It does seem to be easy to transfer. You can just send the bits to another person. It could be anonymous and untraceable, and let's assume that it is. This would be the case if all the IOUs the bank creates are exactly the same. The way we describe it seems like they are. It's very light and portable, so it looks like we're doing really well. We've got four out of five. We've got a big problem, though. It's not hard to copy. It's, in fact, trivial to copy. We can send the same bits multiple times and no one knows which ones are valid and which ones are copies. So this doesn't work. Once we've lost this property, we've lost this one as well. It is hard to forge. To forge it we'd need to know the private key of the bank, but that doesn't matter, because once we have one bill we can make any number of copies of it and spend them as many times as we want. This means it actually doesn't have the universally recognized value. Depending on how you interpret this question, this is what I think is the correct answer, but there are other ways to interpret this where your answer could be equally valid and correct and not match mine. One way to solve this would be to give each bill a unique ID. Then if someone attempts to double spend a bill the bank would check has that bill be spent already. If it's been spent already, the bank would alert the spender that it's not a valid bill. This would also cause problems for the second possibility. It would no longer be easy to transfer if you have to check the ID with the bank every time you get a bill. It would also no longer be anonymous. It would mean the bank can track all the transactions, because every time there's a transaction, the bank sees the unique ID and knows who is transferring money to whom. Our goal is to have unique IDs to prevent this copying problem but preserve the anonymity and somewhat preserve the easy to transfer. We'll still need the bank to check the ID when the money is deposited, but maybe there could be transactions before that, and we can trace them back to who cheated at a point where the cheating happened. That's our goal. The main tool that we need for this is a new cryptographic technique called blind signatures. The solution to this is a technique known as blind signatures. This gives us a way to associate a unique ID with a bill to be able to detect double spending but doesn't allow the bank to associate the unique IDs on the bills with the person who acquires that bill. Here's the idea--Alice will deposit her $100 in the bank, and along with the bill she'll generate a message that says Bill # rA-- some unique ID generated by Alice--Invisible Primes Bank owes the bearer $100. She'll go to the bank, give the banker the $100 bill, and ask the bank to sign the message m. To make it a blind signature, though, she'll insist the bank wear a blindfold before bringing out the message m and the banker will have to sign it without being able to see this message. The bank will give that signed message back to Alice. This protocol has an obvious flaw in it. The bank doesn't know what it's signing. Alice could deposit $100 and ask the bank to sign a message for $100 gazillion. When someone deposits that bill, the banker will no longer be smiling. The solution to this is a technique known as cut-and-choose. This is somewhat similar to what we saw in auditing for mix nets. It has lots of applications to other cryptographic problems as well. The way we would do that with this scheme is instead of just generating one message like this, Alice would generate a large number of messages-- let's say 100 of them--send them all to the bank. The banker who is no longer blind folded or frowny will randomly pick one of these messages. Let's say he picks message 38. We'll look at the other ones. Each one of those should have a message like this but with a different random value. Check that they're all okay. If they're all okay, then without looking at message 38, then the banker will be blindfolded and sign message 38. The point of this is that Alice generates all the messages, transfers them to the bank, but the bank doesn't see them until the bank randomly picks one. Since the bank is picking the one to sign randomly and inspecting all the others, the probability of Alice being able to cheat without getting caught is 1 in the number of messages. That could work with blindfolds. We'd have to be careful how we deliver the messages to the bank and let the banker pick one and then see them without the blindfold. But that could work. It's not very convenient though. What we want to do is figure out a way to do this using cryptography instead of blindfolds. The scheme we look at for doing this is built on RSA, and it uses RSA signatures as we've seen in Unit 4. But this time, instead of using them normally, we'll use them in a way that blinds the message from the signer. Here's the protocol--we'll assume Alice wants the bank to sign a message, and Alice knows the bank's public key. It's an RSA key pair with the exponent and a modulus. So m is the message Alice wants the bank to sign. Alice also picks a random value, we'll call that k. Next, Alice will compute t by multiplying the message by k raised to the eb power. If k is random, select it from the integers from 1 up to nB-1. And it is also relatively prime to nB, well that would make keB mod nB A permutation of the values of ZnB. That means that this is also random in that range. And so, m multiplied by this is random. This doesn't reveal to the bank the value of m. We can safely send that to the bank without leaking any information. And the bank will sign that message using its private key--its private exponent dB. That produces the value t^dB power mod n, which the bank sends back to Alice. As far as the bank is concern, it just signed the random value and it sent that back to Alice. Is there anything Alice can do with this that is useful? The question is, which of these is equivalent to the value produced by the bank. Here are the choices. To answer this question, think about the properties of RSA. The answer is this one. Here is why. The signature is t^dB mod nB and this was t. That's what this is. We can use the property of exponents to simplify this. And now distributing the exponents, we get this. Here we have this term k^eBdB. Well, these are RSA keys--this is the public and the private key for that modulus. That is the equivalent to what happens when we do encryption and decryption. This is equivalent to k because these exponents end up performing encryption and decryption in the RSA system. They're designed so that's equivalent to 1, and so we get m^dB times k mod nB, while this value k--that was the the random value selected by Alice. She can just divide that out, and by dividing out the k, she gets m^dB mod mB, which is exactly what she wanted--that's the message m signed by the bank. Should we be worried that RSA has this property? The answer is we should, and this is related to the homework problem on Unit 4 where we saw that you could forge an RSA signature by multiplying two signatures. In this case, the message that's being signed might be used to produce other messages. We need to be very careful when we output the result of RSA decryptions. We might be signing something different from what we think we're signing. Here's how this works in the blind signatures protocol. Alice will generate not one message now but a hundred messages. Each one of this form but with a different value for r1. Alice will also pick a hundred k values. Send all of those to the bank. The bank picks some random value x between 1 and 100 and unblinds all the other messages so that means that the bank will send x to Alice. Alice needs to send something back to the bank that the bank can use to verify that all these messages, other than message x, contain something like this. So, what should Alice send back to the bank? Te set of all the ri values except for the one corresponding to message x. The set of all the ki values except for the one not matching x. Both of these or none of these work? The answer is sending all of the ki values would be sufficient. Those are the values that the bank would need to unblind the messages, because the bank can decrypt all the messages using its private key and then divide out the k raised to the eB power from the results. It would be okay to send this, but there's no need. There's no security loss in sending the ri values except for the one from message x, but the bank is going to learn those anyway. Since after it divides out to ki^eB power, it has all these values. At the end of this protocol, Alice will be able to compute message x signed by the bank. Now, Alice can spend the bill, give it to Cathy. Cathy can verify the signature, and she could deposit the bill in the bank. The bank checks if this ID has not been spent before and believes that it's a valid bill. If Alice spends the bill again, when Doug gives it to the bank, the bank checks the signature but finds that this ri value was previously used. The bank knows the bill was double spent. The problem is the bank doesn't know who double spent the bill. The bank received the bills from Cathy and Doug, but they weren't the double spenders. They were the ones that were duped in the accepting the bill. Alice was the double spender, and because of this blind signature scheme, the bank doesn't know that Alice is the one who spent the bill, and so for this approach to work, we need a way to trace the double-spent bill back to the person who double spent it, but we also want the bills to be anonymous so that sounds like a contradictory property. There's actually a way to achieve this. There's a very clever solution to this that was developed by David Chaum and colleagues in 1988. The key idea is that if the cash is only spent once, then it's anonymous, but if it's spent twice, then the identity of the double spender is revealed. I want to see if you can guess how to do it. It uses cryptographic tools that we've seen before. The key idea is to provide this property. The answer is the one-time pad. If you can figure out a way to use any of these others to also do it, please post that in the forum, but certainly the easiest way to do this is to use a one-time pad. Remember that to keep property we have with the one-time pad is that we can split a message into two pieces and XOR them to get the message back. That's what we're going to do here. Now we're going to use this in our blind signature scheme. Alice will create a hundred messages like this, each one with a different random nonce. But what she's going to add to the messages is an identity list made up of m of these values. We'll talk more about what these are in a second as well as the usual value of the bill. What we want these i values to be are things that reveal Alice's identity, if someone knows enough about them, but otherwise reveal nothing. Here's what they are. They are a concatenation of two hash values. H is a cryptographic hash function where the property we want is where the two parts XORed gives you Alice's identity, and it's easy to create these values. We can set the first one to a random bitstring, and we send the second one to Alice's identity XORed with the random bitstring. Which of these would be the minimum amount of information needed by the bank to verify these messages? All would be enough to learn Alice's identity if someone had both parts. The answer is the first choice. Alice needs to validate--sorry. My answer is the first choice. The bank needs to validate the hashes. It needs to know the preimage for the hash. It's enough to get just one of those. It isn't necessary to get both of them because the bank presumably starts knowing Alice's identity. If the bank doesn't start knowing Alice's identity, then anyone could create a bill that would blame Alice for double spending. So if the banks knows Alice's identity, then the bank can compute the other part of this computing xk1 by XORing this value with Alice's identity and they can check if both of the hashes are correct. If they are, that validates that each of these identity components is correct. So now let's look what happens when Alice spends a bill to see how to solve the double spending problem. At the end of this protocol, Alice will have a message like this, where each one of these identity pairs is one of those pairs of hashes. To spend the bill, Alice sends the bill to Cathy, and Cathy sends a challenge back to Alice. And what the challenge is, is a list of M random bits. These will tell Alice which parts of the identity she needs to open. So in order for Cathy to accept the bill, what Alice has to do now is, for each one of these, validate one part of the hash. So if Cathy's challenge bit was 0, what Alice has to provide is the 0 part of Identity 1-- and remember what Identity 1 is is the hash of the 0 part of Identity 1 concatenated with the hash of the 1 part where the property we need is that these 2 values XOR to Alice. If the challenge is 1, then it'll send the 1 part of the corresponding identity, and so on. So now Cathy can validate these hashes. So she's going to check that this matches the values sent by Alice. And we'll do that for all the identities. If they all match, Cathy accepts the bill. When she deposits it, she sends these I values to the bank. So suppose Alice tries to spend the bill again. This time she'll send it to Doug. Doug will do the same protocol. He'll make a challenge, send that challenge to Alice, get back the I values, but they'll be the ones corresponding to his challenge. And he'll do the same check before accepting the bill. And now Doug can deposit the bill, sending along the same I values. So now what does the bank have? It has both parts of one of the I values. As long as one of these challenge bits is different, the bank knows there was double spending because it saw R-K value twice. It also knows the identity of the person who obtained that bill because it can XOR the 0 part of Identity 1, which it received from Cathy when she deposited the bill, with the 1 part of the identity, which the bank received from Doug when he deposited the bill. So now the bank can tell the police to arrest Alice for double spending a bill. So to see that you understand all of this, the question is, "If Alice spends a bill twice, "and M the number of identities included in the bill is 10, "what is the probability that she gets caught?" So that probability is greater than 0.999, and the reason for that is the only way she wouldn't be caught is if the two people she spends the bill with take exactly the same challenge. The challenge is 10 bits long, so the probability that would happen is 1/2^10--1 in 1,024. And 1 minus that is greater than 0.999. So that's pretty high. If that's not good enough, we could increase m. So the last thing I'm going to talk about this unit is bitcoin which is a way to do digital cash in a completely decentralized way. This means that there's no bank--there's no trusted authority-- but everyone who participates in the protocol is considered a peer, and they all have an equal say as to what's valued and what's not. Bitcoin combines lots of ideas from previous protocols, but the particular way that bitcoin does it was proposed by Satoshi Nakamoto in 2009. The way it avoids needing a centralized bank is to keep track of every single transaction that ever happens. So in order to track transactions we have a chain of signatures. So here's how that works. Some coin comes in, and we'll have to talk about how that value gets created, next. For Alice to transfer that coin, what she'll do is to create a message that includes the coin, includes that she's transferring it to Bob, and she'll sign that with her key. So then she sends that to Bob. Bob can verify the signature. For Bob to transfer the coin, he'll add transfer to Cathy to that and then sign the whole thing with his private key. And now Cathy would do the same thing-- take everything that Bob sent, add a transfer message to it, and sign the whole thing with her private key. And then she can transfer it on to Doug. At every link in this chain-- as long as they have all the public keys they need-- can verify the entire history of transactions. So Cathy can verify the signature of Bob-- open that up-- verify the signature of Alice inside it, and verify that the original coin. So this is a way to use public key cryptography to keep track of an entire history of transactions. Does it provide enough to provide digital cash? So how many times can Bob spend the coin? The answer is as many times as he wants. There's no way when Cathy receives the coin from Bob that she can verify he didn't already spend it to someone else. So in order for these coins to have some value, they have to be scarce. That has to be the case that you can't spend them multiple times, and you can't just create them. So this this is where the peer-to-peer network is useful. Every time someone receives a transaction, they don't just accept it. What they do is they send it into the peer-to-peer network. So when someone wants to verify a coin what they need to do is send it into the peer-to-peer network. Every transaction in that coin can be verified by all the other members of the network, and before the transaction's considered valid, we need to know that that coin hasn't been already spent in some other way. So there's two important parts to this. That means all the nodes must agree on all the transactions. That requires some sort of timestamp. Nodes are going to receive messages at different times. We need to know it's the case that if this coin was spent twice, that before this one validates the transaction-- We need to ensure that if someone attempted to spend a coin twice both transactions wouldn't be validated by having different parts of the network have different views of that history of all the transactions. So how do we provide this timestamp? And we need to remember that some of these nodes might be malicious. We have no way to know that all the nodes are trusted. Anyone who wants can join the network. We just need to have some honest parties to validate the transactions. But we need to know that dishonest parties can't invalidate the history of transactions. The key to this is requiring a proof of work. For each timestemp we're going to have a new block, and we need to know that creating those new blocks requires work. If it requires enough work to increase the timestamp, then it's unlikely that a malicious user can increase the timestamp faster than the whole network. So how do we make that hard? We need some kind of proof--of-work to be embedded in the timestamps. So here's a way to do a proof-of-work. In order to prove you've done some amount of work, you need to find a value of X where the hash of X starts with some number of 0s. Doing that requires work if this is a good cryptographic hash function. At least if we have a random oracle assumption about that hash function, the only way to find such an X value is to keep guessing and looking at that output. So you need to do the amount of work necessary to find that output. So how much work is that? How many times do we expect to need to compute the hash function H in order to find a value of X where H of X starts with 0 to the 10-- starts with 10 consecutive 0s? So the answer is 1,024. If the hash is a good cryptographic hash and it produces a uniform distribution, the probability that any bit is a zero is 1/2, and we need to find an applet that starts with 10 zeros. Note that it doesn't matter how big the output of the hash function is. This is the same whether it's a 256-bit output or 192-bit output. It doesn't matter how big the output of the hash function is, whether its only outputting 10 bits or its outputting a more reasonable number, like 256 bits, as long as we restrict it to the first 10 bits having to be zero. So that means finding a hash value with certain properties is expected to require an amount of work, and by adjusting those properties we can increase the amount of work. So this is what Bitcoin does. In order to create a new block, which would validate the next history of transactions, it's necessary to find some value X such that the hash of the hash of the state concatenated with X is less than some target. You keep increasing the value of X until you find one that satisifies that property, and that provides the timestamp that allows a new block to be generated. So this is the idea Bitcoin uses to generate a timestamp. You have to keep finding new blocks, and a block will validate a set of transactions. But to generate a new block, you've got to find a new timestamp, which is this target. So you've got to find a value where the hash of the hash of the state concatenated with that value-- and the state does two important things. It includes information about the previous block-- this is how the timestamps form a chain. The state must include the previous block. It also includes some information that's likely to be unique for each member of the network. This is how they avoid it being the case that they're all racing to find the same value X. So given those properties, then the goal is to find the value X that you can concatenate with that to be less than some target. And the value of the target controls how hard it is to find. And the way the Bitcoin currency is designed to work is this value of the target is adjusted in a way that makes the time to find X keep about 10 minutes. That's the time for the whole network to find the next value so that that value of the target will keep decreasing as the computing power in the network increases. And if you go to the Bitcoin website, you can see the current value of the target, which is this value that-- In order to create a new block, you've gotta find an X value such that this property is less than the target. And I should emphasize that these 0s are not just 1 bit. This is hex. So each one of these 0s is 4 bits. So if you've got to find a value where the hash starts with 54 zeros followed by something less than 101 and more-- so if you found a value that hashed where the result started with 55 zeros or started with 54 zeros, a 1-0, and 0, well then you'd be golden. You'd be able to create the next block. That would allow you to earn the value of a new block, which currently is 50 Bitcoins. Each Bitcoin is worth about $5. So finding a hash that hashes to this would be worth about $250. And the rest of the network can verify that. It can compute this same hash function, verify that for the X value that you found the value is less than target, and that will add that block to the Bitcoin network. So the difficultly of finding such hashes keeps decreasing. The reason it's so low is because of using GPUs. If you were using a regular processor, it would use far more than $250 in computing power to find such a hash value. If you have a GPU, there are algorithms for implementing the hash function more efficiently. That would mean that finding a value that satisfies this property should require about that much value in computing power. So to see if this makes sense, I want to ask why the timestamp function-- the one here to find the next block-- uses two hashes instead of one. Here are the choices. See if you can figure out which one would make the most sense as to why there are two hash functions here. So the answer is to increase the amount of work required. Normally, we want to use computation to reduce work, but here the goal is to make this amount of work to find a bitcoin to be stable, to be fairly predictable, and to be about 10 minutes long. And so, we could do that with just one hash function by reducing the value of the target. That would require changing the target more frequently and using more zero bits in the target. The alternative is to make it harder to compute each value of x, and that's what the two hash functions do. Now what happens in the network is at each time step a new block is created that validates all the transactions in that block. At the point where this block is created, this has to be the longest block chain. That's the way that Bob can validate that this was a valid spend. Someone could try to create an alternate block chain. So if Bob wants to spend the coin twice, what Bob would need to do is create a chain that's longer than the longest chain. When a transaction's validated by the network, all the signatures in the coin are checked. This is using the transfer chain, but to prevent double spending there's also a check of this chain of blocks. And the check is: the longest chain is the one that's viewed as correct. So each peer in the network might see a different view of this chain of blocks. If they see different views, the one that has the longest chain is the one that will be viewed as the most correct view of all the transactions. So every participant in the network is effectively keeping track of all transactions. And the version of all transactions that people trust the most is the one with the longest chain, and if an adversary wants to create a longer chain with a different view of transactions-- so if Bob wants to double spend this coin and, say, give the same coin to both Cathy and Doug, what Bob would need to do is create a longer chain that convinces Doug that this is the correct view of the network and this view is incorrect. So that requires finding these hash values. If the power of the network exceeds the power of the adversary, well then it's likely the network will have a longer chain than the adversary can produce. Keep in mind, the motivation for producing this attack is just to be able to double spend one coin. There's still the chain of public key signatures that validates the transfers of each coin. So the resources that you would need to spend to produce a longer chain to convince someone that you didn't spend that coin in the other chain, which is now the one that would've included the previous spend is to find these hash values quicker than the network can. Then centers are set up in such a way to make it unlikely that someone will want to do that since if you do find that next hash value in the chain, you can create a new block, which is worth 50 bit coins If you create a longer chain trying to catch up to the chain that the network has, that's going to require a lot more computing resources, and would only allow you to respend the bit coins that you already owned. So this doesn't provide anonymity in the traditional sense. It avoids the need for a central authority, but each transaction is known to everyone in the network. The way to provide some anonymity is, instead of using your actual names in the transactions, you can have different identities for each transaction. So this is gonna be some new identity. It will still need to have a certificate that validates the public key for that, but it doesn't need to be tied to Alice's identity in any way that's visible to anyone else. So that's the way to provide some anonymity, even though all the transactions in the network are public to everyone who participates. So this actually works. There's at least a reasonable number of people that trust this currency and are providing computation to create these blocks, which is incentivized by creating the value of new bit coins. There's about 9 million bit coins currently available. And the marketplace fluctuates, and it depends on cost of computation as well as the trust in the bit coin network. There have been some incidents where the price fluctuated wildly because there was some concern about the security of the bit coin transaction site but the current market is fairly stable around $5 for a bit coin. So this is the end of Unit 6. Let me summarize what we've seen, and there are lots of connections between the different things we've seen that all use cryptography to solve problems. So the first thing we looked at was Anonymous Communication, and we saw how to do that using onion routing which was a chain of public key encryptions. We looked at how to do voting using mixes. which also involved a chain of public key encryptions with the addition of audited permutations. We looked at how to provide anonymous digital cash in a centralized way. This involved using blind signatures. But blind signatures pose the risk that if you don't know what you're signing that's a big problem. So we talked about the cut-and-choose technique to verify the messages before signing the one that wasn't opened. And finally we looked at bitcoin. That provides a way to do decentralized digital cash. This also used a chain of public keys but unlike onion routing and voting, where we were using chains of encryptions, in this case we are using chains of signatures. And we also saw how bitcoin uses hash preimages as a proof of work. And having proofs of work is useful for many other protocols, So all these protocols use a combination of asymmetric cryptography and symmetric cryptography typically involving hashing. And there's a tremendous amount of things that you can build with just these primatives. In Unit 7, we'll look at a few more interesting applications of cryptography and wrap up the course. I hope to see you there. Welcome to Unit 7. This is the conclusion of the class. I hope everyone's enjoyed it and learned a lot about cryptography. I'll summarize the class briefly, and then I will talk about one new protocol and one new application of cryptography to do secure computation. One important point that I want to make today that I haven't made throughout the class enough is that cryptography is very different from security. When we're thinking about cryptography, these are very abstract things. We're thinking about whether we can use math to solve problems. On the other hand, security depends on a lot more than just cryptography. Security is really mostly about people. No matter how good your math is, if someone picks "123456" for their password, it doesn't matter. You're not going to have security. Security is also about the systems and how they interface with each other. Many of the security problems are because of misunderstanding the connections between systems. In this class, we've largely focused on cryptography and not addressed the broader issues about security. But if you want to build secure systems, well these issues are very important. It's not just about the math. Cryptography is all about secrets, and we've seen lots and lots of ways to use secrets to solve problems. We started in Unit 1, looking at the one-time pad, where we could XOR a message with a key. And we saw that that provides perfect secrecy in a strong theoretical sense that an eavesdropper who intercepts this message can learn nothing at all about the message. But that it requires a perfectly random key that's never re-used that's as long as the message. So it doesn't work well in practice. It also has all sorts of other problems like the malleability of the one-time pad. This led us in Unit 2 to look at some ways to use practical symmetric ciphers to provide nothing close to perfect secrecy, but something useful for many purposes. We saw that we could amplify the security provided by a small shared key to obtain a secure channel for sending long messages. And we've talked about modes of operation that can do that with different levels of security. And then we talked about cryptographic hash functions, which provide a way to map a message to a smaller value that provides pre-image resistance so it's hard to find a message that hashes that value as well as collision resistance. And this is usual for authentication as well as lots of other purposes that we've seen. The big limitation of symmetric ciphers is they require a shared secret between the two parties. In Unit 3, we talked about key distribution focusing on the Diffie-Hellman protocol for key agreement. And that allows the 2 parties to establish a shared secret starting from an insecure channel. That led to asymmetric cryptosystems, in particular RSA as an example of a public key cryptosystem. That provide the additional functionality where the encryption function doesn't reveal the decryption function. This enables lots of interesting uses of cryptography. In Unit 5, we saw how to use public key combined with symmetric ciphers to provide authentication. And we focused in particular on the TLS protocol that's used as the basis for electronic commerce on the web. In Unit 6, we looked at protocols for anonymous routing, voting, and digital cash - all of which combined a mix of asymmetric and symmetric cryptosystems. So I hope you've enjoyed this tour of cryptography. We've seen lots of different things that can be built from a few simple primitives. I'm going to cover one more protocol in this unit, which is looking at secure computation. And it uses many of the tools that we've seen in all of these previous units. So here's the goal of secure computation. Let's suppose Alice and Bob meet at a conference. They don't yet know each other despite the fact that they've been talking to each other through units 1 through 6, but let's pretend they're not quite so friendly yet, and they both have their smart phones. On their smart phone, they have an address book. What they want to do is figure out, do they know any of the same people? This is a pretty common occurrence at conferences. Often, the way people do this is the slow, "Were you ever at this place?" "Do you know someone here?" "Do you know anyone there?" They want to do it more efficiently, so what they'd like to do is have a protocol that allows them to compare their address books and find all the people they know in common. In this case, it would match Dennis since they both know the same Dennis and then they would see the matches but not reveal anything else about their address book to the other person. They want to do this by executing some protocol where at the end of the protocol both parties would know the matching entries but not learn anything else about the other person's address book. More generally, we have two parties - A and B. They have some private information. They want to perform some secure computation, and at the end of that they learn the result of some function on both of their inputs, but they don't learn anything about the other party's input. Now it's time for a question. It's sort of a trick question, but if you were paying attention at the beginning of this unit, you'll be able to answer it correctly. Can we achieve this property using cryptography? The answer is no. We're going to talk about a cryptographic protocol that does this. But actually achieving what we want depends on lots of other things. It depends on people. It depends on assumptions about the adversary, knowing what capabilities the participants have as well as any eavesdropper might have, as well as building the system that actually runs the protocol. These are all things we're not going to talk about. What we are going to talk about is a protocol that, given the right answers to these questions, could provide this property. This idea was developed by Andrew Yao in the 1980s, and it gives a way to perform any function as a secure computation. The idea is that any discrete, fixed-size function can be turned into logic gates, and then if we can find a way to implement logic gates securely, we can implement a whole function this way. We can think of a logic gate as the truth table. Let's take for example the function AND. AND takes 2 inputs and produces 1 output. And we can describe that AND with the table of values. We'll use 0 to represent false and 1 to represent true. This is a regular AND gate. If we evaluate the AND gate, we need to know the actual values of A and B. Suppose the value of A is 1 and the value of B is 0, then we would take this line from the truth table and know that output has the value 0. Our goal is to encrypt the circuit in such a way that we can still evaluate it without actually knowing what the inputs are and without learning what the output is but still producing output that we can use as the input to the next circuit. If we can evaluate each gate this way, keeping the inputs and outputs encrypted, we can evaluate the whole circuit. At the end, we can map the final result to a meaningful value. Our first step to creating an encrypted circuit is to replace the inputs with encrypted values. That means we need some way to represent a 0 on this wire and some way to represent a 1. Same for this wire. We're going to place this 0 and 1 in the table with encrypted values. We'll just look at those as randomized. We're going to pick some random value to represent 0 on this wire. Let's call that A0. We'll pick some random value to represent 1. We'll call that A1. And we'll do the same thing for the other wire. And we'll do the same thing for the output. Here's our new truth table. Well, if we keep things in the same order, we haven't hidden anything. Let's randomly permute it, and now suppose we got a table like this. That means that A0 is either 0010 or 1010. Now the question is can you determine what is the value of B0. If you could, that would mean you know the semantic value of B. You can tell if it's equal to B0 or not and that would mean you're evaluating the circuit and you're learning the values in the circuit, so you don't have the secured computation you wanted to. Can you determine the value of the 0 knowing this garbled table? And the answer is you can. It must be 1100. The way to see that is if you look at the x values, there are three that match 0111. Those must correspond to the output where x = 0, because we know an AND gate has three outputs that have a value 0. And then there's one that has the value 1011 that must correspond to the x value having the output 1, and that means the only way the output could be 1 is if both of these represent 1. That means we know the value of b1. That's this one. The value of b0 must be the other one, which is 1100. We can, in fact, deduce all the lines in the truth table from this pattern. So obviously this doesn't work. We're not hiding in the patterns that are inherent in the logic functions we're computing by replacing them with random numbers as we have to do something so this table is harder to see. And what we have to do is make it so the outputs are encrypted, so that we can't tell the values of the other entries in the truth table. We can only tell the value of the one that we actually evaluate and that hides whether it matches any of the other values. We need to hide that pattern, and the way to do that is going to require encrypting the outputs in some way. We have to encrypt them with different keys, though. If we encrypted them all with the same key, then either the evaluator would be able to determine all the outputs because the evaluator knows that key or she couldn't determine any of them because she does not know that key. We need to encrypt each output with different keys such that when someone evaluates the truth table they can only decrypt the output that corresponds to the row for the actual values they have. The question is which of these values could be used for this entry in the truth table to ensure that a circuit evaluator who knows a1 and b0 would be able to decrypt this output and none of the other outputs. Here are the choices. Be careful that more than one of these could be correct. The answer is the last two could work. Let's go through the other answers. The first one, we're encrypting using a1 the value of X0 concatenating that with encrypting X0 using b0. This means if the evaluator here is either a1 or b0, they would be able to learn the value of X0. That would enable the evaluator to decrypt more than one entry in this truth table if the other entries were encrypted the same way. The second possibility has a similar problem. If the evaluator knows a1, they can obtain the value of X0, that's not secure because they can decrypt two outputs in this table. The other two make it so that obtaining X0 depends on both keys. It depends on knowing both a1 and b0. An evaluator who knows only one of those can't determine the key for this encryption since this is the XOR of those two values and it depends on both of them. They also couldn't do both of these. They could do the first one if they know a1, but they just get an encrypted value there, and they couldn't decrypt the second one to get the value of X0--so, either of these can work. This first one is going to be more efficient because it only involves one encryption. There might be other ways to combine the keys that would be better and provide more security. That means, in our garble table we'll have our outputs encrypted with different keys corresponding to inputs that corresponds to that output value. Of course, we can't send that whole table. That would reveal the values of a0 and b0. We need to remove this part of the table. Then we need to randomly permute the order of these entries. The other thing will do is add some padding. That means after the garbling we'll have a table that's looks like this. Because each of these values is encrypted with a different key, the evaluator can't tell which one is which. The evaluator still needs to be able to decrypt these to produce the right output. The question is how does the evaluator know what the output of that truth table is? The evaluator could try decrypting all the entries and pick the one that seems correct. The evaluator could try decrypting all the entries using the input value keys known to the evaluator, and use the one that decrypts to something of the form pad concatenated with x, and then use this value as the output, or it is not possible based on what the evaluator knows. The answer is the second choice. The evaluator needs to try decrypting each of these. It knows one of the a values and one of the b values, but it doesn't know which one it knows. The ones that aren't correct--well, it's going to have the wrong key. It's going to decrypt some message and it won't start with this pad. That needs to have enough bits--so the chances it would start with this pad if it is not the right key would be vanishingly small. And the one where it does have the right keys will decrypt to the pad and the correct output value. And that's the value--that's the output for that gate. Here's how this works as a protocol--we have two participants. We'll call Alice the generator--that means she is going to make the circuit, and Bob the evaluator--that means he's going to evaluate the circuit. At the beginning of the protocol, they've agreed on some circuit they want to evaluate. For this to be interesting would be much larger than that. It takes inputs from both Bob and from Alice. Alice is going to generate a garble table for each one of these logic gates in the circuit, and send the garbled circuit to Bob. She is also going to send her input values, but because they're random nonces Bob can't tell what they mean. Bob's going to evaluate the circuit using the garbled circuit protocol decrypting one entry from each of these. At the end of this, Bob's going to get some output values. Then they're going to do something to turn that into the semantic value, which possibly Bob receives, possibly Alice receives. We can design the protocol either way. I am not going to talk about that final step of how you turn the encrypted values at the end of the circuit into meaningful values. We'll leave that as a question for your exam. But there is one more question that I do need to talk about. That's the question of how does Bob obtain his inputs to the circuit. To evaluate these tables, Bob needs to have both Alice's inputs and his inputs. Here are the choices. Bob could generate them himself. He could ask Alice to provide his inputs. He could ask Alice for both possible values for all these wire labels, and then select the right one corresponding to his inputs. Or none of these possibilities actually work. The answer is none of these work. He couldn't generate them himself because if he generated them himself, there's no way Alice could generate these garbled tables. He can ask Alice to provide them. That would require him to reveal his input. And the whole point of this protocol is for him not to have to do that. This approach wouldn't work either--if Alice gives Bob both of the values for each input, then he can evaluate these gates with both of these values, and he will learn more about Alice's inputs than he should. The answer is none of these work. We need some other solution. We need some way to enable Bob to obtain these inputs without revealing to Alice what Bob's actual values are. What we need is something called oblivious transfer, and in particular, we need what we'll call "one-out-of-two oblivious transfer" and what that means is Alice can create two values, X0 and X1. Bob will obtain one of those values, but Alice doesn't learn which one Bob learned. Bob can only obtain one of the two values but Alice doesn't know which one Bob obtained. There are lots of different protocols that provide this. The one I'm going to describe was invented by Even, Goldreich, and Lempel in 1985. It builds on RSA encryption and we're going to look at it as we need to use it in the garbled circuit protocol. Our goal is that Alice has two wire labels; this correspond to the inputs to some gate, and she wants to transfer one of them to Bob without revealing the other one. We're going to use Alice's public key. We'll assume that is known to Bob before the protocol starts. Our goal is to transfer one of these two wire labels to Bob. The first step is to create two random values. These are separate from the wire labels. These are going to be transferred to Bob. These have no meaning. They're just two random nonces created by Alice. Depending on which wire label Bob wants, Bob has some input either zero or one. He's going to pick either the first or the second of these. So, he is going to pick Xb. Is it equal to either X0 or X1, depending on his value of b. Then Bob will pick some random value r. Bob is going to use this to blind the response. He can allow Alice to learn whether he pick X0 or X1. That would reveal his input. What he's going to do instead is use this random value to blind the response. He'll compute this new value, which is the value of the X that he selected plus the random value raised to that public exponent modn. We're going to hide the value of Xb by adding a random value raised to the e power to it. That value is what sent back to Alice, and Alice is going to perform two different RSA decryptions. She knows the values that she selected for X0 and X1. She's going to subtract each of those from V. She'll decrypt it using her private key and we'll call the first one K0, that was the one constructed using X0, and the second one, K1, that was the one constructed using X1. The question is if b selected, what is it's input? That means b has the value for Xb is equal to X1. What are the values of K0 and K1? Select the best answer for each choice. It could be meaningless, it could match X0, X1, or r. The answer is the value of k0 is meaningless as long as we have a good random distribution here when we raise a to the e power and the reason for that is this value now v would be x1 + r to the e mod n that is because xb is x1. The value for k0 is that minus x0 raised to the d power modn. Theres nothing that gets rid of this random value and theres no reason this would be a meaningful value. The value of k1 though is meaningful, and it's in fact the value of r, and the reason for that now the value of k1 is equal to the value of v minus x1 so that removes the x1 from that value. So that means the value of k is r to the e modn raised to the d power modn and this is exactly the RSA decryption that will give us the value of the message which in this case is equal to r. That means Alice has now learned that random r is selected by Bob, and if value is stored in one of these keys, she doesn't know which one, the value of k0 is meaningless. Its a meaningless value that cant be determined by Bob because Bob doesnt know the value of d. Thats the important thing that weve done here. Now, Alice has these two keys. Depending on what value Bob picked, one of them is meaningful, has the value r, the other one is not. Now Alice is going to send a message to Bob that will allow Bob to learn the wire label. The way she's going to do that is actually quite simple, she's just going to add these two keys to the wire labels. She'll send these two values to Bob. Another question is how does Bob compute W1, and we assume Bob's input value was 1. We pick X1 here and follow the protocol as specified. Does Bob now have enough information to figure out W1? These are the choices. Is it possible for Bob to now learn the value corresponding to Bob's input wire? The answer is Bob can compute this because Bob knows r. Bob selected r. Bob has received these two and the value of k1 is equal to r because that was the one that Bob sent back here. That means Bob can compute these subtracting r from W1 prime will give Bob that correct value of wire. Bob cannot compute W0 though because the value used for k0 is meaningless and unknown to Bob. This provides the property that we were looking for that Bob can select one of the two values that are the wire labels Alice generated. Alice doesn't learn which one he selected and Bob doesn't learn anything about the other one. Now, we have the last thing that we needed to complete the Garbled Circuit Protocol, the way Bob obtains his inputs is using oblivious transfer. That could be done at this stage to enable Bob to learn his inputs to the circuit. Alice sends the garbled circuit along with her inputs and Bob can evaluate the circuit. Then from the encrypted output wires, Bob can obtain the result of the circuit execution, and he could share that with Alice or they could flip roles and do it again. And then Alice would obtain the output. We have one little step in this protocol that we haven't described yet, which is how you actually obtain that output value. The outputs for the garbled tables are all encrypted, so at the end of the execution what Bob has is a list of encrypted wire labels. He wants to turn that into semantic output. That's left as the question for the exam. I should note that actually executing protocols like this, doing all these encryption is really expensive. This works for any function, but it is very expensive. It's a very active research area, how to do this faster. This is one of the things my research group works on as well as a lots of other researchers to try to find ways to provide properties like the one described here but without as much computation effort as executing the protocol that I have just described. Congratulations! You've made it to the end of the applied cryptography course. The world is now safe for Alice and Bob to share their secrets or not share them as the case maybe. I hope you've enjoyed learning about cryptography and will join us again for a future class. [Narrator] Welcome to Homework 1. The way homework will work in this class is a little different from what you might expect if you took CS101 with me. There will be regular homework questions; these will mostly be multiple choice or questions where you need to enter a number or a string. They might involve using programming to solve problems, but you won't be expected to submit code. You'll be writing programs to be able to answer the questions that are provided in the homework. There will also be open discussion problems, and these will usually involve either designing a solution to our problem, designing a protocol perhaps, or analyzing a provided solution. There's no graded aspect to these discussions, but I think it will be a valuable part of the course for you to think on your own how to solve these problems, and then to join the discussions and the forums to see other how people have solved them, and to see whether your solution is popular or other people notice problems in it. The third kind of problem will be challenge problems. These will be problems that you can submit answers to. They'll be separate from the regular homework, and these are problems that we expect to be quite challenging. They'll probably require a fair bit of time to solve, and they'll get more challenging as the course goes on. It's not necessary to solve these to feel like you're following the course and getting a lot out of the course, but if you do have time and enjoy challenges, I hope you'll tackle the challenge problems as well. If you are able to solve a challenge problem, please don't spoil the fun for other students in the class. Don't post your solution to the problem until after the deadline has passed. [Narrator] The first 3 questions on this homework is so that you understand conditional probability. For these 3 questions, we are going to use the relative frequencies of vowels in English, and I'm giving you those results as a percentage of all the letters in a sample of typical English text. You can assume that 13% of all letters in the text are the letter E, 8% are A, 7% are O, 7% are I, and 3% are U. Obviously, this is not a very audacious sample of English text. For question 1, assume some letter X is drawn randomly from English text with the same distribution as we've collected here. Give the value of the probability that X is a vowel. Your answer should be in the range from 0-1, so a real number where 0 means it could never occur, 1 means it always occurs. [Narrator] The answer is 0.38, and that's just the sum of the probability for each vowel. Summing those up, we get 0.38 as the probability that a randomly drawn letter is a vowel. [Narrator] Question 2: give the conditional probability that X is an E, given that X is a vowel. [Narrator] The answer is 0.34. To compute this, we need to use the conditional probability rule. We already know the probability of B, which is X is a vowel. We computed that in question 1 as 0.38. What we need is the probability of A intersect B. Well, B is--the events were X is a vowel. A are the events where is equal to E. Whenever X is equal is to E, it's also a vowel. That intersection probability is the same as the probability that X is equal to E, which is 0.13, and dividing those we get the result 0.34. [Narrator] For question 3, compute the probability that X is a vowel conditioned on X is not the letter A. [Narrator] The answer is 0.32, and we're computing this using the conditional probability rule, so that means we need the probability of A intersect B, divided by the probability of B. In this case, B is the case where X is not A so that's 1 minus the probability where X is A, which is equal to 0.92. Since the probability that X is A is 8%. The intersection probability is the probability that X is a vowel intersected with the probability that X is not A. That's the probability for all the vowels other than A, which is adding 0.13 + 0.07 + 0.07 + 0.03, and there we get 0.30, and when we do the division, we get 0.32, and this makes sense. Previously, we computed the probability that X is a vowel by itself with 0.38. If we know X is not A, we expect that probability to go down, which is why we get the lower value 0.32 here. [Narrator] The next question checks that you understand one-time pads. Alice and Bob have agreed on a perfectly random key and that Alice will send Bob the answer to the question, "Are you taking CS 387?" As either the single character Y or the single character N. This will be encoded using the string to bits procedure, which takes in a string and gives you a list of bits, and the results of string to bits Y is 1011001, and the route for N is 1001110. Mallory knows nothing about the key or the message, but she intercepts the transmission between Alice and Bob, and intercepts this sequence of bits, and Mallory wants to cause trouble between Alice and Bob. She wants to make Bob hear the opposite of what Alice sent so if Alice sent a Y, Mallory wants Bob to hear an N. If Alice sent an N, Mallory wants Bob to hear a Y. What should Mallory send instead of the original message that she intercepted, and your answer should be in the form of 7 bits, each of which is a 0 or a 1. Once you understand the answer to this, I want to encourage you to join an open discussion question, which is is there a way to solve this problem without using anything other than a one-time pad? Is there a way for Alice and Bob to communicate correctly the answer to this question and know that it can't be altered in transmission and still preserve all the privacy properties of a one-time pad? [Narrator] The answer is 1011001, and the way to get that is to look at the 2 possible messages. We had for Y 1011001. For N, we had 1001110, and what we want to do is XOR them to figure out the difference between those 2. The XOR of Y and N is equal to 0010111, and so this is the value that will flip a Y answer into an N answer. Now we want to XOR that with the actual message that was sent. We don't know the key, but we know that that message is the result of XORing Alice's message with the key, and so by XORing those, we get this value 1011001, and if that's what Mallory sends, when Bob XORs that with the key, what Bob will get is the XOR of Alice's message. Alice's message XOR'd with this value, which will flip Alice's message to be the opposite. The point of this question is to illustrate the malleability of the one-time pad. Someone who intercepts the transmission can alter it and change the meaning in predictable ways. This is a very dangerous property for a cipher to have. [Narrator] A popular toy cipher is a monoalphabetic substitution cipher where each letter in the alphabet is mapped to some substitution letter. This could be the same letter, could be any other letter in the alphabet, and decryption is done by the reverse mapping. We provided python code to make this more specific, what a monoalphabetic substitution cipher does. One way to prove that a monoalphabetic substitution cipher is imperfect is to use Shannon's key space theorem, which says that the number of keys in a perfect cipher must exceed the number of messages or be equal to it. Assuming a 26-letter alphabet, what length message do we need to show that the key space for the monoalphbetic cipher is too small to be a perfect cipher? For your answer give a number, which is the minimum length message needed for this proof. [Narrator] The answer is 19. Some of this we need to compute the key space. Well, the key is just a permutation of the alphabet. We have 26 choices for what A could map to. We have 25 choices for what B could map to. We have 24 choices for what C could map to and so on through the alphabet. This is equal to 26 factorial, which I won't attempt to write out but will show you in python. Let's define a factorial function, and we get this rather large number. What we need to know now is how this compares to the message space. The number of possible messages of length N, while each letter in the message can be any letter in the alphabet. That's just 26 to the N. We're going to find the first number N, where 26 to the N exceeds factorial 26, and we get 19, and we can confirm that. That 26 to the N is indeed greater than factorial of 26; 26 to the 18 is less than it though. [Narrator] For this question, your goal is to provide a shorter proof that the monoalphabetic substitution cipher is imperfect by showing some 2-letter cipher text that could not possibly decrypt to the string CS for any key. [Narrator] There are many answers to this; any 2 letters that are the same cannot decrypt to CS because the way the monoalphabetic cypher works every time we see a G, it maps to the same letter, so G could decrypt to C, but that would mean the other G also decrypts to C and we can't get CS out. [Narrator] One useful property of XOR is that we can use it to share a secret, and that means we can take a secret, divide it among a group of people where each person in that group does not gain any information about this secret, but by combining their shares they can determine the secret. Here's an example: Alice has some secret X, and she wants to keep a backup copy of X. What she does is first she generates some key, selected randomly and uniformly from the key space. Then, she computes S, which is K XOR'd with X, so that means S of I is equal to KI XOR'd with XI. Then, she gives K to Bob and S to Colleen. This means by combining K and S, Bob and Colleen can produce the secret X. Either one by themselves has no information. K just has a sequence of random bits, and Colleen just has X XOR'd with a sequence of random bits which provides no information about X. This works fine as long as Alice trusts Bob and Colleen not to collude and to give back the values of K and S when she asks for them, but what if she's worried that they might collude? She decides to share the secret among more people. How many key bits will Alice need to share a 100-bit long secret among 4 people? [Narrator] The answer is 300, which you'll need to do to share the key among 4 people. Alice will need to compute 3 keys of 100 bits long, and one way to do this is to give 1 person the secret XOR'd with all of the keys, and to give each person one of the keys. There's no way to do this with perfect secrecy using fewer than 300 key bits because that would mean either we're not fully sharing the secret. That some party has no part of some part of the secret or that we're reusing key bits. [Narrator] The question for the challenge is you're given 2 intercepted cipher texts, C1 and C2. You can see those both below, and you know that they were both encrypted using a one time pad using the same key. They're also both messages in standard English. Your goal is to figure out what the 2 plain text messages were. You can enter these in any order, and it should be clear to you that which one is which can't really be determined, but figure out what the 2 plain text messages corresponding to the 2 intercepted cipher texts are, and I should mention that you can assume that both messages are in English and were encoded using string.to.bits that maps English to sequences of bits. I'm not going to provide an answer to this one, but I'll give you a hint how to get started if you are stuck, and that would be to try common English sequences of letters t-h-e would be a good one to try, and assume that probably appears somewhere in at least 1 of the quotes, and you can try t-h-e at each of the starting positions, and then assume that's in say message 1. You're going to guess cipher text 1 is t-h-e encrypted with a key, then you can compute the key from this that gives you what the likely key is, XORing string.to.bits of t-h-e with the corresponding part of that cipher text, and then you can XORing out the guest key from the corresponding part of message 2. If that looks like something that might be part of an English message, well, then, that's a good guess, and you can work on from there and start expanding the letters that you've guessed to see ones that produce reasonable results for both cipher texts. This is a fairly manual process; if you wanted to do this more automated way, well, then, you'd use properties of English to automate this kind of guessing and expansion for you. This question is about testing for randomness. Suppose is<u>random<u>sequence is a Python procedure</u></u> that takes as input a sequence of bits--that's input s-- and it outputs true if that sequence is truly random and false otherwise. The 2 statements are it's actually impossible to implement a procedure that does this. If some source produces random sequences that are guaranteed to pass an is<u>random<u>sequence procedure</u></u> that behaves as described, than that source cannot be a true random number generator. Check all the true statements. Both of these are true. In class, it was shown that it is impossible to produce a random sequence generator, but if we have is<u>random<u>sequence, I'm going to sketch out</u></u> how we could use that to generate a random sequence generator. The first thing that we're going to do in our generate<u>random function</u> is loop through all the sequences of length n. Then for each sequence, we will run our is<u>random<u>sequence test,</u></u> and if that is<u>random<u>sequence test returns true, we will return the sequence.</u></u> We have now created a function that generates a random sequence, which, as we've shown in class, is impossible. Therefore, is<u>random<u>sequence is impossible.</u></u> The second statement is also true. This result is a bit counterintuitive. The reason that this is a true statement is that any true random number generator must eventually produce sequences that fail is<u>random<u>sequence.</u></u> Presumably, is<u>random<u>sequence would fail, for example,</u></u> a sequence of 20 zeros, but a true random number generator would eventually produce that sequence, and this further shows the impossibility of having an is<u>random<u>sequence procedure.</u></u> Now we'll have some questions about modes of operation. This question is to see if you understand the modes of operation of symmetric ciphers. Which of the modes of operation can perform most of the work to do decryption in parallel? And by most, I mean the expensive operations, which are the encryption operations, not the XOR, which is very inexpensive. They can be performed at the same time using different processors. In the lecture unit, we talked about this for encryption. Here I'm asking about it for decryption. These are the 4 modes to consider. Check all of the modes that can do decryption in parallel. And the solution is that all of these modes can be decrypted in parallel. Let's look at each one of these modes one at a time. ECB can be decrypted in parallel because for each C-naught that we have we can decrypt it in reverse to end up with our original message. This can be run on one processor, and you do the same thing on this one and for all the other chunks of the message that we have, and all those can be run in parallel. Counter mode is also decryptable in parallel. For counter mode, the important part that we need to calculate is this bit into the XOR, and we know our nonce, and we know our counter advance, so this whole chunk can be calculated at one processor. This whole chunk can be calculated on another processor, and so on, and once we calculate those, we can calculate our XORs to reverse the C1 into the message, and this can also all be done in parallel. For cipher block chaining mode, similar to counter mode, the important part, this input into the xor again, which for the first block we know the initialization vector, so that can be calculated. Independently, we are already given C-naught, which can be processed in parallel to this part. In C-1 we know, and this could be processed in parallel, and again, we can process--calculate this, calculate this, and CBC can be decrypted in parallel. Similar to cipher block chaining mode, the cipher feedback mode is decryptable in parallel. The idea is very similar to the 2 modes we just discussed. We need to calculate the input into the XOR. This entire chain is known in advance. From C-naught we have this entire chain, which is also known in advance, and we can calculate all these inputs into the XOR in parallel and decrypt our message. For this question, I'm going to introduce a new mode of operation, which is very similar to cipher feedback mode. This is the way cipher feedback mode worked. The change for output feedback mode is instead of taking the cipher text and putting that back into the x vector, we're going to take the output from the encryption and take that, so that's the only difference between cipher feedback mode and output feedback mode. Which of the following are true? Output feedback mode provides stronger protection against tampering than is possible with CFB. Unlike CFB, with OFB it's possible to recover most of an encrypted file, even if one of the blocks in the encrypted file is lost. Like CFB, it could be the basis of a cryptographic hash function using OFB. And finally, it would be safe to use 0 as the initialization vector for OFB. Which of the following 4 statements are true? Only the second one is true. Let's go through each one to see why it's true or false. OFB doesn't provide any stronger tamper protection than CFB does, and both of them don't provide very much tamper protection at all. To see this, it's quite easy for an attacker to mess with a byte of C1, and that will directly effect bytes in the message. This is similar to a way an attacker could tamper with a message in a one-time pad. With output feedback mode, it is possible to recover most of an encrypted file if one cipher block is lost. We can see this because in the decrypt process, we need to calculate this input into the XOR, and that can be calculated without knowledge of C1, C2, C3, or any of the other cipher messages. Even if we're missing a cipher text block, the rest of the MIs can be XORed with the remaining cipher text blocks. Ends are calculated inputs to find our message block. Likewise, this shows that output feedback mode won't make a good cryptographic hash function. A cryptographic hash function must have the property that the final output depends on all the blocks in the message. In output feedback mode, the cipher block text does not depend on previous parts of the message. For example, C2 does not depend on message 1. And this shows that we can easily change a block and calculate to the same final output. For the 4th option, no, it is not safe to use 0 vector as the initialization vector. As with cipher feedback mode and cipher block chaining, the initialization vector is important to prevent an attacker from deriving repetition in the messages. Always using the 0 initialization vector leaves output feedback mode vulnerable to this attack. For this question, your goal is to implement cipher block chaining. We've provided some code in the ID to get you started on this. Here's the function we want you to write. The inputs will be plaintext, which is just an array of bytes that needs to be encoded, key, which will be a bytes array to be used as the key for the block encoder, the initialization vector, the block size, and the block encoder. To reinforce the idea of a block box encoder, I've written 3 examples. The first 2, non<u>encoder and xor<u>encoder,</u></u> are silly toy encoders, and the last one, aes<u>encoder,</u> uses the pycrypto implementation of aes. Your code should work with all 3 of these encoders. Finally, there is a test function which you can use to test your implementation. And here is my solution. First, I define an XOR function to XOR two-byte arrays together because that's not included in the library. Next, we'll loop through the plaintext array. And here we break it into block-sized pieces, which then gets XORed and then fed into the encoder. The encoder output is then saved as the XOR for the input on the next round. The encoder output is then saved as the XOR for the next loop and also added to the cipher output. And finally, the cipher is returned. For this question, your goal is to find a hash collision. We've defined a bad hash function, which is using counter mode to encrypt and then XORing all the blocks of cipher text that come out and using that as the hash output. And we've provided the code for our hash function. Your goal is to find 2 strings where the hash outputs are the same but the actual strings are different. For this problem, we've provided an implementation of counter mode to be used as a hash function. The counter mode will take in a plaintext input and calculate the cipher blocks as expected, which are then XORed together to return the hash. Your job will be to implement find<u>collision</u> to find a collision given an input message. We've provided 4 test cases for you to use to see if your code works along with some utility functions that you might find useful. Here is a diagram representing our counter mode hash function. The counter is encrypted using the key, which is then XORed with the message to produce the first cipher block. And this is repeated for the second cipher block, et cetera. Then each cipher block is XORed together to produce our hash value. One way of creating a collision is to swap 2 of the cipher blocks. For example, the first 2. We can write this new hash as hash prime, and it should be obvious that hash = hash prime. Now let's look back at our diagram, and for notation, let's refer to the value coming out of the encryption as E0 and E1. Then we have M0 XORed with E0 = C0 and M1 XORed with E1 = C1. And we want to find M0 prime and M1 prime such that when XORed with E0 you get C1, and when XORed with E1, you get C0. We can calculate M0 prime by first calculating E0 by reversing out the XOR and then XORing E0 with C1 to get what we want. So, in the code, here is a swap blocks routine that calculates the 2 message blocks needed by first calculating the eblock by reversing out the XOR and then applying the XOR on the eblock with this swapped cipher block and then returning both of them. Here we get the inputs used for the cipher, which we then calculate. These next lines grab the 4 blocks we need, and then we calculate the new message blocks, which then get replaced into the message, which is returned. And then to test this, we call test, and it worked. For this problem, we've calculated a sequence of numbers. There's a pattern in this sequence, and your job is to find the next entry in the sequence. As with the challenge problem in Unit 1, if you get the answer, please don't share it so that other people can have the chance to find it for themselves. This question tests how well you understand the Diffie-Hellman key-agreement protocol. The first two statements are the security of the scheme depends on it being difficult to solve a^x = b mod n for a given b, n and x. The security of the scheme depends on it being different to solve a^x = b mod n for x given a, b, and n. The last two statements are if the value used for g in the Diffie-Hellman scheme is not a generator or a primitive root, the participants might not agree on the same key. The last statement--if the modulus n is not prime, the Diffie-Hellman scheme would still be correct, meaning two participants produce the same key, but it might not be secure. Check all that apply. For this question, the second and the fourth options are true. Taking a look at these, the first one is not true because in the Diffie-Hellman scheme a is public, and so in no way does the security of the scheme depend upon a. The second one is true. To solve for x is the discrete log problem, which in lecture we discussed to show that this is hard. So the security of the Diffie-Hellman scheme depends on this being difficult. The third one is an incorrect statement. We can see that this statement is correct regardless of g. So even if g is not a generator, the two participants will produce the same key. The fourth statement is correct. The security of Diffie-Hellman is related to the difficulty of solving the discrete log problem. For some non-prime numbers the discrete log is not hard, but yet it would still be correct. This next assignment deals with primitive roots. Your task will be to implement a function, primitive<u>roots,</u> that takes in a small prime number and outputs a list of all the primitive roots of that number. In the ide, in addition to primitive roots, we have provided a modular exponentiation function similar to what was used in lecture and two simple test cases to use. Here is my solution. It's a fairly straightforward brute force search. First, I wrote a function that return if a number r is a primitive root. A loop through all the possible exponents to see if the result has been seen before in our set s. If it has, we return False. If no repeats were found, we return True. Using is<u>primitive<u>root, we then loop through all the possible options,</u></u> checking if it's a primitive root and appending it to our list. When we've looped through all the possible options, we return our list. The fast module exponentiation technique used in lecture suffers from an important security flaw. The time it takes to execute depends on the value of the power, which may be secret. This means an attacker who can measure precisely how long encryption takes can learn something about this key. Assuming a simple model of computation, where modulus and multiplication by 1 or 2 is free, which costs essentially nothing, but all other multiplications cost 1 unit of time, which of the following module exponentiations would take the longest? One way to solve this problem is to add a count to the modular exponentiation routine given in class, but I'd like to discuss another way of counting the number of multiplications. First, I want to observer that when a number is represented in binary, it's easy to tell if it's odd or even. All you have to do is look at the last digit. For example, 10 is 1010, a 11 is 1011. Dividing by 2 is a shift right. For example, 20 to 10 to 5 is shift and then another shift. In our modular exponentiation routine, if our exponent is even, we divide by 2. It costs 1 multiplication. If our exponent is odd, we subtract 1 from it, which will make it even, and then we divide by 2. If our exponent is odd, it costs 2 multiplications. From this, we can easily see that for every 1 in the exponent when written in binary, we do two multiplications, and for every 0 we do only do one multiplication. Writing our four exponents in binary, where 1023 is--that should hopefully be ten 1s-- gives us a cost of 20; 1015 has a cost of 13; 4096-- which should be 1 followed by hopefully 12 zeros--gives a cost of 14; and 4097 gives a cost of 15. So 1023 is out most expensive operation, and that is our correct answer. In lecture, Dave introduce the Rabin-Miller test for primality. For this assignment, I want you to implement this test. The Rabin-Miller function your going to write will take in a number that may be composite or prime and target, which defines the desired probability level. The default value 128 corresponds to a probability of 1/2^128. Two useful routines are the module exponentiation function that we've been using throughout this unit and randrange, which is a function of the Python library, that returns a random integer a random integer between start and end. I haven't provided any tests, but you should be able to find some prime and composite numbers to test with on your own. There is some randomness, so there is a small chance the test will return True for a composite number, but if your target is set high enough--and 128 should be pretty high. It's very unlikely that this will happen. Here is my solution. First need to calculate t and s such that n equals 2<b>t s + 1.</b> I've written a helper function to calculate t which factors out 2s to calculate the appropriate value of t. Then to calculate s we divide. N<u>tests is the number of tests needed to get the desired probability</u> then in this loop I randomly pick an a between one and n and see if I've tried it before. If I haven't, I break out of the loop and add it to the side of things that I've tried. Then we run our first tests, which is to raise a to the power of s, mod in, and see if it's equal to 1. If that's true, we found an a and can continue to search. If it fails the first test, we try the second test, which is raising a to the power of 2^(j<i>s)</i> for all j's between 0 and t. If it passes both tests for all randomly chosen values of a, we return True. In this problem, you will be given intercepted messages from a Diffie-Hellman exchange between two people, including p--a large prime number, g--a generator, g of a mod p where a is Alice's private key, g of b mod p where b is Bob's private key, a message which has been encoded with g of a b mod p as the key. You will also be given timing information from the modular exponentiation as described in a previous question. Your job will be to use the side channel to determine the key and decode the message. This question is on Public Key Cryptography. Suppose Alice has a public and private key pair and that Bob has a public and private key pair where we have the subscript A for Alice and B for Bob. Alice wants to send a message to Bob in a way that protects us from eavesdroppers and proves to Bob that the message was generated by Alice and intended for Bob. And so the question is what message should Alice send. Check all that apply. Of these four options, the third and the fourth ones are valid solutions. The first one doesn't work because an eavesdropper could use Alice's public key to decrypt the first part of this message and read the message. The second one isn't possible because Alice doesn't have Bob's private key. For the third one, only Bob can decrypt the entire message with his private key and then he can use Alice's public key to get the message and verify that it came from Alice. For the fourth option, any eavesdropper can reverse the first encryption using Alice's public key, but cannot decrypt the full message without Bob's private key. So both of these will work. In this question, I want you to calculate (831). The (831) equals 552. We can calculate this by seeing that 831 = 277 3 and 277 and 3 are both prime. We showed in lecture the (pq) = (p - 1)(q - 1). And so the totient of 831 = 276 2 which equals 552. For this question, we will use toy-sized values to demonstrate RSA Encryption. You'll be given a public key where e equals 79 and n equals 3737 and a private key where d equal 319 and n equals, again, 3737. If we received an encrypted ciphertext of 903, what is the plaintext? The plaintext is 387. Recall that encryption involved raising a message to the power of e mod n and decryption involved raising a ciphertext to the power of d mod n. And so to decrypt to ciphertext 903, we just take it to the d power mod n and get 387, which we can see in Python. And, if we want to check our answer, we can encrypt 387 using a public key and we get 903--our ciphertext. In this problem, you'll be given Bob's public key along with two messages and their corresponding signatures, and your task will be to calculate a signature for the third message. In the code, here is Bob's public key. This is the first message--387. This is the signature for the first message. The second message--only two. And here's the signature for the second message. And your task is to create a signature for the message 774 and assign it to the variable s3. We've provided a test function for you to use to see if your value for s3 is correct. In this question, we'll introduce OAEP, which is Optimal Asymmetric Encryption Padding, OAEP is a padding scheme for use with RSA. It provides stronger assurances than the PKCS#1 scheme. In OAEP, we use two functions-- G, which is a hash function outputting g bits, and H, which is another hash function outputting h bits. We also have r, a random nonce of g bits. We then take our message m and append to it 0's such that the length of m' is g. We then calculate x by concatenating two parts, First part is our message XORed with the output of the hash of r. The second part is r XORed with the hash of the first part. We then encrypt x using RSA and send off our message. For this assignment, you've been given a ciphertext, which was encrypted using OAEP and RSA, using the given encrypt function. Your task is to decrypt the ciphertext and put your answer into the plaintext variable. Here is my solution. First, we need to decrypt using RSA and the private key. Before we can do that, we have to convert the cipher to an integer and then we apply RSA and the results needs to be converted back to bits and padded if necessary. Next, we break the message into two parts, G and H. We hash the first part and XOR it with the second part to calculate the nonce. We then hash the nonce and XOR it with the first part to get back to the original bits and, finally, convert the bits back to a string. Ill leave it up to you to print out the plaintext. For the homework, you were asked to calculate the signature of a message M3. The first thing to know is that M3 equals M1 times M2. Substituting M from here, we can apply the exponent to each determine the product and we can extend this out applying the modulus to each product. And M1 to the D power mod n is signature 1 and M2 to the power of D mod n is signature 2 and since in our first line, the signature was taken mod n, you take this mod n and this is our answer. For this question, we're just going to discuss threat models and the one-time pad from Unit 1. In order to know that a one-time pad provides confidentiality, which of these do we need to assume about the adversary? Is it necessary that an attacker has limited computational power? Is it necessary that the attacker doesn't know anything about the key? Or it is necessary that the attacker cannot modify the message? Check all that apply. The only one that is necessary is the second one-- if the attacker doesn't know anything about the key. A one-time pad is a perfect cipher, and as a perfect cipher, the computation power of the adversary is irrelevant. The first one is not true. The second statement is true, because if an attacker knows something about the key, she or he will know something about the message, which would compromise the confidentiality. It is not necessary to assume that the attacker cannot modify the message. Modifying the message only effects the integrity of the message, but that does not effect the confidentiality. One weakness of the encrypted key exchange is that passwords have to be stored. One possible improvement would be to encrypt using a hash of the password instead of the password itself. In the first step, the client sends some identifying information along with the first part of the key exchange, but this time it's encrypted with the hash of p. Likewise, in the second step, the server response is encrypted with the hash of p instead of p. We want to know does this work? The options are, yes, it provides further protection against a dictionary attack. Yes, but it doesn't provide much advantage since now an attacker who learns the hash of p doesn't need to learn p. No, it prevents the client and server from establishing a shared key, since the server can't invert the hash. Or, no, it has a serious security vulnerability. The answer is, yes, but it doesn't provide much advantage, since now an attacker who learns the hash of p doesn't need to learn p. The first statement is not true. If there was some dictionary attack using the password, it'd work just as well using the hash of the password. The third one is also not true. The server doesn't need to invert the hash, and a shared key can be established just like before. The fourth is not true. If there was a serious security vulnerability in this protocol, It would exist in the original protocol as well. For this problem you're given six certificates. The question is which ones would be useful to connect to udacity.com. Assume that we know ku,ca, the public key of the certificate authority, and for notation H is a hash function and E is RSA encryption. Check all that are useful to establish a secure session. Of these options, the third one, the fourth one, and the sixth one are useful. In the first one, the public key for Udacity is inside of the hash, and so we have no way of actually getting that. So that's not useful. For the second one, the encryption is done using Udacity's private key, so this is a self-signed certificate, which is not very useful for us. The fifth one is not useful, because the host name "udacity.commune" is not bound with the key. We would need to include that in the inputs of the encryption. The third and fourth ones are useful. We have our host name. We have our public key. Both of these are inside the encryption. For the sixth option, this is is a certificate chain. We have from the certificate authority the public key of trusty certs. The trusty certs signs the public key of Udacity and binds it to udacity.com. This question asks why do most signing schemes use SIGN(m) = m || Ekr(H(m)) instead of the SIGN(m) = Ekr(m || nonce)? The options are because hashing m provides more security, to improve efficiency, to make it so that it is not necessary to perform asymmetric decryption to validate a certificate, or to provide resistance to small message-space attacks. The answer is to improve efficiency. If message m is very long, RSA could be a very expensive operation, so we only want to run the encryption on the hashed message, which wold be smaller. The first option is not true. The security of this scheme depends on the encryption function, not on the hash function. The third option is not true. Asymmetric decryption is necessary in both schemes. The fourth option is not true, because we haven't done anything to increase the message space. On this problem, we will look at a small onion network. We have a network with five nodes--R1 through R5-- where each node has a public and private key. The question is, if Alice wants to send a message to Bob through our onion network, what message will she send to R1 to reach Bob? The notation here is the notation that we've used throughout the rest of the course where KUB is the public key of Bob and this message is encrypted then using the public key of Bob. This question is about voting and mixed nets. Consider a mix net with 4 non-colluding mixers and 1000 voters. The question is, what is the maximum number of votes that can be audited for each mixer while keeping the probability that an observer who can observe all network traffic can trace Alice's vote less than 0.001? The discussion of onion routing in lecture left out many important aspects needed for a practical and secure onion routing network. The following questions are based on this paper on TOR. A link to the paper will be included in instructor's comments. Which of the following topics from this class are used in TOR? Counter-mode encryption. Cryptographic hashing. Diffie-Hellman key exchange. RSA encryption. TLS. Check all that apply. In TOR, to improve performance, instead of routing each packet using a new onion routing path, TOR sets up a circuit between two endpoints to allow further communications to only require symmetric encryption operations. And so the question is--what could go wrong? And the options are--if the onion routers at the beginning and end of the circuit collude, they are able to identify the communicating endpoints by looking at the circuit identifiers. If the onion routers at the beginning and end of the circuit collude, they may be able to identify the communicating endpoints by manipulating the timing of packets in a flow. If an onion router in the middle of the circuit is malicious, it would be able to identify and disrupt Alice's traffic by identifying the symmetric keys used. And the last option--an adversary who can cause arbitrary congestion on the network and who knows someone is sending a steady flow of packets to endpoint B, may be able to identify all the onion routers in the circuit. Check all that apply. This next question asks about the handshake protocol in TOR. In the handshake protocol, which is based on the Diffie-Hellman key agreement, Alice sends Bob g^x encrypted with Bob's public key and Bob responds with g^y and the hash of K plus the handshake where K is g^xy, which is the shared symmetric key. The question is, why does the response include the hash of K and the handshake. Check all that apply. For this question, Alice has created a circuit with onion router 1 and onion router 2, and she wants to extend that to include onion router 3 and onion router 4. And the question is, how many more total messages are transmitted? Count one for every time a message is sent over a single-hop link. To help you get started, the first two messages are Alice sends a message to onion router 1 to relay on c1 where c1 equals circuit 1 to relay on circuit 2 to extend to onion router 3 and continue on with the message. Onion router 1 sends a message to onion router 2 to relay on circuit 2 to extend to onion router 3. And so on. So how many messages do we need? On the next flyer, there'll be a list of threats, and we're asking you to order these by the seriousness of the threat. For the question, assume Alice is communicating with Bob using TOR and wants to keep this a secret. Alice uses the TOR protocol to set up a circuit between onion router 1, 2 and 3, and uses onion router 3 as the exit node to reach Bob. Answer by listing the letters in order from the most severe threat to the least severe-- for example--dacbe, which is not the correct answer. We'll give you that one for free. Do note that some of these are subjective and multiple orderings may be considered correct. And here are the six threats--the first one, An attacker in the middle intercepts the Relay Extend request between onion router 2 and onion router 3 and attempts to set up a different Diffie-Hellman key. The second threat, a malicious attacker controls the directory server from which Alice learns the identity keys of onion router 1, onion router 2, and onion router 3. And the third threat, Alice's Onion Proxy is compromised. And the fourth threat, an eavesdropping attacker can observe all network traffic. And the fifth threat, onion router 1 and onion router 3 are both controlled by the same malicious attacker who has no other access to the network. And the final threat, onion router 1 and onion router 2 are both controlled by the same malicious attacker who has no other access to the network. Order these threats from the most severe to the least severe. And the answer is the first message is the only valid message. The message is no good because its encrypted using the public key of node 5 and so node 1 won't be able to decrypt it. The rest of this, all use the public key of node 1. The third option won't work because there is only one hub. And because there is only one hub, node 1 can see that the message came from Alice and is going to Bob and this is what we dont want to know it to be able to learn. The second option won't work because after node 1, message is directed to go to node 3 which is encrypted using a public key of node 4. And so node 3 won't be able to decrypt the rest of this message. The first option works, its encrypted using the public key of node 1, so node 1 can decrypt it. Its directed to send it to node 2 and its encrypted using the public key of node 2 and so that hub work. Node 2 is then directed to send the message to Bob which is encrypted with the public key of Bob, so Bob will be able to decrypt the message and receive Onions Galore. And the answer is 177 audits. To calculate this, say we audit N votes, then the chance of tracing Alisons vote on one mix net is N over 1,000 and to trace it through four mix nets the probability becomes and over a 1,000 to the fourth power, and we want this value to be less than 1 over 1,000. Rearranging this means we want N to be less than 1,000 to the 3/4 power which we can calculate in python to get 177.8. So N equals 177. Through this problem, you're asked to verify a set of blinded messages, where blinded message B equals the original message M times R random nonce raise to the power E mod n. And so the question is given the random nonce R, how do we get M from B. There are two very similar ways to accomplish this and Ill talk about both of them. And the first way, the first step is to calculate the inverse of R mod n, I would like to talk for a second about what this means and how to calculate it. So in normal arithmetic not modular finding the inverse is easy, we can just divide by R. As a simple example, we have 3X equals Y, you can find X by dividing with 3. But with module arithmetic, this doesnt work. Here are some example. 3X equals Y mod 5 and so Y equals 2. If we just divide, we get X equals two- thirds which doesnt make sense. X has to be an integer. It turns out the answer is actually 4 because 3 times 4 is 12, and 12 mod 5 is 2. And so we need to find a sum Z such that 3 times Z is congruent to 1 mod 5. Its fairly easy to see that Z equals 2 accomplishes this. 3 times 2 is 6, 6 is 1 mod 5. So going back up to our original problem, you can solve 3X is congruent to Y mod 5 for X, writing it X is congruent to 2Y mod 5. So if Y equals 2, X is 2 times 2 which is 4. And for mod 5, here are all the inverses: 1 times 1 equals 1, 2 times 3 equals 6 which is congruent to 1, 3 times 2 equals 6 which is congruent to 1. And 4 times 4 is equal to 16 which is congruent to 1. And so we can calculate inverses using the extended Euclidean algorithm which is implemented in the PyCrypto library in crypto.util.number.inverse. So now that we have the inverse of R mod n, we can calculate our blinded message. You should calculate B to the power D which equals M times Re to the power D since E and D are the public and private keys RSA encryption, we get M to the power D and R mod n. If we multiply by the inverse of R, we get MD mod n, and if we apply the public exponent, we can get M. And the second way of solving this problem. First, take a random nonce to the power E and we call that R. Were going to calculate the inverse of R, if we take our blinded message times inverse of R, we get our message times Re times R since Re in capital R inverses they cancel and we are left with just our message. Comparing the two ways, the second way is quicker. It involves one exponentiation and one inverse whereas the first way requires two exponentiations and one inverse. And here is my solution. In this first step, we calculate what I call capital R big nonce. We use the inverse function from the PyCrypto library to calculate the inverse of our big nonce, modulus D, our N value. We can then remove the nonce from our bill by multiplying the bill times the nonce inverse. Again, modulus N returning. So N the verify function itself, we loop through all the bills and nonces remove the nonce, check the value where message value does give inversions to change message versus an imager into bits. Make sure the padding is right. And then convert the bits to a string and then check the value of that string. We then see if the test value is equal to our target value, if it's not return false so you don't need to verify any more. If we've passed all of these, return true. And the answer is all of these topics I used in TOR. Counter mode encryption is used for the cells to relay messages. Cryptographic hashing, Diffie-Hellman key exchange and RSA encryption are all used in cell creation and key exchange and TLS is used by all the onion routers to maintain connections between each other. The correct answer is the second and fourth options. The first one is non-issue because the circuit identifiers are remapped to between each onion router and so there is no way for the beginning and ends onion routers to collude. The second statement is an issue. And Section 7 of TOR, the second generation onion router paper, the authors discussed this issue in talking about end-to-end timing attacks and introducing timing into messages. The third item is not an issue. Different keys are used between each router. So there is no way for Melissas router to identify and associate a key with Alice. This fourth item is an issue. To see this, lets take this example network between A and B and weve chosen a circuit in red. An attacker can try introducing congestion on each connection, for example, here and see if it causes any disruption on the flow of packets into B. In this case, it won't. And so the attackers learned this connection is not involved. You can track here and observe and he should see disruption of packets into B and the attackers learned that these two onion routers are on the circuit. You can continue on to do this on all the connections and learn which onion routers are involved. And the correct answer is the second and third options. The first one is not true. Alice can determine the key using the typical Diffie-Hellman protocol as discussed in the Unit 3 by taking the value G to the power Y and raising it to the power X. The fourth option is not true; adding the hash to the message actually increases the size of it. To see whether second and third options are true, lets take a look at what could happen without including the hash. So Alice picks the value X, calculates G of X and encrypts G of X with Bobs public key. She then tries to send this to Bob. Now, you could intercept the message, then send a different value to Bob, Bob recalculate the key, G of X prime Y. Bob would then send G of Y which Mallory could intercept and send a new value to Alice. Alice would then calculate a non-sense key and have no idea that the key she has calculated is worthless. Adding the hash value of the key to this protocol allows Alice to verify that she has a valid key that came from Bob and not from some attacker in the middle. And the answer is 14 messages. And the first message Alice is essentially telling onion router 1 to tell onion router 2 to extend onion router 3. Then onion router 1 tells onion router 2 to extend onion router 3. And the third message onion router 2 tells onion router 3 to create a circuit. Onion router 3 responds onion router 2 saying Ive created it. Onion router 2 responds back to onion router 1 saying Ive extended. And onion router 1 responds back to Alice saying that Ive extended. To then extend on to onion router 4, there will be two new messages between onion router 3 and onion router 4. So takes six messages to extend to onion router 3 and eight more messages to extend onion router 4 to get 14. So as Ive stated in the problem, some of these are subjective, but here is my answer. I believe that the most severe threat is if Alices onion proxy is compromised. if an attacker has access to Alices onion proxy, if Alices onion proxy is compromised, an attacker can learn all of Alices traffic. The least severe threat is option A. If an attacker in the middle intercepts the Relay Extend request between onion router 2 and onion router 3, in attempts to set up a different Diffie-Hellman key, the hash check will detect problem. And so I think this is the least threatening between E and F, I think that E is more threatening than F having control of the entry and exit nodes is more powerful than having control of this just the entry and some node in the middle. And so now we need to figure out where to put D and B. I think that D is a weaker threat than F just being able to observe network traffic is not very useful if you cannot correlate anything. Having control of two routers will give an attacker some ability to correlate traffic perhaps through timing attacks. So D is a weaker attack than F. I think that B is a more threat than E. If an attacker controls the directory server, it could possibly return only nodes under its control. Then having control of three nodes is a more serious threat than having control only two nodes So my answer is C, B, E, F, D and A. Welcome to Office Hours for this week. I'm here with Adam Sherwin. Job is sick today, so Adam has kindly stepped in. Adam is one of our new TAs. He is going to be TAing a statistics course that we'll be starting fairly soon. Thanks, Adam. What questions do we have for today? Alice asked about the math used in Unit 3, specifically what is z sub q? This is a good question. I have been fairly informal with the math that we use in this class. I want to focus on the fun parts of crypto, but really to understand things it's important to understand the math deeply. I will put a link to some great free notes from an MIT course that goes into a lot of detail on the math that will have a good definition of this. On the specific question of what z sub q is-- this means the group of integers over multiplication. If q is a prime number that is all the numbers from 1 up to that prime number minus 1, and a group is defined by the set of elements and operations. So we use the numerals that we understand to show the members of the group, but they could be any symbol. What matters is what the operations do on those elements. The multiplicative groups on integers have the properties that correspond to what we think about integer multiplication having, except for modulo the size of the group. When it's a prime, these are very useful for cryptography. That's what I mean by z sub q. It's the multiplicative group of integers, so it's the numbers from 1 up to q - 1. When q us prime that's the same thing as z<i>q, which is what you'll sometimes see written.</i> Fernando has a question about the DH key distribution. He thinks the sets A and B should have primes less than p. Is that true? In Diffie-Hellman we have the exponents a and b that are picked. They don't have to be related to p other than if p is very small there is no point picking really large values for a and b. If p is really big, it's silly to pick small values of a. The exponents are usually recommended to be at least 256 bits. That's because there is an attacker where the work is related to the square root of the size of the exponent rather than to the actual exponent. If you want 128 bits of security, you need a 256-bit value. You need to select a as a random value up to 256 bits. That means if p is too small your attacker will go after p instead of focusing the attacker on a. There is no particular relationship between the size of a and the size of p. He also asks another question about Diffie-Hellman. He wonders why in practice it is less accepted than RSA. This is a good question. They're both actually widely used in practice. RSA is more widely used and more talked about. They do different things. It's not a question of one being thought of as being more secure than the other. The security of both depends on quite similar problems-- Diffie-Hellman depending on the discrete log problem, RSA depending on factoring. There is no reason to believe one of those is easier or harder than the other. The property that Diffie-Hellman gives you is just this key agreement property. RSA can do more things like digital signatures and encrypting messages. That's a more versatile cipher. You can do more things with RSA. All you can do with Diffie-Hellman is agree on a key, but there are ciphers built on top of the same ideas that Diffie-Hellman that can provide the same properties as RSA. Megan has a question about security in the real world. She saw an ebook company that encrypts the ebook using as a key the person credit card number to place reliance presumably on the user to decide whether it's safe to share it. This sounds like a really bad idea to me to be embedding personal information in the ebooks. The goal here is to watermark, to have the digital content specific to each user in a way that if they distribute copies of it--they put it on a file-sharing site or they abuse the copyright in someway-- that it can be traced back to them. There has been quite a bit of work on using cryptography to make watermarks, especially in videos but also possibly in books and images. You can embed something that makes each image unique and as hard to remove without breaking the image. This is probably much harder to do for plaintext than it is for video just because you can't really modify the text, but you could do things that-- and this gets a little towards stenography where you're hiding a message inside something that looks like an unencrypted document. You could be doing things where you are varying the spacing or you're changing things that don't really affect the content. Or, in the embedded information in the ebook that you don't actually see when you read it, you could be hiding some extra information which could embed the identity of the person who bought the book. That would be possibly easy to strip out, so that's something that you need to worry about. If you want to use a watermark--in this case it seems like it's a really improper way to treat the customers if they're not knowing this is happening and especially if it's their credit card information that could be used in a way that's abusing the user. There are legitimate reasons to want to do watermarking of digital content like this and cryptography provides a great way to do it. The challenge if you're doing it for something like video is you can do a lot of things to modify the video that might get rid of the watermark. Simple things like using the lowest bits in each pixel to encode the watermark-- well, those are really easy to remove, because you can change all the lowest bits in the pixels without changing what the video looks like. So there are lots of clever schemes that try to do things that are hard to remove, like changing the timing of black screens between scenes and doing things that are more pervasive in the whole image but still don't make the content broken. There are lots of social issues here about whether you can secretly track people and whether that should be illegal or encouraged or what kind of authorization you need to do that. Then there are interesting crypto issues of if it seems acceptable to do this in some context, like when a Hollywood studio is distributing a movie. They have a pretty good reason and good understanding of the people they distribute to that they want to keep close track on it and find out if anyone is leaking copies early to put a watermark in those things--to find a way to do that cryptographically that can't be easily removed and doesn't damage the content in ways that are noticeable. Vipal De Anshu asks about the challenge problems. He wants to know if you can give more explanation on tips and techniques and tricks to help students solve them before the final to help them prepare. This is a tough question. There's no magic way that I can give you that's going to really solve this. The way to learn to solve challenges like this is really to practice and to solve increasingly difficult ones. Doing cryptography depends on persistence in a lot of cases. It also depends on creativity and being open-minded. And often people think a challenge is going to be really hard, so they don't try the easy things. So definitely make sure that you understand what the challenge is asking for and think about the easy things first before assuming that you have to try something really complicated. Also a big difficulty is deciding are you on the right path and should you keep trying that, or are you stuck in a rut and you need to try something different? This is one of the things that makes cryptography hard. The challenges in this class are all contrived challenges. They are intended to give you a sense of what it would be like to break a cryptosystem that's not contrived to be breakable. But that's a little different, and that means it's a challenge for us to construct ones that are good. I know there was a lot of grumbling about the challenge in Homework 2 that many people spent a lot of effort on it trying to do complicated things to break it and learning about random number generation and the different ways to do that. And I hope you found that worthwhile and got something out of it. It turned out the actual sequence was quite easy to break if you looked at it in the right way. It was not intended to be quite as easy as it was, and I apologize for some of the obvious flaws in it. I won't say specifically what they are now because I know some people still want to try the challenge on their own. But that was a mistake on my part, and I'm sorry that it turned out to be as trivial once you looked at it in the right way as it was. But it was intended to be something where you could break it without doing a lot of research into other types of random number generation schemes. That wasn't the intent of the question. The intent was if you looked at it different ways-- looking at the bit patterns, looking for any kind of patterns in the sequence-- that would be what you need to break it. We will have some interesting challenge questions for Homework 4 that will scale up to levels of difficulty. I think one of the things to get better at solving challenges is to have the practice of solving ones. You build your confidence as you solve easier challenges and move on to harder ones. So what the challenge for Homework 4 will be, there will be some that we give you some pretty good hints about how to solve and we think will still be hard and involve an interesting challenge to solve them, and there are others that we're not going to tell you about what to solve. And we're not going to tell you which ones are the easy ones and which ones are the hard ones. This is going to give you a different kind of experience solving a challenge. I hope people enjoy it and get something out of it. Definitely to be able to attack it, the first step is to understand the underlying crypto behind it, and that hopefully will come out of the lecture and solving the other problems. But the short answer--and I'm getting a little bit rambling because I don't have a good answer that says if you do X, Y and Z all of a sudden challenge problems will get easy and you'll be able to break all these known cryptosystems-- it really takes practice. It also takes sort of having an open mind and thinking about different things to try and creativity, but the way to learn is to practice and to try to solve increasingly hard challenge problems. The other thing that is really the way to learn is to work with other people. If you can do that with someone in person who's experienced at solving these kinds of problems, that's really the best way, to have that kind of back and forth. If you can do that on the forums, that also is great. And so the challenge problems that have worked well, we've seen a discussion in the forum where people have suggested the different approach they have tried. Other people might add to those or suggest an alternative. Those are the things that will really help you develop in terms of solving problems like this--having that kind of a discussion and learning about approaches other people take but still putting the work in to try to think of ways to solve it yourself. So I hope you'll enjoy the future challenge problems and they will give you a good sense and build your confidence in solving them. I realize that it can be frustrating, and that's part of the difficulty is if you knew you were on the right path and making progress every step, that would be great. But that's not usually the way things work in cryptography. Hello, and welcome to our first question and answer session. I'm Joe, the assistant instructor of the class, and I'm here with Dave, our instructor. Hi, it looks like there's been a lot of activity on the forums and some interesting questions to answer, so let's get started looking at some of those questions. Our first question comes from Insa, who wants to know how many of the questions on the future problem set and the final exam will be primarily programming-based questions, and how many will be primarily mathematics based? It's going to be a mix. Cryptography is really a branch of mathematics, so there's a lot of math in crypto, but we also want to build things, and we are interested in programs and the computing aspects of that, so there will be questions that either require you to write a program or more often questions where to solve them well you probably have to write a program, but the goal is to solve the question, not to write the program for its own sake. There will be a mix. There will be a few questions on each homework that do involve writing programs. Many that don't, though, and those will be more involving the mathematics. In lecture, you mentioned that the one-time pad is the only perfect cipher. TimeZombie writes wanting to know why this is true. I probably overstated things a little bit saying this is the only perfect cipher, that you could certainly construct ciphers that are perfect and are not exactly the same as the one-time pad as we've described it. But they'd be essentially the same, and what Claude Shannon proved is that to be a perfect cipher, you have to have the key space as large as the message space, and you can never reuse a key, so every byte of the message corresponds to a byte of the key, and then with the one-time pad, we're XORing that we could do some other operations that would be equivalent of XOR and have another cipher that would be perfect. But the important thing that was proven is that if the key is smaller than the message, there's no way for the cipher to be perfect. There must be some information leaking to an attacker that potentially could be used to break it. Okay, and then as a followup to that, he also wanted you to comment on quantum cryptography. Okay, so quantum cryptography is a very interesting topic, and there's really two sides to that that people think about. The first is quantum key distribution, which is to take advantage of properties of quantum physics to be able to send a message where even if someone intercepts it, they cannot replay it, and that means that the two parties at the end can know that if they establish a key, if that message got through between them and they can verify that they received the right message, that there was no physical way it could be eavesdropped on because the properties of quantum physics are that if you observe properties about a photon, you can observe some parts of it, but you can't observe the whole thing. You can observe, say, both the momentum and the spin at the same time, and I'm not an expert in quantum physics, so that may not be exactly right but you can't observe--and this is Heisenberg's Uncertainty Principle-- you can't observe every property of the photon at the same time, so you can set things up, and I won't go into details of that now. We may talk about that in one of the--maybe Unit 7. I think that would be a good topic to bring up again and go into a little bit more detail. The other interest is in quantum computing, which potentially offers a way to solve problems that we can't solve with classical computers in an efficient way. And the most interesting question there is factoring, which in Unit 4 we'll talk about the importance of factoring being hard for breaking RSA for the security of RSA relying on it being hard to factor numbers that are multiples of 2 large primes. There are ways--if you have a quantum computer--to do factoring efficiently in a polynomial number of steps which no one knows the way to do with a classical computer. If it was possible to build a large quantum computer, which no one has been able to do yet but in theory could be done, then you could break crypto systems like RSA that depend on factoring being hard because you could factor large numbers using your quantum computer. Both of those are sort of interesting aspects where the sort of deep properties of quantum physics can be used either to the advantage of making something more secure or to the advantage of the attacker. And we'll talk about the impact of quantum computing on factoring a little bit in Unit 4. Okay, cool. I look forward to it, thank you. Now I'd like to talk about the open discussion question posted in the Homework 1. To review, Alice wants to send a message to Bob using a one-time pad to let Bob know yes or no if she's going to be taking CS387, and they're worried about Mallory, who could possibly intercept the message and then change around the message so that Bob would get the wrong message. And we saw in the question that Mallory could-- whatever she intercepts, she can't tell whether it's yes or no, but she can XOR it with the difference between yes and no, and that will flip the answer and some of this suggested-- solutions were to change yes and no to other things thinking there was something special about yes and no, that those could be flipped. But you can flip anything, and you're flipping it by XORing it with the difference. Whatever was there was either yes or no XORed with some key. You don't know the key, but XORing that with the difference of them will cancel out the Ys, or if this was an N, it would cancel out the Ns and leave the Y. So whatever the message was, it can be XORed with the difference. There's nothing special about Y or N. It would work--let's say--if you replaced it with yes and not. The difference is just Mallory would XOR it with the XOR of yes and not. All of the solutions that were ideas about changing the content don't really solve anything. Mallory knows the 2 options and can XOR them and can flip the content. There are some other approaches that had better success, though. There were 2 main styles. For example, Steve Krenzel from Seattle suggested using kind of more advanced crypto tools. He wanted to append the nonce to the message and then hash the entire message. So, I think what he was suggesting--which is a good solution to this-- is to take the yes or no, so you're still giving the response. You're going to concatenate that with the nonce, which is just some random value. We'll call it r. This is long random value. And then concatenate that with a hash of the y, whatever the answer was, with the nonce. And we haven't talked about hash functions yet in Unit 1, but we do talk about them in Unit 2, and a hash function is just a function that takes some input and maps it in a complex and unpredictable way to some other value. And that's useful because Bob can't predict what this value is without knowing what the value of r is, and if the value of r is random and not known to Bob, there's no way for Bob to predict the output of this hash. If he tries to change this, well, there's no way to change it in a way that knows the difference between what the result would be if this was an N and when it was a Y, so Bob could flip these bits, but then the hash value wouldn't be correct. I'm sorry, Mallory could flip these bits. Then the hash value wouldn't be correct, and Bob would know that it was wrong, so this is a reasonable way to do it. The difficulty with the solution is it involved cryptographic tools that are much more complex than just the XOR hash functions are, things we'll talk about in the next unit but involve a fair amount of computation. It's not something you could easily do by hand. What we want is some solution that would allow Alice and Bob to do this without using any more advanced cryptographic tools. Yeah, the other style solution was one proposed by StirCat, which involves sending a number of dashes--maybe 8 or 16 dashes in a row-- and then randomly picking one character to set to Y or N, and what are your thoughts on that? This is a really interesting idea. We're going to take--instead of sending just one character, we've got a string of 8 dashes, and we're randomly going to pick one of those and replace it with either the Y or the N. That's going to be the answer. And now this means when Mallory intercepts it, if she doesn't know which one was randomly picked, well, she can XOR out the difference between a Y or N from any one of these 8, but if she picks the wrong one, she's going to change a dash into something else. And I don't know what it would be. We'd have to look at the numbers. But it's not going to be a dash anymore, and so when Bob looks at the result, it's going to look wrong. Bob will know the message was tampered with. It's only if Mallory was lucky enough to guess the right one to flip that she could change the answer, and her probability of guessing it correctly is 1 over the number of positions here. If we have 8, she has a 1 in 8 chance of guessing the right one and flipping that one. The other times when she doesn't guess correctly she'll get caught. This is pretty good. This has a nice property. It means if we make a message long enough, we can make Mallory's success probability as low as we want, but we need a really long message. If we want her success to be less than 1 in 256, we need a string that's 256 bytes long, which is not a big deal but is less elegant. We could get a similar property with a shorter message if instead of having only one of them depend on the choice we make a short message where each of the characters in the message depend on the yes or no value, but Alice randomly picks either to send a Y or a 1 if the answer is yes or to send an N or a 0, so let's say those are the 4 choices. She's going to randomly pick. If the answer is Y, she's going to send something like YY1Y, randomly picking for each position whether to send a Y or a 1. When Mallory intercepts this, the XOR between Y and N is different from the XOR between 1 and 0. Those two are not equal, so for Mallory to flip an answer, she's got to guess whether it was a Y or an N or a 1 or a 0, and she can guess and flip one. If she guesses them all correctly, then she could flip them all, and Bob would see the wrong answer. But if she guesses the one wrong, then Bob will see one of the characters in the string doesn't match, and so now Mallory's probability of guessing them all correct is  to the power of how many there are, so if there were 4 characters, it would be  to the 4th or 1/16, but this is a much lower success probability than we had having the string of dashes where Mallory only had to guess one position correctly out of that whole length. Now she's got to guess each position correctly with  chance for each time. This just involves Alice picking randomly, so we didn't need to add something beyond the one-time pad. Alice has to be able to flip a coin or some other source of randomness to decide whether to send the Y or the 1 for each of these 4 positions or more positions if she wants the success probability to be lower than  to the 4th. Could you also do a lowercase y and a t for true and a lowercase t? You could, yeah. Probably what you'd have to be careful about is if you did a lowercase y and a lowercase n. The XOR of those might be the same as the difference between the uppercase Y and uppercase N. I would guess if you did true and false, those would be different, but you should be careful and know that the XOR is not the same. But as long as the XOR is different, then each byte Mallory has just  chance of making a change in the right way. Okay. I think that's all we have. Cool, so thanks for joining office hours, and thanks for all the discussion. It looked like we got a lot of good responses to open discussion questions, so hopefully we'll see that again in Homework 2. Thank you. Thanks. Welcome to office hours for week two. What questions do we have? Wolfgang from the forum writes asking about random oracles and wanted to know more why they're not realistic in practice. This is a good question. We use random oracles in ways to make it easier to reason about cryptography. The definition of a random oracle is typically you've got this magic black box that you can send an input to in some range, and you get as output a random value in the output domain. For it to be a random oracle, that value needs to be completely unpredictable. We can think about building something like this. You could have a lookup table where for every possible input you've produced using some good physical source of randomness the output for that input. Then you would do the lookup and get that output. That would behave like a random oracle that those inputs would be totally random. There is no correlation. The problem is if we try to make that function any smaller. So instead of having a table where for every input we can look up the corresponding output, we want something smaller. That's what we want with a hash function. That's what we want for any practical use of a mapping between inputs and outputs. We can't have a full table. That would be just a huge amount of memory. The random oracle turns out to be a useful device for reasoning about cryptography where we assume that this black box exists and has this perfect ideal behavior. It's not something we can actually build. We have to be really careful when we reason about cryptography using this assumption about a random oracle, and then we, say, through in a hast function that we think behaves sort of like one, because no hash function actually behaves like a random oracle. For our next question, Sayid Bashir wonders by looking at a ciphertext, is it possible to figure out the encryption scheme that is used? For most academic working cryptography, people assume they know everything about the scheme being used and are trying to find some mathematical weakness that allows you to solve it more quickly than you could by doing a brute force search on the key space. for a lot of real world cryptography, that's not the case. The first thing you've got to do is figure out the scheme being used. That's sort of like the challenge question from homework two where if I told you the scheme for the random number generation it would be easy to figure out the next one. That's more of a better model for most cryptographic work that academics do-- assuming you know the scheme and trying to find mathematical weaknesses in it. For a lot of practical cryptography certainly going back further but even today it's not the case that you know the scheme. If the scheme is good, then even if you know what's one of a set of known schemes, you shouldn't be able to tell from the ciphertext. The cipher text should be purely random. It should look like purely random values. Unless there is some weaknesses in the encryption scheme used, you shouldn't be able to tell what it is. But there are weaknesses in all encryption schemes used. This is particularly an issue if someone invents their own encryption scheme. They think, oh, it's going to be super secure since I've invented this custom scheme just for my own use. Those schemes almost always have weaknesses in them. Even if the scheme is not known, make it much easier than breaking a known scheme where the key is not known. It should be the case that you can't tell anything about the scheme from the ciphertext. In practice, that's probably not the case unless the scheme is a good one. Yeah. I guess it kind of goes back to the importance of keeping your keys secure. This is going back to Kerckhoff principle, which is you should be able to have a cipher that-- even if the adversary knows everything about the scheme you're using-- as long as they don't know your key, you have a good argument why that is secure. Then the last question come from Pavel. He has a question about invertibility. It's not obvious why encrypting different messages with a fixed key cannot give you the same ciphertext. Is this necessary for correctness? The question if you have two messages that encrypt to the same ciphertext, when you try to decrypt them you can't know which one to get. This is why in order for a cipher to be invertible, we need to have the property that each ciphertext corresponds to exactly one input. There are probabilistic ciphers, so there could be more than one ciphertext that corresponds to the same input under the same key, but it can't be the case that there are multiple inputs with the same key that map to the same ciphertext, because then when you try to decrypt it, even though you've got the key, you don't know which one of those two messages was the input. We need this property that easy ciphertext can be decrypted to exactly one message. Otherwise decryption doesn't work.