Welcome to your final exam. First we're going to talk about a new version of the SIR model--one that includes births. Now, when we include birth, which means births of not immunized or susceptible children in immunological model new effects appear. Here we're going to try that out in the SEIR model. You might remember this in the earlier unit. This is just the SIR model with an added stage of being exposed to disease but not yet infectious. In other words, a person who is in the e category here is infected but can't yet spread the disease to other people. Now, eventually we use the birth rate which means adding people into the susceptible part in the mortality rate. All compartments S-E-I-R have the same mortality rate, however. So this means that the disease isn't life threatening since people across the world regardless of whether or not they are infected die at the same rate. This is a model that does have immunity. So that means we don't have the cycle as we did in similar other examples. Instead we just have a straight shot from S to E to I to R. Now as we did before, we call our total population N but in this case N is not a constant, rather N is a function of time. It's still equals the sum of S, E, I and R, but the sum is going to change overtime. Now since N has no other constant, we need to make sure that we compute the number of infections per day correctly. It isn't just going to be some constant times I times S anywhere. Let's build up the equation together. So we're going to start with the number of times any person comes into contact with any other people per day. So that's just equal to contacts per day. Then in order to get the number of contacts per day of a person with susceptible people not just any people, we modify this by multiplying by a factor of S/N. So this is just a fraction of the total population that is susceptible and of course, this is a function of time as well. Then if you want to change the expression to equal the number of infection spread by each infectious person per day, they multiply all these by the transmission probability. Now, lastly, to get the total number of infections per day, we multiply this by I, the number of infectious people. Now what we want you to do in the code is to implement the SEIR model using the forward Euler method. As always, we've given you the important constants that you'll need to use including the birth rate and the mortality rate, and of course, a set of initial values as well. And down here in the for loop, we've included the standard equations for the SEIR model, but we haven't taken into account births and deaths yet so that's your job. Good luck on the first problem of your final. For the second final exam problem, we're looking at something that set the outside boundaries of the material that we've looked at so far. We're going to investigate the decay chain of Uranium-238. This chain contains a number of different steps from one isotope down to the next. Say for example we have Uranium-238 to a string of other things that we're not going to think about right now until eventually we got to Bismuth-210 plus some radioactive radiation over here that we're not going to be concerned about. Bismuth-210 has a half-life of 5 days and eventually decays to Plutonium-210 plus again some radiation and this has a half-life of 138 days. After sometime, it decays to Lead-206 and you can see that in general, the average lifetime of the given isotope is equal to its half-life divided by the natural log of 2. If you want to figure this out, you're welcome to or you can just trust me that it's right. It'll be useful for us to think about how the number of atoms at any given sample changes every time. If you think that the initial amount of atoms is equal 100% of that amount and then t equals the half-life, we have half of those atoms left. Over here, well we see the atoms lifetime worked out. Of course, there are still some atoms left, but many of them are already decayed. Looking at the supplied code, we can see that they are actually ignoring the part of the chain that starts with Uranium-238. In the beginning, I set it with the Bismuth isotope. All initial values showed up all of the atoms are Bismuth at first and later the Plutonium and then the Lead. What we would like you to do is to the backward Euler-- remember backward Euler is not the same as forward Euler--to show how this decay process happens for each population of atoms. Now more hint, think about how the ideal logistic growth applies to the situation and how it's going to affect the different rates of change of these populations. In this problem, we'll return the spaceship, so I'm very excited. Now, similar to what we saw in units 1 and 2, we want our spaceship in this case or it's actually a satellite to get into orbit. However, this is a very special type of orbit. It's a geostationary orbit. What this means is that once the satellite is in space at a certain distance away from the earth, it will orbit the earth at exactly the same rate as that at which the earth rotates. So the people on earth, it's going to look like this satellite isn't moving at all, which is pretty cool. To show you how this happens, we have this lovely diagram right here, which I warned you is not to scale. In the center, we have the earth, and initially, the satellite is orbiting at just 200 km away from the Earth's surface. At a certain moment, in fact, at the moment when the rocket first crosses the x axis on this diagram, the rocket is going to spit the satellite out, and this is going to repel the satellite away from its original orbit and eventually out to the radius that needs to be at for geostationary orbit so this red line right here then it's going to stay along this red path for as long as it needs to. Now, the radius of geostationary orbit is 42,164 km. Also, just an interesting tidbit, this transition orbit, this green curve right here, is exactly half of an ellipse that is tangent to both of these circular orbits, the pink one and the red one. Also, these orbits are both perfectly circular even though my drawing doesn't really show that. So there are three tasks that we would like you to complete for this problem. The first of these is to find out what the speed and radius of that initial smaller orbit is. Remember, like I said, this is a circular orbit so that should make things a little bit simpler. You probably have to use pencil and paper for this part of the problem. Now, on the rocket and the satellite attached to it cross the negative part of the x axis for the first time. The rocket fires its engine to increase the speed of the satellite by the amount given in the variable called boost. This should remind you a lot at the last problem of unit 2. So for your second task, we want you to figure out how to make this boost happen and of course how to make it happen at the appropriate time. Let's go back a bit in the code to get your last task. This is going to be to figure out as you've done before. What the correct magnitude of the boost velocity is? We filled a dummy value of 42 just to sort out with, and I've given you several different options to choose from. One of these is the correct answer. Once you completed the first two tasks, plug in each of these values for boost and compare the path to see which one helps the satellite and the geostationary orbit. The fourth final exam question is another creative application as in concepts you've learned. We're going to deal with a three-body problem. Let's say that we've three stars--one with a small mass, one with the twice the smallest star's mass, and one very large star with three times this mass. Given the certain initial 3D configuration of these stars, we want you to figure out how to position and graph each change of time. One piece of information that's going to be very important is this equation as the gravitational force between two bodies, which you should remember from all the new units. Tremendous that this force is actually vector quantity. I'm going to be super explicit in putting a vector sign on top of that. Remember, all three of these bodies exert a force one way or another. In the code you can see that we've written out the mass at each star for you as well as the gravitational constant big G. We've dealt with everything concerning time for you and all left for you to do is to implement this symplectic Euler method for the motion of the stars we did write above. When that is symplectic Euler method if the kind of half implicit, half explicit method that'll help you find the position and velocity of each star. Just show your result and surprise you. This is a chaotic system. See it showed a pretty interesting looking graph. For our second class problem in the final exam, we're going to talk a little bit more about chaos theory especially in relation to the Lorenz Butterfly. We have some system in a state defined by x, y, and z. Now the solution that we might get for this system depending on how we define different parameters within the equations could look something like this. I'm not very good at drawing in 3D but this should look something like the classic butterfly image that Jorn showed you in the unit. Remember that our solution has two wings. One over here and one over here. Both spiraling outward. We can plot how z, this coordinate, varies as a function of time. And when we do this, we see a series of peaks that ascend and then descend and then reascend and so on and so forth. This set of peaks of increasing height corresponds to spiraling outward along one wing. And then as you jump down to a local minimum to stepping upward again over here corresponding to the spiraling outward along this wing. What we want to know is if we can predict the progression of this local maxima along the z axis. You can make a plot of the current local maximum of z versus the previous local maximum and then take each of these maxima and plot them down here. So here's a preview of what you should get as your solution to this problem. Up here we can see how the peaks in z vary with time. Clearly, these zigzags are a little bit more accurate than mine were. And down here we see how the present local maximum depends on the previous local maximum. You'll know that you've gotten the problem right if something like this appears. Now in order to plot each local maximum we do in fact need to know the exact z coordinate of that maximum. One issue that we have when using approximation methods, however, is that sometimes our time steps are not going to line up exactly with the locations of the peaks. So if you look at this visual for example at step-1 we have this value of z. At step, we have this value. And after one more time step, we've passed over the peak and downwards again. We plotted this on its own completely ignoring where the actual peak is and shows instead it's substantially lower. In order to approximate better where this actual peak is, we can pretend that there is a parabola connecting these three different points. The vertex of the parabola will be our estimate for where the true location of the peak is. To find out the t coordinate of the peak, you can just simply use the quadratic formula. Just a couple of hints for you. You want only one z value to be equals the maximum. So think about what that means in terms of our equations down here. Also, think about how you could use the essential difference formula when you're discussing this local maxima. Remember that one way to calculate a, b, and c is to plug in information about the points that you know. Then you'll have these coefficients and be able to find the proper value of z and t. Let's take a look at the code for a second. Now the first thing that we want you to do for this problem is to use Heun's method, which you remember from section 2.9, with a fixed step size and not a variable step size for the Lorenz system. Remember that Heun's method depends on the forward Euler method so we've included that for you right here. So again just to be clear, use Heun's method with nonvariable step size to figure out how x, y, and z are going to change with each time step. Once you do this, you're going to use the parabola fitting method that I just discussed to figure out better where the local maxima are. Once you've done that, you're going to enter your estimate at the local maximum value. This should be a fun look into chaos theory. Good luck. And now we have our final exam question or rather our final, final exam question. This one is about something called morphogenesis. This relates to how an organism knows where to grow its legs or where to form colored stripes on its coat and things like that. This really concerns pattern formation then. And Alan Turing of all people--what a brilliant man--had an idea that an initially only slightly random distribution of chemicals can evolve into a stable pattern over time. So we're going to look at one model of how that could happen. Let's say that we have two different chemicals. We have one that we're going to call the activator and one that we're going to call the inhibitor. Now the activator causes production of both itself and of the inhibitor and also diffuses slowly through the space that we're considering. The inhibitor on the other hand causes a reduction of both the activator and itself, and it diffuses very quickly through the space. So we can look at our graph over here. The x axis is just the space in general. And the y axis is concentration of either chemical. Now, if we consider what's happening in the curves inside this blue circle, we can see that where there's a little bit more activator than inhibitor the activator level rises due to diffusion. The inhibitor that is also produced then flows quickly away because this is fast diffusion, and it stops other peaks from developing nearby. We have equations for how the concentration of the activator and inhibitor change with time. We're going to call the concentration of the activator a and the concentration of the inhibitor b. This constant right here shows us that yes in fact the activator is very slow moving as opposed to this constant over here which shows that the inhibitor acts on a much shorter time scale. Back over in the equation for da/dt this factor of 1-a² limits the growth of a when a turns to 1 or turn -1. This is basically similar to logistic growth. Back in the equation for db/dt, you see that when a is greater than 0 since a term of just +a is added in, this forces both a and b upward. Since we have -0.7b, however, whenever b is greater than 0 this forces a and b both downward. Looking at the code, the final task of the problem is actually going to be to use the explicit finite difference scheme for this activator-inhibitor model that I just showed you-- the equations for da/dt and db/dt. However, before you get to that step, there are two other tasks that you must complete. Both task one and task two concern using periodic boundary conditions. Just to help you visualize this, I'm going to draw a little diagram. We're considering a grid of some length by some length, and it's made up of all these little squares just like we've seen so many times before. We know how the conditions in this square relate to the conditions in the adjacent square and so on and so forth. We can progress horizontally across the square until we reach the boundary. Now, however, we have a problem. What am I going to do with all the information that I have about this square? In the same way, where does the information about this square come from? Periodic boundary conditions say if I pretend to create one more square outside of the boundaries of the grid then this square is going to be identical to the first square on the opposite side where we started. We can do the same thing moving in the opposite direction horizontally and also in both directions vertically. So for task one and task two of this final problem, you're going to figure out what expressions are needed to ensure that these boundary conditions are put in place. In the end, you should come up with some pretty interesting looking patterns. I'm just going to give you a sneak peak of what you should end up with. So here's what you'll get with your final plot. You have one graph showing the concentration of the activator throughout the entire space we're considering and the other graph showing the concentration of the inhibitor. Congratulations on some awesome work in this course. I hope you've had fun seeing how many different kinds of problems could be solved using numerical approximation methods. I know that this course has taught me to look at the world in a totally different way. Great job on the final exam. Welcome to the first unit of our course on differential equations and simulation. In this unit and the next, we'll look into how to bring the crew of Apollo 13 back to earth. We start with writing programs that model the orbit of the moon and of the spacecraft around the earth. This unit contains quite a number of supplementary segments to bring everybody up to speed. If you know your way around derivatives and vectors and elementary Python programming, feel free to skip directly to the quizzes of those segments. Now let's take a closer look at what happened with Apollo 13. The rocket is already in a parking orbit around earth. To take us to the moon, the rocket is fired again, this will result in such a trajectory. This phase here is called "translunar injection." This trajectory is a so-called free trajectory. It's going to return safely without the astronauts doing anything about it. So in case anything breaks, they are going to return, no matter what, just by gravity. The first thing that happens was planned, namely, to get rid of the rest of the rocket, which eventually crashed on the moon. The second thing was planned as well. A midcourse correction, MCC. The objective was to take the spacecraft to a trajectory that's closer around the moon, but there was a price to pay. This trajectory no longer was a free return trajectory, but eventually they'd not return to earth. So that is an unsafe trajectory. Which everybody learned somehow was after that when one of the O₂ tanks exploded. Now the spacecraft had to be taken back to earth as quickly as possible. The solution was to fire the rocket again. This maneuver was called DPS-1 for distant propulsion system. The rocket that has been fired actually was the rocket that was intended to safely land on the moon. This has to be fired in such a way that we are taken back to a free return trajectory and eventually return to earth. What's going to occupy us this and next unit is how to compute the right amount of acceleration the spacecraft needs at this point in time. When we simulate the trajectory of Apollo 13 around the moon, which of these are of vital importance, which are somewhat important, and which are not important at all? The mass of the spacecraft, the size of the moon, the motion of the moon around earth, the size of the earth, the motion of the earth around the sun, and what about 3D? Should we be doing this in a 3-dimensional fashion? Astonishingly, the mass of the spacecraft is not important at all. Imagine that you have 2 spacecrafts going along the same trajectory. You could join these to form a spacecraft of twice the mass, and that would still follow the same trajectory. So the mass is of no importance-- the mass of the spacecraft, that is. The size of the moon. Well, we should not be hitting the moon, but otherwise the size of the moon is not of importance. The motion of the moon, however, is of vital importance. We have to take care that the rocket is fired into empty space and the moon eventually appears in the right place. The size of the earth is of vital importance to get close to the surface as we return. The motion of the earth around the sun isn't that vital. And should we be treating this as a 3-dimensional problem? Imagine the earth as a sphere and the moon as a sphere, and the trajectory takes us around earth, around the moon and back. This looks very 3 dimensional should we be dealing with xyz coordinates. Actually, however, this trajectory is mostly in a plane. If you look at that motion from above, we see planar motion, and it's almost okay to treat this trajectory in a 2D fashion. This is a supplementary unit in case you've forgotten how sine and cosine and their friends work. You may directly skip to the quiz to check your knowledge. Sine and cosine and their friends are about right triangles. This is a right triangle, a triangle that has a right angle; a and b are the legs of the right triangle; c is the hypotenuse. And I'm interested in that angle, which I call beta. Once this angle is specified, the ratio of every 2 of these sides is fixed. For instance, b divided by c is fixed once beta is given. That's called the sine of beta. And a divided by c is given once beta is known. That's the cosine. If we have a different right triangle, with the same angle beta, you see, in this case, the leg b shrinks at the same ratio as the leg c shrinks. So this ratio b over c stays constant. Same for a divided by c; a shrinks, c shrinks at the same ratio, this ratio, a divided by c stays constant. It's a function of this angle beta alone. In daily life, you would specify beta with angular degrees: 45°, 90°, 180°. In mathematics and in most parts of computer science, you do not use angular degrees. You use radians. The idea is the following: You look at the unit circle, a circle of radius 1. If this is the angle I want to express, 90°. I'm not using the number 90°, but I'm giving the length of the part of the circumference that I am cutting off here. The total circumference would be 2π, a circle of radius 1, so this would be π divided by 2. And that's the angle in radian. 90° becomes π divided by 2; 3.14 and so on, divided by 2. A very ugly number. If I want to specify 45°, that's of course just half of that number; π divided by 4. So 60°. What would that be in our new units? ½? Or is it 1 radian? Or is it 2 radian? Or is it 3 radian? 0.1 radian. How many degrees would that be: 6°, 7°, 12°, or 24°? And if we have this right triangle, which has an angle of 40 angular degrees here, a right angle here, a hypotenuse of 10 units, how long would this leg be? Would it be 5.0 units, 6.4 units, 6.5 units, 6.6 units? 60 angular degrees is about 1 radian. That's easy to remember once you get used to it. But remember, it's ABOUT 1 radian. It's not precisely 1 radian. 360° is 2π. 2π is 6.28 something. 60° is a 6th part of it, a 6th part of 6-point something is 1. And now we can work backwards. If 1 radian is 60°, then 0.1 radian should be 6°. And this requires the use of a calculator, I guess. The ratio of this leg to the hypotenuse is the sine of 40°, so what we are needing is the ratio of this leg to this hypotenuse is the sine of 40°. The number that we are looking for would fulfill the following: that number divided by 10 is the sine of 40°, which means multiply both sides by 10. We need the sine of 40° multiplied by 10, which is about 6.4. Now we can come up with an equation for the motion of the moon around the earth. The earth should be sitting at the origin of our coordinate system. The moon should be starting on the x axis and should be moving in a counterclockwise direction. Of course, there's something with cosine and sine. Specify the details. What do we have in front of the cosine and sine? Is it 4 x 10⁸ m, is it 27 days, is it 2.5 x 10⁻⁹ m, or is it 2π x 27 days? What's on the inside of cosine and sine? 2π x t, or t, or π x t? Then times, or divided by? 27 days, or 4 x 10⁸ m? The cosine outputs the ratio of 2 lengths which means that the unit of measurement that the cosine returns is 1. The cosine does not return meters or kilograms, it returns bare numbers. So whatever is in front of that cosine determines the measurement unit of the result. The result is not measured in days. We can immediately reject these. The result is measured in meters. It can only be this one or that one. The cosine and the sine range between -1 and +1. If you multiply by something x 10⁻⁹m, We would be dealing with very, very tiny lengths. That's not what we are into here. It has to be the first one. This is the radius of the orbit of the moon around the earth. Let me do these in reverse order. The radius of the orbit of the moon should not appear inside here. This has to do with the period of the moon's orbit. How long does it take the moon to make one full orbit around earth? 27 days. It must not be the meters. Which is also clear from the units. If we have meters in here, something has to cancel the meters. We have to have an angle in here. If we speak of radians, again, something without measurements units. There must not be any measurement unit in here. If it were meters, these meters would have to cancel somehow. How would I do that? Not with time, obviously. Seconds or days don't cancel with meters. Once we are here, it's clear that we have to use the division. Days divided by days. This gets rid of the measurement units. We can't mulitiply by days, otherwise we would be getting days squared. And the remaining choice, 2π x t. Not this one. Not this one. If we let t grow by 27 days, 1 complete period, the numerator increases by 2π x 27 days. And this ratio increases by 2π x 27 days, divided by 27 days, by 2π. As t advances by one period, this ratio advances by 2π, but that's precisely what we need for cosine and sine. The angle has to advance by 2π 360°. One full revolution. This is a quick overview on the concept of velocity, which is more or less the same as the concept of derivative, which could be called rate of change in general terms. If you are already acquainted with that, you may want to skip to the quiz. Imagine this situation: that we have a high-speed train on the track that is labeled from -100 km to +100 km. So the track is 200 km long. And this is our timetable. The train starts here at -100 km, and drives at a pretty high speed 'til 1½ hours after the start. Then it stops at this position, 50 km. It continues somewhat slower for 1 hour, 'til it reaches that position at +100 km. Then it immediately returns to the position x = 0, 4 hours after it started. This is, if you will, a mathematical view of the train's time table. Let's rotate this by 90° to get something that's more typical for physics. So now I've taken this diagram and rotated it by 90° to come up with that diagram. The t-axis now points to the right, and the x-axis now points upward. We can create a second diagram from that that shows the velocity. So here the train advances at 100 km/h, here it's just 50 km/h. Here the train goes backward with 100 km/h. So maybe a good idea is to label this axis with +100 km/h and -100 km/h. So here we have a velocity of +100 km/h, here too, and then the train stops, which means we have a velocity of 0. And here the train starts again and makes 50 km/h which is just half as much as before. And here we are going backward with 100 km/h as its speed, so the velocity is -100 km/h. This is what the velocity looks like. So if you're going forward, it's positive, if you're going backward, its negative. So what we see here is the velocity of the train, depending on time. That's nothing but the rate of change of the position, which we call x, by how much does x change if a certain amount of time passes. The rate of change would be the ratio. For instance, this ratio: 100 km divided by 1 hour -- the velocity of 100 km/h. In this case, the ratio would be 50 km divided by 1 hour; 50 km/h. So what we are drawing here is the rate of change of the x position, nothing but the velocity, and this is the very same as the derivative. The derivative tells us the local rate of change. Here the derivative is 100 km/h, here it's 0, here it's 50 km/h, and here it's negative. As the time advances, the x position decreases. A negative derivative, negative velocity, negative rate of change. The standard way of writing that: v of t, the velocity depending on t. One way of writing it is dx of t by dt, or x prime of t, or, and this is what the physicists are using, x dot of t. I'm going to stick to that notation. Whenever physicists want to compute the rate of change with respect to time, they put a dot on top of that variable. Assume that this is the curve that describes the dependence of position on time. What would be the curve that describes the dependence of velocity x dot on time-- the red one, the green one, the purple one-- and this is getting a little more complex. Assume that this is again a curve that describes the dependence of position on time. What about the second derivative? If you repeat that same process again-- the derivative of the derivative, the rate of change of the rate of change-- that's nothing but acceleration. Acceleration is the rate of change of velocity. How quickly does velocity change? So what would the acceleration be on the red curve-- the green curve or the purple curve. If this is the position, we see that position always increases with time. There is no way that the derivative can be negative, that the rate of change can be negative. Otherwise, the position would have to go down if that was the case, so we can rule out the red version. Now let's look at this step, almost a step. We need to ramp up the velocity and ramp down the velocity. Here, we accelerate, and here, we decelerate. Okay. And this is an almost precise replica of that one, so the same shape--it has to be the green one. It cannot be the purple one. Now for this curve, we accelerate positively The velocity grows. You see that we start with small velocity. The velocity gets larger as we go up to that point here. So acceleration has to be positive. The rate of change of velocity has to be positive, which rules out the green curve. We are left with deciding whether the red curve is the right one or the purple curve is the right one. Look at this part of the curve. The position is decreasing. If position is decreasing, velocity has to be negative. See, we start with a velocity of almost 0, and then the velocity becomes very negative. This is very steep. The rate of change is very high in a negative direction, which means that we need acceleration to the negative--strong negative acceleration. It has to be this red part. It can't be that green part. And here, acceleration is positive again-- --high and positive-- to turn that negative velocity into a velocity of 0. We got a positive peak in the acceleration. And eventually, the acceleration is 0, the velocity is 0, and the rate of change of the position is 0, so it's the red curve. Now let's come up with some concrete equations for derivatives. For instance, if this equation describes the dependence of my position on time, the position should be (t + 1)³. Actually, we are ignoring physical units of measurement right now. What would the rate of change be? Let's magnify a portion of that curve. It's saying this is some time t, and this is, a little later, t + h. Throughout this course, h will denote a small increment of time. Then the x value here is (t + 1)³, and the x value here is (t + h + 1)³. And the question now is what the rate of change is. What you're interested in is the ratio of the gain in x to the timestep that we're making. This side obviously has a length of h, and this side has a length of (t + h + 1)³ - (t + 1)³. So let's write down the ratio-- [(t + h + 1)³ - (t + 1)³]/h. Now comes the question, if h is really small, what would this expression become? Would it become t + something that can be neglected for every tiny h? Would it become (t + 1)² + something that we can neglect, 3t² + something that can be neglected or 3(t + 1)² + something we can neglect? Let's spell out what this first term means -- (t + h + 1)³-- here comes the trick -- I write it as ((t + 1) + h) * ((t + 1) + h) * ((t + 1) + h). I just reshuffled the different terms -- t + 1 + h, t + 1 + h and 3 factors of that sort gives the 3rd power. What do i get? (t + 1) * (t + 1) * (t + 1) = (t + 1)³. Then I get terms of that sort-- h * (t + 1) * (t + 1), h * (t + 1) * (t + 1) is, of course, (t + 1)². How many terms of that sort do I get? h * this one * this one is one of that sort. (t + 1) * h * (t + 1); second one, and (t + 1) * (t + 1) * h; a third one, 3 terms of that type plus everything else has h² in it -- h * h * (t + 1) or (t + 1) * h * h or even h * h * h, So, whatever happens to appear next is the h² times something. So what do we get? 1/h times -- (t + 1)³, (t + 1)³ cancels -- 3h(t + 1)² + h² times something. And look what happens -- we divide by h, h/h cancels, h²/h becomes h, and in the end, we are left with 3(t + 1)², and this term is negligible. So the answer would be 3(t + 1)². Of course, you may already know that there's tons of rules how to compute derivatives. In the end, they're just shortcuts of such computations. Now let's apply derivatives to physics -- Newton's Second Law of Motion. We're looking at one-dimensional motion. Think of the train we saw before. Some force is exerted on object of mass m. What does happen? The velocity of this object changes. And Newton's Second Law describes that relationship. The mass times the acceleration is that force. If you keep the same object, the same mass and double the force, we double the acceleration -- the change of velocity. If we want to accelerate an object of twice the mass, we need twice the force. Mass is measured in kilograms internationally. Acceleration is the rate of change of velocity -- v dot -- the derivative of the velocity with respect to time, or as velocity is the derivative of position with respect to time, the derivative of the derivative of the position with respect to time. Velocity is meters per second internationally, and the change would be meters per second -- per second by how many meters per second, thus the velocity change per second -- that is m/s². Force is given in kg times m/s², and there's a short name for that -- Newtons -- because obviously, Newton was the one to find this. If you never wrote any program in Python, here is some advice on the specifics of Python. If you know your way around Python simply skip this segment. To indicate that the rest of the line is a comment, simply put a # in front of that line to create variables simply type the name and write equals whatever you want, you do not specific the type of a variable, unlike in C or in Java or in C++ or in C#, Python is going to figure out the type on its own, like Javascript if you will. So that’s an allowed name. answer = 42, radius underscore of underscore earth is an allowed name, 6.371 times 10 to the 6. I’m writing comment M to say meters by just simplest way of indicating what the measurement unit would be. And you must not start names with digits: 13test, too is not a correct name. The print command, print out, what you’ve given after it, very simple. If you want to specify more things, simply use commas. This now shows some basic arithmetic’s advantage I wrote it first for us to discuss the results. 42 is assigned to answer, 6.37 something times 10 to the 6 is assigned to radius underscore of underscore earth, now we do some computation of three times answer plus four and the result is assigned to a variable called A. Note that multiplication and division are executed before plus and minus. Now we take the radius of earth to the second power. Asteric asteric (**) represents the power operator in Python unlike C, unlike Java. Parenthesis work as they work in school and again the power operator 0.5 of course meaning this is the square root, that’s one way of writing this square root of A+3, the result is assigned to B. See this is what our output here. The second one gets pretty ugly. The content of answer is incremented by seven. Plus equals seven, that’s the same as in C and in Java and in Javascript. The increment operates on the same work for minus equals time equals divided equals. What you can’t do is plus, plus unlike C and its sibling. If you want to do plus plus you have to write plus equals one. This again is similar to C, if you divide integer numbers five divided by four, the result is an integer number. Lets look at up. C is the fourth one one, two, three, four.. 1... 5 / 4 is 1 in Python as is for C and for Java and so on. If you want to see a floating point division at least one of these two numbers has to be of floating point numbers, simply write 4. (i.e., 4 dot) d.. Let's look at the second to last.. 1.25.. it works.. or 1.0 times five divided by 4, Python starts from the left as does C, as does Java 1.0 times five is a floating point number that’s divided by the integer number results of floating point number. Keep that in mind, there is something that easy leads to not so obvious errors. There is more to mathematics than arithmetic’s. To do more we need to import the math library. This is done best at the start of a program when we need the square root, we then can then write math.sqrt(4) which actually is the same as four asteric asteric 0.5, if we need the sin function its math.sin parenthesis, the angle. Of course this angle is specified in radians, not in angular degrees. To convert from angular degrees to radians multiplie by pi - math.pi, it’s a built-in constant, divided by 180. If lines get longer, we may need to break the line to keep it readable, if there is an open parenthesis, you can break at any point then sometime close that parenthesis. If lines get longer, we may need to break them to make things – to keep things readable. Python is picky about lines. You cannot break anywhere; that’s unlike C and Java. But if there is an open parenthesis you can break anywhere. Python is going to wait for that closing parenthesis. For the standard mathematical functions, such as square root and sign, we need to import the math library. This is done best at the beginning of a program and then you invoke the square root for instance by typing math.square root and the argument is given in parenthesis. That’s pretty similar to Java and to C#. The result, of course, is 2.0. We could also have written four to the 0.5 power. The sin function is math.sin. The cosine would be math.cos and so on. The constant pi is included as math.pi. The sin function and it's friends work with radians, not with angular degrees. So if 23.4 is an angular degrees, we need to convert it. For instance, by multiplying with pi and dividing by 180. If lines get longer, you may want to break them to keep things readable, Python is picky about line breaks. That’s very much unlike C and Java. You cannot break a line at any place, but if there is at least one open parenthesis, you can break the line. This is what I’m using here. If you need something to be done a given number of times, you can use the 'for' loop, which looks quite different from C. The first thing that you notice is a colon at the end, no braces, but a colon here, at the end of the first line, and indentation. The indentation and that colon replace what C users know as curly braces. What happens here? h starts with 0, gets 1, 2 and so on and at the end its 6 and then the loop terminates. So you do not write 'for h =0;' and so on. h < 7, you write 'for h in range 7' That’s the Python way. So if we look at the result... so it prints out square numbers starting with 0, and eventually terminating at 6 to the second power. The 'while' loop is good for loops that run as long as something has to be done. So in this case, this loop runs as long as K is less than 130 or M is equal 140. So unlike C, you do not write '|| 4', you simply spell out 'or'. The same for &&. Don’t forget the colon, so we see that K runs up to 130, this first condition fails and the second one is not true either because M is 7,000 something. Now to conditional execution - if the value of K is larger than 42, don’t forget the colon, output 'Hi'. The text string and single quotes and 'elif' – that means if K was not larger than 42, check if M is larger than 43. If so, output 'Hello', if not which means if M is not larger than 43, output "Buy". And of course K is larger than 42, its 130. So we output "Hi" and never see the rest. The 'else' is optional and the 'elif' is optional. We need to work with long lists of data. This package called numpy; it comes in handy, yeah. numpy.zeros creates a one dimensional array and this case with five entries numbered from 0 to 4, like in C. Here I change the entry number 0 and here I change the entry number 4, which is the last entry. I print the complete array and here I print the ultimate entry. This comes in very handy and is different from C and other languages. So the complete array contains 20,0,0,0,99 because I changed the entry number 0 and I changed the entry number 4. And the entry number if you will, minus 1 is simply the last one. Often we do not get away with one dimensional lists, we’ve to use two dimensional tables. This creates a table that’s filled with 0s and has two rows and 3 columns. Here I change the entry that’s in the upper row, row number 0 and in the middle column, column number 1 (column is ranged from 0, 1 to 2) to 7. And here I change the entry in the lower row, the rows are 0 and 1 and in the right most column to 8, I output that. And then I do some arithmetic’s with matrices that is we’re going to cover that later, one plus twice the complete table that I built here. Let’s look at the output. So, that’s our array. 0,7,0 is the row number 0. 0,0.8 is the row number 1. We changed the entry second column, row number 1 to 8. Thats this guy . And here I computed the one plus twice the array as above. The 7 becomes one plus two times 7, 15, the 8 becomes one plus two times 8, 17. This again is something that you can’t do with regular arrays and C or in Java. This comes very handy for mathematics. And there is one more thing that resembles arrays in C and arrays in Java, if you think you copy that array, you didn’t actually create a copy, you just created a reference to the very same thing in memory. So yeah, it seem to create a copy Q equals P, I changed the copy to make the left – the top most left entry equal to 42. And then I output P and you see, if I look at P its too has the value 42 in here. There is just one instance of that array in memory; both Q and P refer to the same instance and memory. This is how we define functions in Python. We say def function name, the parameters that we want to hand over to the function, colon and then comes what the function should do. Again, of course, with indentation. And you call this function like you call math.sin and other build in functions: with parenthesis, similar to C and Java. So in this case U is set to 1, we compute two times U, R becomes 2 and we return 2 plus 1 equals 3 and then this case its 7, U is set to 7, R becomes 14 and we return 14 plus 7, that is 21. This is a quiz to get you started with Python. We define a function that’s called sine underscore cosine and this function should create arrays called X sine underscore X, cosine underscore X and fill these arrays with values that allow us to eventually plot the sine and the cosine curve from X equals zero to X equals two pi, exactly two pi. Your task is to fill in the code in this loop that iterates over fifty points. Make sure that X ranges from zero to two pi exactly. Sine X is the corresponding sine value, cosine X is the corresponding cosine value and then the matplotlib package will take care of plotting the curves. To compute the appropriate value of X, we divide I by the number of points fifty minus one, that’s important, minus one. I ranges from zero to forty nine, fifty points, zero to forty nine. The last I will be forty nine, if I do five by forty nine the last value I get here is one. So I divided by a number of points minus one is a number that ranges from zero to one. Multiply that number by two pie that’s X. – X ranges from zero to precisely two pie and then I compute a sign and I compute the co-sign and that’s it. Close to the surface of the earth, every object experiences an acceleration of 9.81 m/s². This is independent of that mass of the object because assume that you have two objects of the same mass. Each one is accelerated at the same rate. So they stay on the same height as they fall, which means you can combine these two objects, add a little glue. and this larger object is going to fall, but the same acceleration. But now it has twice the mass. Based on this, you should be able to answer the following question. What's the gravitational force of a mass on 1 kg -- 0.1 N, 1 N, 10 N or even 100 N? We apply Newton's Law -- F = m * a. The mass is 1 kg ; the acceleration is 9.81 m/s². So in total, we have about 10 kg times m/s², which is 10 N. This gives you an idea about this unit of 1 N. The weight force of a mass of 100 g is almost equal to 1 N. So now we have equations that govern the rates of change of the position and the velocity. The rate of change of the position is the velocity, and the rate of change of the velocity is the acceleration, which is by Newton's law, force divided by mass, and in a typical setting, we also know the position at time 0 and the velocity at time 0. Now we are going to use the computer to learn what these equations are leading to, and the easiest way for doing so is called the Forward Euler Method. The most complex thing about that method may be the name of Euler. Euler was born in Basel, Switzerland, that is. Most people these days use the German pronunciation of his name -- Euler -- even though the Swiss pronunciation may be different from that, and in the U.S., you may also hear the pronunciation Euler. I try to stick to the pronunciation Euler, but forgive me if I, from time to time, fall back into saying Euler Euler's idea was to solve these equations by walking in small time steps. If we start at the initial position-- x(0) and the initial velocity -- v(0), what would happen after a short timespan we call h? The position would approximately increase by h times the velocity. If the velocity is 2 meters per second, and we wait for 3 seconds, we will be changing the position by 6 meters, for instance. But, of course, we're using a much smaller time step. Similarly for the velocity, after some small time h, the velocity will be its original value plus the time step times the acceleration, which is F/m. So these equations will take us from time 0 to time h approximately. I'm cheating a little when I write equal signs here. In the same fashion, we can get from time h to time 2h doing another step. This is what this second step looks like. We know the position that we have reached at the end of the first step, and we continue with the new velocity, and that results in the new position -- similarly for the velocity. You iterate this process over and over again and find estimates for positions and velocities as you go. Now apply the Forward Euler Method to the following situation: We drill a big hole and let a mouse fall into that hole. The mouse starts at height 0 and at velocity 0. The height is going to be our x-coordinate. That means that x(0) = 0. V(0) = 0, and a(t) = f/m = -9.81 m/s2-- minus because the velocity is going to become negative. Your job is to provide the appropriate code to compute the t and x and v values for all steps. We create these arrays with 51 entries so that the entry number 0 can be the initial value, and you compute 50 additional values for each array. In our solution, we always compute the t and x and v value at the beginning of the next step, and you make use of the fact that the entry number 0 of t and the entry number 0 of x and the entry number 0 of v are already equal to 0, which is what we want. If you run the program, you see that per second, the velocity decreases by 10 m/s. It starts out at 0 and keeps decreasing, which means the speed, the magnitude of the velocity keeps increasing. This falling object gets faster and faster, but in a negative direction, which is our x coordinate starts at 0 as intended. It decreases slowly and then faster and faster. You see gravitational acceleration in action. One dimension in motion is not enough for us. We need 2 dimensions or even 3 dimensions. We need vectors. If you know about vectors directly, skip to the quiz. If you don't, stay with me. Vectors can be represented by arrows. Typically, we denote vectors by lowercase Latin characters with an arrow on top of them, and we can also express them in numbers. This arrow vector is 4 units to the right and 3 units up, which is why we can write 4, 3 in this way. These are called components of the vector -- x-component, y-component if the x-axis points right, and the y-axis points up, Let's look at a second vector. If you call this vector B, that would be 2, 1 x-component 1, y-component 2. Let's have a third vector that's called C, and C should be 1, -3. C goes right by 1 unit and goes down by 3 units. And now we can do some basic arithmetics. We can add vectors, for instance. Let's add B and C. If we want to add B and C, we put them together. The tail of C goes to the tip of B or the other way around, and the resulting vector, which is B + C goes from the tail of B to the tip of C. So this is the sum of the vectors -- B + C -- and if you look closely, you'll see this is 1, 2, 3 units to the right -- and this one is canceled with that one -- and 2 units down. So what we actually could've computed is the x-component of B plus the x-component of C -- 3 -- and the y-component of B plus the y-component of C makes 2. That's the sum of vectors. You simply add the components, and geometrically, you chain these arrows. And there's a second operation we can do. We can multiply vectors with numbers. For instance, if I want to multiply this vector B by 3/2, I get an arrow in the same direction, but with 50% more length. I can do that in components, of course, as well. So what I get is 3/2 times 2 and 3/2 times 1, which is 3 and downstairs 1.5. Obviously, 3 units to the right, 1.5 units up. If you do this multiplication with negative numbers, we are going to invert the direction of the arrow. So if I compute, say, -1/2B, I get an arrow of the opposite direction and 1/2 the length. and components -1/2 * 2 = -1, -1/2 * 1 = -1/2. So this goes 1 unit to the left and 1/2 unit down. If you remember the NumPy package in Python, this vector addition and vector multiplication and the NumPy package in Python this addition and this type of multiplication is already included. The third operation that we can apply to all vectors, is determining their lengths. I'm writing it like that with double bars. To determine the length, you can simply use the Pythagorean Theorem. One leg of this right triangle is 4 units long. The other leg is 3 units long, which means that the hypotenuse is 4² + 3² square root units long, which means 16 + 9, 25 square root, 5 units. What is the sum of these 2 vectors--which one of these 4? If we take twice this vector minus this vector, what do we get--which of these four? To form the sum of these 2 vectors, take this vector and put its tail to the tip of this vector, and you'll see what's resulting--this vector. So the last one is the right choice. If I take twice this vector, I get this one. plus the negative of this one, which is this one, and eventually, you'll see the first choice is the right one. We know that a mass of 1 kg experiences a rate force of about 10 Newtons on the surface of the earth, but for space travel, we need to know more. What's going to happen if it double the distance between our mass and the center of the earth? For instance, what's going to happen if we double the distance between our mass at the center of the earth? Obviously, the force is shrinking, but by which factor? It turns out that as we double the distance, we have only one quarter of the force. If we tripled the distance, the force goes down to 1/9, which means that in our creation for the force of gravity, we have to have a distance squared in the denominator. If we use twice the mass, we should have twice the force, so the mass appears in the numerator. And if all masses are created equal, the mass of the earth should have the same role, so the mass of the earth, too, should appear in the numerator. >From the units of measurement, you can see that this doesn't work out so far -- kg²/m² That doesn't look like force-- this universal constant appearing in front-- the constant of gravity, which is 6.67 * 10⁻¹¹, N m²/kg². These units are pretty much of human scale-- Newtons, meters, kilograms--and you see that gravity seems to be pretty small on these scales. The force exerted by 2 masses of 1 kg each in a distance of 1 m would amount to 6.67 * 10⁻¹¹ Newtons-- a pretty small force. This is why we need something as big as a planet to see gravity in action. Now it's time to compute the mass of the moon. We know that the gravitational acceleration on the surface of the moon is about 1/6 of that of the earth or 1.67 m/s² to be precise. We know the radius of the moon, which is 1.737 x 10⁶ m. Now you can compute its mass. Enter a number with one decimal place in this first text field and an integer number in the second. The force on this mass refers to mass times the acceleration, which is 1.63 m/s², but we know where this force comes from-- namely, from gravity, so it has to be equal to the constant of gravitation times the product of the masses, the mass of the moon -- let's call that m₁, times our mass, m₂, divided by the distance squared. The distance is the radius of the moon. Now let's ignore this part of the equation. Then you see that m₂, the mass of our object, doesn't matter, and we can solve for m₁. If you plug in the value of the constant of gravity and do the math, you get 7.3 x 10²² kg. Actually, by the book this equation for gravity--this equation for the force of gravity-- is only true for point masses. We would have to concentrate all of our earth in one single spot. So do we cheat when we don't do so? The nice thing about gravity is that this equation still holds for a rotationally symmetric distributed mass. This is called Newton's Shell Theorem, and you are going to see in the upcoming quiz why this is called "shell" theorem. To see why Newton's Shell Theorem holds, it's best to introduce an analogy to water. This looks outrageous, but please bear with me. If we leave our first mass where it is and move the second mass around, we can determine a force vector at every point in space. We get a force field. Imagine that this wasn't the solar system but a pool of water, and the water has velocities, and the velocity of the water is described by our force field. Water flows downward here at a high speed. It flows to the right at a low speed at this point. It flows to the left at a somewhat higher speed at this point, and so on. We interpret the force field a velocity field in our pool of water. Of course, we're ignoring the units of measurement right now. Of course when setting force equal to velocity we're ignoring units of measurement for a moment. To make that work, there has to be a huge drain in the center. The first mass acts like a cosmic drain. Why can we do so? Why does water behave in the same way as this force field does? To understand that consider two imaginary spheres in our pool, both centered at the drain. And the radius of the outer sphere should be twice the radius of the inner sphere, which means that the surface area of the outer sphere is 4 times the surface area of the inner sphere. All lengths are being multiplied by 2. These are square meters. They are multiplied by 2 squared. Now the important quantity to consider is the flow of water through these spheres. How many cubic meters per second flow through these surfaces? This number has to be the same for the outer and for the inner sphere. We are not losing water or gaining water in between, but as the outer sphere has a surface area of 4 times that of the inner sphere the velocity has to go up to transport the same amount of water per second through each of these spheres the velocity at the inner sphere has to quadruple as compared to this velocity at the outer sphere. This is precisely what we saw before with gravity. As we double the distance the force shrinks to 1/4 of the original. Of course, this doesn't only work for a factor of 2. This works for all ratios. Hence, we can indeed identify the force field of gravity with the velocity field of water. Once this analogy between gravity and water is in place, it's easy to see that the flow through each of these surfaces has to be the same. Let's assume that we have three masses at the center of this sphere. And this same masses are at the center of that smaller sphere. And the same three masses are somewhere inside this weird closed surface. Then the flow through each of these surfaces has to be the same. If we replace the force field of gravity by the velocity field of water, we have to seem the same number of cubic meters per second for each of these surfaces. Because we're draining water in three spots at the same rate. Somehow that amount of water has to enter from the outside. And it has to be this same amount of water, because we're draining the same amount of water in each of them. With the help of this analogy between gravity and water analyse this case -- a huge spherical object of constant density in which we drill a hole that's half the radius deep. Now we put a test mass on the surface, blue position, and in that hole, red position. What's the ratio of the force of gravity in the blue position to the force of gravity in the red position. Are these two forces equal? Do we have twice the force at the surface or do we have twice the force at the inner point? Or is it four times on the outside or four times at this inner position? As we go deeper the force of gravity shrinks, and it does so by a factor of 2. Let's see in detail why this has to be the case. The trick is to add an imaginary sphere of half the radius. Then we can look at the force from the core part and the force from the shell, and this is where the name "shell theorem" comes from. The core part is again a sphere as is the complete object, so we can compare what's going to happen as we go from the large sphere to the small sphere. Distance is multiplied by 1/2 as we go down that hole. The mass is multiplied by 1/8 as we are shrinking the original sphere to half of its radius. We're shrinking the volume to 1/8. And hence the mass to 1/8 of the original value. If we take these two together, the original force will be multiplied 1/8, because the mass appears in the numerator, divided by 1/2 squared, because the distance squared appears in the denominator. And in total, this is a factor of 1/2. And to see what's happening with the force from the shell, let's put an imaginary sphere close to the inside of that shell. If there is a gravitational force on the surface of that sphere, it has to be rotationally symmetric, for instance, like that. Can this be? No. Where would the water be flowing? We would have to drain that water somewhere, which we don't. Can the force points to the outside? No. Where would that water come from in the first place? So the only reasonable value for the magnitude of the force is 0. And hence, in total, the force goes down to 1/2 of its original value. Now we want to put numbers to gravitation. Do this in 2D, which means we need to have vectors. Let's place the earth at the origin. The moon sits at a vector xm. The spacecraft sits at a vector xs, and we need the distance of the spacecraft from the moon. Let's call that dms, and we need the distance of the spacecraft from the earth. Let's call that des. Now we can put both of Newton's laws together. The mass of the space craft times its acceleration, as a vector now, equals the force that is exerted on the spacecraft. And this force stems from gravity. It has two components. One is the force that is exerted by the earth. The other is the force exerted by the moon. So, the force exerted by the earth would be gravitational constant times the mass of the earth times the mass of the spacecraft divided by the distance between them squared times a unit vector, that is, vector of length one pointing to earth -- something like this. And for the moon, we get the gravitational constant times the mass of the moon times the mass of the spacecraft divided by the distance between them squared times a unit vector pointing to the moon. So how do we get these two vectors? For the first one we can take minus xs -- that's a vector that takes us from the spacecraft to earth -- and divide by the length of xs, which is nothing but des. For the second one, we can take the difference vector xm mines xs -- that's the vector that has its tail at the spacecraft and its tip at the moon -- and we divide by the length of that vector, which is the distance from the moon to the spacecraft. Now, let's forget about the force. Then we see that the mass of the spacecraft cancels. What we need to know are the mass of the earth and the mass of the moon. Now the task is to actually write a Python function to compute the acceleration that we have derived before. The mass of the earth and the mass of the moon and the gravitational constant are already given in that code. The function that you should write gets the position of the spaceship as 2-dimensional vectors and should return a 2-dimensional vector that describes the acceleration of the spaceship. There is one nice function of the NumPy package that you will be needing, namely, the norm function which computes the length of a vector. You may want to introduce auxiliary variables such as, for instance, this vector from the spaceship to the moon and this vector from the spaceship to earth. And then the rest is simply Newton's equation. We're dividing by the second power of the distance from the spacecraft to the earth. You're combining all occurrences of the distance of the spacecraft to earth in this single place which is the third power of the length of the vector from the spacecraft to the earth. The same occurs here with the moon. So now we have reached our first real world differential equation -- the second derivative of the position vector of the spacecraft, which is its acceleration vector, equals the gravitational constant times the mass of the earth times a vector that points from the spacecraft to the earth divided by the 3rd power of its length. This boils down to the 3rd power of the distance between the spacecraft and earth. Now comes the component due to the moon-- gravitational constant times the mass of the moon times vector from the spacecraft to the moon divided by the 3rd power of its length. This is a differential equation -- an equation that governs or is governed by rates of change. The acceleration vector is the rate of change of a rate of change. This actually is a system of differential equations. It's not a single equation. We've got two components in here. Each of these two components has to fulfill this equation. The highest derivative that occurs is the second one, which is why we call this differential equation a differential equation of order 2. The highest derivative stands alone on one side and does not occur anywhere else. This is what we call explicit. If the highest derivative was hidden somewhere, we would first need to solve that equation, which is ugly. These two terms mean that this differential equation is nonlinear. A linear differential equation would only contain terms such as 42 times xs or sin(t) times xs. Look at these equations. What is their respective order? Are they a system or not? Are they explicit or not? Are they linear or not? The highest derivative here is the first one. The highest derivative here is the second one. And here it's the first one again. So that's the order -- 1, 2 and 1. The first one isn't a system. It's just a single equation. The second one either. The third one is a system -- two equations. The first one is explicit. The highest derivative appears on one side alone and nowhere else. This one is not explicit. The highest derivative is hidden inside the sine. Same here. The highest derivative -- one of the two highest derivatives, I should be saying -- is hidden, so this one is not explicit either. This one is linear. In this case, we have the sine of the function we're looking for. This is not linear. And in this case we have the square of the function we're looking for, which, again, is not linear. So what did we achieve in this unit? We found a differential equation that governs the motion of that spacecraft around the earth and around the moon. And we know a simple solution method to solve differential equations on the computer--the forward Euler method. But there is lots of stuff left for Unit 2. We have to actually solve that differential equation, and it turns out the forward Euler method isn't quite the method to be using. We have to actually solve the differential equation by computer. And it turns out the forward Euler method is somewhat too simple for that purpose. And of course, we have to eventually find a trajectory that's going to bring Apollo 13 back to earth. Hi! In this unit, it's time to get Apollo 13 to the moon and back. Doing so requires quite some accuracy in our computations. This is why we look into some better tricks to solve differential equations on the computer but eventually we can fire the rocket boosters to save the crew. Accuracy is an obvious issue with our computer-based solution. To check what the computer does, the easiest way to learn what goes wrong with the computer simulation is to use a toy example, which we can compute by hand as well. This toy example will be a circular orbit. A space craft moving around earth for instance with the earth at the center of this orbit. Let's call the radius of that orbit r and let's call the orbital period the time its takes for one orbit capital T. The velocity vector of that spacecraft will be tangential to that orbit and the speed that is the length of that velocity vector. The speed will be constant and we can't easily compute the speed. It takes the spacecraft amount of time, capital T, to go around that circumference and the time capital T, the distance traveled by the space craft is 2 πr circumference of that orbit. The acceleration vector of these points perpendicular to that orbit to the center of the earth. The acceleration does not change the speed, the length of the velocity vector that is, it simply changes the direction of that vector. We can compute the length of that acceleration vector in a similar fashion. This goes as follows. We start with the velocity vector that takes us up and then we move and then the velocity vector changes to the left. At this point, it points to the left and eventually it goes round to full circle. After one orbital period, it's going to be that blue vector again with phase shift of 90°. So we can't say that the velocity vector executes the same circular motion as does the position vector that we have a phase shift of 90° between them. Nonetheless, we can still apply the same equation. It takes us one orbital period to make one full revolution and that has a length of 2 π*r. All of these vectors have the length of 2 πr/T. So the circumference would be 2π times this radius 2 πr/T, which boils down to (2π)² r/T². So given the radius and the orbital period of this circular motion, we can determine what the speed is and what the acceleration is. What about the geostationary orbit? The spacecraft should fly around the earth in about 24 hours, so that it seems to be staying above the same spot on the earth. The question is what's the appropriate radius. Give the number of kilometers. Do you know that the acceleration equals (2π)² r/T², but on the other hand we have Newton's law telling us that this acceleration is G*mE/r². This yields the following. Multiply both sides by r² , (2π)²r/T²=G*mE/r². Let's take r² over here, T² over here, (2π)² over here, so we have, if r³ is whatever remains on the right hand side, r itself is the ³ √ GmET²/(2π)² and if you plug in the numbers, that's about 42,000 km, which is the same as the length of the equator of the earth. So this spacecraft, the distance between the spacecraft and the center of the earth has to be almost the same as the length of the equator of the circumference of the earth. Remember that the distance between that spacecraft and the center of the earth is not the height of that spacecraft. To get the height, you would have to subtract the radius of the earth. Now that we have an equation for that almost geostationary orbit. Let's look at what the computer simulation does to that, so we start at this point, wait for 24 hours, precisely 24 hours, and look at what's going to happen. Most probably, we're not going to reach that same precise location but end up in a slightly different location, and the objective of this quiz is to compute the distance between the initial position and the position after precisely 24 hours to learn about the error of our method. We've got tons of stuff predefined ready for you to use. Your job is to insert the Euler forward method right at this point. Your job is not to return the positions and the velocities, your job is to return just a single number, namely the error--the distance between the initial position and the final position-- and up front of course compute the step size. Given the number of steps, make sure that you actually are taking, for instance, 200 steps after that initial position or 500 or 1000 and so on. Of course, the step size simply is the total time divided by the number of steps. To store the positions and the velocities, we have to make space for one more entry, which is the initial condition. We start the position vector at the right location, meaning that its x component is equal to the radius computed above, and the y component of the velocity is equal to the speed computed above. The other components are equal to zero. This is the forward Euler method as we know it. And then we compute the distance between the ultimate position and the initial position using this number function and we return that as the error. So this is the result, what do we learn if we have the step size of 100 seconds, our end result will be off about 1.5*10â ˇ m about 15 million meters, and you see that this relationship seems to be linear. We are going to look into that in the next second. The error that we just saw happening in the quiz is called the global truncation error or GTE if you will. This error stems from using a finite step size. If you would be using an infinitely small step size, you could theoretically achieve zero error. That's part of the truncation using a finite step size and as we saw, this global truncation error depends approximately linearly on the step size. It's more or less some constant times h for the forward Euler's method. This is different for other methods and beware this constant depends on the simulation time. If you look at the global truncation error at this point, it maybe smaller if you take at some further orbital periods, this truncation error may explode. Beware of that this constant is not a constant when it comes to changing the time. When there's a global truncation error, there's of course a local truncation error. That's the error that we're making in every step. See this supplementary material for the local truncation error if you're interested. So now we know that when we compare the result of the forward Euler method, it's going to deviate from the exact solution. Now assume that you have two numerical solutions. One solution with the forward Euler method of step size h1 and another with a step size of h2 larger than h1, and assume that these two and the exact solution almost line up. The question then is--can we estimate the error between this numerical solution and the exact solution that we do not know based on the difference between those two numerical solutions, which we know. So given the distance between two solutions using the forward Euler method, how would you estimate the error between the exact solution and this first solution with the step size h1? Should it be E times the ratio, E times the ratio of the difference to the first step size, E times the ratio of the first step size to the difference, or E times the inverse ratio? A sensible estimate would be E times the ratio of the first to the difference. Let's look at why that is the case. We know that for the forward Euler's method, the global truncation error that this kind of error depends on the step size in almost the linear fashion. This error that I am looking for here is the GTE for h1. We see that E, the error between the two numerical solutions, appears here. So now there's two similar triangles. The first one is here and the second one is here. Let's look at the following ratio error/h1. This has to equal E/h2-h1, and hence, the error is equal to E*h1 times up divided by the difference. I'm writing an equal sign here but of course that's just a rough estimate. This dependance is not strictly linear and we're assuming that these three points are almost on a line. This may not be perfect in practice. Now we can say what the order of a solver is. A solver is a method such as the forward Euler method that we used to solve differential equations on the computer going step by step. For the forward Euler method, we know that the global truncation error, the error after a finite time is something like a constant times the step size. In the general case, it may not be the step size but this step size raise to a certain power, and that power is called the order. The forward Euler method has an order of one, so that we end up with the constant times h. More powerful methods we have larger orders, say two or four, and again, we have to keep in mind that this constant depends on time. It may grow exponentially with time for instance. This constant does not depends on step size, however. Know this site. Be sure not to confuse this notion of order with the notion of order of a differential equation. When we have differential equation of this sort for instance, this is of order two. The order of a differential equation is a different story. If we had a solver of order 4, remember the forward Euler method is of order 1. If we had a solver of order 4, and if we wanted to shrink the error by a factor of 10, we want to gain one decimal place in accuracy, by which factor do we need to change the number of steps? Should we be using 50% more, 80% more, twice as many, 10 times as many steps? You should be using 1.8 times the original number of steps. The original error before I change the number of steps would be something like this. Let's call it the old error would be a constant times the old step size to the fourth power. What I want to achieve is that the new error with the new step size is just the 10th part of this, and at the same time, because this is supposed to be a solver of order 4, this is the same constant as before times the new step size to the fourth power. So now we've got two equations that involved the old error and that constant C, this is the first one, this is the second one, we can solve for the new step size which is just basic algebra and you'll find that the new step size is the old step size divided by the ⁴√10 and the ⁴√10 is about 1.8. So we are reducing the step size by a factor of 1.8 which means that we are increasing the number of steps by that very factor of 1.8. So now we want to actually look at the solver of order 2, the method invented by Karl Heun, Heun's method. It also goes under the name of improved Euler's method. And it is also is one example for a simple Runge-Kutta method in case we haven't got that term. I'll start with the forward Euler method. The position for the first time step is the initial position plus time step times the initial velocity and similarly for the velocity, the velocity is changed by time step times force divided by mass Force divided by mass is acceleration. I have explicitly indicated the dependence of the force on time and position. And what we do now, we do not actually use these results for the next step, we use them as stepping stones if you will. Let's start this result as Xe for Euler and this result is Ve for Euler and the trick now is to use these into immediate result to get a better estimate for the next x and the next v. So how can I improve the estimate for the next position based on Euler's result? And the next position is the initial position plus time step times the average of the initial velocity and the next velocity as predicted by Euler. This takes care of the fact that the velocity is not going to stay constant over the cause of the first time step. Euler's method is based on the assumption that this velocity is almost constant. How can I use this to improve Euler's result? The bad thing about Euler's method is that the velocity actually is not constant. It's not going to stay at the value it has at time equals zero. A better estimate is use the initial velocity and the velocity predicted by Euler and form the average of both. This is what Heun's method does and similarly for the acceleration used to compute the next velocity, I found the average of the initial acceleration F at times zero divided by m and the force after the first time step at the position predicted by the Euler method. For the velocities, I'm using a similar approach. I formed the average of two forces and divided by m to get acceleration. The first force is the initial force times zero, initial position. The second force is the force that I get at the end of the first time step using the position predicted by Euler's method. So the task now is to actually implement Heun's method, we have provided for you Euler's methods. You can copy these lines and turn them into Heun's method in the end. Again, the error is determined so that we can see by how much Heun's method is better than Euler's method. I'm working with acceleration here other than the force because acceleration is already offered by the rest of the program, and I said before, I can reuse one of these accelerations, which is why I start the initial acceleration in this variable code, well, initial acceleration. Here comes Euler as we know it, these two lines, and then I'll form the average of the velocities to go to the next step and I'll form the average of the accelerations to go to the next step and that's all. If you run that program, you see two series of data. The upper one is the one that we know, the errors of the forward Euler's method, and the lower one are the errors of Heun's method and you see these are really low. So this method really rocks out. On top of that, you see that this seems to be a parabola. While Euler's method almost forms a line here, Heun's method forms a parabola. So it's good and it even gets better as we shrink the step size. There is a price to pay--we call that acceleration function twice. If you look at what we get for that price, that really pays off. There is one additional nice thing about Heun's method, it gives us Heun's estimate for the next step and in parallel, we get Euler's estimate for free and we can compare these estimates to find out the error that we are making. By the way, the technical term for this type of method is an embedded Runge-Kutta method if you want to look that up. Remember the two types of errors are introduced before. The one type of error that we're actually interested in, in the end, is the GTE, the global truncation error by how far are we off after a finite's time? What Heun's method allows us is to estimate the LTE, the local truncation error-- the error that we are making at each step. Consider the distance between the Euler result for that step and the Heun result for that step. As we saw earlier, Heun's result is much more accurate than Euler's result. So what we see here is about the error of Euler's method. The LTE of Euler's methods. I'm cheating a little here. I must take the velocity into account. Of course, we have several dimensions, two dimensions concerning x and two dimensions concerning velocity. We have to take all of them into account. So this term as well by how far is the Euler estimate follow velocity of from Heun's estimate? You could simply form the sum here but that would look strange for physicists. These are meters and these are meters per second--that does not work. The most simple idea is to introduce a time in here and time of that simulation. It did makes sense when the velocity is off by a certain amount, but early in that simulation, that error did grow over the course of that simulation. There's other ways of combining these differences--you could form the square here, the square there, and then take the square root but to make things simple, I'm simply using this distance plus the simulation time times that distance. We know that the GTE, the global truncation error for Euler's method is a constant times the step size. We can also show that the local truncation error is another constant times the step size squared. You can check the plausibility of hÂ˛ appearing here. If I double the step size, I only need half as many steps. Each step has four times the error. I need half as many steps which means that the total error is multiplied by two which we know is the case, but I can do know is to change the step size as I go. I do not need to specify the step size in advance. Once method is able to estimate the perfect step size for a given accuracy, assume that I did the first step with some step size whichever one I chose, but what I want is that this error, the local truncation error of Euler's method is of a fixed size. Let's call that tolerance. What I want is to find the step size. Let's call it hÂ˛ new, so that this error is off the fixed size called tolerance which would be measured in meters, by the way with this type of error. I can compute C from that very first step. It would be the local truncation error divided by hÂ˛ old and I can solve for the new step size. So the method actually determines the right step size I should be taking given the tolerance. I can solve for the new step size and find that the new step size is the old step size times the square root of the tolerance divided by this local truncation error that I can compute numerically. So after we did the first step with the old step size, we got an estimate for a perfect step size that produce the local truncation error of such tolerance for Euler's method and we can keep doing that as we go. The step size will be adjusted automatically. We do not have to worry about it. The method would choose the step size for us. One benefit of this method choosing the perfect step size for us, is that we do not have to choose it upfront. The other benefit is that the solution we'll be stepping at appropriate step sizes for instance, when we come close to a planet and the trajectory gets more bent, we need more steps. This method will automatically choose smaller steps as we go and eventually use larger steps again. Hi! Miriam here making a guest appearance in the middle of the unit. Your task for this quiz is going to be to implement adaptive step sizes. We've already provided Huen's method for you. So you just have to determine the next value of the step size h. There's one important change to the framework, however. You see we're dealing with single vectors here. We're no longer filling arrays of vectors. So the reason that we just are using single vectors is because we don't know upfront how many steps the simulation is going to have. The number of steps is going to depend on the values h that are being computed. So we're not storing a history of x and b anymore because we don't know how many entries that history would have to contain. So when we plot the result, we're not going to be plotting a whole curve. We're going to be plotting just a single dot at each different location. Now you can see that in addition to the step size h right here, we've also created a variable for you called h new. Inside this while loop, we want h to store the current step size and then we'll compute the next step size as h new. At the end of each step, we want to add h to the time it has past, just what's happening right here and I want to set h equal to h new, which signals that we move onto the next step. What we find here is almost a little interpretation of the equations. However, it's important to notice we compute h new as a modification of h but only use h in the implementation of Heun's method up here. We might run into some difficulties if the numerical error becomes really, really tiny because then as you can see right here in the computation of h new, we're dividing by the error and that would basically make us end up dividing by zero. So one option to prevent this from happening would be to divide not just by the error but by the error plus some really small number. Another precautionary measure that we might want to take would be to limit the range of possible values of the step size. So if we do that, then here's what happens. If h is smaller than 0.1, then the maximum becomes 0.1 and eventually you'll end up with a value of 0.1. Now if time gets larger than 1800, then the minimum becomes 1800 and the new step size becomes 1800 since it limits the range of h new to be between 0.1 and 1800. Now let's look at the output. So we can see that the program takes huge steps along its trajectory and keep in mind that these steps are not at regular time intervals because they represent the steps that we're taking with adaptive step size. So time interval down here is much smaller than the time interval up here. One last thing that we can notice here is that the very first step size is small because we initially set h to 100 seconds and then the method automatically chooses the larger one as you can see right here and then it keeps that larger one, although of course it does change. We've seen errors appearing on different time scales. The LTE, the local truncation error, that happens at each time step. There is one more time scale that when they look into mainly what happens in the long run, what happens if this system runs for 1,000 years, 10,000 years for the age of the universe, is this solution going to explode or to implode in the long run or will it stick close to the exact solution. The GTE for one final time does not tell us much about that form. One way of looking at the long-term error is for instance to look at energy. If the systems is such that energy should be conserved, we can look at the long-term evolution of energy to learn whether there is an issue. In this problem, we are concerned with two types of energy, kinetic energy and potential energy. Kinetic energy is due to the speed that is the length of the velocity vector and it grows with speed in a quadratic fashion. It contains a factor of the velocity vector of the spacecraft, length of that vector squared, meaning if increased the velocity by a vector of two, we are going to increase the kinetic energy by a vector of four, which tells us that speeding is really dangerous. There was another factor in here, the mass of the spacecraft and then there is an additional vector of one-half. Potential energy concerns the position of the spacecraft. As we move the spacecraft away from earth, we have to invest energy and that's going to become potential energy. This is the distance of the spacecraft from the center of the earth. When you fire the rockets to get it away from earth, you invest energy and that energy leads to a change in potential energy. In the case of gravity, it's easiest to have potential energy be negative all of the time and only when the spacecraft reaches infinity, it's potential energy go to zero. And the formula for that is pretty much related to Newton's universal law of gravity. We've got the gravitational constant. We've got the product of the masses. Mass of the earth, mass of the spacecraft, and in this case, divided by the first power of the distance. If you remember in the force, we have the second power of the distance. And of course, we have the minus sign in front if we double the distance, we go to half that original value but still with the minus sign and these are the only kinds of energy in our problem and if energy conservation holds, we know the following. The sum of these two energies has to be a constant. It must not depend on time. It has to stay at its initial value and that's something we can use to check our numerical solutions when this expression does not stay yet its original value, we do know that we're in trouble. Note that energy conservation does not hold in all models. In particular, if we have friction and all the heat produced by that friction, energy won't be conserved. Assume our spacecraft follows this elliptic orbit around the earth. At which point, A, B or C does it have the highest speed? The highest speed is reached at position B. We have energy conservation in this system that points B the distance of is the smallest, which means that at point B the potential energy is the smallest, But if the sum of potential energy and kinetic energy remains constant, kinetic energy has to be maximum at that point, which means that the spacecraft has achieved its highest speed at this point. The task now is to study what happens to energy with the forward Euler's method. The method is provided and your task is to fill a ray called energy, that's also provided, with the correct values and eventually the results are plotted to see what happens to energy. To fill the ray called energy, we go through all steps which is one more than you may think. The equation as such isn't really surprising. This time for the first time to use a line break, which is a break slash. What the results tells us is that energy keeps increasing. With every orbit, there's a gain in energy and that's pretty much visible in the trajectory, which of course is not the actual physical trajectory. The trajectory keeps expanding and expanding so the long-term behavior of this orbit is pretty poor. In physics, the conservation of energy is highly related to the conservation of area in phase space. I'm going to introduce that notion in a second. Solvers did also conserve that area, are pretty good at conserving the energy, not exactly but almost exactly. For simplicity, I'm looking at a one dimensional system. The position is one dimension just a single number, the velocity has one dimension, just a single number. Phase space is the coordinate x and v and this is actually use the momentum which is mass time velocity. I don't want you to be confused, so I stick to velocity. For every point in this phase space, you can look it's temporal evolution given the value of x and given the value of v, we can solve the system of differential equations and something is going to happen about that point and another point and to another point. You start from that initial condition to given x and the given v and look at what happens to this x and that v and look at what happens to position and velocity. The trick now is to look at infinitely many initial conditions at the same time. A complete some area in phase space. This is what we're talking about. Let's look at all of these initial conditions. What's going to happen with them? Where is the evolution going? Where is the differential equation taking us? Can this way, we see an evolution of area in phase space? This set of initial conditions will have to say one second end up here and after another second may be end up here and so and so on. An area in phase space remains constant, that's called Liouville's theorem. Why does the area in phase space stay constant as it evolves? Let's look at the first equation. The position changes with the rate of the velocity, so if you rate for a small time and look at wherever ending, x will increase a little here, increase more here, more here, and so on depending on v and this motion does not depend on x, it just depends on v. So, our initial area will be deformed to this which, is the same area. Simply use this part, glue it to that part and you'll see that the blue parallelogram has the same area as the top of the rectangle. So the first equation alone would not change the area. If you look at the second equation on its own, the rate of change of the velocity depends on x and on x only, so if this velocity is changed by that much, also this velocity will change by that much and that velocity will change by that much. The rate of change does not depend on v, it just depends on x, so we could have a different rate of change here but it has to be the same for all velocities and maybe there's a little bit of change here and again it has to be the same for all velocities. So our purple rectangle is transformed into something that may look like this and that still has the same area as the original purple rectangle had. To see that, simply take this slice and push it down and take this slice and push it down and so on and so on. So each equation alone would not change the area. If it continuously apply these rates of change, together we also have no problem. If x evolve for an infinitesimal short amount of time, and then we let v evolve for an infinitesimal short amount of time and then it's the turn of x again and so on and so on--that's going to work. If we apply the first equation for an infinitesimal amount of time, and then we apply the second equation for an infinitesimal amount of time, the area stays the same, and we apply the first equation again and so on and so on. If you step through this and infinitesimally short time steps, that's going to work. Problem is can be applied both equations at the same time and do not use infinitesimally short time steps then it's going to break and that's precisely what the forward Euler's method does. You can see one solution to that problem, you apply the first one for a finite time step then the second one for a finite time step then the first one and second one and so on one after the other, that's going to work, that's going to conserve a phase space area. This leads to the conservation of that area and solvers which have the same property are called symplectic. Sometimes people also use the term geometric integrators. We are going to build the simple most one of these and have a look at the long term behavior and there's something to note all of this works because the force neither depends on time nor on velocity. If the force was influenced by time or velocity that would most probably break this nice behavior. Homework 1 is going to show the evolution of phase space in action producing some nice diagrams. Let's look at the position and the velocity of the weight that forms the end of a standard pendulum that swings and eventually stop swinging. What happens to a given area in the phase space of that system? Does it remain constant, does it shrinks to half the original size, does it shrink to zero, does it grow to infinity, or does it grow to twice the original size. The solution is whatever area we're starting this, it's going to shrink to zero. We know that because the motion dies out. Eventually, the position will be fixed and the velocity will be zero. So any initial condition will eventually be taken to the same single point, which means that if I start with any area in phase space and forget all of these initial conditions of a time, they will converged to that single point which means the area becomes zero. Another one to take the forward Euler's method that you already know so well and turned it into one of these wondrous methods that consist of phase space area. Here I wrote down the equations for standard forward Euler's method. Now, there is a single change that's going to do the job. I do not compute the force at the original position. I compute the force at the new position. This leads to the following. This looks like a programming error on first sight. Soon, I'll be using the initial values on the right-hand side to compute the new values, but actually, it turns out that this the right thing to be doing in contrast to the forward Euler's method. Now, these two steps are executed in sequence. First, we change the position. And you already saw that this does not change the area in phase space and then we take the new position. The velocity remained unchanged and compute the new velocity, which also does not change the area in phase space, which we saw before and then we repeat the first step and so on and so on. This is now a symplectic method. It does not change the area in phase space. Your job is now to change the standard Euler's method, which we provided here for you, into the symplectic Euler's method and then take a look at the long-term behavior of energy. There's just a single change in here-- instead of using x at the old step, we're using x at the new step. We see that they're taking almost eight orbits around that planet and the trajectory does not seem to explode or to implode. Even better, we see that the energy follows a periodic pattern. There is no drift up or drift down in energy, and the overall changes of the energy are pretty small, but keep in mind that the energy is not precisely conserve. With some mathematical sophistication, one can show that there's a series of quantities that are close to energy and are conserved better and better. Now that we know how to reliably solve differential equations, we can return to rockets. What make things easy is that we're going to fire the rocket only for seconds, not for minutes. Think of what would happen if you could give the spacecraft an ultra-short but ultra-strong boost that would lead to an instantaneous change of the velocity. After that of course, also the position changes. We have different trajectory but the only thing that we need to take account of to incorporate that boost is to change the velocity. Your job is to boost the rocket 2 hours after start by 300 m/s. It maybe a good idea to use this variable that starts whether or not the boost has occurred yet. We provide the rest, namely, Heun's method in this case to provide accurate results. One way of solving this is the following. We check h times the step number, is the current time. If the current time is larger than or equal to 2 hours, and if we didn't fire the boost before, we fire the boost and then remember that we actually did fire the boost. So next time around, the boost is not going to be fired. You could in principle also check for equality over the time exactly equals to ours or not. But that's very dangerous with floating point numbers. You may miss by a very tiny fraction when the boost is to be done, we change the velocity by 300 m/s that is times the vector which looked somewhat crazy. What we need is a vector of length 1 in the right direction, that is in the direction of travel. 300 times that vector is going to provide us with a vector of length 300 in the direction of travel. So what happens to velocity? We take the existing velocity divided by it's length. This way, we get the vector of length 1 in the direction of travel. 300 times this vector is just the vector that we need to change the velocity vector. There is one more line in this solution. If we fire the rocket, we place a red dot and this is the result. We see the orbit starting here and then the rocket boost takes us to a different orbit. Now, everything is in place so that you can compute how to rescue Apollo 13, which is homework too. You can use sine and cosine to make the moon go around the Earth. You can use the differential equation to describe the motion of the spacecraft. You know how to fire the rocket to decelerate the spacecraft to take it to a low trajectory around the moon and you can compute by how much to boost the rocket to guarantee a safe return to Earth. The method to be using for that would be Heun's method because we need a quite amount of accuracy. This diagram is really not to scale the Earth and the moon are far more distant as shown here. This figure 8 loop is much more narrow. The packing orbit is pretty close to the surface of the Earth. And this loop around the moon is much closer around the moon as you see here. Actually, for the stimulation, we have to make this loop around the moon a little wider to not spend that much computation time on that. I want to close with some general comments about the uses of models and simulation. All simulation of Apollo 13 is just one example of what can be done with these techniques from mathematics and computer science. These techniques allow us to do experiments that can't be done in real life. They may be too expensive in real life. For instance, building 1000 cars to find the right aerodynamic shape. They may be too dangerous. For instance, if we ask the astronauts to actually try out different boost or they may be outright impossible--for instance, we want to look at the universe that has different fundamental constants of nature. We may use simulations for simple prediction, for optimization, to study causes and effects for instance of earthquakes or climate change, and we may use models and simulation to determine unobservable quantities such as the composition of the interior of the earth. What do you think--which of these four questions could be answered with the help of simulation? What's the time of sunset tomorrow, are dolphins more intelligent than elephants, can neutrinos travel faster than light, or who invented the telephone? What's the time of sunset tomorrow? For sure, we can answer that by simulation. We already simulated the motion of the moon around the earth and in the similar way, we can simulate the motion of the earth around the sun and the rotation of the earth. Are dolphins more intelligent than elephants? To answer this question by simulation which we try our model of how intelligence develops and depends on biology, that is pretty hard to do and may be impossible. Can neutrinos travel faster than light? That's something that physicists have to measure. You could imagine the model that contains more fundamental objects than neutrinos super strings for instance and the neutrinos could be simulated using super strings, but that's again far from the current state of the art and may be impossible. Who invented the telephone? To answer that question, you would have to model all of human history inside the computer That doesn't sound plausible. And this course we are mostly using the computer to do simulations numerically, that is by number crunching. The major reason for that is that most problems can't be solved by clean equations, unlike school book problems. The real road is way too complex to do that. A second reason is that particularly we have to base our simulations on huge numbers of measurements be it climate's data, be it financial data, be it data from biology, but the numerically approach also has a series of drawbacks. In particular, it's very hard to prove general results using numerics. Is there no way that a particle in some system escapes to infinity? You can look at a single trajectory at a time, but you cannot look at an infinite number of trajectories and numerical methods almost always introduce discretization errors also known as truncation errors. In particular, we have to work with finite steps in time. For which of these problems that a numerical approach be appropriate: to determine the heat capacity of the oceans, to prove that π/4=1-1/3+1/5-1/7 and so on and so on up to infinity, or to determine whether a control system is unconditionally stable or not. The numerical approach is just about perfect to determine the heat capacity of the oceans. In particular, as we need tons of geographic data, the numerical approaches are not good to prove something about mathematics, and particularly not when it comes to infinite numbers of operations, and to determine whether a control system is unconditionally stable or not is also not a great task for numerical approaches. We should first do some mathematics to get some general statements about that control system and then we can possibly use numerics on a very abstract level, for instance, to analyze matrices. So in this unit and the one before, we looked into how to model forces and acceleration and how to come up with a reasonably good numerical solution of a differential equation. This knowledge is of course something that you can apply in all situations in which velocities and forces and acceleration play a major role. We are going to apply it in a later unit to design an anti-lock breaking system. But differential equations can do much more than just describe vehicles. In the next unit, we will look at the spread of contagious diseases for instance by modeling a population as consisting of susceptible, infectious and recovered persons. Welcome back! Differential equations are all about acceleration and forces aren't they? Hmm, not really--in this unit, we use them to predict how many people have to be vaccinated to stop the outbreak of an epidemic and to find out about the effect of mosquito nets. As we do so, you'll see some more tricks to make simulations faster and way more robust. How can we model the spreading of a contagious disease? The straight forward approach may be to model individual persons as either infectious or susceptible If one infectious and one susceptible person meet, there is a certain chance that the susceptible person becomes infectious too and then the first infectious person as well as the newly infected person further spread the disease. This seems to cry out loud for a spatial model in which we take care of the location of these persons that would be complex. Luckily, there is another way. This is a much simpler way of thinking about such diseases. The SIR Model. We do not look at the spatial distribution at all. We simply say that our population is made up of susceptible, infectious, and recovered persons and we only look at the total number of these persons. Susceptible persons can get infected and then become infectious. The rate of that process is controlled by the number of infectious people. The more infectious people there are, the larger the probability is that a susceptible person meets an infectious person. Hopefully, people are going to recover from the disease, so that is the process that takes us from infectious to recovered. And then the standard version of that model, we assume that the recovered persons are immune and cannot be infected again. So they form a third compartment. This is a much simpler way of dealing with our problem. We forget about the spatial distribution altogether and simply construct a mechanical model if you will in which the population consists of three compartments, susceptible persons, infectious persons, recovered persons depending on the number of infectious persons, susceptible persons become infected and hence infectious and after a while, infectious persons recover and we assume that recovered persons are immune against the disease and cannot be infected again. This is the SIR Model. Susceptible, Infectious, Recovered. Now, let's turn the SIR model into equations. We're doing that one part after the other--first, the recovery. If I(t) is the number of infectious persons--it's I(t), and R(t) is the number of recovered persons--it's R(t), so which of these equations makes the most sense. The derivative of the number of recovered persons with respect to time equals 5 days times the number of infected persons 1/5 days times the number of infected persons 1/5 days times the number of recovered persons or minus 1/5 days times the number of infected persons. With this answer, the rate of change of the number of recovered persons would be negative, meaning that that number shrinks. This cannot be true. That number has to grow, so we can rule out this answer. This derivative of a number of persons with respect to time has to be something about persons per days. It cannot be days times persons, so this cannot be true. The remaining two answers are correct in that respect. This answer which need to unbound its exponential growth. This works like compound interest. The rate of change is proportional to the current value. This cannot be true. So we are left with this answer, which make sense. For instance, if we have twice the number of infectious persons, this rate would go up by a factor of 2--twice as many persons will recover per day. And imagine you have 5,000 infectious persons, then you get a recovery rate of 5,000 persons/5 days, 1,000 persons per day that recover. These 5 days seem to be something like a typical duration for the disease. So this is right answer. Now, let's look at the infection part of the SIR model. We've got a number of susceptible persons S and we've got a number of infectious persons I, and we watch the model the spread of the disease, so which of these equations makes more sense for the infection process. The derivative of the number of infectious persons with respect to time equals 5 * 10⁻⁹ per day times the number of susceptible persons minus 5 * 10⁻⁹ per day times the number of infectious persons 5 * 10⁻⁹ per day the number of susceptible persons divided by the number of infectious persons or 5 * 10⁻⁹ per day and person times the number of infectious persons times the number of susceptible persons. This answer would simply lead to a decay of the number of infectious persons no matter what the rate of change is a negative factor times the current value. This doesn't make sense With this answer, the rate of change which depend only on the number of susceptible persons. It would not matter to the rate of change of the number of infectious persons whether we started with 1 or 10 or 1 million infectious persons. This doesn't make sense. So we have to have a solution that incorporates both quantities. The number of susceptible persons and the number of infectious persons. So what about that ratio? This would mean that if the number of infectious person is low, this ratio is high. We divide by a small number and the rate would be high which again doesn't make sense. And there is another reason why this doesn't makes sense, the unit would be 1/days times persons/persons. This canceled and unit of the result is 1/days but what we need is persons per day, so this cannot be true for several reasons. This is what is left and it makes sense. Let's look at the units. A number divided by days and persons, times persons squared. What we are left with is person per day, that's what we need and this product has the right behavior. If we increase the number of infectious persons, we increase that rate. We have more encounters. That makes sense. We already look into the rate of change of the number of infectious persons. It was a constant times the number of infectious persons times the number of susceptible persons. If this is the number of persons per day that move from susceptible to infectious, the same number has to get lost from the susceptible compartment. So the rate of change of the number of susceptible persons has to be minus the same quantity, and then there's the second effect mainly that people recover. We've already looked at that term. This is the increase of the number of recovered persons per time. If this number of persons is added to the recovered compartment each day, we have to subtract it from the infectious compartment as well. We have to subtract it from the infectious compartment because these persons leave the infectious compartment and join the recovered compartment. This completes the equations of the SIR model. Obviously, this is a very rough model. In particular, as the numbers that we are dealing with here are not integer numbers, in real life they should be say 1,000 or 1,000 at 1 or 1,000 at 2 persons, but with these equations, we're dealing with say 1,000.1, 2 , 3, 4, 5 persons. These equations can only make sense for large numbers of persons so that the fractional part can be ignored, but if these numbers are large, the numerical constants that we're using here may for instance be used to model influenza. Now, that we have the equations for the SIR model, compute the sum of rate of changes. The rate of change of the number of susceptible persons plus the rate of change of the number of infectious persons plus the rate of change of the number of recovered persons, then look at the result. What do you learn from that result. Do you learn from the result that an epidemic will develop or that no persons are lost or that everybody recovers eventually or that the number of infections per time stays constant. The correct answer is that no persons are lost. Let's look into the reason for that. These are the creations of the SIR Model again. If we form the sum of these three derivatives and see that this term minus coefficient times I times S plus the same coefficient times I times S, cancelled in the sum and the same happens with this term, minus coefficient times I plus the same coefficient times I. So the sum of these derivatives is zero. For simple interpretation of that left-hand side here. If you look at this number, the number of susceptible persons plus the number of infectious persons plus the number of recovered persons form the derivative with respect to time, we get this expression. The rate of change of the sum is the sum of the rates of change. You see, in this case, I'm switching to the d/t notation because it's hard to place the dots on top of the sum. So what we learned is, that the rate of change of the sum is zero. If the rate of change is zero, there is no change and if there is no change, the sum is constant which means nobody is lost. Persons move from susceptible to infectious and from infectious to recovered but never move out of that system. We could see the same equation also if there was immigration, or births and deaths that compensate each other precisely. If you look at the equations of the SIR model, the meaning of this constant in front of the I here is pretty clear. This number of 5 days controls the infectious period of the disease, but what about the first constant here that controls the rate of infection. Assume that we're looking at a secluded small town with 1,000 inhabitants in total and assume that everyone meets 25 persons per day, and third, assume that the infection probability is 2% per contact, which number should be entered here. Just input that number. This number would be 0.0005 per day and person. Let's do the computation. Let's look at the given data. What do we know about each infected person? As every other person, each infected person that meets 25 persons a day. All this susceptible persons can be infected, so the question is how many susceptible persons are among these 25? We can compute that by ratio. If there are 1,000 people in total, then the ratio of the number of susceptible persons divided by the total of 1,000 persons times these 25 persons that would be susceptible. Imagine, everybody was susceptible then we would compute 1,000 persons times 25. All 25 would be susceptible. If every second person was susceptible, 500 divided by 1,000 results in one half times 25, every second person would be susceptible. So this is the number of contacts between a single infected person and susceptible persons per day and each of these contacts will lead to infection with a probability of 2%. So now, we can compute how many persons are infected by the single infected person per day versus the number of contact of susceptible persons per day and multiply with probability. This will be the number of new infections per day. We can cancel the persons, these three numbers combine to 0.0005 per day and per infected person, and that's precisely the expression we are needing in the SIR equations. Constant times the number of susceptible persons times the number of infected persons. But keep in mind that we are dealing with average values here, 25 persons per day give or take a probability of 2%, things can occur more often or less often. Go back to this. Let's classify the types of differential equation that we are seeing here. What's the order--is this is a system, is it explicit, is it linear? The highest derivative is the first one. We have the first derivative, the rate of change, but we do not have a second derivative of a rate of change. Yes, it's the system. We have several equations in parallel. It's explicit. The highest derivative appears alone on one side and nowhere else, and this maybe a surprise, it's not linear. The problem is this term and this term as well. They are multiplying one component of the solution by another component of the solution. This is something like x(t)². If we take the standard SIR model and add this term 1,000 persons per day in the equation for the rate of change of susceptible persons, which effect could that be-- vaccination, immigration, births, or deaths? If we increase of change of susceptible persons, that cannot possibly be vaccination. We also would increase the total number with that if you form S+I+R, the total number. You'll see that the rate of change of the total number would be precisely these 1,000 person a day positive rate of change. It cannot be deaths, so we're left with births and immigration. Births do not make too much sense. The total number of persons would increase, but as the total number of persons increases, the number of births should also be increasing. It should not be constant. What's left is immigration, which makes much sense. 1,000 susceptible persons are added to the population per day. Again, we start from the SIR equations. Now, we include initial conditions, namely that the initial number of susceptible persons should be 98,900,000. We want to have 100,000 infectious persons at the start and 1 million recovered persons at the start. Your task is now to model that 5 million people are vaccinated at the beginning. Here are four options of which you have to pick the right one. A--change the 5 to a 4 in both equations, B--subtract 100 persons per day from the S equation, C-- change these initial conditions to 93,900,000 persons and 6 million persons, D--add 5 million persons per day to the R equation. Now, comes the answer--option D would mean that we have a huge amount of immigration of recovered persons--no. Option B would mean that the number of susceptible persons is reduced by 100 persons per day that looks like immigration but not like vaccination. With option A, we would be changing this coefficient. If you remember where this coefficient came from, it had to do with the probability of being infected, mainly more precised with the probability that a susceptible person is infected by an infectious person and it had to do with the number of contacts. It doesn't have to do with vaccination. It's option C, we move 5 million persons from the susceptible compartment to the recovered compartment. Of course, we're cheating a little there. These 5 million persons have not recovered from the illness. They are immune as are all other recovered persons. Let's have another look at the SIR Model to learn about an important phenomenon and that's Herd Immunity. On the right-hand side of the I equation, the factor I appears in every term. If we factor it out, this is what happens. This helps us to investigate what happens when a huge percentage of all persons is vaccinated. We model vaccination by taking persons from susceptible to recovered, which means that the number of susceptible person decreases if the size of the population says constant. When the number of susceptible persons shrinks, the first term in the left parenthesis decreases and eventually if you vaccinate enough people, the complete expression is going to become negative. The rate of change is negative, this means that the number of the infectious persons is decreasing over the cause of time, more susceptible people get infected. The number of susceptible person decreases which means that this first term here gets even smaller which means that the expression in the right parenthesis gets even more negative. The number of susceptible persons can only decrease when enough people have been vaccinated, we can never have an epidemic. This is called Herd Immunity. Not everybody is vaccinated, but if the number of people who are vaccinated is large enough, we can not have an epidemic nonetheless. Assume that we have a population size of 100 million, which percentage of that population needs to be immune so that we are at the threshold of Herd Immunity and enter an integer number of percent here. Now, the answer--that's a 60% and here is why. At the threshold of Herd Immunity, the expression in the red parenthesis has to be 0. If we vaccinate even more persons, the number of susceptible persons shrinks further, and this expression becomes negative, and now we can solve for the number of susceptible persons. And we get 1/5 days divided by this coefficient--the days answer and we get the number of 40 million persons--that's 40% of the population of 100 million. If the number of infectious persons is negligible at that point of time, the remaining 60% have to be in the R compartment of the model. They have to be immune. If more than 60% of the population are in the R compartment, this expression is negative and we can never have an epidemic outbreak. So 60% is the threshold of Herd Immunity. So this is the model that we started with. There's a compartment of susceptible persons, a compartment of infectious persons, and a compartment of recovered persons. Depending on the number of infectious persons, susceptible persons become infectious, and infectious persons automatically recover after awhile. Now, we're introducing another effect latency. Persons who get infected should not immediately become infectious and we model this by introducing another compartment--exposed persons. Depending on the number of infectious persons and susceptible persons, susceptible persons become exposed--that is infected and after awhile, become infectious. This should obviously be called the SEIR model. Now, that we have the mechanical idea behind the SEIR model, turn that into a set of equations. For each term that may appear in these equations, enter the corresponding letter into one of these boxes. Leave that box empty if it's not needed. 1/1 day, 1/5 days, 5*10⁻⁹ per day and person, the number of susceptible persons, the number of exposed persons, the number of infectious persons, the number of recovered persons, the product of the numbers of infectious and susceptible persons, and the product of the number of exposed and infectious persons. In our model, the number of susceptible persons always decreases. It cannot have any component here with a + sign and leave this blank. And similarly, the number of recovered persons always increases. It cannot have something with a - sign here. A certain fraction of the infectious persons is taken per day to the recovered compartment. What worked best is 1/5 days, b times well the number of infectious persons f. We need the same here with a - sign. Everybody who leaves the infectious compartment joins the recovered compartment. The similar process is in place with e and i. Exposed persons become infectious afterwards. The rate should be a, 1/1 day seems to be plausible latency of 1 day times the number of exposed persons. Certain fraction of the exposed persons per day gives the exposed compartment and joins the infectious compartment. That same number that leaves here has to joint here, a,e and here we have the infectious process. The number of exposed, infected that is, persons increases per day by 5*10⁻⁹/day times the product of the number of infectious persons times the number of susceptible persons. So, it's c, h and what we gained here, we have to use here, c, h. There's one thing to keep in mind though. This latency period of one day is an average. It's not a strict number similar to this recovery period of 5 days. These are average numbers. The small percentage of exposed infected disease persons will be taken pretty quickly to the infectious state and the small percentage will take quite a while to get to the infectious state. This times of 1 day and 5 days are not rigid. They are averages. Now, it's time to solve the SEIR model by computer. We've provided the constants that we've been using all of the time. We are providing erase to write the results. We're providing the initial values. And your job is to actually solve this system of differential equations. One way of solving this is to find out which terms appear in several places-- for instance, this one--the number of persons taken from the susceptible to the exposed compartment and then the number of persons taken from the exposed to the infectious compartment, and the number of persons taken from the infectious to the recovered compartment. If you write it like that, it's easier to read and you can immediately see that the total number is conserved. The number of persons we lose here we gain here, the number of persons we lose here we gain here, and the number of persons we lose here is gain here. So up to very tiny round off errors of the total number and result you'll see that the infection takes quite some time to evolve. The maximum of the exposed compartment comes first and then comes the maximum of the infectious compartment, and as we go, you'll see that the number of susceptible persons shrinks drastically and the number of recovered persons grows by the same amount. This quiz builds on the one before. Now, the Euler method is in place and we want to experiment with the step size. If you look at the solution curves, you could get the impression that a step size of half a day is way too small. Why should we be stepping in half days. What about steps of say 5 days. Shouldn't that be working given these curves. Let's step size in and leave that step size in for which the solution just doesn't explode and accuracy of 1 decimal place is enough. This is what happens when we set the step size to 2 days, which look pretty reasonable with the original curves. This is what we get--the numbers explode to 1.4 * 10⁷⁹--that doesn't look too good. In case you're wondering about the conservation of the total number, some of these variables get huge in a positive direction and some of these get huge at the negative direction. Actually, the total number stays pretty much constant at the price of some of these becoming negative. With a step size of 1.9 days, which is the solution, we don't see an explosion, the number stays limited but the behavior is, of course, highly implausible. Now, we look into what's going wrong here. In the previous quiz, you saw that the solution evolves on a pretty long time scale, but the step size that we are required to use and the solver is pretty short. That's a phenomenon that can occur with certain types of differential equations. It's called stiffness--it's of course annoying if we are forced to use a very small step size even though the solution develops on much longer time scales. There is no generally accepted definition of what stiffness precisely means. But if you think about this situation, you may get an idea about where the term comes from. Imagine that we have a pendulum, a mass attached below and this is a stiff spring and the pendulum swings left to right and can oscillate up and down. Then we have two different time scales, the time scale of the swinging motion and the time scale of the oscillation of the spring and this time scale of the oscillation is going to be much smaller than the time scale of the swinging motion. The observer, however, mostly sees the swinging the motion and can't see the oscillation up and down. This oscillation, however, will force us to use a very tiny time step in a forward Euler solver. So in which of these three situations would you expect stiff differential equations-- the motion of a spacecraft around a planet, cloth simulation where you, for instance, simulate how one shakes a tablecloth, or a 1-dimensional harmonic oscillator, which strictly moves up and down? The vital ingredient for stiffness is that the model includes to vary different time scales. That's not the case for this spacecraft and that's not the case for the 1D harmonic oscillator. But with the cloth simulation, we do have to vary different time scales. Imagine this cloth to be composed of tons of tiny, tiny springs. This is actually how cloth simulation is being done. If you push the cloth in this direction, you're working against the stiff spring, meaning you're working on a very small time scale. If you push the cloth up, you're rotating that spring, meaning you're working on a slow time scale. This is a very annoying problem for the simulation of pieces of clothing, in video games, and for special effects in movies. You do not want to use time steps of microseconds for something that seems so simple. To study what's going wrong when trying to solve stiff differential equations numerically, we'll typically looks at this type problem of very simple differential equation which by the way is not stiff but serves to help us understand what's happening here. The derivative of x with respect to time should minus a constant times x itself. This constant is supposed to be a positive number that's called Dahlquist' test equation and it's easy to figure out what the solution should be. The rate of change is proportional to the current value with the negative proportionality constant. This means that per time step, we are losing a specific percentage of the current value. If we start here, we lose that specific percentage and lose that particular percentage and again and again and again. This is exponential decay, so we know that as t goes to infinity, any solution will approach 0. This decay to 0 takes place regardless of the value of k. As long as k is positive and this proportionality constant, any solution is going to decay to 0 regardless of the value of k. If k is larger, this decay is faster. If k is smaller, this decay is slower. And of course, we want that our numerical solution has the same behavior when we simulate this equation in the computer, we want to see the decay to 0. Technical term here is stability. How small does the step size have to be for the solver to stay stable? Now we're going to analyze the forward Euler method with the help of Dahlquist test equation. In which range should the step size h lie to maintain stability. Should that step size be smaller than k²/2, k being our constant. Should the step size be smaller than the inverse of that constant. Should it be smaller than twice that constant, or should it be smaller than 2 over that constant. Let's look at what the forward Euler method does to this equation. X after the first step will be the initial value of x plus the time step times the rate of change at the initial time plus the rate of change at times 0. What's the rate of change at times 0? It's -kx(0). We can factor out x or 0 and end up with this expression. That's the result after the first times step. The initial value gets multiplied by 1 minus step size times the constant that appears in the test equation. Now, let's do the second step--it's x at the end of the previous step plus h times the rate of change at that time or actually our estimate of that rate of change at that time which would be -kx(h), again we can factor out and we can plug in the value of x(h) from the first step, so we end up with (1-hk) times now x(h), (1-hk) x(0) which is if we combine these two, (1-hk)² x(0) and you see how this is going to proceed if you go to (3h), this will become (4h)^4 and so on. The question was whether or not x is going to approach 0. This boils down to whether or not (1-hk)ⁿ approaches 0 as n times to infinity. So what happens to the nth power of (1-hk), we are interested in how large h the step size can be? If the step size h gets large, the expression inside the parenthesis becomes negative, we are subtracting from 1. If we are subtracting more than 1, the inner term here gets negative. So the question here is what happens to powers of negative numbers to some huge power. As an example, what happens if the inner expression equals -2 and we take the 1st, 2nd, 3rd, 4th and so on power. (-2)¹ is -2. (-2)² is 4. (-2)³ is -8. (-2)⁴ is 16 and so on which means that this expression, the nth power would explode and this is precisely not what we want for a stable solution. So let's try a different number. So what would happen if the inner expression was -0.9, and we took powers of (-0.9). (-0.9)¹ is 0.9. (-0.9)² is 0.81. (-0.9)³ is -0.723. (-0.9)⁴ is 0.6561. Each time, we are losing 10% in magnitude but you see that the sign keeps changing, minus, plus, minus, plus, so this indeed decays to 0 but in a strange fashion. It oscillates. You may remember this behavior from the SEIR model that we simulated before. If this expression would equal -0.99, it would still see decay with this sort of oscillation. If it would equal -0.999, this would still work but become very, very slow. So the threshold would be that (1-hk) is in the expression equals -1. The threshold, it's 2/k. So the condition is h is smaller than 2/k for this power to decay to 0 and for the forward Euler method to be stable. In the quiz, we analyze the stability of the forward Euler method. Now, we introduce some changes to turn the forward Euler method to backward Euler method. This method is called an implicit solver. We are going to see why. The backward Euler method works like this to go from the initial value of x to the value after the first step, it added the step size times the rate of change at times 0. The backward Euler method works with the rate of change at the end of the first step, x(h). Actually, our estimate of the rate of change at the end of the first step. Keep in mind that I'm writing x(h) but I'm referring to my estimates, the numerical solution of the exact solution. This means that the value at the end of the first step appears on the left-hand side and on the right-hand side. This is why this is called an implicit method, we have to solve for x(h). It's hidden in this equation. It's not explicit so let's solve for x(h). When this term over to the left-hand side, then we have x at the end of the first step plus time step times constant times x at the end of the first step equals the initial value, we can factor out x(h) and find that x(h) equals the initial value divided by 1+hk. This is something like 1/1+100 if you choose a really large h which would be 1/101. If you form powers of these, they are going to decay pretty quickly and there is no oscillation whatsoever, so we have no problem at all with this expression. It's always going to work no matter how large the steps as is. This is very handy when one tries to solve stiff differential equations. Your job is now is to implement the backward Euler method for the SEIR model. We provide the forward Euler method as a starting point. So this end, we have reshuffled the order--EISR to make things easier. We can start with the R equation using what you have computed for E and I and S before and then you can continue with S and with I. Doing the backward Euler method for E, it's a little harder. We provided the solution in case you don't want to spend that time. If you start from the bottom, you can check your result whether or not it make sense by running the software for that single line. When you're done, set the time step to 5 days to see that this method can handle such a long time step. Let's start with the output of that software. For reference, this is the result of the forward Euler method with a step size of half a day. You already saw that the Euler method had step sizes of 2 or more. But there's a price to pay for the stability of the backward Euler method. The code gets really complex as opposed to the code of the forward Euler method. You really do need pencil and paper to derive these equations. That's what we're going to do now. The equation for R, the number of recovered persons, is the easiest one. We'll start with that and to save space let me write under score 1 for the current quantities and subscript 2 for the quantities of the next step. The forward Euler method would turn this into the following. The value of R one step later is the current value of R plus step size times the current rate of change, which is 1/5 days times the current value of I. I₁ in my new notation. This would be the forward Euler method but now we're concerned with the backward Euler method, which means that we have to use the rate at the end of that step meaning I₂ has to go in here. And now we're done. We know the current value of R. We have computed the next value of I in the lines before, and that's all we need to compute the next value of R. Next up is S. If we were using the forward Euler methods, this would work as follows. The next value of S is its current value plus the time step times its current rate of change meaning -5x10⁻⁹ per day and person times I at the current step and S at the current step if we were doing forward Euler. But now that we're doing backward Euler, this has to be I one step after that and S one step after that. We bring this down to the left hand side because we're interested in S₂ and we factor out S₂. So the factor of 1 times S₂ and now it's plus this coefficient times I₂ times S₂. 5x10⁻⁹ divided by day person I₂ and all that remains on the right hand side is S₁ and now that is simple to solve. S₂ equals S₁ divided by the very same expression that we've got here. I'm lazy. I don't spell it out. This completes the computation of S₂ for which we need I₂. But we have computed that in the line before. And now the equation for the rate of change of I, the number of infectious persons. Those persons that is infected persons become infectious with a certain time constant and infectious persons recover with another time constant. If we were implementing forward Euler, it would be doing the following. I₂ the value of the next step equals the current value plus the time step times the current rate of change, which is 1/1 day E₁ minus 1/5 days I₁ but now we want to implement the backward Euler method, which means that we have to use the values for the next time step and this rate of change. I bring this term with the I₂ over to the left hand side and factor out the I₂. This leaves me with one I₂ plus h times 1/5 days. On the right hand side, I₁ remains and h times 1/1 day and E₂ remains. And now we can divide by this expression and find I₂ equals the right hand side divided by this expression I₁ plus h times 1/1 day times E₂ divided by this expression. This completes the equation for I. The main message here is that implicit methods come at a price. Solving these equations can get very ugly. If you are convinced at this moment, you may want to skip the rest of the segment. Otherwise, stay with me to see how we can compute E₂, which by the way is needed to be able to compute I₂. A trick to find the value of E for the next time step is to look at these two differential equations and parallel. For the first one, the backward Euler method says the value for the next time step is the value for the current time step plus the time step times our estimate of the rate of change for the next time step which is 5x10⁻⁹ per day and person times I₂ S₂ backward Euler minus 1/1 day E₂ backward Euler again and not E₁. And for S the new value is the old one plus the time step times minus our coefficient 5x10⁻⁹ per day and person times I₂ S₂ backward Euler again and that's it. If you now form the sum of these two equations, you'll get E₂ plus S₂ equals E₁ plus S₁. This ugly expression cancels with that ugly expression, and we are left with h times minus 1/1 day E₂. So this is what we have reached by now. E₁ the current value is known. S₁ the current value is known. We want to determine E₂, the next value. At this point, we do not yet know S₂. But luckily enough, we derived an equation for S₂ to be for namely S₂ equals S₁ divided by 1 plus the step size times 5x10⁻⁹ divided by days and persons times I₂. And we have an equation for I₂ that says I₂ equals I₁ plus step size times 1/1 day E₂ divided by 1 plus step size times 1/5 days. If you use this equation to express I₂ and then insert this result as S₂, you end up with an equation that contains E₂ and E₁ and S₁ and it can be solved for E₂. E₂ appears in three places and with a little edge ??? you can turn the resulting equation into a quadratic equation which is the standard solution formula for quadratic equations. The result is what you see in the code. Hopefully, this is now really convincing that working with implicit servers requires quite some effort with pen and paper. Let's summarize facts about implicit solvers. The simplest of which is the backward Euler method. The big benefit is the stability. We can use large step sizes. And stiff differential equations don't force us to use overly tiny step sizes, which results in speed. Some types of simulation are almost impossible without implicit solvers. Think of cloth simulation and the simulation of water vase. These types of simulation may be forbiddingly slow without the speed offered by implicit solvers. The big disadvantage of implicit solvers is that they are well implicit. You get equations that we have to solve, which is hard to do or can even be impossible to do with standard functions. Mathematicians have come up with quite a range of methods to address this issue. For instance, in a predictor-corrector approach, you use an explicit solver such as the forward Euler method to predict the value for the next step and then use an implicit method to correct it, actually Heun method, that we learned about in a previous unit is of that type. All you can use is a rate of methods to numerically solve this implicit equation, not with pen and paper. And the second disadvantage of implicit solvers, most of them tend to lose energy in physical simulations. We saw that the forward Euler method, an explicit solver, increased the energy. Implicit solvers tend to work the other way round and to lose energy. Now, let's have a look at the zoo of methods that we've seen so far and then let's add one further method to that collection. The forward Euler method work by starting from the current value and then incrementing that by step size h times the rate of change at the current point. I'm writing f for the rate of change here. Heun method also known as the improved or the modified Euler method advances by the time step times the average of the rate of change at the beginning plus the rate of change at the position predicted by the forward Euler method. With the forward Euler method, the error of a numerical solution grows linearly with step size. If you double the step size, the global error also doubles approximately at least. The forward Euler method is a method of order 1. This function, the error grows like h¹. Heun method, however, is the solver of order 2. The error grows like h squared. If you take half the step size, the error is shrinking to one quarter, which is much more efficient. For the backward Euler method, you advance by the rate of change at the new position, which makes this equation difficult--it's an implicit equation. To complete our zoo of methods, we add a method that looks like Heun method but is implicit like the backward Euler method. This new rule is called the trapezoidal rule. Like Heun method, it's a method of order 2. And if you compare these equations, they look pretty similar. The trapezoidal rule advances by the average of the rate of change at the beginning and the rate of change at the end, which means that this is an implicit equation. Now apply the trapezoidal rule to Dahlquist test equation. How large can we chose the step size and still have numerical solution that approaches 0, x(t) goes to infinity. Do we have to chose a step size that's less than 1/k, k being the constant of Dahlquist test equation, or a step size that's smaller than 2/k or a step size that's smaller than 1/k², or could we pick any positive value for the step size h. It turns out that all positive values for the time step are okay no matter how high. This is how similar to the backward Euler method and now we're going to look into why that is the case. The trapezoidal weir turns this differential equation into the following. The next value of x equals the current value of x plus the time step times the average of two rates of change, the current rate of change (-kx₁) and the next rate of change (-kx₂). I bring the x₂ to the left-hand side and combined this x₁ with that x₁ and get (1+hk/2) * x2 = (1-hk/2) * x1. Let's check that, 1x₂, it's here. (hk/2)x₂ appears on the right-hand side with a - sign. 1x₁ appears here minus (hk/2)x₁ - (hk/2)x₁ appears here. Correct. And that's easy to solve for x₂. x₂ = (1-hk/2)/(1+hk/2)*x₁. If you look at what happens with the next step. We of course, get a second factor of that sort. The next next step, another factor of that sort. So what happens is that we get powers of this factor. The question is what's going to happen about powers of that factor. So we see that to get it from x₁ to x₂, we have to multiple by this factor and this is going to occur every step if we want to go from x₂ to x₃, we again multiply by that factor and so on. So in the end, we get the powers of this factor. So the question is as the nth power of this expression converts to 0, as it intends to infinity to check that, one can analyze three different cases. Let's first check what happens if the step size is moderate. For instance, if this expression hk/2 equals 0.1 and we get (1-0.1)/(1+0.1) meaning (0.9)/(1.1), this is a positive number, <1. If you take higher and higher powers of that, this number is going to become 0. The numerator becomes 0.9. The denominator becomes 1.1. The fraction is a number between 0 and 1 and if we take higher and higher powers of such a number, the results converge to 0, so this is stable. So in this case, we have stability. If hk/2 happens to be equal to 1, we get a fraction with a 0 in the numerator, that's always 0, no problem with that, so we have stability in that case. And if you use a really high value of h, for instance, such that hk/2=9, we get something like 1-9=-8 divided by 1+9=10. This is a negative number with absolute value less than 1. So, if we take higher and higher powers, the result converges to 0, we also have stability in this case but you see that this convergence comes along with an oscillation that's time changes from minus to plus to minus to plus as we form even powers and odd powers. The result has a positive sign. For odd powers, the result has a negative sign. So we see some kind of decaying oscillation here. Let me summarize some observations that we made in this unit about the complexity of models. For instance, in the SIR and SEIR models, we worked with fractional population numbers. We didn't care whether or not the numbers of persons are integers. In the SEIR model, we used an average amount of latency. We did not build in a fix delay and we assumed perfect mixing. There is no geographical distribution, there is no age distribution, there are no isolated towns. One of the advantages of the simple models we looked at in this unit is that we were able to derive general statements such as the existence of Herd Immunity. Of course, we have to be careful. A general statement about a cartoonish model may not be that helpful, and the simple equations that we've got from these models are helpful in may ways. For instance, when it comes to working with implicit methods such as the big foot Euler method. In this unit, we look at models with flows between different compartments. The obvious use was in epidemiology where we had susceptible persons who became ill and eventually recovered, but what about economy. Money may flow from one account to the other or goods may flow from one country to the other. In chemistry, you may have several types of molecules that combined to form other types of molecules. In biology, the members of a species are born young, become adolescent, and then mature. In the next unit, we are going to look at examples from biology, in particular, to study equilibria that is situations in which one falls is exactly balance by another falls. Hi! How can we survive without overusing nature's resources? It's of vital importance here to maintain a balance. This is what we examine in this unit, focusing on fishing-- trying to find the best strategy that serves both humans and nature. On the way there, we encounter some fundamental concepts of differential equations and in the larger fleet of simulation. We're looking at two different processes that counteract each other. The growth of the fish and the harvesting of the fish. Let's ignore harvesting for the moment. A simple assumption about the growth would be that the amount of fish increases year by year by the same percentage similar to compound interest. To make things simple, we are measuring the amount of fish as a number of tons not by counting. Here is a quiz for you. Let's not include harvesting. What should the right of change of the amount of fish be: 0.5 per ton times the amount of fish minus 4 * 10⁵ years per ton, or minus 0.5 per year times the amount of fish plus 4 * 10⁵ tons per year, or plus 0.5 per year times the amount of fish minus 4 * 10⁵ tons per year. The first answer can't be true because the units of measurement are wrong. This would be tons divided by tons which would leave us with a measurement unit of 1 and years per ton won't work either--we had to have tons per year, the rate of change of a number of tons, so this doesn't work. This expression would not more the growth but the decay of fish minus the constant times the number of fish and this would be something like immigration not harvesting. Some constant is being edit to the rate of change. We have an immigration of 4 times 10⁵ tons per year, so these two can't be the answer, and we are left with the third one--0.5 per year times the number of fish. The positive growth rate as is appropriate for exponential growth and then there is a loss with a minus sign, which models harvesting. Of course, exponential growth is highly unrealistic. Let's have a look at this diagram. Now the horizontal axis is not the time, it's the amount of fish, and the vertical axis is the growth rate. With exponential growth, the growth rate does not depend on the amount of fish. Even if the ocean were full of fish, we would still have the same growth rate, which cannot be true. There should be some sort of carrying capacity at which the growth rate becomes zero. The easiest way to include this effect is to use the growth rate. that depends on the amount of fish in the linear fashion. This is called logistic growth. If the amount of fish is very small, we start with the high growth rate and as the amount of fish increases, the growth rate drops and if the amount of fish reaches the carrying capacity of the environment, there won't be any growth anymore. How do we turn this effect plus constant harvesting into an equation. Here are three options for you to choose from. Should the rate of change of the amount of fish be of 0.5/1 year times the amount minus 10⁶ tons minus 10⁵ tons per year or should it be 0.5 per year times 1 minus the amount over 10⁶ tons times the amount minus 10⁵ tons per year or should it be 0.5 per year times the amount minus 10⁶ tons times the amount minus 10⁵ tons per year. With the first one, the units of measurement would be okay for a rate of change of tons per year. If you look at this first expression, you see that it's not doing what it supposed to be doing. Actually we are still multiplying the current amount of fish by 0.5 per year but still the same growth rate all of the time. The second term 0.5 per year times minus 10⁶ tons is just similar to harvesting a huge the amount of harvesting--this cannot be the solution. The third option doesn't work because the unit of measurement are wrong. Tons times tons divided by year, tons squared per year. That's not what we want to have. The second one is the one that works. The initial rate of change that's 0.5 per year is multiplied by a factor that's 1 if the amount of fish is 0--1 minus 0 over something and is multiplied by 0 if that amount is 10⁶ tons 1 minus 10⁶ tons/10⁶ tons. This factor modifies the initial rate of change in the way that we need it and here of course, we don't have harvesting. Now let's use the computer to learn what happens with different harvest rates. If we would not be harvesting at all, we would expect to see something like an exponential growth at the beginning, but after a while the amount of fish would approach the maximum carrying capacity. If you were harvesting too strongly, you would expect the number of fish to quickly go down to zero and then of course stay zero--that's clearly over fishing. Now, implement this using the forward Euler method and make sure that you stop harvesting when the amount of fish reaches zero. All constants that you need are already set up in the code. You only have to provide the implementation of the forward Euler method to fill the array called fish. Let's first have a look at the output. You see that with rates of 20,000 and 50,000 tons per year, we do not seem to be over fishing. However, with rates of 100,000 and 200,000 tons per year, we clearly are over fishing, fish go extinct. The line of code that actually implements the forward Euler method is produced straightforward. The harder part is to stop harvesting and this solution is as follows-- we check whether or not the amount of fish for the next step is equal to zero or negative. If so, we set a flag that tells us whether I should be using zero for the amount of fishing in all upcoming steps. Note that you cannot simply check whether the amount of fish for the next step is exactly equal to zero--that's probably not going to happen. And these simulations for harvesting fish, you see several situations in which the amount of fish stays constant. You could say the falls up and the falls down exactly balance each other. This has caused an equilibrium or steady state or fixed point. These three terms are almost interchangeable. I'm going to use the term equilibrium. In most cases, equilibria can be classified as either stable or unstable even though in some cases you may have a mixed form. This position at the bottom of the valley is a stable equilibrium. If you start close to it, you will end up at that position. This position on top of the hill is an unstable equilibrium. If you happen to be in this precised spot, you are going to stay there, but if you're off just by a little, you're going to roll down the hill. Let's look at this equation for just the growth and constant harvesting and let's take care of the fact that we cannot harvest any longer once the fish become extinct. At which amount of fish do we find an equilibrium? All of these are given as multiples of 1 million tons. 0, 0.33, 0.50, 0.55, 1.0, 1.21, 1.45 and 2.0. Obviously, there is an equilibrium at 0--f stays 0 once it becomes 0. This is not nothing but an equilibrium and there's two more-- one is at 0.55 million tons and the other is at 1.45 million tons. The first one is obvious, but now let's look into where these two other equilibria come from. If we reach an equilibrium, the amount has to become constant. The rate of change becomes zero, which means that the gain per time has to balance the loss per time. The loss per time is constant but the gain per time depends on the current amount. Let's build this type of diagram to show how the gain depends on the amount. If you look closely, you see that this has to be a parabola. If the amount of fish is zero, we are multiplying by zero here and the gain will be zero. That's no surprise. If there is no fish, there is no growth. We know this point. When the amount of fish is equal to the maximum carrying capacity, this factor becomes zero. The product is zero again so we know this point. As a parabola is symmetric, the maximum has to occur in the middle between the zeros. This maximum sits at an amount of 1 * 10⁶ tons. We can plug this value 1 million tons into this expression and get that the gain here is 2.5 * 10⁵ tons per year, which is a little more than the loss, which is 2 * 10⁵ tons per year. We have one equilibrium here and one equilibrium there. At both of these points the gain and the loss, the gain and the loss balance each other. If you want to, you can solve this quadratic equation to find that this equilibrium sits at 0.55 * 1 million tons and this equilibrium sits at 1.45 * 1 million tons. Now we know that there are three equilibria--one trivial, one at 0, and 2 further ones down the road. Which of these are stable and which of these are unstable? The one at zero is stable, the next one is unstable, and the last one is stable again. Why is that the case? Assume that we are close to zero so that we have a small amount of fish and this harvesting rate is going to quickly reduce that amount to zero again, which makes zero stable. By the way, we can't approach zero from the other side. There won't be any negative amount of fish. The second equilibrium is unstable. Imagine that we had a little less fish that we would harvest more than we gain in this specific period, which means we are going to further drive down the number. We are moving away from that equilibrium. Similarly if you have a little more fish then the growth rate exceeds the harvesting rate. We are going to have even more fish. In both directions, we are moving away from the equilibrium It's unstable. And now the third equilibrium, if we have a little less fish, the growth rate being curve exceeds the harvest rate, more growth than harvest. We are going to end up with even more fish. We're moving up on the horizontal axis towards the equilibrium. If we have a little more fish, the harvest rate exceeds the growth rate. We are fishing more than we gain, which curves down the amount. So from both sides, we are going to approach this equilibrium, which makes it stable. If we are in this region, we are definitely over fishing. The amount of fish is going down all of the time towards zero. The harvesting rate is way to large for the small amount of fish that we have in this region. Now we look at different harvest rates--2.0, 2.5, 3.0 times 10⁵ tons per year. The middle one happens to be the same 2.5 * 10⁵ tons per year. The question is will the amount of fish eventually decay to 0. Do we have a strong issue with over fishing? This is the quiz for you--what about the amount of fish for the blue harvest rate-- will it never shrink to 0, does it depend on our circumstances whether or not it shrinks to 0, will it almost surely decrease to 0, or will it even surely decrease to 0. The same for the hour's rate of 2.5 * 10⁵ tons per year and the same for the harvest rate of 2.0 * 10⁵ tons per year. This is going to lead us to the concept of maximum sustainable yield. We have already covered the lowest harvest rate of 2.0 * 10⁵ tons per year, and we saw that we have a problem with over fishing if we are below this equilibrium point. If we are above that equilibrium point, the amount of fish will not decrease to zero. For the lower value, it depends. Namely, it depends on the initial amount of fish. The upper harvest rate clearly amounts to over fishing independent of the initial amount of fish. The loss is always larger than the gain. In this case, we surely have the situation that the amount of fish decreases to zero. If we use this harvest rate that just touches the peak of our gain graph, things get a little tricky. If we start with an amount of fish that's below 1 million tons, we lose more than we gain per time, which means that the amount of fish is going to become zero eventually If we are in the region above 1million tons also the loss is larger than the gain so we are moving towards lower amounts, but in principle, we would be stopping at this equilibrium point where the loss equals the gain. This is what would be called semi-stable equilibrium. On the one side, it is unstable and on the other side it's stable, but imagine what happens if we have eventually reached this equilibrium from the other side, just a tiny disturbance would suffice to make you drop down to the left end of that graph. Almost surely the amount of fish will decrease to zero if we pick the rate of 2.5*10⁵ tons per year. And this rate is called the maximum sustainable yield. In theory, you could be harvesting at this rate forever and ever, but of course, that's pretty dangerous whenever there's this largest disturbance. We saw that fishing with the harvest rate that's equal to the maximum sustainable yield is pretty dangerous. Let's try to come up with something that's safer-- for instance, ramping up the harvest rate over the course of time. We don't harvest during the first four years and after a time of 6 years, we're not using the maximum sustainable yield but we're using something that's a little smaller 0.8 times the maximum sustainable yield. The 0.8 would be a safety factor and between 4 years and 6 years, let's have a linear ramp. Now implement this ramp in the computer model. The forward Euler's method is already in place. You just have to provide the code that computes the harvest rate, which is used here. We can solve this for instance by distinguishing between three different cases. First case, time is between 0 and 4 years, the output is 0. Second case, the time is 6 years or greater. The output will be 0.8, the maximum sustainable yield. Third case, we need a function to describe this line if the time is between 4 years and 6 years. The slope of this line is 0.8 times the maximum sustainable yield divided by this time span 6 years minus 4 years. The slope is multiplied by t, but if we're just using this expression, this would be the result that we get. A line would intercept zero. The trick is to subtract 4 years from that time, which would make it work. We still have this expression still describes the line with the right slope, but if you plug in 4 years, this coefficient times 0 and thus the result is 0 for 4 years. In the resulting diagram, you see how the growth of the population softly bends down between 4 and 6 years,but we are not over fishing. The implementation makes use of a factor to modify the maximum harvest rate which is 0.8 times the maximum sustainable yield. If the time is larger than 6 years, we are using the factor of 1. If the time is not larger than 6 ix years but it's larger than 4 years, we're using the linear ramp, and otherwise, we are using the factor of 0. This type of diagram--called a slope field--helps to understand why everything is determined once we fix a certain initial amount of fish in this case. The general term would be initial value. These arrows point in the direction in which our solution curve has to run at every point in time and every amount fish we could draw such an arrow. This arrow specifies the rate of change. Here the rate of change is slightly positive. Here it's approximately zero. Here it's slightly negative and here it is really hugely negative. To see what this system is doing, we simply follow these arrows or take this one here as an initial value. We first have to go down and then move from between here. Once you have specified the initial condition to initial value in this case the initial amount of fish, you can trace out such a solution curve that describes the dependency of the amount of fish on time or whatever your differential equation is about. There is such a solution curve and there is no second solution curve. This would not work. This is what mathematicians mean when they speak of existence and uniqueness of the solution. Once you specified the initial condition, there is a solution curve and there is only one solution curve. Well, unless I have to say first thing that may happen is that the solution escapes infinity in finite time then we don't know how to continue afterward or the solution may fall into a hole of the definition set of our equations. For instance imagine that we are simulating a machine and we do not have any idea about what's happening to the machine in this region then we can't continue either. The following quiz shows that this escape to infinity in finite time can happen easily. This looks like a pretty harmless differential equation. We want the derivative of x with respect to time to be exrep as 1/2 and the initial value-- you know we have to specify the initial value to get one single solution and the initial value should be 3. Implement the forward Euler's method for that differential equation and then experiment what setting the ends time of that simulation, try to find out what the time of explosion of that solution is and use that in here with two decimal places as you submit the solution. The Euler forward method is pretty simple again and the time that you should find for the explosion should be 0.64. This is a diagram resulting from that setting and this really makes it obvious that the solution is escaping to infinity and a finite amount of time. Actually, this behavior is not that much of a surprise. X always has to grow because the right hand side is always positive. The square is always positive and if you're adding to it and dividing by a positive number, but as x grows, the square is going to become really huge, so the rate of change becomes really huge, which leads to x become even higher and so and so on. This is going to escape to infinity in a finite amount of time. Footnote for the mathematically proficient, what we're getting here is actually the shifted version of the tangent function. To finish this discussion on existence and uniqueness of solutions of differential equations, here is a final item from the mathematical cabinet of curiosities another unless, if you will, the derivative of x with respect to time should be the cube root of x, and if we're going to try out three different initial conditions--0, 0.01, and a really, really tiny number 10^-300. Implement the forward Euler's method for this differential equation and then have a look at what happens with these three different initial values. Even though the overall equation is simple, there's two things to be careful about. First, you cannot write 1/3 because that would be 0. Integer divided by integer would be an integer so you have to put in at least one decimal point here. The second thing to be careful about is Python's power operator. If you forget the parenthesis here, it would be forming the first power and would be dividing the result by 3, which is not what we want obviously, and here's the result. If you start with an initial value of 0, the results stays at 0 all of the time, but if we start with a positive number that's ever so slightly above 0, such as 10⁻³⁰⁰, we get a completely different evolution and as you can see, there's hardly any difference between starting with 10⁻³⁰⁰ and starting with 0.01. From a numerical view point, this differential equation looks as though 0 is a highly unstable fixed point. Once you get a tiny bit above 0, the solution is going to quickly divert from that and mathematically speaking, this is a differential equation whose solution curves are not unique. If you start with 0, you will branch off at any point in time in this fashion and get another solution. Now let's return to fishing but look at it from a different perspective-- the perspective of optimization. We could run our simulation with different but constant harvest rates and check what the total amount of harvest in 10 years would be. If we are using to a high a harvest rate, we are over fishing and hence lose from the total amount that would be possible. If we harvest too little, we are under fishing which is also not optimal. Optimization aims at finding the right value for the harvest rate-- the value that would maximize a total amount of harvest. In technical terms, the function that we are optimizing is called the objective function, and the harvest rate is a parameter it depends on. The optimization should find the best value of that harvest rate, meaning the value for which the objective function is maximum. The same would have to happen for instance if we look at the numbers of units sold in some simulation or the money earned in some simulation. Most probably, we want to maximize that value, but there's also a large number of optimization problems in which we want to minimize the objective function. For instance, we particularly want to minimize the duration of a trip or the money spent. Finding the best value for a single parameter is a pretty easy thing to be doing. Let's try something more difficult--optimizing the linear ramp, that is finding the best value for the point of time at we which we start ramping out the value. the start may be earlier, the end may be earlier, the end maybe later, the start maybe later, and so on. We want to find out which of this curves would produce the highest output. Not every combination of these two parameters make sense. We do not want this start time to be larger than the end time. That won't really work, so in this diagram the combinations below the diagram now are forbidden, and what our program will be doing, it will be varying both parameters between 0 and 10 years only in the allowed region and it will put dots in that region that show the total amount of harvest for all of these combinations of parameters. this blue dot would mean that this specific combination of parameters leads to a rather large total amount, and this smaller dot would mean that this combination of parameters leads to a smaller total amount and so on. The stepping of these two parameters that control the ramp process is already done. Your job is to compute the total amount of harvest and return it in this variable. To compute the total harvest is simply take care of by how much it changes in the current step then add this to the total amount. This is the result and you can see that there is not a single combination of start and end time, but rather a line of combinations, all of which work similarly well. So if you transform this 2-dimensional view into a 3-dimensional view, it would look like the reach of a mountain, not a single peak of a mountain. To apply stimulation in real life and to apply optimization in real life, we have to have an idea about the sources and the amounts of errors that we are introducing. The model as such may not be good enough. For instance, we may be missing some vital effects in that model. The values that we are using for the model's parameters may be wrong. We may be using the wrong growth rate or we may be using the growth rate that's too big or we may be using a maximum carrying capacity that's too large. The initial data may be off. Do we really know the amount of fish we're starting with? And we may have numerical issues, maybe due to round off errors or maybe, which is more probable, due to finite step sizes. We look at numerics in Units 2 and 3. I'll discuss the problems with this model at the end of these units. What I want to consider now is how to study errors concerning the parameters and initial data. The nice thing about these is that we can check them in the model. We can use a computer to get an idea about the effects at least of errors we are introducing here, and the computer can help us to find out which of these parameters and initial data is the most critical concerning its range of uncertainty. If we try to get more certain data, which of these would we try to improve? That's one of the major objectives of sensitivity analysis. The most straightforward way to do sensitivity analysis is to do it one at a time. One factor at a time, that is. For instance, there is an uncertainty concerning the initial data. Actually, we may start here or we may start there and our knowledge is to where precisely we start may be limited. There is an uncertainty concerning the initial data, or we may not be able to precisely control the start of the ramp. If we start a little early, the total number of fish will be lower, as if we start a little late. As we change the single factors, sometimes the result goes up. As we change a single factor for instance, as in this case with the initial value, and sometimes as we look at the upper limit of one of the factors, the result may go down and it may go up as we look at the lower limit. Like in this case for the time at which the ramp starts. We can apply this to all factors that influence the result and check the degree by which they influence the result. I am just using the base value and then for every factor changing that factor to its upper limit and then to its lower limit. That's a little lazy. At times, this can be too lazy. Most often, the other possible values for that factor will lead to sensible curves in that range, but it may happen that sometimes you get very weird results for values in between. Nonetheless, in the upcoming program, we're looking at the outermost values. This code makes heavy use of dictionaries, a special feature of the Python language. A dictionary associates keys and values, so I can ask for the string initial_value and get 2 * 10^e5. I can ask for the string maximumgrowthrate and retrieve the value 0.5. This is a nice trick to have all factors in one place, because we need to change them one at a time. These are the base values and here we specify the uncertainties. So what we're saying here is that we know that the initial value of the amount of fish is between 2 * 10⁵ - 5 * 10⁴ tons and 2 * 10⁵ + 5 * 10⁴ tons and so on and so on. This dictionary is simply to make things colorful. The different factors will get different colors as well in the diagram. If you look at the code that we provide, you'll see more details about dictionaries in there. In other programming languages, you may find dictionaries too, or you may have to look for associative arrays or for maps or for hash tables. From the information provided in the code, you should be able to return the key of the most critical parameter in this variable. The diagram that we get works as follows. Let's for instance look at the two red curves. They show the effect the initial value has on the result. It changes the initial value up and down by 5 * 10⁴ tons and you get this curve and that curve. You can immediately spot that this blue curve belongs to a factor that's more critical. The blue one is the carrying capacity, and obviously, the most critical is M for magenta, the maximum growth rate. If we want to improve the reliability of our result, we definitely need to work on the uncertainty of all maximum growth rate as opposed to what we just did. This code doesn't look at the complete temporal evolution, it just looks at one single value, the total amount of harvest for that particular set of parameters. It compares what happens when that factor goes up to what happens when that factor goes down and then determines the factor for which this difference is largest. When you show here as we increase the value of that factor, the result may go down, and as we decrease the value of that factor, the result may go up, so that we end up with a negative sign of the difference here. This is why we're using the absolute value and corresponding to what we guessed from the curves, the most critical parameter is the maximum growth rate. Let's take a second look at the sources of errors. Mathematics can help us find and solve numerical issues. We can use the computer to a evaluate our model concerning uncertainty and sensitivity but what about the model as such? The model that we are using is really myopic. It completely ignores external influences. It doesn't cover spatial variation. What's going to happen if I over fish these fishes but I'll leave these fishes alone for instance? It doesn't include by-catch or if I try to harvest one species of fish, I will almost invariably also find other fishes in the net including some of that effect is one of homeworks, but the most drastic omission is that we don't think about food webs. Consider for instance this situation, the purple species of fish feeds on plankton but at the same time, these purple species is prey to the blue species. If you harvest the blue species of fish, the purple species of fish will grow in volume and may deplete the plankton and of course, there is a lot more of issues with this model. Let's improve our myopic model a little. What could these two differential equations stand for that fish of type 1 die, there are young fish and mature fish, and that fish of type 2 are harvested. For each of these options, pick yes, maybe or no. Let's consider the first one, two fish of type 1 die. This expression pretty much looks like it, but then again it appears here with a plus sign for f₂, so this rather models that fish of type 1 become fish of type 2 after a certain time. Remember the SIR model for contagious diseases where a person is moved from one compartment to another compartment. No, fish of type 1 don't die. It's rather that we have two types of fish. The first type becomes the second type after a while--young fish and mature fish. This growth rate can model both births and deaths. It takes care of the difference, so in a way we have taken care of deaths and births from the beginning. What about our fish of type 2--harvested or not? We've already explained the first term. What about the second one? At least, this is not constant harvesting. For constant harvesting, we should be subtracting constant rate so and so many tons per year. This could be, however, harvesting that's proportional to the actual amount of fish, maybe just every second fishes being harvested, All in all, however, the second term looks like that fish of type 2 died of old age. In this unit, we looked at growth and harvesting to study fundamental concepts in differential equations and simulations. In particular, how to find optimal values for parameters and how to find which data are most critical for our simulation. The most fundamental concept of all, however, may be that of an equilibrium. This is going to play a vital role in the next unit, may be design an anti-lock breaking system. Hi. My name's Andy. And I work at Udacity and your instructor, Yoren. Well, he lives in Germany. And so he couldn't be here today, but we're really lucky to have Leonard Suskind here with us today. And Leonard Suskind is a theoretical physicist at Stanford. Can you just give a little bit of, of information about your background. Well, [LAUGH] I'm a human being. I come from earth. I I came from New York City. I am a Theoretical Physicist by profession. I think about things like black holes and quantum field theory and elementary particles. In cosmology and I spend most of my day doing that. A differential equation? Every single field of physics used differential equations. There is nothing that doesn't differential equations. so, yes I think a lot about differential equations. I, I, I don't solve them very often. They are kind of hard to solve. And the, I don't do a lot of numerics with it, but the mathematical structure of differential equations is terribly important to every branch of physics, every branch. what do you mean when you say mathematical structure? Well, there are different kind of deferiential equations. They behave differently. There are wavy equations that describe waves moving along. There are diffusion equations. A diffusion equation describes how a, a drop of ink that falls into some water. Okay, how does the drop of ink diffuse throughout the water? It doesn't make waves. It just, grows and becomes a blob. Those are two [NOISE] very, very different kinds of differential equations. There are differential equations. That describe electrostatic fields for example. Then electrostatic fields don't change with time, so those are differential equations that don't have to do with time, they just tell you how things vary in space. All these various differential equations, these different kinds of differential equations have different kinds of mathematical structure. If an experienced person looks at an equation, a differential equation. In a partial differential equation, usually you can tell very quickly is that describing waves or does, does it describe diffusion, or is it describing the electrostatic behavior of something? And you get a pretty good feel by looking at the equation by its mathematical structure. What kind of phenomena is describing. So, yeah, different equations have different kind of structure and the structure is mathematical. Alright so this first of, Bjorn's questions, I'll just read it word for word because this is coming straight from the instructor of the course. so at first sight, current physics seems to be concerned with sophisticated mathematics. Like Riemannian manifolds, self-adjoint operators, path integrals over anti-commuting spaces [LAUGH]. But, wouldn't it be fair to say that most of this higher mathematics in the end boils down to differential equations. Even if we're dealing with elementary particles or with the entire universe. It almost always has to do with differential equations. it, let me take back one phrase, Almost. It always has to do with differential equations, yes. It may be that other things, other mathematical things are more important than the differential equations in this particular area or that particular area. But it always has to do with differential equations, yes. Why is that? Why is it that differential equations become the Because what we're interested in is how things vary. You know where physics is about It's prediction. Basically, to predict the future from the present or the future from the past and that means understanding how things change. And you not only want to understand how they change with time, but you want to understand how they change with space. You, like for example, you want to predict the weather. Alright we theoretical physicists usually don't predict the weather. But they do predict similar kinds of things. How do you predict the weather? Well the first thing you have to know is called an initial condition. You have to know what the weather is exactly every wheres on the earth. What does that mean? That means you need to know the pressure, you need to know the temperature. Whatever it is, you need to know the wind velocity, and these are all a bunch of things which vary from place to place. So, the first thing you have to input is the conditions at an instant of time. Now, you want to know how they change. How do they change? They change by little incremental changes from instant to instant. And the incremental changes at one position depend on what's going on in the region nearby that one position. Whenever you have a situation where the change in something depends on what's going on nearby, that's a partial differential equation. That's a partial. So whenever you're in the business of predicting how the world changes. Or how some part of the world changes both with respect to position and with respect to time, you're in the business of partial differential equations. Alright. So i like this one, if you had to pick one differential equation as the one that advance the study of physics the most, which one would you pick? Hamilton's equation for Classical Mechanics, Maxwell's for Electrodynamics, Schrodinger equation for Quantum Mechanics, Einstein Field Equations, or something else? That's like asking me to choose between my children. I mean, they're not my children but that is like, they're all remarkable structures. Which one has changed the, the physics the most? Go back [LAUGH] give them to me again. Hamilton. Okay, so Hamilton's equations are the basic equations of classical mechanics. They tell you how classical mechanical systems. They have a predictive equations for how particles move, or how anything changes with time. Obviously they have had a enormous impact on Physics. And the impact covers every area of physics, every area. They're there in quantum mechanics, they're there in classical mechanics, they're there in quantum field theory, they're every where. Okay, what was the next one? Next one is Maxwell's equations Alright, Maxwell's equations are a little more special. They're a little more special, they have to do with a specific class of phenomenon. Electricity and magnetism, electromagna-, electromagnetic fields. So they're a little bit narrower, but on the other hand, they did something remarkable. They taught us what light is. Up until Maxwell's equations, nobody knew what light was. So how do you compare something like Maxwell's equations with. that other fellow, Hamilton's equations. Which one is more important? It's a matter of taste. Do you think explaining light was more important or less important than having a framework for, in a sense, all of physics? It's a matter of taste. I like those t shirts that have the Maxwell's equation. Have the Maxwell's equation. Let there be light. What was the third one? next was Schrodinger's equation. Okay so Schrodinger's equation there, Schrodinger's equation has different meanings to sometimes the physicists It can mean a very general kind of equation which tells you how the quantum state of a system changes with time. Or it can mean the very specific equation that Schrodinger wrote down. The very specific equation that Schrödinger wrote down had to with again a special class of phenomena, motion of a particle, quantum mechanical motion of a particle. But there's a, there's a generalization of it. There's a generalization of it that has to do with how any quantum system behaves. In the sense it's as broad and as important as Maxwell's sorry, Hamilton's equations. It's, it is the quantum mechanics. What Hamilton's equations were to classical mechanics. So extremely important. But I think in the end you're going to have a hard time getting me to say I favor one over the other. What about what about the field equations? The Einstein field equations. Yes, They of course are the sort of quintessential deep, deep, equations about what space and time are. They're like Maxwell's equations in that they govern a special class of phenomena, gravity, in this case. But, they're more than that. They're telling us something extremely deep about the nature of space and time, that it's curved, that it it that the geometry of space and time is not what Euclid thought it was. And how that geometry responds to matter. So again its you, you certainly picked what I would say are the four most important deepest favorite equations of a theoretical physicist for sure I, I am going to refuse to pick between them. There are four of them. I have four children, and never would I pick between my four children. [LAUGH] When we get a theory that describes the encompasses all of this, is it going to be described with a differential equation? Oh boy. Is it going to, is the final theory going to be described by a differential equation? Okay. So there're a lot of people And I'm not going to place right now. There are a lot of people who think the world is really discreet. Space and time might be discreet. Comes in little elementary steps. If that's the case, then at that level, at that, the smallest possible distances in the world. It might mean that the mathematics of it is discrete mathematics, the mathematics of little steps, indivisual, indivisible steps, and not smooth continuous evolution. If that's the case, no, it won't be described by differential equations, but we don't know. If, if I knew the answer to that, I would publish it. As in this classic model, the model phenomena, discretely. Yes. You have even with the underlaying assumptions that it's continued. Exactly For the computer programs I'll have to [INAUDIBLE] Exactly, so see that, that in many cases it really is not possible to tell Whether a certain equation is describing something which is truly continuous or whether it's describing some discrete phenomena where the little steps occur fast enough and one after another, too fast to be able to see. Okay, so we don't know what the final theory of nature will be So, this is build machines with the size and the power consumption of an entire town, to study tiny subnuclear effects. They built telescopes distributed over several continents orbiting the earth and space, to look back or out into the universe. So isn't our knowledge physics accurate enough already? And, how many significant digits do we really need? The purpose of building big accelerators, is not to improve the number of significant figures. it is to discover new phenomena. It is to discover, in discovering, for example. That a nucleus is made of protons and neutrons. That was a discovery at one time, all right? One wasn't, one wasn't improving the accuracy of the old theory. One was discovering something genuinely new. New phenomena that there were things called protons and neutrons. When the atom was discovered to be electrons going around in orbit in quantum mechanical orbits, that was not improving accuracy. It was discovering a whole new way to fear/g, to think about the world. The big machines that are being built, to understand, let us say, elementary particles, are not being built to imp, for the purpose of improving the accuracy. Lets say for chemistry or for whatever they're being built to try to understand what, what the real phenomena at the at the base of our basic understanding of the world. and of what makes up matter, what makes up things, and it, it's not that it's going to improve the accuracy. It's going to change the whole picture of the way we think things work. The same is true with the big telescopes. Now, sometimes it's important to improve accuracy. We have various ideas, for example, about something called dark energy. Dark energy is the energy that's out there in space that is causing the universe to accelerate and expand faster and faster. the various ideas may be very different than each other, but they may give rise to answers which are pretty similar. And in that case, we build the machines to be more accurate to describe, to try to discriminate between different possibilities. But the different possibilities really describe very, very different kinds of understandings of the world. So, sometimes yes, we just really do want to improve the number of significant figures. Also, sometimes in proving the number of significant figures, figures, you discover discrepancies. An example might be, studying the properties, the detailed properties of electrons. The properties of electrons, many of them are know to God knows how many significant figures. eighth while they say, that's a lot of significant figures. Supposing we make a more accurate measurement and we discover that in the ninth significant figure we don't agree experiment with theory. That could really tell us something, very deep about what the next step will be in understanding elementary particles. So yeah, we do want to improve accuracy. But mostly, we want to discover new phenomena, new, not just new phenomena but new understandings of what the old phenomena are. So, more than half a century ago, Eugene Wigner wrote about, The Unreasonable Effectiveness of Mathematics in the Natural Sciences. Yeah. And, and, indeed one man can sit in one's living room and do Duncan experiments and, maybe you know what that word means? I don't. Duncan means thought. Okay, thought experiments. Thought experiments. You can do these thought experiments and come up with something like the gene-, General Theory of Relativity. You discover black holes and the curvature of space-time. And why is that so? Could it be that we only perceive what we can describe, without mathematics. And vice versa? Because mathematics is co-evolved with our brain. Look, Eugene Wigman made a big problem for every generation that came after him. To answer that question, why mathematics works. Why does mathematics? And sometimes I think to myself, what, how could it not work? The world has some coherence to it. Things don't just randomly happen. How do you describe things that don't randomly happen? If they don't randomly happen, you have to have some kind of quantitative framework for explaining what happens. So I can't imagine, on the one hand I can't imagine a world that didn't work according to some kind of laws and those laws being written mathematically. On the other hand, I also can't quite understand why mathematics works. So Eugene asked a very difficult question. And he didn't know the answer. And I think I, I, I don't know the answer, either. But here's what I would say. Here's what I would say. Not, why does mathematics work, but why do we need mathematics to explain physics, why is it so hard to explain physics in the English language? And the reason is, because, yeah, we may have evolved as a species on mathematical abilities, but we evolved them, in the context, of rather ordinary things. The ordinary things would be, how stones move when you throw them. When it comes to waves, we have some perspective about what a wave is because we see water making waves and so forth. We have a good concept of what a force is because if I give you a good shove, you'll say, oh, don't force me. We know what force means. But every time we enter into a new range of parameters We make things smaller than anything that we were, than we grew up with. Other words, we go to the quantum world. Or we go to the world of very, very heavy things and dense things. In other worlds, we go to the world of black holes. We suddenly find that the intuitions and the concepts that we evolved with are not sufficient to understand. An example, we use higher dimensional spaces all the time. We think we sometimes think for example, that the that space itself may be more than three dimensions. You evolve. You and I evolved in a world of three dimensions. We can't visualize more dimensions. And we can't because our it, it's not that we're not smart enough. It's that the neural architecture, the architecture of the brain itself. Was purposefully built, well it wasn't purposely built, but it evolved in the world of three dimensions. The architecture is appropriate for three dimensions and it's not appropriate for four, five, six, seven dimensions. If we think that we're interested in a space with seven dimensions or whatever, how would we describe it if we can't visualize it. We describe it by pure mathematics. We say a three dimensional world is described by a bunch of points that are labeled by X, Y, and Z that's an abstraction. I can visualize three-dimensions on my head, but I use x, y, an z to describe them mathematically. Now I go to four dimensions, or five dimensions. I can't visualize it anymore. I can't visualize it anymore. I can't close my eyes an see a, a five-dimensional world. But I can add two more letters to the alphabet. I can add the x, y, and z, w, and v. And now I have a five dimensional space. How do I work with it? I work with it using algebra. I work with it using using abstract mathematics. And I don't try to see it in my head. In trying to explain it to people who don't have the mathematical background, we get stuck. I can't say, now close your eyes and view in your head five dimensions, because I can't do it either. What I, all I can do is say, take x, y, and z and add v and w. Eventually if, eventually a person with this kind of mathematical background begins to develop mathematical intuitions for things. They don't need the same kind of visualization. So, we are stuck. We're stuck needing mathematics because evolution didn't equip us to be able to visualize quantum mechanics. It didn't visualize us to be able to, there sorry, it didn't equip us to be able to visualize 11 dimensional space-time. And tha, that's the reason. Evolution did it to us. So, thanks to Leonard for joining us. This was really great to hear his insights. Welcome back and fasten your seat belts, because in this unit we're going to brake as hard as possible. You design an anti-lock braking system that controls the brakes to apply the maximum available amount of friction, but rest assured it won't leave skid marks on your computer. Breaking means to apply friction, lots of it, and for breaking it's vital to distinguish between two different regimes of friction--static friction and kinetic friction. If this object is at rest with respect to that surface, we can apply forces up to a certain threshold force without setting this object in motion, which remains static. Experiments showed that this threshold of force is a constant times the rate force, which often is called N because it's a normal force. It is perpendicular to the surface. Its coefficient is called the coefficient of static friction µs. As long as the force that we apply doesn't exceed this force, this object won't be set in motion. When this force becomes larger than the threshold, the object will be set in motion. For example, if the mass of this object is 1 kg, the gravitational force would be almost 10 N. A pretty high value for this coefficient of static friction would be 1, which means that the threshold force is at 1 10 N. It would have to exert a force of 10 N or more to set this object in motion. If we apply less, it remains static. Typically, this coefficient of static friction ranges between 1/3 and 1, which means that typically to set an object in motion, we need to apply less force sideways than the force with which gravity pulls that object down. Things are easier in the kinetic regime. We need to apply a certain force to keep this object going at a constant velocity. So this force is nothing else but the force exerted by friction, and from experiments, we find that this force is some constant times the normal force, the force exerted by gravity in this case, and this is the coefficient for kinetic friction. In the kinetic regime, we got the same expression but it's an equal sign. In this case it's not a threshold--we compute the force as such. What's of imminent importance for breaking is that the coefficient of static friction tends to be larger than the coefficient of kinetic friction. The coefficient of kinetic friction maybe be 1/4 less than the coefficient of static friction or it may even be smaller, and now think about breaking. We want to apply the maximum amount of friction. The maximum amount of friction is found in this static regime, because the coefficient of static friction is higher than the coefficient of kinetic friction. What we need to prevent is the locking of wheels. When wheel stays locked, it's sliding on the ground, and we're in the regime of kinetic friction. Here is the situation for you to think about. Your notebook computer rests on a lectern, the surface of which is inclined by an angle of 45° to the horizontal. Will the notebook computer stay there or will it slide down? Assume that the coefficient of static friction equals 0.8. Gravity exerts a certain force on that object, and this force results in forces of equal magnitude perpendicular to the surface and parallel to the surface. This happens because this angle equals 45°. If the coefficient of static friction was 1.0, we would be up a threshold of motion. The magnitude of the force that pushes the object along the surface is the same as the magnitude with which the object is pushed toward the surface. If the coefficient of static friction is 0.8, the threshold force would have a magnitude of 0.8 times this force, which means it's smaller than the actual force. This object is going to move. It's going to slide down. Now let's look at how the distinction between static friction and kinetic friction applies to the rolling wheel. Assume that the wheel first occupies this position and then rolls on towards this position a fraction of a second later. This would be the distance covered by the car. Let's mark the same distance on the perimeter of the wheel. If the wheel was rolling without slipping, it would have rotated by this angle, but we want to study breaking so there will be a certain amount of slippage between the road and the wheel. The road will not turn by this complete angle. It will turn with less. There would be a certain amount of distance left marked in blue. This leads to the definition of wheel slip. Wheel slip is the ratio of that missing distance to the distance traveled by the car. If we have perfect rolling, there won't be any difference. The wheel slip becomes zero. It won't move at all. The blue distance equals the red distance and the wheel slip becomes one. Now one can study how friction depends on wheel slip. If a wheel slip equals one, the wheels are locked. They don't move with respect to the car. They slide on the streets, which means that we are in the regime of kinetic friction. The coefficient of friction will be less than optimum. If the wheel slip equals zero, the wheel is rolling perfectly, and the car only experiences the rolling friction, which is so small that we neglect it here and simply set the friction to zero. When there's just a slight amount of slippage between the wheel and the surface, the wheel develops the maximum friction. What's going to follow? we're working with one single curve to describe how the coefficient of friction depends on the wheel slip. Actually, that's far from true. With the higher speed of the car, friction will be reduced, and of course if there's water or ice on the street, we see a very strong reduction of friction. To keep things manageable, however, we're going to stick to one single curve. How much difference does the coefficient of friction make? Let's compute what's going to happen if we brake with a very high friction 1.0, which is really optimistic, and with a slightly less friction. Our car should be traveling at 120 km/h, which is approximately 75 miles/h. How long will it take to come to a complete stop? Enter the numbers of seconds with 1 decimal place. Here's a hint--think about this diagram, how does the velocity of the car depend on time and think about Newton law from Unit 1. If we apply this higher amount of friction, it's going to take 3.4 seconds to bring the car to a complete stop and for the smaller amount of friction, it's 4.9 seconds, and now let's look at how to compute this. We have to take two different forces into account. The force due to friction, biking that is, and the force due to gravity. Newton says F=m<i>a.</i> The force due to friction equals the mass of the car times the deceleration as we are breaking off the car. But the force exerted due to friction is amountable of the force exerted by gravity with the proportionality constant µ and what's force the exerted by gravity? It's the mass of the car times the gravitational acceleration of the earth. You see that's Newton law applied in the reverse direction. If you look closely, you'll see that mass cancels from both sides of the equation and the deceleration equals the constant of friction times the gravitation acceleration. This is one funny consequence for astronauts. When you try to break a car on the moon, the gravitational acceleration is far less than on earth and so the deceleration is far less than on earth. Breaking is far more ineffective on the moon than on earth. Now we can use this result to look at how speed depends on time. The deceleration is constant so the velocity decreases by a certain fixed amount of meters per second. The relationship between velocity and time is linear and the slope of this line is given by the acceleration. This side has a length of 1, this side has a length of a, the acceleration, which is, as we now know, the constant of friction times the gravitational acceleration. You'll see two similar triangles in here. We have the time it takes to come to a complete stop divided by the initial velocity equals 1 divided by the deceleration. The time it takes to come to a complete stop equals 120 km/h divided by our deceleration which is the constant of friction times the gravitational acceleration. We have to go from kilometers per hour to meters per second. The gravitation acceleration equals 9.81 m/s² . To make this work, we should be using m/s for the velocity as well. One kilometer is 1000 meters and 1 hour amounts to 3,600 seconds. Now you can do the math. This function that describes how the coefficient of friction depends on the wheel slip is a vital part of the simulation. The tricky thing about is that it stems from experiments. It contains empirical data. We do not have any nice equation for that curve--that is not until now. One way to deliver such empirical data in this simulation would be to use a lookup table such as this one. If we input a wheel slip of zero, we output a coefficient of friction that's zero as well. If we input a wheel slip of 0.1, we output a coefficient of friction that's 0.9 and so on. This would constitute a lookup table. The tricky thing is what would you do for a wheel slip of say 0.01234. What do you do about values of s that are not contained in this column? We have to somehow interpolate these values, which gets complex and it's not easy to do in a smooth fashion. A simple approach is to use a phenomenological equation to guess how one could possibly describe this in mathematics without worrying about the reasons for why it looks the way it looks. We just collect some mathematical functions that produce the right shape. If you look at the shape of this function, you may come up with the following idea-- for reasonably large values of s, this function almost is a line so why not use that line as one component in our equation. Close to zero it looks as though we are subtracting a quickly decaying function such as e^(-20s). Your job is now to make this work. Pick the right value for the constant a and then implement this function in Python. To get the equation for the line, you can look at the slope and the intercept-- the intercept being 1.1 and the slope being -0.4. An important point to take care of when we pick the constant in front of that quickly decaying expression is that the coefficient of friction should be 0 when the wheel slip is 0. So if you plug in 0, you'll find -A * 1 + 1.1, meaning that A has to be 1.1. And implementing this in Python is of course straightforward. Now we are in the position to write down differential equations. We look at a single corner of that braking car. Just a single wheel not all four wheels and just a quarter of the mass of that car. Let's called the velocity of the car V and the velocity of the rim of the wheel W. We specify the velocity of the wheel in m/s similar to how we specify the velocity of the car, and which will show you or rather find a discussion that uses the angular velocity of the wheel and that uses the moment of inertia of the wheel, which gets overly complex for all purposes. This quarter of the car exerts a certain force on the ground, namely-- the mass of that quarter of the car times gravitational acceleration. By friction, the road surface exerts this force on the wheel. If you wonder about the direction, imagine that the wheel was locked and friction will put it around in a clockwise fashion. The first thing to be computing is the wheel slip s and given these two velocities, the velocity of the car and the velocity of the rim of the wheel, we can compute the wheel slip by 1-W/V. Let's do this. I need to check. If the wheel is lock, the velocity of its rim will be zero. W will become zero. What do we get? 1-0. A value of one fitting to the wheel being locked. If we have a perfectly rolling motion, W will be equal to V and we compute 1-1/1 which is zero. This seems to be very plausible. Once we've got the wheel slip, we can compute the force from friction and it's the coefficient of friction which again depends on the wheel slip times the force perpendicular to that surface, 1/4 the mass of the car times gravitational acceleration and now it's time to look at the velocities. The car is decelerated. The rate of change of the velocity of the car minus the friction force divided by the mass of that 1/4 of the car. Think about Newton's law--F=m*a. The rate of change of the velocity is the acceleration. This our force, this is our mass, and we have to take care of that we are braking decelerating. We get a similar equation for the velocity of the rim of the wheel. Its rate of change increases through a friction, so we've got a plus sign now. The mass to be used here is not the mass of this 1/4 of the car. It's rather something like the effective mass of the rear. In reality, we would have to take care of the circumstance that the mass of the wheel is distributed over the cylinder. To make things simple, I'm using an effective mass here. That's my own term. As I said before, the typical way of dealing with this would be using angular velocities with moments of inertia, which considerably complex. Let's cheat a little and introduce this effective mass of the wheel here. If the mass of the wheel was concentrated at its circumference, then this would actually be the mass of the wheel. This is not all. We have to model the action of the brake which acts on the rear. The break decelerates the wheel. Let me put it like that minus capital B and this quantity B represents the strength of that braking process. If you put it like this, it acts like a deceleration of the wheel so it's something measured in m/s². If it wasn't for the friction force, the wheel would be decelerated by this amount. Now we have reached four equations that describes the process of braking. The effective mass of the rear is obviously way smaller than the mass of the quarter of that car. Due to its low mass, it is much easier to accelerate than decelerate the wheel than to accelerate or decelerate the car which has a larger mass. So when we want to study equilibria of the wheel slip that its values which stays constant, we can start by thinking about the velocity of the car is a slow variable and the velocity of the rim of the wheel is a fast variable. We have cheated a little and say, okay, let's assume that the velocity of the car is almost constant but assuming that the velocity of the car is almost constant, this idea about the fast changing W and the slowly changing V helps us to think about the equilibria of the wheel slip. If that wheel slip reaches an equilibrium, its temporal, its rate of change becomes zero. It does not change anymore. This is the meaning of equilibrium. If s doesn't change and V is almost constant, we learned that W must not change, but we have an equation for the rate of change of W. It's the friction force divided by the effective mass of W minus the braking deceleration of the wheel. These two expressions have to be equal. We find approximately this equation for the equilibria of the wheel slip. When the wheel slip does not change with time, the friction force divided by the effective mass of the wheel has to be almost equal to the deceleration of the wheel by the brake. Given this equation, the task is now to determine the number of equilibria, the number of stable equilibria, and the number of unstable equilibra. For that, assume that the car is braking slightly below the optimum value and ignore the case that s, the wheels that equals one, which means perfect locking. In turns out that there is one stable equilibrium and one unstable equilibrium and now, we will discuss the details. Let's take this equation and put it in this form so it's easier to see what has to be balanced. The force divided by the mass has to balance the deceleration by the brake. The force can be expressed with the help of the friction coefficient, the mass of 1/4 of the car, and the gravitational acceleration. So this equation boils down to the following--that the coefficient of friction has to equal the deceleration of the wheel over the gravitational acceleration times the ratio of the masses and now we can check with our diagram what this means. If we brake with an intensity slightly below the optimum, the coefficient of friction has to be on such a line slightly below its maximum value. This has to be the value of our coefficient of friction and hence, of the right hand side. We have two options to satisfy this equality and hence, two possible equilibria. This value of s and this value of s. The lower one is stable. The upper one is unstable. I just illustrate one part of the argument for that. Imagine s was a little lower than the value of the equilibrium, then we would have less friction because the coefficient of friction decreases, but if we have less friction, the brake acts stronger on the wheel, which means it's going to increase wheel slip and moving in that locking direction, which means upward. From the left side, this equilibrium is stable. When we are slightly below, this equilibrium point if you're moving up and you can argue in a similar fashion that we are slightly below that point if we are moving down. For this point, it's the other way around. If we are slightly below, we keep on moving down. If we're slightly above, we keep on moving up. Once we are past that point, we inevitably run into locking a wheel slip of one and strictly speaking, there is a 3rd equilibrium which is stable again. When the wheel slip is one, that means when the wheel is locked the deceleration by the brake always supersedes the force applied through friction. We are going to stay in that locked state. There is a practical consequence of this--if you push the break too hard, you're inevitably going to lock the wheels, which is far less than optimal. If we really stump on the breaks, there is no equilibrium below s=1 at all. No matter what, we're going to end up with locked wheels. Now it's time to see the single corner model of a car in action. The program test different strengths of braking so that we can learn about the equilibria that we've analyzed before. The mass of 1/4 of the car and the effective mass of the wheel are provided 20 kg for that effective mass of the wheel is a little on the high side, but helps to make the simulation more stable. Your first task is to fill in the initial values. You want to start with the car speed of 120 km/h and no slip, perfect rolling. The other task for you is to implement the forward Euler method. We make sure that the wheel slip never decreases below zero. This helps to make things more stable. Your task is to compute x which is the position, V which is the velocity of the car, and W which is the velocity of the rim of the wheel and then we're cleaning up a little to make sure that the velocity of the rim of the wheel is never negative. Again, we measure to make things more stable. The initial value for the velocity of the car is of course 120 km/h converted to m/s and the initial value for the velocity of the rim of the wheel has to equal the velocity of the car so that there is no slip. The implementation of the forward Euler method should be straightforward. The top most subplot illustrates the braking distance. The endpoint of each these curves is at the braking distance. The second subplot shows that the car velocity decreases almost linearly. This is not quite sure of the wheel velocity. In particular, you can see that the rear velocity quickly deceases to zero when the strength of braking this too high. The most interesting part, however, is the lower subplot. It shows the wheel slip. If we apply just a little of braking, we're going to run into the lower equilibrium, which is a stable one and which is always below the optimum value of the wheel slip of 0.2, and to see that for this curve colored in green and this curve colored in purple, we do not reach such a reasonable equilibrium, however, the wheel locks up. The wheel slip becomes one, which we have seen in the plot above as well. Among these five options, the red one seems to work best and you see that we must not apply too little braking and that we must not apply too much braking. You maybe wondering about this artifact here. Remember how we compute the wheel slips. One minus the velocity of the rim of the wheel divided by the velocity the car. If these two gets close to zero, the ratio is going to vary widely. This is a numerical artifact. So the reason for this artifact is built into our equations. At the end of the simulation, we're dividing numbers, which are close to zero. That's always a bad idea when it comes to maintaining stability. By now you may have gotten the big idea behind anti-lock braking systems. Looking at this curve, it's obviously best to have a moderate amount of wheel slip, say, 0.2. So the wheels must not be rolling perfectly. It's also not good if the wheels do not roll at all. If they are locked, the wheel slip of about 0.2 would make more sense. This is the objective. We have to have some type of technology that make sure the wheel slips stays around 0.2. So that we apply the maximum amount of friction. This ensures that the braking distance is as short as possible but it also ensures the maximum amount of steerability. The larger the friction is the better we can steer the car while it is braking. You may ask yourself, okay, all of this is good and well but why don't we simply build a brake that applies this amount of friction and then we're done with it. No electronics, no sensors, no actuators whatsoever. The problem is that this curve looks different under different circumstances. With a higher speed, it may look like that. When there's water on the road, it may look like that and so on. So we do not know in advance which amount of friction would be best. What we do know that the slippage of 0.2 almost always make sense. Put note, a wheel slip of 0.2 actually does not make sense on snow and on sand. If you're on snow and on sand, locking the wheels as quickly as possible is the best option. Before we get started with automatic control, let's try manual control. We want the driver to slam on the brake from 0 second to 0.7 seconds, then release the brake, then slam on the brake again from 1 second to 1.7 seconds, release the brake and so on. The code of the simulation runs twice--the first time with the strength of the brake close to optimum and the second time re-release slamming on the brake. Your job is to do something about that second time. In that case, this volume variable is pumping becomes true by the way these parenthesis are just for readability. Without them, the code look somewhat strange, and when we are pumping, simulate that this value of p is being modulated, which means that we have to complete this line here. What is the velocity of the rim of the wheel in the next step. Let's look at the results first. The blue curve is the one that we know, almost optimal braking. And you see that this type of pumping doesn't really cut it. It takes more than 5 seconds to bring the car to a complete stop. As the driver slams on the brake, the wheels locked every second, which means that the wheel slip is almost never close to the optimum. The implementation uses this on/off factor to where switch the brake on and off. To compute when to switch off the brake, it'll get the fractional part of the time-- the time minus its integer part. In the solution, we won't be overly picky about this precise instance of time to take the finite step size into account, which require some more thinking. Now it's time to have some automatic control of the brake. We want to set the wheel slip to 0.2, but actually the only thing that we can control is the brake. We can't directly control the wheel slip. So the job is to provide the right input to the brake so that the measured wheel slip is about 0.2. That's lot of things going on in between. Think about the dynamics of the car, the surface of the road, and so on. Situations such as these are studied in control theory. How to provide the right input to get the output that you want and this is the approach one would typically be using. We found the difference of the optimum value and the actual current value to get the current error. This is fed into a code controller, which as the name says controls the brake. So what we are doing right now is more than just a simulation. We're simulating the brake, the dynamics of the car, the sensors but in addition to that we're also be going to design a controller. That's a piece of software or maybe hardware that's going to actually be part of that real car. When we're going to implement the most simple controller I may think of? A p-type controller. P stands for proportional. It's simply going to multiply the error by some number to produce the input for the brake. To be more concrete, let's multiply the error with 100,000 m/s². When doing so, we can, of course, get huge positive and negative numbers that don't make any sense to control the brake so we have to clamp the output. Whenever the result of that multiplication is negative, we return 0 instead. Whenever the result of that multiplication is larger than 200 m/s², we return 200 m/s². Why could that possibly work? Assume that when we're fitting this controlled value of 130 m/s² into the brake, we're actually ending up with the wheel slip of 0.1987. And the difference between the optimum value and the actual value is 0.00130. The controller is going to return the same number again, 130 m/s². So this foot constitutes an equilibrium. One thing to know about this naive version of the p-type controller is that the value of the equilibrium is always slightly less than the optimum. If anything changes concerning the dynamics of the car or the vault conditions, this actual value may change. When the actual wheel slip goes up a little, the difference is going to decrease. We're subtracting the actual wheel slip. This is going to become 0.00129. The error gets a little smaller. We're multiplying this error by 100,000 m/s², which results in 129 m/s². We're braking less, at least we're braking less hopefully. The slip is going to decrease again. So this looks like a stable equilibrium at least from the one side, but obviously, this also works when the actual wheel slip is a little too low. Now implement this in Python. Your first job is to implement the p control as a function. The function receives the actual value and the target value, 0.2 that is, and returns the output of the p control, which eventually controls the brake. Your second job is to call that function to control the brake. To compute the function for the P-controller, we're computing the difference multiplied by our large number and then clamp the result. If this product is smaller than 0, negative that is, 0 will be the maximum. If this inner value is larger than 200, the minimum will be 200. So this min and max does the job. We could also use in construction business if or that would take several lines of code and then the forward Euler solver for the velocity of the rim. You simply insert the result of the control as the deceleration of the brake and of course, you're plugging in the optimum value of 0.2 for the target. If you look at the result, you see that the wheel slip indeed stays close to 0.2, a little less, but just before the car comes to a complete stop, strange things happen. We're multiplying tiny errors by huge numbers. That's pretty dangerous when it comes to instability. When you use this type of controller, you have to be really careful about stability, but actually in the next segment, we are going to show that we can use this type of controller anyway for completely different reasons. Things are actually even worst. We cannot set the wheel slip directly and we can't even set the brakes directly, at least not with the standard type of hydraulic brake that's part of common cars. So realistic approach to anti-lock braking is the following--the driver slams on the brake as strongly as possible and then the electronic checks whether the wheel slips becomes too large. If so, it starts to decrease the pressure of the brake fluid and so on and so on. The implementation of this will be part of the homework. Look at these few curves. Which of them tells you how strongly the brake x on the wheel? Which of them tells you what the wheel slip is and which of them tells you what the velocity of the rim of the wheel is? Obviously, the middle one showed something like hydraulic pressure. The driver pushed the brake better and then the electronics modulates that value at a constant slope. The lower one has to be the wheel slip. As you brake stronger, the wheel slip increases. As the system decreases the pressure, the wheel slip eventually starts to drop again. The upper one has to be the velocity of the rim of the wheel. When the wheel slip is slow, the velocity of the rim of the wheel is large, approximately the velocity of the car. When the wheel slip is high, the velocity of the rim of the wheel has to be low. Now that we've learned that it's harder as expected to control the brake, it's time to learn that it's hard as expected to measure the wheel slip. One simple approach to determine the velocity of the car is to use the W of all four wheels and to use the largest of them, so that when we have strong slippage for some wheels, we hope that at least one of those wheels has full contact to the road's surface. What do you think is the easiest way to improve that estimate of the car's velocity. Should we be using GPS, should we be using a camera and do motion analysis of the camera frames, should we be using a 5th wheel--a lightweight one with no brake, or should we be using a accelerometer of that kind you know from smart phones and game consoles. GPS provides us with very accurate velocities, which is nice, but on the other hand, we want to be able to use our system in the tunnel too. Using a camera with motion analysis is really high tech. It's hard to get results that are really reliable in 99% of the cases. For experiments, you may actually use a fifth wheel, but in real life this would of course be awkward and costly. I would say the accelerometer is the way to go. It's just a tiny chip to be added to the electronics. One issue about the accelerometer is that it does not output the velocity, it outputs acceleration. Hopefully, you have a good estimate of the velocity before we start braking and then we can use the acceleration to estimate the evolutional velocity. By now, we have a nice cartoonish model of a car, but there's lots of issue to consider to make this model really life-like. First thing to mention, the car can turn left and right. Our model should be able to do the same. This should be including a lateral motion. Each of the four wheels is not an exact cylinder. The cross section of a wheel is not a circle. The wheel is deformed as it touches the ground. The wheels are mounted with the help of some suspension, which may oscillating, and there's interaction between the wheels. What happens with one wheel may influence all three other wheels. In the next quiz, we're looking at one mechanism for such a sort of interaction between the wheels. Here are two ways of transporting a heavy object. You can put it on the backseat or you can transport it on the roof. Now, imagine that you brake in this situation or in that situation. In which situation does the front of the car lower more or is there no difference at all? Think about the center of mass and think about levers. In the second situation, the front of the car will lower more when we brake. To see that, let's look at the center of mass. In this situation, the center of the mass is put to the back of the car--may be it's here. In this situation, it's put up--may be it's here. Now let's look at the forces due to friction and the levers. We can see that in the second situation, the levers get longer and the angle between the forces and the levers is closer to perpendicular. So there are two reasons for this car is pushed stronger in the clockwise fashion, which means that its front lowers more. One cannot discuss anti-lock braking without asking whether or not such an improved safety feature actually saves human lives. Some researchers argue that humans tend to stick to a certain riskiness of the behavior. The envelope of what can be considered risky behavior is pushed up with introduction of anti-lock braking. And many drivers are going to stay with the original level of riskiness, which means that with the introduction of anti-lock braking systems, they are going to drive faster and brake harder relying on the improved brakes. In the end, it all boils down to saying that psychology matters. To improve safety, we cannot only look at technology or to cite an example from history-- when your ship is supposedly unsinkable, why should you care about icebergs? In this unit, we look at how to model a 1/4 of a car, seeing physics in action. I guess you can imagine how to model other types of machinery with the help of differential equations and the computer. And we took a glimpse at control theory trying to come up with feedback loops that ensure the system behaves in an optimum fashion. Hi! In the previous units, you've seen how to do simulations of systems that develop in time but where we've got some more dimensions--the x, y and z of space. This unit introduces differential equations that govern time and space. We'll use the computer to model combustion and the propagation of heat to predict the cause of a wildfire. When we want to model a wildfire, heat transport is obviously a vital phenomenon to consider. It's going to occupy us for the major part of this unit. To get started, let's look at this system. It's a tank that completely isolated from its environment. and that contains two different compartments one that is filled with water that's initially hot and another compartment that's filled with water that's initially cool. Now, I want you to describe the flow of heat from one compartment to the other and the temporal evolution of the temperature of the left part and the temporal evolution of the temperature of the right part. Temperature is just another expression for energy. Energy is being exchanged between those two compartments. To include that idea in our equations, we should be using the Kelvin scale for temperatures, not the Celsius scale and not the Fahrenheit scale. At least for an ideal gas, the temperature measured on the Kelvin scale is proportional to the energy content of that gas. To model the system, we could say that for instance that every hour 10% of the energy content of the left compartment flows to the right compartment and the same percentage per hour would flow from the right compartment to the left compartment. What's the right model for this idea? Should the rate of change of the left temperature be 0.1/h times the difference between the right and left temperature? Should the rate of change of the right temperature be 0.1/h times the same difference should the rate of change of the right temperature be 0.1/h times the left temperature? Should the rate of change of the left temperature be -0.1/h * left temperature or should the rate of change of the right temperature be 0.1/h times the difference of the left and the right temperature? Check all that apply. This differential equation would mean that T1 decays exponentially to zero. We're losing a certain fraction every hour. This does not make sense. With this differential equation, T2 would gain from T1. The red process would be in place, but we would never lose anything--this is not true. In the second differential equation, initially T2 is smaller than T1, meaning that this difference is negative. The rate of change of T2 would initially at least be negative which doesn't make sense T2 has to grow. The first one is correct. The percentage of 10%--0.1 per hour of T2 flowing into T1 and there is minus 10% per hour of T1 flowing out of T1 and in a similar fashion the lower one is true as 10% per hour of T1 flowing into T2 a plus sign here and there is minus 10% per hour of T2 flowing out of T2 minus. So actually what's of interest to us is only the difference of the temperatures. Footnote. So we could have been using degrees Celsius and degrees Fahrenheit all the way because we're forming this difference and are not interested in the total amount of energy contained in each compartment. And now we come to a model that has way more compartments than the one before. A model for heat conduction in a wire. Let's draw imaginary lines after each millimeter of that wire and treat each millimeter as one compartment. Every single compartment is treated in the same way, so we can just look at one single of them to get an idea of what happening, let's take no. 8. Compartment no. 8 loses a specific percentage of its energy per time to compartments no. 7 and no. 9. To be specific, let's say it's going to lose 1%/ms to the left and 1%/ms to the right, but there will also an energy flow from compartment no. 7 to no. 8 with the same percentage of the energy of no. 7 and there will be an energy flow from compartment no. 9 to compartment no. 8. Again, the same percentage but of the energy content of compartment no. 9. So after a short amount (h) of time, the temperature of the compartment no. 8 will be its initial temperature plus that amount of time times 1%/ms to be gaining 1%/ms from compartment no. 7 and we are gaining 1%/ms from compartment no. 9 but we are losing 1%/ms to the left and to the right so we're losing thrice that percentage. One final thing to be doing to clean things up a little, let's get rid of this 1%/ms here. If we work with seconds instead, 1 second amounts to 1000 ms, so we need 1000%/s. 1000% is 10--so now, we have an equation for the temperature of the compartment no. 8 after one time step. Of course, this works similarly for all other compartments. We've just change the numbers. Its about the left neighbor, the right neighbor, and ourselves. So now, we have an equation that can be implemented on the computer. Let's start by simulating the following situation. A wire is briefly heated by a candle so that some part of the wire is at the temperature of the flame and the rest of the wire is at room temperature is times zero We blow out the candle so that there's no more heating and then we watch heat conduction in action. How does this energy spread to the left and to the right. The code comes with some pretty fine constants for your convenience. It provides to erase one with the old temperatures at the beginning of the step and one for the new temperatures at the end of the step. And you have to compute the new temperature from the old temperatures for the compartment number i A note about the implementation in case you're wondering about this line here, we're exchanging the roles of these two arrays temperatures new, temperatures old after every step. Once the new temperatures have been computed, they become the old temperatures for the next step, which means that one of these arrays can be reused for the new, new temperatures if you will. This saves us from using tons of arrays. We're just using these two arrays with different roles. The implementation is almost of a verbatim copy of the equation. We just introduced one little changed because the temperature of the old compartment is used twice. So we introduced an auxillary variable to store it. The diagram starts with the initial distribution of the temperature. You see the action of the flame and then the heat starts to spread out further and further. And, of course, as the total amount of energy has to stay constant, we're not heating anymore--the peak has to go down. We didn't treat the endpoints in our simulation. So after this final step, we should be careful about what's going to happen. This strange expression appeared in our equation for heat conduction along the wire. Now, we look into its mathematical meaning. The scary name for this topic is central difference formulas for derivatives. This is about determining the derivative, that means the slope of the tangent of functions that are based on empirical data, measurements, and now your approach to determine the slope of this function at this point would be to look a little further to the right, determine another point of that function there, and then connect this point to our original point and you can see that this doesn't really work well but we want to determine the derivative of our function namely the slope of the tangent. The simple trick is to go a little to the left and a little to the right. Determine the points of the function, left and right and then connect these. To be clear, this works far better. Now we can write down some equations. That's Δx stand for the total distance from left to right. Δ always is denoting the difference. The estimate for the slope would be what we gain in height divided by how far we went to the right. The denominator is simple, Δx is how far we went to the right. The numerator is a little more complex. It's the difference of this value of the function minus that value of the function. That's what we are gaining in height, so it's our function on the right-hand side, f(x₀ + Δx/2) from x₀, we are moving by half of this distance to the right plus Δx/2 minus the value of the function on the left-hand side f(x₀ -Δx/2) that go into the left by half of the distance minus. Now you can get an idea about why this is called central difference. Obviously, it's a difference and the difference is centered at x₀. we're moving as much to the right as we're moving to the left. Now, we're going to apply the same formula to the second derivative. The rate of change of the rate of change, and think acceleration. We simply use this formula not for the original function, but for its derivative. You replace f by f' wherever you find an f. So this f becomes an f'. This f becomes an f' and this f becomes an f'. Prime, the second derivative. But now we can use our original formula for these derivatives. This is going to get a little long. When estimating this first derivative, we get an expression with denominator Δx and in the numerator, we got the following. We are moving to the right by Δx/2, so we're ending with f(x₀+Δx). We are moving one further Δx/2 to the right minus we are moving Δx/2 to the left which means this is f(x₀) and now for this derivative. We are moving by Δx/2 to the right minus Δx/2 + Δx/2, canceled. So this becomes f(x₀) minus now we have to move by Δx/2 to the left which becomes f(x₀ - Δx)/Δx. Let's now collect what we have found in total, we are dividing by (Δx)² and in the numerator, we've got plus the value one step to the left plus the value one step to the right minus twice the value at our original position. So what originally appeared in our equation, T7 + T9 -2(T8) is 1 mm² plus the second derivative of the distribution of the temperature. Given this central-difference formula for the second derivative, try to figure out essential difference formula for the fourth derivative. The rate of change of the rate of change of the rate of change of the rate of change. Obviously, there's is a factor of 1/Δx⁴, but how much of these values of the functions do we need as we go one step to the left, two steps to the left, one step to the right, two steps to the right. Insert the correct numbers. Think of this like a Python. It's 1 times the value of one step to the left--1 times the value one step to the right-- minus twice the value at the original position. And now we have to insert this Python into itself. If you look at this one, one step to the left, we have to go one further step to the left times 1, this 1 times -2 and this 1 times 1. So one further step to the left is this one, two steps to the left minus 2. This was the one, one step to the left--1 is our original one. Similarly for the right one, we go one further step to the right and so on and so on. One step to the left and minus 2 times the original one. For the central one, we have to multiply the Python by -2, so we've got -2, 4, -2, which boils down to 1 times the value 2 steps to the left minus 4 times the value 1 step to the left 6 times the value at the center minus 4 times the value of 1 step to the right and 1 times the value 2 steps to the right. We use this equation to describe the conduction of heat along the wire and over in a position to put some mathematical meaning to it. As time goes on, the value of the temperature changes by the length of the time step times this expression. So this expression has to be the rate of change of the temperature with respect to time. Ṫ₈ if you will at times 0. Of course, we're always dealing with estimates here even though I'm writing an equal sign. But now that we know about central-difference formulas, we have another interpretation of the term. It's 1 mm squared delta x squared times the second derivative of our temperature. Of course, I'm again cheating a little here. We only know the temperatures of the different compartments. We don't have any curve of which we could form the second derivative. Now, we're going to write this equation in a professional fashion. Let's do this right. Temperature depends both on space and time. It changes along the wire that is both position, and it changes with time. We see temporal evolution. What we need here is the rate of change with respect to time and the way to be writing this is this. It's called the partial derivative of the temperature with respect to time. It's not written with lower case letter d but with curly type of d. This derivative of the temperature with respect to time equals 10 over 1 second times 1 mm². And this one is the second partial derivative of temperature with respect to position written like this Note how this is being written. The curly d is squared not the complete expression in the numerator. Even though in the denominator, everything is being squared. They're going to discuss partial derivatives in the next segments so bare with me. This equation is called the heat equation. In this case, it describes the conduction of heat along the wire along one dimension, and it's a partial differential equation. PDE is the technical term. It contains partial derivatives. That's why the differential equations we have seen so far are ordinary differential equations, ODEs. PDEs, partial differential equations, are typical for problems in space or problems in space and time. The constant involved here is often called alpha the thermal diffusivity. The name already hints at this being a diffuse equation. The heat diffuses along the wire. Now, we have a function that depends on two variables. The temperature depends on time as well as on position. The typical way to visualize this is to use a 3-dimensional drawing. For every time and every position, we go up by the value of the function, and so in the end, we get some kind of curve surfers that describes the function. The partial derivative with respect to time computes the rate of change as time increases, but we leave x constant. The slope of this tangent line is the partial derivative with respect to time. The partial derivative with respect to x keeps the time constant and only looks at what happens when we change x towards the slope of such a line. In the end, it boils down to computing regular derivatives and simply leaving all the variables fixed that are not mentioned. If you form the partial derivative with respect to T, we treat x as though it was a constant. If you form the partial derivative with respect to x, you treat T as though it was a constant. Here comes a simple situation in which to apply the heat equation. Assume that the heat is distributed in such a way along the length of the wire, the temperature depends linearly on position--one end is cool and one end is warm. If you wait for a short time, what happens with the central part of that function. Will it move left, will it stay as it is, will it move right? The correct answer is it will stay as it is. One way of seeing this is the following--the central compartment loses temperature to the left and gains temperature from the right if you will. The amount of gain and loses proportional to the difference in temperature, but to the left and to the right, the difference in temperature is the same, so we lose as much as we gain. The temperature of the central compartment has to stay constant. Of course, the same is true for the compartment to the left. We'll just look at one further compartment down the road and see that this compartment to the left gains as much as its going to lose. Another way of seeing this behavior is to look at the heat equation. The partial derivative of the temperature with respect to time equals thermal diffusivity times the second derivative of temperature. Now, with respect to x, this is a line of constant slope, which means we have to form the derivative of this derivative with respect to x, but when the first derivative is constant, the derivative of this derivative is zero--this line has a slope of zero, so the right hand side of the heat equation equals zero, which leads us to the statement that there is no change in time. In reality, of course, we can never have this type of linear gradient going on and on. The temperature must not shrink below 0 K, but of course in practice, there's some upper limit as well. So eventually, we will see some changes to that curve because energy accumulates downstairs and is depleted upstairs. This is again the simulation of heat conduction along a wire. Now, thermal diffusivity and Δx² are spared out and only the result after 10 seconds is being shown--no intermediate results. The result after 10 seconds is going to be complex enough because we're going to modify the step size. Find the step size that barely works. We don't want to see temperatures below the ambient temperature and in particular, we don't want to see negative temperatures. Leave three decimal places of that step size. The step size of 0.050 is just at the threshold of working. This is what we get for the distribution of temperature and you can already see quite some artifacts cropping up. If you use just a slightly larger time step, hell breaks lose. You'll get huge temperatures, you'll get negative temperatures, and lots of oscillation. Our method becomes unstable. The kind of method that we are using to solve the partial differential equation, the heat equation that is, is professionally called the finite-difference scheme, because we are forming differences of values that fit in finite distances not infinite decimal distances. Diagrammatically, what we are doing looks like this. They are using the current value of our current position and the current value to its left and the current value to its right to estimate the next value at our current position. There are other ways for doing this. This is the most simple, but obviously, it leads to instability. In the previous quiz, we saw that this scheme has a particular issue with this sort of function. A function that oscillates up and down with each step in space. So let's compute what happens with this Berry function. Assume that this is our initial distribution of temperature. Temperature at times 0 depending on x. Of course, physically speaking, negative temperatures won't make sense and particularly are not without the unit of measurement. But now, we're doing a little mathematics. Now, let's compute what's going to be the next temperature at x equals to 0. The result of the first step of the temperature at time h, h is the step size, and position 0 will be its former value times 0 and position 0 plus step size times the rate of change, and the rate of change is thermal diffusivity times the second derivative with respect to x, which we estimate 1 over Δx². The old temperature to the left minus Δx plus the old temperature to the right plus Δx minus 2 times the old temperature at 0. So what's that going to be? The old temperature at position 0 equals 1. Same here. The old temperature to the left equals -1 and that's to the right is -1 as well. This is 1-1-1-2, which makes -4 step size times thermal diffusivity divided by Δx². Let's look at this result--after one time step, the temperature at position 0 equal 1 minus 4 times the time step times thermal diffusivity divided by the square of Δx. What we want temperature should diffuse to the left and to the right. Eventually, everything should ever reach out--in this case, to zero. So we want to pick the step size in such a way that our function eventually decays to zero as time tends to infinity. What choice of time step do we have, does the time step have to be less than 1/4 Δx² divided by thermal diffusivity or twice that ratio or exactly that ratio or 1/2 that ratio. It's the final option. The time step has to be smaller than half of Δx² divided by thermal diffusivity. Let me explain why--if the time step was exactly equal to 1/2 (Δx)² divided by thermal diffusivity. This expression would equal to and our result would be -1. What would happen is that our initial temperature distribution would be reflected after the first time step where it was 1 before, it would be -1 after, and it's easy to compute that where it was -1 before, it would be 1 after. The next time step would undo this reflection and here the blue distribution again and so on and so on. We would see infinite oscillation. We would see oscillation but no decay. If age is smaller than 1/2 this ratio, this expression is smaller than 2. If age is smaller than 1/2 times this ratio, this expression is smaller than 2, we're subtracting less, so the result is larger than -1. Oh a new temperature, 1/2 to 1 times step at position 0 is not -1 but a little more than that, and of course, it's similar for the others. So you see that when it totally reflecting our function, we're losing a little, and with the next time step, we again are going to lose a little, so that's going to work out well. As soon as age is smaller than 1/2 this ratio, we're going to see decay, but to stop that oscillation during the decay, we indeed have to pick age smaller than 1/4 of that ratio, so that we're subtracting less than 1. This condition shows that our method is really poor. If you use a spatial subdivision that's finite by a factor of 2, we have to use a time step that's smaller by a factor of .4. Hopefully, this reminds you of Unit 3, the recovered implicit methods to deal with such instabilities. For partial differential equations, implicit methods maybe unavoidable. It's about time to get even one dimension more, heat conduction in 2D. So, we're talking about temperature distribution that depends on time, on x and as a premiere on y. One way for visualizing this is a movie. Each frame of the movie depicts the temperature distribution in x and y at that precise instance of time, then we come to the next frame of the movie which shows us the next distribution in x and y and so on and so on and what we expect to see is diffusion. If there's only one hot spot at the beginning, it should gradually dilute over time. The heat equation in 2 dimensions does not contain any surprise. The partial derivative of the temperature with respect to time equals α times the second derivative of the temperature with respect to x plus the second derivative of temperature with respect to y. Note aside, this expression is often called the Laplace operator applied to T. Don't confuse this triangle with the delta. It's also often called the Laplacian. You can imagine the Laplace operator to measure something like the bend of a surface in 2 dimensions and there's a very simple finite-difference scheme for that differential equation as well. Now, we're looking at one point at the current time and the current values left and right, below and above to compute the next value at that central position. That next value is temperature after one time step at x and y and we can spell out our estimate. This is the value before times 0 x and y plus the time step times the diffusivity divided by the spatial step size squared and now we got lots of terms The values of our function to the left, to the right, below, and above all at times 0 to the left (x - Δx0₁), to the right (x + Δx), below (y - Δx), and above (y + Δx). Hopefully, it's not already confusing that I'm using Δx to also denote the increment of y. Now that we've covered the outer point, the inner point remains. In one dimension, we are to subtract twice the value of the inner point. In two dimensions, we have to subtract four times its value. It's easy to see that this has to be a factor of 4. Imagine that it's equal to 100 Kelvin everywhere then we get 100 + 100, (+), (+) makes 400. We subtract 4 times 100 so we get 0. The new temperature will be old temperature as it has to be. There's nothing that could be smooth out if the temperature is already constant in space. This factor out 4 also hints at what's going to happen with instability. We have to be even more picky about the step size. It can only be half as large as in the one dimensional case. And the other artifacts that we are going to see close to instability are checkered board patterns. Now, it's time to implement the diffusion equation in 2D. Of course, the arrays for the old and the new temperatures are now 2-dimensional arrays, and we initialize the arrays by putting in a right angle of high temperature. You may wonder about the order of y and x here. Shouldn't x be the first one and y be the second one. We use this order with the y appearing first and the x appearing last because we stick to the idea of the arrays being matrices. With the matrix, the first index refers to the number of the row and the second index refers to the number of the column. Changing the row mean to go up or down, which is like changing y, and changing the column means to go left or right, which works like changing x. Once you've got the order on y and x right, the rest should be pretty straightforward. We're going one step left, we're going one step right, we're going one step down, we're going one step up, and subject four times the original value. And the result is that our initial square has turned into a soft blob. In case you're wondering, we're using 50 steps from the left to the right and from the bottom to the top. How could we get such a smooth picture with just 50 50 values. The trick is interpolation. So let's switch off interpolation to see how our raw data really look like. Now, you can see those 50 steps in x direction and y direction. Interpolation turns this pixelated image into one that looks really smooth. Now, let's go shopping. What do we need to build a simple model of a wildfire. The most important simplification that we are introducing is to treat the forest as though it was a football field. We neglect height, we've reduced everything to two dimensions. In two dimensions, we can already describe how heat diffuses, but in addition to that, there's also heat loss, there's wind, and first and foremost, there's combustion. Now, we are going to look into these three starting with heat loss. Which of these differential equations should we be using to model heat loss. We're introducing two constants here, the ambient temperature of 300 K and time constant for the heat loss of say 2 minutes. Should the rate of change of our temperature be that temperature minus the ambient temperature divided by the time constant or just our temperature divided by the time constant, or the difference the other way around divided by the time constant, or just the ambient temperature divided by the time constant. Three of these solutions won't work already for the reason that they would lead to an increase in temperature even though the forest is warmer than the ambient temperature. With this solution, the rate of change of our temperature would always be positive. That doesn't look like heat loss. The same happens here. If the temperature of our forest is larger than the ambient temperature, we again get a positive rate of change here. This does not make sense. This is the right one. Think about two compartments. We have one compartment with temperature capital T and another compartment with a temperature of 300 K. The difference between these two is what controls the decay of the temperature, and of course, this difference is negative as it needs to be for our temperature to decrease. Modeling the effect of wind seems to be pretty straightforward. It looks as though we simply have to push the fire through the forest with a certain velocity. If the left curve is the temperature distribution at times 0 after 1 time step, h will be shifted to the right curve, and the amount of shift amounts to the time step times the velocity. As we are storing these values in an array, the wind will simply shift their position in the array. The tricky thing, however, is that the time step times the velocity is really small. We would have to shift these entries by a fraction of a step. How would you do that. Here comes a different approach. I want to get the new value, the value on the right curve at the position X₀. I get that new value by looking it up on the original curve step size times velocity to the left from here, and now the trick is to use the derivative of this function. Let's write down what this new value is. The new value after 1 step time equals the time step h. At this position, x₀ is approximately the old value T(0, x₀) and now we have to take care of the difference between these two, and if we estimate it using the derivative, so all our correction the length of that green line will be step size times velocity this length times the derivative--this slope of our tangent. The derivative is the partial derivative of temperature with respect to x at times 0 and position x₀, and we have to be a bit careful about the sine. If the derivative is negative such as in this case, we have to go up, which means we need a minus sign in front. So that's the trick, we use the spatial derivative to accomplish this shift. As usual, however, as with all these methods, we have to be careful about not taking too large steps. This value with step size in time times the velocity has to be reasonably small. In particular, it should be pretty much smaller than our grit size Δx. The way to compute this derivative is, of course, to use a central-difference formula. Now, implement the shift of the curve. There's no diffusion, no heat loss, no combustion. It's just about shifting the curve. The velocity is already specified and we have different initial conditions. Now, it's just the sine wave. We'll experiment with that in a second. You can already have a look at what's happening at the boundaries. They are pretty ugly. No surprise in the implementation. We're subtracting step size times velocity times the estimate of the derivative and that estimate stems from our central-difference formula. In the residing diagram, you see that overall you achieved the right effect, the sine curve is being shifted, but we definitely need to take care of the boundary-- at least when our signal reaches the boundary. And let's try an experiment, I'm inserting the number 30 here, meaning that I want to start off with 30 periods of the sine--not the single period of the sine. And to make the image less clutter, let's use the number of 500 down here. This is the result that we get--we can see the artifacts at the boundaries, which we already know, which we already know, but there's a second phenomenon. We can see that the amplitude is slightly growing. We're growing from the blue curve to the green curve. So this looks unstable. The higher the frequency that we fit in, the more unstable this becomes. This is why I start with the sine wave and not with some rectangular function. The good thing is if we combine this way of shifting the function with diffusion, diffusion washes out the high frequencies and saves us from instability It's time for the final ingredient, combustion. Here, we're just looking at one-singled cell at a time. We're treating each cell on its own. We're not looking at the neighbors unlike as we did with diffusion and with winds. For this singled cell that we're currently looking at, we have to take two different quantities into account. The first is the temperature which we already know, it's measured in Kelvin. The second is the amount of woods that can be burned. Actually that should be something like a density of wood to be measured in kg/m². As the combustion goes on in the cell, this density is reduced possibly to 0 and in addition we can use this density to model a variation in vegetation. In a thick forest, the value of this density goes up and at a location at which there's a clearing, the value of this density goes down. Now, we have to distinguish between two different cases. First, the temperature is below the ignition temperature which is something between 500 and 600 K. Second, the temperature is above the ignition temperature. Let me put an equal sign in here to really cover all possible cases even though in reality of this temperature, they'll never be exactly equal to that ignition temperature. When we are below that point of ignition, the amount of wood does not change. It's not burning. No heat is being produced because there is no combustion. Later on of course, the temperature is going to change through diffusion, through wind, and through heat loss but at the moment, I'm just looking at combustion. In the second case, we do have combustion. Wood is burning so the rate of change of wood with respect to time has to be negative. That's called the burn rate and the simple model for the burn rate would be to say that it's proportional to the current amount of wood. The more wood we have, the more we burn and we introduce a time constant for the burning that's called tB The larger this time constant is, the smaller the burn rate is, the less we burn. This time constant describes so to speak the time it takes to burn a specific amount of wood. Now we have to think about temperature. In the second case, wood is burning so the temperature should be increasing and that should be proportional to that burn rate. We need some proportionality constant in here versus something like the heating value converting from an amount of wood to an increase in temperature. This is our simple model of combustion. Now let's add the effect of heat loss in all equations about the rate of change of the temperature. We need to include heat loss which is + T amb-T/tHL for that heat loss and the same when there's combustion going on plus Tamb-T/tHL As always, it's a good idea to look into equilibria, to understand what's happening. Most probably the time constant that governs the burning of wood is much higher than the time constant that's responsible for the heat loss. Heat loss will be quicker than burning. We can safely assume that the density of the wood is almost constant on the time scale on which the temperature develops, and now we can draw this nice diagram that shows how the rate of change of the temperature depends on temperature. If we are below ignition, we only have heat loss. That's the linear function with negative slope. And when the temperature equals the ambient temperature, the rate of change will be zero. So that's this part of the curve. A linear function with negative slope and that function is zero at the ambient temperature, and the second regime, when there is combustion, we have to add this constant. And as we assume that the density of the wood stays almost constant, the burn rate is almost constant. What we get is simply a shift upwards. The same linear function as before but shifted upward like this. Now, here's a question for you--how many stable equilibria do you see here. Enter that number. In equilibrium, we had to have a zero rate of change. The temperature should be staying constant so there's three candidates. The middle one is a little tricky. Actually, our function never becomes zero. It jumps, but even if it was continuous, this point would constitute an unstable equilibrium. If we are below the temperature of ignition, there is no fire and we simply lose heat and if we are above the temperature of ignition, there is fire and we gain heat. This point would be unstable anyway. The lower equilibrium is stable. If we are below the ambient temperature, we gain heat from the environment. If we are above the ambient temperature, we lose heat at the environment. So no problem. This is stable and the upper one is stable as well. Here we have combustion. Combustion is going on. If the temperature was a little lower, the rate of change of the temperature would be positive--it would be moving up. If the temperature was a little higher, the rate of change would be negative--we're moving down, meaning at this point, we lose as much heat to the environment as we gain from combustion. The answer is there's two stable equilibria. It's relatively easy to get some reasonable values for the time constants, but it's pretty hard to estimate the heating value. There's just something that we can do with this curve. The reasonable value for this upper equilibrium is 1200 K. Just from the color of the flames, the wood should be burning at this temperature. Now, let's say that the ambient temperature is 300 K, the time constant for the heat loss is 2 minutes, the time constant for burning is half an hour, and we've got a wood density of 100 kg/m². From this data, compute the value of h--the heating value and enter that as an integer number of K/kg/m². The trick is to add these two lines to the diagram. We know the slope of this line, which is -1 over the time constant for the heat loss. We know the length of this line, which is the jump, and the rate of change of the temperature. The heating value times the burning rate. This term makes the difference between burning and not burning, and now we can use the slope to express the ratio of this leg to this leg. 1 over the time constant for the heat loss equals the ratio of the vertical length to the horizontal length. The vertical length, the green line, is h times the burn rate and the burn rate is the wood's density over the time constant of burning, and the horizontal length is 1,200 K minus the ambient temperature. You solve this equation for h, plug in these numbers, and then you get 135 K/kg/m². Now, we have all the ingredients to that model of a wildfire. The heat equation in 2D to describe the diffusion of heat. We have a model for heat loss. We can describe the effect of wind at least in one dimension, but that would be easy to generalize to two dimensions, and we have a simple model for combustion. In the homework, you're going to put all of these together to model a wildfire. This boils down to adding the different contributions to a rate of change of temperature. This is what the result could look like. In the region where the fire has started, the wood is depleted so they can't be any combustion going on. The fire has advanced to the right, a little to the left, and to the top. In this unit, we look into spatially extended phenomena governed by partial differential equations rather than the ordinary differential equations that we have seen so far. Equilibria, again, played a vital role and we saw that solving partial differential equations on the computer leads to severe numerical issues. Next time, I'm going to give an overview on more advanced topics including some hands-on experiments. Welcome back. By now you have seen all kinds of models based on differential equations and all kinds of tricks to solve them on the computer. If that was all there is to it, thousands of phD students would be jobless. In this final unit, we'll touch upon more advanced topics such as modeling buildings and air and water. We'll explore why it's hard to predict the weather, and we'll look into aspects of software engineering, that is, how to do computer simulations in a proficient way. Let's start with a bird's eye view on the finite-element method--FEM. These days it's the workhorse of almost all mechanical engineers. FEM can answer, for instance, what happens when you put a huge truck on a bridge. To compute this deformation is application of FEM to the static case, but FEM can also be applied in a dynamic setting, for instance, to simulate the effect of a crash on a car body. Here it's important to look at the process of buckling. Whereas in the static case we're not really interested in how the truck was placed on the bridge. It just has to be there. I want to outline three fundamental ideas of the finite element method. The first is discretization. The continuous structures are approximated with the help of--guess what--finite elements-- elements of finite size, not infinitesimal size. When we do so the first question is which geometry these finite elements should have. Should they be tetrahedra? Should they be cubes? Or should they even be curvilinear? The second fundamental idea is that of interpolation. Given the finite elements, how do I compute a value at an arbitrary location? For the static case, a really fundamental idea is that of minimization of the potential energy. Think about a ball that rolls on a terrain of mountains and valleys. Eventually, it's going to come to rest in a valley. In the static case, potential energy is minimized. Let's have a closer look at that valley. In the Gedankenexperiment, it splices this object a little further to the left or a little further to the right. Then the energy stays almost the same because we're at the bottom of the valley, which means that to displace this object in this way require no work. This is the concept of virtual work. For all infinitesimal displacements that are allowed-- we can't go down and we can't go up, obviously-- the virtual work equals 0. In mathematics, this way of posing the problem with the help of virtual work is called a weak form. The strong form would be to ask for all forces to compensate. The weak form asks for the virtual work to be equal to 0 for all allowed virtual displacements. This weak form results in a finite number of equations that we can solve on the computer. This finite number, however, may range in the hundred thousands or even millions. Now let's try out some of these ideas with the flexible rope. We'll fix both ends of that rope and want to see what the equilibrium shape is going to be under the influence of gravity. The obvious choice of finite elements is springs. Springs of a given rest length, so that the potential energy of each spring amounts to 1/2 times the spring constant times the square of the extension of the spring, which is the distance between the two endpoints minus the rest length of the string. To model the mass of that rope, we attach mass points to these strings. Of course, these mass points carry potential energy due to gravity. Given the constants that we provide, compute the potential energy of that rope. The one end of that rope will be fixed at x = 0, y = 0. The other end of that rope will be fixed at x = 1.3 m and y = 0.4 m. Our code starts with a random initialization and then applies a pretty simplistic strategy to minimize the energy. For a certain number of times it's going to pick one of the masses and change the position of that mass point by a random vector. If the energy decreases, it keeps that new position. If it doesn't, it returns to the position before. Very simple, but highly inefficient. In the implementation we are summing up the energy for all points, which is mass times gravitation acceleration times y position for every point and the energy for every spring. The resulting diagram may be regarded as abstract art, but also shows the evolution of that rope. We are starting with a random configuration, but eventually the shape of the rope becomes plausible. Keep in mind that this is not--I repeat--not temporal evolution, even though it looks like that. This shows a path of optimization toward an equilibrium. We are not talking about forces or acceleration here. It's a search process, like finding the shortest path to connect 10 cities. A topic highly related to the finite element method is computational fluid dynamics--CFD-- the study of gases and liquids with the help of the computer. For instance, to find the optimum shape of an airfoil or the optimum shape of a car body or to design hydraulic machinery. I've titled this section "Why CFD is hard," so now let's look into that. If we were looking at a single particle of mass m that is subject to a force F Then the rate of change of the velocity would be proportional to that force. That's one of Newton's laws--force equals mass times acceleration, which is the rate of change of velocity, the derivative of velocity with respect to time. For the fluid, something similar has to happen, but now we're not dealing with a single particle. We're dealing with a virtually infinite amount of particles. What we are working with is not the velocity of the particle. It's the velocity field. For every position in space, we specify the velocity, so the velocity that we specify is the velocity of that particle at that instant of time. Before and after, most probably, this location is going to be occupied by other particles at other times. When we write down Newton's equation for this particle-- force equals mass times the derivative of velocity with respect to time-- We have to be a little careful. Let's look at what happens after a very short time step. It's mass times 1 over the time step, and now we have to form the difference of the velocity after that time step minus the velocity before that time step. The before part is easy. That's simply our velocity field at the current time and the current location. The tricky thing is the after part. It's the velocity field at the later time--t plus time step. Now we have to take care of the fact that our particle has moved a little. We don't need the velocity field at that later time. At position x it has to be a slightly different position, namely, how far did we advance? We advanced by time step times velocity. Now, this is going to make things ugly. The velocity field of something that includes the velocity field. A function applied to itself. This is what makes things ugly and eventually leads to computational fluid dynamics and eventually leads to computational fluid dynamics being hard. If we do the math right, this becomes the following. First we have to look into the change of the velocity field with time, so we get its partial derivative with respect to time. But then we also have to look into its change with position, which is the partial derivative with respect to x, for instance. The larger the velocity is, the more effect the spatial derivative has. What we get in the end is the x component of the velocity times the partial derivative of the velocity with respect to x. Of course, the same happens with y and z. This is going to be the acceleration, and this is inherently nonlinear. We have a product of a function that we're looking for--the velocity field--with itself. This is going to make solving the differential equation that results from this really hard. Finally, however, even though the resulting equation-- the so-called Navier Stokes equation--is going to look pretty complex, it's nothing else but Newton's law applied to the velocity field. At every point of our fluid we have a velocity, but also we have pressure. Velocity has direction. Pressure doesn't. Now let's try to find out how to incorporate pressure in our equations. To make things simple, we look at a one-dimensional situation-- a straight pipe filled with fluid. Capital a is the cross-section area of that pipe. Let's replace some amount of that fluid with the piston of width Δx, and let's assume that the pressure to the left of that piston is p₁ and the pressure to the right of that piston is p₂. Then this force is proportional to the difference of those two pressures. If p₁, the pressure on the left, is larger than p₂, the pressure on the right, the force is to the right. So we have to choose sides like that. The proportionality constant is simply the area. If you double the area, you double the force. If we say that this piston has a volume of v, then the area, of course, is the volume of the piston divided by its width. If we want to use this as part of our differential equations, what should we be writing? What's the force per volume? The partial derivative of pressure with respect to x? Minus the partial derivative of the pressure with respect to x? Half the partial derivative or the pressure divided by the square root of the area? It's the second one--minus the partial derivative of pressure with respect to x. If you look closely into what we've got here, (p₁ - p₂) / Δx-- if this was (p₂ - p₁) / Δx, we would get the derivative, but it's the negative of it. Because fluid dynamics is so hard, many people try to come up with toy models to at least get a glimpse of what's happening. One of these toy models--a particularly interesting one--is the Lorenz system. By the way, this name is not to be confused with that of the physicist who spelled with "tz." This is what is described by this model. There is a layer of incompressible liquid between a surface of constant high temperature and a surface of constant low temperature. We've got some melting ice-cubes here. The velocity field in that liquid is described by three highly abstract parameters, called x, y, and z. They describe the relative amount of different sorts of motion. Even though this is a very sketchy and abstract description, some people have succeeded in implementing these equations with an actual water wheel. The idea about studying these equations is that they may be telling us something about every system in which convection is present. When we plot the solutions of the Lorenz system in 3D this is what we get-- a very intricate butterfly-like pattern. Here we're seeing two solutions--one in green and one in blue-- starting from almost the same point--something to explore in the next section. The trajectories make some turns on one ring, then jump to the other ring, jump back to the first ring and so on. The number of turns they take per ring seems to be almost unpredictable. There is one more thing. Even though both trajectories have almost the same starting point, you can see that they quickly diverge in the course of time. We're going to study that in the next segment. Now we are going to look into the strange behavior concerning the two solutions that start at almost the same point. This is about deterministic chaos. Implement the forward Euler method for the Lorenz system and do this computation for two initial conditions that are only slightly--every so slightly--different from each other. Our program will eventually plot how the distance between these two trajectories evolves. If you first want to see the butterfly, uncomment the lines below and comment the lines above. This is a straightforward implementation of the forward Euler method. This x at the initial step is actually 2D vector. S is the y. The result will again be a 2D vector with those two values for the two trajectories in it. The resulting diagram may need some explanation. We see the distance between the x values of the two trajectories over the course of time. Initially, they are really close--10^-14. And then they grow and eventually become macroscopic--10^100. The important thing to be seeing is this linear slop. But actually it's not linear. We are working with a logarithmic vertical axis, which means that the distance grows in exponential fashion. It takes about 30 units of time to increase the error from 10^-14 to 10^-4. These are 10 orders of magnitude, a factor of 1 to 10 billion. This leads to the following conclusion: whenever there is an uncertainty about the initial conditions-- however tiny--the effect of this uncertainty will quickly grow to a visible size. So, even those the system is deterministic, once we have fixed the initial conditions, everything is determined-- even though that is the case, it's behavior looks random. This sensitive dependence on initial conditions is the hallmark of deterministic chaos. That is to be expected. Every system, which includes convection, also inherits this aspect. If somewhere a butterfly flaps its wings, some days or months afterward this is going to change the weather at some remote location on the earth. Up to now we have only dealt with the effect of a finite time step, but actually also the representation of the values on the x axis is cross-grained. Now we have a look at what this leads to, and for simplicity it's again the forward Euler method. As a test case, we're using that satellite that's almost geostationary from Unit 2. It takes precisely 24 hours to complete one orbit around the earth. What's critical about the forward Euler method in terms of round-off error is this addition and this one. X(t) is a number of reasonable size. In this case quite an amount of meters. I'm just drawing some ghost figures here to give an impression, but the step size times the velocity is pretty small in comparison, because we're multiplying by that small step size. If the velocity is of reasonable size and we multiply by that small step size, we get a number that's rather small in comparison to x. Then we form the sum of these two numbers and get something that looks like this, but actually this number again is computed and is being stored at the position of x--the loose position in that process. Actually, you see the value of x didn't have enough position to start with. The smaller we choose the step size, the smaller this number becomes, the more grave the effect of round-off error will be. The effect of the round off error is that we actually are losing the details of velocity and force. We could have gotten away with not computing these digits anyway. To see a noticeable effect of round-off errors in a manageable time, I'm cheating a little here. I'm using standard Euler, but them I'm multiplying by a number of 1 plus tiny random number. This epsilon is defined to be 2 * 10⁻⁶ in advance. This looks as though I have a pretty huge round-off error every time. This is the result that we get. First I have to explain why we see several points for each step size. This is because we're dealing with random numbers and because I let the computer try out every step size 10 times. The overall behavior is what we know. With the forward Euler method the error depends linearly on the step size, at least for step sizes that are small enough. But for very small step sizes you see--uh-oh--the error explodes, and we have many small steps. Round-off error accumulates and destroys our result. What you should be learning from this reducing the step size only takes you so far. There is a threshold after which the error goes up again and may become severe. Let's look at the same data from a different viewpoint. Let's note that the error depends on the step size, but let's plot how the error depends on the number of steps. This is what we get. For a small number of steps, the step size is large, and the forward Euler method introduces a huge error, which then decreases in a linear fashion, but as the number of steps grows, we get more and more error due to round off. The simple model for this behavior would be to say that the error introduced by rounding is proportional to the number of steps and hence inversely proportional to the step size plus h. Now that we have an idea about the effect of round-off error, we can actually compute some estimates. For the forward Euler method, the total error is composed of two parts-- the part due to the Euler method, the global truncation error, if you remember that term. A constant times the step size plus the error due to round off, which seems to be proportionate to the number of steps and, hence, inversely proportional to the step size Epsilon denotes the relative size of the error. It's something like 10⁻⁷ or 10⁻¹⁶. By which fraction does our number tend to change due to round off? Again, we should see proportionality here. If we have twice the amount of error in each step. We should be seeing twice the amount of total error, even though, again, the error induced in the earlier steps is growing over time. D is some proportionality constant that we don't know yet and that depends on the problem as does C. So, there is one component to the error that grows from zero to infinity on principle. There is another component to the error that starts at infinity and decays to zero. We have to have a minimum in between. This is what we saw in the experiment. It's called the step size at that minimum hmin. It does not make sense to use more steps, that is, to use a smaller step size, because the error will be growing again due to round-off. Now, here is a quiz. For the two types of floating point numbers that we're typically dealing with, we can specify the value of epsilon. It's about 10⁻⁷ for single precision. In C and related languages, that's called float. It's about 10⁻¹⁶ for double position. Python works at double position, and the arrays of numpy can be configured as to which position they should be using. Now here comes the quiz. Say we do an experiment at single position and find that the value of hmin amounts to 10⁻⁴. If we try to solve the same problem at double position, what do you expect to be the value of hmin? Pick one of these: 3 * 10⁻⁹, 4 * 10⁻¹⁰, 5 * 10⁻¹¹? Here is a hint: at the minimum, it turns out that the contribution of both parts is the same. Ch will equal Dε/h. It's 3 x 10^-9. Let me show why. At the minimum, both contributions are equal. I'll explain why that is so in a second. So we find that the constant, C, x the value h minimum equals the constant D x ε over h minimum. >From which follows that h minimum is the square root of Dε over C. ε is multiplied by a factor of 10^-9, which means that the value of h min, for double position, is multiplied by a factor of square root of 10^-9. Starting from the value of h min for single position, we have to multiply by the square root of 10^-9. In total, this is 10^-8.5. And that's about 3 x 10^-9. An important remark: This model was made for the forward Euler method. If we have a method of higher order, if we have Ch square, or even Ch 4 in here. So from this result we learn that as we go from single to double position, we may be using a step size that's about 4 to 5 orders of magnitude smaller, and hence the error will be smaller by 4 to 5 orders of magnitude. The price that we would be paying is that we have in the order of 10^4 to 10^5 more computation which looks a little prohibitive, but then again, with a method of higher order, you need half your steps than with the forward Euler method. So the affect of roundoff errors tend to be way smaller with more advanced methods. In case you're interested, let me explain why these 2 contributions are equal at the minimum. Otherwise, skip over the rest of this video. Substitute the step size h, by e^-u, with an appropriate value of u. Then the total error becomes C x e^-u + Dεe^u. Because we are dividing by that power. Now you can see that the 1st part becomes a falling exponential function, and the 2nd one becomes an increasing exponential function. Both fall, and due to symmetry, the minimum has to be where these 2 curves meet. A completely different approach to that problem is, of course, to compute the derivative and set it equal to 0. Choosing the right algorithm and choosing an appropriate floating-point format are just 2 of the issues we encounter when we implement a solver, a program to solve a differential equation. On a higher level, one may think about reusability and maintainability. This is my replica rendering of the Plug and Play style of implementation that's used in the book "Numerical Recipes." With this type of implementation, you can mix and match components, which comes in very handy for experiments. Let's have a closer look: 1 minor but vital component is the right hand side; the right hand side of the differential equation, that is. The rate of change. It's used by the algorithm, the algorithm being, for instance, the forward Euler method, Hunt's method, and similar. The algorithm is called by the stepper. which controls which steps are being made at which size. The stepper is responsible to adapt the step sizes and maybe to reduce the steps when it finds the error has grown too large. So the algorithm is only able to do 1 single step. The stepper calls the algorithm. The driver is the interface to the user, which, of course, is a program and not a human. The driver can be sent commands to start and stop, and eventually the driver returns the data to the user. To this end, it has a subobject that's called output. The output may collect values at predefined instances of time, or it may interpolate the data it receives from the stepper to regular time intervals. The stepper will pick smaller and larger time intervals. The output may interpolate them to produce data at regular time intervals. Given the division of labor in the solver as discussed before, which component would be affected by each of these actions? To use an implicit method? To stop at time 42? To determine the value at time 13.000? And to set a different value for the mass? Enter the first letter of that component. The driver, the stepper, the algorithm, the right-hand side, the output, or is it unclear? Setting a different value for the mass is obviously something that has to happen in the right-hand side. The differential equations, as such, contains our physical model. Determining the value at one precise instant of time is a job for the output. Most probably it has to interpolate between values before and after, because there is no step at exactly this instant of time. So, this is O. Stopping at the given time--that's a job for the driver. It will stop the stepper. Using an implicit method is tricky. That's unclear. Somebody would be responsible to solve the equation. Who would that be? There is no explicit right-hand side of that equation. 1 decision that has to be made early on in the interpretation of a server is whether or not to use parallel computing. There are a range of ways for doing so and all of these can be combined with each other. We have vector units on the CPU, which are units that can operate on vectors; apply the same operation to say, 4 different values, at the same time. Multicore CPUs allow us to do several very complex things at the same time. The same is true for multiprocessor systems. GPUs, graphics chips, are optimized for working with huge arrays of pixels, but they can work with everything else as well. Which means, of course, they are also good at working with matrices. It's rather difficult to speed up servers for ordinary differential equations. The server does 1 step at a time, and every following step depends on the step before. How could you do some of these in parallel? That does not seem to work well. Speeding up a differential equation server for ordinary differential equations with the help of parallel computing may work, for instance in a situation where you have lots of isolated entities which rarely influence each other, such as the planets orbiting the sun. As you have seen in unit 6, it's of vital importance to speed up servers for partial differential equations. And in this case, parallel computing really comes to help. 1 option for this is to use spatial subdivision. Each processor of a multicore system can handle its own spatial domain. Treating the borders between the different domains right requires communication. This can become an expensive overhead. We'll look into that in the next quiz. When we apply finite elements or work with implicit servers for partial differential equations, we often find huge matrices. They are an ideal workload for GPUs. Now let's look into the communication overhead required by parallel computing. Assume that we have n-squared course or processors, each of which handles its own square of our complete domain. The complete domain should consist of l * l cells. What would be a reasonable model for the time taken by the computation? A constant C by the number of course or processors and squared times the side length of the total domain plus another constant times n times l? Or constant times l-squared divided by n-squared plus another constant times (n - 1)? Or the constant times n times l-squared minus a constant times n times l? A constant times l-squared over n-squared plus a constant times (n - 1) times l-squared? Pick one. The second one would be a good choice, and I'll explain why. The total amount of computation to be done is proportional to l², the number of cells in the domain. So if we have n² course or processors, the amount of work for each of these is proportional to l² over n². Which means that the first term describes the time taken by the computation alone. But then the processors, or the course, have to exchange data at the boundaries. Add that, and the amount of that data is proportional to n minus 1. We have n minus 1 vertical boundary lines and n minus 1 horizontal boundary lines, and it is proportional to l, because we have l cells along each boundary line. So of these expressions, the second one makes the most sense. Footnote: When we use this expression, we'll find that the optimum number of processors is proportional to the cubed root of the number of cells, l². This is a little surprising on first sight. You may expect that the optimum number is proportional to the number of cells, not proportional to its cubed root. This relationship is easy to verify by looking at the minimum of this expression. Instead of developing your own, in many cases you can, of course, use somebody else's solver. There are lots of ready-made software for that. First of all, there is general mathematical software-- application packages, which can do differential equations, but which also can do lots of other stuff. First of all, MATLAB and its clones, such as Octave or SyLab, Wolfram Alpha if you manage to formulate your problem in one line, Mathematica, and tons of others. Second, there is dedicated modeling and simulation software, such as Simulink, which comes with MATLAB, COMSOL, Multiphysics, and Wolfram Alpha System Modeler, and PyDS Tool, which is based on, as the name says, Python. Then there are lots of code libraries that you can use to build your own application. One important library to mention here is scipy integrate-- obviously for Python again. To create plausible motion for games, there are a number of physics engines. Finally, there are cook books providing snippets of code to be reused or adapted. The most prominent one of these are the Numerical Recipes. Most solvers available on the market do not accept the differential equation in this form-- the second-order differential equation, which is not an explicit form. What you instead have to provide is a function that returns the first derivative, the rate of change. The trick is to turn this differential equation into two differential equations of first order. Here is a hint for you--the first equation should be that the derivative of x with respect to time is some new variable called y, and now you have to provide the second equation for the derivative of y with respect to time. Should that be 7-3y+4x or 7-3 times the derivative of x plus 4x or 1/37 minus the second derivative plus 4x or 1/37 minus the first derivative of y plus 4x. The first option is the correct one. You bring 3 times the derivative of x minus 4x over to the right hand side and write the derivative of x as y, so this becomes y and the second derivative of becomes the first derivative of y, so we end up with the first derivative of y=7-3y+4x. This is the standard trick to convert higher order differential equations into first order systems of differential equations. You maybe asking--why are we not using the second one. The solver needs the rate of change. It provides x and y, the current values so that function should depend on x and y and not on the rate of change, which is to be determined. Writing your own software or using somebody else's software is one thing--what about getting it right? Generally, one distinguishes between three different aspects of that. First is verification, checking whether your code really implements your model. Second, calibration. Ensuring the parameters of your model so as to reproduce given observations. And the third one is validation. Check whether the real word reflects the predictions of your model. Of course, within a certain application domain and to a certain accuracy. Verification, calibration, validation--to which of these three aspects do these three problems belong. The first one--our actual car has a braking time of more than 6 seconds. Think about Unit 5. The second one--the orbit of the spacecraft around earth is not closed. Think about Unit 2. And the third on--without harvesting the amount of fish reaches 10⁷ tons. Think about Unit 4. If the orbit of the spacecraft is not closed even though it has to be in the model, we've got some problem with implementations, so that's a verification issue. The maximum carrying capacity was a model parameter for the fish model, so in this case the calibration was done right. And the other one is a prediction of the model doesn't work out well, so that's a validation issue. Here comes one idea about how to test the implementation of a differential equation solver. We provide the solver that we've never covered before that looks pretty complex. The question is what's the order of that solver. Enter that order here. The problem that we're using is again the satellite the takes 24 hours to orbit the earth and we are comparing its final position to its start position and plot the arrow. From that graph, determine the order of that solver. Look closely--both axis are logarithmic and you can see as the step size increases by a factor of 10, the error increases by a factor of 10⁵. Remember, forward Euler was of order 1, Heun was of order 2. This is the solver of order 5. Read this the other way around as you shrink the step size by a factor of 10, the error decreases by a factor of 10⁵--that's a huge gain and you can see that this solver, when it takes steps of 3 minutes around earth gets back to where it started within just millimeters. This is quite an efficient method. It's called the Runge-Kotta feedback method. Now assume that with all these strange numbers we mistyped one-- mainly we didn't enter 56,430 but we entered 56,431. Let's look at the result. This doesn't look that great anymore. We've got an error that's well above 1 km, so getting these numbers right, which among other things belong to verification. There is one fundamental issue about calibration and validation, namely that prediction does not always go hand in hand with understanding. Look at these two models for the motion of a planet in the solar system. Let's look at the left model--here the earth is fixed in space and close to the earth--thus the center of some large circle. This point moves around that circle and a point on that second circle, which then moves in this direction. This is called an epicycle, and we can continue that process. That point could be the center of another circle and so on and so on until finally we say that the planet is moving around that final circle. That's one of the antique models and let's have a look at the right one. Here the sun is at the center, the earth moves in a circle around the sun, and that planet moves in circle. Here comes the last quiz for you--which of these models allows a more flexible calibration and which of these two models allows a more accurate prediction of the apparent motion of the planet as seen from earth. Maybe somewhat surprisingly, the left model that of Ptolemy has so many knobs to turn that it allows more flexible calibration and that it even allows more accurate prediction of the apparent motion. Footnote--in a modern perspective, this stack of epicycles actually is a Fourier series. Historically, it turned out that circles around the sun are not good enough. Kepler noticed that you have to use ellipses to make this really work and then we get a high accuracy. The lesson to be learned from that is you may want to replace a sophisticated model with accurate predictions by a far simpler model, less accurate predictions, just for the sake of understanding. This is Occam's Razor in action, trying to find the most simple explanation that does the job. We started this Unit by looking into some fundamental ideas of finite elements and computational fluid dynamics. These are two related and highly application-oriented field that of particular use in mechanical engineering. The toy model of fluid dynamics left to the notion of deterministic chaos-- the sensitive dependence on initial conditions, but we saw that randomness can be produced virtually out of nothing. Then we look into a range of topics from software engineering-- how can we build modules that can be mixed and matched, how can we reuse software, and how do we calibrate and test simulations. These questions eventually led us to a little bit of philosophy, and now it's time to apply all of these in the final exam. Euler's number and the exponential function based on that number are of vital importance in the theory of differential equations. Whenever we are dealing with exponential growth or exponential decay, and whenever we deal with oscillations, Euler's number plays a big role. This is a brief and informed introduction to that topic, for which we need a minimum amount of prerequisites. In particular, you should know that five to the first power equals five. Five to the zeroth power equals one. Five to the minus second power equals one over 25. Five to the power of one half equals the square root of five. Five to the third power times five to the fourth power equals five to the power of three plus four--seven. And five to the third power to the fourth power equals five to the power of 12--three times four. This is what you need to know about powers of numbers. In addition, you should have an idea about derivatives in this D, DX notation--for instance, the derivate of X squared with respect to X equals 2X. And you should have an idea of how this is connected with the slope of the tangent line. Here we have two pretty simple exponential functions. Two to the power of X and three to the power of X. Two to the power of zero is one; three to the power of zero is one; two to the power of one is two. Two to the power of two is four. Two to the power of minus one equals one over two, one half, and so on. Three to the power of one equals three. Three to the power of two gets pretty big--nine. Three to the power of minus one, one over three, one-third, and so on, and so on. Here comes a quiz for you: What's the slope of the tangent line at X equals zero? Is the slope larger than one? Is it equal to one, or is it smaller than one? In both cases, look at the tangent lines--what would you say? For two to the power of X, the slope is below one. You can see that by connecting this point and that point. The curve passes through X equals zero, Y equals one, and X equals one, Y equals two, and the same is true for this line with a slope of one. Obviously, our power function, two to the power of X, makes a left turn. If it hits this straight line in these red spots, it has to dive below that red line. Meaning that its slope at x equals zero is less than the slope of the red line. Less than one. The slope of three to the power of X at X equals zero is larger than one. Let me add this straight line again that has a slope of one and a Y intercept of one. And if you look really closely, you'll see that this exponential function, three to the power of X and this red line intersect at two points. The first intersection being close to X equals -.17. This means that the blue curve dives below the red line on the left-hand side, meaning that it has to have a slope that is larger than one. So now that we now that the slope of two to the power of X at X equals zero is slightly less than one, and the slope of three to the power of X at X equals zero is slightly larger than one, so it's not hard to imagine that somewhere in between two and three, there is a number called E-- Euler's number--so that E to the power of X has a slope of exactly one at X equals zero. This function is called The Exponential Function. Whereas two to the power of X and three to the power of X are just exponential functions. As this exponential function, The Exponential Function is so important, it gets a special name; It's not just E to the power of X. It's called exp of X. This is the natural exponential function. And we already know that its basis, E, is a number between two and three. Now we can start from this idea and compute Euler's number. E is nothing but E to the first power. Great. Let me write this one as 10,000 divided by 10,000. Up to now, this looks completely crazy. Where am I going? I'm going to apply the laws for powers. This is E to the one over 10,000 to the power of 10,000. Remember that rule--if you take a power of a power, you multiply the exponents. This is what you get. But E to the power of one over 10,000, that's something pretty close to X equals zero on this blue curve. So rather than looking up the value on that blue curve, I'm using the red line, the tangent line, to compute that value, to approximate that value, that is. The slope is one, so we can just put X in here, plus the Y intercept is one as well. And now I'm using this equation for the tangent line to approximate this value. The value of E to the X at X equals one over 10,000. So this is approximately equal to one over 10,000 plus one. We simply looked up that value using the tangent line. Let me plug that in. Then we have E is approximately equal to-- I changed the order here to make it look nicer--one plus one over 10,000, to the power of 10,000. Guess what? We have an equation, well, an approximate equation, for Euler's number. The official definition for Euler's number is the following: E equals the limit of N goes to infinity of one plus one over N to the power of N. I hope that you can now easily guess where this comes from. The larger this number gets, the better these approximations get. And the result is 2.7 something. Footnote: This equation is pretty elegant when it comes to deriving E, but when it comes to computing the actual value of E, this equation is pretty inefficient. So here is a quiz: If you wanted to compute E to the power of pi--E to the power of 3.14 and so on-- which of these expressions would be the best approximation? One plus one over 31,416 to the power of 10,000? One plus one over 10,000 to the power of 3.1416? Or one plus 3.1416 over 10,000 to the power of 10,000? It's the final one, because E to the power of pi equals E to the power of pi divided by 10,000 times 10,000. And now we take this apart into a power of a power. E to the power of pi divided by 10,000 to the power of 10,000. And then we use our approximation again. E to a number that's close to zero is one plus that number, which leaves us with this final expression. The funny thing about this is that we can compute such a weird expression-- E to the power of pi, with just basic arithmetics. We're adding numbers--one plus something-- We're dividing numbers, and we're multiplying 10,000 numbers. That's it. You could do that on the cheapest pocket calculator. So now we've arrived at these two equations. This is what we learned about Euler's number, and this is what you saw in the previous quiz about the exponential function. Where X was equal to pi. These equations are very elegant, but highly inefficient. Who wants to compute the product of millions or billions of factors? Power series come to our rescue. If you actually want to compute Euler's number or the exponential function, you're going to use power series. So let's try and turn this product into a sum of powers of X. To get started, here's a quiz for you: One plus X over four to the fourth power. What would that be? Some number without X plus some number times X, plus some number times X squared, plus some number times X to the third power, plus some number times X to the fourth power. Fill in these numbers using integers such as three or fractions such as five over 42. So what happens here is that we write down four equal factors--one plus X over four times one plus X over four, times one plus X over four, times one plus X over four. And then we look what this is going to turn into. For the first box we need everything that has no X whatsoever. This is just one times one times one times one. Anything else would have an X in it. So this becomes one. For the next box, we need everything that has precisely one X in it. For instance, X over four times one times one times one. One times X over four times one times one. One times one times X over four times one, and there's a fourth one: One times one times one times X over four. We get four of these. Four times X over four, which is one times X. Four X squared. We have X from the first two parentheses, but one from parentheses number three and parentheses number four. X from the first, one from the second, X from the third, one from the fourth. Or X from the first, one from the second, one from the third, and X from the fourth. Or one from the first, X from the second, X from the third, one from the fourth, one from the first, X from the second, one from the third, and X from the fourth. And the final one, one from the first, one from the second, X from the third, X from the fourth. Six options to form something that contains X squared. Six times X over four times X over four. We can cancel a factor of two, which leaves us with three over eight. To get X to the third power, we need to pick X from three parentheses, and one from the other. We can pick X from the first three parentheses, and the one from the last, or X from parentheses number one, number two, the one from the third, X from the last, X from the first parentheses, one from the second, X from the third, X from the fourth, and one from the first, X from the second, X from the third, X from the fourth. So, four ways of doing this, four times X over four times X over four, times X over four, which leaves us with X to the third power over 16. One over 16. And the final one: We need four factors of X. You have to pick this and this and this and this. That's it. Only one option for doing that. Which leaves us with X over four times X over four times X over four times X over four. X to the fourth power divided by 256. One over 256. So this is a crude approximation to the exponential function, and we find a way of expressing this with powers of X. The beginning of a power series. Let me restate the general equation for the exponential function again. E to the power of X is the limit as N goes to infinity of one plus X over N to the power of N. We have now looked at the special case of N equals four. Hopefully, you can imagine that this also works out for 40 or 400, 4,000, four billion, four whichever number. And the result is it's one plus X-- We have that already. One plus one X, plus X squared over two. You see we're close. Four over eight would be one half. We've got three over eight, with that four here. Plus X to the third power over six, plus X to the fourth power over 24, plus, and so on, up to infinity. To prove this, you need a better method than we have used here; you need binomial coefficients. But hopefully, you get the idea where this power series comes from. And actually, it turns out that the six is the factorial of three; this 24 is the factorial of four, and so on. So the general term would be X to the power of N divided by N factorial. This also gives us a way of computing E itself. E equals E to the power of one. So we simply set X to one, which means E equals one plus one plus one over two, plus one over six, plus one over 24, plus, and so on, up to infinity. And now you can see how efficient this power series is. One plus one, so we have two already, plus one half, is 2.5, plus one over six equals 2.6. You see we're getting pretty quickly pretty close to 2.71 something. So if you actually want to computer Euler's number, or the exponential function, you're using this power series. Now it's about time to compute the derivative of the exponential function. We already know one value--the derivative of the exponential function at X equals zero is one, because the slope of the tangent line is one, which was our definition of the exponential function. Everything else can be derived from that, with the help of this equation. E to the power of X plus H equals E to the power of X times E to the power of H. This looks rather simple, but this is the key to compute the derivative for every X. Let's say H is a number close to zero. Then E to the H describes this part of the curve of the graph, that is, of the exponential function. E to the power of X plus H describes some other part of that graph. One that's centered around the value of X. Now, this equation tells us that we can use the green part of that graph, magnify it by E to the power of X, and get the red part of that graph. Of course, the same thing would happen to the slope triangle here, which means the slope increases by a factor of E to the power of X. We do know that the slope at the origin is one, so this slope has to be E to the power of X times one. It's the exponential function itself again. This is a fundamental result. The derivative of E to the power of X with respect to X equals E to the power of X. The exponential function--the exponential function, I should be saying, is its own derivative. So here is the connection with differential equations: The exponential function is the unique solution to the following differential equation: The derivative of Y of X, with respect to X, equals Y of X, for all X, and the initial value, Y of zero, equals one. This is about as simple as it can get with differential equations. We're looking for a function, Y, that is its own derivative, and starts with a value of one. It's clear now that the exponential function is a solution to that differential equation, but as most other differential equations that are of interest, also this differential equation is so well-behaved that the exponential function is the unique solution of this differential equation. Now try to do this in a more general fashion. I'm looking for a function, y of x, the derivative of which is minus three times its value for every x, and its initial value for x equals zero should be five. What's that function? Enter your solution here. A number, possibly negative, times E to a number, possibly negative, times X. If you plug in x equals zero, the result has to be five. Look at this term. If x equals zero, we have e to the power of zero, which is one. To turn that into five, this has to be five. In the other box, we need to minus three. This is easy to see if you know about the chain rule. The inner derivative provides us with a factor of minus three, which we need. If you don't know the chain rule, you can argue as follows: We start with e to the power of X. The standard exponential function. Next, let's form e to the power of minus x, which means to flip that blue curve along the y axis. To compute E to the minus X for a specific value of X, we can compute E to the X for minus that value of X. So we're mirroring the blue curve to get the green curve. This additional factor of three accelerates the green curve. E to the power of minus 3x is three times as fast, if you will, as E to the power of minus X. To compute E to the power of minus 3x for this X, we can compute E to the power of minus X for three times that X. And now look at the derivatives. By going from the blue curve--the regular exponential function-- --to the green curve, we are changing the sign of the derivative. Here the derivative is positive. Now it's negative all of the time. And with that additional factor of three in the exponent, you're boosting the derivative by a factor of three. Look at these triangles. The horizontal leg of this slope triangle is reduce by a factor of three, meaning that the slope increases by a factor of three. So now you are able to solve such a differential equation with the help of the exponential function. We can easily say now that N of T equals its initial value times E to the minus T divided by 20 seconds. Such an equation can, for instance, be used to model radioactive decay. N of T would be the number of atoms that have not yet decayed at time T. And this derivative, the rate of change, tells you the current speed of decay, which is the current number divided by 20 per second. You would expect 1/20th of the current number of atoms to decay within the next second. This is not entirely true because that number, of course, is not constant during that second. And there's another caveat. This is a number, a natural number. So keep in mind that this, again, as everything, is just a model; it's not reality. One important thing to learn about such a process is the mean lifetime of a particle. If you look at one single particle, how long does it take, on average, for it to decay? If you look at, say, a billion particles, and form the average of their lifetimes. And now we're going to figure out a way to compute the lifetime from this differential equation. To get started, let's look at a different process--namely, one that's described by this staircase curve, rather than the nice and smooth exponential decay. What's the mean lifetime for this process? Enter that number of seconds here. We've got 50 particles with a lifetime of one second. So that would be one second plus one second plus one second, and so on, 50 times. And we've got 50 other particles with a lifetime of five seconds, plus five seconds, plus five seconds, plus, and so on, 50 times. Divided by the number of particles, which is 100. So it's 50 seconds plus 250 seconds. Which makes 300 seconds divided by 100. Is three seconds. So the solution should be three seconds. And there's something to note here. We could have computed the area below that curve-- Well, below that staircase, and divide that area by 100. That's the same. The area is 50 times one second, plus 50 times five seconds, which is our numerator, and we have to divide that by 100. And of course, that's going to be true no matter what the shape of this curve is. So this is the area divided by the initial number, the total number of particles. Now let's try to apply the same idea to the exponential decay. You start with some initial number of particles--N of zero seconds--and decay smoothly with that exponential curve. If we have this area below that curve, it's easy to say that the mean lifetime equals that area divided by the initial number of particles. Here comes a trick: I introduce a different area. I start at a time H, close to zero. And look at this area. Let's call that A of H, and consequently call this A of zero seconds. The funny thing about A of H is that it's a scaled down version of that original red area. If you multiply with the right factor, you're going to turn that red area into that green area. You might multiply with the right number to get this curve from the blue curve, and the area below that curve equals that green area. The right factor is easy to figure out. It's the ratio of the number of particles present at time H to the number of particles present initially. So this is N of H over N of zero seconds. For H close to zero, we can use the differential equation to get an estimate on N of H, the number of particles present at time H. That's the initial number minus how many particles have decayed. If you use the rate of change, then it's one over 20 seconds times the initial number, times H, the time that we waited. That's, of course, just an estimate. If I want this to be more accurate, I need to take care of the remainder, which is something of order H squared. And now here's the tiny area left. This area amounts to its width, H, times its height, the initial number of particles, where not perfectly, we're neglecting something, but that's of order H squared. And now let's put all of that together. The complete area--the red area--equals the blue area plus the green area. So that's the initial number of particles minus one over 20 seconds times the initial number of particles, times H, plus something of the order of H squared, divided by the initial number of particles, times the total area. And now look at what remains: We've got the complete area on the left-hand side, and we've got the initial number divided by the initial number, times the complete area on the right-hand side. We can get rid of these. Then we have terms that are linear in H, then we have terms that are of first order in H, H times a constant, H times a constant, and we have terms that are of second order in H. The first order terms have to match, and the second order terms have to match. So we can get rid of the second order terms. Now we can cancel the initial number here and the initial number there, and you see that zero equals H times the initial number, minus one over 20 seconds times H, times the complete area. Now we can also get rid of H, to find that the area divided by the initial number equals 20 seconds. So this ratio, which is the mean lifetime, equals, guess what? 20 seconds. So astonishingly, this number that appears in the differential equation is nothing but the mean lifetime. It's that easy. There is a second time constant that often is used to describe decay processes: The half-life, as opposed to the lifetime, which we've covered before. After the half-life, half of the particles have decayed, half of them are still there. Often, this is written as T subscript one half. What's the relation between half-life and lifetime? Is the lifetime smaller than the half-life? Is it equal to the half-life? Or is it larger than the half-life? Turns out that the lifetime is larger than the half-life. One way to see this is the following: Look at this part of the exponential decay, and now rotate it by 180 degrees to get this curve. If our decay looked like that, the lifetime would be equal to the given half-life. This and this average to the half-life. This and this average to the half-life, and this and this average to the half-life. And so on. But you see, the exponential decay extends far more to the right. Actually, it goes to infinity. Some particles live really long, and this pushes up the value of the lifetime in comparison to the half-life. So now we have an idea about the half-life being larger than the lifetime, but we can also actually compute the half-life from the lifetime. This is what we've seen before, the differential equation for the exponential decay, 20 seconds being the lifetime. This is its solution. Here's one more ingredient: A is equal to E to the power of B if, and only if, the natural logarithm of A equals B. Use this to determine the half-life in this situation. So what's the half-life? Is it 20 seconds divided by the natural logarithm of two, minus 20 seconds times the natural logarithm of one half? 20 seconds times the natural logarithm of two? Or minus 20 seconds times the natural logarithm of two? Check all that apply. If T equals the half-life, this vector has to be one half, so that the number of particles remaining is half of the initial number of particles. So we get this equation: E to the minus half-life divided by 20 seconds, equals one half. And now you take the natural logarithm on both sides and get minus half-life, divided by 20 seconds, equals the natural logarithm of one half. You bring over the minus and the 20 seconds to the right-hand side, and what you get is the half life is minus 20 seconds times the natural logarithm of one half. So this one is correct. But that's not the only one that's correct. The next one is two. If you found the logarithm of an inverse, you get minus the logarithm of that number. Remember, if you form an inverse, one over A, that would be E to the minus B. So the logarithm of the inverse is minus B. If the logarithm of the number as such is plus B. These two yield the same result. The last one can't be true because it has a different sign than the one before, which is correct. In the first one we're dividing by a number that's smaller than one. So we get something that's larger than 20 seconds, which can't be true, because we know that the lifetime, 20 seconds, has to be larger than the half-life. Hi! I'm Miriam and I'll be the assistant instructor for CS222. Now, I personally think that Math matters a lot but I know there's some skeptics out there so hopefully by the end of this course you'll know better. This is the first course that I'm working at Udacity and I'm really, really excited to be part of it. Aside from the fact that I really love Math and Physics and learning about pretty much anything, I'm a yoga teacher and I really love to bake anything. I'm not really into cooking though. I really only like baking sweet things like pie and cake and brownies and cheesecake and kind of anything that taste good. In terms of this course, I'll be helping you guys out on the forums and by filming to explain the problem sets. So now that you know a little bit more about me, I'm looking forward to seeing you in the forums and good luck in the class. In this problem set, I want you to create a function that models the orbit of the moon around the earth. We're going to assume in this case that the moon's orbit is perfectly circular, which means that we can model it using just sine and cosine functions. This assignment should then help you practice translating simple mathematical ideas into expressions in Python. So if you look over at the supplied code right here, you can see that we've helped you out with some pretty important parts of the code already. We've already told you that the moon's distance from the earth is 384 million meters, and we've also defined a function called orbit for you. Inside orbit, you can see that the number of time steps that we're going to look at as the moon moves in its orbit around the earth is 50. We've also created a 2-dimensional array called x, which is initialized with zeros. The number of rows in x is equals to the number of time steps in the moon's orbit plus 1. The reason for this is that if you're watching the moon step around the earth 50 times, then the number of positions that we're actually looking at it is 50 + 1. Looking back at x, we see that there are two columns in the array-- one for the horizontal position of the moon compared to the earth and one for the vertical position. One important thing to note is that the cosine and sine functions from Python math library that we're using in this problem, take arguments that are measured in radiant not in degrees. When you finish filling in your code here and run the program, you should get a plot of a circular trajectory of the moon around the earth. So in the solution to this problem set, we've added a for loop whose index ranges from 0 to the number of steps. We put the number of steps plus 1 as the argument for range because i will go up to the number inside minus 1. Inside the loop, we've created a variable called angle, which is equal to 2π times the index divided by the number of steps that are taken in the trajectory. Have you look over this picture. You can sort of see what's happening. The moon starts in this position, then after 1 time step, it's into this position and so on and so forth. The number of segments will equal the number of steps and the number of lines will equal the number of steps plus 1. After this, we use the array x to define the horizontal and vertical positions of the moon at any given time step. It's at this point that we have to remember our trigonometric functions--sine and cosine. In this first line right here, we're defining the horizontal position of x for the i-the timestep. In the second line, which is a sine, is what's going to show us the vertical position. This is one of those cases where being able to see a right triangle in this picture is very helpful. Looking at our picture, this horizontal line right here that extends from the earth out to the moon's orbit is the 0 radian or the 0 degree line. We're measuring all of the angles that the moon is going to be marked at up from this line. That means that at any given moment, we can draw a line from the moon down to this horizontal line to create a right triangle, which will then allow us to use sine and cosine in conjunction with this angle to determine the moon's vertical and horizontal distance from the earth. Going back to our code, after all of these is filled in, if we run the program, we end up with a perfect circle, just like we were hoping to get for the trajectory of the moon around the earth in this ideal simulation. So now it's time for us to get some practice with an application of the Forward Euler Method. This time, we define a function for you called trajectory -- that's going to determine the trajectory of a particle. We've already set the step size, the acceleration to the gravity and the initial speed of the particle. Inside the definition, we've created an array called angles that's starts out at 20 and increases with increments of 10 up to 70. This creates a set of angles at which the particle can take off. Over here, we see an example of what a ballistic trajectory of a particle with a given initial angle might look like. The next step is to create variables for position and velocity. Each of these is an array where the number of rows equals the number of steps that are going to be taken plus 1 and 2 columns. Your job is to find equations for the position and velocity of a particle that show how changing the initial take off angle of the particle changes its trajectory. Fill in your code here and good luck. The first thing that it's important to notice about the supplied code is that in the array angles, the angles are measured in degrees but as we said earlier, the cosine and sine functions in Python's math library take arguments that are measured in radians not in degrees. So first thing that we need to do is convert each of the angles in the array angles into radians. We do that by multiplying each angle by π and dividing by 180. This gives us a new variable angle in radians or angle rad. The next step is to set the initial position and velocity of the particle. As you can see from these two lines, we've decided that the particle is going to be in a zero in both the horizontal and vertical planes. This means that it's starting at the origin. Moving on to velocity, we set both the horizontal and the vertical velocity by taking the initial speed which we set up here, and multiplying by the cosine of the angle which we just converted for the horizontal velocity or the sine for the vertical velocity. Lastly, we finally make use of the Forward Euler Method. If we want to find out what the position after the next step is going to be, then we begin with a position at the step they were at and then add the time step times the velocity at that present step. You remember that this makes sense in terms of units because h is measured in time and v is measured in distance over time which makes this term also measured in distance just like x. We do something very similar with velocity except that this time, the acceleration is a constant which we set up here as the gravitational constant. This means that the value of the acceleration is not going to change from step to step although the value of the velocity will. Something else that you might want to know is that we've only set the range to go from 0 to num steps - 1. You might initially have thought that we should insert num steps +1 as the argument of the range. But because we're looking at the step +1 row in both x and v, we need to restrict the range to be one less than we would have otherwise needed. In fact, I made this error myself when I was looking over the code. So it makes perfect sense if you were thinking that too. After all this has been filled in, we can run the program and get this plot. Now this is a plot of the position, not of the velocity. You can see that the horizontal axis measures the horizontal position in meters. The vertical axis measures the vertical position in meters. Each different series here corresponds to each different element in the angles array. As you can see, because each of these particles is leaving out a different initial angle. >From this graph, we can see that as we change the initial angle, we also drastically change where the particles going to end up landing. Now, we move on to a problem that combines both this idea of ballistic trajectory and the idea of orbits from the previous problem. So in this problem set, we're going to try to plot the trajectory of the spaceship by creating functions for its acceleration and its trajectory. We'll start it out with some helpful information. We'll set the step size to 1 second. I told you the mass of the earth and giving you the gravitational constant, which you might recognize as G from the equations in other videos. We've also already created arrays for you for position and velocity, which right now are filled with zeros and set the initial position and the initial velocity of the spacecraft. Now, I'm going to remind you of some information that you've already learned that's going to be especially helpful for this problem. We talked earlier about the equation for the gravitational force between two objects that's what this little g here next to the F for force stands for gravity. This big G in the equation is this gravitational constant that we've given you in the code that's multiplied by the mass of the earth and the mass of the spacecraft and this is divided by the distance between the earth and the spacecraft squared. Now, this d with a hat over it represents a unit vector that any given vector has a unit vector associated with it. This unit vector, which is denoted by this hat over top of it, is equal to the original vector itself divided by that vector's magnitude. So this d with a hat over it up here shows us that the gravitational force has this magnitude and is pointing in this direction, which is a line pointing from the earth out to the spaceship. Now, if you look back at the code for a second, you'll see that this function that we're asking of you to find is actually for acceleration not for force, but we know that force and acceleration are very intimately related by Newton's second law, which is that force equals mass times acceleration. This means in the case that we're considering right now we can find the acceleration of the spaceship by simply taking the equation for force that we just read out and dividing by the mass of the spaceship, which leaves us with this expression right here. Your job is to translate this equation into an expression in Python for acceleration and then to use that expression along with the information we've given you in a Forward Euler Method to describe the position and velocity of the spaceship at any given timestep. So now, we're going to look at the solution to this final problem set for Unit 1. The first thing we have to do is fill in the definition for the acceleration function. Now the acceleration function takes an argument that we've called spaceship position. Well, we can define the position of the spaceship in relation to earth with this variable vector to earth. A vector pointing from the earth to the spaceship is just the opposite of a vector pointing from the spaceship to the earth. So we define vector to earth as the negative of this argument, spaceship position. Now since we're trying to define the acceleration, what we want to return is an expression for acceleration. So we rate return and then a translation of this equation over here that we talked about earlier. So we have the gravitational constant which is G times the earth mass which is mE, divided by this expression that looks pretty tricky, so let's dissect that briefly. Now, the first thing we have right here is a method from a linear algebra library which gives us the norm or the magnitude of whatever vector we put into it. We, of course, are inputting the vector from the spaceship to the earth. Now over here, we can see in the original equation that the denominator is just the norm of that vector squared. Within the code, we have it to the 3rd power. Now, the reason for this is that we are also multiplying by that same vector again. Whereas in the equation we're multiplying by the unit vector. This means that in the Python version of the equation, we effectively have the magnitude cubed in the denominator and the magnitude to the 1st power, the numerator which gives overall a factor of 1 over the magnitude squared which is exactly what we see in the original equation. This makes our job a little bit easier because we don't have to define a unit vector for the vector to earth. Now we move on to the definition of the function ship trajectory. Since we gave you all of this code, we skip down here to this final for loop. This is something that we've seen several times before. For example, in the last problem set these two lines implement the Forward Euler Method to show how position and velocity are going to change with each step. Once we have all of this code filled in, we can run the program, and our final product is this picture. Now this picture looks a lot like what we would expect from elliptical orbit. We have the earth here and the spaceship moving around it. Now everything looks pretty normal until we get almost to the end and we realized the ellipse actually won't close. This is a great example of how the Forward Euler Method is a method using approximations. If we are using more exact expressions for position and velocity, then we would end up with a closed ellipse and we can also play with the number of steps that we used to show how increasing the steps gives us a more exact approximation of the trajectory of the spaceship. Now we've done something that I think is pretty cool right here. We've changed the number of steps from 13,000 to 1.3 million. This means that since we cut the step size the same, this picture shows what the position of the spaceship would be like after much more time has passed after it has gone through many, many orbits. So even though in that initial orbit, the trajectory only seems slightly off. If we let that happen over and over and over again, eventually the spaceship will be much further away from earth than it was originally. Now, just to play with this example a little bit more, we've gone in and first change the step size, so that's 1/10 of the size that it used to be. This should give us a much more exact approximation since we're taking steps more closely together. However, we've also had to change the number of steps from the original 13,000 up to 130,000, so that we still have the same number of seconds total that the spaceship is in orbit which is a result of this changed code. At first, it might look pretty similar to what we had after we ran the program the first time. However, if you'd examine the end of the trajectory a little bit more closely, you can see that this tail looks like it's actually going to intersect the beginning. This time, it actually looks like the ellipse might close which we remember is what the trajectory in real life would look like. This is a great example of how changing the step size and using the Forward Euler Method can greatly decrease the error and the results we produce. In the next unit, we'll look at another numerical method that deals with similar situations but in a slightly different way. Now we come to the first problem of Unit 2. This time we're still dealing with orbits. We have a pendulum and we want to create expressions for the position, velocity, and acceleration. As always, we've given you some code to help you out. Here you can see the time set, the magnitude of the acceleration due to gravity, and the length of the pendulum, which is just the length of the string to which the bulb of the pendulum is attached. What we're asking you to do is to first fill in this definition of the acceleration of the pendulum showing how it depends on the position of the weight. Now if you think about the way that a pendulum swings, you can imagine that if we extend the trajectory, we would get a circle. So position is the length of this curve path. The next thing that you need to do is to fill in this function called symplectic Euler in which logically you will use symplectic Euler method to calculate both distance and velocity. To help you out a little bit, here is a refresher on what the symplectic Euler method says. Another important piece of information if you're not super-comfortable with physics stuff is Newton's second law right here showing the relationship between force, mass, acceleration. So looking back at our code, you can see that we've created empty arrays for you--for position and for velocity. It's up to you to fill these arrays in including the initial conditions. Remember you'll need to initialize x and v where x is zero and v is zero equals something that you're going to figure out. I'm going to give you a few hints though--you can see that we've created this constant called num_initial condition and set it to equal to 50. What you're really doing overall in this problem is looking at 50 different pendulums which each have different initial values for x and v, and to give you a visual of what this looks like, I'm going to show you the final plot that you'll get with this program. So here's the set of graph that you should get as your final result if the program is working correctly. You're going to ignore these top two graphs for now. Let's focus on this bottom plot. This is showing velocity on the vertical axis and position on the horizontal axis. If we look at the top two graphs, we can see that this green set of points right here corresponds to what's happening at time zero. Since green ellipse down here is showing the set of initial conditions for the 50 different pendulums that we're looking at. This green ellipse is kind of like a snap shot of what's happening to all these different pendulums at time zero. The way that I want you to figure out how to set the values for each pendulum for x and for v is just think about this ellipse. Now in Unit 1 you learned about orbits and you'll actually be able to use some of that knowledge in this problem knowing that this green shape is an ellipse. You can see that its major axis right here lies along the line v=0 and its minor axis lies along this line x=2. Now the half length of the major axis is 0.25 so I guess actually it don't looks like the major axis here. It's really the minor axis if these two sets of axes had the same scales applied to them. Either way, this rightmost point on ellipse corresponds to x=2.25 and the leftmost point corresponds to x=1.75. In terms of v, we have values ranging from -2 down here to 2 up here. So think about what equations for x and v you'll need to create an ellipse with these dimensions. That would be how you set the initial conditions for x and v. I think this is a very interesting problem so I hope you enjoy doing it. Good luck. Let's go over the solution to this problem starting with the definition of the acceleration function. We know the acceleration due to gravity points downward. So let's put this factor into two components. We have one component right here that is parallel to the string of the pendulum and another component that is perpendicular to this green one. Now we know that the acceleration in this direction is going to be exactly cancelled out by the acceleration due to the tension in the rope. So that means that the acceleration we're looking for is really just this pink component, which any point along the path is going to be tangent to the trajectory. Now if we call this angle θ right here, we can figure out the length of this pink component by just saying that it is equal to length of the resultant vector times the sine of θ. The θ down here is actually exactly equal to θ in the diagram of the pendulum itself. So that means to figure out the length of this component, we can use information that we already know about this larger diagram. Since position is just the arc link right here of this imaginary circle, then the measure of that angle in radiant is going to be equal to the length of the arc that it corresponds to divided by the radius of that circle. So that means that in our case, θ is equal to arc length over radius or position over length. So since θ equals position over length and we want the sine of θ, we fill in our definition for acceleration as -g or negative magnitude of the acceleration due to gravity times sine of the position over length. Okay, moving on towards symplectic Euler function. We have to fill in this for loop with the input num_initial conditions. As I said in the InterVideo of the problem, we wanted the initial x to vary from 1.75 to 2.25 and the initial v to vary from -2 to 2 corresponding to the coordinates of every point along that green circle that I'd showed you. Now a convenient way to make a variable cycle through values that are symmetric about an equilibrium value is to use sine or cosine. So we're going to keep that in mind. Now as you learn from the circular orbit problem of Unit 1, if we consider any point along the circumference of the circle, then we can define an angle that corresponds to that point. These are coming from right here as a zero radian mark. You can write the coordinates of this point then as the radius of the circle times the cosine of the angle. That's for the horizontal component and for the vertical component, we get the radius times the sine of the angle. In the phase based plot that I showed you in the InterVideo, we saw that position lying along the horizontal axis and velocity lying along the vertical axis. So we wanted to plot the coordinates of the points on that green circle--the initial condition circle where the position is going to correspond to cosine and the velocity is going to use sine. Now I created a variable called phi. You could pick any name you want I guess. And phi effectively split the circle into 49 segments by marking out 50 different points along the circumference. So every time I increases by one, we're going to step to the next point along the circumference. Since as we saw in the phase base plot, we have a complete circle of green points. The x values of those green points vary like this with 2 as the middle value and the v coordinates vary like that. You noticed that the amplitude in either case corresponds to the half link of the green shape in that direction. So actually we have in a phase base plot is an ellipse for that set of initial conditions. Now that we have our starting additions figured out, we can finally use the symplectic Euler method to proximate the values with x and v at later sets. This code right here is just a direct transition pretty much of the equations that I showed you earlier. Now let's go back to looking at the plot that we get things plugged in but first let's look at our top two plots. The horizontal axis in both of them represents time measured in seconds. The vertical axis in the top one is x measured in meters and here it is v measured in meters per second. So you can see that our initial values of x go from 1.5 to 2.25 and v from -2 to 2. So that corresponds to this green ellipse right here. The most important thing to notice about this bottom graph, which like I said earlier represents phase base is that if we look closely at each one of these ellipses they all have the same area. Now let's look at the shapes that we have down here in this bottom graph. If you look closely and do a bit of calculating, you'll notice that all these different color shapes have the same exact area even though they are well shaped very differently. This is a great example of how phase base is conserve in the system where energy is conserve. Now the fact that its conservation principle holds in this diagram shows how the symplectic Euler method improve upon the accuracy of the forward Euler method. When the forward Euler method is used, it often result in the energy suddenly increasing. So it means that the area of each of these shapes down here will get progressively bigger. Symplectic Euler method, however, confirms much better the equations of motion in physics, It never reflects exactly radical predictions more accurately. Great job with the first problem in Unit 2. For this problem, we're about the claim the spaceships. Actually, we're dealing with the Apollo 13 mission. We should go from 1970. They were supposed to take astronauts from the earth and land them on the moon. However, at some point during the trip, they encountered a problem with one of their oxygen tanks and they had to abort their mission of landing on the moon and instead figure out how to get the crew back to the earth safely. Well in this problem, we're going to show how changing the velocity of the ship using boost rockets can change the trajectory of the ship to make it loop around the earth instead of landing on the moon and bring it back to where it started on the earth. This is definitely the most complex part that we've given you so far. So don't worry if it seems a little bit complicated at first. I'm going to help you out by continuing to chunk and giving you some hints. As always, we told you some important information to start up with. We have some information about the earth and gravity, and also significant information about the moon including the period of the moon or the time that it takes the moon to orbit around the earth. Well, once you have unique value that you see here is no initial angle and this is just the angle at which the moon is initially placed from the earth. This is a zero degree or 0 radian line and the angle between this line and the position vector of the moon is equal to moon initial angle. Let's have the total duration of the flight that we're considering in the times measured in meters which has resulted to local truncation air that we're going to allow the flight path to have. Well, Jorn mentioned this brief in the unit, and I'll give you little bit more explanation about it in a minute. We've laid out five tasks for you to complete to make it fundamental as a cohesive halt. The first one, which you can see right here, is to find the moon's position vector as a function of time. And here we're going to consider the earth is the origin here, as the position vector of the moon is just the vector from the earth to the moon. Now remember this is dependent on time. You can also say that the moon is going to move in counter clockwise around the earth or at least that's how it looks in this picture. Then the next thing that you need to figure out is how to calculate this spaceship's acceleration to the gravity. Well the input here is time and position. The position right here is not the same as position that you just computed in the task above. Instead, this position is a position of the spacecraft and not of the moon. So the second task is going to define the position of the spaceship depending on time and then you need to figure out how the gravitational attraction to the earth and to the moon affected deceleration. You think about how you can use vectors to do this and also as I said earlier, pretend that the earth is not the origin. Well, each of the first two tasks notice what your return values are and make sure that you define variables with the event in their proper places and it's function. So now you come to the slightly trickier part. In order to adjust the spaceship's trajectory to make it go around the moon, and then back in the right spot on the earth. It would be helpful if the crew had some control over it's velocity during the flight, lucky for them and lucky for us too. Those spaceship has rocket mechanisms that will allow to reduce it's speed on the way to the moon and increase it's speed on the way back to the earth. And we applied boost function right here. We've created two variant variables for you. And now I'm going to help you program some of these changes in velocity. This first one first two mid course correction or MCC and this task is whether or not the initial deceleration has occurred. Right now, it's set to false since at the beginning of the flight, this hasn't happened yet. Similarly, we have a second variable which at first does descent propulsion system or TPS. And this represents a second change in velocity. And the amount of which we're going to change the velocity at this point is what we're going to call the variable boost. Now, at these two moments, remember that we want to change just the management of the velocity. We don't want to mess with the direction at all. That would deal with itself. So, if your task is going to be to include these two velocities shifts and of course include them at the right times which I listed in the code. You can notice in the setup position of velocity vectors for you right here and plugged the initial values as well; however, it's up to you to figure out how to adjust their values with each step using Jorn's method and adapt a step size. You're going to that down here in this while loop. Here's Jorn's for you just to help you out a little bit. Once you calculated position and velocity using Heun's method. We also need to adjust the step size of the end of the while loop to reset it was a new value for the next time that we go to the loop. Now, just as we did in section 12 of the unit, we're going to store whatever the present step size is as h. And whatever the step size that would be in the next step of h new. To figure out the relationship between h and h new, you need to calculate the local truncation error or LTE. We also use basically the error that is made in a single time step. The difference after each time step between exact quantity and the quantity estimated by the approximation method that we're using. So new step size, h new is going to depend on the current step size, the tolerance, which we gave you earlier and the LTE. So consider calculating a local truncation error takes into account the difference between the values of position calculated by Euler and by Heun and the difference in values of velocity. T right here is the total duration of the flight so far. This needs to be here to let all of the units line up and LTE to be measured in length. Once you filled everything in, we're going back to the top of the applied boost function and figure out what the proper value of the constant boost is. Remember that our goal is to get the spaceship all the way around the moon and then have it just as it touches the earth when it comes back. Once you filled in everything down here, go back to the top of the applied boost function and figure out what the proper value for the constant boost is. We've given you just an initial value of zero, but we've also given you some other values to try out. So try plugging each these in and see what happens. Remember that our goal is put the spaceship all the way around the moon and then come back the earth and just touch the place where it initially took off. And because of the total time that we set at the top of our code, the shoot trajectory is not actually going to stop in the plot right initially when the ship gets to the earth. Instead, it's going to continue out in the earth as the spaceship is going to go back to the moon the second time. And the reason we did this was to try to make it a little bit easier for you to tell which value of the boost you should use. The correct value of boost will have the tail of it's trajectory reaching just back to the radius of the moon's orbit. Incorrect values of these could make the trajectory end short, go off in the wrong direction, but even if the spaceship crash into the earth which we definitely don't want to happen. And I'm sure the crew doesn't either. Well, now that this is a complex problem, but I'm sure you'll be able to return the crew safely home. If you need any help, just come to the front Since you have so many different tasks to complete in this problem, let's go through them one by one. First things first the moon position function. Now we ask to start the moon off at moon initial angle. So we define moon angle or the angle of the moon at a given time as moon initial angle plus 2π times the time that has past divided by the total period of the moon. So this is basically the fraction of its orbit that the moon has gone through so far. We then created a 2-dimensional array for the moon's position but looks like the horizontal position in this drawing is equal to the distance between the earth and the moon or moon distance times the cosine of moon angle and the vertical position is the same except use a sine instead. So next comes the function of the acceleration of the spacecraft. Now let's talk about gravity for a second--we know that the gravitational force that one body feels as a result of another body is dictated by Newton's law of universal gravitation. We know that our spaceship is going to be pulled towards the earth and also towards the moon. So we need to take into account how the acceleration would be affected by both of these objects. Since we're interested in acceleration rather than force, we can use Newton's second law along this law of gravitation to come up with an expression for the total acceleration of the spaceship. To figure this out, however, we need information about the relative position of our three objects. So we started in the acceleration function by declaring a new variable called moon pause, which is just equal to the moon's position at any given time. Now we can define a vector from the moon to the spaceship, this green vector right here, as just the position vector of the spacecraft minus moon_pos. Since moon_pos is just the position vector of the moon. Then we can define a new variable called acc or at least that's what I'm going to pronounce this word. Using the equation of the acceleration due to gravity. We really owe Isaac Newton a lot of credit in this problem consider we're using two of his different laws. So you can see modifying his law of gravitation using the second law give us an expression for the vector sum of the total acceleration of the spacecraft. Here everything with a subscript as it stands for spacecraft. Now our next task was to make the velocity changes happen. If the first shift in velocity has not happened yet but it's after the time at which it was suppose to happen, then we decreased the velocity by 7.04 m/s but keep the spaceship moving in the same direction by dividing our quantity by the norm of the velocity vector and then multiplying it by this vector again. We do pretty much the same thing for boost but this time we replaced the time with the correct value 212,100 seconds. In each case since you've changed the velocity in the way it's prescribed, you can change the proper booleans to true. Now finally we get expressions for velocity and position using Heun's method. Right here we have a simple translation of his equations into Python. You can see Euler values with the e's and Heun's values with the h's . At the end of this, we set velocity and position equal to velocity h and position h. So that each time we go through another iteration, they will shift to equal the new Heun's values. Now in order to adjust the step size for each position of the spacecraft, we need to know the difference between Heun's and Euler values for position and velocity. You'll remember that we use this to calculate the error. Then the new step size is set as prescribed here in this line of code. Since we're up finishing up the step in which h, the current step size is valid, we add this to the current time and now that the next time step is about to happen, we set h equal to h new--updating its value. So this explains why we use h in the equations for Heun's method but are still able to lock the value cycle. Since our program is now complete, we can run it using different values for boost. We're getting zero as a value to start out with but let's try -10 first so we can check out one extreme of the range that we gave you. But a boost of -10 m/s, we can tell that the ship is pretty far away from touching the earth and then when we extend the path out again, it travels past the moon's orbit. We can clearly tell that this is not right. We definitely want to make sure that the crew actually gets back to the earth not just circles around again. Let's try a different value. Now since 100 is the largest value that we gave you to try, let's see what happens with that. Well that certainly looks different from the last plot that we saw. Though, this also doesn't look exactly right. So up close we can tell that it looks like the ship is actually crashing into the earth and that's definitely not what we want to happen either. We wanted to just barely scan the surface. Zooming out again, you can see that the tail of its trajectory doesn't reach out to the radius of the moon's orbit like we said that it would. So the correct value for boost is in between these two values at 10 m/s. Here is the solution plot. This looks perfect. You can see that the spaceship takes off, loops around the moon, comes back and just cruise the surface of the earth, and then, if we let it go out again, it will return to exactly the same radius that it left to initially. This is a really complex problem so great job for getting it done. The fact that we're done talking about spaceships and orbits, I get excited for Unit 3. We're going to talk about how diseases spread to the populations. In this problem, you're going to use the SIR model to look at a situation in which people's immunity doesn't last. In other words, after someone who has been infected moves to the recovered population. Here, she can become susceptible again. We're going to call this waning immunity and we'll set the constant over here waning time to equal the amount of time that it takes for a person's immunity to fade away. Most of the information we've given you in the supplied code you've seen before, but there are couple of things that are particular to this unit. You'll see another transmission coefficient, which is a measure of how frequently each day, each infectious person might infect someone else. You could also think of it as the likelihood that an interaction between an infected person and a susceptible person went up with the susceptible person becoming infected. Just make sure that you know that its unit is 1 per person per day and the first thing that we want you to do is to define waning time right here. So eventually we have a system in stead state where the number of recovered people is equal to 2 times the number of infected people. Think about what this means in terms of the time that people should spend infected versus the time that they should spend recovering. Just for reference, I've include the equations for the standard SIR model right here. Remember that this equation don't include waning immunity. First think about how you need to change them in order to reflect the situation at hand. Once you figure that out, you'll need to translate your mathematical expressions into code. We've already created list for S, I, and R for you. And we set the initial values for susceptible, infected, and recovered populations right here. Next comes a for loop, part of which you'll have to fill in. We've given you S to I, right here, which is a number of people moving from susceptible to infected during one-time step. We've also given you I to R, which is a number of people going from infected to recovered in a given time step. What you need to do is to define R to S. The number of people who in one-time step become susceptible again after they've recovered. Just to be clear, all three of these are measured in number of people, not in number of people per time. Make sure that you check that your units are correct. Your last task is to modify the recursive functions right here for S, I, and R. Now, let's look at the solution for a problem concerning waning immunity. First things first, the constant waning time should be defined as 2 times the infectious time. We can show this mathematically using the equations we have for the derivatives of S, I, and R with their respective time. We know that after a long period of time you want to attain a steady state situation and there's a number of people in each portion to the population-- susceptible, infected, and recover stays constant. Since they want to find out how long people should spend in the recovered stage, we start with the time derivative of R and set that equal to zero. Since you know that Rdot now has an extra term added to it or actually subtracted from it showing the number of people that are leaving the recovered population and going back to the infected population. We can set these to terms right here equal to zero as well. Just to note, I've used CINF to stand for the infectious time and CWAN to stand for the waning time. Now with just a little bit of Algebra, we come up with the answer that R=2I. Since we know that we want the number of recovered people to be twice the number of infected people, we can plug in this extra information to the equation above and end up with the answer that the waning time is equal to twice the infectious time. This put us directly into the next part of the problem. We defined R to S in the same way that I to R is defined except that we replace infectious time as the waning time and I step with R step as you can see right here. Then moving on to our recursive relations, for each element in a step plus one position, we need to take into account the value of the previous element in the number of people added and subtracted from the population during each time step. We know that the one thing that has changed in this model from the standard SIR model is that people are now moving from the recovered population back to the susceptible population. This is why we've needed to add in this extra term R to S, which we subtract from R and add to S. Now, let's run the program and see what we get. Here we see we end up with this fancy graph, which has three different series for the three different parts of the population we're looking at. Remembering how we set the initial values for S, I, and R. Remember that the blue line here stands for the susceptible population. The green line stands for the infected population, and the red line stands for the recovered population. And the maximum time that we're looking at 60 days, you can see that the red line is graphing twice as many people over here as the green line is marking, which is exactly the answer that we wanted to end up with. Congratulations on successfully completing the first problem of Unit 3. Welcome to the second problem of Unit 3. If you thought that the whole situation has started to look grim in the last problem when we dealt with waning immunity, this time circumstances had become even more dire. There's no immunity at all. And so with the typical SIR model, people simply switch back and forth between susceptible and infected. We're assuming that more people are lost in the scenario. So that means that the total number of people which I'm going to call N stays constant. Since every person is either in the S group or the I group, we know that in any given moment S+I=N. Here once again are the standard SIR model equations. You'll need to change this which reflect the new situation similar to what you have to do in the last problem. Start by reducing this system at differential equations to a single equation that contains only I and N but not S. Then solve this equation numerically using the trapezoidal method, which in case you forgot I will show you right now. Remember that the trapezoidal rule finds the value of a quantity after an additional time step by taking their original quantity and adding to it h times the average of its rate of change at the two points in consideration. Now use notation here that probably make sense but just to be clear x sub-n is just equal to x measured at the n time step and x do- n is just equal to the time derivative of x at the n time step. Now once you go through using the trapezoidal method, I want you to solve for S and then convert your equations for I and S into Python. Right now the supplied code contains expressions for S and I that come from the forward Euler method. Before you fill in the program with the trapezoidal rule run it as it is right now. Try it first with h set to 10 as it is here and see what happens then change h to equal 0.5, run the program, and compare the difference between the plots. When you change the code as we've asked you too, see how each of these step sizes works with the trapezoidal method instead. After you're done with everything, set h to 10 and press submit. Now for the solution to the second problem of Unit 3, let's start it by adding in a constraint to the system with this line right here about n. This shows, I've explained earlier, that the total population n remains constant in all times. The people just switching between the S and I groups. Now let's move on to the core of the problem. Switching the contents to the for loop to use the trapezoidal rule instead of the forward Euler method. We begin with the standard expressions for Ṡ and İ, but this time showing that people can move from the infected population back to the susceptible population directly. Just for simplicity, I'm going to call our constants a and b right now and I'll change them later to show what's actually in the code. Now we can get a single equation that encapsulates the information in all three of these equations if we just substitute N-I for S in the equation for İ. This is what this line right here shows. If we do a little bit of rearranging and simplifying, we get a quadratic equation for I-dot. Now this make things a lot simpler altogether because now we have one equation and one independent variable instead of three equations in two independent variables. Now it's time to move on and use the trapezoidal rule. You'll notice here that I'm using a t in the subscripts instead of an n this time. That's just so that we don't get confused between big N's and the little n's. So we know that for the t+1 time step, I is going to equal this expression just in the trapezoidal rule. However, we just created an equation for I-dot that depends only on I. So we can plug that expression in to get this expanded equation right here. Since many of the terms in this expression have factors of h/2 and a outside of them, I'm just going to pull out a factor of ha/2. With a little bit of rearranging, we come up with this long expression. Now we can think of this equation as made up of two parts-- one part that involves I sub-t+1 and another part that just involves I sub-t. Since we want to think of I t+1 as the only independent variable, we can lump everything else under other variable names. Looking at the equation, we have one term with (I subt+1)² . A couple of terms with I t+1¹ and several terms that don't have I t+1 at all and we're going to call everything that's in front of I t+1¹p. And we're going to call everything that doesn't involve at all q and this is going to give us the simplified quadratic expression that shows very clearly how this is a quadratic equation using I sub-t+1. Using the quadratic formula and rearranging a little bit, we get this expression right here using p and q to express I sub-t+1. Now this actually has two possible solutions-- one with a plus sign here and then one with the minus sign here. So how do we decide which one to pick? Well we know that this h to times that goes to zero. We want I sub-t+1 and I sub-t to get closer and closer together since the curve is going to become smoother and smoother. Since p has a term of 1/h inside of it, this means that p is going to infinity as this happens. If we pick the minus sign over here, they're going to have minus infinity, which means that I-sub-t + 1 is definitely going to go to negative infinity and this is definitely not what we want. So just to be safe, we will pick the plus sign here. Now we return to the code where we've translated our expressions for p and q into Python. To do this, we have to remember that the coefficients a and b that we were using when we're writing things by hand actually each have different names in the code. A is equal to a transmission coefficient and b is equal to one over the infectious time. Using this information, you've just done a simple translation of p and q into Python. This makes writing out the expressions for I step+1 and S step+1 very easy. For I step+1, we just write in the solution that we got from using the quadratic formula using of course the plus sign right here in the variables p and q since they are just written right here anyway. Because we know that I plus S is always equal to n, our expression for S is simply equal to n-I at the time step. Now, let's run this first with step size 0.5 and see what happens. This looks very nice. We have two really smooth curves with the infectious population is green and the susceptible population is blue just like in the last problem. We see that since each curve in the plot approaches a constant value at long times we're approaching a steady state situation. Now let's run the program instead with step size 10. So this is what we get from our model if we're using h=10 instead of h=0.5. Even though now that the curves is particularly smooth, we can see that they obey the same overall pattern as we saw in our graph using the smaller step size. The most important thing is that we still are approaching a stead state situation at the end of the graph. Now think back to what happen when you run the supplied code like we ask you to. When you run the supplied code with h=0.5, you should have gotten a graph that looks like this. This definitely looks a lot like what we saw when we run the solution code using h=0.5 as well. However, let's peek at what happen when you use h=10 using the forward Euler method. You can see that there are a lot of differences between this and the graph that we just saw and also between this and the graph with the solution code when we had h=10. The most important thing to notice though is what's happening over here in this region. You can see that instead of each curve approaching one set value as time goes on they keep crossing paths at each other. In other words, since I and S here don't obey nearly the same trend that they do with smaller step sizes, this shows that the forward Euler method doesn't allow for stability with large step sizes nearly as well as implicit methods like the trapezoidal method do. Hope that was an interesting investigation with the difference between implicit methods and other kinds of methods and get ready for the last problem of Unit 3. Last problem of Unit 3, we find a population faced with a malaria epidemic which is definitely very scary for everyone involved. Luckily, we're here to help them out. In order to slow the spread of the disease, we're going to make people start using mosquito nets in their houses, so they can avoid being bitten as often. We're going to introduce the nets after 100 days and they immediately decrease the probability of a person getting bitten by 90% as we can see from the constant that we've defined right here called bite reduction by net. Bite per day in mosquito tells us that every person gets bitten by each mosquito, 0.1*per day. Now when a mosquito bites a human, there's a chance of two different things happening that are a particular interest to us. Now, if a mosquito has malaria, it can give a disease to human or if a human has malaria, then he or she can give it to the mosquito. The probability of either of these things happening is shown right here. Once infected, it takes a human approximately 70 days to recover. At this point, it's important to notice that mosquitoes only live for 10 days. Clearly, humans also have a finite lifetime but compared to mosquitoes, we might as well be immortal even if we do end up with malaria. Sets that we're going to pretend for the purposes of this problem, that we can model a scenario like this, we need two separate bubble diagrams-- one for the mosquito population and one of the human population. The mosquito life cycle is kind of a sad one--as soon as they are born, they fall into the S part of the population, the susceptible part. From here, they can become infected and then eventually they die. There's no option for them to become susceptible again. A couple of other things to keep in mind about mosquitoes. For one thing, they can't get malaria until after they're born. Second, only female mosquitoes can transfer malaria. So just pretend that all the mosquitoes that we're talking about in this problem are female. Now this problem requires you to take into account all the factors affecting the infected population and to include them using the forward Euler method. Think critically, keep track of your units, and good luck. If you have any trouble at all, visit the forums. The first thing that we need to deal in this problem is the issue of the mosquito nets. Well we know that at first there are no mosquito nets in place at all, so we're going to create this variable called net factor and initialize it to 1. We know that after 100 days, net factor is reduced by bite reduction by net, this constant that we created up here. We add in this if statement right here saying that after 100 days, net factor is reduced by 0.9 which is bite reduction by net. Remember that this is basically the probability that a person is going to be bitten. It makes sense that if you add in the mosquito net, the probability of you being bitten will go down. Now for implementing the forward Euler method with the infected population, both human and mosquito. For the number of infected humans at the next time step, we of course, need to start with the number of infected humans at the previous step. To this, we're going to add on the number of people that are being added to the infected population. We are going to subtract the number of people that are moving out of the infected into the susceptible population. Let's first think about the term that's being added to the number of infected humans. We take the probability of a person being bitten which is in that factor times the number of bites per day and per mosquito that a person gets also multiply that by the number of infected mosquitoes and then we multiply this by the fraction of the total human population that is still susceptible also multiply this by the probability of a mosquito transmitting malaria to a human. On the other side of the coin, we have factors that move people away from the infected population and back into a susceptible population. This is a very simple term. We simply take the number of people that were infected at the previous time step and divide this by the time that it takes a person to recover. Remember that all of that was multiplied by age and added to the number of people infected at the previous time step. Now, let's take a look at how the infected mosquito population was changing. Since we're using the forward Euler method, we started with the number of infected mosquitoes at the previous step and then add to this age times a bunch of other things. Now to find out how many mosquitoes are moving into the infected group, we start with the probability of a mosquito biting a person, which of course might give the mosquito malaria as well, multiply this again by the bites per day of mosquito, multiply this by the number of susceptible mosquitoes and the number of infected humans. We then divide this by the total number of humans and multiply the transmission probability from human to mosquito. If you remember the drawing that I showed earlier on in the video introducing this problem, I told you that the only way the mosquitoes can no longer be infected is to die. They don't move back to being susceptible. This means that the only thing that is going to decrease the number of infected mosquitoes is death of mosquitoes. So, we subtract from this entire expression if 1 over the mosquito lifetime times another infected mosquitoes at the previous step. I know that that was a lot of constants and variables to keep track of, but great job if you're able to fill all this up. Now, let's take a peek at what our final graph for Unit 3 looks like. Now in this graph, the blue curve shows the fraction of humans that were infected and the green curve shows the fraction of the number of mosquitoes that are infected. At first glance, it might look like there are many many more humans infected the mosquitoes. But remember that because this is fraction, this doesn't actually show us plain numbers. In fact, we started with 10¹⁰ mosquitoes, but only 10⁸ humans. So even if a much smaller percent of the mosquito population was infected from the human population, the mosquitoes are able to infect such a great portion of humans because there are so many more mosquitoes than there are humans to it. You can see that the number of both humans and mosquitoes infected, at first, increases sharply and then levels off and stays at a very high number. However, as soon as 100 days hits, and we introduce the mosquito nets., the number of infected in each population starts to decrease certainly. We can see that a mosquito net idea was definitely very effective for helping slow the spread of malaria in the human population as well as the mosquito population. Great job on all three problems in Unit 3. I hope you found this discussion of the SIR model and its variations interesting. Welcome to the first problem of Unit 4. In this situation, we're going to pretend that we are fishermen. And we're interested in catching this red type of fish right here. However, whenever we try to catch the red fish we also end up catching a bunch of these smaller green fish, as you can see from the contents of the net right here. What we're going to do in this problem is model logistic growth for both species of fish-- the red and the green with the constants and initial values that we've already included in the supplied code. Just a reference--I included a general equation from logistic growth right here. F is the population that we're looking at, r is the growth rate of the population, and c is the carrying capacity of the surrounding environment. Now, remember when we model the change in population for both types of fish, we need to include both population growth, which is dictated by this equation, but also the harvesting rate. We've created this parameter called p to represent the fraction of the total fish that we catch that are part of the green fish population, so we might also call this the fraction of the by-catch. Just to start, we've inserted a value of 0.5 for p. What you're going to do is calculate the threshold value of p for fish 2 to not go extinct. In other words, what is the highest possible value of p that will allow for equilibrium situation in the population of fish 2? After you've calculated the correct value of p, come down to the for loop and use, as we've done so many times before, the forward Euler method. You'll notice that we've already included a line for you to prevent the population of either type of fish from becoming negative--good luck. Now let's look at the solution to the first problem. We know that we already used the forward Euler method for both the populations of fish one and fish two. We need to know how each population changes with time. I've written out expressions for ḟ₁ and ḟ₂ showing the time derivatives of each population. Now on each equation, we can see that this first term is in the form of the logistic growth function that I showed you in the question introduction video. We have a growth rate and then 1 minus the population over the carrying capacity times the population again and the same thing up here. These first terms show the positive rate of change of the population. However, each equation also has a second part tacked onto it that shows a negative rate of change. This expresses how each population is harvested by the fisherman. As I said earlier, p is the fraction of the fish that we catch that are fish type two or the green fish. We know then that the rate of harvesting of the green type of fish is going to be p times our total harvest rate. We can maintain the same total harvest rate if we make the coefficient in front of the harvest rate a new equation for the red fish 1-p. Remember we're trying to prevent the green fish from going extinct. This means that we need to create a situation in which the rate of change of population two is greater than or equal to zero. If we rearrange the equation for ḟ2, which should adapt with this term, which is showing how the population can gain fish, should be greater than or equal to this term, which is showing how the population can lose fish. The value of p that we're trying to calculate, however, is the exact threshold of extinction. This is going to happen where growth exactly equals harvesting. In other words if there are anymore harvesting or any less growth, population two will become extinct eventually. This graph over here like you've seen before has a blue pair of fish that shows how the growth rate changes in population, and it also has a red curve that represents harvesting rate. This first red line is just an example of a harvest rate, but the one that we're interested in is this dotted red line up here, which just touches the maximum of the growth curve. You might remember that this point on the growth curve is also known as the maximum sustainable yield. You can see that if we move the red line any higher, then the harvesting rate would always succeed the growth rate and we would definitely end up with an extinct fish two population. Similarly, if we move anywhere else on the blue line, the growth rate would again be below this harvest rate and we will begin to deplete the population of fish two. Right in the center, however, we have a point of semi-stable equilibrium just like we're going to talk about in the Unit. Any tiny bit that the population moves to the left side of the curve will result in it eventually falling all the way down to zero, which clearly means that fish two has become extinct. Instead, we have slightly larger population than 5x10⁵ tons, we will again start to see a decrease in population, and if by chance the population has not stopped at exactly this maximum point--it will fall to the other side of the curve and also become extinct. Remember all this is assuming that the harvest rate is exactly at this dotted line. Now we need to find what value of p would place the dotted line right at this position. Again at this point we have growth rate exactly equal to harvesting rate. I've taken the two parts of the equation for ḟ2 and written them as equal to each other right here. We plug in the proper value for f2, which we know is 5x10⁵ ton and we can solve using simple algebra for the threshold value of p, which equals to 0.375. Finally looking at our code, we can see that first we've plugged in the value for p that we just calculated, 0.375, and down here in the for loop, we have a very simple translation of the equations that we created for the growth rates of both fish one and fish two in this box for the rate of change of either population. So for fish one we have this and for fish two we have this. As always, these rates of change are multiplied by h, the step size, and added to the initial values for either fish population. Now it's our new program and see what we get. Here is our plots, the blue line plots the population fish that we are interested in harvesting measured in tons and the green line plots the population of the second type of fish, which we're trying to prevent from going extinct. You could see that we have maintained a high level of population of the fish that we're interested in and we've also prevented the green line from going down to zero. You can see that as time increases, the slope of this line is approaching zero, which means that if we plotted time going even farther out, no matter how far out in time we decided to to plot these curves, this green line would never approach zero. Just for fun, let's see what happens if we set p=0.5. Well this graph looks very different from the last one that we had. By just increasing p by the third of its former value, we've made fish population two go extinct in just over 20 years. Since we're harvesting more of fish two and less of fish one, the fish one population has actually risen to a value that is higher than it was in the last part, but we've violated our first main objective, which was to prevent fish two from going extinct. I hope this problem has helped you submit your understanding of logistic growth, harvesting, and equilibria. Now we’re going to look at a very simplistic model of a food chain. It’s so simple that we’re only going to consider two species of fish. One is the predator and one is its prey. You are going to model the change of either population the predator and the prey using differential equations in the form of these equations right here. It’s up to you to figure out, however, which of these corresponds to the predator and which one to the prey. There is several other features of this situation that must be taken into account. The prey multiplies at a certain rate. The prey is consumed by the predator that’s kind of a given. The predators, however, only die of old age and the predator’s population also multiplies depending on the amount of prey that is present. Now in these equations A, B, C, and D are constants. And in our program over here, you are going to fill in their values in this dictionary called base values. We’ve already provided you with the values corresponding to two keys in base values. We’ve included the initial amount of prey and the initial amount of the predator population measured in tons. Here however is some other important information that you’ll need to know in order to do this problem. The rate of growth of the prey is 0.5 per year and the life span of the predator is five year. In addition, we will reach an equilibrium situation or a steady state situation with the amount of prey reaches five times and the amount of predator reaches one times. You are going to use this information, a little bit of logic and these equations to calculate A, B, C, and D. Once you enter A, B, C, and D into base values, we’ll move down to look at this function called food chain. Here you’ll use the familiar forward method to calculate the future populations of the prey and the predator at each time step. Lastly, you’ll do a bit of sensitivity analysis. We are going to add in a degree of uncertainty to this problem to make it a little bit more complex. Let’s say that A, B, C, and D as well as the initial amounts of both fish are uncertain by plus or minus 10%. Similar to your earlier quiz you are going to create dictionaries for the minimum and maximum values taking this uncertainty into account and then determine which of these six parameters A, B, C, and D and the two initial values has the largest impact on the maximum amount of prey. Again, office is very similar to the quiz insensitivity analysis that you saw on the unit itself. So if you’re having any trouble, you might want to review that quiz and of course you can always come to the forums for more help. Good luck. Looking at the two model equations we gave you, you can figure out which one belongs to prey population and which one to the predator population by considering the signs of the various constants in the information that we gave you. We told you that the prey grows at constant rate of 0.5 per year. So since population growth is, of course, positive it depends on the population already present, we can see that this term Ax(t) should be part of the equation dealing with the prey. Similarly, we told you that the predators die out. Similarly, we told you that the predators have a fixed lifespan of five years. This means that the rate of change of the predator population while the negative component that is also directly proportional to the total population at a given time. This matches this term right here, -Cy(t). Now that we know that x represents predator and y represents prey, Let's talk about these terms that includes an x*y factor. This factor of x*y represents predator times prey and this proportional to basically how often individuals of the two species meet. We know that whenever a predator meets its prey, the situation, as it has been, pretty advantageous for the predator but not so great for the prey. This was shown in our equations by the fact that this interaction turn in the equation for the rate of change of the predator population has a positive sign in front of it. Whenever the predator meets its prey, this population can grow, and opposite is true for the prey population since this interaction will lead to members of the prey dying. Determining these constants A, B, C, and D should actually remind you of it of dealing with our SIR model problems in Unit 3. Our coefficients dealing with lifespan and growth rate corresponds to the different time constants that we use from moving people and mosquitoes to the infected, recovered, and susceptible populations. Since we've already matched with the growth rate of the prey population, we can write down A=0.5/year In the same way, since C corresponds to loss in the predator population, we can set its value equal to 1 over the predator lifespan or 1/5 years. In order to calculate B and D, the other two constants, we need to make use of the last piece of information that I gave you. That our ocean populations reach an equilibrium situation then there are 5.010⁶ tons of prey and 1.010⁶ tons of predator. We do some simple algebraic manipulations of the equations that I showed you above and also plug in 0 with the rates of change divided by population showing equilibrium situation. We can solve for B and D by simply inputting the values for A and C that we just decided upon. This results in B=5.010⁻⁷, 1/yearton. D=4.010⁻⁸, 1/yearton. We plugged all four of these values into base values. We can move on to the next part of the problem. In the function food chain, we implement the forward Euler method using the rates of change that we just figured out using our differential equations. Of course, you've plugged in prey in the place of x and predator in the place of y. And finally, we move on to our sensitivity analysis. To account for the lower limit values, we make a copy of the base values dictionary and multiply each value corresponding to each key by 0.9 and we do the same thing for the upper limit by multiplying each value by 1.1. These lines down here select the most critical parameter based on which parameter has the greatest difference between its high results and its low results. However, we can also figure out the most critical parameter by just looking at the plots that we get when we run the program. Our first plot shows the amount of prey in tons versus time and the second shows the amount of predator in tons versus time. The colors dictionary which you may have seen throughout the top of the code paired each of the code parameter that we're looking at with a different color. Remember, we're interested in seeing which of these parameters has the greatest impact on the maximum value of the prey? So in order to pick out the most critical parameter, we need to compare the maximum amount of prey graphed by the upper and lower curves of either series in this top graph. You can see for example that if we look at the yellow curves, their maximum values here and here are not really that far apart. If we look at every pair of curves that are the same color, we can see that this one that is cyan or the light blue has the greatest distance between its peaks. Looking back at our colors dictionary, we see that cyan or C is paired with D. Let's look at our final graph again for just a little bit more analysis. We can see that both graphs exhibit periodic behavior. It looks almost like we have a Cos and Sin function. Initially, our prey population increases which leads to an increase in the predator population shortly afterward. This however leads to a decrease in the prey population and so on and so forth back and forth. Upon closer inspection, you can see that these are actually not perfect Sin and Cos functions. In fact, the Euler method leads to a blow-up of periodic functions overtime. We see a very clear example of this explosion of amplitude if we increase the end time and step size. Let's see what our plot looks like now. This is a very interesting-looking plot. We can see that the in the predator population, we start with a peak-to-peak height of approximately 60,000 tons and after 200 years have passed, we have a peak-to-peak height of approximately 150,000 tons. That's a pretty dramatic increase. If we use this implicit Euler method instead of the forward Euler method here, we could prevent this expansion and amplitude just as we talked about in our earlier problems involving mechanics. You can see that many methods prove useful in different situations whether they're dealing with diseases or fish populations or pendulum. Great job on Unit 4 and get excited for Unit 5. We're going to look at something totally different. How anti-lock brake systems work. For this problem, you're going to develop a semi-implicit solver to model just a single corner of the car and its motion. You saw the diagram that look pretty much just like this in the unit. Now all of these errors represent different types of quantities but they're just use for showing direction. V here represents the velocity of the car. W is the tangential velocity of the wheel. Mqc is the mass of one quarter of the car and therefore, mqc*g is equal to the gravitational force on this quarter of the car. F here represents friction or the force of friction that is acting in this direction against the motion of the car. The force of friction is a function of S, which stands for wheel slip. We can write out the equation for the force of friction just like this. The force of friction is equal to, as you've seen before, the coefficient of kinetic friction, which is the function of wheel slip, times the normal force, which in this case has a magnitude equal to that of the gravitational force. We can relate some other important quantities to the force of friction, and right here we can see that v-dot or the acceleration of the car is impacted definitely by the force of friction. Change in the velocity of the wheel depends both on the force of friction and on B, which is the braking acceleration. S here, as you remember, stands for wheel slip as I just said. We also have this expression from U, the coefficient of kinetic friction. Your first task in this problem is going to find an equation for W at the n+1 time step that fits this form right here. In this equation, C, D, F, and K all could depend on a number of different things. For example, it might depend on W sub n, on h the time step, or the mass at the corner of the car, on the affective mass of the wheel, mew right here, mg the acceleration due to gravity, on B the braking acceleration, or on v the velocity. For v which you see right here, we're going to use v-sub-n because as it turns out v changes very slowly compared to w. In fact, what we have is a stiff system of differential equations. You remember for our work earlier on in the course that stiff systems of differential equations require you to use a very small step size or else we'll end up with completely unreasonable results. If you want to test this out, try plugging in h=0.1 seconds for the step size and see what kind of craziness this is. In order to figure out what to plug in for C,D, F, and K here, we want you to use the backward Euler method along with the information that we've given you here. Looking at the code, you can see that we've given you a substantial portion of the program already. In fact, we've handed you a solver right here. Since we're asking you to use the backward Euler method, which is an implicit method, you're going to end up with an equation that is not linear and therefore, you can't solve explicitly. Once you've used the backward Euler method to come up with values for C, D, F, and K, we want you to use this solver in conjunction with the backward Euler method to come up with an expression for w[step+1]. In the solution video, I'll give you a more indepth explanation of what do solvers do and then how it works. In order to use the backward Euler method to come up with an expression for w at the n+1 step, we know that we're going to need an expression for w dot. So that's the first thing we deal with. Beginning with our equation for the force of friction. We know that the acceleration of the wheel depends on the force of friction divided by the mass of the wheel. So we plug in this expanded version of the equation for the force of friction into the equation for w dot and of course the track b. In the backward Euler method, it says that w at the n+1 step is equal to w at the n step plus the step size times the change in w at the n+1 step. Now that we have expression for w dot, we can plug into the proper location in the equation below. If rewritten as the wheel slip to see here and here as 1-w at the n+1 step over v, which of course is just an equivalent expression. This gets rid of any explicit dependents on this. Remember from what I said earlier that v here changes much more slowly than w. We then specify which v we're referring to right here. But if you were to be more clear, this is the subn+1. There's a little bit of rearranging we can rewrite this equation for w n+1 as this equation right here. The amazing slightly counter intuitive to rearrange things like this since this expression looks maybe longer and more awkward than this one does. Remember, however, that our initial request for w was that it obey an equation of the form that I wrote up here. We have a term with w subn+1 and then term with e to that value and also a constant. All just added together equal in 0. That's exactly what we have down here now. This makes our job very simple. Remember, we want to identify what c, d, f, and k are. And now, we can just read them directly off of this equation. In order to implement the backward Euler method and the code now, all we have to do is translate the variables c, d, f, and k into code just like I've done right here. The expression for w that the solver is going to output. is equivalent to the one that we just found using the backward Euler method. It's very simple since the solver has already established for this. All we have to do is fill in the proper values for these variables. In the introduction video, I didn't talk much about what the solver is actually doing. So now just take a second to explain that. It's implementing something called a Newton-Raphson method. This is a method used for solving implicit equations that are not linear. So it's a perfect fit for the situation that we are faced with right now. Let's say I have a function like this green curve right here for which I want to find the x value of a certain point. Maybe for example I'd like to find the x value of this x intercept right here. So the thing that we actually know about this point then is the y value, which is 0. To use in Newton-Raphson method, I start by making a guess at what the x value that appears to that y value is. Maybe I guess the x coordinate that corresponds this blue line right here. What the Newton-Raphson method does is it takes the slope of the line tangent to the green curve at this point. This may be that tangent line looks a little something like this and extends that line down to the y value that we're interested in. Now, the next x value that I will guess is going to be the point of that tangent line intersects the x axis in this case then I will do the same thing for this point and so on and so forth. And eventually, my results will converge to the actual point that I'm looking for. This is a very effective method for solving implicit equations. The general equation showing the movement from one guess to the next guess is shown right here. We can specify this to deal with the situation we're looking at by using this equation right here. And in fact, that's exactly what's written in the code in this expression for w_new. Now that we understand how are solvers working, let's look at the plot we get. What we have here is a series of plot that all depends on time. First, we have position then car velocity then wheel velocity and then wheel slip. The difference series shown by the different colors in each plot represent different values for the magnitude of the breaking acceleration. Fusing them really close in the different parts of the top graph, you'll be able to tell that 70 m/s² is the blue line; 100 m/s² is the green line; 130 the red line; 160 the cyan line; and 190 the magenta line. So we can see then how different braking accelerations affect these 4 different quantities. Interestingly enough, it's the 130 m to second braking, which is one of the middle values that makes the car stopped first. When we slam on the brakes really hard which is shown at the magenta and cyan lines, the wheel stopped rotating very quickly and see the wheel velocity was 0 right away. However, in tandem with that, the wheel slip increases dramatically, and the wheels are locked very early on. As a result, the car velocity does not actually slid down much. Of course, as we would expect, we just don't put the brakes on very hard like with the blue one then the car does go a pretty far distance, and it actually does not come to stop it all in the time that we've allowed; however, this is the great example of how slamming on your brakes is not the most effective braking method. Great job on this problem. In this problem for Unit 5, I want you to implement the control algorithm that you saw in sections 2 and 15 of Unit 5 for the hydraulic braking system. We've set up a number of important values for you, as always, including two things called low slip and high slip. These are both possible values for the real slip. If the real slip gets smaller than low slip, we want you to decrease the pressure of the brakes. Similarly, if the real slip gets higher than high slip, we wan you to increase the pressure. Increasing and decreasing the pressure like this leads to rate of change called hydraulic speed in the strength p of the brake. Hydraulic speed is shown right here while p the braking strength which you can see right here is measured as the deceleration of the velocity at the rim of the wheel if there is no friction from the road. Possibly logical, the strength of the brake cannot be a negative number and we've set the maximum value that it can take on with this constant called max brake right here. You're going to need most of the contents of the for loop inside this function, but there are a couple of line left that we want you to write. We want you to model how the brake pressure changes between these maximum and minimum values that we outlined above, and using these, how the brake acceleration changes step by step. This might require a little bit of creativity, so if you need some hints, come to the forums. The first thing we did in order to model the zigzagging oscillation of the brake pressure was to create this perimeter called brake change. Just for simplicity,we set the initial value of brake change to zero. If you move down to the far loop, you can see that brake change alternates between having a value of 1 and -1. If the wheel slip falls below the constant low slip, then the brake change value switches to 1. And as soon as the wheel slip passes high slip, they change, switches signs, and becomes -1. An expression for b after the next times step We've ensure that b will not drop below zero by setting zero as the minimum value that it can take on. Then using this min method right here, we are assuring that its magnitude will not exceed max brake. However, anywhere in between zero and max brake, it will take on this value right here. We take the value of b at the previous step and add it to the step size times brake change, which is either you're going to make this a positive or negative quantity and multiply that by the hydraulics speed. Remember that hydraulic speed is just the rate of change of the strength of the brake. To visualize what we've done, let's look at our plots. This bottom graph shows how the brake strength changes with time. Just as we hoped, we see that our line either has a negative slope or a positive slope, but of the same magnitude just alternating signs. These changes in sign correspond to the flip in the sign of brake change. The crest of the brake strength curve are points at which brake change switches from being positive to being negative. We see that the brake change switches to -1 if the wheel slip exceeds the value of high slip and if you look at the corresponding values in wheel slip, you see that these points of transition in brake strength corresponds to the highest values in wheel slip and the lowest value in wheel slip which we can see signals a switch in brake change to +1 correspond to a change in the sign of the slope of brake strength from negative to positive. Zooming out and looking at these plots as a whole, they say that the wheel slip is controlled very nicely but the value of b does oscillate very strongly and this would look pretty stressful for the brake mechanism. What we would want instead would be a value of b that will be close to the correct value. One option for this would be to limit the range of b by holding the pressure constant or keeping the value of b constant then b gets either too large or too small. Let's try that out by making a small change in our code for just a second I'm going to comment out this line and instead infinite this line. Our original line of code kept the value of b between 0 and max brake. We are now going to replace those respectively with 100 and 150 so b is going to be within a much smaller range of values. Now, let's see what happens with this change. The first thing that I noticed when I looked at this is that as you would expect the graph of the brake strength, it doesn't look like it will put much strain and stress in the braking system as our previous system did. The wheel slip also still stays within a very nice range and their car does come to a complete stop. Despite the improvement that this would allow, it will require some additional knowledge that may be difficult to find. We have to be able to project a reasonable range for b depending on current road conditions and we also have to measure the hydraulic pressure of the brake. There are cheaper options that we could consider such as looking at the deceleration of the wheel to decide when to stop increasing or decreasing the pressure and to hold the pressure constant instead. I hoped you enjoyed this brief introduction into the intricacies of actuators with the speed dynamics. Unfortunately, this is the end of our work for now with cars and braking systems. We're moving on to something very exciting, wild fires. I know that playing with fire is risky, but luckily we are doing all that on computers so we don't really have much to worry about. Get ready for some very interesting problems and great job on this unit. In this problem, we're going to deal with heat conduction along the wire. Just like you saw in the unit, we're going to split the wire that we had into discrete chunks. In fact, we're going to split it into 100 different chunks and went from 0 to 99. We can then measure the temperature of each chunk and labeled that with a proper number. So for example the temperature in the last chunk is going to be labeled to 99. As you already heard from Jörn, the temperature in any given segment--let's take the 8th segment that's simple--depends on the temperature in the surrounding two segments. In those segments that are touching one another are going to feed energy into each other. So this segment and that segment can both be energy into the 8th. In the same way, the 8th segment is going to feed energy into both the 7th and the 9th segments. To express the temperature of the 8th segment after time step done, we can use the finite difference scheme to come up with this equation right here. This equation tells us that the temperature after the time step, right here, is equal to the temperature before the time step plus this term, which we can see depends on the initial temperatures of the 7th, 9th and 8th segments. This is obviously an explicit method for calculating T₈(h), but just for a change of pace since this is a pretty unstable solution, we're going to try an implicit method instead. Remember that implicit means that you're going to have this term over on this side as well. So we're just going to switch all these 0s to h's. We're going to use this implicit method, of course, converting it to code in this problem. One problem that we have to deal with is how to end with segments in other end of our wire. Both of these have an adjacent segment to one direction but not to the other. Since the segments start at 0 and count to 99, our problem was explicitly is that we don't have a segment numbered -1 nor a segment number 100. To deal with this, we're just going to set the value of T₋₁ to equal T₀ and similarly for T₁₀₀ set in out to T₉₉. This is going to be true for all times t. Your main job in this problem is going to be to define the coefficient for every equation in the set of equations right here. Each equation involves an expression, which is just the sum of all the different temperatures of every segment along with some coefficients stuck in front of certain one, and all these equals the initial temperature at that certain segment. There seem to be the set of a 100 different equations. If we're going to consider this system of equations as a whole, we can actually think of it more conveniently as dealing with matrices. This website right here numpy.matalg.solve deals with equations involving matrices. We have a matrix as the coefficients which are all these pink question marks multiplied by a single column factor, which contains all the temperatures after time step each. These multiplied together equals this vector, which contains all the initial values of the temperatures. As we already know the temperatures at times 0 and you have created a matrix of coefficients, we can use this method numpy.matalg.solve and input the main information and solve the vector that we want--the one containing the temperatures after time step h. Taking a look at our supplied code, we can see that we've created an initial temperature distribution for you. We've also created a parameter for you called c, which I think if you look back at the finite difference formula, you'll see if's going to be very useful for you. As I told you earlier, your first task is going to fill in the array named coefficients with the appropriate values. Remember, a lot of the coefficients will be 0. Another hint that I want to give you is that using for loops could be a great way to fill in your coefficients to all the different slots in the matrix. Once coefficients are filled in, come back up to the step size and set it to 0.5 and then submit. In the end, you should have a curve that looks nice and smooth like this despite the pretty large step size that we're going to be using. Now let's talk about the solution to our problem with heat along the wire. First things first, I've rewritten the implicit equation that I gave you in the intro video. The only change here that you'll see is how I've replaced the coefficient in front of the parenthesis with C, the parameter that we created for you in the code. Now, if we rearranged this equation a little bit, we can get it in the form that's super convenient for using numpy.matalg.solve. This is because we already know T₈(0) and now we know what the coefficients are. So we are to solve for all of the temperatures after times that of h. This shows us then that two of the elements in the matrix coefficient are going to be -C and 1+2C. The one exception to this is going to occur at the boundaries. I told you these were going to be a little bit tricky, so let's talk about how to deal with them with right now. We take this equation right here but instead plug in the correct values for T₀. We see something interesting. There's a dependence on T₋₁. Now we defined our boundary with additions earlier, that T₋₁ is going to equal T₀ for any time. Once we plugged this n, we're actually going to get a pretty simple equation something that only depends on T₀ and T₁. To help you better visualize how are we going to fill in the array using loops, I've created this diagram right here which color codes the different possible coefficients that we could have in any slot in the matrix. As you just saw, a bunch of the terms in the matrix are going to be 0, so I filled in big 0s for those portions of the matrix. We know that the temperature of the second that we're looking at, is going to have a coefficient of 1+2C in front of it corresponding to this blue spirals. We know that both adjacent segments who have coefficients of -C which are these pink spirals over here. The two exceptions of course occur at the boundaries, for we have one pink spiral and one green spiral. For one coefficient -C and then one coefficient of 1+C. Now if I felt like working really hard, I guess I could have drawn this diagram with 100 rows and 100 columns making up the actual size of the matrix that we're looking at. But I was feeling lazy so I scaled it down a bit. However, if we think about it, we know that the number of blue spirals in that imaginary expanded diagram will be equal to 98 because there's going to be one blue spiral in every row with the exception of the first and last. Similarly, we would see 198 pink spirals, two for every row, except only one in the top and bottom rows. In the picture I just showed you, the blue spirals corresponded to the coefficient 1+2C and we said that we would need a loop using 98 iterations to express that. That's exactly what we have here in this top for loop. You can see that the next for loop contains one more iteration, since it starts at the index 1 lower. As expected, this corresponds to the pink spirals in the diagram with the coefficient equals -C. Here we have two exceptions for the two different boundaries and you remember that these were the green spirals in the diagram. We can check and see that we've set the step size equal to 0.5 seconds as we said we want to do. We can run the program and see what we get. Sure enough, we end up with the same solution that I showed you earlier. Since in our initial situation, we had a flame placed under the center of our wire, it makes sense that after some time has passed, we still see a peak in temperature toward the center of the wire, but the distribution has smooth out quite a bit. This has right in line that what we expect from the heat equation. Right now, the end time is set to 10 seconds which means that the final graph that we're looking at is the distribution of the temperature at this time. Let's see what happens if we set it to something smaller showing what's happening earlier on in the temperature change. So here's our distribution after just 4 seconds have passed. Consider we have a nice smooth curve, a lot like what we saw earlier except that this time the peak is at a much higher level than it was earlier. When end time was at the 10 seconds, the peak was just under 500 Kelvin and now we can see that it's resting right above 600. The distribution is also quite a bit narrower showing that we're farther away from the equilibrium position than we were with the larger step. So now we come to the last problem of unit 6 and actually the last problem for entire course aside from the final exam at least. Now you have a chance to put together all of the informations that we gave you in unit 6 to create a 2-D wild fire. We're going to be looking at a big grid of forest and seeing how diffusion, heat loss, wind, and combustion come together to affect the time derivative of the temperature. As I talked about before, these four factors can simply be added together to show how T changes with time. Let's look at the code for a second to figure out what are we going to ask you to do. As you can see, they've given you a number of different constants that are going to help you out a lot. One thing that's kind of a small detail that maybe interesting to know is that the velocity due to the wind, which we can see right here, is actually very low. It's much lower than the velocity of the wind itself since the fire is only going to follow the wind at a fraction of the wind's real speed. As I said earlier, your first task is going to be to take into account heat diffusion, heat loss, wind, and combustion in the 2-D heat equation right here. Fortunately, we're not going to ask you to use the implicit method that you used in homework 1. You can just use the explicit method you used before so basically the forward Euler method. And important thing that we need to deal within this problem is especially the varying conditions. We must be careful to not introduce any hard edges anywhere, or they will probably lead to artifact. First in the solver to the huge values in the derivative and second in terms of the model itself. We want to make sure that we don't see any staircases or pixelated contours or really anything that does not look like reality. To help deal with this, we have already included a smooth calcium bell shaped for the initial distribution of the temperature. The second thing that you need to do, however, is to create an initial distribution for the wood density. Now, I'll give you a little bit more information about that right now. Remember the grid that we're creating using the code is basically making a map of the terrain that we're looking at. This graph right here is going to give you an idea of what we are looking for in terms of wood density distribution. Now here we have a bunch of lines that are parallel to one another but slightly displaced. X and y are actually just the physical coordinates that we are looking at. But the different colors of the different lines correspond to changes in the wood density at each location. So every point along the blue line is going to have the same density as every other point on that line. Every point on this purple line has the same density and so on and so forth. In the code, we created 2 constants for you, wood 1 and wood 2 which have underscores in the code. That would be very clear what they are referring to. In this mark out, the highest and the lowest possible values of wood density that we have in our forest. Now, all of these lines are parallel to one another, and we've given you the slope and the code. That means that the only factor differentiating from one another is their y intercepts. If you make a graph of wood density versus y intercept to make something looks like this. You can see that we are only letting wood density vary between wood 1 and wood 2, right here. Another hint that might be helpful when you're doing this problem is that you can solve for the y intercept of given line by saying that the y intercept equals some y value minus the slope times the x value of that point. So it's kind of a tricky problem but remember it's your very last one before the final. If you have any trouble, come to forums. Good luck. Your first task was to fill in the rest of the code of this for loop down here. So the first thing that we changed is the wood density. The new wood density is going to be equal to the old wood density minus the time step times the burn rate. Since remember the burn rate is really just the time derivative of the wood density. Now for the change in temperature. Of course, we start with the initial temperature. And then we add to it h times a bunch of stuff. Now all of this code is just taking into account those four factors that we were saying influenced the change in temperature. So the first thing we have right here you can see is a 2D heat equation. And this is actually implemented using the finite difference scheme. After this, we also take into account heat loss and then the wind and then also the combustion. Now going up to the top of our wildfire function, let's talk about this initial distribution of wood density. This first line right here is just a translation of that mathematical expression I had written out for you. The y intercept of the point that we're considering is just equal to some y value minus the slope that we're given times x and actually this xy is the xy that we're talking about at this moment. That means that the wood density then is equal to wood value 1. Well, a bunch of typos. Well that's a problem. Let's fix that right now. See everyone makes mistakes. This should be wood 2. This should be wood 1. Okay now that all of that is fixed we have the wood density at our point xy is equal to wood 1 the maximum possible wood density plus all of this. This is the difference between wood 2 and wood 1. It's that range in which w is allowed to vary across the terrain divided by the difference between intercepts. So you remember on the graph that I showed you earlier. This graph right here actually. That is the slope of this line since wood density here is on the y axis and the intercept is actually on the x axis. If we take the difference in wood density divided by the difference in intercept we get the slope of this pink line right here. So now you can see that we have the slope of that pink line times the intercept of the line up to the point we're considering is on minus intercept 1. So this is really just using point slope form to calculate w. Remember, we only want w to range between wood 1 and wood 2. So this line restricts it to that range. Now let's look at the plot we get. So finally a graph of our forest. In both images, we're looking at the xy square of terrain that we've been talking about. Here we're graphing the temperature shown by these different colors, and here we're graphing the density of wood. Now unfortunately this is only a 50 x 50 pixel image. If we wanted to see what's happening with much better resolution, we should use 200 x 200. Unfortunately my computer is not cooperating so we're going to have to settle for this less perfect image. Even though this isn't really a perfect picture, you can still tell that the wood has been depleted in the center. The density of wood is very low right here compared to the rest of the area where the fire hasn't spread yet. You can also see that as we wanted the density of the wood does vary along lines like this. In the 200 x 200 image, you would be able to see that the wood had been very depleted in the center and that the fire had marched out the rim of this area. We can kind of tell that there is sort of a red circle around here. However, we'd be able to better that the fire is moving slower, and it's less hot in the area where there's less wood. The spreading the fire is way to slow on this image and one reason for that is that the heat is averaged over larger pixels so actually not lowering the temperature reached the temperature of ignition in a lot of places. I hope you found talking about wildfires as exciting as I have. Awesome job on this problem and throughout the entire course. I hope you're excited for the final exam. I think it'll give you a great look at a ton of different examples not just from the material we've learned about but also from some new kinds of applications.