Welcome to the first unit of Online Introduction to Artificial Intelligence. I will be teaching you the very, very basics today. This is Unit 1 of Artificial Intelligence. Welcome. The purpose of this class is twofold: Number 1, to teach you the very basics of artificial intelligence so you'll be able to talk to people in the field and understand the basic tools of the trade; and also, very importantly, to excite you about the field. I have been in the field of artificial intelligence for about 20 years, and it's been truly rewarding. So I want you to participate in the beauty and the excitement of AI so you can become a professional who gets the same reward and excitement out of this field as I do. The basic structure of this class involves videos in which Peter or I will teach you something new, then also quizzes, which we will ask you about your ability to answer AI questions, and finally, answer videos in which we tell you what the right answer would have been for the quiz that you might have falsely or incorrectly answered before. This will all be reiterated, and every so often you get a homework assignment, also in the form of quizzes. And then we also have video exams. An AI program is called wetware, a formula, or an intelligent agent. Pick the one that fits best. So congratulations, you just finished unit 1. You just finished unit 1 of this class, where I told you about key applications of artificial intelligence, I told you about the definition of an intelligent agent, I gave you 4 key attributes of intelligent agents (partial observability, stochasticity, continuous spaces, and adversarial natures), I discussed sources and management of uncertainty, and I briefly mentioned the mathematical concept of rationality. Obviously, I only touched any of these issues superficially, but as this class goes on you're going to dive into any of those and learn much more about what it takes to make a truly intelligent AI system. Thank you. And the answer there is there are three Pareto-optimal solutions, here, here and here. Everything but the top left. Let's see why. So a Pareto-optimal solution is one where nobody can unilaterally switch their strategies to improve themselves without making someone else worse off. Let's start up here. So here, B has a reward of zero, that's the best anybody could hope for, so B doesn't want to switch. A would like to switch over here, but that would make B worse off. Similarly, over here, A is happy to stay there, B would like to switch, but that would make A worse off. And same thing over here. If A switches, it makes B worse, and if B switches, it makes A worse. So all three of these are Pareto-optimal, whereas this one is not. And this is a very tricky question. The answer is the second one on the left over here. And the reason is we can correspond black pixels just like this, assuming that both of these red pixels are subject to occlusion. So the penalty is 20 for two occluded pixels. Now I should remark, in the initial version of this course, I got this wrong as your instructor, and I checkmarked this box over here, stipulating that a bad match would be better, but then the penalty is 40 for two bad matches, the R and the R over here. This is clearly the wrong answer, and this is the correct answer. And the answer is no, there is no solution that the agent can come up with because Bucharest doesn't appear on the map and so the agent doesn't know any actions that can arrive there. So let's give the agent a better chance. Now we've given the agent the full map of Romania. The start is in Arad and the destination, or goal, is in Bucharest and the agent is given the problem of coming up with a sequence of actions that will arrive at the destination. Now is it possible for the agent to solve this problem? And the answer is yes. There are many routes or steps or sequences of actions that will arrive at the destination. Here's one of them. Starting out in Arad taking this step first, then this one. And then this one, then this one, and then this one, to arrive at the destination. So that would count as a solution to the problem. So, sequence of actions, chained together, that are guaranteed to get us to the goal. Now, let's formally define what a problem looks like. A problem can be broken down into a number of components. First, the initial state that the agent starts out with. In our route finding problem, the initial state was the agent being in the city of Arad. Next a function actions that takes a status input and returns a set of possible actions that the agent can execute when the agent is in the state. In some problems, the agent will have the same actions available in all states. And in other problems they'll have different actions dependent on the state. In the route finding problem, the actions are dependent on the state. When we're in one city we can take the routes to the neighboring cities, but we can't go to any other cities. Next, we have. A function called result which takes as input a state. And an action and delivers as its output, a new state. So for example, if the agent is in the city of Arad, and, that would be the state, and takes the action of driving along route E671 towards Timisoara, then the result of applying that action in that state would be the new state where the agent is in the city of Timisoara. Next, we need a function. Called GoalTest which takes a state. And returns a boolean value true or false, telling us if this data is a goal or not. In a route finding problem, the only goal would be being in the destination city, the city of Bucharest, and all the other states would return false for the goal test. And finally, one more thing, which is a path cost function which takes a path, a sequence of state action transitions and returns a number which is the cost of that path. Now for most of the problems we'll deal with we'll make the path cost function be additive so that the cost of the path is just the sum of the individual steps. And so we'll implement this path cost function in terms of a step cost function. The step cost function takes a state, an action, and the resulting state from that action. And returns a number n which is the cost of that action. In the route finding example, the cost might be the number of miles traveled, or maybe the number of minutes it takes to get to that destination. Now let's see how the definition of a problem maps onto the root founding domain. First, the initial state we're given. Let's say we start of in Arad. And the goal test, let's say that the state of being in Bucharest is the only state that counts as a goal. And all other states are not goals. Now the set of all the states here is known as the states space. And we navigate the states space by applying actions. The actions are specific to each city. So when we're in Arad, there are three possible actions, to follow this road, this one, or this one. And as we follow them we build paths or sequences of actions. So just being in Arad is the path of length zero. And now we could start exploring the space and add in this path of length one. This path of length one and this path of length one. We can add in another path, here of length two and another path here of length two. Here's another path of length two. Here's a path of length three, another path of length two, and so on. Now at every point, we want to separate the state out into three parts. First, the ends of the paths, the farthest paths that have been explored, we call the frontier. And so the frontier in this case consists of these states and are the furthest out we can explore. And then, to the left of that in this diagram, we have the explored part of the state. And then off to the right we have the unexplored. So let's write down those three components. We have the frontier, we have the unexplored region, and we have. Explored region. One more thing. In this diagram, we've labeled the step cost of each action along the route. So the step cost of going between Neamt and Iasi would be 87, corresponding to a distance of 87 kilometers. And then the path cost is just the sum of the step cost. So the cost of the path of going from Arad to Oradea would be 71 plus 75. And the answer is no, we're not done yet. We have reached a goal state. We put a path onto the frontier that reaches the goal of Bucharest, but we haven't popped that path off the frontier. And uniform cost search continues to search until we pop it off the frontier. We continue looking to see if there's a better path that also reaches the goal. So let's see. I forgot to say Thagoras is explored, so let's continue. Let's take the cheapest path on the frontier and expand that. The cheapest path is this, 146. We'll expand that, get another path into (Sibiyu). That's a worse path than we had before, so we'll drop it. Then let's see what's next. Looking on the frontier, the cheapest now is here at 299. We'll expand that. We get a path of cost 374. Put that on the frontier. Now let's go again. Now the cheapest path is over here at 317. We'll mark that as explored and add two more paths--one here that's a worse path, so it gets dropped. And one path that also reaches the goal, and that has a total cost of 418. So that just shows it's a good thing we waited, a good thing we didn't stop when we found the first path to the goal, because now this second path found is actually cheaper than the first path found. But we're not going to stop here because we still haven't popped off a path that reaches the goal. So we'll continue. What's next? Now the cheapest path on the frontier is here at 366. We expand that, and we get paths that are worse paths to points we've already seen before. So nothing new goes on the frontier. Next, the cheapest path on the frontier is at 374. Again, expanding that leads nothing useful. Only worse paths than we've seen before. And now finally, the cheapest path on the frontier is this 418 path to Bucharest, so we pop that off, and now we reach the goal, and now we stop. So even though we found the 460 path first, we don't stop there because there might be another path that also reaches the goal that's cheaper. We keep on going until we popped a path off of the frontier that reaches the goal, and that's why uniform cost search is guaranteed to find the cheapest path to the goal. Now we're going to talk about how to do inference on Bays Net. We'll start with our familiar network, and we'll talk about a method called enumeration, which goes through all the possibilities, adds them up, and comes up with an answer. So what we do is start by stating the problem. We're going to ask the question of, "What is the probability that the burglar alarm occurred, given that John called and Mary called?" We'll use the definition of conditional probability to answer this. So this query is equal to the joint probability distribution of all three variables, divided by the conditionalized variables. Now, note I'm using a notation here where, instead of writing out the probability of some variable equals true, I'm just using the notation, "Plus," and then the variable name in lowercase. And if I wanted the negation, I would use negation sign. Notice there's a different notation where, instead of writing out the plus and negation signs, we just use the variable name itself, P of E, to indicate E is true. That notation works well, but it can get confusing between, "Does P of E mean E is true, "or does it mean E is a variable?" And so we're going to stick to the notation where we explicitly have the pluses and negation signs. To do inference by enumeration, we first take a conditional probability and rewrite it as unconditional probabilities. Now we enumerate all the atomic probabilities and calculate the sum of products. Let's look at just the complex term on the numerator first. The procedure for figuring out the denominator would be similar, and we'll skip that. So, the probability of these three terms together can be determined by enumerating all possible values of the hidden variables. In this case there are two--E and A. So we'll sum over those variables for all values of E, and for all values of A-- In this case they're Boolean, so there's only two values of each. We ask, "What's the probability of this unconditional term?" And that we get by summing out over all possibilities, E and A being true or false. Now, to get the values of these atomic events, we'll have to rewrite this equation in a form that corresponds to the conditional probability tables that we have associated with the Bays net. So we'll take this whole expression and rewrite it. It's still a sum over the hidden variables, E and A. But now I'll rewrite this expression in terms of the parents of each of the nodes in the network. So that gives us the product of these five terms, which we then have to sum over all values of E and A. If we call this product F of EA, then the whole answer is the sum of f for all values of E and A. So it's the sum of four terms, where each of the terms is a product of five numbers. Where do we get the numbers to fill in this equation? From the conditional probability tables from our model. So let's put the equation back up, and we'll ask you for the case where both E and A are positive, two look up in the conditional probability tables, and fill in the numbers for each of these five terms, and then multiply them together and fill in the product. Peg Solitaire is not partially observable because you can see the board at all times. It is not stochastic because you just name all the moves and they have very different mystic effects. It is not continuous as there is fairly many choices of actions and fairly many board positions, so therefore it's not continuous. And its not adversarial because there is no adversary, it's just you playing. So the loaded coin example is clearly partially observable, and the reason is, it is actually useful to have memory. If you flip it more than one time, so you can learn more about what the actual probability is. Therefore, looking at the most recent coin flip is insufficient to make your choice. It is stochastic because you flip a coin. It is not continuous because there's only one action, a flip and two outcomes. And it isn't really adversarial because while you do a learning task, no adversarial interferes. The path through the maze is clearly not partially observable, because we can see the maze entirely at all times. It is not stochastic, there's no randomness involved. It isn't really continuous. There's too many just finely many choices to go left or right. And it isn't adversarial, because there's no real adversary involved. Breadth first from Left-to-Right is 6. 1, 2, 3, 4, 5, 6. Depth first from left to right is 4. 1, 2, 3, 4. Breadth first search from right to left is 9. 1, 2, 3, 4, 5, 6, 7, 8, 9. And depth first from Right-to-Left is 9. 1, 2, 3, 4, 5, 6, 7, 8, 9 The correct answer for breadth-first, left-t- right, is 13. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, and for depth-first is 10. 1, 2, 3, 4, 5, 6, 7, 8, 9, and 10. For a right-to-left search, the right answer for breadth-first is 11. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11. And for depth-first, the right answer is 7. 1, 2, 3, 4, 5, 6, 7. The right answer over here is 10, for breadth-first, left-to-right. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Depth-first is 16, for all nodes. 1, 2, 3, 4, 5, 6, 7. 8, 9, 10. 11, 12, 13. 14, 15, 16. And notice how I never expanded a node twice. Correct answer for breadth-first, right-to-left is 7. 1, 2, 3, 4, 5, 6, 7. And the correct answer for depth-first, from right-to-left, is 4. 1, 2, 3, and 4. Well, clearly this is an admissible heuristic, because the distance to the goal is strictly underestimated. From here, it would take one step, from here, it will take one, two steps. So the answer is yes. Now, to understand A star, let me also draw the g function for development part of this table. Clearly g is 0 over here. To understand which node to expand, this one or this one, let's project the g function, which is 1. And we'll see that 3 plus 1 is smaller than 4 plus 1. Therefore, this is the second node to expand, which is b1. Now let me, for the next step, expand the g function from this guy here, 2 and 2. So 3 plus 2 plus 2 is 4. Whereas this 3 plus 2 is 5. To expand this node next, which is c1. And finally, the g function from here would go 3 and 3. 3 plus 1 is better than 3 plus 2, so we will expand d1 next. And notice how in the sum of g and h, this node over here, which has a total of 4, is better than any other node that is unexpanded. So in particular, 4 plus 1 is 5, and 3 plus 2 is 5 as well, and 2 plus 3 is 5 as well, so this the next one to expand. Given the following base network with P of A equals 0.5. P of B, given A, equals 0.2. And P of B, given not A, 0.8. Calculate the following probability. Consider a network of the following type. A variable A, that is binary, connects to three variables X1, X2, and X3 that are also binary. The probability of A is 0.5, and for all variable Xi we have the probability of Xi given A is 0.2, and the probability of Xi given knot A equals 0.6. I would like to know from you the probability of A, given that we observe X1, X2, and knot X3. Notice that these variables over here are conditionally independent, given A. Let us consider the same network again. I would like to know the probability of X3 given that I've observed X1. In this next homework assignment, I will be drawing you a base network and will ask you some conditional independence questions. Is B conditionally independent of C? Answer yes or no. Is B conditionally independent of C, given D? Answer yes or no. Is B conditionally independent of C, given A? Answer yes or no. And is B conditionally independent, given A and D? Answer yes or no. Consider the following network, I would like to know whether the following statements are true or false. C is conditionally independent of E, given A. B is conditionally independent of D, given C and E. A is conditionally independent of C, given E. And A is conditionally independent of C, given B. Please check yes or no for each of these questions. In my final question, I look at the exact same network as before, but I would like to know the minimum number of numerical parameters, such as the values to define probabilities and conditional probabilities, that are necessary to specify the joint distribution of all five variables. Here are the answers. The first sentence is true half the time and false half the time. It would have been true all the time if we had written fire or not smoke, rather than smoke or not fire on the right hand side. The second sentence, is false when fire is true and smoke is false and otherwise true. The third sentence is always true. And this is called the contrapositive. Smoke implies fire is the same thing as not fire implies not smoke. The fourth sentence is always true. And you can figure that out by writing out the full truth tables, or by reasoning about the variable big. When big is true, the whole sentence is true, because big is one of the disjuncts. And when big is false, it's true, because big implies dumb is true whenever the antecedent is false. And the final sentence is also always true. And this is known as De Morgan's Law. The answers are, the first sentence has erroneous syntax. We're using an and here between two terms, but you can't do that. An, and can only be used between sentences or predicates in first order logic. The second sentence does correctly encode the English sentence, Paris and Nice are both in France. Similarly, the third sentence does correctly encode, there's a country that borders Iran and Syria. But the fourth one incorrectly encodes it. Here, we have an x essential. There exists a C, and then an implication. And that's usually the wrong thing. The problem here is not if C represents a country, but what if C represents something that's not a country, say my dog. My dog is not a country, so there does exist a C, which is my dog, such that this implication is true because whenever the antecedent of an application is false, my dog is not a country then the whole thing is true. But the final sentence in English, no two bordering countries can have the same map color. Both of these are correct encodings. The first one seems more obvious and the second one, we've just manipulated things a little bit. We know that a implies b is the same thing as saying not a or b. So here, we've just taken the left hand side and negated it, and then put those all together with an or. So those, these two sentences represent the same thing, and they're both correct. The answer is, that we start off knowing nothing. So we're in this belief state here, where any of the eight possible states are possibilities. Then, our path to the goal is to move right, and we arrive in this belief state, and then suck up the dirt there. Then move left and then suck up the dirt there. And we end up in a belief state with only a single world state and that's one that reaches the goal. Where we're on the left and both squares are clean. And yes, that is guaranteed to reach the goal. The answer is, the right movement is stochastic, so it may fail, so that means we may stay in the left and we may move to the right. And the world is dynamic, which means, dirt may appear in either the left or the right location. And we didn't know for sure if there was dirt or not in the right, so that means any of the eight possible states belong to the belief state for the update, for the or rather for the prediction of moving right. That means any of the eight states belong to the belief state for the prediction of moving right. The answer is, state six and state two. Those are the two states in which the vacuum is on the right, and that state is dirty. The other state we can't observe, and it could've been in any state before, so now it can be either clean or dirty. The answer is states four and eight. We know that the suck action will make it clean in our current location, but we don't know what's going on in the other location. The answer is nothing has changed. Our belief state is still 4 and 8. We didn't really get any new information from that input because we knew that the result of the suck action was going to clean up locally, and we still didn't know anything about the other non-local state. The answers are, yes, the monkey has the bananas. No, the box is not at C, it's been pushed to B. Yes, the monkey is at B. Yes, the bananas are at B. No, the height of the monkey is not high, because he climbed down. Which meant that the effect was, that is at height, low. But yes, the height of the bananas is high according to these definitions. Now you would think, once the monkey grasps the bananas and climbed down that the height of the bananas should be low. But if we look at the up rate of climb down, it doesn't say that. It refers to the monkey. But it, it doesn't refer to anything that the monkey is holding. because that kind of thing is difficult to express in the language of classical planning. So you could say, that's a weakness in the definition of climb down. The answers are, in this case only the second is a correct representation. Any combination is possible to be dialed. It's not the case that we can, it's only possible to dial the correct combination. Now here, we said that the lock button works at any point. Whether it's open or not, the lock button will always lock it. And so that's represented by the third option. True implies it's possible to lock. In this case, the first one is a correct representation of the successor state axiom for open, and the second one is not. Because note what it says. If we already have the lock open, and then we execute some dialing action that's not dialing the correct combination X, we want it to remain open. But this second axiom would make it be closed, which is not what we want.