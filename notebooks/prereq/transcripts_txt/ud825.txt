For the majority of Android developers out there, the concept of performance is the last thing on their minds. Most app development is a mad sprint towards getting features in, making the UI look perfect, and figuring out a viable monetization strategy, or a lack thereof if you're in Silicon Valley. But application performance is a lot like the plumbing in your house. When it's working great, no one really notices or thinks about it. But when something goes wrong, suddenly, it's everyone's problem. You see, users notice bad performance before any other features in your app. Before your social widgets, awesome image filters, and how one of your supported languages is Klingon. And guess what users unhappy with performance give bad reviews at a higher percentage than any other problems in your app. This is why we like to say that perf matters. It's easy little side of performance as you're developing your app. But frankly perf is involved in everything you do. Users feel bad performance. They complain about bad performance. They uninstall your app, and then vengefully give you a bad review all because of performance. When you think of it this way, performance sounds more like a feature that you should focus on, rather then a burden that you have to put up with. But in honesty, improving performance is a really tough thing to do. Fixing these problems means knowing why it's slow, which means knowing how to use the right tools in the right ways to get insight into that data. But don't worry. Everything in this Udacity course has been specifically built around helping you know the tools, see how problems manifest in them, and understanding what they mean from a theory prospective. Basically, it's everything that you need to become a performance ninja. I'm Colt, this is Chris, let's get started. The process of improving application performance can be a daunting task, but it's actually much simpler than that. Once you shift gears into focusing on performance problems, you've now entered what we call the performance improvement life-cycle. It's a very small set of tasks you must perform, to find and fix a problem. It namely consists of three steps. Number one, gather information. When someone says that your app is slow, you have to go and figure out why. This means you need to run the profiling and feedback tools to collect information about your app. What you can measure, you can optimize, right? Which means that in the beginning of optimizing any of your applications, the process hinges on being able to measure its state and then evaluate it after you make changes. Step number two, is to gain insight. The data that you gather is not always obvious. Now, most of the time, the content is a big collection of floating point numbers, which then, if you're lucky, gets turned into some snazzy visualization by a tool. But even then, chances are that you're still at a loss for what all those colors, lines, and charts actually mean. This is where developers become performance gurus. Now you see interpreting a sheet of floating point numbers and then realizing that you're spending too much time serializing XML, is the modern equivalent of a shaman reading chicken bones to predict the future. And honestly these first two stages of gathering data and then gaining insight, happen in a really tight loop. You may use one tool, gain some info from it, and then realize that the problem is in another part of your pipeline. Which then you'll need a separate tool to diagnose. But of course that leads us to the third step, take action. This is often the most difficult part of the loop this is where you take all the numbers and all the insight, and know where the problem is, and then you have to go convince the other programmers the right way to fix it. Of the three stages, this has the most human component to it. Because solving the problem isn't enough, you need to solve it in a way that meets your coding standards for your complexity or your company or takes in account libraries or a particular module or a platform that you're running on, or other crazy restrictions that your code base might have. Before your solution is accepted, you usually have to take all of these things into account. But before you run off into the weeds and become a performance engineer I have one small piece of warning about performance. Throughout this course we'll be teaching you what's going on under the hood of Android and teaching you how to use the tooling to gain insight into some of those actions. Some of that content is going to be prescriptive like if you see this, it means this, so do that. But in honest, that might not always be the case for every application sure something may be inefficient but if it's not impacting your performance, it's not worth stressing over. This is why we like to say, tools not rules, while it's important to understand the rules and the flow of things, you need to validate the true real issues by trusting what your tools say first. It may not make sense for you to solve a problem that you actually don't have. Always go back to the data and validate the true nature of a problem, decide if it's a problem, before moving on to the optimization process. You've been warned, Huh. 140 grams of fiber. Why would you need that much? [SOUND]. Hey. Morning, Colt. Hey, Chris. What's going on? How's it going? I'm just recovering from a long day yesterday. I spent all day making this new app. It's got a material theme, some tasteful animations, a nice transition. When I tilt the screen, look at that, it subtly adjusts the color scheme. Nice! Pretty slick huh? Yeah, it looks really good, Chris, but you know your application is running really slow, right? Yeah, for some reason, Android feels a bit slow. I mean, I think you guys might need to tune it up a bit. It would make our lives a little easier. Ha ha. I'm going to let that one slide. Why don't you back up and tell me what you're doing to create this transcendent experience of yours. Yeah, sure. So to get this nice color blending happening here, I have to layer a bunch of views, color them, and then when I animate them, I actually have to cut out circles, redraw them back to the screen. And as the user scrolls, I apply this cool new animation to draw the new content on top of the old. I might even put an Easter egg right there. Okay. Timeout, Chris. I think you are violating some of the principles of modern application performance design. Just because you can overlay 90 views that are animating and transparent, doesn't mean you actually should. Okay. But that's the user experience that seems to be selling these days. If it runs slow, it's Android's problem, right? Not mine. No, you gotta remem, put it this way, Chris. You wouldn't use bubble sort on an array that's got 20 million elements in it, would you? That's true. I'd probably use some, something way faster like merge sort or quick sort. See exactly. You gotta remember that the Android system is doing a ton of work to make things fast and look beautiful, but you still have to live up to your end of the bargain and do a little lifting too, right? All of those overlays and transparencies are beautiful, but they actually have an implicit performance cost that comes with them. This is why rendering performance is one of the biggest pitfalls that most developers fall into. Right. They spend so much time making these beautiful visuals. They forget all of the performance cost that comes along with it. All right, well. But, I still want to keep this user experience. So, how do I make it fast? Huh. That's the easy part, Chris. You watch some videos. Rendering performance is the most common performance issue that you run into while building your app. On one hand, your designers want to produce the most usable transcendent experience for your users, but on the other hand, all those fancy graphics and transitions may not work well on every single device. Let's take a look at what rendering performance is all about. Now firstly, know that the system will attempt to redraw your activity, every 16 milliseconds or so, which means that your application needs to run all of the logic to update the screen in that 16 millisecond window, so that you can hit 60 frames per second. This frames per second number, generally comes from the phone's hardware, which defines how fast the screen can update itself in a single second. Now, most devices will refresh at about 60 hertz, meaning you've got 60 milliseconds to do all of your drawing logic each frame. If you miss that window, say take 24 milliseconds to finish your calculations, you get what we call a dropped frame. The system tried to draw a new picture to the screen, but one wasn't ready yet. So it didn't refresh anything. The user ends up seeing the same graphic for 32 milliseconds rather than 16. Hm. Any animations going on during a dropped frame will manifest themselves to your users with a jump in their smoothness. Now because even if there's one dropped frame, the animation won't look smooth to anybody. And multiple dropped frames are at the core of what users start calling a laggy or janky experience. It gets even worse when lag happens while your users are interacting with the system, for example, when they're dragging a list view or trying to type in some data. This is what users mercilessly start complaining about. Now that we have a better understanding of how much time it takes per frame to draw, let's take a look at some of the problems that cause lag, and how you can address them in your applications. The Android rendering pipeline is broken up into two key sections, the CPU and the GPU, which both work together in order to draw images on the screen. Each one has some specific processes that are defined by it and need to obey specific operating rules in order to be performant. Now on the CPU side, the most common performance problems come from unnecessary layouts and invalidation's that is part of your view hierarchy having to be measured, torn down, and rebuilt. This generally comes in two problematic flavors, either the display lists are being rebuilt too many times during a frame or you're spending too much time invalidating too much of your view hierarchy and needlessly redrawing. Both cause taxing CPU overhead in refreshing display lists and other related cache GPU resources. Now the second major problem is on the GPU side, involving an inefficiency called overdraw, which is effectively when we waste GPU processing time coloring in pixels that end up getting colored in later, by something else. Now, in this lesson, we're going to talk more about invalidation's, layouts, and overdraw, and how you can use the tools available in the SDK to find out if they're hurting the performance of your application. And don't worry about all the crazy diagrams you've seen so far, once we've covered the theory, Chris is here to walk you through the code samples that will teach you how to fix these exact kind of problems in your applications. So, let's get started with something easy, overdraw. Understanding how to make a great performing app has everything to do with understanding what's going on under the hood. See, if you don't know how the hardware's working you've got a good chance of using it poorly. Now the main question is this. How does your activity actually get drawn to the screen? Or rather how does all that crazy XML and markup language turn into pixels the user can see and understand? At its core, this is done with a process known as rasterization. This is the process of taking some high level object like a string or a button or a path or a shape and turning it into pixels in a texture or on your screen. Now rasterization is a really, really time consuming process. And as such, there's a special piece of hardware in your mobile device that's designed to make it happen a lot faster. The graphics processing unit, or GPU, was introduced to mainstream computers back in the early 90s, to help accelerate the rasterization process. Now, the GPU itself is designed to use a specific set of primitives. Dominantly that is polygons and textures, a.k.a, images. And your CPU is responsible for feeding these primitives to your GPU before it can draw anything to the screen. This is done through a common API on Android known as OpenGL ES, which means that anytime your UI objects, like buttons or paths or check boxes, need to be drawn to the screen, they're first converted to polygons and textures on the CPU, and then passed off to the GPU to rasterize. And, as you would imagine, this process of converting a UI object to a set of polygons and textures is not the fastest of operations. Likewise, actually uploading that process data from the CPU to the GPU isn't that fast either. So it makes sense then that you'd want to reduce the number of times you have to convert an object, as well as the number of times you have to upload it for drawing. Thankfully, the OpenGL ES API allows you to upload content to the GPU and then leave it there. When you'd like to reference drawing a button again in the future, you simply have to reference it in GPU memory, and then tell OpenGL how to go about drawing it. The general rule of thumb is this. Optimizing for rendering performance means getting as much data onto the GPU as fast as possible, and then leave it there without modifying it for as long as possible. Because every time you update a resource on the GPU, you lose precious processing time. Now, since the release of the Honeycomb version of Android, the entire UI rendering system works with the GPU, and with every release since then, more improvements have been made to the rendering system performance. The Android system does a lot, and, and I mean a lot of work to reduce, reuse, and recycle GPU resources on your behalf so that you really don't have to think about it. For example, any resources that are provided by your theme, that is Bitmaps and drawables, et cetera, are all grouped together into a single texture, and then uploaded to the GPU on your behalf alongside commonly used meshes, like Nine Patches for instance. This means that you, anytime you need to draw one of these resources, you don't have to do any conversions. See, all the content is already stored on the GPU, making these type of views really fast to display. However this whole rendering process gets more and more complex as your UI objects get more and more advanced. Take for example, displaying images. This actually means uploading the image on to the CPU into its memory and then passing it off to the GPU to render. Using paths is a whole separate mess. As you might need to create a chain of polygons in the CPU or even create a masking texture to define the path shape on the GPU. Now, drawing text is a complete double whammy, right? I mean first, we have to draw the characters in the CPU to an image, and then upload that image to the GPU, and then go back through and draw a rectangle on the screen for each character in our string. Now, the performance of those operations are mostly handled by the Android system on your app's behalf, and unless you're doing something excessive, you really shouldn't see much of a GPU problem there. However, there's one GPU performance bottleneck that challenges every single developer equally. It's called overdraw. If you've ever painted a room or a house, you know that it takes a lot of work to color in those walls, and if you need to end up repainting it again, you've wasted a ton of work doing it the first time. This same concept of wasting effort to paint something can also contribute to performance problems in your applications. See, at the intersection of performance and design lies a common performance problem. Overdraw. Overdraw is a term used to describe how many times a pixel on the screen has been redrawn in a single frame for example if we have a bunch of stacked UI cards, all of the cards that are on the top part of the stack closer to the user will hide large portions of the cards that are hidden underneath. Meaning that we'll spend time drawing those cards which are mostly invisible. This is actually a large problem because each time we're rendering pixels that don't contribute to the final scene, we're wasting GP performance. And with modern layouts, it's easy to fall into a trap where we're stacking and layering views to give us this beautiful, transcendent design. But also creating the same overdraw problem. To maximize performance in your application, you'll want to minimize overdraw. Fortunately, it's easy to see the amount of overdraw in your applications right on your Android device. Simply go into developer mode, and then turn on the Show GPU overdraw flag. Now, don't panic while your phone turns into some sort of visual awesomeness. This is completely normal. Android is using different colors to highlight areas of overdraw occurring on your screen. If you've only rendered a pixel one time, you should see it in its true color with no tint. However, as the overdraw increases, so do the colors. A one x overdraw, for example, is tinted blue, meaning that you've now redrawn this pixel one extra time. And, of course, two x, three x, and four x follow the same pattern. So when you're tuning your app's user interface, you'll want to reduce as much overdraw as possible, reducing all of those areas of red in favor of the nice blues. To accomplish this goal, there's two main ways you can remove overdraw. Now firstly, you'll want to eliminate unneeded backgrounds and drawables from views that won't contribute to the final rendered image. Remember, this is wasted performance. Secondly, you can define areas of your screen that you know will hide portions of your view, which can help reduce CPU and GPU overhead. So let's get started at the top and join Chris as he's going to take us and help us look at overdraw. Hey, what's going on? And thanks Colt. Now, I'm really excited to join heads and perf out a developer situation involving overdraw. Now, if you want to follow along, you'll want to clone the code sample at the URL in the instructor notes. All right, so here we are in our sample code application. Now, let's imagine, you just created a chat application, and you want to find out how well you did in respect to overdraw performance. The first thing you'll want to do is gather information about how your app is performing. To do this you're going to turn on the GPU overdraw debug setting on your device. You will find it within the Developer Options section of your systems settings, like so. Okay, it's on now. Now, back to our chat application. Uh-oh, look at all this excessive overdraw happening here. You want to reduce this, particularly the red areas. So here's a little reminder about what the colors mean. Cool, ready to dig in? Let's dive into how the UI is currently built, and see if we can clean it up a bit to reduce some overdraw. Now, as Colt mentioned, one way to do this is to remove unnecessary backgrounds and drawables. So let's take a stab at doing that. How about we spiff up Chatum to be like this? But let's chip at the problem one step at a time. For example, notice that we have a green or two x overdraw occurring in Chatum's background right here? Now why could this be? Well it turns out that Chatum's base activity uses a frame layout with an opaque white background that fills the entire screen. We like this, but it collides with Android's material theme defaults, particularly the window background drawable. This causes unnecessary overdraw. Now as a developer, we have a design decision to make. Let's say we want to keep our white background, which means there's no real purpose for the material's theme. So one optimization we can make here is to set our activity's background drawable to null. Now, let me show you how you can do this in code. In Chatum's base activity, let's look at the onCreate method. Use the following statement to nullify the background. All right, sweet. By nullifying the background we've reduced this overdraw from a green to a blue, effectively making the overdraw now one x. Nice. Now this was a programmatic change. But lets take a look at our XML markup to see if we can tweak anything else. Now, you probably already noticed that there are three XML files that specify Chatum's user interface. There's the base activity Chatum Latinum, the XML for our chat fragment, and then lastly, the individual XML for our chat items. As mentioned before, we intentionally want to keep this white background right here. So, let's not remove anything. But maybe there's some opportunity for tweaks in the remaining two XML files. This is where I could use your help. Do me a favor and comb the remaining XML files, and see if you can remove any unnecessary backgrounds declared. When you're finished, enter the number of backgrounds you've removed in this box here. If you're having trouble, no need to worry. Just move on to the solution. So there should have been four unnecessary backgrounds in the remaining files. Let's go ahead and review. In our base activities XML, remember, we wanted to keep this white background. Now, in the XML file for our chat fragment, we've declared an unnecessary white background right here. We don't need this because we can use the white one from our main activity. That's it for this file. Now, in the XML for our individual chat items, we have three unnecessary backgrounds. We have one right here that we don't need, one right here that we don't need, and lastly, down here, we don't this white background in our text view. So we can go ahead and remove these. All right. Cool. Let's see what kind of progress we made as far as overdraw is concerned. Now this is what your screen should look like with these backgrounds removed properly. Much cleaner, right? Okay, good work. Now, we're just about done, but there's actually one last optimization you can make. Notice there's overdraw here by the avatars, because we are drawing a rectangle and then the avatar image on top of it. Let's try and be a bit smarter here, let's only set a background when an avatar can't be found. Now, we can do this with some conditional code. All right,. So let's jump into our chat adapter code, which is responsible for filling in our individual chat items as they get loaded. Let's go to our get view method. Now, down here at the bottom we actually have some logic that's displaying both the avatar and setting a background color at the same time. Let's see if we can get a bit more intelligent here. Let's write some code that only sets the background color when an avatar isn't present. And when it is, we'll set the background color to be transparent and just load the avatar. We can do it like this. All right, so here's our updated code. Notice that when an avatar isn't present, what we're going to do is load a transparent color into where the avatar would normally be, and then set a true background color for the avatar. Now, in the else case, which represents when the avatar is present, we'll go ahead and load the avatar properly, and then we're going to set the background color to transparent. This way we're minimizing overdraw. All right, so let's go ahead and see how this improvement helps us out. Nice. As you can see here now by our avatars, much less overdraw with our updated code. All right, so that was our last optimization, which means we're done. Let's go ahead and recap. When we started, our overdraw was much more prominent. The first thing we did was set the background drawable to null. The second thing we did was remove unnecessary background declarations from our XML markup. Now, the third and last thing we did was display a background color only if there is no avatar present. Now, with these changes, we ended up with this. Much, much cleaner in terms of overdraw performance. So, awesome job. And remember, perf matters. Now in terms of running performance, keep in mind that some overdraw might be necessary and acceptable, like the screen here from Android's action bar. But if you want to polished app experience, you'll want to make sure you reduce it as much as possible. All right, cool. Let me send it back to Colt to tell you a little bit more about clipping, another optimization that you can use to reduce overdraw. All yours, Colt. It's worth pointing out that the Android framework knows that overdraw is a problem, and will go out of its way to avoid drawing UI widgets that may be invisible in the final image. This type of optimization, called clipping, is highly important to UI performance. If you can determine an object will be fully obscured, there's no reason to go about drawing it. In fact this is actually one of the most important performance optimizations that Android performs. But sadly this technique doesn't extend itself to complex custom views, where you're overriding the canvas dot on draw method. In these cases, the underlying system doesn't have insight into how you're drawing your content, which makes it really hard for it to remove hidden views from your rendering pipeline. For example, in this stack of cards,only the top card is fully visible and the rest of the cards are mostly hidden, which means that drawing all of those overlapping pixels is wasted processing time. To address this problem, the canvas class is equipped with a few special methods that you can use to tell the Android framework what parts of the canvas are hidden and don't need to be drawn. The most useful method is canvas dot clipRect, which allows you to define the drawable boundaries for a given view, meaning that if any canvas drawing happens outside these boundaries, it'll be ignored. This could be really helpful when you've got this type of staggered view, like our cards for example. If you know how much of your custom view is visible, or rather you know how much is obscured, you can define the clipRect boundaries, which will prevent any drawing from happening in the obscured areas. Now the clipRect API helps the system know what to avoid drawing. But it would also be helpful for your custom view to do some of this work on its own, ahead of time for example, it would be really helpful if you knew what you were going to draw if it's actually outside of the clipping rectangle. Fortunately, you don't have to figure out all that intersection logic yourself, the canvas.quickReject method tests whether a given area is completely outside the clipping rectangle, in which case you can skip all that drawing work. Now let's take a look at a common situation where this happens and take a swing at fixing it. All right. I hope you're all stretched out because it's time to step up to the perf plate again. Let's take another look at an overdraw situation, this time involving custom views. Now as a developer, these are custom situations where you want to create a unique experience or behavior that isn't quite covered by Android's built in views. For example, in the following code, some of the UI elements or at least portions of them may be invisible to the final scene, and thus the app probably doesn't need to draw them. So, as Cole mentioned, these are situations where you might want to employ clipping to prevent unnecessary use of GPU resources. So, let's hop back to the render app and take a look at Droid cards. All right. Now in the background here, I've brought up some of the source files for this activity. Now as you can see in this DroidCardsView Java file, the UI for this activity is implemented using a custom view that displays a stack of overlapping Droid playing cards. Now, we don't have time to cover it now but if you'd like to review creating custom views, feel free to check out some of the useful resources in the instructor notes. All right, so let's gather some information about our app. Let's turn on GPU overdraw debug again and see how this activity looks. Whoa! There's clearly lots of overdraw happening here. Particularly these red areas right in the center. Now, I have a question for you. What could possibly be causing this much overdraw? Is it, we've inadvertently declared extraneous backgrounds, we're drawing cards to the screen in a way in which they overlap leaving some portions hidden, we shouldn't be using a custom view in this scenario? Or is it the frustration that builds up from developers who chose not to address rendering problems like overdraw? If you've concluded that all this red overdraw is the result of wastefully drawing hidden portions of the cards that lie underneath the top card, then you'd be correct. As it currently stands in this code, the cards are drawn in their entirety through this loop and slightly offset from each other, and thus there is natural portions of overlap. Now unfortunately, as the cards are drawn from the bottom up here, each successive card ends up covering the existing cards below it. This needlessly wastes GPU cycles. So let's go and fix it. All right, so here we are back in the Android Studio, reviewing the source for our DroidCardsView. Again, this is our custom view. Just to recall, it's in this custom view that we build our stacked playing cards. For example, each card has its own bitmap, and we draw them to the screen by overriding the onDraw method. Now remember, by overriding the onDraw method, Android won't be able to optimize the rendering of this view, so it's our job as developers to properly clip each card as it's drawn to the screen to avoid the unnecessary overdraw. Fortunately, the Canvas API gives us just the right methods we need to draw our cards more efficiently. Let's take a look at the documentation. Let's use the canvas.clipRect method to improve our code. We're going to use this variant here that takes four floats as parameters. All right, now we're back in Android Studio. Let's tweak how we draw each cascaded card so that we reduce this overdraw. I'm going to use this nifty feature within Android Studio to bring up a diff view, so you can see the before and after state of the code. 'Kay, all right, here we are in our comparison view, and let's look particularly at the onDraw method, which is the one we're overriding. We've got the old state of the for loop here on the left, but let's focus here on the improvements that we're going to make on the right. Now, as we iterate over the cards, let's go over each step of improvement. Number one, first, we're going to calculate the position for the card. Then we need to call this function canvas.save, which is going to save our existing canvas state. In other words, it maintains the existing state of your screen before we apply the clipRect API. Now, when we call this clipRect method, we're basically doing some geometric restrictions. When we pass in these parameters, we're basically telling the system to only draw a portion of the card that we want to see visible. Obviously, the rest of it would be hidden. Now, only the parts of the card that lie within the bounds of the restriction that we just put will get drawn. Finally, we're going to call canvas.restore, which is going to revert the canvas to a non-clipping state. In other words, it's going to undo the restriction that we put in place when we called clipRect. And then we repeat this loop for all the cards except the top one. Now specifically, we process the top or last card differently than its underlying siblings. For this one, no clipping is needed, so we're going to go ahead and draw it in its entirety. You can see that via the statement right here. All right, let's build our improved code and see if we've reduced a bunch of overdraw. All right, awesome. As you can see here, much less overdraw. Now I hope it's clear that with the Canvas API, we have a straightforward way to draw efficiently when working with custom views. Oh, and in case you want to explore the Canvas API some more, be sure to check out the full documentation in the instructor notes. Now, views like these are great for helping us create a unique and compelling UX, but remember that we must clue in the system to help Android render such views in a performant fashion. So whenever you're creating a custom view, be sure to check for overdraw, and be ready to revive your friend, the clipRect method. All right, awesome work. Let's head back to Colt for more perf wisdom. Now that you've gotten overdraw under control, it's time to bubble up a bit and start looking at the CPU portion of the rendering pipeline. In order to draw something on the screen, Android generally needs to convert that high-level XML into something that the GPU can accept and use to render to the screen. This is done with the help of a internal Android object called a display list. A display list basically holds all the information needed to render a view on the GPU. It contains a list of any GPU resident assets that might be needed, as well as a list of commands to execute with OpenGL in order to render it. The first time a view needs to be rendered, a display list will be created for it. And when we need to render that view to the screen, we execute that display list by submitting its drawing commands to the GPU. If we want to render that view in the future again, like if its position changed on screen, we simply need to execute that display list again. However, in the future, if some visual part of this view changes, the previous display list may no longer be valid. As such, we'll need to recreate the display list and then re-execute them to update the screen. Now, remember this. Anytime the drawing content of your view changes, it will repeat the process of recreating the display list and then re-executing it to get it to the screen. The performance of these operations varies, depending on how complex your view actually is. And depending on the type of visual change, there's other impacts on the rendering pipeline as well. For example, let's say that a text box suddenly doubles in size, causing the parent container to reposition other sibling views before updating its own size. In this case, we've updated one view and it cascaded to other work that needed to be done. These types of visual changes require additional stages of the rendering pipeline to occur. See, when the sizing of your view changes, the measure phase will be kicked off. And it will walk through your view hierarchy, asking each view what its new sizes are. Basically, this happens anytime you change the size of a view, for example, padding, drawable size, set text's scale, width, height, et cetera. And if you change the position of things or call request layout, or a view group repositions its children, then the layout phase will kick off, traversing the hierarchy and determining where all the new locations of the objects should actually be on the screen. Now, the Android runner system is really, really efficient at handling the record and execute phases of the pipeline. And unless you're doing something crazy with custom views, or suddenly a ton of views are getting drawn at once, these shouldn't account for much of your frame time. Measure and layout phases are also pretty performant, but are more prone to run into performance problems when your view hierarchy gets out of hand. See, the time taken to execute these functions is proportional to the amount of nodes in your view hierarchy that have to be processed as a result. The more views that this system has to touch, the longer it takes, and some views may have worse overhead than others. And the number one overhead for this operation is having a lot of redundant unneeded views lying around in your view hierarchy. See, these views don't visually contribute to the final scene, and generally end up just eating up your performance when a measure or layout phase actually kicks off. Thankfully, there's a great tool available to help you find and fix these rogue views called Hierarchy Viewer. Let's take a look. Thanks, Colt. Let's take a look at our first tool. Hierarchy Viewer is a tool that'll help you visualize your entire UI structure at a glance, and also provide you with a better way to understand the relative rendering performance of distinct views, within that structure. Here's what it looks like, so let's go ahead and set it up. You're actually going to need some actual code to try this on, so we're going to use Sunshine from the Android fundamentals class. You can find the details on how to pick up this code from the instructor notes down below. When you're done, click Continue to move on. Oh, and one more note, if your phone is rooted and running Android Jellybean or newer, you're good to go, otherwise, you're going to need to set up an environment variable on your computer, so that Hierarchy Viewer is able to communicate with your device. You can also find the details for that, in the instructions as well. All right, so let's go ahead and start up Hierarchy Viewer, the first thing you want to do is hop into Android Studio. Then, in order to get the Hierarchy Viewer, we're going to want to start the Android Device Monitor. Now, you can do that through a menu, you can come up to the top here, click Tools, go to Android, and then select Android Device Monitor. Or you can click this little handy green little icon, that also does the same thing. So I'm going to go ahead and click that to start the Android Device Monitor. So here we are in the blank default layout of Android Device Monitor, now to get to Hierarchy Viewer, you're going to want to open up a perspective. So I'm going to do that, by clicking this menu here, and then clicking on Hierarchy view, and then click OK to launch it. All right, so here's the default layout for Hierarchy Viewer. If your screen looks different than this, you can go to the default view by clicking the Window menu, and click on Reset Perspective, that'll reset things for you. All right, so let's talk about the different panes that are available to you in the UI layout for hierarchy viewer. On the left here you're going to find the windows tab, you should see your phone and a list of running applications in it. Make sure you can see sunshine amongst the list, and then go ahead and double-click to load it up. Now as you can see, hierarchy viewer shows you,your entire view structure for the activity you selected, in this case this was the main activity of our sunshine app. Now, if you hover over here this is called the tree overview, and it gives you a bird's eye view of your entire UI structure. Now you'll also see this view port, which allows you to move around and change what shows in the detailed view on the left. This is the detailed view. As you can see when I move the view port, the details also change in the detailed view. This is a great way to zoom in on portions of your hierarchy. Now if you hopped over to the tree view, there's this other zoom control that allows you to zoom in and out on your structure as well, like so. I can also drag the layout to move the position, zoom in and out to get a little bit more context. Now when you click on a view, you can see all of its properties, in the view properties tab, that opens up here on the left hand side. I'm going to go ahead and expand it, and it's got these collapsible menus that show you various details about the view. For example, I can look at events and other details related to drawing on the screen. Things like alpha, things like pivot, things like rotation. Now, double-click on a view to get a preview of what's drawn to the screen of this particular view, that you've selected. Now, let's move over here to the lower right. This is the Layout view. Now, when you click on a view on the hierarchy, the Layout view highlights the same view in that Layout view. In other words, you can look at this Layout view as a wireframe off your device's screen markup. So you can select items in the detail view and see where they will be laid out on your device's screen. You could also go the reverse direction and click items here, and see items light up in the tree view on the top above it. So for example, if I click on this box right here, this corresponds to this view over here, and if I move the view pager, and zoom in a little bit, I can drill down into details about this view. Now unfortunately you can't go directly from hierarchy viewer to your source code, because it's actually connected to your running app, not the source itself. But each node shows the type and the ID, so that you can reference it later or find it later in your source code. All right, so I want to step back for a second and I want to make sure that you know that hierarchy viewer is really useful in two key ways. The first, it's going to help you understand your structure of your user interface from Android's perspective. And two, it can help you figure out, where you might have superfluous views, or opportunities to flatten your view hierarchy to save memory, and improve rendering performance. But now some real performance insights come from our per node profiling feature and hierarchy viewer. The tool models, the overall rendering process of the entire user interface, and provides rendering data for a particular node relative to the other nodes in that particular tree. All right, so lets go back over here to our detailed tree view. Now click on a root node of a sub-tree that you actually want to profile. Just for test purposes right now. I'm going to go ahead and click on this action bar container. Now in order to invoke the profiling feature of hierarchy viewer, you're going to want to click on this Venn diagram icon, right here. So, I'm going to go ahead and do that right now. Now, let's drill down a little bit more specifically. Now, as you may notice, each view gets three distinct dots, and they can be of different colors, green yellow or red, and we'll talk about what that means in a little bit. But, there's also a meaning to the order of the dots. Now the left most dot represents the measure phase of the rendering pipeline, the middle one, the layout phase, and the right most one, the draw phase of the rendering pipeline. All right, so lets talk about the colors now. Now the colors of the dots indicates the relative performance of this node, in respect to all other profiled nodes. So what do we mean by relative performance? Now let's take a look at this one. Greens means that for this phase of the pipeline, this view renders faster than at least half of the other views. Now, if you see yellow, it means it's in the bottom 50%. Now, if you see red, that means it's one of the slowest nodes in your view hierarchy. All right, so here's a little bit of insight. A red node is a potential problem in any situation where you would not expect slow performance. For example, in a leaf node or a view with only a few children, that should be a red flag. Now also remember, when dealing with a large hierarchy something does need to be the slowest node, so the question is, is it the node that you expect it to be? Also keep in mind that just because it's performing relatively slow, it might not be performing absolutely slow, that's where the actual numbers are helpful. All right, so now that you're familiar with hierarchy viewer, let's go ahead and try it on some sample code for this lesson. Now as you may know, when you're building a user interface for Android, it's generally good practice to keep your layouts as simple and as flat as possible. Fredo had some good advice. Remember that inflating your layouts is an expensive process. Each additional nested layout and included view directly impacts the performance and responsiveness of your application. So remember, gain information and then gain insight into your app's behavior. All right, so here I am back in Android Device Monitor and I've got the hierarchy view perspective open. And as before, let's go over to our windows pane here, select our device, and choose the activity we'd like to inspect. In this case, we're going to look at the mobile perf compare layouts activity. So I'm going to go ahead and double click that to load it up. All right, so I want to zoom in on the particular area of the layout here. In particular this root node here which is our linear layout. This is the root view group that's going to be displaying these two lines that we're looking at right here. Now I want you to notice the two different childs that come off of this parent linear layout. One of them represents the first line here of our chat interface, but it's implemented using a nested linear layout. Now the second child here corresponds to our second line in our layout. And instead of using a nested design, we've used a flat design using the relative layout view group. So, if we hop back into Android Studio, this is what it corresponds to in XML. We're back in Android Studio, looking at our source, and we're down in our layouts folder, and we're looking at the activity compare layouts XML file. Now, let me bring the phone back on the screen so we can compare. All right so we have a parent container that's a linear layout of vertically oriented items. So the parent container here is a linear layout and the orientation is for vertical items, so items that are going to be listed from top to bottom. So what I want to draw your attention to here is for our first chat line right about here, we have one type of implementation using more of a structured or nested layout. This is little bit more intuitive and might be similar to a way when you build an application in steps and it's very logical. So, for example, we start with a parent linear layout that's horizontal in nature and on the left, we're going to set up a our ImageView. And on the right, we're going to create another nested linear container to contain our text. But in this case, the orientation is going to be vertical instead of horizontal. So that represents the first slide. Now as I mentioned in hierarchy viewer, we also have the second line of our chat template here. And instead of using a nested structure, we've decided to take the same visual design and instead of build it in terms of a hierarchy, we've decided to look at all the elements in terms of boxes and see if we can describe them with their relative positions to one another. Now what this gives us is a flattened layout. So instead of two layers of depth, we have one or a flattened layout using this relative design. Now, what we've done here is a relative layout and we've specified our text now in relation to the image view or the placeholder for our avatar here. For example, this first line on the top is going to be the right of the avatar, and then similarly the second line of text is also going to be the right of the avatar. Now, we can also delineate which order we want the text items to follow. In this case we use the attribute layout below and this specifies that this particular text view actual comes below this particular one up here. All right, so what does that really mean? What does that gain for us in terms of performance? So let's hop back the hierarchy viewer. All right, so here we are back in our layout. Again, we've got the linear layout design up top and then the flattened relative layout design on the bottom. Now we can use the rendering/profiling feature of hierarchy viewer. We kind of get a sense of how each design behaves. So let's select the root node here. And we're going to click on the profile rendering button represented by this Venn diagram here. So let's go ahead and click it. And as you can immediately see, what I want you to notice is via the yellow color here. The linear layout design is slower than the relative layout design in terms of its rendering process measure layout and draw. So let's go ahead and click that a few more times. Again this linear layout design is imposing a bit more overhead on the rendering pipeline compared to the relative layout which is all green here. Let's sample it a few more times. All right, they're about even here. Let's try one more time. All right now, so it should be clear that when we profile this parent node here, and we look and inspect at the two distinct layout designs, we notice that the linear layout, or the nested version, is quite a bit slower, or at least clearly slower than our flat and relative layout version. So again, when you can see these opportunities to flatten in your layout design or when you are re-factoring, see if you can take steps to flatten your design when possible. So let's go ahead and apply this best practice to our Chattinum Lattinum design that we looked at earlier. Let's see if we can find some inefficiencies and flatten that layout to see if we can gain some performance. All right, so here we are, back in Android Studio again. Let's go ahead and look at the layout design for Chatum Latinum. So, there's actually three files that we looked at that make up the design, but the most important one that we want to see if we can essentially flatten is the layout XML for our individual chat items, or distinct rows. All right, so just to be clear, whenever I'm going to bring up the Chatum Latinum activity again. And the layout that we're concerned with the most here is the individual chat items that we see horizontally here. So let's go back and take a look. So here's the markup for our individual chat items. As you can see here, we have a similar design to almost the unoptimized version of the compare layouts activity that we looked at just previously. Now, this is a natural way to create a list item, again it's nested in structure or following something that's similar to the dom like the web. But this nested nature of linear layouts is not necessarily needed. And we can go ahead and flatten a bit more. So let's take a look at how we can do that, I'm going to bring up a branch that has an optimized version so we can talk about it. All right, so I've brought the phone back on this screen to kind of compare how we're able to achieve the same design, that originally started as nested, into a flattened version using a relative layout. Now as before, we have this image view in our layout that represents the chat avatar, but when we implement using a relative layout, this can be sort of like an anchor, that we specify some other items relative to him. For example, we have a bunch of text on this screen, we've got the chat author name, we've got a date and time, and then we have the actual chat text. And this is all declared in the IDs for the individual text views. Now, let's see how we can orient them in a relative nature. So, for example, we took the chat author name, and we used this attribute called layout to the right, to describe it to be on the right of the avatar. In similar we also did the same for the chat text, which is also specified to the right of the chat avatar. But then we need to determine that the core of the message, actually lies below this chat author name. So we use this layout attribute to say below the chat date and time. And then lastly, we have a chat date and time here, and we do a small little adjustment to make sure that stays to the right, using the align pair right attribute. So look at that, with this relative description and this relative design we've flattened our layout, achieved the same thing visually and gained a bit more performance, as you saw in hierarchy viewer. All right, so the main idea is that when you're revisiting and refactoring your layouts, see if you can look for opportunities where you might have inefficiencies like nesting, and replace them with implementations that are a bit more flat, like this relative layout design. All right, that was great, thanks a lot. Huh. So Chris, how did that rendering performance go the other day? Oh, man, everything's great. It's back to being superfast. Yeah. You know, I hate to say it I think you were right. Well Chris. It's not about being right or wrong, even though I'm right. It's about understanding that in the mobile application development space, it's all about trade offs, right? You have to make performance conscious decisions between the features you want and the applications and devices that people are going to be running on. You can't just run off into the weeds, make whatever you want, and then get mad at the system when it doesn't perform the way you want it to. Yeah, I know. That's totally fair. Oh wait. With all of that extra frame time, more animations. Well, wait, no, Chris, that's not what I was talk- Hey cool. Hey what's going on man? So I found this stack overflow post, that says if I change the syntax of my for loop, and use a pre-increment over post-incrementor, I'll get like a hundred x performance on my for loops. Pretty awesome yeah? Mm, that's not a thing, Chris. What do you mean? That seems really useful all I need to do is plus plus i versus i plus plus, I mean, then I get a boost for every loop I write. Chris you should probably stop now, I mean we are recording, it's, it's not a thing. Jeez. That's kind of harsh if you know better, why don't you show me then? Let me check this out. Okay, so here's the deal, you need to remember that Java running on Android is effectively executing in a virtual machine environment, which means there's lots and lots of layers of complexity going on here, from the precompiler to the compiler to the optimizer to the code itself, actually running on the device. What you're identifying here is something that we call compute performance. So, like, the performance of my computer? No, more like how the algorithms or the computing processes are performing. Which has everything to do with how the compiler is generating the code and how the virtual machine itself is actually executing it on the hardware. See, what this post has identified was a very, very specific instance where the compiler could make a pre-fetching optimization on gigantic four loops where a collection class has more than 20,000 elements in it. Effectively by changing the syntax there for your incrementer it was able to hint to the compiler that this particular type of optimization could actually be made. Totally. You know, I was hoping, I could use it in general, though. Well, see, it's not that easy when you actually want to get huge wins in computer performance scenarios, it actually means understanding what each piece of code is actually doing on the hardware. Which usually means, having to go and make small little changes throughout your entire code base, just to get the performance wins that you're actually looking for. Jeez, that sounds like a lot of work, I'm already exhausted. Chris, my friend it's the whole reason I'm bald. So let's start in a place very familiar to all of us, slow function performance. This is your basic computer science 101 concept of performance. That is, you've written some code and it executes more slowly than you'd actually expect. Now, this often happens innocently enough. I mean, you're focused on creating some code change to solve a particular problem in a particular way. But soon you realize that it's taking much, much longer to execute that code than you'd like. Now the primary reason that code can be taking too long to execute has everything to do with how the language and of course the associated hardware is handling the execution of your code. For example on some older hardware which we will not actually name executing a branching statement on a floating point comparison took almost 4x as long as doing the same thing on integers or booleans. The reason for this was the chip architecture. The part of the CPU dedicated to floating point calculations occurred after the Branching logic stages. Meaning that any floating point comparisons would have to wait until the end of the cycle pipeline, stalling the rest of the operations until it could finally execute its branching logic. But, listen don't freak out here. Modern hardware generally doesn't have these type of nuanced issues to deal with. But it does illustrate a very, very important point. How you write you code affects performance, depending on how the language executes on the hardware, all the way down to how your silicon chips are actually structured. Let me be clear on this. To optimize your code, you have to understand the system that runs it. Now, slow function performance generally comes in two flavors. Firstly, you have your single slow function form. Now, this is pretty straightforward. You've got some function that's taking like, 2x or 50x longer to execute than you actually want it to. This is actually pretty easy to fix. I mean, you find the slow function, take a look at its code, figure out what the problem is, and then try a few fancy ways to fix it. Now much harder to figure out is our second type. Death of a thousand cuts effectively this is when you have a hundred functions, each taking one millisecond longer than it should resulting in a hundred millisecond extra in your overall program execution. This type of problem is the hard one to track down and even harder to fix because you usually end up slogging through every piece of your executed code to find small wins that in the end, can make the difference between shipping and your company going under. Now, fixing these tiny performance problems is all about profiling. That is, timing your code to figure out what portions of it are running slower or longer than the others and then making some small tweaks and then timing your code once again. And, once you find one of the offending functions, you'll then need to start timing individual lines of code in that function and in all the functions that it actually ends calling later. Now this can get really gnarly unless you're an expert in the field but fear not. The Android SDK has some excellent tools to help you track down these problematic portions of your code so you can fix them up right. Let's take a look. Let me show you how you track out some compute-based performance problems in your apps. For this demo, we're going to hook up the Sunshine application with a profiling tool called Traceview. Let's go ahead and load it up. First, make sure, your device is connected. And then, start the application you want to profile. In this case, we'll bring up Sunshine. Okay. And now, let's hop back to Android Studio. And in order to access the trace view tool, we're going to want to start the Android Device Monitor. Now, you can either do that through the Tools menu by going to Tools > Android and Android Device Monitor. Or you can click this little green guy here up at the Toolbar in the Android Studio IDE. So I'm going to go ahead and click this button. Okay. So once Android Device Monitor is up and loaded, make sure you come over here and check your tabs and make sure that the DDMS tab is selected. Then you'll want to hop over here to your Devices pane and select the activity that you want to profile. We're going to go ahead and check Sunshine. Okay, so now I want to draw your attention to some icons here in the taskbar. Particularly this one right here, that looks like three arrows with a red dot on top of it. This is the button that we can press that has a little tooltip that says start method profiling. This is how we invoke Trace View. Let's go ahead and click it. You'll get a pop-up with two ways of profiling your app. You can either record every method's entry and exit which is very resource intensive down here, or you can do some sample based profiling. What that means is by default a profile is going to ping your application every one millisecond to find out and record what function is actually being executed. So let's go ahead and use the default settings. I am going to go ahead and click OK. So now that the profile is recording, let's go ahead and interact with our application and see if we can profile some actions. So I want to hop over here and interact with Sunshine a little bit. Oh, nice. Some pretty good weather here in Mountainview. Unfortunately not so good coming up this weekend. Looks like we have some rain in store. But, why don't we go down the coast and see how our friends are doing in Southern California? Oh, weird. They've got a strange winter, very untypical of San Diego. How about we check on our friends in Texas. All right, well maybe a little bit more warmer weather, there. 68 and clear. Not too bad tomorrow. All right. So let's go back to Android Device Monitor, where we want to stop our profiling. Now we can do so by clicking the same icon that we did to start it. It's got this black icon or black square on top of it now so let's go ahead and stop our profiling by clicking on it. Now, it might take a few moments to load the trace. Which is going to show up here in the tab at the top of your window. Just keep in mind it might take a little longer depending on how much recording you actually did. All right. Let's, so let's talk about trace view. Now trace view has two main components to it. It's got this top panel here called the typeline panel and then it's got this bottom panel with a lot of information. It's called the profile panel. Now the timeline is great in showing you how your code is executing over time. Each row that's shown here in the, in display, actually corresponds to a thread, and each color that's displayed corresponds to a particular method that's running. So for example here we have all the activity on our main thread. And we can, we can see spikes when methods were being started and stopped. What's even more useful is we can zoom down and we can try to figure out particular methods and how they're behaving. They kind of show up in these U shapes. With the left bar here that denotes when the method is actually starting and then another bar here on the right which actually denotes when the method is finishing. And the width of the bar actually represents how much time it took for that method to be executed. Now let's go ahead and select the particular method. Let's hop down here to the bottom of the trace view window and here we're going to look at some profiling data that shows up. Particularly, we get some information about what methods actually called the one that we've selected, which are represented by their parents in this blue highlight here. And we actually get some additional information about which methods are actually called within this one. So, in other words we called dispatch input event method with a native pole once which is what is selected at the moment. And for each of these methods that we have selected we get a whole lot of additional statistical information. For example, we have the exclusive CPU time. This is the time that's spent in our method itself. And we can use it to find specific issues for that particular method. Now the inclusive CPU time is for the particular method and all the methods it calls internally. This can help you find problems within the individual indication tree for the method that you selected. Oh, and another really useful statistic is how many times the method was called or called itself recursively. We can find this information if we scroll to the right and we've got this column here called calls and recursion. Again, that measures how many times the method was called or how many times it was called recursively. Now there's a whole lot of additional information here in this profile panel. And if you don't have a really big display it's kind of hard to see. So you have to do some scrolling back and forth to kind of find the metrics that you care about. And don't forget this little nifty search box that can help you kind of zoom in on the functions that you care about. Okay so that's a little trace for you let's go ahead and use it on some real code I want to introduce you to my two most favorite performance techniques, batching and caching. As we already talked about, some functions or operations have a specific amount of overhead involved with them that is different than the performance costs of the operation itself. For example, loading data into a new place in memory before executing on it, or sorting a set of values before doing a search through it. When executed multiple times, where multiples are really big number, this overhead can become a serious performance burden for your application. Batching is the process of fixing this performance problem by trying to eliminate the per execution overhead of these operations, kind of like sharing a car instead of everyone driving themselves, thus saving gas. This is most often seen in your code where you have to prepare your data before actually operating on it. Now, for example, let's say the most efficient way to find a value existing in a set is to sort it, and then execute a binary search on it. Now, wait to, to be clear, this isn't actually the most efficient way, but just stay with me, I'm trying to make a point here. Anyhow, the simplest way to do this would be to write a function that, given a set and a value, would sort the set and then search it to see if the value exists in it. Now, this may be fine for some performance level but let's say you've got like 10,000 values you want to test, and the size of the set is in the millions. Suddenly you're incurring a ton of overhead per test in the form of the sort. The answer here is pretty clear. You'd want to create a sorted version of the set once, and then allow all 10,000 values to be tested for inclusion after that point. This is batching in action. We factored out the operation that is repeated and do it over once. Now, similar to this is actually a concept known as caching and is by far the most important performance technique you can understand, mainly because it drives everything in modern computer technology. Take your computer, for example. The whole point of your RAM hardware is to provide a place to store information that's faster to access for the CPU than the hard drive is. Or take networking, and look at the modern internet itself, huge warehouses of servers called data centers exist all over the world. Their only purpose is to store or cache frequently accessed content so that your computer doesn't have to hit a server 12,000 miles away every time your friend in Egypt posts a picture. Well, unless of course you're in Egypt, but, you know, you get the point. Now in your code, the most common place that you can find optimizations for caching has to do with data that is calculated multiple times, but the result is always the same. For example, if you're in the middle of a loop that you're calculating the derivative of a four by four matrix, and that result is always the same, then you're actually wasting performance, recomputing it for each iteration of that loop. Instead, compute and save the results of that derivation outside of the loop, and then let your inner portions of the loop reference that cached result. The reason that I love batching and caching so much is that pretty much every performance improvement that you can think of, including the ones that we're talking about in this course, is effectively a variance of these two basic techniques. And if you're serious about becoming a performance ninja, then you better become a pro at what it means to leverage the awesome power of these techniques. So, let's get started. So the answer we're looking for here is actually the computeFibonacci method. So let's review what I did to figure it out. All right, so running Traceview on the following activity, and basically profiling the function that happens when I press this compute Fibonacci function. This is what the Traceview output looks like. All right, so here's an output that I got from running Traceview. You should see something similar. Notice this large pink area. This is a bad thing, this basically indicates that something is taking up a lot of CPU time on our main thread. So if you sort by exclusive CPU time, or by hovering over this pink area. You'll notice that the computeFibonacci method, which is coming from our caching activity, is the one that's actually occupying the most CPU resources. So this is something we want to fix All right, so what are good ways to fix this problem? Go ahead, and select all that apply. Now, we shouldn't be doing additional work on the main thread that isn't necessary, right? So, let's leave that thread only to handle user input and drawing to the screen. Now this is a good intuition in general, but for the sake of this example, let's see if we can optimize this function to run much faster and have less computer overhead using one of those handy techniques called shared. So let's go ahead and cache intermediate values. Now, it's important for performance that each individual function runs as efficiently as possible, but equally important for performance is when and where in your code that function executes. See, whenever you first start an Android application, the main thread of execution is created, this main thread is very important because it's in charge of running your code, dispatching events at the appropriate views, and executing your drawing functions, which we've kind of already talked about. Basically, this main thread is where your application was. Now the main thread is also sometimes called the UI thread, because this is also the thread your users interact with. For instance, if you touch a button on the screen, then the UI thread dispatches a touch event to the view, the view sets the button state to pressed, and posts a validate request to the event queue, then, the UI thread processes the request and notifies the button to draw itself in a pressed state. Whew! Now if you have any on touch event handlers, those will be executed in the middle of that huge flow, and the longer it takes your on touch functions to process, the longer it will take, before your draw function is ticked off, and the view is updated visually for your users to see. The takeaway here is that your input handling code is sharing cycles with your rendering and update code on this same thread. This means the UI can't draw while you're updating in the middle of a computation that may be dealing with touch events, network access, or database queries. In simple situations, this can cause you to miss the 16 millisecond window dropping a rendering frame and letting your user experience annoying lag. However if you end up pausing the rendering of your UI thread for more than five seconds, the users presented with the infamous application not responding dialogue. Which basically asks the user if they'd like to close your application, which I'm not sure is ideal in terms of user attention. Right? So how do you fix this? Well, you identify any pieces of work that do not need to be done on the main thread that is the work doesn't need to be completed in order for the draw to occur. And then you move that work onto a separate independent thread of execution, where it doesn't block the UI thread. For example, if pressing the submit button completes an order, then composing and sending the confirmation e-mail can be done on a separate thread. Now that may sound daunting but don't worry, Android has a cool set of APIs to make this easy for you to do. In this exercise, I'm going to show you how to use traceview, to identify a function that's messing with your apps frame rate. Now if you want to follow along, you'll need to download the compute and memory sample app from the linked in the instructor notes below. After you install the sample app, go ahead and press the slow on click handler button, you'll notice the familiar dancing pirate. Then go ahead and press the button labeled,display an image. See as you can see, the pirate's dance pauses and then resumes again when you do this, and then the Android image is displayed underneath the button. Just like before, it seems like pressing the button, relates to the pause in the pirate's dance, so hopefully, you know what comes next, and more importantly what tool to use. We're going to profile the app using traceview, so let's go ahead and do that. All right, so I want to come over here to my devices list and make sure that the compute activity's selected, before we start profiling, I'm going to click on the slow unclick handler to launch the activity, let's go ahead and start a trace, like so. Go ahead and hit OK, all right, so the trace is running now, let's go ahead and click this display an image button, and there we go. Let's stop our trace. So let's go ahead and look at our output. All right, so we have a lot of data here in our output, but what does all of it mean? Now here's a task, look through the traceview output and see if you can identify some methods coming from the sample app,that are using significant CPU resources. Jot down your top two answers here. All right, so let's take a look at some Traceview output. Notice this big section pf activity here, but let's talk about a few observations. You might notice that the top function here is the most resource-intensive when sorted. Now, there are a few others, such as this nativePullOnce. But these are system methods that we don't own. So if we go down a little bit further, notice that we have this nativeSetPixel and this nativeGetPixel. So let's see where they're coming from. Let's go ahead and expand a little bit. Ah-ha, here we go. So setPixel looks like it's being called from something within our busy UI thread activity, which is from the sample app code. And the same is the case for getPixel, which also seems to be coming from our busy UI thread activity. So now that we've identified that setPixel and getPixel come from our busy UI thread activity, let's go ahead and explore them further. So, getPixel and setPixel aren't methods that we wrote. So what parent method in the code is actually calling getPixel and setPixel? Write the method down in the box here, and make sure to use proper casing. All right, so the parent method is actually sepiaAndDisplayImage. So let's go back to trace view and see what we mean. All right, so here we are back in trace view. Now if we expand our collapsible menus here, we can actually walk up the parent calling stack and see that set pixel is actually called by sepia and display image. Now, this is within our busy UI thread activity. So, let's go ahead and see how we can optimize. So we've talked about how your code could be slow because the type of hardware that's executing underneath it, remember the whole floating point branching issue problem? Well, that's mostly a non-issue for today's hardware. There's one set of issues that you still need to worry about, that is, the performance of the primitives in the language that you are using. Take a fundamental algorithm such as sorting. Now there are many ways in which to sort, and some are better than others, depending on the circumstances. For examples, quicksort is generally faster than bubble sort except when you have a fewer than a thousand elements or take searching for objects in a large sorted list. Generally, the best way to do this is with a binary search. But completely different is finding objects in an unsorted array. Now instead of in comparing each object for the value you're looking for, you can use a hash function to find it immediately. Now this is all basic canon of modern computer science and data structures. And thankfully, modern languages like Java supply these containers and algorithms on your behalf, so that you don't have to rewrite the Murmur 3 Hash function or a quicksort over and over and over again. But let me reveal something here. In all of my years of programming, the one problem that consistently bites performance of your project has to do with the performance of these language-provided container objects. I mean, it's awesome, right? Java's providing you with an implementation of a vector class that you can push, pop, add, and remove objects as you see fit, but in order to get that flexibility, it has to use a linked list structure under the hood, which has a unique set of performance characteristics. As long as you are only operating on the front of the list, it's super fast. But if you're trying to insert or remove in other places, it's going to default to the worst possible time. The point is that just because the underlying system provides these containers doesn't mean that they're performant with respect to how your program is going to actually be using them. James Sutherland published a series of microbenchmarks on the performance of specific data structures provided by the Java framework and found that there's some differences in performance versus functionality that people need to be aware of. For example, he found that Hashtable performance was about 22% faster than HashMap performance, depending on how you're actually using the containers themselves. The point is this. Have you profiled your container classes that you're using in your code? Are you confident that they are using the absolute fastest container for what your code is actually doing? Mm, yeah, that's what I thought. But the good news is that you can gain visibility into the performance of these containers with some handy profiling MPIs in Android. So let's see if Chris's code stands up to our scrutiny. In this exercise, you'll see the performance implications of choosing suboptimal data structures in containers when building an app. To do this, we use tools in the Android SDK to identify performance issues related to an inefficient data structure choice. Now as developers, it's easy to overlook the performance impact that comes with writing code to store and manipulate your app's data, so let's take a look at a situation that explores this problem and apply a performance mindset to it. First, make sure you download the compute and memory sample application linked in the notes below. For this example, we focus on the runtime of a method that generates a list of numbers ranked by their popularity. For demonstration purposes, this method is invoked when we press the following button, dump popular numbers to log. Now similar to previous examples, the code seems to impact the frame rate of our dancing pirate via slight stutter. And if you take a look at logcat, you'll see it running via the tag popularity dump, like so. Let's take a look at what is happening under the hood and learn to measure how fast this code is running. We want granular measurements of exactly what's happening when we click that button, so we use the trace classes, begin section, and end section methods to specify exactly where we want the start of measurements and where we want to finish them. First, we zero in on the code that is computing the popularity ranks, which is this method right here. See the instructor notes below for the code stubs we'll use to instrument the code and measure its runtime. After the next two quizzes, we'll run Systrace and retrieve the running time in milliseconds. All right, so I have a question for you. Now, if you're going to go ahead and compute the running time of the dump country by population ranks method, then Trace.endSection should go after which line? All right, so here's where you'd want to place it. So you'd actually want to call Trace.endSection after line 51. In other words, line 52, right here. So go ahead and run systrace on this code. Now in this case, how long did the method dump country population ranks take to run in milliseconds? Write your answer in this box here, and remember to use numbers only. All right, so here's a screenshot of our output of systrace. Now for us, the unoptimized code took about 23 milliseconds to run. Now, I think we can do better than that. So let's go to the next section to find out how. So did you see an improvement? We certainly did. Our optimized code in systrace took about 8 milliseconds. That's almost one-third the original 23 milliseconds. Awesome. You know, I gotta say, Trace View is an awesome tool. [LAUGH] I was using it to profile my app last night and I got so much awesome data about my function performance. Who knew that the vector class is slower than the array list? You know, that's the trick with Trace View, man. I mean it's got a ton of data in there, right? And- Yeah. Makes it really hard to use because of it, but once you actually start getting rolling and actually start understanding what it's doing. It's kind of like free awesome in a box, or better yet, like a one stop performance shop. You know, I like that, like [SOUND] we should like, make that into a meme or something like that. Hey, you think if I put that on a t-shirt anybody will buy it? Yeah, maybe, I mean, yeah, perhaps. Oh. But. [LAUGH] I'm going to do this. Oh no, no, no, no, no! Aw. Yes! Come on, man. That's how it's done, man. Best three out of five. Heh. If you think it's going to matter. Eh. Come on. [SOUND] Oh, hey Colt, what's up? [INAUDIBLE]. Yeah can I get a latte? [INAUDIBLE] Oh, you're right, yeah. Dude, by the way, I have to say, Java's manage memory is so cool, I saved so much time not having to care about when my objects are released. [INAUDIBLE] Yeah, it's like the perfect setup for a programmer. Allocate objects, this doesn't freeze them. I can focus on doing cooler stuff. [INAUDIBLE]. Yeah and the best part, it's for free, the system just does it. [INAUDIBLE]. No, seriously, the manage memory is free, you don't have to pay a cent. [INAUDIBLE]. Oh, dude, just embrace it, I can write code however I want, Garbage Collector does the rest. [INAUDIBLE]. Oh. [INAUDIBLE]. Yeah. [INAUDIBLE]. You're right. [INAUDIBLE]. Oh, I see, so you're saying there's a tax for garbage collection events. [INAUDIBLE]. Yeah when they happen in a short period of time. [INAUDIBLE]. Okay, yeah I'll be more careful when I write my loops. [INAUDIBLE]. Yeah, I totally hated to have draining into my apps framerate. [INAUDIBLE]. Oh so how do I spot these problems then? [INAUDIBLE]. Run the tools, got it. [INAUDIBLE] Okay, sounds good, I'll see you. Oh, hey wait, make it a double, I'm going to need it. Now that all of our code is running fast and awesome, let's talk a bit more about memory and how it affects the performance in our system. Many programming languages that are known for being close to the hardware, or rather, high performance, like C, C++, and Fortran, usually programmers to manage memory themselves. Effectively programmers are responsible for allocating a block of memory and then sometime in the future de-allocating it when they're actually done using it. Since you define when and how much memory to allocate in free, the entire quality of managing memory depends on your skills and effectiveness. That's a lot of responsibility. And the reality programmers aren't always the best at keeping track of all those bits and pieces of memory. I mean think about it, product development is a muddy and crazy process and often memory ends up not getting freed properly. These un-liberated blocks of memory, are called memory leaks and they just sit around hogging resources, that you could use better or somewhere else. To reduce this chaos, stress, and sometimes big money losses, caused by memory leaks, managed memory languages were created. The run times of these languages track memory allocations and release memory back to the system when it's no longer being needed by the application itself, all without any intervention from the programmer. This art, or rather science, of reclaiming memory in a managed memory environment is known as garbage collection, this concept was created by John McCarthy in 1959 to solve problems in the lisp programming language. The basic principles of garbage collection are as follows, number one, find data objects in a program that cannot be accessed in the future for example, any memory that is no longer referenced by the code. And number two, reclaim the resources used by those objects. Simple concept in theory, but it gets pretty complex once you've got 2 million lines of code and four gigs worth of allocations. Now think about it, garbage collection can be really gnarly, I mean, if you've got some 20,000 allocations in your program right now. Which ones aren't being needed anymore? Or better yet, when should you execute a garbage collection event to free up memory that isn't used? These are actually very difficult questions, and thankfully we've had about 50 years worth of innovation to improve them, which is why the garbage collector in Android's Runtime, is quite a bit more sophisticated than McCarthy's original proposal. It's been built to be as fast and non-intrusive as possible. Effectively the memory heaps in androids runtimes are segmented into spaces, based on the type of allocation and how best the system can organize allocations for future GC events. As a new object is allocated, these characteristics are taken into account to best fit what spaces should be placed into depending what version of the android runtime you're using. And here's the important part. Each space has a set size, as objects are allocated, we keep track of the combined sizes, and, as a space starts to grow, the system will need to execute a garbage collection event in an attempt to free up memory for future allocations. Now it's worth putting out that GC events will behave differently depending on what Android runtime you're using. For example, in Dalvik many GC events are stop the world events, meaning that any managed code that is running will stop until the operation completes. Which can get very problematic, when these GCs take longer than normal or there's a ton of them happening at once, since it's going to significantly eat into your frame time. And to be clear, the Android engineers have spent a lot of time making sure that these events are as fast as possible to reduce interruptions, that being said, they can still cause some application performance problems in your code. Firstly, understand that the more time your app is spending doing GCs in a given frame, the less time it's got for the rest of the logic needed to keep you under the 16 millisecond rendering barrier. So if you got a lot of GCs or some long ones that are occurring right after each other, it might put your frame processing time over the 16 millisecond rendering barrier, which will cause visible hitching or jank for your users. Secondly, understand that your code flow may be doing the kinds of work that force GCs to occur more often, or making them last longer than normal duration. For example, if you're allocating a hoard of objects in the inner most part of a loop that runs for a long time, then you're going to be polluting your memory heap with a lot of objects and you'll end up kicking off a lot of GCs quickly, due to this additional memory pressure. And even though we're in a managed memory environment, memory leaks can still happen. Although they're not as easy to create as the other languages. These leaks can pollute your heat with objects that won't get freed during a GC event, effectively reducing the amount of usable space you have and forcing more GC events to be kicked off, in a regular fashion as a result. So that's the deal, I mean, if you want to reduce the amount of GC events that happen in a given frame, then you need to focus on optimizing your apps memory usage. >From a code perspective, it might be difficult to track down where problems like these are coming from, but thankfully, the Android SDK has a set of powerful tools at your disposal. Let's take a look. Let's go ahead and look at a tool called Memory Monitor. Memory Monitor is a tool that can show you how your application is using memory over time. Let's go ahead and start it. As with other demos, if you want to follow along, make sure you start Android studio. Connect the physical device, enable debugging, and then make sure your Sunshine application is in the forefront. So if we want to start Memory Monitor, we can do so through the menu, like so. We're going to go to tools, Android, Memory Monitor. Or if you have an icon here at the right, you can go ahead and click that. So let's do that. Okay, so the tool opens as a tab at the bottom right of your Android studio window. And if it finds your application, it's going to immediately start recording memory usage right from your app. Just like you should be seeing right about here. At the top left of your Memory Monitor window, you will be able to toggle your currently connected devices. And on the right, you can select the application you want to monitor. The stacked graph that takes up most of the window shows the total memory available to your application. The amount of memory that your application is currently using is shown in dark blue. And the free memory, or unallocated memory, that's available to your application, is shown in light blue or light gray. The graph updates continuously to show you changes in memory usage. And it shows how much memory the app has available to it over time. Now, in a world where your app isn't doing much of anything, you should see a flat graph like this one. >From a performance perspective, this is actually an ideal scenario. But as your application allocates and frees memory, you'll see that the allocated amount changes in your graph. And if your app suddenly needs a lot more memory, the overall memory allocated for your app will also increase, represented by this box here. Because if it didn't, your app would run out of memory and essentially crash. Now anytime you see the allocated memory drop by a significant amount like here, that's a pretty good signal that the garbage collection event has occurred. Now in this example, the garbage collection looks pretty healthy. On the other hand in this graph it looks like there might be some problems. You see, the app allocates a lot of memory within a short time, then frees that memory almost immediately, creating these narrow, skinny spikes. And it does it over and over. This means that your app is spending a lot of time garbage collecting. And the more time you spend doing GCs, the less time you have to do other stuff like rendering or streaming audio. So let's see what this looks like live. Okay, so here we are with memory monitor running on Sunshine and once we start tapping on a day and then looking at the detail view and then say, tap the back button or say we do this a few times. We're going to see that our memory starts to fill up. Just like around here. And say we were to go ahead and change the zip code a few times just to get some new data. And go back and check out some more detailed weather information. Cool, looks like it's going to be clear on Wednesday. Ooh, a little rain on Friday. So gradually we're seeing how memory is filling up. And eventually, it will actually grow until we actually have no more free memory left. If we keep going, at this point, a garbage collection event will be triggered to free up a chunk of memory. You can see this drop happen right here. Now remember, this does not free up all the memory because Android's memory management system is generational in nature. In our nifty tool, we can actually force individual garbage collection events. There's this garbage truck tool here on the top-left of the Memory Monitor. If you press it, it's going to go ahead and trigger an individual garbage collection. Notice what's happening here on the right of the graph. Now, we can go ahead and actually press it a few times. And if you do that a few more times, you can free all the freeable memory like so. And this seems to be back to our original state. Okay, so next, we're going to take a look at memory leaks and the heap viewer tool. And finally, we'll use all of these together to find and fix memory leaks in some real code. One of the best things about Android's Java language is that it's a managed memory environment that is, you don't have to be super careful about handling when objects are created or destroyed. While this is generally great, there's some hidden performance problems lurking under the surface here. Now remember, the memory heaps in Android's runtimes are segmented into spaces, based on the type of allocation and how best the system can organize allocations for future GC events. And each space has its own reserved memory size. When the combined size of an object in a space begins to approach its upper limit, a garbage collection event is kicked off to free up space and remove unneeded objects. These GC events aren't generally a noticeable problem to your performance. However, a lot of them recurring over and over and over and over again can quickly eat up your frame time. The more time you're spending doing GCs, the less time you have to do other stuff like rendering or streaming audio. One common situation that developers can fall into that cause a lot of these GCs to occur is known as memory leaks. Memory leaks are objects which the application is no longer using, but the garbage collector fails to recognize them as unused. The result is that they stay resident in your heap, taking up valuable space that's never freed up for other objects. As you continue to leak memory, the available space in your heap's generation continues to get smaller and smaller and smaller, which means that more GCs will be executed more often to try to free up space for normal program execution. Finding and fixing leaks is tricky business. Some leaks are really easy to create, like making circular references to objects which the program isn't using. While other are not so simple, like holding on handles to class loader objects as they're being loaded. In either case, a smooth running, fast application needs to be aware and sensitive to memory leaks that may exist. I mean, your code's going to be running on a federation of devices and different types, and not all of them are going to have the same memory footprints and sizes. Thankfully, there's a simple tool that's available to help us see where these leaks might exist inside the Android SDK. Let's take a look. Okay. To get more information on the state of our memory and the objects that are taking up space, we can use a handy tool called Heap Viewer. Now with Heap Viewer, you can see how much memory a process is using at certain points in time. Now as before, if you want to follow along, go ahead and start Android Studio and bring sunshine to the foreground on your connected device. In order to start Heap Viewer, you'll want to start Android Device Monitor first and there are a few options to do that. One way is through the tools menu where you can click on tools, android, and android device monitor. Or, you can click on this nifty android icon here in your tool bar at the top. So I'm going to do that and android device monitor is starting, then we're going to want to go ahead and click the DDMS tab. The heap viewer is one of the DDMS tools and we're going to go over here to the left. And we're going to select the app we want to profile, so we're going to select Sunshine now I'm going to pull up this panel down here. So once you have Sunshine selected you're going to want to select this heap tab to get more information. Now initially you might not see much, but you notice this little. Hint here at the top that reads Heap updates will happen after every GC for this client. Why don't we go ahead and click on this and cause a GC to update your data. Whoa, look. We have all this new information now. Now the table updated and shows you what data is currently available and alive on the Heap. If you want to get some more details go ahead and select a single data type. I'm going to click this class object. Now you'll see a lot of data will update in the panel down below. You can now see a histogram for the number of allocations and also the specific memory size for that data type. In this case we're talking about the class object. Now the heap viewer is really helpful to see what. Types of objects your application has allocated, as well as how many and what sizes they are on the heap. Again here, see we see the total sizes. Of particular types on our heap. Like for example there's over 1400 two biter arrays on our heap, that's taking about 120 kilobytes. Whereas there are only 27 one byte arrays and it's roughly only taking up about two megabytes. Now the heap viewer is really helpful to see what types of objects. You application is allocated. As well as how many, and their respective sizes on the heap. For example, if we look here we have 27 one byte arrays that are taking up roughly two megabytes of data. And then we have about 2000 four byte arrays that are taking up currently 228 kilobytes of data. This information is super helpful when you're trying to track down memory leaks. All right, so let's talk about memory leaks. Memory leaks are sneaky. They can be slow and insidious, sometimes taking days or weeks before you even realize that you have one. In fact, you might only realize memory's an issue when your users start complaining about mysterious slowdowns that happen after using your app. Don't let this happen to you. Fortunately, with some patience, a perf mindset, and the right tools, you'll have the opportunity to abolish these leaks from your app, period. We'll use Memory Monitor to watch the behavior of a leak in action, and then in the next video, we'll use Heat Viewer to gain initial confirmation. Now let's look at a micro example of what a leak can look like, and see how the SDK tools can help us identify such a leak. In this example, we're going to go ahead and rotate the device for a few minutes and profile it with Memory Monitor. This is by design to showcase a common leak situation that can arise during the creation and destruction of an activity. We can intentionally trigger this cycle by changing the device's orientation. And yes, I know, it may seem that this is a totally weird thing to do, but we're going to do this to demonstrate how a leak may happen and to show how they can be slow and insidious. Now, in the first pass, the leak slowly consumes the free memory available to your app, until eventually it causes a garbage collection or GC event. More important, the key thing to notice is, the garbage collector isn't able to reclaim that much energy due to the leak in the app. And then, eventually, a second GC event occurs much sooner, about 30 seconds later. Now, notice when the leak consumes all of the free memory, Android actually adjusts and grants the app a higher memory ceiling. While this is a nice adjustment by the system, if the leak isn't fixed, it will keep consuming memory until the system can no longer allocate any more. This will slow the performance of the device and eventually lead to your app crashing. You can wait a little bit longer, and the third GC will occur. And then a fourth somewhat similar to the first two. Now, as you can see, the pattern continues, and more memory is allocated by the system. You can also similar behavior using Heat Viewer. Using heap viewer, we can see that after the first GC, only 1.39 megs is free. This may indicate that the garbage collector wasn't able to reclaim much memory due to a leak. After a second GC event, heap viewer indicates that the system has decided to accommodate a larger memory footprint for this app by allocating more memory. Increasing the heap size to 32 megabytes, which is up from 20 megabytes in the first GC. This time, we have 12.9 megabytes free in our heap. At this point, the system is dynamically accommodating for the larger memory footprint of this app. If the expansion repeats, this may lead to an app crash if the system can no longer allocate more memory for the app. So remember, memory leaks are slow and they're insidious and require time and the proper test environment to confirm. Also, keep in mind that sometimes a pattern like this might represent a legitimate use of memory. For example, imagine an application that was designed to manipulate large graphics or photos. The takeaway here is be on the lookout for slow leaking memory, but always weigh the data you collect, against the memory implications of your app's core functionality. Now at this point, you should understand how memory leaks manifest in the SD. At this point, you should understand how memory leaks manifest in SDK provided tools such as Memory Monitor and Heap Viewer. But you might not know where they originate. Here are some best practices you can take to avoid a leak. Track the life of your objects throughout your code and clean up references when you no longer need them. Okay, so in the next slide, we'll identify what might be causing this leak. Now what's also interesting, is that you can use the allocation tracker tool to identify the extraneous memory bloat, that arises from stale views residing in memory. Now as you can see here, I've selected a common set of objects or classes that are still residing on the heap. Now these objects are put onto the heap, when we call onCreate on this particular activity. Now each time the device is rotated, a new activity is created and thus a similar set of objects are basically inflated and put on the heap. So if a leak exists, and we rotate the device, the garbage collector won't be able to cle, clean up these items and will essentially replicate a large set of these on, on the heap. An allocation tracker will help you see this. Now that we've found a way to clean up all those nasty leaks, we run into an even larger problem called, memory churn. Remember that each heap space has a finite size that limits the number of objects that it can accommodate, and as the size of the heap gets too big, a GC event is kicked off to eliminate unneeded objects to free up memory. Now, memory churn is a term used to describe the process of allocating lots of objects in a very short amount of time. For example, if you're allocating a load of temporary objects in the middle of a for loop, or you're allocating a bunch of objects in your on draw function. This is effectively the same problem as an inner loop, any time the screen needs to be redrawn or an animation is occurring, you'll end up with calls to these functions every frame, which can quickly add up, adding pressure to your heaps. In both cases, we've created a scenario where a high volume of objects can be created in a very short period of time. And depending on how many of these objects are created, or their size per object, you can end up quickly consuming all of the available memory in the young generation, forcing a GC to kick off. And as more of these are kicked off, the more of your precious frame time is going to be eaten up by them. As such, it makes sense that for a high performance application, you would want to identify and move allocations out of inner loops or any code that's going to be executed repeatedly. To make finding these allocations easier, Android Studio has a handy tool built just for this problem. All right. Here we are again with another handy tool. Now looking at a heap view of your apps' memory allocations is a great way to understand where most of the data is going and what type of data is being allocated. This helps you find data that is allocated and maybe shouldn't be. But sadly, heap viewer doesn't tell us exactly where in your code that data is being allocated. So for that, we need a tool called Allocation Tracker. As usual, go ahead and start Android Studio, connect your device, and load Sunshine into the foreground. Okay. So if we want to get to the tool in Android Studio, we can click on the Android tab, which is at the bottom left of a standard Android ID window. This is going to open the Android DDMS tab. I'm going to go ahead and enlarge this so we can see this a bit easier. So, the DDMS tab shows the devices and logs related to your device. Now depending on what you've been working on in Android studio, this tab will show different views. Make sure the Devices tab is the one that you have selected. And we'll go ahead and select the Sunshine app. Now next in the left margin go ahead and click the bottom most icon. This one right here. The icon has a start allocation tracking tool tip. Okay. So, the DDMS tab shows the devices and log. Depending on what you've been working on in Android Studio, this tab will show different views. Make sure the Devices tab is selected and select Sunshine. Next, we're going to want to go to this left margin and click the bottom most icon. This has a tool tip that says start allocation tracking. I'm going to go ahead and click it. And see, we're enabling allocation tracker here. All right, so go ahead and bring up your Sunshine application and go ahead an interact with your app for a little bit. So let's go ahead and take a look at what the weather is like in Cleveland. Unfortunately looks like there's some rain tomorrow, but a high of 74, humidity is 93. Ooh, sticky. And we see rain Wednesday, still pretty warm. And then rain on Saturday, but still pretty warm. Fortunately, less humidity then. Looks like you got warm and rainy weather, Cleveland. All right so go ahead and you can finish interacting with your app a few times. And when you're done, click on the same button as before. And the tool tip should say Stop Allocation Tracking. Now depending on how long you've been doing stuff. Processing the data might take a little while. And the button will stay depressed. Don't click it multiple times. Just be cool and hang out for a bit. All right. It looks like our's finished. Now if you look closely up here, a new tab was created in our IDE. It's labeled something like DDMS followed by a large number that we have it right here. Now, this view lists all the allocations that occurred during that sampling period when you were interacting with your application. And each row in this view represents distinct allocation. Now the order column is going to tell you. When the allocations happened. The allocated class column is going to show you what type of data was being allocated, the size of it, and you'll also have some information telling you what thread made that specific allocation. Lastly, the allocation site column tells you what function in your code actually allocated that memory. For example here, we select integer. The value of method is what made this allocation of this integer. Now if you click on an allocation, you can see its full call stack. There's a lot of information in this table. If you don't see everything, you may need to resize the panels. Now, you can also move the columns around to your fitting to get the ones that you care about within view. Or you can sort columns by clicking on the header of a column. Now obviously this tool is super handy for tracking down memory churn. Let's go ahead and take a look at this in practice. For this exercise let's launch the memory churn activity. Oh hello there pirate. Now press the button, do interesting things with arrays. I'm sure you noticed that the dancing pirate hangs or pauses, but then eventually resume dancing. As you know, this is Jank and it's bad for many reasons. So let's go ahead and fix it. Now this should sound familiar, so go ahead and profile this activity using trace view and when you're done, you should see a tool output similar to this one. Notice the frequent GC events that are happening within a short period of time. Now given what you know about garbage collection, what do you see in this timeline view that might be hurting app performance? Write your observations in this box. Now if you guessed too many and too frequent garbage collection events, then you'd be spot on. Also note that we could capture this memory monitor as well. Now this screen shot illustrates how memory churn would manifest using the memory monitor tool. Now that we've used the SDK tools to capture enough data to know when we have a memory churn situation, let's zero in on the code that's causing it. Well it turns out TraceView offered us a clue. Let's take a closer look at the profile view, when selecting a method on the main thread. Now if you were to scrub the main thread's methods, you'd find multiple recurrences of Java character ray copy operations, like this one. Now walking up the call stack, we get more confirmation that the array copies were used to enlarge a string buffer. So now let's look at the source code for the MemoryChurnAcitivity. As you can see here in the OnClickListener, we go ahead and call this function imPrettySureSortingIsFree, so let's take a look at that code. Now, in this method aptly named imPrettySureSortingIsFree, the code is constructing a new string, one cell value at a time, using string concatenation. See the instruction notes for the code I'm referring to. But, here's specifically, where the concatenation occurs. Now, it might look harmless at first, but, why would this code lead to memory churn? The frequent garbage collection events are the result of two things. Number one, each cell value concatenation requires the creation of a new character array and this is compounded by it occurring in rapid succession within the loop. So, it's also this one. Now, you can also confirm this character array bloat via allocation tracker. We're going to go ahead and improve the code and let's see what happens in the next video. We can make a small adjustment in our code to prevent excessive churn. Let's take a look at a comparison view. Rather than concatenate one cell value at a time to build each row, let's use a StringBuilder instance, and construct each row using a single string. Note that the StringBuilder's instantiated outside the loop, and thus its memory is allocated once. And then we simply use it as a buffer for each iteration of the loop where we first clear it, and then we append a single string of ints to represent the row for that loop iteration. Now see the instructor notes for more details into this code segment. All right, now it's time to verify. You want to go ahead and load the improved branch of code, which is called memory_churn_optimized, into both trace view and memory monitor to confirm we've reduced the amount of GC's occurring in the short time window. You may also use allocation tracker to verify. If you use allocation tracker, or if you got something unexpected in trace view, or memory monitor. Share a screenshot of your output in the discussion forums. We're interested in seeing what you've got. Now for us, even with these changes, the Perf pirate still pauses. But this time for less time. Now this point, this also might mean that this function is probably a good candidate to be backgrounded. To recap, you learned how to use multiple tools to diagnose memory problems. Now we started with our familiar friend Trace You, but you also learned about some memory-specific tools including Memory Monitor, Heat Viewer Allocation Tracker. But most importantly, you want to evaluate your problematic code against several tools and seek multiple perspectives as to what is actually happening underneath the hood. Finally, it's important to understand each tool has a strength. Memory Monitor's a great way to get a dynamic view of your memory over time and Heap Viewer's great for showing what exactly is on your heap. And lastly, Allocation Tracker shows you where a particular location came from your code. So let's change this. This line is- Man this managed memory thing is so cool, so much better than C++ and a ton faster. I think I never have to worry about performance again. [SOUND] Wow, what a day. Hey Chris? What's going on? Oh, hey, what's up? I got tell you, man, I'm really impressed with this application you wrote, that actually shows where all your coworkers are in real time. This is pretty cool, like it even shows me as I'm moving it around. Well, cool, thanks man, yeah, it was fun to make, the key is to keep an open connection when the phone screen is off, so that we can update your location in real time. Whoa, no, tell me you're not doing that. Well, you know, there were some problems stopping the GPS when folks entered the bathroom, but I think that's a small. Dude, Chris. To fix. My battery just died. Oh, dang that sucks maybe it's a dud. Dud, Chris this is a brand new phone, your application ate through my entire battery in eight minutes, are you even taking into account the battery churn? Those open GPS and networking connections are taking up? I thought so, I ran it through a bunch of tools. Oh wait, timeout. What, what tools are you using to measure battery, Chris? You know. The usual ones like Logcat. [LAUGH] Okay. Stop what you're doing, we're going to go fix this now, because this is an abomination, all right, come on, let's get going. 'Kay. Oh, bring your backpack, you're going to need it. As your mobile device is busy executing tasks, calculating how to split a bar tab and uploading photos of your cat, the underlying hardware is effectively pulling energy from your battery to accomplish this work. And, as we've all seen, the more work your device does, the more battery it pulls and sooner rather than later, your users are left holding onto an uncharged phone that doubles as an expensive doorstop. The key to writing applications that are light on battery drain has everything to do with understanding how the process works under the hood. In the electrical engineering world, this action of a hardware pulling energy from the battery to execute tasks is called current draw over time and anyone who has an undergraduate in electrical engineering will tell you not every action on your device draws the same amount of battery in the same way, over the same period of time. To prove this, let's take a handy Nexus 5 device, turn it on airplane mode and let it sit on our desk. If we leave the phone like this, doing nothing, we'll roughly get a month of life before the battery is completely exhausted. Feel free to try this one at home, kids. We can consider this the baseline in terms of battery life but that's not really how these devices are typically used. As soon as your active, you're going to be eating up more battery. Now active in this context includes things like the CPU doing work, cellular radio transmitting data, and the screen itself being awake. So the question is this, what tasks are my application doing that's eating up the most battery? Sadly though, that's not easy to answer. Monitoring battery drain at the hardware level is a catch-22 because of course the monitoring itself needs to drain battery to execute the actions to record how much battery is being drained and as such, most mobile devices don't do this. The only real way to gather these types of statistics on battery draw is to attach a third party piece of hardware to your Android device, which can record the operations without using up the phone's power to do so. Now when we do this, we actually get some really interesting information. For example, when a Nexus 5 is in standby mode, there's actually not much power drain going on, right? But when we wake the device up or turn on the screen, we can see a huge power spike in the battery monitor, which I mean, you kind of expect, right? Turning on LEDs, painting the screen and then all that CPU, GPU work required to draw to the screen isn't free from a battery perspective, which by the way is completely different from when the application wakes up the device, say if it's using wake clock, alarm manager or the job scheduler API. When the device is asleep and it's woken up through one of these APIs, you'll see an initial battery spike as the processor first wakes up, followed by a bit of work as it's executed. Now, it's important to note that once the work is done, the device will go back to sleep on its own, which, is really important. Keeping the device awake for long periods when doing little or no work will easily chew up your battery life. Now your cellular radio, on the other hand, is a completely different beast in this regard. When your device tries to send data over the mobile network, you can see that there is a quick wake up cost associated with getting your mobile hardware ready, followed by a large spike for sending out a data packet and then another large spike for receiving one back. And because getting the radio started is so expensive, after it's done executing work, it will stay on in a wait state for a short period of time in case there are more packets ready to come in right away. So all this is great data but most developers don't have access to this kind of equipment, right? I mean but with the L release of Android, you've got a whole new set of tools to help you optimize the battery draw of your application. Let's take a deeper look at Battery Historian. Hey, glad you're back. Let's talk about another useful tool that'll help you gather data and get better insight into how your applications using energy. It's called Battery Historian. Basically, you're going to use ADB to dunk data from your phone. And then use the Battery Historian tool to convert that data into a nice HTML table that you can view in your web browser. Now Battery Historian is a separate open source python script that you're going to need to download from GitHub. So. So let's go ahead and do that. Okay. So here we are at the GitHub page for Google's version of Battery Historian. Now in order to download this file, we want to come down here to lower right and click this download zip button. For this demo we're going to work from the desktop. But you can use any directory of your choosing. Now to get started let's go ahead and double-click this zip file which is going to extract the folder with the python script that we care about. Okay. Let's go ahead and open the folder. As you can see there's a license file, as well as the historian python script. Let's go ahead and drag this script to the desktop. Okay, that's it for setup. We should be all set to use Battery Historian. Now the next thing you're going to want to do, is make sure you connect your phone to your computer. Also make sure that USB debugging is turned on. I'm going to go ahead and run ADB devices just to confirm. All right, the next steps will work from any terminal window. But if you have Android Studio opened like I do, you can actually open a terminal right within the ID, like so. Go ahead and cd to your desktop. Now as a precaution, we're going to go ahead and shut down our ADB server. The way you do this is by typing adb kill server. Now this is an important step because while we're developing, lots of stuff can be turned on that might cause conflicts, while trying to do battery recordings. So to be safe here, we want to start ADB clean. All right. So I'm going to go ahead and type ADB devices. As we can see, ADB is restarting, and our phone is attached. If you don't see any devices, make sure your phone is connected and your USB debugging is turned on. Then go ahead and kill and restart ADB once more. Now the next thing we want to do is reset the battery data gathering so you can start from a blank state, we do it like this. So I just type this command adb shell dumpsys batterystats, With the reset option. This is going to go ahead and reset all the battery gathering data. Now if you don't do this, you're going to get a lot more data than is actually useful or relevant. In other words, you're going to see a lot more noise than you want when you're reading your battery stats. All right. So now you can go ahead and disconnect your phone from your computer. When you go ahead and pull this cable out right here, so you don't draw any current from your computer instead of the battery. Right so we're back in Android studio. Let's make sure that our phone is reconnected. So we're going to go ahead and type ADB devices once more. Just to confirm that it's back here in our list. All right so now it's time for the real fun. Let's go ahead and look at some battery statistics. And the way we do that is we run this command. You type in ADB shell. Dump Sis, which is stands for dump service battery stats. Let's go ahead and pipe that to a file. Now the next thing we're going to do is run the same command again, however we're going to restrict the data that it outputs. By passing in the package name of the application we're profiling. Let's do it for sunshine. All right, so with these commands we've partially generated some data that we're. going to use. Unfortunately the data in these files aren't very readable. So we're going to use the historian Python script to extract the battery of information and turn that into some HTML that we can then view in the browser. So this is how we do it. All right, so let's go. Go ahead and invoke that Python script called historian.py that we downloaded earlier. That's the battery historian script. So I'm going to type in python historian.py. Let's look at the sunshine file. And let's generate a sunshine battery stats HTML file. Okay let's go ahead and open this file in a browser. All right, here we are looking at the battery story and output now as an HTML file in our web browser. Now there's a good amount of information in this file. Let's go ahead and see what some of it is. And what it might mean. And at the top left, you can see the file name and some overall information such as the battery level. In this case, we're at 67%. This can give you a quantitative idea of battery drain over time. As you see, this point here is actually when our battery went down. By 1% to 66. All right, so if we scroll down a little bit, towards the bottom here, there's a timeline and a scale. And then, the time in this table runs from left to right, and everything that is vertically aligned happened at that same time. Now you can see the scale here but you can also adjust. That zoom level right here. Now, when you see these little bars here that represent activity, you can actually mouse over and you can actually see when this particular item started and how long it's been active. Now, what battery storage does not tell you is how much battery was used for each individual action, only how often and how long it used battery. But you can look at the battery level over time to get a general sense of the absolute battery drain. Now the rows are pretty obvious from their names so we're not going to explain them right now, you'll get that more deeper during the tools exercise. Now you see these bars, and you can actually mouse over each one of them. To find out when it started and how long it took. Now battery storage does not tell you how much battery was used by each individual action. Only how often and how long it used battery, but you can look at that overall battery level over time to get a general sense. Of absolute battery drain. Now the rows here, they're pretty obvious from their names, so we're not going to explain all of them right now. We'll get deeper into the tool during the exercise. Check out this item here, this one called screen. Now, this represents how often the screen is on. In this case, when I was interacting with the phone. You could see that screen was on the entire time. We also have this section for top which actually lists which application was in the forefront at the time. In this case, we've interacted with Sunshine for a little bit and then if we scroll to the right here, this is when we hopped into Maps. And this is actually when we switched to YouTube. Now what's neat about this file is we see a lot of things additionally that might impact the battery. For example, we have stats of when WiFi was one, in this case all the time. Video playbacks seems to be happening. You'll. Notice the GPS started here. And if we scroll up, this is the time that we actually hopped into the maps application. So that would make sense that when our maps application was started, our GPS was turned on. And this time the GPS was using a bit of battery here. We also see bursts of spikes here in our WiFi strength. And that's changing over time as well, dynamically. Now here's a more in depth example output from a battery store and capture. You can see that there's a very large amount of mobile radio and wave clockwork that's occurring on this device. It looks like away clock is turning on running some networking work and then going back to sleep over and over again. This level of behavior definitely deserves a closer look to see if it's using too much battery. In the next section, we'll use battery storing to find out battery problems in our code. Let's go ahead and do it. All right, clearly Cheyenne over here isn't going to sit still. So taking a picture isn't something that you can delay, especially if you're trying to capture that special moment. So in that case, and for the sake of this example, the answer is filtering a picture. Okay, so let's go ahead and take a look at a class in the Android framework that'll help you defer certain tasks. All right, well done. Now let's hop back into Android Studio and take a look at the solution for this quiz. All right, first, let's look at our applyFilter method. See the instructor notes for the code snippet, specifically. Now here, we call checkForPower and if it returns that the phone is not charging, we then change the text to inform the user to plug in their phone and return before completing the method. Simple enough. All right, let's take a look at the logic within checkForPower. Again, see the instructor notes for the code snippet. Okay, so here we are within checkForPower. Now the first thing we do is we set an intent filter to describe for changes to the battery state. And then we actually get an integer, right here, representing the battery plugged in status. We then compare this integer to various constants within the BatteryManager class. For instance, one for whether it's charging via AC or alternating current. This means it's plugged into the wall. Another for USB. And finally, whether it's charging via wireless. We wrap the wireless check in an if statement to make sure that the SDK build version is high enough. And then finally, if it was charging via USB, AC, or wireless, we return true. Otherwise, false. It's no secret that in order for your app to be successful, a user has to run it. And for it to continue to provide value, sometimes it needs to keep running. However, this can cause problems when the user or the system puts the phone to sleep. While it might seem like a right idea to wake up the phone to do your work, there's a massive battery problem waiting to happen here. To illustrate this point, let's talk about my good friend Raido whom you all may remember from the Android fundamentals course. He's an avid social user and a game player, and his running applications are constantly checking scoreboards, uploading photos, and pulling down the latest hashtag crazes. But they do this even when Raido puts his phone to sleep, which means by lunch, his phone is already out of battery. Raido has to constantly recharge his phone because it suffers from sleep deprivation. That is because these applications are constantly waking up the phone, they're eating up battery life instead of letting the phone go to sleep. Remember that Android will turn off various parts of your hardware in order to prolong battery life. First, the screen will dim followed by it eventually turning off. And finally, the CPU will go to sleep. All to save precious, precious battery life. But even in this inactive state, most apps will try to do work, and they will have to wake up the phone to do so. Now the easiest way for your app to wake up your phone to do work is with the powermanager.wakelock API. Which is used to keep the CPU running and prevent the screen from dimming or turning off. This allows your app to do things like wake up the phone at some future date and do some work, and then let the phone go back to sleep. But see it's that last part that's actually the tricky one. Acquiring way clocks is really easy but knowing when and how to release them is more problematic. There's lots of ways that this can go horribly wrong. Like a what if your application takes 60 minutes instead of the expected 10 seconds to analyze an image content before uploading it to the web. Or what if the server crashes and you never get a response from your social ping and end up waiting forever. Well the result is the phone would stay on, waiting for the content and thus draining the battery in no time. Which is exactly why you should be using the version of wakelock.acquire that takes a time out parameter. This will force the wake lock to be released in these cases of crazy so that your app doesn't keep the phone awake until the battery is dead. But even this doesn't solve the problem completely, what should you set the timeout value to be in these cases and how long should you wait after a failure before using a wake lock to try again? The right solution to this might be to use inexact timers which allow you to schedule work for some future date. But if the system detects that it could occur sooner or later in order to conserve battery life then it will wait for that time to do that work. For example, if another process has woken up the phone, your app might wait to be woken up with that group of processes rather than happening ten seconds sooner like it was originally scheduled. And in reality, there's a whole group of situations that might be better to do battery intensive work. For example, when the phone is charging, or connected to wifi or already woken up by another process. If any of your work can be deferred to a future date when these conditions might be ideal, then you can help improve battery life significantly. And this is exactly what the job scheduler API is for. See this API is perfect for scheduling some future work to occur in a battery efficient manner. For example, any non-user facing work waiting until the unit is plugged in waiting until you're on wifi, or batching a bunch of tasks to occur together at the same time. Basically, with one simple API, you can get all that scheduling for free. So, let's take a look at this API in action. Hello again. So Colt just talked about Rado's phone suffering from sleep deprivation. Now this refers to the sad situation where Rado's favorite apps would continually drain his device's battery by preventing it from going to sleep. Now here's an example of what this kind of hyper-active app behavior looks like in battery storing and output. This is a screenshot from a Google IO 2014 talk called Project Volta, that shows the activity of 50 simulated apps. Now I want you to notice a few things. Notice the frequent wake lock activity. Lots of gaps usually mean lots of wake up cycles for the device. Also notice the bursty network activity. As we know, all these things contribute to significant battery drain. By the way, I strongly recommend that you watch The Talk. It's linked in the instructor notes and it's a good 30 minutes well-spent. All right. Let's look at some code that might contribute to a graph looking like this. If you'd like to follow along, go ahead and check out the 3.21 wake locks branch in the sample app. Traditionally, if you want to keep the device awake we would implement a Wake Lock. Now just an FYI, when you build the code and run it, you'll notice a new button in your sample app called Release the Wake Lock. Now this launches a new activity with a button to trigger some code. This code models what it might be like for an app to poll a server for an update of some kind. Perhaps retrieving some new news data on some periodic basis. So let's look deeper into the code. All right so we're going to start looking at the free the wake clock activity source. Now as we can see, we have a set onClickListener that basically is going to call this function called pollServer when we press that button on our app. Let's look into that code. Now this method writes some code to mimic what it's like to poll a server for some type of uptake. Now this method is going to use a wake lock. Now, in code the first thing that's done is you require a wake lock to keep your device awake. So that you can then proceed to do the network update. We then check to see if the network is connected. Now in real life if it's not connected the app might just retry until it is. If it is connected, we use an async task to retrieve new data from the server away from the main UI thread. Now, for demonstration purposes. Within our simple download task, we basically open up an HTTP connection to google.com and get a simple get request. And, when our background thread finishes executing, we then go ahead and we release the wake lock. The inherent problem with wake locks is that they're manually held and released. So if you hold onto the wake locks for too long, you could be draining the user's battery needlessly. Maybe it's okay if one app is doing this, but if multiple apps are using wake locks inefficiently, that's a recipe for sleep deprivation. So the moral of the story is, make sure to release your wake lock as soon as you are finished doing your work. In the next video, we're going to see how we can still achieve periodic background updates. Its but implement them in a way that will help conserve more of your devices battery power. Let's take a moment to make something insanely clear. As far as battery is concerned, networking is the biggest, baddest, dirtiest offender that there is. Remember that inside of your phone is a small piece of hardware that's effectively a HAM radio, its whole purpose it to communicate with local cell phone towers and transmit data to them at high volumes. But the trick is that this chip, isn't always active, you see, once you send a packet of data the radio chip will stay on for a certain amount of time, just in case there's a response from the server that it's expecting. But if there's no activity, the hardware will shut down to save battery life. And as we've seen before, there's a large battery spike when this chip first turns on, and as long as it's keeping itself awake to wait for responses, it's going to keep on draining the battery at the same time. Now it's worth pointing out that there's two primary ways in which most apps interact with the radio. Firstly, there are events that need to occur right now. These events are the result of some user action, or they arise from the immediate need to update the UI of your app. For example, imagine when the user asks to load a new batch of tweets for a trending hashtag, since this is a user initiated action, your app should respond ASAP. On the other side of this coin are all the networking jobs that don't need to happen in a time critical manner for example, uploading user data, synchronizing background statistics, or re-sizing all of your social photos. So while the first set of tasks happen, has to happen immediately, in order to provide feedback to the user, the second set of tasks can easily be put off until later, when they can be performed in a battery efficient manner. And there's a high probability, that the majority of your network requests in your application fall into this second category. [LAUGH] Converting networking jobs over to being more efficient is a two step process. Firstly, take a hard look at the mobile radio row in your battery historian tool for your application. Each of those red bars that you see here represent an active mobile radio, any gaps between those bars represent when the radio is asleep. If you see lots of narrow bars and gaps in your graph, this can point to your performance problems, since it means that you're churning through lots of wake up and sleep cycles. What you want instead, is to see large gaps next to large blocks of activity. This way you've reduced overhead by minimizing the number of network requests, and even better, don't use the radio at all. I mean you can wait until the phone is connected to WiFi and then let the WiFi hardware do all this work with much less battery train. Now, the problem is that writing the code to batch, cache, and defer all these networking requests is really difficult to get right, which is why we've done the work for you. The JobScheduler API that rolled out with the L release of Android provides a full suite of API's that do all of this network request management work and more, on your behalf. But rather than telling you about this wonderful API, why don't you take a swing at it in practice? As Colt just talked about, there are certain tasks you can delegate to the system because the system will have a better idea of when to schedule those tasks in order to save battery. Now as a developer, the key is to identify which tasks in your app should be delegated to Android's job scheduler using the job schedule API. For example, this is non user facing work. Tasks that can wait for the device to be plugged in or connected to WiFi or just general situations where a bunch of tasks can be batched to run at the same time. All of these are good candidates for the job scheduler. So let's see how we can modify our sample app code to use it. To follow along, check out the branch 3.22_wakelocks_optimized. Now when you check out this optimized branch, you'll notice that your activity's been updated. You have a new button within your freeway clock activity called pulse modder bees modder and it's going to go ahead and execute our improved code. So, let's go ahead and see what this code actually does. All right, so here we are back in Android studio and I want to call out a few new additions to our project here. Namely, a new file called my job service. We'll quickly learn what we're going to do with this file. Let's go ahead and focus back on the main source for this way clock activity. Now in order to use the job scheduler, you're going to need to create a service endpoint. This is what the system will call when it's appropriate to run your job. And this is exactly where our job service source comes in. All right, so here we're extending the framework provided job service class and then all we need to do is implement the onStartJob and onStopJob callbacks. So let's look at them. Now, in our onStartJob method, we can implement the logic to retrieve our new updates from the network in the background. As you can see, this is similar lines to what you've seen before. We're calling our simple download task and executing it in the background. Now, you should already be familiar with this code from before. You check to see if the device is connected to a network. And then if so, we go ahead and execute our simple download task in the background. And we don't do anything major in the on stop job callback in this mock situation. But this method is invoked if your job must be stopped prematurely, i.e., before your code calls job finished. Now, this might happen if the requirements for your job aren't met. For example, if your device loses connection to WiFi. Now, if you are shipping in an app, make sure to add code here that would be able to prevent your app from misbehaving due to your job being halted. And when you're finished with your work,. It's important that you call the job finished method. This lets the system know when you're done with your work so it can release the wake lock that it's been holding on your behalf. Now since the majority of our work occurs in the simple download task, it would make logical sense to come down here and call job finished during our onPostExecute method. So now that we know what will happen when our job is actually called upon by the system to run. But how do I actually do submission to the job scheduler? Let's take a look. So I want to go over here and launch our FreeTheWakelockActivity again. So let's go ahead and modify what this pollServer function does. Again this method pollServer is actually called when we press this button Poll Smarter, Be Smarter. As you can see, it's scheduling jobs. Okay, so here we are within the pollServer method. Now the first thing we do is we get a reference to the job scheduler itself using a call to get system service. The next thing we do is the important part. We're going to go ahead and create what the framework calls a job info instance. And this is a basically an encapsulation for the requirements for a particular task you'd like to defer to the system to do on your behalf. Now the way we generate a job info object is we use this builder class with the job info. And it's got a set of handy helper methods within it to basically help specify the constraints for our particular job. In this case, we call setMinimumLatency, which means that the system should wait at least five seconds before running this job. We then set an override deadline of 60 seconds. That means if this job hasn't been executed within a minute, go ahead and execute it. The next thing we do is we set a required network type to be any. So, we allow this job to run on WiFi and any data connections. Finally, we finish it by calling .build which generates our job info. Now, once we've encapsulated our entire job constraints within the job info object, all we need to do is call scheduler.schedule and pass in the job info instance that we just created. Pretty cool, huh? Now it's worth noting by using the JobScheduler API, you're being a good Android citizen. And the more good Android citizens there are, the greater the collective impact you'll have on battery life. Now what we've shown you is just a small sample of what job scheduler supports. There are way more options and configurations that you an tailor for your own jobs. Now for more details on how to use this API, see the Android documentation in the instructor notes. Also be sure to check out the Google dev bite on how to use the job scheduler. You'll find the link in the instruction notes as well. All right, now it's time for a little quiz. What jobinfo.builder method would you append to your job to make sure it runs when the device is plugged in? Go ahead and enter your answer right here. Okay, so the answer we're looking for here is setRequiresCharging with true passed in as a parameter. So hopefully you were able to find your way to the JobInfo.Builder documentation. And then fortunately when you reviewed the public methods, you'd be pleased to find this function called setRequiresCharging. And it specifically addresses what we want it to do. Which is to specify that to run this job, the device needs to be plugged in. Now, let's apply the JobScheduler API to minimize the unnecessary wakeup cost of the cellular radio because we know that frequent reoccurrences of this comes at a significant cost to the battery life. Instead, we're going to use the JobScheduler to avoid waking up the cellular radio and schedule our network activity only when the user's device is connected to wi-fi. All right, so let's go ahead and look at some code. Now, in order to follow along, go ahead and check out the 3.31, wait for wi-fi branch in the repo for your sample app. Now just as a reminder when you build this branch in code, you'll notice another button just like similar exercises added to your sample app's UI. This one's called no wi-fi, no deal. When you click on it, it'll launch a new activity called find the wi-fi activity. In this case when you press the button download all the data signal or no, we're going to be triggering some code that we're going to be looking at. It looks like it's trying to make some connections, and it looks like it's fetching some HTML. All right, here we are back in Android Studio, and I've gone ahead and opened up the source code for FindTheWifiActivity. Now similar to our previous exercises, we have a button's OnClickListener basically invoking a function that's doing a particular task. In this case we call the function downloadSomething. Now, now, this code should look familiar. It's not too different than the previous exercise when we're trying to basically make a network connection and do a simple download from a server. Again, here in our downloadSomething method, we're basically mocking the behavior that an app would take to essentially check to see if the network is alive, and then execute a task in the background to download some data. Now if you remember from Colt, we'll know that cellular network access is one of the biggest offenders for reduced battery performance. So in order to conserve battery life, we're going to try to reduce the amount of times we start up the cellular radio, thus incurring less of a tax when having to start up the cellular radio incurring this long network tail of energy. All right, so let's go ahead and hop back into our sample app code and let's see if we can repurpose it and focus on executing our network tasks only when wi-fi is available. Now what this really means is we're going to want to repurpose I would downloadSomething method. This time around, we're going to want to go ahead and schedule a job for the job scheduler. So let's take a look. Now instead of trying to download something in a manual fashion, let's go ahead and see how we can craft a job for the job scheduler to check for a wi-fi only situation. So what I have up here is a comparison view of basically the wait_for_wifi branch, and then our 3.32_wait_for_wifi_optimized branch. You can go ahead and check this out in your sample app code repo if you want to see the difference, but I'm going to go ahead and describe the change. Now in our optimized code, we're going to look at a method called downloadSmarter. Now this is hooked up to our on click listener for our button in our UI. But mainly this downloadSmarter method is going to go ahead and use the job scheduler. Now similar to previous examples, the first thing we do is we get a reference of the scheduler via the getSystemService method. Now in the area where we normally submit jobs, we're going to go ahead and bundle up a job info. Again, that's the encapsulation of what we want to do for our particular job. Essentially it's the constraints for running it. In this case, we've changed one particular line from our previous example. In this case we used internal method .setRequiredNetworkType to specify a network that's unmetered, essentially, wi-fi only. And then we go ahead and we call .build and that's going to go ahead and generate a JobInfo instance for us. And then last but not least, we go ahead and we pass that to the schedule function of the Scheduler. Again the last app that we actually have to do is called the scheduler, with the schedule method passing in our new job info instance. And when it returns, our job service will be called and the task will be run. And when the system deems that it's time to run our job it's going to go ahead and call back into our MyJobService class, and it's going to call our onStartJob method that we implemented before. In this case we're going to go ahead and check for a network connection, in this case it should be wi-fii, and then execute our SimpleDownloadTask. Then lastly when we're finished doing our network activity, which again is implemented in our SimpleDownload class, particularly in the doInBackground method, we go ahead and we open up an http connection and do a get request. Then, lastly, when onPostExecute is called on our Async task, we go ahead and call jobFinished, which is critical in telling the system that we are done with our work, and it can go ahead and release the wake lot that is held on our behalf. All right, so this was an example on basically how to repurpose some of our sample app code to use job scheduler, this time to minimize cellular radio wakeup. Now as mentioned before, the framework offers a lot more possible combinations for specifying a particular job and its associated requirements. Now I suggest that you go ahead and review the developer documentation for all the detail information and see the link in the instructor notes. Now, throughout this class, we've been giving you sample code to work on. Now I've got a little interesting opportunity for you. Now it's your turn to apply what you've learned to your own apps. So your final mission is to profile your app using battery historian. In number one, go ahead and identify opportunities to use the JobScheduler. For example, look for inefficient wake lock and network access that you can reduce. Number two, go ahead and generate a report and battery string and take a screenshot. Make sure to keep your raw HTML file as well. Third, go ahead and make improvements to areas in your code that you've identified as potentially inefficient or problematic. Number four, go ahead and re-profile your code, then go ahead and generate a new report in battery storing and take a new screenshot. And then when you're done, go ahead and share the before and after screenshots on the forum, and then call out any specific improvements that you see in your graph. Or if you don't see any real clear difference in the before and after screenshots, post us to why this might be the case. We'll be checking the forums periodically and we want to see what you guys come up with. All right, cool, so this wraps up our course and I want to thank you for all your hard work, it's certainly been a pleasure to come along and assist in your journey. I hope you learned to always apply performance mind set, when writing your code and maybe hope you had a little fun too. Take care and we'll see you soon. All right, I'll take that as an action item and let's wrap this up. It's really cool that your fixed the battery issues on this phone. I finally can find all my friends. Oh yeah, I mean you know with the right tools and the right process it turns out turning your code from battery issues is really easy. There, there's still one bug left. You know this right? Oh really? It says that code has been stuck in there for 16 hours. Oh you caught that. Yeah. The truth is that's not really a bug. No? Yesterday we overheard these guys talking about managed memory environments. Like, they come for free in terms of performance. He completely flipped out so we had to lock him in the elevator. [SOUND]. Now this place ain't so bad. Put some curtains in. Maybe a window over there. That would. Be bad. Some air. [NOISE] I just want to make apps faster. Is that so bad? [NOISE] We can do this. We can do this. I'll get you, Chris. Mark my words.