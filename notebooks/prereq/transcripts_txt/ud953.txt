Inside the module, you'll find a vector class with three functions, init, str, and eq. I will give a brief description of these functions in case you're not familiar with Python. In that case, I also encourage you to make use of some of the resources in the instructor notes on this page. The initializer creates a vector based on an input list of coordinates, and also sets the dimension of space that the vector lives in. For example, if we wanted to create the vector 1, 2, 3, we could write my_vector = Vector([1,2,3]). Then, my vector would be a variable referring to the vector that we want. The str function gives us the ability to print out the coordinates of the vector using Python's built in print function. Using our example, if we input print my_vector, we would receive the output Vector: (1, 2, 3). The eq function gives us the ability to test whether two vectors are equal. As I mentioned earlier, two vectors are equal if they have the same amount of change in each coordinate direction. Therefore, since we are storing vectors using their coordinates, we just need to compare two vectors' coordinates. To see how this works in Python, we can input my vector2 = Vector ([1,2,3]) and my vector3 = Vector ([-1,2,3]). Then if we input print my vector == my_vector2, we'll receive the output True, since all coordinates of my vector in my_vector2 match. On the other hand, if we input print my_vector == my_vector3, we'll receive the output False, because the two vectors have different first coordinates. Now, let's start defining some basic operations on vectors. Now let's start defining some basic operations on vectors. The first operation we will describe for vectors is addition. One way to visualize the sum of two vectors is to lay them end to end. The arrow formed starting at the tail of the first arrow and ending at the head of the second arrow gives the overall total amount of change by the two vectors. This new arrow is the sum of the first two vectors. In addition to this geometric interpretation of vector addition we have a numeric interpretation. Given the first two vectors as a list of coordinates, we can add the two vectors by adding the corresponding coordinates of each vector. Since this first vector is [1 2] and the second vector is [3 1]. There are some vector is [4 3]. Notice that if we lay out the vectors in a parallelogram shape the sum vector connects the points or both chains of vector start in this case the origin and where the chains both end. We can subtract vectors in addition to adding them. Here is the geometric interpretation of vector subtraction. Consider the vectors the v and w both emanating from the same point what vector would we need to add to w in order for the sum to be v? We represent this geometrically by drawing an arrow from the head of w to the head of v. This vector is what we call v- w, since w + the vector v- w equals v. Numerically we can compute the difference between two vectors by subtracting their corresponding coordinates. So taking this vector to be [1 3] and this vector to be [5 1], their difference is [-4 2]. This is reflected in our drawing since the difference vector represents a change of four units to the left and two units up. This time, if we look at our parallelogram formed by x and y. Notice that x- y connects the heads of the two initial vectors in our chains. If we reverse the direction of this arrow, we'll obtain y- x. Another important operation on vectors is scalar multiplication. In this case a vector is multiplied by a number which we call a scalar. The term scalar is suggestive of the word scale. Which is fitting because the geometric interpretation of scalar multiplication is to scale the size of a vector. Multiplying a vector by 2 doubles its length. Multiplying a vector by 1 keeps it the same length. Multiplying a vector by 1/2 cuts its length in half. We can also multiply a vector by a negative number. This reverses the direction the arrow points in addition to possibly growing or shrinking the vector. Numerically scalar multiplication is computed by multiplying each coordinate of the vector by the scalar. So, 2 times the vector [1 2] equals[ 2 4] and -1/2 times the vector [3 2 1] equals the vector [-3/2 -1 -1/2]. Now it's your turn to put these operations into code. I'd like you to please implement the addition, subtraction and scalar multiplication functions in a language of your choice. Use your code to find the answers to the following questions. After the quiz, I will show you the solution I used based on the python module I introduced earlier in the lesson. For the first question please compute the vector 8.218 minus 9.341 plus the vector minus 1.1229 2.111. For the second question, please compute the vector 7.119, 8.215 minus the vector, -8.223, 0.878. For the third question please compute the scalar 7.41 times the vector, 1.671, -1.012, -0.318. Please enter your solutions within an accuracy of at least three decimal places. Here are the answers I got. For the first question, I got 7.089 minus 7.230. For the second question, I got 15.342, 7.337. For the last question, I got 12.382, -7.499, -2.356. I could have computed these relatively easily by hand but I'd like to go ahead and code functions to compute sums, differences in scalar products of vectors that way I can use these functions later. To fill in the plus function I used a list comprehension after zipping together the coordinates of self and v. This allows me to add corresponding coordinates and form a list from the sums. If you're not familiar with list comprehension in Python or if you're using a different language you could obtain the same result using a for loop to loop through each of the two vectors coordinate lists and insert their sum into another data structure. We can mimic this pattern in the minus function. I again used a list comprehension. But this time instead of adding, I just subtracted the corresponding coordinates of the vectors self and v. Finally, in the time scalar function. I again used a list comprehension to compute the product of each coordinate of the vector self, with the number c. One difference this time is, since I'm only dealing with one vector in this function, I didn't need to use zip in the list comprehension. Now if I execute this code below the class definition, I see the output of the vectors as expected. I stated earlier that two of the most important geometric properties of a vector are its magnitude and its direction. These two terms are suggestive of the mathematical concepts they represent. The magnitude of a vector refers to how much movement it quantifies, and the direction of a vector refers to where the director's movement is pointed. Now let's see how to define these concepts more formally. If we think of a vector as an arrow from one point to another, the magnitude of the vector is the length of the arrow, that is, the distance between the two points it connects. To calculate distance, we can use the Pythagorean formula. In two dimensions, we can make a right triangle by taking one leg to be along the x-axis, one leg to be along the y-axis, and the hypotenuse to be the segment between the two points in question. The length of the horizontal leg is the difference between the x-coordinates of the points, and the length of the vertical leg is the difference between the y-coordinates of the points. Then, by the Pythagorean formula, the length of the hypotenuse squared is the sum of the squared lengths of the legs. Now let's interpret this in terms of the vector. The coordinates of the vector give the difference between the coordinates of the points the vector connects, and the length of the vector corresponds to the length of the hypotenuse of the triangle. So the length of the vector squared is the sum of the squares of its coordinates. In other words, the magnitude of v is the square root of its x-coordinates squared plus its y-coordinates squared. Note that we use the double bars around a vector as notation for its magnitude. A generalization of this formula holds true in higher dimensions too. If v is a vector in n-dimensional space, and we call its coordinates v1, v2, up through vn, then the magnitude of v is the square root of the sum of the squares of its coordinates. Now, using the notion of magnitude, we can define the direction of a vector. Notice that all vectors pointing in the same direction are scaled versions of each other. This vector, v2, is twice the length of this vector, v1, and this vector, v3, is a third of the length of v1. So, for each possible direction of vector, we have a whole class of vectors pointing in that direction. So, to define the direction of a vector, we will choose a canonical or representative element from each class. For simplicity, we'll use the vector of magnitude one from each class. We'll call these vectors the unit vectors, since they each have a length of one unit each. The process of finding the unit vector pointing in the same direction as a given vector is called normalization. Normalization has two steps. If we want to normalize the vector, v, the first step is to find its magnitude. The second step is then to perform a scalar multiplication. We multiply v by the number one over the magnitude of v. This scales the vector up or down, so that its length becomes one. So, if we wanted to normalize the vector, v equals -1, 1, 1, we first find its length. The magnitude of v equals the square root of -1 squared, plus 1 squared, plus 1 squared, which equals the square root of 3. Then we multiply v by 1 over the square root of 3. The resulting vector, u, would be -1 over the square root of 3, 1 over the square root of 3, 1 over the square root of 3. We can check that u is actually a unit vector by computing its magnitude. In this case, the length of u is the square root of -1 over square root of 3 squared, plus 1 over square root of 3 squared, plus 1 over square root of 3 squared, which equals the square root of one-third, plus one-third, plus one-third, which is the square root of 1, which is also 1. Since u has a magnitude of 1, it is a unit vector. It is the unit vector pointing in the same direction as v, so u is the normalization of v. I should note one last point. If all the coordinates of a vector are zero, we call that vector the zero vector, denoted as the number zero with an arrow above it. The zero vector is a vector indicating no change. Note that the magnitude of the zero vector is 0. However, what happens if we try to normalize the zero vector? We would end up dividing by a magnitude of 0, which causes all sorts of problems. So we simply say that the zero vector has no normalization. Another way to interpret this is that the zero vector has no direction. Just as it doesn't make sense to ask which way a stationary car is moving. Now let's code two more functions in our library. A function to compute a vector's magnitude and a function to compute its normalization. As before, use your program to compute the answers to the following questions. For the first question, please find the magnitude of the vector negative 0.221, 7.437. For the second question, please find the magnitude of 8.813, negative 1.331, negative 6.247. For the third question, please find the normalization of 5.581, negative 2.136. And for the last question, please find the normalization of the vector 1.996, 3.108, negative 4.554. Please enter your solutions within an accuracy of at least 3 decimal places. Here are the answers I got. The magnitude of the first vector is 7.440. The magnitude of the second vector is 10.884. Now for the normalizations, for the first vector I got 0.934, -0.357. And for the last vector, I got 0.340, 0.530, and -0.777. Now let's take a look at the code I wrote to get these answers. I worked on the magnitude function first, since I can leverage it in my normalized function. Remember that the magnitude of a vector can be found by squaring its coordinates, adding them together. And then taking the square root of the sum. Let's implement that in this function. First, I imported square root from the module math since I'll need it in my function. Next, I used a list comprehension to make a list of the squares of the coordinates, x** for x in self.coordinates. And I called this list coordinate_squared. I can find the sum of this list using the built in sum function, and then I can return the square root of the sum using the square root function. So it returns sqrt(sum(coordinates_squared)). Now using the magnitude function, I can code the normalized function. Recall that we can normalize the vector by multiplying it by1 over its magnitude. Therefore, if I let magnitude = self.magnitude(), I can return self.times_scalar(1./magnitude). To deal with the case that self might be the 0 vector, I wrap this code in a try block. If self is the 0 vector, then magnitude will become 0. And then dividing by magnitude will raise a 0 division error. I can then catch this error with the statement, except ZeroDivisionError. And then raise a new exception with the message Cannot normalize the zero vector.. Note that I'm already starting to leverage some of our preexisting functions to define the normalized function. This trend will continue as we progress throughout the lesson. Now, using the two functions I've coded, I can compute the magnitudes and normalizations of the vectors, giving me the answers that I showed earlier. So far, we have seen how to add and subtract vectors, and we have seen how to multiply vectors by scalars. This might lead you to ask, can we multiply two vectors? It turns out that there are several different notions of vector multiplication. Perhaps the first one we might think of, just multiplying corresponding coordinates, doesn't turn out to be useful in many applications. However, there are other notions of multiplication, which are very useful. One of the most important operations we can perform on vectors is the inner product, also called the dot product, since we often represent it by placing a dot between the vectors being multiplied. It's important stems from its geometric interpretation. It lets us find the angle between two different vectors. More precisely, the inner product of the vectors, v and w, in Euclidean space, satisfies the following identity, v dot w equals the magnitude of v times the magnitude of w, times the cosine of theta, where theta is the angle between the vectors, v and w, if we were to draw them emanating from the same point. Technically, there are two angles between the vectors, a shorter one and a longer one. We always use the shorter one by convention. Note that the inner product of two vectors is a number, not a vector. Using the inverse of the cosine function, also called the arc cosine function, we can solve for this angle. It equals the arc cosine of the dot product of v and w, divided by the product of the magnitudes of v and w. Another way we can think of this formula is as the following. The arc cosine of the normalization of v dotted with the normalization of w. Of course, unless we have a nicer way of computing the dot product of two vectors, this formula doesn't do us much good. It turns out that there is a nicer way to compute the inner product, and it is similar to our first candidate notion for multiplying vectors. To compute v dot w, we can multiply the corresponding components of v and w, and then we can add those products together. So, for example, the dot product of the vectors 1, 2, -1 and 3, 1, 0 is 1 times 3, plus 2 times 1, plus -1 times 0, which equals 5. This means that the angle between these two vectors is the dot product of v and w, which is 5, divided by the magnitude of v, which is the square root of 6, and the magnitude of w, which is the square root of 10. This gives about 0.87 radians, which is about 50 degrees. There are some interesting consequences of this connection between the inner product of two vectors and their angle. First, recall that the cosine of x is always bounded below by -1 and above by 1, no matter what x is. Using the earlier equation, that the dot product of v and w is the product of the magnitudes times cosine of theta, we see that the dot product is bounded below by negative magnitude of v times magnitude of w, and above by magnitude of v times magnitude of w. We can express this inequality more succinctly as the absolute value of v dot w is less than or equal to the product of the magnitudes of v and w. This is a famous inequality called the Cauchy-Schwarz Inequality, and you will see this inequality pop up all over linear algebra and mathematics, in general. Let's assume, for the moment, that neither v nor w is the zero vector, which would mean that their dot product is automatically zero. What does it mean if v dot w actually equals the magnitude of v times the magnitude of w? >From our earlier equation, it means that the cosine of theta equals 1, and this tells us that theta must equal 0 in both degrees and in radians. In other words, the vectors, v and w, are pointing in the same direction. Similarly, if v dot w equals the negative of the product of the magnitudes, we can conclude that cosine of theta equals -1, which means that theta equals pi radians, or 180 degrees. And that, in turn, means that v and w are pointing in opposite directions. Finally, if v dot w equals 0, then, since we assumed that neither v nor w was the zero vector, this means that cosine of theta must equal 0. This means that theta equals pi over 2 radians, or 90 degrees. So, v and w are at a right angle to each other. One last thing to consider is what happens when you take the dot product of a vector with itself. In this case, the angle between the two vectors is 0, both radians and degrees, so, cosine of theta will equal 1. So our formula reduces to v dot v equals the magnitude of v squared. After a little rearrangement, we can see that the magnitude of v equals the square root of v dot v. This gives us another interpretation of the magnitude of a vector, this time in terms of its dot product. Let's implement two more functions now. The first will compute the dot product of two vectors and the second will compute the angle between two vectors, both in radians and in degrees. Use your code to answer the following questions. Remember that like with the normalization function earlier, if one of your vectors is the zero vector, there may be a special case you have to deal with. For the first question, please find the dot product of the vectors 7.887, 4.138, and -8.802, 6.776. For the second question please find the dot product of -5.955, -4.904, -1.874, and -4.496, -8.755, 7.103. For the third question, please find the angle in radians between the vectors 3.183, -7.627, and -2.668, 5.319. And for the last question, please find the angle in degrees between the vectors 7.35, 0.221, 5.188, and 2.751, 8.259, 3.985. As before, please enter your solutions within an accuracy of at least three decimal places. Here are the answers I got, for the first question I got -41.382. For the second question I got 56.397. For the third question I got 3.072 radians. And for the last question I got 60.276 degrees. Let's take a look at how we can code functions in Python for computing these answers. For the dot product, I followed a similar pattern as before for my time scalar function. I use zip with a list comprehension to compute products of corresponding coordinates between self and v. And then I use an outer sum function to add these products together. >From my angle with function, I have an optional argument called in_degrees which is set to false by default. If it's set to true, then it will output the angle between self and v in degrees, otherwise it will output the angle in radians. For the angle with function, I start by computing the normalizations of self and v. I then take the dot product of the resulting unit vectors, and then that dot product is fed into the arc cosine function, which in Python is the function acos from the module math. The output of acos gives me the angle in radians. If in_degrees was set to true, I can multiply the angle in radians by 180 divided by pi. Pi is also found in the math module. This obtains the angle in degrees. I also make sure to check for the zero vector, by seeing if an exception is raised with the message that comes from attempting to normalize the zero vector. If that message is seen, an exception with a different message is raised, stating that angles cannot be computed with the zero vector. I also want to point out that I made some modifications to my vector class. I'm now using the decimal package, which gives me better numerical precision. Specifically, I changed my initializer to ensure all coordinates are decimal objects instead of floating point numbers or integers. I also made some other changes to ensure that numbers coming from outside the vector object are treated as decimals. Why did I do all this? Well, remember that if two vectors point in the same direction, their dot product is the product of their magnitudes. So If self and v point in the same direction, then the dot product of their two normalized unit vectors should be one. However, sometimes I was getting a ratio greater than one due to loss of numerical precission, and this will give me a domain error inside the acos function. Now that I finished coding these two functions, I can execute this code here and it will output the answers to the questions from before. I mentioned these concepts briefly earlier in the lesson, but it's worthwhile to explore them in some more depth. We say that two vectors are parallel if one is a scalar multiple of the other. So if v is a vector, then v and 2v are parallel, as are v and one-half v, as are v and -v. In fact, since the 0 vector equals 0 times v, it follows that v and the 0 vector are parallel too. Also it's worth noting that v is parallel to itself, since 1v=v. Note that we consider two vectors to be parallel even if they point in opposite directions, such as the case of the v and -v. All that matters is that we can multiply one by scalar to get the other. In contrast, we say two vectors are orthogonal if their dot product is 0. Recall that if v dot w = 0, then there are two possible cases. Either one of the vectors v or w is the 0 vector, or else they're at right angles to each other. This is where we get the terminology orthogonal. Orthogonality is sometimes difficult to imagine in higher dimensions, so this algebraic definition comes in handy when we lose our ability to picture our vectors. An interesting fact is that the 0 vector has the unique property that it is parallel and orthogonal to all other vectors. In fact, the 0 vector is the only vector that is orthogonal to itself. To see why, suppose that v is a vector, some vector, maybe not 0, but it has the property that it's orthogonal to itself. That means that v dot v = 0. But recall that v dot v equals the magnitude of v squared. So that means the magnitude of v squared equals 0. In turn, that means that the magnitude of v = 0. Since the 0 vector is the only vector with 0 magnitude, that means the 0 vector is the only vector that v could possibly be. So if some vector is orthogonal to itself, it must be the 0 vector. Let's take a moment to add some functionality to our code that lets us check whether two vectors are parallel, and whether two vectors are orthogonal. To check your work, consider the following pairs of vectors. Compute whether each pair of vectors is parallel or orthogonal, and then check the appropriate boxes. Some of them might be both or neither. Here are the answers. The first pair is parallel but not orthogonal. In fact, they differ by a factor of negative 3. The second pair is neither parallel nor orthogonal. The third pair is not parallel but it is orthogonal. The last pair is both parallel and orthogonal. As stated previously, the 0 vector is both parallel and orthogonal to all other vectors. So we didn't need to use a program to answer this part. However, it's worthwhile to make sure our program can correctly handle the special case. Here's the code I wrote to implement these functions. I'll start with the function is_orthogonal_to. This one is very short. We should just need to check whether the two vectors have a dot product of zero. However, due to precision issues we should probably check that the dot product has a very small absolute value. Maybe within plus or minus 10 to the minus 10. Otherwise you might get a lot of false negatives when you check whether two vectors are orthogonal. The function is_parallel_to is a bit more complicated. Here I first check whether either of the two vectors are zero. To do this I wrote another function called is_zero that checks whether a vector's magnitude is smaller than some tolerance. If one of the two vectors is zero, then I know the two vectors must be parallel, so I go ahead and return True. However, if they're both non-zero then that means, to be parallel they must either be pointing in the same direction or in completely opposite directions. So the angle between the vectors will be either zero or pi radians. Here's the code I ran after defining my functions. When I executed, I get the answers to the questions asked previously. The notion of orthogonality is extremely powerful. Fundamentally, it gives us a tool for decomposing objects into combinations of simpler objects in a structured way. Let's explore one of the first phenomena that are consequences of orthogonality. For this video, let b be a constant vector. Or you can think of the b as standing for the word basis. It will be useful to think of b as emanating from the origin and then to imagine a line l through the origin that contains the arrow representing b. We're going to consider the process of projecting a vector v onto the basis vector b. One way to describe what this means is to let v emanate from the origin and then imagine rotating your point of view until you are looking at b and v in the same plane. Then measure the apparent magnitude of the arrow v from this point of view. Unless v and b are perfectly parallel, v will look shorter than it actually is. The shorter arrow is the projection of v onto b which we will denote by proj sub b of v. Or v with two little bars next to it for short. Another way to describe projection is to draw a right triangle whose hypotenuse is the arrow depicting v. And one of whose legs lies on the line l. If we draw a vector from the origin along this leg, that drawn vector will also be the projection of v onto b. The remaining leg is perpendicular to the line l, and therefore orthogonal to b. If we draw a vector along this leg, the drawn vector is the component of the orthogonal to b which we can denote v with a an upside down t called v perp. Remember a definition of the sum of two vectors? Since we have formed a chain of two arrows from the tail to the head of v, we can say that the sum of these two vectors is v. In other words, v equals the parallel plus the perp. We have written v as a sum of two vectors, one that is parallel to b and the other that is orthogonal to b. This is something that occurs often in mathematics. Given a nonzero basis vector we can express any vector as a sum of two vectors, one that's parallel to the basis vector and one that is orthogonal to the basis vectors. All right, so is there a nice way to compute the projection of v onto the basis vector b? Let's start by making a simplifying assumption. Let's assume that the angle between v and b is at most 90 degrees. If you remember soh-CAH-toa from trigonometry remember that the CAH means that the cos sign of the angle of a right triangle is equal to the ratio of the adjacent edge to the angle and the hypotenuse. In other words, the cosine of theater is equal to the length of v parallel divided by the length of v. After some rearranging, that means that the length of the v parallel is the length of v times the cosine of theta. Now we're going to use our dot product formula from earlier. Substituting this expression in for cosine theta we get that the length of v parallel equals the length of v times this expression. Since we have a factor of the length of v in the numerator and denominator of the expression, we can cancel them. And then we can rewrite what's left over as v.u sub b whereas u sub b is the unit vector in the direction of b i.e., the normalization. All right, so I cleaned up some of these formulas and just left what we need going forward the length of v parallel is equal to the dot product of just v with the normalization of b. Now because we made this simplifying assumption that theta is less than or equal to 90 degrees we know that v parallel will point in the same direction as b. We also know that the normalization of b is a unit vector. So it will have magnitude one. What these two facts tell us is that if we scale the normalization of b by exactly the length of v parallel there will actually get the vector v parallel. imagine taking a unit vector that's right here and just stretching it just the amount needed to get to v parallel that's what we're doing. Now let's plug in this expression for the length of the parallel. And what we end up with is this formula. V parallel is exactly the normalization of b times the scalar v dot the normalization of b. Now what if we didn't make the simplifying assumption? What if theta were greater than 90 degrees? That negative sign from earlier will carry through all the way to this expression here. We'll also have to make a change here because v parallel will now point in the opposite direction of b. Since v parallel is pointing in the opposite direction this formula will now become a negative but now when we plug this expression in for a length of v parallel then the negative signs will cancel out and we'll get the same formula. So regardless of the assumptions that we make on this angle will always get the same formula for v parallel in other words the projection of v on to b. Now that we've walked through the derivation of the projection formula. It's your turn to implement it in your library. Also write a function to compute the component of a vector orthogonal to a given basis vector. After you finished, use your code to compute the following. First find the projection of the vector 3.039, 1.879 onto the vector 0.825, 2.036. Second, find the component of -9.88, -3.264, -8.159 orthogonal to the basis vector -2.155, -9.353, -9.473. Finally decompose the vector 3.009, -6.172, 3.692, -2.51 into a sum of two orthogonal vectors, one parallel to, and one orthogonal to, 6.404, -9.144, 2.759, 8.718. As usual, please make sure to express your answers within an accuracy of three decimal places. Here are the answers. The answer is 1.083, 2.672. For the second question, the answer is -8.350, 3.376, -1.434. For the last question, the answers are 1.969, -2.811, 0.848, 2.680 plus 1.040, -3.361, 2.844 and -5.190. You could've put these in either order. So I didn't specify which one was parallel and which one was perpendicular. The first vector is the parallel component, and the second vector is the perpendicular component. To add this functionality to my python code, I implemented the functions component_parallel_to and component_orthogonal_to in my vector class. Let's look at component_parallel_to first. I used the formula derived previously. That the projection of a vector onto a basis vector is found by first normalizing the basis vector to form a unit vector u. Then, taking the dot product of u, with the vector we're projecting. In this case self, and then multiplying u by the result of that dot product. The only problem that could happen here is if the basis vector is zero. In this case the normalization will fail, and an exception will be raised. I check for the appropriate message and raise a new exception detailing that there is not a unique parallel component to the zero vector. Now having coded the component_parallel_to function I can implement the component_orthogonal_to function. I do this by recalling the fact that v equals the parallel plus v perp. Which I can re-express as the equation v perp equals v-v parallel. So I find the parallel component, set it to the projection variable and then return self.minus(projection). In the case an exception is raised by finding the parallel component, I check to see if the NO_UNIQUE_PARALLEL_COMPONENT_MSG is found. If so, I raise a corresponding exception with the message NO_UNIQUIE_ORTHOGONAL_COMPONENT. Now that I'm done coding those functions, here's the code corresponding to the questions asked earlier. And here's the output after I run it. There is a second notion of vector multiplication that is common especially in the context of engineering, physics, and computer graphics. This product is the cross product. The cross product is very useful in three dimensional linear algebra but it does not have a generalization to higher dimensions. Geometrically, the cross product of two vectors, v and w, is a vector that is orthogonal to both v and w. And has magnitude of v times magnitude of w times sine of theta where theta is the angle between v and w. Note that unlike the dot product the output of a cross product is a vector, not a number. Let's look at this definition a bit more closely. Notice that if theta is 0 or pi that is 0 or 180 degrees, then the cross product will have magnitude 0. So in other words, the cross product of two parallel vectors is the 0 vector. Also if either v or w is the 0 vector, then the cross product will be the 0 vector as well. Assuming we're not in one of these degenerate cases, notice that there were, in fact, two vectors that meet the geometric description given so far. These vectors are negatives of each other. In order to determine which vector to use, we proceed by the right-hand rule. Here's how the right-hand rule works. If the cross product is v cross w, imagine your right thumb pointing in the direction of v and your right index finger in the direction of w. Then extend your right middle finger, and then the direction it points will be the direction of the cross w. So, in this case, it will be pointing up. Notice that if we switch the order of the cross product to w cross v, then using the right-hand rule will give us the downward pointing vector. This gives us the equation v cross w equals the negative of w cross v because switching the order of multiplication negates the product. We call the cross-product anticommutative as opposed to the commutative dot product in which switching the order does not affect the product. A consequence of this geometric formula can be seen in the parallelogram spanned by v and w. Take the length of the v arrow to be the base of the parallelogram. Then the height of the parallelogram is the length of this segment from the head of the w arrow to the line spanned by v perpendicular to v. Using the trigonometric identity, sine of theta equals opposite over hypotenuse which is the SOH from SOH-cah-toa. We have that sine of theta equals the height divided by the length of w. In other words, the length of w times sine of theta equals the height of the parallelogram. Remembering that the area of a parallelogram is base times height, this means that the area of this parallelogram is the magnitude of v times the magnitude of w times sine of theta. In other words, it's the length of v cross w. Another geometric note is that the area of this triangle spanned by v and w is exactly one half of the area of the full parallelogram. That means the area of the triangle will be one-half times the magnitude of the cross product. Again, all this is great, but it's of little use to us without a practical formula for the cross product. So given vectors v with coordinates x1, y1,z1, and w with coordinates x2, y2, z2, here's such a formula. The cross product of v and w is the vector. First coordinate, y1z2- y2z1. Second coordinate, it's the negative of the quantity (x1z2- x2z1). And the third coordinate is x1y2- x2y1. So for an example, if we wanted to find the cross product of the vectors, v, which is 5, 3- 2, and w, which is- 1, 0, 3, then we would compute. First coordinate, 3 x 3- 0 x -2. Second coordinate, negative, quantity of, 5 x 3- (-1) times (-2). Third coordinate, 5 x 0- (-1) x 3. And that would give us the vector 9, -13, 3. One way to help confirm that we did our arithmetic correctly is by checking that our outcome is orthogonal to both of our vectors v and w. The dot product of v with the cross product will be 45- 39 + 6 = 0, and then the dot product of w with the cross product will be -9 + 0 + 9 = 0. So our result is orthogonal to both v and w. We can also use our cross product to find the area of the parallelogram spanned by v and w. To do this we would take the magnitude of our cross product, which would be the square root of 9 squared + -13 squared + 3 squared. Computing this gives us approximately 16.093. Yay, it's coding time again. This time I'd like you to write three functions. A function to find the cross product of two vectors. A function to find the area of the parallelogram spanned by two vectors. And a function to find the area of a triangle spanned by two vectors. You can assume the vectors will all be in three dimensional space. However, as an optional exercise, it's possible to extend the area functions. To vectors in two dimensional space by treating them as 3D vectors with a Z coordinate of zero. After you're done with your functions, use them to fill in the answers to the following questions. Within an accuracy of three decimal places. First, find the cross product of the vectors 8.462, 7.893, negative 8.187, and 6.984, negative 5.975, 4.778. Second, find the area of the parallelogram spanned by the vectors negative 8.987 Negative 9.838, 5.031 and, negative 4.268. Negative 1.861 and they give 8.866. Third, find the area of the triangle spanned by the vectors, 1.5, 9.547, 3.691 And -6.007, 0.124, 5.772. Here are my answers to the questions. For the first question, I got the cross product was the vector -11.205, -97.609, -105.685. For the second question, the area of the parallelogram is 142.122. Finally, the area of the triangle is 42.565. Now, let's take a look at the Python code. The first code I wrote was the cross function where I more or less just copied the formula from earlier. The meat of this function is in some exception catching clauses in case self or V isn't a vector in three dimensions. I won't take too much time to go through them except to say that in my implementation. If both vectors are in two dimensions I add a Z coordinate of 0 to each. And then take the cross product of the resulting vectors. Otherwise the function raises an exception. The remaining two functions are much smaller. To find the area the parallelogram, I first take the cross product of the two vectors and return its magnitude, and to find the area of the triangle, I just return one half the area of the parallelogram. After coding all the functions, I ran this code here to find the answers to the questions asked earlier. This lesson will focus on one of the quintessential problems of linear algebra. Given a set of flat geometric objects such as lines and planes, what are their common intersection points? In the abstract it isn't immediately obvious why this problem has much interest outside of pure mathematics. How do the intersections of lines and planes come up in real life? Geometric objects are often defined to be sets of points whose coordinates satisfy a particular relationship, like an equation. For example, this ellipse in the plane is the set of all points x,y that satisfy the equation 1/4 times x squared plus 1/9 times y squared = 1. In this shape in 3 dimensions called a hyperboloid is the set of all points X, Y, Z satisfying the equation x squared + y squared- z squared = 1. Flat objects like lines and planes satisfy a special type of equation called the linear equation In which the only operations allowed are adding and subtracting variables and constants and multiplying variables by constants. So for example, x + 2y = 1 and y/2- 2z = x are linear equations. But x squared- 1 = y is not linear because x is being multiplied by x and y over x = 3 is also not linear because a variable y is being divided by another variable x. In real world situations these equations typically come from observed or modeled relationships between quantities. Let's take a look at a modified example from machine learning for trading. Suppose I'm trading stocks and I have a choice of two stocks, A and B, that I can put into my portfolio. Let's denote the proportion of each in my portfolio by w A and w B. Which are weights between 0, meaning none of that stock is in the portfolio, and 1, meaning that the portfolio is entirely that stock. Note that wA and wB have to add up to 1 since any portion of the portfolio that isn't A must be B. This gives me the linear relationship wA + wB = 1. Each stock also has an associated metric called its beta value representing how correlated its price movements are with those of the overall stock market. The beta value of my portfolio is a weighted average of the beta values of its individual components. So if the beta value of A is -1, and the beta value of B is 2, then the beta value of my portfolio is wA(-1) + wB(2), or simplified, 2wB- wA. Having a beta value of 0 means that the portfolio is uncorrelated with the market. So it has minimal market risk. Suppose I want to choose my weights so that I minimize market risks. Then that means I need the weights to satisfy the equation to wB- wA = 0. This gives me a second to linear relationship between the weights of the stocks in my portfolio. In other words, I now have two lines. Finding what values of wA and wB satisfy both equations. In other words finding at what point the two lines intersect at tells me what the weights of the stocks I choose should be in order to minimize my portfolios market risk. Often variables are factors that are within our control but which we want to set to correct values in order to satisfy some constraints. In the previous example our variables were the weights of the stocks in the portfolio. And we had two constraints that we wanted the weights to satisfy in order to achieve some goal. Interpreting this problem in geometric language, we created two lines and we want to find the point where they intersected. What we created is called a system of linear equations. Finding points that satisfy all the equations at the same time is called solving the system. In practice this is how problems involving the intersections of geometric objects come about. These types of problems show up in fields as diverse as economics, biology, chemistry, physics, engineering, and of course computer science. In this lesson we will use our tools from the first lesson on vector operations to solve this ubiquitous problem of finding intersections. As our first step into studying intersections, we'll consider the most basic situation, two lines in two dimensions. While basic, we will start to see structures and procedures that will remain present in more complex examples. A line in two dimensions can be defined by two essential properties. A point it passes through which I'll call the base point and a vector that gives the direction of the line. Given the base point x naught and the direction vector v, we can represent all points on the line in the form x of t equals x naught plus times v. Notice that I'm abusing notation by treating points as vectors here. This means given any number t, we can find a point on the line by plugging in t to the expression x naught plus t times v and computing it. If we were to theoretically plug in all possible values of t and plot them, we'd end up with a plot of all the points on the line. This expression is called a parameterization of the line. Since we are expressing the line via the perimeter t. For example, for the line x of t equals the vector one negative two plus t times the vector three, one. Taking t equal to zero gives us the output one negative two which is the base point for this parameterization of the line. If we take t equals one then we get the output four negative one another point on the line. If we take t equals negative one-half then we get negative one-half negative five-halves, yet another point on the line. Note that a given line has infinitely many base points, since we can pick any point on the line to be the base point. And it has infinitely many direction vectors, since we can multiply any direction vector by a positive scalar and it will still point in the same direction. We could also multiply the direction vector by a negative number and the same set of points will be in parameterized just starting from the opposite direction. We can multiply the direction vector by zero though, or else we'll collapse the line to a point. You may have seen lines written in the form y = mx +b where b and m are numbers. This equation and codes both a base point and a direction. b is the y intercept, the y coordinate of the point at which the line intersects the y axis. This gives the base point 0,b. m is the slope of the line which can be translated into a direction vector of the form 1 m. For every unit of movement in the x direction there are m units of movement in the y direction. This equation for representing a line isn't perfect however, since we do not have the ability to define vertical lines. A more general form of equation for representing a line is Ax+By = k. Where A, B and k are numbers and at least one of A or B is non-zero. To reconstruct a base point from this equation, there are two cases. Assuming B is not zero, you can set x to be any number. I'll take it to be 0 for this example, which becomes the x coordinate of the base point. Then you can solve for y to find the y coordinate of the base point. Having taken x equals zero, this becomes y equals k over B. So a base point for the line is 0, k over B. On the other hand if B does equal 0, then A will not be 0. Let y = 0 in this case solving for x gives us x = k/A. So a base point will be k/A, 0. So for this example here two possible base points would be 0, 2 and 3, 0. Now let's think about the direction vector. If we write down any two points on the line, then the vector connecting the point will be a direction vector for the line. In this example, we've already written down two points on the line but for a general case, how would we find the direction vector? Let's start with a very particular case, when the constant term k = 0. Here the equation reduces to just Ax+By = 0 and notice that this line passes through the origin since the base point for this formula is 00. If we draw the vector from the origin to another point x, y on the line then the corresponding vector xy will be a direction vector for the line. Now here's a neat algebraic trick, rewrite the left hand side of this equation as a dot product of two vectors. Ax+By becomes the vector AB dotted with the vector XY. Now what does this equation mean? It means that the point x,y is on this line. If and only if the vector xy, a direction vector for the line is orthogonal to the vector AB. This means we can find a direction vector for this line by picking some vector orthogonal to AB. One good way to do that, in two dimensions at least, is to just swap the coordinates in the vectors in the gate one. So picking B,-A, would give us a direction vector for this line. So for the case of this pink line, a direction vector would be 3,-2. Now what if k doesn't equal zero? What changing k does is it shifts the line off of the origin. It doesn't actually change the direction of the line at all. So a direction vector for Ax+By = k regardless of what k is will still be B,-A. So, both the pink and green lines will have three negative two as direction vectors. One reason this way of looking at the equation Ax+By = k is important, is that it gives us an implicit description of the line as a single equation. In particular as the set of points such that, a particular vector dotted with those coordinates x and y equals a given number k. this form also lets us quickly read off the coordinates for a vector that is orthogonal to the line also called a normal vector to the line. In this case a normal vector for the line 2x+3y = 6 will be 2 3. Since any vector that is parallel to this line will be orthogonal to the vector 2 3. We will explore this concept further when we look at planes in three dimensions as well as when we start using linear algebra to solve systems of equations. Let's start exploring intersections with the following. Given two lines in two dimensions, where do they intersect? First, let's make a definition. Two lines are parallel, if their normal vectors are parallel. So for example, these lines are parallel because their normal vectors differ by a factor of negative two. Whereas these two lines are not parallel because their normal vectors are not parallel. It may seem strange that we are using normal vectors in our definition instead of direction vectors. In two dimensions, the definitions are equivalent. However, in higher dimensions, there will not be a single direction vector, so using the normal vector will be more extensible later on. If two lines are not parallel, then they will have an intersection at a single point. However, if two lines are parallel, then there are two possibilities. They might not intersect at all, or they might turn out to be the same line, so-called coincident lines, in which case they will intersect in infinitely many points. In linear algebra, this trichotomy of cases for intersections is the rule. When asked whether two linear objects intersect, the answer is that there will either be a unique point of intersection, there will be no common intersection or there will be infinitely many intersection points. There cannot just be two or three intersections. So given two lines in two dimensions, how can we tell whether they have an intersection? Well, first, we should check whether they are parallel by looking at their normal vectors. If their normal vectors are parallel, then we need to check whether they are the same line. How can we do this? Suppose the two lines are coincident, that is, the same line. Then, if I take any point from the first line and any point from the second line, in reality, from the same line, then the vector connecting the two points will be parallel to both lines. In other words, this vector will be orthogonal to both lines' normal vectors. On the other hand, suppose the two lines are not coincident, but still parallel. Then these lines have no common intersections. So if we pick a point from the first line and a point from the second line, then the vector connecting the two points won't be in the direction of the lines. In other words, this connecting vector won't be orthogonal to either of the normal vectors. This gives us a computable test for whether two lines are the same once we know that they are parallel. Two parallel lines are coincident or equal if and only if the vector connecting some pair of points, one from each line, is orthogonal to both the lines' normal vectors. Okay, now let's suppose we have two nonparallel lines. So we know they will intersect. How do we find their intersection point? Let's write the lines in their standard forms. Ax plus By equals k1 and Cx plus Dy equals k2. I write this left curly brace to indicate that this is a system of simultaneous linear equations. So how do we go about solving this system? It would be nice to divide the first equation through by A, so that we can solve for x. However, what if A equals 0? Well if that were the case, we could move to the other equation and then try dividing this second equation through by C. But what if C is also zero? Actually, that can't happen because if A and C are both zero, then the lines are just of the form Y equals k1 over B, and Y equals k2 over D. Which are both horizontal lines, but since we assume the two lines were not parallel, they both can't be horizontal. So that means either A or C has to be non-zero. To make matter simpler, let's just do the following at the beginning of the whole procedure. We check to make sure A does not equal zero. If so, then we swap the order of the two equations. So A and C would swap places. B and D swap places, and k1 and k2 also swap. Then we can always assume that A does not equal zero. So with that assumption that A is not zero, we can go ahead and divide through by A giving us x plus B over Ay equals k1 over A. Now instead of solving for x directly, I'm going to do something slightly different. I will now subtract C times the left hand side of this equation from the second equation to maintain the balance of the second equation. I'll also subtract C times k1 over A from the right-hand side of the second equation. So now our second equation looks like Cx plus Dy minus Cx minus Bc over Ay equals -Ck1 over A plus k2. The C terms cancel, and after collecting the y terms together we get quantity D minus BC over A times y equals negative Ck1 over A plus k2. Now let's multiply through by A to make the equation a bit nicer to work with, and then to solve for y, we can divide through by AD minus BC or can we? What if AD minus BC equals zero. Well, if this is the case, then let's rewrite the left-hand side as a dot product using the trick we used before. This gives us the vector AB dotted with a vector D minus C equals zero. Now this first vector is a normal vector to the first line and the second vector D minus C is parallel to the second line. Looking at this geometrically, this means that the line Ax plus By equals k1 is parallel to the line in the direction of D minus C, which is Cx plus Dy = k2. But this violates our initial assumption that the lines were not parallel. So AD minus BC cannot equal zero under this assumption. Great so that means we can divide through by AD minus BC, so y equals negative Ck1 plus Ak2 over the quantity AD minus BC. Now let's use the same subtraction process to go from this equation back to our first equation. If we subtract B over A times y from the left-hand side, then we can subtract B over A times all of this, from the right-hand side. This gives us the messy looking equation, x = ((k1 / A)- (B / A)) ((-Ck1 + Ak2 / AD- BC)). Let's pull a factor of one over A to make it a bit cleaner and then forming a common denominator for the k1 term allows us to consolidate the fraction inside the parentheses. This lets us kill the BCk1 terms and then cancel the factors of A to get this final form x equals Dk1 minus Bk2 all over AD minus BC. So the point of intersection of these two lines, Ax plus By equals k1 and Cx plus Dy equals k2. Assuming they are not parallel lines, is this point x equals Dk1 minus Bk2 over AD minus BC and y equals -Ck1 plus Ak2 over AD minus BC. This formula might look rather messy at first blush, but it actually exhibits some nice symmetries that we'll explore a bit later. For now though let's go ahead and code up a function to compute this intersection between two lines. For this exercise, I made a new Line class that I'll be using in my Python examples. Here are the basic functions for the class that I've written. You can find the stub with the initial Python code in the downloadable section on this page. The Line class will make use of the Vector class from the previous lesson. Like in the previous lesson, I'll ask you to write some functions, this time for the Line class, to implement some of the concepts we've discussed. The initializer takes two pieces of information. A normal vector to the line and the constant term for the lines equation. The normal vector gives me the coefficients for the standard form of the line. Note that I don't take a directional vector as an argument. This is because in two dimensions, we can quickly find a directional vector for the line from the normal vector if we need to. More importantly, as we'll see later on, the normal vector representation is more easily generalizable to higher dimensions. Similarly, we can compute a base point quickly by choosing a variable with a non-zero coefficient and setting the other variable to zero. I've implemented this in a separate compute base point function. The str function outputs the standard form of the lines equation using the variables x1 and x2 instead of just x and y. It's a bit messier, but it's set up so that later, we can generalize it to equations in higher dimensions. Finally, there is a helper method called first_nonzero_index. We will use it to find the first non-zero coefficient of an equation. At the bottom, I have implemented in extension to the decimal class that allows us to test whether a number is within some tolerance of zero. We will use this to avoid incorrect answers due to round off error. There are three functions I would like you to implement for this exercise. First, I'd like you to create a function that determines whether two lines are parallel or not. Here, I'd recommend using functionality from the Vector class to make your function easy to write. Next, I'd like you to write a function that tests whether two lines are equal, where we define equal to mean consists of the same set of points. So for example, if l1 is the line with normal vector 1, 1 and base point 0, 1, and l2 is the line with normal vector negative 3, negative 3 and the base point minus 3, 4, even though these lines have different normal vectors and base points, they still define the same line. Since their corresponding equations x + y = 1, and negative 3x- 3y = negative 3 are satisfied by the same points. Feel free to use your parallel checking function in this function. And you may want to refer back to the previous video to see how to determine if two parallel lines are equal. Finally, write a function that determines the intersection of two lines. Ideally, if the lines are not equal but parallel, the function should return some indication that the lines have no intersection. And, if the lines are equal, the function should return some indication that the intersection is the entirety of both lines. Otherwise, just return the single point of intersection of the two lines. Again, it will help to leverage the functionality of these two functions in writing the intersection function. Once you're done writing your functions, here are the questions I'd like you to answer to check your work. For each pair of lines, fill in the coordinates of the intersection point to three decimal places of accuracy, or else check the appropriate box under no intersection or same line. The first system I'd like you to solve is 4.046x + 2.836y = 1.21, and 10.115x + 7.09y = 3.025. The second system I'd like you to solve is 7.204x + 3.182y = 8.68, and 8.172x + 4.114y = 9.883. The third system I'd like you to solve is 1.182x + 5.562y = 6.744, and 1.773x + 8.343y = 9.525. Here are the answers. The first two equations define the same line. The second two equations define lines with a unique intersection, which is the point 1.173, 0.073. The third pair of equations define parallel lines which are not the same. So there is no intersection. Now here are the functions I added to my Python module to find the intersections. I have the function is parallel to first, it leverages the function of the same name we coded earlier for the vector class. It just returns whether the normal vectors of the two lines self and ell are parallel since this is how we define lines to be parallel. Next I added the function E.Q. which checks whether two lines are the same. Remember that we can check whether two parallel lines are equal by picking one point from each line and looking at the vector connecting them. If that vector is orthogonal to the normal vectors of both lines. Then the two lines are equal. Here I first checked that the two lines being compared are parallel. If not then there's no chance of there being equal then I compute the vector connecting the two lines base points. Finally, I check whether that vector is orthogonal to the normal vector of the first line. I don't need to compare it to the normal vectors of both lines, since I know the lines are already parallel. So being orthogonal to one normal vector is equivalent to being orthogonal to the other. Here I've added some more code to deal with the technical case that we haven't encountered yet. What to do if the normal vector of a line is the zero vector. This case will actually come up later in the lesson. But I want to go ahead and put it into our code now. Basically a normal vector of zero means that the left hand side of the equation defining the line is zero, since the coefficient of each variable is zero. In that case I check the constant terms of the two lines to see whether they are the same. And if so then I declare the lines to be equal. If not then they are different lines. You don't need to add this to your code yet but you may find it helpful to come back to later on. Finally, I added the function intersection width, using the formula from earlier I compute the X and Y coordinates of the point of intersection. However, the two lines turn out to be parallel then I'll end up with a zero division error when trying to compute one_over_denom. If this happens, then I check whether the lines are equal. If so, then I return self since the intersection of the two lines is a line object itself. On the other hand, if the lines are not equal, then there will be no intersection in which case I return none. After coding the functions, here is what I used to answer the questions from earlier, and here is the output when I run the code. So far, we've been looking at intersections of two lines in 2D. But what happens if we introduce a third line? Let's look at the system of equations Ax + By = k1, Cx + Dy = k2, and Ex + Fy = k3. Now in general, unless AB and CD happen to be parallel vectors, the first two lines will have a single intersection point. Similarly, unless AB and EF are parallel, the first and third lines will have a single intersection point. And likewise the second and third lines will have a single intersection unless CD and EF are parallel. However, there's no guarantee that these intersection points will all coincide. Sometimes they can as in this set of lines that all pass through a common point. More generally, as in the set of lines, the three intersection points are all distinct. Since systems of linear equations are questions about common intersections, that is points that are common to all the objects being considered, this latter case would have an empty common intersection. To describe systems with new common point of intersection, we use the term inconsistent. We will be using this terminology in the future. So, I wanted to go ahead and introduce it here. Let's move up a dimension into a three dimensional space and let's start out by thinking about planes. We're going to start by studying planes in three dimensional space because they are highly analogous to lines in two dimensional space. Just like a single linear equation Ax + By = k in codes aligned in two dimensional space A single linear equation Ax + By + Cz = K encodes a plane in three-dimensional space. Another way to view the equation Ax + By + Cz equals k is like before, a dot product of two vectors, A B C. x y z = k. And like before with lines. If we take A, B, C dot x, y, z = 0. That means that vectors from the origin to points x, y, z in the plane are orthogonal to the vector A, B, C. Thus A, B, C is a normal vector to the plane. And also like with lines changing the value of the constant term k shifts the plane but it doesn't change the direction of the plane at all. So regardless of the constant term the plane defined by Ax + By + Cz = k has ABC as a normal vector. Let's call two planes with parallel normal vectors parallel, since their faces are in the same direction. If we have two parallel planes Ax + By + Cz = k1 and Ax + By + Cz = k, the same coefficients in front of each variable but different constant terms And that means any point XYZ satisfying the first equation can't satisfy the second and vice versa. So two different parallel planes do not intersect. This is just like the case with lines in two dimensions, two different lines with the same normal vector don't intersect. We also have another analogy to our treatment of lines in two dimensions. Remember that if we knew two lines were parallel then we could check to see if they were the same line by finding the vector between a point on one line and a point on the other. If that vector was orthogonal to each of the lines normal vectors then the lines are the same. Similarly if two planes are parallel that is they have parallel normal vectors then we can pick a point on one plane and another point on the other. If the vector connecting them is orthogonal to the planes normal vectors then the two planes must be the same. We know enough now to start building a plane class that will be very similar to our line class. Let's go ahead and do a quiz so we can get the framework built before starting to tackle the problem of intersections of planes. For this quiz, I've provided a Python module called plane.pie. In it, I provided very similar functionality to the line.py module. In fact, it is the same except that all instances of the word line have been replaced with plane. You can find this module in the downloadable section of this page. For the quiz, I'd like you to add two functions to the module. One to test whether two planes are parallel, and another to test whether two planes are equal. To check your code, here are some test cases. Please indicate whether each of the following pairs of planes are equal, parallel but not equal, or not parallel. The first pair of planes is -0.412x + 3.806y + 0.728z = -3.46. And 1.03x- 9.515y- 1.82z = 8.65. The second pair of planes is 2.611x + 5.528y + 0.283z = 4.6 and 7.715x + 8.306y + 5.342z = 3.76. The third pair of planes is -7.926x + 8.625y- 7.217z = -7.952. And -2.642x + 2.875y- 2.404z = -2.443. The first pair of equations define the same plane. In fact, you can obtain the second equation from the first by multiplying through by the common factor negative 2.5. The second pair of equations are neither parallel nor equal. The coefficients of the variables in the first equation cannot be transformed into the coefficients in the second equation by multiplying by a common factor. So the normal vectors for the two planes are not parallel, meaning that the planes are not parallel. Since equal planes must be parallel this means the planes aren't equal either. The third pair of equations define parallel planes. If you just consider the coefficients of the variables in the first equation, dividing them each by a factor of three will give you the coefficients of the second equation. This means that the planes have parallel normal vectors, so they are parallel. However, the planes are not equal. Now the fun part of this is that the code I wrote for this problem is almost exactly the same as the code I wrote for the lines. The only difference I made is that I use the variable P to stand for plane, instead of L to stand for a line. But, logically, my is_parallel_to and eq functions are equivalent to the functions for the line class. And here's the test code I used to answer the questions. Running it gives me the answers I showed earlier. Now let's work on finding the intersection of planes. Let's start by assuming we have two planes Ax + By + Cz = k1 and Dx + Ey + Fz = k2, where the normal vectors A, B, C and D, E, F aren't parallel. Given two non-parallel planes, I claim their intersection will be a line instead of a single point. >From the picture, this may seem intuitive but how can we show this fact algebraically in all cases? Let's use a tool from the previous lesson, the cross product. Consider the cross product of these two normal vectors n1 and n2. Since the two normal vectors are not parallel their cross product will not be zero. Also remember that n1 cross n2 will be orthogonal to both n1 and n2. Let's pick an arbitrary point in the intersection of the two planes and call it x0. If x is any other point in the intersection, then let's call d sub x, the vector from x0 to x. Since x0 and x are both in the first plane, the vector connecting them, d sub x, will be orthogonal to the normal vector to the first plane, n1. By similar reasoning since x0 and x are both in plane two, the vector connecting them dx will be orthogonal to n2. Since dx is orthogonal to both n1 and n2 and since we're in three dimensions, dx must be parallel to the cross product of n1 and n2. Since dx is parallel to n1 cross n2 no matter what x we picked, that means the intersection of these two planes has to be a line that is parallel to n1 cross n2. So what are the consequences of this? Well if we have a system of two linear equations in three dimensions, then the set of point satisfying the system, could form a line's worth of solutions, if the equations define two planes that are not parallel. No solutions, if the equations define different parallel planes, or a whole plane's worth of solutions, if the two equations define the same plane. However, if we introduce a third equation to our system, then we could have another possibility. The common intersection of the three planes could be a single point. Or it could be a line, a plane, or nothing at all. There are a few important observations we should make here. Recall that one equation in two variables defines a line or a one dimensional linear object. A single equation in three dimensional space defines a plane, which is a two dimensional linear object. In general a single equation in some number of variables, will define a linear object that has a number of dimensions one less than the number of variables. However we need to be careful to distinguish between an equation say in two variables, and an equation in, say, three variables, where one of the variables has a coefficient of 0. In the former case, the equation defines a line in 2D. But in the latter case the equation defines a plane in 3D. Being aware of the context of your problem will help you avoid this error. Another observation is that the coefficients of the variables in a linear equation give the coordinates for a normal vector to the linear object it defines. So for the line Ax + By = k we obtain the normal vector AB and for the plane Ax + By + Cz = k, we have a normal vector of ABC. This trend will continue as we move into higher dimensions. And again, this is why the normal vector is a useful piece of information for defining a linear object. One last observation is that we need at least two lines in two variables to obtain a unique intersection of lines. And we need at least three planes in three dimensional space or three variables to obtain a unique intersection of the planes. This observation will generalize as we progress into higher dimensions, and in a little bit we'll see an algebraic reason why it holds. Now let's consider the computational problem at hand. Given a system of three planes. Can we determine their unique intersection point? As we can see doing just a few steps. The algebra is going to be a nightmare if we try to symbolically solve for the solution. Also, it's not clear how generalized any formulas we end up with to systems and higher dimensions, which we eventually like to be able to solve. Instead, is there some sort of algorithm we can follow that will always find the solution to a linear system? In fact, there is. The algorithm we will follow is based on three operations that we'll apply to the equations in the system. However, before we start trying to manipulate equations. Let's think about what sort of manipulations would be valid. Primarily we don't want to do anything that would change the geometry of the planes represented by the equations, and therefore change the solutions. Also, we should be able to reverse the operations we perform. In order to obtain the original pair of equations. First of all, it's certainly legal at any time to swap the order of two equations in the system. Since intersections of planes don't depend on the order in which we list them. This is perfectly fine. We don't change our solution set at all. Also, we can easily undo this operation just by swapping the two equations back to their original order. What's another type of operation we could do. Well, if we have an equation and we multiply both sides of the equation by the same number. Then the equation will stay true. However if we were to multiply through by zero. Then the operation would not be reversible. We would not be able to divide through by zero to obtain the original equation. So we need to add a caveat. We can multiply an equation by any non-zero number. Now, does this operation preserve the solution? Yes. Multiplying through by a non-zero scalar just multiplies the normal vector. Whose coordinates are these coefficients, by a scalar. So we'll be pointing in the same direction. Moreover, you can algebraically verify that any point you select on the original plane satisfies the new equation and vice versa. So the two planes consist of the same sets of points. I.e., they're the same. Also since we specify that alpha must be a non-zero number. The operation is reversible simply by multiplying through by one over alpha. Third, if we have two equations. We can add one equation to the other. Does this operation preserve the solution set? Well, suppose we had a point x, y, z that was in the intersection of two planes. In other words the point x, y, z satisfies both of these equations. Now suppose we added the first equation to the second equation. This equation we know to be true, because it was assumed true in the previous system. In the new second equation, this quantity is equal to k one, because of the first equation. And this quantity is equal to k two, because of the second equation from the old system. Therefore, this new system is true as well. X, y, z satisfies both of these equations. Therefore x, y, z is in the first plane and in the new second plane. Which means that the point x,y,z is in the new intersection of the two planes. Since this is true for any point, x, y, z in the old intersection. It means that all of the points in the old intersection are contained in the new intersection. Also, because we can easily reverse this operation by subtracting the first equation from the second in the new system. This means that the new intersection does not contain any points that weren't in the old intersection. So the intersections are the same. So the solution is preserved. Now suppose I modified the operation. So that instead of adding just one times the first equation to the second, I add an arbitrary amount. Alpha, times the first equation to the second equation. Notice that the same logic I applied earlier also holds in this case. So instead of just adding some equation to another equation. I can add a multiple of one equation to another. As I stated earlier, it's easy to reverse this operation as well. We just need to add negative alpha times the first equation to the second equation, to get back to the original system. What we would like to build is a procedure for reducing a system of equations to a simple a form as possible. Ideally, finding the unique solution of a system if it exists. We'll start by informally describing the procedure and using it to solve some example systems of equations. Then we'll explore some special cases that can occur during the algorithm. Once we have a better intuition about the algorithm then we'll begin to write code which will define the algorithm's precise operation. The algorithm we will discuss is a classic algorithm called Gaussian elimination. Though it is named after the nineteenth century German mathematician, Carl Gauss. The algorithm was known to the Chinese as early as the second century BCE. The basic idea of Gaussian elimination is to gradually clear variables and successive equations. So the first equation will have all the variables x, y, and z. Then the next equation will end up with just y and z. And then the last equation will just have z. That way the last equation just becomes z equals some number and then since the second equation only has y and z and we know the value of z, this allows us to find the value of y. Finally, we use the values of y and z in the first equation to find the value of x, giving us the solution. Presuming there is a unique solution, after all. We can do this by adding or subtracting appropriate amounts of an equation from all equations underneath. Let's look at this example system. x-3y+z = 4. -3x+9y+z = 0 and y-z = 1. In order to clear the x term from the second equation, we can add to three times the first equation and to the second. Remember that adding or subtracting multiples of equations is a valid operation. This transforms the second equation into 4z = 12. Now we could go ahead and use the second equation to find the value of z. But for now we're going to ignore the short cut and show how the procedure works in general like we did before with the x term in the first equation. We want there to be a y term in the second equation, so that we can use it to clear y terms below the second equation. That way we have successive loss of variables. If like in this case, there is no y term in the second equation, we look to below the second equation for one with a y term. Then we can swap the second equation for the equation with the y term. In this case, we'll swap equations two and three to get a y term in the second row. Again, this is one of our valid equation operations. Now the second equation is y-z = 1. And the third equation is 4z = 12. We have no y terms underneath the second equation so there are no more terms to clear. Finally, since we're at the bottom equation there are no terms underneath the z term in the third equation. Notice that our system of equations is in a special form each variable that appears as a leading term doesn't appear in any further equations. this form is sometimes given a special name, such as triangular form or row echelon form. In this lesson we will call it triangular form. However, this system does not represent the same set of planes as the system we started with by adding and subtracting equations. We do change the planes involved. However, we have changed them in such a way. That the common intersections of the planes remains the same. Once we've reached triangular form, our next task is to clear upwards. So that way we isolate each variable in its own equation. First, we're going to solve for z in the bottom equation by multiplying it through by one-fourth. Notice that this is the third equation operation we had defined earlier. This gives us the bottom equation now reading z = 3. To clear upwards we can add the third equation to the second equation. This makes the second equation become y = 4. Then subtracting the third equation from the first equation gives us x-3y = 1. Notice that z is now in its own column. Now let's focus on y, we've already solved for y, y = 4. So the last step is to clear upwards so that y is also in its own column. To do this, we add three times the second equation to the first which makes the first equation become x = 13. Using just the three operations we spelled out earlier we have found the sole point of intersection of the three planes described in the original system of equations. Note that we can check that the point 13, 4, 3 satisfies all three equations in the original system. I'd like to run through this set of operations one more time but this time including a plot of the three planes that these equations represent. That way you can see visually what Gaussian elimination is doing. One thing to keep in mind is that the common intersection of the planes defined by these three equations should never change during our operations that we use. After the first operation, notice that the green plane has rotated along its intersection line with the red plane and is now entirely horizontal. The second operation, which swaps two equations, just changes the color of two of the planes. The third operation, which multiplies equation three by one-fourth, doesn't change the blue plane at all. By adding the third equation to the second equation and isolating the y variable. The green plane which is y = 4, is now perfectly orthogonal to the y axis. Next, clearing the z variable from the first equation means that the red plane no longer has any dependency on the z variable. Finally, eliminating the y variable from the first equation makes the read plane perfectly orthogonal to the x axis and now the meeting point of the three planes is easily identifiable. Just to check your understanding so far, take a few moments to work out the solution to the following system of equations. The system consists of -x+y+z=-2, x-4y+4z = 21, and 7x-5y-11z=0. Please enter the solution by entering the x coordinate, y coordinate, and z coordinate in the answer boxes. The solution to the system is x equals one y equals negative three and z equals two. I'll run through the steps of Gaussian elimination on the system, so you can see how I solved it. First, I want to clear the x terms in the equations below the top equation. To do this, I add one times the first equation to the second equation. The second equation now reads negative three y plus five z equals nineteen. And then I add seven times the first equation to the third equation, which becomes two y minus four z equals negative fourteen, having cleared the x terms in the second and third equations. I now want to clear the y term in third question, since this will put the system in triangular form. To do this, I add two-thirds times the second equation to the third equation, which becomes negative two-thirds z equals negative four-thirds. Now that the system is in triangular form, I will work backwards to solve for the variables. First, I multiply the third equation by negative three halves. This gives me that z equals two. Next, I subtract five times the third equation from the second equation. Which gives me negative three y equals nine. And then I subtract the third equation from the first equation to get negative x plus y equals negative four in the first equation. Now to solve for y, I multiply the second equation through by negative one-third, and that shows me that y is negative three. Next, I subtract the second equation from the first, giving me negative x equals negative one, and I multiply the first equation through by negative one, which yields x equals one. This completes the procedure, and gives me the unique intersection point, one, negative three, two. In the previous examples we looked at systems of equations that had a unique solution. However, as we've seen, it's not always the case that there is a unique solution to every system. We need the Gaussian elimination procedure to be robust against these cases. Let's look at the following system. x + 2y + 3z = 1, 3x + 2y + z = 0, 7x + 6y + 5z = 2. We can start solving the system as usual, clearing the terms beneath the top x term. Subtract three times the first equation from the second to get -4y- 8z = -3. And then subtract seven times the first equation from the third, to get -8y- 16z = -5. Then we drop down to the second equation and start to clear the y terms beneath -4y. Subtract two times the second equation from the third, and now we're in a very interesting situation. There is nothing remaining on the left hand side of the third equation, but there is a one remaining on the right hand side, giving us the equation 0 = 1. What does this mean? Well, the original system of equations assumes that there are numbers x, y, and z, which satisfy the three simultaneously. Since we used legal operations on the original system and obtained a false statement, it means that these supposed numbers x, y, and z, which satisfy the system, must not exist. In other words, this contradiction tells us that the system is inconsistent. It has no solutions. In fact, if we ever run into the case that the left hand side of an equation is zero and the right hand side is not zero, we can immediately stop and say that the system has no solutions. This is a nice criterion for finding systems with no solutions. But what about systems with more than one solution? This is a bit trickier. Let's modify the system from before just a little bit, making the constant term in the third equation one instead of two. Again, we start by subtracting three times the first equation from the second and continue by subtracting seven times the first equation from the third. The third equation now reads, -8y- 16z = -6. Now as before, subtract two times the second equation from the third, unlike last time, the bottom equation is not a contradiction. Certainly zero equals zero. So what are we to make of this? Well since zero always equal zero, regardless of what X, Y, and Z are, the last equation becomes redundant. The system would have the exact same solution if we dropped it all together. This is how to interpret an equation of the form zero equals zero, the row it occupies was originally a redundant equation. It gives no extra information about the solutions that was not contained in the equations above it. In fact, notice that in the original system, if we added the first equation to two times the second equation, we would get the third equation. So what do we do now? The system essentially has only two planes in it, meaning that the solution cannot be a single point, since the planes aren't parallel, the solution is that will be a line in fact. Is there anything else we can do? Yes, like we did with lines earlier, we can parameterize the solution set, in other words, we can describe all the points in the solution set as a function of some number of parameters. How can we do this? First after dropping the zero equals zero equation, notice that we cannot clear terms downward anymore. Instead what we will do is identify what are called pivot variables. These are variables whose values will be determined by the values of the non pivot or free variables. The pivot variables are the variables which are leading terms in our equations in this case X and Y are leading terms. So they are the pivot variables. Since Z is not a leading term in any of the equations, it will be a free variable. Since Z is a free variable, it will become a parameter in our parametrization of the solution set. Now let's continue our Gaussian elimination algorithm. So we can end up with the parametrization of the solution set of our system. In the second equation. In order to make the pivot variable have a coefficient equal one, multiply through by negative one fourth. The equation now reads y + 2z = 3/4. Then like we normally do in Gaussian elimination, we will clear the terms above the leading y term. So we subtract two times the second equation from the first. This gives us x- z = -1/2 in the first equation. Since the leading coefficient of each equation is now one and the leading variables only appear in one equation each, we have simplified the system as much as possible. This form is called reduced row-echelon form, or rref for short. We can now parametrize the solution set in the following way. Suppose xyz is a point in the solution set, then x will have to equal -1/2+z by rearranging this top equation. Similarly, by rearranging the second equation we see that y has to equal 3/4- 2z. Finally since z is a free variable, we set it equal to itself. This means that for every real value of z, this point is in the solution set. Another way we can write this is to split the vector into a sum of a constant vector and a vector with a z term. This form should be familiar. It is how we parameterized the line previously, except now we're using z instead of t for the parameter name. The base point of this parameterization is negative one-half, three-fourths, zero, and the direction vector of this parameterization is one, negative two, one. Here are some different points that are in the solution set that we can generate by picking different values of z. Picking z=0 gives us the basepoint. Regardless of the value of z that is picked, we can check that the point will be a solution to the original system of equations. All right, let's do some more practice exercises with Gaussian elimination. Each of these systems could either have a unique solution, no solution, or infinitely many solutions. The first system is x-2y+z=-1, x-2z=2 and -x+4y-4z=0. The second equation is y-z=2, x-y+z=2, and 3x-4y+z=1. The last system is x+2y+z=-1, 3x+6y+2z=1, and -x-2y-z=1. For this quiz, if there's a unique solution, fill in the appropriate coordinates for the answer boxes labeled x, y, and z. If there's no solution, check the box on the right. If there are infinitely many solutions, fill in these answer boxes with the appropriate expressions using the free variable or variables as parameters. So if x and z were the pivot variables, then the expressions should only contain numbers in the variable y. Here are the solutions for the systems. The first system turns out to be inconsistent. The second system has unique solution, X = 4, Y = 3, Z = 1. And the last system has infinitely many solutions. The parameterization of the solution set is X = 3- 2Y. Y equals Y and Z = -4. Let's look at how to solve the first system. Start by clearing the first column underneath the top X term. We'll subtract the first equation from the second to get 2y- 3z =3 and then add the first equation to the third equation to get 2y- 3z=-1. These bottom two equations don't look so good. We have the same quantity on the left hand side equaling both 3 and negative 1. In fact, after subtracting the second equation from the third, we have the contradictions 0 =- 4. So we can stop here. The system is inconsistent and has no solution. Now let's look at the second system. We need to have an x term in the top equation in order to proceed with eliminating the first column. So let's start by swapping the first and second equations. And then we can subtract three times the first equation from the third. The X column is cleared, so now we can start clearing the Y term in the third equation. We can do this by adding the second equation to the third. We're now in triangular form and we have three distinct leading variables in the three rows. So things are looking good for us to have a unique solution. Now let's solve for z by multiplying the bottom equation through by negative one-third. And now we have that z = 1. Now let's add the third equation to the second equation, which incidentally gives us that y = 3. Finally, let's subtract the third equation from the first to finish clearing the last column. This gives us x- y = 1 in the last thing we need to do is add the second equation to the first to finish clearing the why terms. This gives us the unique solution x = 4 y = 3 z =1 to the original system of equations. Now let's look at the last system. Our first step will be to subtract 3 times the first equation from the second equation. This makes the second equation into your- z =4. Then, we'll add the first equation to the third, which gives us an equation reading 0 = 0. Our equation is in triangular form now. So we can identify the pivot variables which will be x and z. Since y is not a pivot variable, we have a free variable, meaning we're going to have an infinite number of solutions to our system. Now, we can go ahead and ignore the 0 = 0 row, because that's just redundant information. Concentrating on the first two equations, to isolate z, we need to multiply the second equation through, by -1. This gives us z=-4, then we can subtract the second equation from the first to eliminate z term in the first equation. This means the first equation reads x + 2y= 3, since the pivot variables x and x are now in only one equation each, we can parametrize the solution set for the system in terms of the remaining variable y. If the point x, y, z is in the solution set then by rearranging the first equation x will equal three minus 2 y. Y, since it is a free variable will be equal to itself and z because of the second equation will equal negative four. This gives us a parameterization of our solution set. With the base point of 3, 0, -4, in a direction vector -2, 1, 0. So in our answer boxes we write x = 3- 2y, y = y, and z = -4. We've solved several examples of systems of planes now using galaxy and elimination. Before we start tackling the programming aspect of this problem, let's take a few minutes to review some characterizations of systems of equations that will help us in programming the algorithm. Here's the first question. Which of the following statements will be necessarily true about a system of equations that has no solution? And which statements are sufficient to prove that a system has no solution? And let's assume for now that we're only talking about systems of equations in three dimensions. The first statement is, the planes defined by the equations do not have a common intersection point. The second statement is, there are more than three equations in the system. And the third statement is, we encounter an equation of the form 0 = k for some non-zero number k while doing Gaussian elimination. Check the appropriate boxes next to each statement. All right, let's look at the solution to systems with no solutions. Let's go through each of the statements. For the first statement, the planes defined by the equations do not have a common intersection point. In fact, this is more or less the definition of what it means for a system not to have a solution since. A solution to a system is a common intersection point of the planes defined by each equation. Second, there are more than three planes or equations in the system. It may be tempting to select this as a criterion for having no solutions, especially if you are familiar with overdetermined systems of equations. However, there are instances of systems with more than four planes having a solution. Take a look at this system, x + y = 1, y + z = 0, z = negative 1, and x + 3y + 2z = 1. The first three lines of this system actually have a unique solution, 0, 1, negative 1. The last plane also contains the point 0, 1, negative 1. In fact, this last equation is just equation 1 plus two times equation 2. So even if a system of planes has more than three equations, it's still not enough to show that it's inconsistent. But is it necessary for a system to have more than three planes in order to be inconsistent? No, you can have inconsistent systems of planes with three or even just two equations. Here's an example, x + y + z = 0, and x + y + z = 1. These two planes are parallel since they have the same coefficients in front of their variables, but they aren't the same plane since they have different constant terms. Therefore, they don't intersect, so this system is inconsistent. Third, we encounter an equation of the form 0 = k, for some nonzero number k while doing Gaussian elimination. This condition is both necessary and sufficient. If we encounter a contradiction like this while going through the elimination procedure, then we know that the initial system could not have had a solution. Conversely, if we don't encounter a contradiction like this, then we always end up with the solution of some form, though it may be a parameterization of an infinite set of solutions. Now for the next question. Which the following statements are necessarily true about a system of equations that has a unique solution and which statements are sufficient to prove that a system has a unique solution? Again let's assume that we're only talking about systems of equations in three dimensions and also for this quiz we'll assume that the letter K refers to a non zero number. The first statement is the planes defined by the equations in the system all have a common intersection point. Second there are no equations of the form 0 equals 0 or 0 equals K found during Gaussian elimination. Third there are exactly three equations in the system and four after reaching triangular form each variable can be found as the lead variable in an equation. Let's also assume here for number four that we don't run into any equations of the form 0 = K if we do Gaussian elimination, since that would automatically mean the system has no solution. Check the appropriate boxes next to each statement. Okay let's look at the first statement. The planes defined by the equations have a common intersection. This is definitely necessary for the system to have a unique solution. Having a common intersection point means that there is a solution to the system. However, it could be the case that the planes have more than one common intersection point. So, having a common intersection point isn't sufficient proof that the system has a unique solution. Second, there are no equations of the form 0 = 0 or 0 = k found during Gaussian elimination. It might be tempting to say that this property is related to having unique intersections. But it turns out that this property is neither necessary nor sufficient. Why is this the case? Let's consider the system x + y + z = 0 and y + z = 1. If we solve the system, we never get an equation of the form 0 equals 0. In fact, the only operation we can do is to subtract equation two from equation one, giving us x = -1. The solution set is a line that can be parametrize as X, Y, Z equals negative one one line is E and Z. In other words, we have an infinite number of solutions, even though, we never saw a Euro equal zero. So never seeing zero equals zero, is not sufficient to say we definitely have a unique solution. On the other hand, we can run into instances of zero equals zero and still have a unique solution. Let's look at the system which we saw in the previous quiz. The fourth equation is equal to the first equation. Plus two times the second equation. So during L.C. elimination, it will be reduced to zero equals zero. Nevertheless, the system has a unique solution (0,1,-1). Let's look at the third statement in the list now. There are exactly three equations in the system. Well, we actually just saw a system with four equations but with unique solution. So certainly, this property isn't necessary, nor is it sufficient. Since we've actually seen plenty of systems with exactly three equations, which have either infinitely many or no solutions. For example, in a previous exercise, we saw a system of the form x plus 27 plus z equals negative 1. 3x plus 6y plus 2z equals 1 and negative x minus 2y minus z equals 1 and this system had a line's worth of solutions. However, there is a modification of this third statement that does have a relation to whether a system has a unique solution. If we change the word exactly to the phrase at least. There are at least three equations in the system, then the property does become necessary. But not sufficient for the system to have a unique solution. We've already talked about this geometrically since we showed that two planes cannot intersect in a single point. But we'll see an algebraic reason, which ends up being more generalizable after we consider the last statement of the quiz. After reaching triangular form each variable is a lead variable in an equation in the system. This statement is both necessary and sufficient for a system to have a unique solution. Under the assumption that we don't run into any zero equals k equations. Also, each variable that is a lead variable in an equation after reaching triangular form. Could be solved for any variables left over cannot be solved for and will end up being parameters for an infinite family of solutions. Thus, having all variables be leading variables in triangular form is equivalent to saying there are no free parameters. That is, each variable can be solved for explicitly. This last statement gives us a useful computable criterion. For determining whether a consistent solution will have one or infinitely many solutions. We just run the Gaussian elimination algorithm until the system is in triangular form. And then, check whether all the variables appear as leading terms in the resulting system. This criterion also explains why the statement "there are at least three equations in the system" is necessary. For a system to have a unique solution, because if there were fewer than three equations. Then there won't be enough equations for all the variables to show up as leading variables. Meaning, there isn't a unique solution. Last, let's talk about characterizing systems with infinitely many solutions. Which of the following statements are necessarily true about a system of equations with infinitely many solutions? And which statements are sufficient to prove that a system will have infinitely many solutions? Again, let's assume we're only talking about systems of equations in three dimensions. And we'll still assume that k refers to a nonzero number. The first statement, there are 2 equations in the system defining the same plane. Second statement, we encounter an equation of the form 0 = 0 while doing Gaussian elimination and assume we don't find any 0 = k. Third, there are fewer than 3 equations in the system. And last, after reaching triangular form, there's some variable which isn't the lead variable in any equation in the system. Assume again, that we find no 0 = k equations. Check the appropriate boxes next to each statement. Let's consider the first statement. There are two equations in the system that define the same plane. This seems promising, since it means that we'll get redundant information. But it is not actually a criterion for infinitely many solutions. Remember, that removing redundant information doesn't change the system solution. So we have to look at the rest of the system, not just the repeated planes to know what the solution set will be. For example, a system of any two non-parallel planes will intersect in a line as we saw earlier. This means the property is not necessary to have infinitely many solutions. Also, it's not sufficient. Just take any three planes with a unique intersection, and then just add a copy of one of the planes to the bottom of the system. Then we have a duplicate plane, but we still have a unique intersection. Now, let's look at the second statement. We encounter an equation of the form 0 = 0 during Gaussian elimination. This falls in the same trap as the first statement. The equation 0 = 0 is redundant information that tells us nothing about the solutions set. It is not necessary to run into an equation of the form 0 = 0 to have infinitely many solutions, just take any two non-parallel planes. Nor is it sufficient to find a 0 = 0 equation and declare that a system has infinitely many solutions. In the example we've seen several times now, the final row become 0 = 0 in Gaussian elimination, but the system has a unique solution. As for the third statement, there are fewer than three equations in the system. It is again neither necessary nor sufficient to form a counter example to necessity. Just take three copies of the same plane, the system will have three equations but infinitely many solutions, any point in the plane will be a solution. A counterexample for sufficiency is any two distinct planes that are parallel but not equal. There are just two equations but there is no possibility of an intersection. An interesting point though, is if we add the condition that the system is consistent. If we know there's a solution, then having fewer than three planes is sufficient to show that there are infinitely many solutions. Why? Well, remember that we need at least three planes to have a unique solution. Since we're assuming there is some solution, the only possibility left is that there are infinitely many solutions. Finally, let's consider the statement, after reaching triangular form, there's at least one variable that isn't the lead variable in any equation in the system, and there are no 0 = k's. This criterion is a bit more technical and unwieldy, but it's the one we're looking for. It's both necessary and sufficient to characterize a system as having infinitely many solutions. We talked about a similar statement for characterizing systems with unique solutions. Like we said then, a system with no contradictory equations will have a unique solution if and only if every variable can be found as a leading variable in triangular form. If not, then the variables that aren't leading variables will become parameters for the infinite family of solutions. As we saw in the first of the last three quizzes, deciding if a system has a solution at all comes down to looking for equations of the form 0 = k for some non-0 k. If we ever find one then we know that the system has no solution. But deciding whether a system has a unique solution or infinitely many solutions is much trickier. As we saw, it's not enough to just count the number of equations or look for an equation of the form 0 = 0. Instead, deciding whether there's a unique solution or not comes down to seeing if in triangular form we end up with every variable as a leading variable, which we also called pivot variables. If every variable is a pivot variable, then there's a unique solution. Otherwise, we're going to have infinitely many solutions. In fact, each non-pivot, or free variable, will be an independent parameter in our solution set. This means that the dimension of our solution set will be equal to the number of free variables we have. If there's one free variable, the solution set will be a line. If there are two free variables, the solution set will be a plane. As we move into higher dimensions, this pattern will continue. All right. Now it's time to try coding the Gaussian elimination algorithm ourselves. To start us off, I've written the template for a python module that represents a system of linear equations. I'll give a brief overview of it here. It is also available for download in the downloadable section in this segment and the rest of the lesson. We can initialize a new linear system object by providing a list of plain objects. The initializer make sure all of our planes live in the same dimension. So we can't mix planes living in three-dimensional space with objects such as lines living in two-dimensional space. There are currently three empty functions. Swap rows, multiply coefficient and row, and add multiple times row to row. These will be the focus of the first stage of building the Gaussian elimination algorithm. There's also a function which obtains which term of each equation is the first non-zero term. This will be useful for finding the pivot variables in each equation. Note that the terms are zero index instead of one indexed. Though we shouldn't need to worry about that. Since we won't be directly manipulating the indices of the lists that make up the system. We also have some utility functions available to us. The LEN function returns how many planes are in the system. In other words, how many equations are in the system. We can also use subscript notation to access a particular plane, or equation in the system. As demonstrated in the example code at the bottom of the module. There's also an str method that lets us print a pretty version of the system of equations. Finally, in the module I've included a simple extension of the decimal class. It gives us the ability to quickly check whether a decimal object is within a given tolerance to zero. In this case, the default tolerance is ten to the minus ten. If you execute the test code given at the bottom of the module. You can see the output of these functions on an example system. Feel free to take some time to play around with this example. For instance, by changing the coefficients to get a feel for the functionality of the linear system class. The first step in coding Gaussian elimination will be to code the three basic operations we've been using in the elimination procedure. Swapping two equations, multiplying an equation by a non-zero number, and adding a multiple of one equation to another. In the Linear System class, I've defined three functions to correspond to these operations, but have left them blank. For this quiz, go ahead and fill in those three functions, either in the module I've provided or in your own program. Make sure to test your code before moving on. You'll find some test cases in the instructor notes to run your code against. They will be in Python and will use the notation from the Linear System Module. But please feel free to modify the syntax as needed to run them in your own code. Once you're confident your code is working, check the box here and click Submit to complete the quiz. Now that you've finished coding, let's take a look at the code that I wrote. For the multiply_coefficient_and_row function, I have to make sure to multiply both the coefficients of the variables in the equation represented by the plane's normal_vector and the constant_term by the same multiplicative factor. Notice that I make a new plane object here instead of just altering the normal_vector and constant_term of the current plane object. If I had altered the existing plane, I'd need to reset the plane's base point attribute as well or else I would introduce some subtle bugs. Finally, for the add_multiple_times_row_to_row function, I again make sure to add the appropriate amount to both the normal vector and the constant term. Finally, I make a new plane to replace the plane currently existing in the slot, row to be added to. When I scroll down to the test code and run it, I get no output from the tests, since the tests only output anything if they fail, that means my code passed all the tests. Now that we've coded the three basic operations we'll be using, let's start on the first part of the Gaussian Elimination Algorithm. Taking a system and putting it into triangular form. Here, I've added a function called compute_triangular_form to my module. It won't take any inputs but it will return a new system in triangular form that has the same solution set as the original system. Note that I'm using deepcopy from the copy module, so that I'm not modifying the original system directly. That way it's still left intact in case I need it later on. Now go ahead and code a compute_triangular_form method of your own. So you can check your work, I'll provide some test cases for you to run. I should note that there are many valid triangular forms based on some choices that you make. For example, which row to swap for a row with a zero coefficient. But the answers that will be used in the test cases will assume the following. First, if you swap a row with a zero coefficient for one below it, swap with the topmost row below the current one that has a non-zero coefficient. So for example, if we have this system, y + z = 1, 2y- z = 2, x + z = -1, and 2x + 3y + 2z = 1, then in order to get a non-zero coefficient for x in the first equation, we could swap row one with either row three or row four. But according to our assumption, we will need to swap rows 1 and 3 in this case. Second, for this portion of the exercise, do not use the operation multiply coefficient and row. We will use it later in the algorithm but not for computing triangular form. And third, only add multiples of rows to rows underneath. For example, you don't want to add a multiple of row three to row two or row one. Good luck, this might be a bit challenging. If you need a hint, the first portion of the solution video will have some suggestions on how to write the algorithm. You can access the solution video by just clicking submit without checking the box here and then clicking view answer. Hint portion. In the first portion of this video, I'll write some pseudocode describing how I structure the algorithm. Then in the second portion of the video, I'll fill in some of the details. If you just want to see the pseudocode as a hint, watch up until the timestamp listed in the instructor notes on this page. In general, to compute triangular form, we will need to proceed downward through the equations in our system one at a time. Let's start with a for loop to do this. Let m equal the number of equations and then start the for loop with for each row i = 0 to m -1. I'm specifying the start index here as 0. Since many languages use 0 as a start index for a list or an array. Next, we need to check the coefficient of the appropriate variable in the current row. How do we know which variable is appropriate? Of course, we'll want to start at the left most variable on the first equation. But sometimes, we won't want to examine the ith variable, in the ith row. We'll want to skip down to further terms. Let's say that j will represent the current variable. So, for this example, j = 0, would mean the x term, j = 1 would mean the y term, and j = 2 would mean the z term. Let n equal the number of variables and let j = 0 and we'll put both of these initializations outside the for loop. Now at the top of the for loop, let's call c the coefficient of the jth variable in the ith row. In most cases, we'll just want to clear all the terms below this term. So let's add that instruction near the bottom of our for loop. Clear all terms with variable j below row i and then we'll increment j to move on to the next variable. However, if c, the current coefficient, equals 0, then we will need to swap with an appropriate row. So let's add, if c is equal to 0 after our assignment of c. If we find a row under row i with a non zero coefficient for variable j, then let's go ahead and swap that row with row i. Of course, it could be possible that there's no row underneath row i that has a non zero coefficient for the jth variable. If this is the case, then we are done clearing the jth variable and we need to move on to the next one. So we will increment j. We also need to make sure that j isn't getting too big. However, we don't want to increment the row counter i, until we've performed a successful variable clearing. How I solve this problem is I put a while loop inside the for loop. While j is less than n. Then, we need to make sure we perform this check, is j less than n, after we increment j after failing a row swap or after a clearing. So, if a swap fails and we have to increment j, we'll run another iteration of the while loop. And then when we clear a variable underneath the row, we'll break out of the while loop to allow an increment of the for loop. So this is the basic structure of my solution to the problem. What's left is to rewrite all this in proper syntax and then to fill in what is meant by these two statements. If there's a row under row I with non zero coefficient for variable j, and clear all terms with variable j below row i. In the solution portion of this video, we will define these functions more precisely. This is the end of the Hint Portion of the video. The full solution will now follow. Okay, here's the function compute_triangular_form, now in proper Python syntax. Instead of the non descriptive letters m and n, I've used num_equations and num_variables to represent the number of equations, that is the length of the system and the number of variables given by the system's dimension. Some technical changes I've made from the pseudocode include wrapping the value of c in a MyDecimal object because I want to allow for some tolerance in what is considered 0. So instead of asking if c equals equals zero, I use c. is_near zero instead. If c.is_near_zero, then I call system.swap with row below for non zero coefficient, if able. This is somewhat of an unwieldy name but it's fairly descriptive of what's supposed to happen. It will return either true or false depending on whether the swap succeeded. I also call the function system.clear_coefficients_below which clears all coefficients of variable j below row i. Looking at the swap with row below etcetera function, we can see that it's fairly straightforward. It looks at all rows beneath the given row looking for a non zero coefficient for the given variable, specified as COL short for column. If it finds such a coefficient that isn't near zero then it makes the swap and returns true. Otherwise, it returns false. The clear_coefficients below function is also fairly straightforward. It computes the ratio alpha it needs to multiply the given row by, in order to clear each row k underneath the given row. And then it adds alpha times the given row to row k. To compute alpha, it multiplies the reciprocal of the coefficient of the given row times the negative of the corresponding coefficient in row k. Now, let's run our test code and make sure it works. Hooray, no fill your messages were printed, let's move on. Now we'll write the second half of the Gaussian elimination algorithm, which brings a system into reduced row-echelon form, or RREF for short. Remember that this is the unique triangular form in which the pivot or leading variables each have a coefficient of 1 and are each in exactly one equation. There are no terms above or below that equation with that variable. To really ensure uniqueness, we have to add the technical condition that if the system contains equations of the form 0 = k for non-zero k, then k must equal 1. And all such equations must come before any equations of the form 0 = 0. But for our purposes, it will be fine as long as you put all equations of the form 0 = 0 and 0 = k at the end of the system. And this will happen if you use the triangular form algorithm we computed last time. I've added another function called compute_rref to my module. It will return a new system in reduced row echelon form that has the same solution set as the original system. Notice that I make sure the system is in triangular form by first running compute_triangular_form, which we coded previously. This gives me a copy of the system in triangular form, which is helpful, because I can then assume that the system is in that form for all my later operations. Now go ahead and finish coding compute_rref. So you can check your work, I'll provide some test cases for you to run in the instructor notes like we did for triangular form. If you need a hint, the first portion of the solution video will contain pseudocode for the main structure of the procedure. This is the hint portion of the video. In this portion, I'll write some pseudocode describing how I structure the algorithm. Then, I'll fill in some of the details in the second portion of the video. If you just want to see the pseudocode as a hint, watch up until the timestamp listed in the instructor notes on this page. Let's assume the system is already in triangular form, in order to compute our RREF. We work from the bottom equation up dividing through each equation by the coefficient of its leading variable and then clearing that lead variable from all the equations above. Let's call M the number of equations, and then let's make a for loop iterating upward through the rows. First, let's check if there are any non-zero coefficients in our given row. For example In these rows zero equals blank. There is no variable to solve for. So we need to move up to the next row. So, if there are no non-zero coefficients in row I we continue to the next row. Let J be the index of the variable. That's the pivot on row I. Luckily, there's a function already included in the linear system class that finds this for us. Then we need to make the coefficient of the variable j in row i equal one. So let's scale row i to make this coefficient one. Finally, we need to clear all terms with variable j in rows above i. There's still a bit of work to be done in filling in what these last two steps mean, but that's the sketch of the algorithm. Next in the full solution, I'll show my python code for this pseudocode and also my code for the last two steps of the for loop. This is the end of the hint portion of this video. The full solution will now follow. Here's the function compute RREF. Now in proper Python syntax. Although it was given in the code stuff I showed earlier, it's worth pointing out that the variable tf refers to the triangular form of the system. And then, all the rref operations are performed on this triangular form. I'd also like to note that to compute pivot indices which is a list giving the index of the leading variable on each row. I use the preexisting function indices_of_first_nonzero_terms_in_each_- row. Since we followed the pseudocode, more or less the only thing left to do is to fill in the two functions scale_row_to_make_coefficient_equal_one and clear_coefficients_above. scale_row_to_make_coefficient_equal_one multiplies the given row through by the reciprocal, of the coefficient, of the specified variable, in the specified row. Nothing too special going on here. And clear_coefficients_above is almost identical to the function clear_coefficients_below that we used in computing triangular form. The difference is that the range of equations I loop through is in reverse order. Also, since I know that the coefficient of the pivot variable is one. I can simplify my arithmetic a little bit. Otherwise, it's exactly the same logic. All right we finished coding the Gaussian elimination algorithm. Now we need to extract a system solution from its reduced row-echelon form. Given the system of equations, you should use your previous function to compute its rref and then use that form to output one of three things. The unique solution, perhaps as a vector object, some indication that there is no solution, or some indication that there are infinitely many solutions. For this quiz though, we won't try to work out the parametrization of the solution set, if there are infinitely many solutions. Once you are done, use your function to solve the following systems. If the system has a unique solution, enter it in the appropriate answer boxes with an accuracy of three decimal places. Otherwise, check the no solutions or infinitely many solutions box as appropriate. The first system is 5.862x plus 1.178y minus 10.366z equals negative 8.15. And negative 2.931x minus 0.589y plus 5.183z equals negative 4.075. The second system is 8.631x plus 5.112y minus 1.816z equals negative 5.113 and 4.315x plus 11.132y minus 5.27z equals negative 6.775 and negative 2.158x plus 3.01y minus 1.727z equals negative 0.831. The third system is 5.262x plus 2.739y minus 9.878z equals negative 3.441 and 5.111x plus 6.358y plus 7.638z equals negative 2.152 and 2.016x minus 9.924y minus 1.367z equals negative 9.278 and 2.167x minus 13.543y minus 18.883z equals negative 10.567. [BLANK_AUDIO] The first system turns out to have no solutions. The second system has infinitely many solutions. And the third system has a unique solution of x equals negative 1.177. Y equals 0.707. And Z equals negative 0.083. Here's the code I wrote to compute the solution. I return the output of a function called do_gaussian_elimination_and_extract_sol- ution. If an exception is raised, I return either a message saying there is no solution, or infinitely many solutions, depending on which was raised. In the do_gaussian_elimination, etc., function, I first compute the reduced row echelon form of the system, and then I check for contradictory equations. That is, equations of the form zero equals K. And for too few pivot variables. Both of those will raise exceptions if those conditions are found. If those conditions aren't found, then I return a vector whose coordinates are the constant terms of the first inequations in the system. Since the reduced-row-echelon form of the system with a unique solution will just consist of equations of the form x equals some number, y equals some number, and z equals some number. In the function raise_exception_if_contradictory_equat- ion, I'm just checking each plane to see if it has all zeroes in its normal vector. If so, I check its constant term for a non-zero entry. If I find a non-zero entry in the constant factor, then I know I found a contradictory equation and I raise the appropriate exception. In the raise_exception_if_too_few_pivots function, I use list comprehension to count the number of indices that are non-negative. Since indices of first non-zero terms in each row returns negative one if there is no non-zero term in a row. So the number of non-negative indices is the number of pivot variables. And I compare that number with the total number of variables to see if it's smaller. If so, then I know I must have an infinite number of solutions as we talked about earlier in the lesson. So I raise the exception. Now that we've finished coding the Gaussian elimination algorithm and we've coded how to extract a solution from the reduced row echelon form of a system, let's go one step further and give our program the ability to output a Parameterization of the solution set if there are infinitely many solutions. I've added a new class called Parameterization to my lynsis module. To initialize a Parameterization object, you need to pass in a base point, which is a vector and a list of vectors, the direction vectors. For an example of what this means, let's look at the plain x + y + z = 1. Suppose this is the solution set to some system of equations. We could rewrite x as 1-y-z and then treat y and z as free variables. So if we parameterize x, y, and z, the base point would be 1 0 0, because y and z don't have constant terms, and x has a constant term of 1. Then the vector multiplied by y would be -1 1 0 because x has a -y term, y has 1 times y and z has no y term. And similarly, the vector multiplied by z would be -1 0 1. In this case, y and z are the parameters of the parametrization. The base point of this parameterization would be the constant vector 1 0 0 and the direction vectors are the vectors multiplied by y and z, the parameters. These are the vectors -1 1 0 and -1 0 1. You can find the code for the parameterization class in the instructor notes as well. For this quiz, your goal is to write a program that outputs a parametrization object representing the solution to a system with infinitely many solutions. To test that your program is working correctly, here are three systems with infinitely many solutions. Enter the correct parameterizations into the boxes provided. The first system is 0.786x + 0.786y + 0.588z = -0.714 and -0.138x- 0.138y + 0.244z = 0.319. The second system is 8.631x + 5.112y- 1.816z = -5.113. And 4.315x +11.132y- 5.27z = -6.77 and -2.158 + 3.01y- 1.727z = -0.831. The third system is 0.935x + 1.76y- 9.365z = -9.955 and 0.187x + 0.352y- 1.873z = -9.991 and 0.374x + 0.704y- 3.746z = -3.982 and -0.561x- 1.056y + 5.619z = 5.973. In the code for the parameterization object, the parameters are named t sub i for some number i. But, for the purposes of this quiz, please use the letters t for a parameter if there's just one parameter. This would be called t1 in the Python module. And used t and s if there are two parameters. So t would be t1 in the python module, and s would be t2. Good luck. The first system has the parameterization x equals negative 1.346- t, y = t and z = 0.585. The second system has the parameterization x = negative 0.301- 0.091t, y = negative 0.492 plus 0.509T and z = t. The third system has a two parameter parametrization. X = negative 10.647 minus 1.882t + 10.016s, y = t and z = s. And here's my code for solving the problem. I still use complete solution to wrap my Gaussian elimination method. Which I've now renamed to do_gaussian_elimination_and_parametrize- _solution to reflect the fact that it now parametrizes a solution. I no longer check for infinitely many solutions as an exception since that's what parameterizing is supposed to handle. But if a no solutions exception is raised, it is still handled at this level. In the Gaussian elimination method, I again first compute the rref of the system and then I check for contradictory equations. Then I use two methods. One to extract the direction vectors and one to extract a base point for the parametrization. I then create and return the appropriate parametrization object. In my case, I still use this object even if there is a unique solution. In this case, the variable direction vectors would just equal an empty list. In the vector extraction method, I determine which variables are free variables by seeing which aren't pivot variables. And then for each free variable, I construct a direction_vector. The coordinate corresponding to the free variable will be one to reflect our setting the variable equal to itself. For example, z equals z or y equals y in the parametrization. Then for each equation in the system, I determine the pivot variable, find the coefficient of the free variable in that equation. And then I set the coordinate corresponding to the pivot variable equal to the negative of that coefficient. Since we have to subtract the free variables from both sides of the equation when we set up our parameterizations. For example, in x + y + z = 1, we set x equal to 1- y, one of the free variables and minus z, another one of the free variables. I do something similar for computing the base point. One observation is that only coordinates corresponding to pivot variables can be non-zero. Looking again at our example, there are no constant terms in the equations for free variables. So we can just set all the coordinates initially to zero and then just loop through the pivot variables. Since the constant term is already on the right-hand side of each equation and the variables are on the left-hand side, we don't need to do any subtraction. And now the moment of truth. If I run the code down here, we can see the nice output of the parametrized solution sets. The first and second systems, since they only have one parameter in their solution set will be lines. However, the third system solution since it has two parameters will be a plane. This is because all the equations in the system actually defined at the same plane. So their intersection is a plane which is a two-dimensional object. One benefit of the algorithm we've developed is that it is highly generalizable. It does not depend on working only in three dimensions. In fact, just by changing the word plane in our algorithm to the word line we can immediately use the code to solve systems of equations in two dimensions. On the other hand, this algorithm will allow us to work with higher dimensional systems of equations too. Recall that a two dimensional plane in 3 dimensions is defined by a single linear equation in 3 variables. If we move to end dimensions a similar notion is called a hyperplane. Which is defined by a single linear equation with n variables. The geometry of higher dimensional objects, even though we can't visualize them in three dimensional space, is analogous to the geometry we've studied so far. As with lines and planes, a hyperplane in n dimensions is an (n-1)-dimensional object and it can be specified by a normal vector and a base point. Also like lines and planes the coefficients of the variables in the equation have a normal vector for the higher dimensional hyperplane. And we need at least n different hyperplanes and n dimensions in order to obtain a unique intersection point. Most importantly Gaussian elimination works just as well on systems of equations in more than three variables as it does with just two and three variables. Similar to the line in plain classes we created, let's now create a hyperplane class. To make it, I almost literally copy and paste my code for the plain object just replacing all instances of the word plane with hyperplane. The only other change I need to make is in the initializer function. Here, since the hyperplane could live in an arbitrary dimension. I'm adding an optional parameter, dimension, to the list of arguments. If both the dimension and the normal vector are missing in the initializer call, then I can't figure out which dimension is being asked for, so I raise an exception. Otherwise, I either set the normal vector to be all zeros in the given dimension or I set the dimension to be the dimension of the normal vector given. And that's pretty much it. Going back to the linear system class after importing the hyperplane class from my hyperplane module, and without changing any of the code in my linear system class. I can now build a linear system out of hyperplanes and as long as the dimensions of each hyperplane in a system are the same it will work as expected. Here I have a system of lines, here I have a system of planes, and here I have a system of four dimensional hyperplanes in five dimensional space. And running the code I obtain parameterizations of the solutions for each of the systems, or I'm told that the system doesn't have a solution. In this lesson we discussed the interplay between the geometry of intersecting flat or linear objects in space and the algebra of solving systems of linear equations. We then use the geometric insight into the algebraic process to develop an algorithm, the Gaussian elimination algorithm, for solving systems of an arbitrary number of linear equations in an arbitrary number of variables. This is a valuable tool, since it lets us solve a large class of interesting problems that come up often in science and engineering. In future lessons, we will develop vector algebra more systematically looking at the problem of solving systems of equations in a different light. As questions about physical transformations of space. We will then use this new insight to develop more tools for solving problems. Hi, I'm Chris Pryby. I'm a course developer with Udacity and also former Georgia Tech graduate student. I'll be presenting this course on linear algebra designed specifically for students of the OMS CS program. Since linear algebra is the backbone of many techniques and algorithms in computer science, we wanted to provide a resource for OMS CS students to learn or brush up on some of the fundamentals of this subject. But instead of giving just another set of lectures on this topic, we wanted to put linear algebra in the context of how you will use it in this program. In this course, in addition to learning some theory, you will be implementing the algorithms we discuss yourself. By the end you will have built your own library of linear algebra functions. The only necessary background for this course is knowledge of a programming language. But some familiarity with elementary algebra and trigonometry will be helpful as well. I'm looking forward to this course and I hope you are too. Let's go ahead and get started. Two fundamental objects in geometry are points and vectors. Essentially, a point is a location in space. We represent a point visually as a dot. And given a coordinate system, we can represent it point by its coordinates in that system. For example, in two dimensional space, the standard coordinate system is the Cartesian coordinate system. We represent a point in the system as an ordered pair of cordon it's first the x coordinate and then the y coordinate. So this point located two units to the right of the origin and one unit below the origin would be represented as 2, -1. In three dimensions we can represent a point in the similar way as a triple of course. First its x coordinate, then its y coordinate, and then its z coordinate. In contrast to a point, a vector is an object representing a change in position. In Euclidean space, a vector can be thought of as an arrow connecting two points. The important properties of a vector in Euclidean space are its magnitude, that is the length of the arrow and its direction. Given the coordinate system, we can numerically represent a vector by the amount of change it represents in each coordinate direction. For example, this vector moves right by two units so we would give it an x-coordinate of 2. Since it moves up by four units, we give it a y coordinate of 4. Notice that I write a vectors coordinate representation differently than I do a points. I will usually write a vector as a column surrounded by square brackets. While I will write a point as a list of coordinates it surrounded by parentheses one major difference between a point in a vector is that a vector does not have a fixed If I draw the same arrow starting at a different point, it will still be equal to the first arrow. This is an important distinction. Two vectors are equal if they represent the same amount of change in each direction while two points are equal, only if they are located in the same place. Despite this difference, while studying Linear Algebra, we usually conflate the notion of a point in a vector in the following way. Given a coordinate system, the point P is associated with the vector obtained by starting at the origin and ending at the point P. While this is usually a useful mental shortcut, it is sometimes helpful to remember the difference between the two concepts. Now let's check your understanding so far. Here are two points, P and Q. P is located four units to the left of the origin, and four units below the origin. Q is located four units to the right of the origin, and two units below the origin. Here are also two vectors, v and w. v starts at the point P, and moves up four units and one unit left. w starts at the point Q, and moves two units left and zero units vertically. Using this information, go ahead and fill in the coordinates for these points and vectors in the following quiz. Since P is 4 units to the left of the origin, it's x coordinate is -4. Since it's 4 units below the origin, its y coordinate is also -4. So we can represent P by the ordered pair (-4, -4). Similarly Q can be represented as the ordered pair (4,- 2 ) since it is 4 units to the right of the origin and 2 units below it. Remember that the only properties that distinguish vectors are their magnitude and direction. Vectors don't have a location property. So we only need to know the amount of change in the x and y directions for each vector to represent them in coordinates. Since V moves one unit left its X coordinate is minus one. And since V moves four units up, its Y coordinate is four. Finally, since W moves two units left, its X coordinate is minus two and since it has no vertical movement, its Y coordinate is zero. Now that we've defined the basic geometric objects of study for this course, vectors, we can talk about how to manipulate them. In the upcoming lesson, you will begin programming your personal library of functions for operating on vectors, which you will use throughout the remainder of the course.