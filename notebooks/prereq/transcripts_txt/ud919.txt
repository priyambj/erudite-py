Welcome to the class. We feel this course will be very beneficial for aspiring and practicing data scientists in industry and in academia. In this class, we will learn how to build and validate models. Many of you may have already taken a course in machine learning or data science, and may already be familiar with machine learning models. In this course, we will take a more general approach, walking through the questioning, modeling and validation steps of the model-building process. Now, the goal is to get you to practice thinking in depth about a problem and coming up with your own solutions. Many examples we will attempt may not have a one correct answer, but will require you to work through the problem, applying the methods that we hope to illustrate throughout this class. It is somewhat like a game of chess. The rules are quite well-defined, but one needs to have a strategy for winning. In any given situation, there may be multiple correct moves leading you to victory and numerous incorrect ones. It is to study and practice. You learn the strategies for winning. At the end of this class, you will feel confident in your ability to conquer new problems by applying an iterative process of analysis that takes you from a vague initial question to a rigorous model addressing it and quantifying the uncertainty that exists in any model. In addition, you might learn about new types of models to apply to problems, and you will learn how to use the best model for the task that takes the most advantage of data instead of applying one black box model after another and hoping for the best. But before all of that, let's get to know each other. Hi, I'm Don Dini. I'm a principle data scientist at AT&T. As a data scientist it is my job to build models. My background is in artificial intelligence and computer science. Now among other things today, I work on using simulation to solve problems that are often too hard for traditional analytical approaches. Such as understanding city populations, and the design of communication networks. I will appear mostly in lesson One and lesson Two to guide you through the first half of the class. Hi, my name is Rishi Raj Prabahan, I'm also a principle data scientist at AT&T. My background is in experimental high energy physics. As part of the team that built the Atlas Detector that ended up discovering the Higgs Particle, completing the standard model of particle physics. A model that in a single mathematical equation, describes the behavior of all known matter and their interactions. I will follow up, after Don, and appear in the second half of the course. We are both working on various problems effecting networks, systems, data and customers of AT&T. In this course, we will try to introduce you to as much of the formal mathematics, statistics, machine learning and programming as the everyday experience of data analysis and model building. Even the simplest of models can drastically improve the way we understand and interact with systems. Consider the following example, in the United States the EPA tries to protect the American public by examining new compounds and classifying them as toxic or nontoxic. In the 1970s, a trained chemist would make the determination based on chemical properties of the compound. This process was slow and expensive. So, the EPA started a project to see if you could automate the classification of compounds with software instead. The answer turned out to be a decisive yes. A decision tree based model was able to algorithmically detect patterns in the chemical properties that indicated toxicity. Just like that. A fast and inexpensive data driven model replaced an expensive and difficult approach to solving the same problem. You can find more information about this EPA project in the book linked in the instructors notes. Another interesting example is the field of advertising. So originally, the advertising age was created by Sigmund Freud's nephew Edward Bernays. He used his uncle's theories of the mind to craft ways of communicating with people. In fact, in addition, he was a pioneer of public relations as well as advertising. See the instructor's notes to learn more about Bernays if you're curious. Today, advertising looks like this. Amazon sees what items you look at, collects the viewing and buying history for you and other customers, and recommends things that you are statistically likely to purchase. Now this relies on models like collaborative filtering to reveal patterns from data about buying habits among similar types of people. Advertising using data-driven models is now one of the primary sources of revenue for a lot of small and large companies and is the focus of a lot of data scientists. Our goal is to create a model to understand a phenomenon. We cannot do this in isolation. Modeling must be done in tandem with forming and reforming questions, and validating the model that we create. The process of model building in data analysis can be thought of as occurring in three phases. Questioning, modeling, and validating. In the course of analysis we are constantly iterating back and forth among these things in successive loops that build upon each other. So it turns out that the single most frequent call to AT&T's helpline is, when will my UVerse technician arrive. And so AT&T thought it would be valuable to create a model for performing this prediction. So my modeling process began with a simple question. When will my service tech arrive? Now in order to make this question answerable, it has to be made quantitative. And so we must choose a metric that captures this phenomenon Before I tell you how I did it, why don't you come up with a few ideas about metrics, about how you would attack this problem. Please share your answer with other students in the forum link that I have listed in the instructor comment section below. So, this is how I did it. We have a few options to consider. For example, the dispatch system has its own predicted time of arrival. The technician has their own self-reported arrival time, which they write down as soon as they reach the customer's home. Or we might use the time of completion of the technician's previous job. All of these are valid quantitative metrics to consider using when capturing the question, when will my service tag arrive? We decided as an initial guess at a metric to consider using the technician's reported arrival time. So the reason for selecting the actual recorded arrival time is that it seemed as close as possible to the actual quantity we were trying to predict. So, the computer's predicted arrival time is based on various assumptions. In addition, as far as using the time of the technician's prior finish job time, much can happen between the end of the technician's previous job and the start of the next one. So for this reason, starting with the tech's actual recorded arrival time seemed like a reasonable way to go. Having selected a reasonable metric, we then had to determine how do we turn the relatively vague problem statement, when will my service tech arrive, into a rigorous statistical inference problem? At the end of the previous video we chose a metric which we thought would be a good way to quantify the question when will my service deck arrive? We ended up going with the technician's actual reported arrival time at the customer's home. Now to proceed we have to find a way to turn that question into a rigorous statistical inference problem such as point estimation, confidence sets, classification or parameter estimation. For example, the problem could be cast as a classification problem in which we attempt to predict if the technician will arrive before noon or after noon. At the complete other end of the spectrum, we could create a regression estimator which predicts a precise time of arrival. Both of these have advantages and disadvantages. Now before telling you how we approach this problem. We would love to know if you cast this as a classification or a regression problem. Why or why not? Like the previous assessment, please share your thoughts with your fellow students in the forum post linked in the instructor comment below. So while looking at the data, we very quickly discovered that the data showed quite a lot of variance, which was very likely to impact the accuracy of any regression estimator we might want to build. To see why this is, consider the opposite end of the spectrum. The data shows no variance. In this case, if the values of your features of the same, the measured response variable, in this case, arrival time is always the same. That is you have a static functional mapping. And the learning problem amounts to learning aesthetic functional mapping. As you introduce variance into this, it slowly becomes more difficult to predict the response variable as the introduced variance shows up added to the output of the regression. To combat this, I considered going the classification route So at that point, I have to consider which model I was going to go with to accomplish that classification. As you know, there are many options. We can use a Bayes network or a perceptron or even perhaps logistic regression. The actual classification was not simply less than or greater than nu. But rather, a few time windows during the day. But to start off with something simple, to get a feel of the characteristics of the data, I wanted to start off with something that would create linear separations between the classes. And so I attempted to use a perceptron. I then immediately performed some tests of validation. After doing so, it became clear I had to revisit the question of which model to use. Or likely even, which features to consider. As it was likely I did not capture all of the features that might have an influencing force over arrival time. I thought perhaps the data might exhibit some locality properties and considered models to exploit this structure in the data. Now, by locality, we mean the following. Let's suppose we have a featured space of x and y values. Each pair of x and y values is labeled with a value, v. Now, let's suppose that in this space, we're given three training values. Point 1, x1, y1, point 2, x2, y2, and point 3, x3, y3. And each of these is labeled with a value. So point 1 has value v1, v2 and v3 and so on. Now these training points are just the ones that we're given, and so we'd like to create an estimator that tells us the values of the points everywhere in this space, not just at the training points. We can ask what the value is at this new point in the middle, x,y. Now if we can assume that the value at this point, x,y is approximately the same as the values that surround it, then we can say that this data exhibits locality. So if the value at x,y is approximately equal to v1, v2, and v3, the data has locality. Now of course, this isn't always the case. Not all data sets exhibit locality like this. But of course, some do. Now a model that uses this assumption of data locality is k nearest neighbors. So it seemed like a reasonable model to attempt next. And at last, we have the final diagram summarizing everything that we have done. This example is just meant to illustrate what real problem solving is like. In real model building, we make choices that we think will lead to new insights, which then reveal new paths of analysis, which build upon previous knowledge to gradually arrive at the model. Very soon, we'll work together through a complete example from the ground up to see the iterative cycles of question, model and validate built upon each other. You'll see how to turn a vague question into a rigorous problem of statistical inference through successive refinement. You'll learn to identify possible next iterations and how to use them to generate possible paths of analysis. By the end of this lesson, you will feel confident in your ability to tackle new difficult problems and start making progress with them. The choices we make during each phase can be diagrammed in a tree like the one here. It's a useful tool for considering our possible paths forward during analysis, as well as revealing options for revisiting steps when you must inevitably adjust decisions made during earlier phases. In the rest of this lesson, we will apply each of these steps to an actual data set and you will get a chance to follow along and practice these steps with us. So the candy bowl example is actually a simplified case of a field of study called consumer choice modeling. In consumer choice modeling, our goal is to understand and characterize how consumers make choice decisions. In order to answer this, we try to understand first, is it even possible to find preference ordering for product brands? Second, can we infer even that there exists a preference between brands? So let's look at a simple real life version of this problem. We will see how building a model can help us understand if there exists user preference between brands of candy. So before proceeding with the candy bowl data, I wanted to take a moment and describe the data that we collected in order to perform this analysis. So if you recall, the goal is to understand what influences consumer choice. In this case, the influencer's a candy selection, and so we elected to record any attributes which we thought might possibly have an impact on selection preference. So, each record corresponded to someone selecting a candy. For each record, the attributes that we thought to collect, which might have an influence on candy preference were, name, gender, the type of candy. We lumped color and flavor into one giant category. Age of the person who's selecting, and ethnicity of the selector. Now in addition to that attribute, in order to establish a baseline, we also want to include a spurious attribute. That way, any attribute that was ranked as having a weaker relationship than that one, we know that we can safely ignore it. So as our selection of a spurious attribute, we chose shirt color. I think we can all agree that shirt color shouldn't have an impact on what candy someone selects. So, when we're evaluating the strength of relationships of any one of these other attributes to candy choice, we know that if it has a weaker relationship than that of shirt color, we can safely ignore it. Now in addition, the records are chronological, meaning that the records appear in the order in which selections were made. Now that you know what all of the attributes mean, go grab ahold of the data yourself so that you can follow along while we do the analysis. We must have a starting point from which to conduct our investigation. Sometimes, this will be highly specified and may even come to you from a colleague or perhaps a supervisor. Often however, we start with vague direction we want to investigate. And it is our job to refine this into something we can answer. Let's start by asking the high-level question, how popular were the different kinds of candy? Note that this question can not be answered in the way that it is phrased. We have no meaningful description of popular. Much less a way to say that one candy is more popular than another. To get the iterations analysis going, our task is to refine this initial question until it is cast in the form of a precise, statistical inference problem. To do this, we will first need to make a guess as to how to quantify popularity. So how would you quantify popularity? What metrics would you use? Type your answer in the text below. Don't worry, there are no right or wrong answers. And I will share my answer with you in the next video. So that's one possibility for selecting a metric to quantify the popularity of brands of candy. The number of turns between selections of candy, otherwise known as the inter-selection time. Another possibility is the total number of records selecting a candy. For our initial guess, let's use the inter-selection time. Let's talk for a moment about this metric of time between selections. Suppose that an Airhead is selected. Followed by a Starburst. Followed by two Hershey's. Followed by an Airhead again. Let's define the number of candies that come between two Airhead selections as the interselection time between Airheads. So if someone were to choose an Airhead here, there would be zero turns in between Airhead selections. If someone were to choose an Airhead here, there would be one turn, two turns, and so on. Until here, where someone actually chose an Airhead again. We have an interselection time of 4 turns between Airhead selections. And just to reiterate, the interselection time for a candy c is the number of turns between selections of that candy c. To understand what this metric means, inter-selection time, let's examine some plots. Let's create a plot of the airhead inter-selection time. Each point represents the event of an airhead being selected. For example, this point represents the tenth time an airhead was chosen. The y axis value is the number of turns since the last time someone selected an airhead. Thus higher values represents lower demand. This metric seems promising as a way to capture popularity. At the end of the previous video we ended at the following iteration with the following paths forward. We started with a vague question. How popular were the different brands of candy? And we considered the two possible metrics, interselection time and total number of selections for quantifying this question. We ended up choosing as an initial guess interselection time. Now that we have a metric interselection time that we feel captures what we're interested in, we must use it to cast the above question as a problem of a statistical inference. So there are many options for doing so. One is point estimation, in which we explicitly try to create a estimator for our quantity interselection time. Another is confidence sets, in which we try to create an interval, which traps the true value of inter selection time with some confidence level. Classification, in which we choose various descriptive classes which may not be ordered. And try to classify interselection time into one of those classes. So for example in contrast with point estimation in which we explicitly try to come up with an estimator for the actual value of interselection time. In classification we may choose just broad categories like short or long and try to match the actual quantity to one of those classes. Hypothesis testing in which we for example, might try to determine if two different collections of inter selection time values came from two different probability distributions and how much confidence we have in that being true. Or lastly, parameter estimation in which we try to do things like. Find the parameter of a Poisson distribution, or find the bandwidth value on a kernel density estimation. Which we'll talk a lot more about, in a later lesson. Depending on the outcome of later phases, we may return to the question phase, to recast the same problem as a different statistical inference problem. For example, we may find the regression estimator we create has a very high variance, thus poor performance but can be recast as a classifier with a very high accuracy. Let's state this problem more formally. Our goal is to build a regression estimator, r. R will tell us its estimate for an interselection time for a candy, c, at any given turn. Our data points are just the candy selections and the order in which they were selected. With the inter-selection time written beside them. In other words, given the list of candy selections, and the number of turns since each candy had last been chosen. We are going to build an estimator, R, that will tell us at a given turn for a given candy how long it has been since it was last selected. Let's consider our next possible paths for analysis. At this stage really our only options are to proceed with the regression estimator. Which means identifying features or revisiting an earlier stop. In the next video we'll proceed with feature selection. At the end of the previous video, we refined our initial question, how popular were the different brands of candy to a precisely formulated statistical inference problem in which we want to create a regression estimator for interselection time. Now before we can do that, we must identify what features we'd like to use in our model. As you know, if our goal is prediction, we must choose features which have some kind of influential force over our response variable. Which in this case is interselection time. Our goal now is to identify what those features might be. What features would you use for the model? The types of brands? The number of each type of candy in the bowl? The time of day? Or the type of candy? So remember that we're trying to choose something that has an influencing force over the interselection time for the different brands of candy. Anything that we think might be able to do that will fit the bill. I ended up choosing brands and in addition to that, the popularity of candies as the features from my model. Let me explain the reason behind choosing these features in depth in the next video. Let's try to summarize what we've done so far. We started with the initial question, how popular were the different brands of candy? We then thought about, what sort of quantities should we use to make this question quantitative. Interselection time, total number of selections. And we ended up going with interselection time. With a way to quantify the problem in hand, we then had to cast the question as a rigorous statistical inference problem, such as confidence sets or classification. We ended up going with point estimation. Specifically, we chose to create a regression estimator in which, given the candy and the particular choice number, what is the interselection time of that candy? Specifically, how long has it been since someone chose that particular candy. Previously, we took a look at some plots of interselection time for individual candies. Let's examine multiple plots on the same graph at once. Here is depicted the interselection times of Rolo and Reese's, with Reese's in blue and Rolo in green. As you can see, the average interselection time of Reese's starts out lower, but jumps a bit at the end. And vice-versa for Rolo. Recall that lower interselection time corresponds to higher demand. And conversely, higher interselection time corresponds to lower demand. Thus Reese's starts out in high demand, but then gradually gets selected less and less often. Conversely, Rolo starts out in low demand, and gradually becomes selected more often. This plot lends some support to the notion that, in fact, there is some interaction between different brands of candy in the candy bowl. If true, this should not be very surprising. The brands of candy do not sit in the candy bowl in isolation. A person's choice of a particular candy is influenced by the presence of other candies in the bowl. If, in fact, people preferred one brand of candy overall to another, then you'd expect the time between selections for one brand to be low at the same time the other was high. As the first one starts to be depleted, the interselection time for the second type starts to go down. So, we're trying to create a regression estimator for the interselection time for candy c. Right now, the only features that we're working with are the candy we've selected, c, and the choice number. >From visual inspections of the scatter plots, however, it seems that there may exist a relationship between the popularity of a brand of candy, and the presence of other brands sharing the candy bowl, as measured by interselection time. This suggests that in order to predict the interselection time for candy c, we should instead be looking at the presence of other candies in the bowl, specifically measured by the interselection time of those candies. So, let's go ahead and adjust our regression estimator. Where previously, we tried to predict interselection time, based upon the candy we selected and the choice number, now we're going to try to predict interselection time, based on the interselection times of the other candies that are in the bowl. So, here I've defined, for example, i of c sub 2 is the interselection time for candy c2. Or, to give a concrete example, we could try to build a predictor of the interselection time for Starburst, given the number of turns since someone has chosen Airheads, Hersheys, Reeses, Kit Kat and Rolos. After this last adjustment to our question, we have an even more precisely formulated statistical inference problem. Just to summarize what we did, we started out with an initial guess as to what our features should be for our regression estimator. After looking at a few scatter plots, we decided to refine our feature set, to include the presence of other candy in the candy bowl. At last, we're ready to apply a model. As you know from other courses you may have taken, there are many options to choose from. For example, neural networks, kernel density estimators, or KDEs, linear regression, or polynomial regression. Before telling you about which model I chose, why don't you tell us which model you would choose and why. It could be one of the models I mentioned or a completely different model. Why don't you share your thoughts with your fellow students in the forum post that we have linked to in the instructor comments below? In our last video, we considered possible models for implementing our regression estimation problem. In later lessons, we'll discuss in much greater detail which models are most appropriate to take advantage of structure that might exist in data. At this moment, let's choose linear regression. At the end of the previous video, we ended at the following iteration. We arrived at a precisely formulated statistical inference problem. Specifically, to estimate the number of steps since candy C was chosen. We will do so using the regression estimator, which uses as features, the interselection time of Airheads, Hershey's, Reese's, Kit Kat and Rollo's. And we choose a linear model to implement that regression estimator. Let's now build that model using IPython Notebook. Then in the validation phase, we'll examine how well that model works out. I've added a link to the IPython Notebook for you to download and follow along. To use our linear regression model, we first need to prepare our input data. Recall that our feature set is the interselection time of each candy type. Thus, each record will have a column for each brand of candy and the value in each column will be the number of turns since someone selected that type. First, we create a variable called shared state events containing these values. Then we convert the data to a data frame. There are six types of candy and so we have six columns. We can see that by examining the data frame. Now, I'm going to use the linear regression implementation for module SK Learn. Feel free to use any module that you'd like. We split our data into features and labels. And we also split into a training and test set. We then trained our model and we can see it resulted in the following coefficients. This means that the model that we've learned for our aggression estimator, is the following, just to read it aloud, 0.31 times the interest election time for airheads, plus 0.07, times Hersey's, plus .15 times kitkat, plus 0.06 times reese's, plus 0.14 times rollo's, plus a constant. In our next video, we'll start validating this model. Now that our model is built, let's examine how well it performs. There are many ways to evaluate models, but the bottom line is to evaluate predictive accuracy. If our model cannot generalize to instances it has not seen, in most cases it is not a very useful model. First, we'll evaluate our model on both a training set and a test set. On the training data, our model produced a mean squared error of 32.87, and an R squared of 0.072. We can also run this cell to visualize the performance of our model. The green line is our actual interselection times of Starburst. The blue line represents our predicted interselection time. Now, we can see a couple of things in this plot. First, the prediction is missing the occasional bursts and drops in demand. Second, it seems unnecessarily wiggly. It's clear that we must return to a choice that we made earlier to improve the performance of our model. Let's continue adding to our diagram on the right side over here. It's clear that we must return to a choice we made earlier to improve the performance of our model. For example, we might want to revisit the feature selection phase. Or we might want to switch to a different model to use the same features. The highly wiggly behavior of our regression function suggests that our model suffers from overfitting. Let's try to address this by returning to the question phase and reducing our feature set to the top three performing features. Specifically, we will update our statistical inference problem from this expression to this expression in which we only use the interselection time of Airheads, Hershey's, and Kit Kat. Let's add a new Q node to the tree to reflect our update. We ended at the following iteration. We looked at how linear regression performed on our initial feature set and decided to reduce the feature set to a simpler selection, specifically the top three performing features, Hershey's, Kit Kat, and Airheads. Simpler models are usually more effective. Than complex models. In particular, reducing the number of features as much as possible to those that matter most is almost always beneficial. Moreover, lower numbers of features avoid the curse of dimensionality. Which is something that we'll talk about much more later on this course. Let's now evaluate the newer model with the reduced feature set. And in this plot just as before, the green line refers to the true inter selection time and the blue line refers to our model. Now, in this model. Our predictors using less information to predict the demand for Starburst. You can see here the result of doing so. Notice that our predicted line is a bit less jagged than the line that uses all of the features. Which indicates that we're over-fitting less. Revising the feature set choice from the question phase successfully resulted in reducing overfitting. We can also see now the new means square error of our new model. Now let's make a plot on the test set. The overfitting issue has been somewhat resolved, but now we need to examine the issue of our model missing the spikes in interselection time. Clearly this time we cannot avoid having to make a different choice at the model phase At the end of the previous video, we ended up the following iteration. We proposed reduced feature set that uses only the most influential features on interselection time of Starburst. We then evaluated how well that new updated model worked out. We were able to refine our model to address the overfitting issue. However the linear regression model we chose clearly was not capturing the spikiness of the behavior that we observed. We can use the diagram depiction of the history of our analysis to more rigorously identify options forward. We might continue using linear regression. And return to the feature selection stage. We initially chose the complete set of interselection times. Then used the reduced set. We can revisit this phase by including terms of higher order thus making it a polynomial regression. Alternatively we could attempt to represent the regression estimator with a different model altogether. Specifically we might try neural networks or kernel density estimation. We might find that producing a point estimator is too high a level of precision altogether to be supported by the data we have, and instead try to recast as a different statistical inference problem. For example, we can conduct a hypothesis test for interselection times between Starbursts and Airheads. To provide a degree of confidence that they are not of the same popularity. The thing to notice is, because we've kept track of the decisions we've made along the way, we're able to easily identify new possible paths. At this point, hopefully you feel like you can attack a new, big investigation, bring it within the scope of a statistical inference problem, and then apply iterations of the QMV process to gradually hone a solution. You've seen a taste of what model building and data analysis problems looks like in the real world. And how it is a continual process of investigation, finding results and then proposing new lines of investigation based upon those results. Sometimes retrying and adjusting previous decisions. Specifically we get the process going by iterating on an initial question, until we arrive at answers to the following. What metric will we be using to capture the phenomenon? What statistical inference problem will we be examining? What features will inform the inference problem? Once we've determined how to measure the problem, as well as what form of question we'll be answering, we enter the modeling phase, in which we choose a method for answering this question. Ideally, this method should take advantage of any structure within the data that achieves the maximum effectiveness. Lastly, once we've built a model. We evaluate how good the model is at generalizing to the wider phenomenon in the validation phase. At this point, hopefully you feel like you can attack a new big investigation bring it within the scope of a statistical inference problem and then apply iterations of the QMV process to gradually hone a solution. You've seen a taste of what model building and data analysis problems looks like in the real world. And how it is a continual process of investigation. Finding results and then proposing new lines of investigation based upon those results. Sometimes, retrying or adjusting previous decisions. Specifically, we get the process going by iterating on an initial question until we arrive at answers to the following. What metrics will be used to capture the phenomenon? What statistical inference problem will be examined? What features will inform the inference problem? Once we've determined how to measure the problem as well as what form of question we'll be answering, we enter the modeling phase in which we choose a method for answering this question. This method should take advantage of structure within data to achieve maximum effectiveness. Lastly, once we've built the model, we evaluate how good the model is at generalizing to the wider phenomenon in the validation phase. In our previous lesson, we got an overview of the iterative QMV process of analysis. And how it can be described as a way to go about solving data modeling problems. In this lesson, we dive deep into a specific problem. We devote the entire lesson to understanding an individual's Twitter usage pattern and how it is influenced by the relationships as well as other factors. This lesson is all about the impact of data and how the quality and quantity of data can impact the models we build. We try to gain a sense of how much data is enough to say something with any accuracy. Once we've got data we can use, we think about how to determine what to pay attention to within data, otherwise known as the feature selection problem. Lastly, once we know what to pay attention to, we learn how to detect certain kinds of structure that may exist in our data, and what types of models are motivated by them. So before we get started with the analysis in this lesson, I want to talk a little bit about the general process of questioning, modeling and validating. The purpose of the iterative QMV process of analysis is meant to take a vague question and turn it into something that is answerable. We start with something that is vague and general. For example, when will a person tweet next? And we try to turn it into something that is quantitative, concrete and with uncertainty bounds that are clearly indicated. A good way to think about this process is like a camera looking at something initially out of focus. Over time, we slowly bring it into focus. The important thing to note is that this is a process of discovery and exploration. Like exploration, we initially don't really know anything about the problem. It's only through the iterations that we come to understand the problem, or even what the questions are that we should be asking. For example, initially we think that the really important thing to ask is, when will the person tweet next? But only after performing an investigation, we might discover that the more important question is, what is this person's immediate social network? Now, let's get started with a real problem. Now the loftiest desire would be to predict the precise time of day. And so, we would consider using time of day as the quantity. Now I said the loftiest desire, because this would actually be the most difficult form of prediction. This means we could produce any time of day in the future at all and ask, when is the next time the person is going to tweet, relative to that time? For example, at eight in the morning, I could ask, at 3 p.m. later today, when is he going to tweet soonest after that time? Now alternatively, we could think of the phenomenon of tweeting as a process that starts over after each tweet. In that case, a more appropriate quantity to think about might be the amount of time since the last tweet. Let's make the hypothesis that this is the right way to think about the phenomenon. How long until you tweet next, and how it is influenced by any other factors. Let's call this quantity the inter-tweet time. At the end of the last video, we posed the question, can we predict the next time a person will tweet? In order to make this question quantitative, we proposed two ways forward. One was to use the quantity, the time of day. The second was to use the quantity, number of seconds since the last tweet. And we propose that using this quantity makes sense if we think about the tweeting process as starting over each time some one makes a tweet. Let's go ahead and add a Q note here, to signify our selection. Now as far as our next step, we know that our goal is to create a regression estimator of inter-tweet time. Although as we know, we can capture this quantity in another statistical inference problem. For example, we could capture this as a hypothesis test or also as a classifier, but our goal is to be bold and create an estimator. I've tried to capture that here with the expression r of delta t. So to make that more precise, I'm going to use the quantity, time since the last tweet. And let's abbreviate that as delta t. And r of delta t is just time until next tweet. The thing we're trying to predict. So now that we've arrived at a statistical formulation of the problem, the next step is to propose a model to represent this estimator. But in order to do that, we must understand what structure exists within the data. To do that, let's examine a histogram of the inter-tweet times. So to create a histogram of the inter-tweet times, we first need to collect the values. So let's go ahead and do that. So in the first line here we're just using Pandas to read in the JSON data. Let's go ahead and do that. Then we're going to pull in the created at attribute for every row. Now, for each tweet that we've collected, we're just going to evaluate the difference between the times in which each tweet was created. Okay, now we've got most of those values in the time until next list. Now, Pandas has a very handy data type called a Series. A Series wraps a list of values, and provides a lot of handy functionality around that list. For example, once you've got a list of values that has been wrapped in a series, you can evaluate a histogram very easily. Now that's very interesting. The histogram looked like this graph. Now, from what you've learned about statistics so far, what kind of distribution does this graph show? Is it a normal distribution, an exponential, a poisson distribution, or a gamma distribution? This graph looks very strikingly like an exponential distribution. Now, what does that mean about our process? Well exponential processes are very frequently used in the industry to model rare events. For example, they're used to model the distribution of everything from heart attacks in a given city to traffic accidents to the arrival of requests in a file system. So, let's go ahead and fit our collected data to an exponential distribution, we'll adjust our analysis graph as such. Now, as you recall from your introduction to statistics class, the form of an exponential distribution is 1 over beta times e to the power minus x over beta. Thus, fitting an exponential curve to the earlier histogram, means finding the parameter beta forming the PDF from which the observed data is most likely to have been produced. Now this gets into something called a maximum likelihood estimation. This is something that another Udacity class goes into a lot of detail about, we've put a link to that lecture in the instructor's notes. But very briefly, the maximum likelihood estimation strategy takes the following approach. Let's suppose that I've got a collection of data points, data point d1, d2, through dN. The maximum likelihood estimation approach, asks the question, what's the probability that I saw each of these observed data points all at the same time. I don't know the specific values of these probabilities, but I do know the form of them, because I know that they conform to the exponential distribution. The only thing I don't know is beta. And, so I can write this expression as a function of beta. When I maximize this, I can then find the value of beta that gives me that maximum. That's maximum likelihood estimation in a nutshell. Now, what does this get us? If it is the case that an exponential process is a good model for describing tweeting behavior, then we can use it to predict the next tweet time. So this comes actually from a simple expectation calculation. Let's suppose that we already know the parameter beta. Let's define that x is the random variable time until next tweet, a.k.a inter-tweet time. Now the PDF for x at time t is just the exponential evaluated at t. The expected value of the distribution, it's just this integral here, which should look very familiar, which is equal to the parameter data. So let's think about what this means fora second. If we have a fitted exponential distribution meaning we know what the parameter data is, then it turns out we can use the parameter data as a prediction of the number of seconds to wait until the next tweet. Now, Let's go ahead and find the beta parameter using Python Notebook. Let's go ahead and find the beta parameter using IPython Notebook. So here we have again the same histogram describing intertweet times that we saw earlier. So let's first think about what we're trying to fix. We want to find the beta parameter which produces a curve which most looks like this. In order to perform the exponential fit, we're going to use a function from the scipy package called curve_fit. Let's just go ahead and import that. So curve_fit does the following. Let's suppose that we got some values which we're trying to reproduce. And we got a callable function here called fitFunc, which you can pass an independent variable t and a parameter, any number of parameters, but in this case we've just got 1, the beta parameter. And as you can see, all that does is just evaluate an exponential at the appropriate value. You can see here that the number b is defined as the inverse of what it would typically be defined at in an exponential distribution. So instead of 1 over beta, it just appears as b. So you can, anywhere you see b, you can think of that as 1 divided by beta. So again, fitFunc is just a callable function which takes the input independent variable time t and the beta parameter represented here as its inverse just as the letter b. And then evaluates the exponential function at the appropriate time. So if you hand curve fit that callable function and the values which are trying to reproduce here captured in the variable count. Then the curved fit finds the value of beta to put into fit func that best matches the count values, it does this using mean squared error loss. Let's go ahead and evaluate that. You can see here that curve_fit actually returns two parameters, fitParams and fitCov. The first one, fitParams is just the beta parameter that we're interested in, and you can see that the actual beta parameter is one over the parameter that we specified. So, 1,451.5 seconds, but curve_fit also returns a covariance matrix. The second thing that cur fit returns is a covariance matrix. There's one entry in the covariance matrix for each pair of parameters that we're trying to fit for. So in our case, we have only single parameter, b, that we're trying to fit for, and so the covariance matrix with only a single entry. Although in general there is more. So what does the matrix mean here in this case. Well, what curve_fit actually did, is it tried to the best value of b over many bootstrapped samples. This number here which amounts to the variance of that number describes how much b changed over those bootstrapped samples. So, the higher this covariance value here means, the more that be changed over the bootstrap samples. That means that we should be less certain of what the parameter's value is. As you can see here, the covariance value is quite low, 1.04 times 10 to the negative ninth. That translates to high confidence. So, we got our beta value, 1451 and a half seconds. Let's take a look at how well this fit does. You can see, here, in blue, is the original intertweet times. The exponential fit that we just did, is displayed in yellow. So we got our beta value, 1451 and a half seconds. Recall from earlier that if we are going to use an exponential fit as our model, then we use the beta parameter as the number of seconds to guess until the next time that the person tweets. And so, the model that we would use in this case is that we're going to guess 1451.5 seconds, until this person tweets next. Let's take a look at a histogram of the signed error of always making that guess. So, here, we're looping through each value in timeUntilNext. And evaluating the difference between that and our guess. Now, let's look at a histogram, that's very interesting. In the next few videos, we'll take a deeper look into how well this guest turned out. But, before we do that, let's just summarize where we are. Now that we've found the distribution that results in our observed data, what confidence can we have that the data parameter we captured is the true value. The answer is it turns out it depends on how much data we have. To understand why this is, let's consider the following. Imagine that the graph on the left and the right are going to show the PDF for a random variable x. Let's suppose the one on the left looks like this, and the one on the right looks like this. We can ask the question what's the probability that the random variable x will be greater than a value t. For the graph on the right, more of the probability mass is over on the right side, so it's not surprising that the probability that x is greater than t is larger for the graph on the right, than the one on the left. There is an inequality called Markov's Inequality, which quantifies this notion. Specifically, Markov's Inequality says that, for any random variable, x, the probability that it is greater than t is less than or equal to, its expectation divided by the number t. Now, for the derivation of this, please see a link in the instructor's notes. Now, this becomes really interesting when you substitute x with x minus mu. Meaning the mean value of x. That is you can think of this quantity here on the right. As x's deviation from its mean value. When you take this and substitute it into Markov's equality. It then becomes something called Chebyshev's inequality. Chebyshev's inequality says that, the probability that x deviates from it's mean value by an amount, t, is bounded by an expression described by it's variants and t. This is actually really amazing, and quantifies exactly the concepts that we have intuitively talking about. It says the probability that x deviates from its mean is directly related to its variance and its true for any random variable at all. Now here's where we get back into thinking about our beta parameter from our exponential fit. Recall that if you have a bunch of data points x1 through xn, and you suspect that they might be fit by an exponential distribution. The maximum likelihood estimation method says, the way we get the beta parameter for that exponential distribution is to take the average of the values you have collected. Now, x bar itself is a random variable, and so we can reason about how much does the X bar that we've calculated deviate from the true beta value. And let's note the actual beta value with a beta star. Now to calculate this we can just directly use Chebyshev's inequality. Now all we're doing is we're substituting x bar minus beta star with x. If we do that, we have this expression, the probability of x bar minus beta star is greater than epsilon is less than or equal to the variance of x bar divided by epsilon squared. And that number turns out to be less than 1 divided by 4n epsilon squared. So in the previous video, we described how the probability that our estimate for beta deviates from the true value, is the following. The probability that x bar minus the true beta star is greater than epsilon, is less than or equal to 1 divided by 4 and epsilon squared. We want to get a feel for how many data points that we need to be able to say something with accuracy. Let's suppose that we want to deviate from the true beta by no more than 0.01. And that the probability of that occurring should be no more then 10%. How many data points do we need? Write your answer in the text box below. So if we just plug in our values into Chebyshev's inequality, we see that x bar should deviate from the true beta star by no more than 0.01, and the probability of it doing that should be less than or equal to .1. So if you solve this for n, you get 25,000. Now this is troubling because we only have about 3,200 points. So what can we say with 3,200 points? Well, if all the other parameters are the same. And we just replace n with 3,200. The probability that deviation from the true data star is greater than 0.01, is 0.78 which is pretty high. However, we can decrease this probability to a more manageable 5%, if we drop our accuracy bound from 0.01 to 0.0395. So, just to summarize what we found out. For our 3,200 data points, if we accept a deviation from the true beta star of .0395, we can be sure we'll deviate from it with a probability of 5% or less So just to recall where we're at, we started out taking a look at histograms of inter-tweet times in order to get a feel for the data. We observed that it seemed to follow an exponential distribution. Now as we recall the form of an exponential distribution is the following expression. And so we did a maximum likelihood estimation fit for this distribution to the observed data. But the problem is, how sure are we that the beta value we fit is the correct value? And as we discovered, that confidence depends on the amount of data we have. And we found that we could quantify that uncertainty via Chebyshev's inequality. Which simply says that the absolute value of the deviation of our estimate of beta from the actual value of beta, that the probability of that being greater than epsilon is one over four n epsilon squared. But there's actually a tighter bound on this confidence interval via something called Hoeffding's inequality. So Hoeffding's inequality is saying exactly the same sort of statement about our estimate of the true beta parameter. It's just talking about the probability that our deviation from the true data parameter is greater than epsilon, but the bound on it is a little bit better. Instead of one over four n epsilon squared, it uses two times e to the power of two n epsilon squared. A link to more about Hoeffding's Inequality is in the instructor's notes. But again, as you can see, the inequality depends on the amount of data that we've collected In the previous video, we introduced Hoeffding's inequality, which puts a probability bound on the amount by which our estimate of beta will deviate from the actual value. We saw that this bound is actually a function of the number of data points that we have. So, let's try to apply that to our own problem, and try to find a confidence bound for our beta parameter. Recall that the beta value that came out of our exponential fit was 1451.5 seconds. Let's used Hoeffding's inequality to put a 95% confidence interval around this value. So, first let's recall the number of data points we have is approximately 3200. So, what we want is for the right side of this expression to equal 5% or 0.05. So specifically, if you solve this expression here for Absalom, you get a value of about 0.0240. Now, what does this mean exactly? That means, this interval traps the true value of data with a probability of 95%. So, if we're going to use this exponential to predict the amount of time to wait until this person Tweets next, the best that we can say is that it lies somewhere within this interval Re-shooting video ten, create and evaluate an estimator. Let's summarize where we are. So recall we started with the initial general question, can we predict the next time a person will tweet? So we started with a general question, can we predict the next time a person will tweet. We decided to use the number of seconds since last tweet, in order to quantify the problem. We decided that we wanted to estimate the number of seconds since last tweet, using the regression estimator. And we modeled that regression estimator using an exponential fit, which gave us an initial data parameter of 1451.5 seconds. We then used half things inequality to put a confidence band around that. Now, lets see how this guess turns out. So, for each value in our time until next list, we're going to subtract from it, our guess of 1451 and a half seconds. Then we'll just collect those values, in this list (exp_diffs). And, just to pick up from where we were before, here's a histogram of those sign differences. So, the first thing you might notice, which is very interesting, is that most of this graph occurs before 0. The meaning of this is that most of the time our guess is over estimating the actual intertweet time, so to make this quantitative let's use the describe method. As you can see here, over 50% of the time it overestimates the time until next tweet, by at least 18 and a half minutes. Now in the next video we'll continue to investigate the results of our model, but before we do that it's time for a quiz. So in the previous video, we created a histogram, of the signed errors of using our guess of 1,451 1/2 seconds. Please create a histogram of the absolute differences, and write the mean value of that histogram in the box below. Reshooting video 11 solution. So, previously we found a histogram of the signed errors of using our guess of 1,451 and a half seconds. Here we're trying to find a similar histogram, but of the absolute errors. So, to do that we're going to do something extremely similar. Lets again run through the time until next list. So, here again we're going to run through the time until next list. But instead of just depending the signed difference here, we're going to create a new list, abs_diffs. And we're going to append the absolute value of the very same difference. So, lets just go ahead and evaluate that. Now that we have those values, we can create a histogram. So, let's just recall what we're looking at, we're looking at each actual intertweet time and subtracting from it, our guess of 1,451 and a half seconds. We've collected all of those differences together in a list and this is the histogram of those differences. And, let's use describe to get the answers that we want. You can see here that the mean absolute difference is 4446.8. Now if you recall, when we looked at our histogram of signed error, using our guess of 1,451 and a half seconds, we saw that most of the histogram occurs before zero. This suggests that if we adjusted our model just a little bit by perhaps adding an offset, we might be able to improve our accuracy. So let's think about that problem a little bit. So our initial guess, of the exponential fit, says that our initial for the PDF of inter-tweet times has this form. One over beta, times E to the power of -t over beta. And then we found the value of beta, which is most likely to have produced the data that we saw. Now, our insight that we might be able to add an offset, means that we're suggesting the model. Might look, something like this instead, and then, in addition to finding a value of beta, we should also look for a value of C, that then makes it most likely to have produced the data that we saw. We can actually generalize this, to a slightly larger class of exponential models. In addition to a constant c, we can also think about another constant, A. Where A and C are constants. So what does this mean, exactly? Well, we're still trying to solve the same problem. We have a collection of data points, describing a histogram and we're trying to find a curve that best fits it. So previously, we just used beta. And we used the knob of the beta parameter, until we found a PDF that best fit the curve. Now, in addition to using the knob of beta, we're going to attempt to use two additional knobs, A and C. And the thought is, can we use the three of these together to get a fit for the PDF, which is more accurate? Lets find out. So we're going to use precisely the same method in i pi fun notebook to perform a fit. Specifically we're again going to use psy pi's curved fit in order to find the best parameters. This time however. Fitfunc is gong to have a couple of extra parameters. So in addition to just having the constant B, now we're reasoning about the constant A and the offset C. And so fitfunc lets you pass to it in addition to the independent parameter time, the parameter which we had before, B. But as well as that A and C. Other than that, it's exactly the same. We're setting to curve_fit, we've got this callable function with independent value t and a bunch of parameters, and we're going to feed it, these values that I want you to fit to. Now given that, find me the parameters which best fits those values and give me the covariance matrix which describes how certain we are about those values. Let's go ahead and do that. So, here are the parameters that it came up with for A, B and C. So curve fit says the value for a is 3.34 times 10 to the negative 1, or 0.334. We have a new B value, of 2.172 times 10 to the negative 3. Which means, in order to get our beta value, we just find the reciprocal of this and then the offset value C, is quite small. 3.185 times 10 to the negative 6. Pretty much negligible. So, what does that mean? It means that the adjusted model, looks something like this. Recall that the generalized form of the adjusted exponential model. Is a times 1 over beta, times E to the power of minus T over beta, plus offset C. Except that now, we have actual values for A, beta and C. So, let's just plug those in. So, the result of our new fit, looks like this. It says that, our new model is approximately 3.34 x 10 to the negative 1, times 2.172 times 10 to the negative 3, times E to the power of minus 2.172 times 10 to the minus 3 times t, plus an offset that is nearly zero. So recall that once we've got our model, in order to go from our model to our guess, of the number of seconds to wait. We take the expected value of this. So what we want is expected value of inter-tweet time, given that the above is our model. And so if we are to evaluate that, that comes to about 153.83 seconds. So that's to be compared against our initial value of 1,451 and a half seconds, quite smaller. Now let's see how well this new guess of 153.83 seconds works out. So first thing's first, let's evaluate our adjusted histogram. So just as before, the blue indicates the original intertweet times. And the yellow, is the initial exponential fit that we did. To that graph, we've added the red, which is the generalized exponential fit, using the A and C parameters. You can see that it's a little bit of a tighter fit, just sort of qualitatively. Now, let's form a histogram of the signed error for our updated guess of 153.83 seconds. So again here, we're collecting signed differences and absolute values of differences, just now for our updated guess instead of the original guess. So we see that the mean of the absolute error, has decreased to 1401 seconds from 4446.8 seconds, as it was before. So it's gone down quite a bit. In addition to that, we've gone from being within about 1,437 seconds 75% of the time to being within 676 seconds 75% of the time, of the actual inter-tweet time. So that's saying that with our adjusted model were good to within about 11 minutes, 75% of the time, which is actually not terrible, given that we still don't really know anything about the dynamics of the problem. So, before we proceed, let's just summarize where we are. So we decided to build a regression estimator to estimate the number of seconds since the last tweet. We decided to model this regression estimator, using an exponential fit. Our initial fit. Suggested always guessing 1451.5 seconds. After forming some confidence bands on this value of beta and taking a look at the absolute and assigned error, we decided we want to take a step back and look at a slightly more generalized version of that model. With a greater number of parameters. After fitting the new generalized model, we came to a new guess, of 153.83 seconds, and discovered that it performed a bit better than the initial fit. Now, in the next few videos, we're going to dig deeper into this data, and the dynamics of the problem, as we do a more formal investigation. We would like of course to do better in terms of prediction accuracy. Unfortunately there's quite a lot of variance in this data. So, we've already been looking at intertweet time. Meaning if there's a tweet, t1 and t2, intertweet time is the number of seconds between them. But what we're trying to predict is more granular than this right? Specifically, if it's been delta t seconds since the tweet occurred, we want to try to predict the amount of time, labeled p here, until the person tweets again. So in order to do this, we have collect a bunch of training examples. So our training examples, are going to look like this. We want lots of pairs. Of elapsed time since the last tweet. And how far we have to go, until the next tweet. To get an understanding of the variance in our data. Let's create this training data from the given raw data. And then make some graphs. You can see here, we're just producing these very same points. >From the given raw data and timeUntilNext, which is just the time between tweets, we produce a series of more granular points. How much time has elapsed, and how much to go until the next tweet. And they collect them together in the list data_points. So, to get an idea of variance let's take a look. At a few fixed points. Specifically, let's look at, what the data looks like when ten seconds has elapsed, 150 seconds has elapsed, and 100 seconds has elapsed. To get an initial idea about what the variance looks like in our data, let's fix it at a couple of points. Specifically. Let's look at what the data looks like after 10 seconds has elapsed, and 150 seconds has elapsed. Let's zero in on the 10 second graph first. So let's look at what this graph is saying. This is just a histogram, saying that if 10 seconds has elapsed since my last tweet, how long did I have to wait until the next tweet occurred. So here, zero seconds had to elapse until the next tweet occurred. And at the other end of the graph, 90,000 seconds have elapsed until the next tweet occurs. And as you can see, it's skewed strongly towards zero seconds. Again, it looks like an exponential distribution. Now, let's look at what this looks like for 150 seconds. Again, this is a graph of. After 150 seconds has elapsed since the last tweet, how long did we have to wait until the next tweet. At the left end, we had to wait zero seconds. At the right end, 90 thousand seconds. As you can see again, it looks just like an exponential distribution. In fact, let's look at a graph of the standard deviation. Again, the x axis here is the number of seconds that has elapsed since the last tweet. So zero seconds, 10 seconds, 20 and so on. The y axis describes the number of seconds until the next tweet occurs. And this graph describes the standard deviation in that quantity. So as you can see. The spread of the times that we have to wait until the next tweet occurs increases as the time elapses since the last tweet. Let's visualize this in another way. This graph shows a 95% confidence interval of the time we have to wait, until the next tweet as a function of lapsed time. So after 20 seconds has elapsed. Here's the spread of how long we have to wait. As you can see it spreads all the way from zero seconds to 30,000 seconds. After 60 seconds has elapsed the spread goes all the way from zero seconds to 35 thousand seconds. So as you can see, this is just another way to describe the graph we just saw of the increasing standard deviation. The confidence band just increases as time goes on. This is just another way of saying that as time goes on, we're more and more uncertain of how long it's going to be until they tweet next. Any estimator that we create, which is a function of delta t. Will do so by essentially averaging over this spread. At any given point along the graph. Again as you can see the spread gets wider as the elapsed time goes on. Reflecting greater and greater uncertainty in the data. As the time since the last tweet grows. Elapsed time becomes less and less good. At determining what the inter-tweet time is. And any predictor we make based on delta t will suffer from this. This problem is called inherent variance. Note, this is not the same thing as model variance, which refers to how the output of your model is sensitive to training data. Inherent variance refers to the following concepts. Let's suppose, that our real model, meaning the actual real life phenomenon was in fact a linear one, meaning you had some function f and it was a function of the single variable x. As data scientists we collect records of the form x, f. >From these records, we will try to build an estimator and we'll probably have a pretty easy time of it, right? For a given fixed value, x1, we will always see the very same value of f beside it. A times x1. Over and over as we collect records. Every time we see x1, we will always see a times x1 beside it. Now, let's suppose we introduce some noise into the situation. The way to think about this is, x occurs but before we can measure it as f. Some noise process occurs to x, and the result is the thing that we measure. The consequence is, now we no longer see the same f for a given x. The first time we see x1, we might see value 1 beside it. The next time value 2 and so on. This spread of values, v1, v2 and so on, produces the confidence band effect that we saw earlier. That of course makes finding the right value to guess for a given x1 much more difficult. In the next video. We'll talk about sources of this variance, and what we can do about it. What do you think is the cause of the phenomenon as illustrated by this graph from the previous video? Specifically why does the spread get wider as the elapsed time goes on? Please share your thoughts in the forum. I have shared a forum link in the instructor comment below. Now, this is how I think about the problem. Does this phenomenon occur because the process which produces intertweet time is in fact random, or is it in fact influenced by factors which we have not seen. If the answer is the former, then the best we can do is our exponential fit estimator with our guess of 8,585.5 seconds, which we developed above. However, if it's the latter, then we can reduce the inherent variance, effectively reducing the spread, making intertweet time more predictable by finding the factors on which intertweet time depends. To see this, let's look at a simple example. But if it's the latter, then we can reduce the inherent variance, effectively reducing the spread, making intertweet time more predictable by finding the factors on which intertweet time depends. Now, to see this, let's look at a simple example. So in our last video, we started talking about how uncaptured features, might be having an impact on the variance of inter-tweet time. Now in order to understand how this might be, let's look at a simple example. Let's suppose, we know that the model that is producing data is a known linear model, 2x plus 3. In addition to this we are going to add some background noise. Specifically, we're going to sample randomly from a normal distribution, with a mean of 0 and standard deviation of 50. We're going to sample many points of the input and output, and create a graph of the 95% confidence interval. So, here, we're just sampling points from x, our linear model and then some random noise. We've added our collected data points onto data points_1. Now for each value x, we have a spread of values. For that spread, we're going to calculate the 95th quintile, and the fifth quintile. And now we'll just plot. So in this graph, the red line is a real line, the true model. And the blue and green lines, represent the top and bottom end of the confidence interval. As you can see, as a result of introducing some of the noise, there's a little bit of a spread here. Now, let's suppose that instead of using the single variable model, with some normal noise we're going to adda new feature. So our model now depends on two features, x and y. But we're still only going to collect x values and the response values. We're not going to collect the y values. That is lets pretend we don't know about y and just let it influence the observed values of f. As you can see, not capturing the additional feature y, has resulted in dramatically increased uncertainty and consequently variance in the response value. In this way, we can see not capturing enough features is potentially a source of inaccuracy and uncertainty in our estimator. Thus let's try searching for features which we have not yet captured. If we successfully find features which have an influencing force over tweeting behavior, we just may be able to reduce the variance of our data. In the next few videos, we'll talk about a tool from the field of information theory for systematically doing so. In the following videos, we're going to do an aside on information theory and how it's used in data science, specifically with regard to feature selection. The purpose of this video is to equip you with the necessary background knowledge to use information theory as a tool in the future. In the next video, we'll go back to building our inter Tweet time predictor, and how we can apply information theory to it. Information theory is a field of computer science developed by Claude Shannon while working at AT&T's Bell labs to understand how to send reliable digital signals. It quantifies the notion of the amount of information contained in a message. The science of information theory is a powerful tool used all the time by data scientists to understand the relationship between parameters. I want to give an overview of what information theory is all about and how it's used in data science. So, let's consider the problem. We're collecting a bunch of data records because we're data scientists. But, we want to zero in on two in particular, X and Y and the question is, do X and Y have a relationship with each other? Let's suppose that x and y are numerical. Do x and y have anything to do with each other? Let's try to pose the question in a more precise way. You've been collecting records and so, you can form a joint probability table between x and y. So we can say in an absolute sense, in a prior probability sense. What degree of certainty do we have about x? If x takes on the values x1 or x2 we can ask, what's the probability with which it takes on those values? So, from the table here we know we can just evaluate those numbers. They're just 0.29 and 0.71. Now let's suppose that we know y is stuck at y1. Does this make us more certain at all about what x is? Specifically what's the conditional probability that x is x1 or x2, given that we know y is fixed at y1. Well again we can just directly evaluate it from the table. It's 0.16 and 0.84. So it seems that knowing that y is stuck at y1 has told us something about the value of x, right? Intuitively we know that. It caused the probability to skew a little bit to one side. In this case, towards x2. It turns out there's an expression to measure the degree of skew of a probability distribution. This expression is called entropy. Specifically, let's suppose that a random variable x takes on the values x1, x2 though xk. The entropy for the probability distribution of x is labeled h of x. And it is defined as minus p1 times log of p1 minus p2 times log of p2 and so on through pk. Otherwise written as minus 1 times the sum of pi log pi. Now the base of the logarithm isn't necessarily important. If the base is base two, then, the units of entropy is called bits. If the base is e, then the units are called nets. Traditionally in computer science, we use base two, but, it's not really super important. Now to see what's behind the expression h of x. And why this formula was chosen over other options for formulas and much more details about information theory in general, see the instructor's notes. So last time we talked about how entropy can be used to measure the degree of skew of a probability distribution. And it's defined to be the expression here. The sum of pi times log of pi times minus 1. Now let's examine the properties of H of x. H of x has a minimum value of zero. This happens when the probability distribution is maximally skewed. And, by maximally skewed, I mean it looks like this. The probability that x will take on any of its values is zero, except for a single value which has a probability of one. This represents a state of absolute certainty about what value x is going to take on. Conversely, H of x takes on its maximum value when x's probability distribution is at its most random. Meaning, if it can take on k possible values. The probability that anyone of them will occur is one over k, thus as the distribution of x becomes more skewed it's entropy goes down Now, this gives us exactly what we wanted to describe quantitative at the beginning. Recall the problem, with the random variable X that takes on value X1 and X2, and we ask the question, what's the probability distribution between X1 and X2? And further, does this probability distribution, become more skewed to one side or the other, when we know that Y1 happened? So now we know, that this schemeness, is precisely captured by the entropy, before and after conditioning on Y1. Specifically, we want to look at H of X and H of X given that Y equals Y1. If the entropy, went down as a result of Y1 happening, then we know that Y has an impact on X. Now, the amount that H went down due to Y being Y1 is H of X minus H of X given Y equals Y1). Similarly, the amount that H went down, due to Y being Y2 is H of X minus H of X given Y equals Y2. The impact, from Y1 and Y2 are averaged to form what's called Conditional entropy. As you can see, Conditional entropy, is simply the expected value of H given the value of Y. And thus, the expected value of reduction to entropy on X due to knowing Y is H of X minus H of X given Y. Finally, this tells us what we need to know about selecting features. It we're interested in the random variable X. And we've been collecting other variables, A, B, C, and so on. The variable with the maximum influence over X, is the one for which the reduction and entropy is the largest. More precisely, the variable which has the biggest impact on X, is the arg min of this expression. In the next video we will apply this to identify important features influencing enter tweet time. It's your turn to try an example. Let's suppose we have a random variable X, which has a relationship with random variable Y, and separately with Z. The relationship between X and Y is described in this joint probability table. And separately, the relationship between X and Z is described in this table. The question is, which feature is better, Y or Z, for describing random variable acts? Recall that, if we have a random variable x, and we want to understand its relationship with another random variable, v. The way we do that is we examined the expected loss and entropy due to that variable. Specifically, to understand the relationship between x and y, we examine the expected loss and entropy due to y. Similarly, for z, we do exactly the same thing. We examine the expected loss and entropy due to z. After we know these two quantities the feature that we want is the one that results in the biggest drop from h of x. So as you can see in order to do this we'll have to calculate three quantities. h of x, the conditional entropy, the entropy of x. The conditional entropy of x given y and the conditional entropy of x given z. I've written them here, over off to the side, so that we can keep track of them as we do our calculations. Let's go ahead and find them. So h of x is just this expression minus one times the probability of x times log of probability of x. Minus the probability of x2 times the log of probability of x2. Now, remember the base of the logarithm doesn't really matter. Traditionally, either base two or base e is used. Let's assume for these calculations we're using base e. So, we can just pull the values for the probability of x1 and x2 straight from the joint probability table. And if we do that, we find that we get minus one times 0.472 times log of 0.472 minus 0.528 times log of 0.528 which is approximately 0.691. So let's just keep track of that by writing it up here. So one of three down. Now we just have to do h of x given y and h of x given z. Recall that the conditional entropy of h of x given y is just a conditional expectation. It is the average of how much we expect h of x to change due to y1 and due to y2. So, the first term is just this expression below. So first, so first, let's tackle h of x given y1. H of x given y1 is just minus 1 times the probability of x1 given y1, times the log of probability x1 given y1, minus probability of x2 given y one times the log of probability of x2 given y1. And again we can just pull these numbers straight out of the joint probability table above and if we do that we find we get minus one times .0318 times log .0318. Minus 0.779 times log 0.779 and that's approximately 0.3043. Let's just make a note of that here. The calculation for h of x given y2 proceeds in exactly the same way. And if you evaluate that, you get approximately 0.6888. And let's just make a note of that here. So if we put it all together, h of x given y is .32075 times .3043 plus .67925 times .6888, which is approximately .566. Let's make a note of our second term above. Now the calculation for the conditional entropy of x given z proceeds in exactly the same way. I'm not going to step through the calculation for this one as well, I'll leave that to you. But you'll find that if you do, you'll get approximately 0.5787. So now we have all three of our terms. Before knowing anything about y or z, h of x is .691. It drops to .566 when we condition on y and it drops to .5787 when we condition on z. So recall that the feature that we want is the one that results in the biggest change in entropy. Which is the arg max of h of x minus h of x given a variable. Where in this case the variables range over y and z. So you can see that in this case the answer is y. Although not by that much. This difference for y is 0.125. Whereas for z it's just slightly less at 0.1123. So although y is the winner here, z doesn't do that much worse. And so in practice, if I were going to build a model in order to understand x, I might consider including both y and z. But if I had to choose one, y is the winner. Let's recall our path of analysis so far. We started with the question, can we predict the next time a person will tweet? In order to quantify this question, we chose the number of seconds since the last tweet. Now, the nature of the question suggests creating a regression estimator. We want to directly predict how long until the next tweet. Taking a look at the initial histogram of inter-tweet times suggested creating an exponential fit. After seeing how well that did, we saw that it was usually underestimating the actual inter-tweet time by just a little amount. And so we added an offset. After adding that offset, our model became to always guess 8,585 and a half seconds. We found that when we did that, our model was correct to within about five minutes, 50% of the time. Now, of course, we would like for it to be more accurate, but there was a problem. When we started our analysis, we attempted to model intertweet time using a simple exponential process. What do you think? Do you think intertweet time results from a random process? So the answer is, it's a little bit of both. It is a random process in the sense that we're modeling intertweet time as a random variable and we're taking draws from that variable. Now, if it was totally random, we wouldn't be able to do any better than modeling it as an exponential process. But of course, as we have just found out, there are clearly features that are influencing this process. Specifically, length of a tweet and mention distance. How long it's been since this person has been mentioned by one of his close friends. So it's not as totally deterministic as a functional mapping. But then on the other hand it's not just complete randomness. But if I have to chose one or the other, yes is the best answer. Let's examine our analysis so far. So when we left off, we had updated our model to add an offset so that our guess is always 8,855.5 seconds. However, we've just done an information gain analysis to discover that intertweet time is very strongly affected by tweet length and mention distance. Let's update our diagram in order to reflect that. Let's update our regression estimator to include the new features. In our new estimator, we're going to pay attention to, in addition to elapsed time, the length of the previous tweet and the number of seconds that has elapsed since Marco Arment or John Siracusa has mentioned this person. We're ready now to think about a model to implement this new regression estimator. What kind of model should we use? Well, let's think about it. We're trying to build a model that captures the relationship between intertweet time, elapsed time since previous tweet, otherwise known as delta t, the length of the previous tweet, and the amount of time that has passed since a close friend has mentioned him. Let's first consider the classical statistics way of looking at the relationship between two random variables. That would be covariance. Specifically, let's go into IPython Notebook now and examine the covariance between intertweet time and mention distance. So to evaluate covariance between intertweet time and the mention distance, we're going to use the linear regression model from the sklearn package. We're going to ask the question, essentially, what does linear regression think the relationship is between intertweet time and mention distance? So we fit the model. The coefficient of the model will give us our value for covariance. As you can see, the coefficient is quite low, 0.015. This is very strange, right? Information gain and the two histogram plots that we looked at showed very clearly that mention distance has quite a high impact on intertweet time. What's going on here? Information gain and histograms that we looked at before clearly say that mention distance and intertweet time are very strongly related. However, the covariance score that we just looked at seems to say that they're are not related. What's going on? Write down your thoughts in the text box below The answer lies in understanding covariance. Covariance only examines linear relationships and is at the heart of linear regression. A relationship which is not linear will have a very low covariance score. Let's look at an example. Let's look at y equals x squared between negative 1 and 1. So y is totally determined by the value of x in this example. Y and x have a very strong relationship. Now, let's suppose that I were to sample points from this curve totally uniformly at random. So I'm going to collect points x and y from points along this curve and just select them totally uniformly at random. And now, given these points let's calculate covariance. Well, the covariance between x and y is, if you recall the expectation of x minus the mean of x times y minus the mean of y. And if you evaluate this for the curve above you'll find that the answer is 0. So you'll find that the covariance takes on a maximum value when the degree of the polynomial is 1. As they increase in degree, but remain odd, the covariance gradually decreases slowly. But for even degree polynomials the covariance always evaluates to 0. Similarly, inter-tweet time and mention distance are related, but they're not linearly related. Now, this makes intuitive sense. If someone you know calls out your name, you are likely to immediately respond. In general, in addition, in real life relationships are not linear. A linear model is not going to be helpful here, because we require a model that is capable of portraying a non-linear relationship. Now, there are several options for accomplishing this. We might use k-nearest neighbor or we might consider using neural networks or we might consider using a kernel density estimation at each point of elapsed time. In fact, that might be where we'd go if we did not get anywhere using any other features. To make use of our feature set, let's attempt to use a k-nearest neighbors fit. Before we jump into fitting our k-nearest neighbor's model. Lets talk briefly about preparing out data points. So lets talk briefly about how we collect a series of training points that contain intertweet time, tweet length, elapsed time and mention distance. So recall in the original data set. We're given points, such as t1 and t2 representing two succeeding tweets. The amount of time taken between them is considered the intertweet time, but we want something that's more fine grained than this. What we're looking for is given an amount of time that has elapsed since the previous tweet, how much more time do we have to go until the next one? We're going to produce that information for each pair of given points. For example, if this is the intertweet time between two given tweets, we can choose a point here. And consider the beginning to that point to be elapsed time and that point to the end to be the time to go. And we can consider putting that marker at many points, giving us many values of delta s and p. This gives us many values of elapsed time and time to go. And for each one of those points, we still know the length of the previous tweet and the number of seconds that has elapsed since the last time this person has been mentioned. In this way, we construct out training points. So after constructing our training points in this way, we get a total of 1,381,258 points. And we will split those points into 70% training and 30% test. Let's go ahead and fit the data and perform some steps of validation and measure the performance of using k-nearest neighbors. So we're going to use package sk learns knears neighbors implementation. In the case of sk learn it is called k neighbors regressor. For fun I created a knears neighbors model using five neighbors and one using three neighbors. Lets collect our training data. And we'll use it to train our three nearest neighbors model. Now let's create a list of values containing the error values for the training data. Here's a plot of the histogram of the signed error on the training set. As you can see, it looks like performance on the training set is quite good it had a mean absolute error of 1,876 seconds. Although 50% of the time, it predicts time correctly within eight seconds or better. Let's look at performance on the test set. As you can see, the model did quite well in the test set as well. The predictor had a mean absolute error of 1,837 seconds. Although 75% of the time. It predicts time correctly to within 32 seconds or better. As you can see, our knears neighbors fit totally blew away our initial exponential model. Just to put that in perspective, let's examine how far we've come. As you can see, adding the additional features of text length and mention distance and then using the updated model of knears neighbors made a dramatic difference in our prediction accuracy. We went from guessing 8585 and 1/2 seconds. And being right to within about five minutes, half of the time to using a knears neighbors predictor, that uses elapse time, length of the previous tweet. And amount of time elapsed since the previous mention. In order to predict the amount of time remaining until the next tweet. Using that model. Were able to guess within 32 seconds of accuracy 75% of the time. Looks pretty good. In the next video we'll think about some possible ways forward. Along the way, you've picked up some useful tools for model building and data analysis. We've gained some insight in how the amount of data relates to accuracy in our models. We've learned to look for structure in our data. Sometimes, there is no functional relationship in data and what you're seeing is the simple result of a random process. In which case, the best performing model we can come up with is a fit to that process. We've seen how to identify non-linear relationships between variables. Sometimes, these relationships cannot be picked up by linear model tools, such as linear regression. We've seen that information gain is a more general and powerful tool for identifying relationships. And finally, we've learned how to quantity and communicate the uncertainty in our model using confidence bands. At this point, we could perhaps make updates to our model that increase its accuracy. Its accuracy is quite high at this point. And it would probably be more fruitful to consider new questions all together that our analysis has opened up. For example, how well is this model generalized to other users of Twitter? Can we classify Twitter users into different classes distinguished by which type of model characterizes their tweeting habits. We've made a model that predicts time to next tweet overall, but can we now build a predictor for time to tweet about a specific subject. As you can see, our analysis is opened up the room to many possible new investigations. Model building is a way to understand phenomena that are often complex and a result of multiple interactions. It is seldom possible to to clearly write down mathematical formulas that can describe the phenomena at all scales and complexities. Moreover, many phenomena can be repetitive or periodic with a pattern that we can identify. While other phenomena can be non-periodic, unusual, and anomalous. We will in this lesson look at some simple ways we can model phenomena. It is often a matter of experimenting with different techniques to understand what works best. The goal in this lesson is to get you to start thinking iteratively through asking questions and making models. We also want you to start thinking in terms of probabilities. Given that you have already had some background in machine learning, we will choose for an example non-parametric statistical models. The techniques we illustrate should carry over to parametric models, mathematical models, and machine learning models. We invite you to try these other models that we will not have a chance to explore in this lesson by using the methods that we demonstrate here. So, follow along. We're going to briefly explore various modeling techniques. But to be able to build any type of model, we will need to first investigate the data. We will be following the same iterative process of writing down our questions, selecting features from the data that can best answer the questions, build models to get some answers, and using validation techniques to determine the model that best answer the questions. Although we'll emphasize the process of model building, we will still keep iterating through the Q, M, V process, and use our diagrams to track our progress. Before we dive into a particular example problem, let's look at various approaches and techniques that we can use to build models. Let's look at some of the modeling techniques in use. One of the first techniques that comes to mind is mathematical models. They're usually useful in domains such as natural, social or economic sciences. The tools and concepts required to build mathematical models involve differential and integral calculus, differential equations, algebra, and other advanced mathematics. The advantages of mathematical models is that they can give generalized knowledge of systems. They also reduce complex phenomenon to simple cases, and we can use inductive and deductive reasoning to gain knowledge about our system and the models. Another important technique of model building is statistical or machine learning. They are applicable in industry, in businesses, and in general cases where we don't have domain-specific knowledge. The tools and concepts required for statistical machine learning models is a good knowledge of statistics, machine learning, linear algebra, and some information theory. The advantage of these models is that we can treat phenomenon in all its complexity, and we can make inferences using computational power. In practice, we often use a combination of mathematical modeling techniques and statistical and machine-learning techniques. These are applicable everywhere in science, as we mentioned, social and economic sciences, business, engineering, as well as optimization problems. All the tools and concepts listed above can come in very useful to build these combination models. The advantage is that we can use whatever model we find necessary and as per requirement to solve our problems. Hopefully, this table here gives you a general idea of where we use what sort of modeling technique. Although we're not going to cover mathematical models, we invite you to go through the tools and concepts required, and approach some problem with mathematical models if need be. We will start with the model building workflow diagrams you are now hopefully becoming familiar with. These diagrams will guide us to through model building and validation process and help you follow the thought process needed to build robust models. At the time of writing this lesson, there has been widespread suspicion that Medicare is being severely harmed by some fraudulent charges. And that large portions of payouts are allegedly going to a very tiny portion of providers. Less than a month after the data set was released, people already found some interesting things in the data. For example, the New York Times in this article by Reed Abelson and Sarah Cohen published on April 9th, 2014, writes about how the data reveals a single ophthalmologist who was paid $21 million in the course of a single year. We have put a link to this article in the instructor's notes. To identify and reduce cases of fraud in Medicare, the government has released the data to anyone who wants to inspect and see what is in it. We could try to answer this call to action. Together, we can examine the raw data released by the US government in order to detect anomalous behavior. Along the way, we'll learn methods for building models. So now, we can ask the first question. How do we detect if there are anomalous charges in the Medicare data? That question immediately leads to the next question, how do we define anomalies in data? To give you an intuitive understanding, if you have all your data clustered together here in one region and you have some points far away like these red Xs, we can consider the red points here as outliers. Anomalies can also show up as peaks and bumps in distributions. In this case, we can have an original distribution with a little bump in the tail here. This bump might indicate some sort of anomaly in the data. Another way we define anomalies in the data is by looking at domain specific signatures. Imagine, for example, a sequence of events in logs indicating an intrusion in a data center. We're going to further investigate outliers in distributions by looking at simple non-parametrics statistical techniques. We're going to start looking at the Medicare data from the Centers for Medicare and Medicaid services website. The link is provided in the instructor's notes. If you click on the link, you will come to the page that looks like this. >From the web site, look for the link that says Inpatient Charges for Fiscal Year 2012. Clicking on that link will take you to the page where you can download the data. You're welcome to use the entire data set, especially if you want to explore the data further than we do in the lesson here. However, to make things easier and faster, we have reduced the original data set and put the needed files in the instructor's notes. So look for files with Illinois and California in the instructor's notes, and that's the files we are going to use for the rest of this lesson. As in the previous lessons, we will be using the IPython Notebook to perform the analysis. Look in the instructor's notes if you have any further questions as to how to get started with the Notebook for this lesson. We now set up some of the libraries that we will need to use for this lesson. We're going to use seaborn and matplotlib for plotting. We're going to use Pandas for handling the data set. If you scroll down further, you will see a file in the Medicare data folder that says Medicare_Data_IL_2012.csv. This is a comma separated variable file. We created this file specially for you so that you don't have to use the entire data set. If you're interested in using the entire data set, use these commented out codes here to load the entire data set. Now we use the read_csv library from Pandas to read the data and load it into a data frame. As before, to run this piece of code, we just use the play button here. Once the data set is finished loading, you will receive a message like this. The f _IL is the Pandas data frame. We can use the describe function to look at the summary of this data. Let's run this piece of code. Once this finishes running, you should have a table that looks like this. The total number of events we have in the data frame is 387,623. You can slide this bar here to look at the rest of the table. Now let's look at the top five lines of the data frame. We can do that by using the head command. Once you finish running that, you will see another table. This table shows the actual data that has been loaded to memory. The top row gives you the names of various columns in the Pandas data frame. Each column consists of a NumPy array. If you are not familiar with Pandas data frames, we have linked to a lesson in the Intro to Data Science where it will teach you the high-level concepts and about Pandas and data frames. If you scroll further down, you will see it shows 5 rows and 28 columns. The 28 columns indicate there are 28 vectors in the Pandas data frame. Use the box below to write a Python code that uses the file for California included in your instructor's notes. Load it to memory as a Pandas Data Frame and print a summary of the data. Once you have finished writing your code in this box provided here, click on this box. Solution we'll look back in the iPython Notebook. Your code should look something like this. You could also use the original data set by un-commenting the commented lines here. Now we're on this piece of code, we can again use the describe function to look at a summary of the data. Once that finishes running, you will get a table that looks very similar to the one we got for Illinois. We can then look at the first ten lines of the f underscore California data frame. Running that code, then gives you this table. When you scroll down, you will see the ten rolls we wanted printed and twenty eight columns. So now, we have a new data frame for California. Throughout this lesson, we'll show examples with the state of Illinois dataset. At some point in quizzes, we'll ask you to use the California dataset and repeat what we have just shown. We're going to explore the Medicare dataset somewhat further. Remember, we wanted to see how we can find anomalies in the data. Although we wanted to find outliers, the x size here is not necessarily to find farther outliers. Note the exercise here is not necessarily to find outliers or fraud in the Medicare data, but to walk through the ways we can go about finding anomalies in data in general. So the next important question we are going to ask is what are the variables of interest in the dataset? Let's switch back to the iPython Notebook to investigate this. We had seen from the summary of the dataset that we had 28 different columns of data. We had seen from the summary of the dataset that we had 28 different columns in it. Let's try to print the names of the columns. To do so, simply execute this line of code. The list of columns is long. If you scroll to the bottom of it, you would see six variables that may be of interest to us. One of them is called average Medicare allowed amount. The other is average submitted charge amount and another is average Medicare payment amount. The six variables that may be interesting are the average and standard deviation of the Medicare allowed amount. The submitted charge amount and the Medicare payment amount. These are the variables that have directly some indication of charges. We're going to investigate these variables further. Some of the other columns are very specific to Medicare and could be useful if we did some more research on Medicare and the data. However, without doing some domain specific research, we can still see that these variables could be of interest in detecting unusual charges. To keep things simple, we are only going to use the average Medicare amount. We can use the standard deviation of the Medicare allowed amount and the submitted charge amount and the payments amount as inherent and reducible variants in the average charges. After looking at the variables of interest in the dataset, we came to the conclusions that the allowed amount, the charged amount and the payment amount might be of interest for our investigation. As in the previous lessons, we will be using the iPython Notebook to perform the analysis. Look in the instructor's notes if you have any further questions as to how to get started with the notebook for this lesson. We now set up some of the libraries that we will need to use for this lesson. We're going to use seaborn and matplotlib for plotting. We're going to use pandas for handling the data set. If you scroll down further, you will see a file in the medicare data folder. That says Medicare_Data_IL_2012.csv. This is a comma separated variable file. We created this file especially for you so that you don't have to use the entire dataset. If you're interested in using the entire dataset. Use this commented out codes here to load the entire dataset. Now use the reads CSV library from pandas to read the data and load it into a data frame. As before, to run this piece of code, we just use the play button here. Once the dataset is finished loading you will receive a message like this. The f_IL is the pandas dataframe. We can use the describe function to look at a summary of this data. Let's run this piece of code. Once this finishes running you should have a table that looks like this. The total number of events we have in the data frame is 387,623. You can slide this bar here to look at the rest of the table. Now let's look at the top five lines of the data frame. We can do that by using the head command. Once you finish running that, you will see another table. This table shows the actual data that has been loaded to memory. The top row gives you the names of various columns in the pandas data frame. Each column consists of an npi array. If you're not familiar with pandas data frames, we have linked to a lesson in the intro to data science where it will teach you the high level concepts and about pandas and data frames. If you scroll further down, you will see it shows five rows and twenty eight columns. The 28 columns indicate there are 28 vectors in the Pandas Data Frame. Use the box below to write a python code that uses the file for California included in your instructor's notes. Load it to memory as a Pandas Data Frame and print a summary of the data. Once you have finished writing your code in this box provided here, click on this box. The solution will look back in the iPython Notebook. Your code should look something like this. You could also use the original dataset by uncommenting the commented lines here. We're going to explore the Medicare Dataset somewhat further. Remember, we wanted to see how we can find anomalies in the data, but to walk through the ways we can go about finding anomalies in data in general. So, the next important question we are going to ask is what are the variables of interest in the data set. Lets switch back to the IPythonNotebook to investigate this. We had seen from the summary of the data set that we had 28 different columns of data. The list of columns is long. If you scroll to the bottom of it, you will see six variables that may be of interest to us. One of them is called average Medicare allowed amount. The other is average the submitted charge amount. And the Medicare payment amount. These are the variables that have directly some indication of charges. We're going to investigate these variables further. Some of the other columns are very specific to Medicare and could be useful if we did some more research on Medicare and the data. However, without doing some domain-specific research, we can still see that these variables could be of interest in detecting unusual charges. To keep things simple, we are only going to use the average Medicare amount. We can use the standard deviation of the Medicare allowed amount and the submitted charge amounts and the payments amount as inherent, irreducible variants in the average charges. After looking at the variables of interest in the dataset, we came to the conclusions that the allowed amount, the charged amount, and the payment amounts might be of interest for our investigation. We are going to now look at the distribution of the variables of interest. Let's first define f0, to be the average submitted charge. F1 to be the average payment amount. And f2 to be the average allowed amount. It's clear that this data is aggregated, and shows us only the expected or average values for charges. For our purpose, we could treat them as the representative of the actual charges. This however, is not always a good assumption. But we will work with this for illustrative purposes. Now, let's switch to the i path in notebook and look at the distributions of the average values in the data. So here, we have now defined the three variables f0, f1 and f2. Once we have defined the three variables we can now plot histograms with these variables. The code block here, plots two histograms on the same graph. Let's execute this code. Once the code has run, it will show you a histogram that looks like this. It's a simple histogram, with the average charged amount shown in green and the allowed amount shown in blue. We can try to draw the same histograms for the average charged amount, and average payment amount. You can do that by running this piece of code here. Once you run that code, you will have a very similar histogram, with the average charged amount shown in green again. And the average payment amount shown in red. Let's inspect this histogram a little bit further. We had plotted both the green and the red histograms in the range from 0 to 1,000. We also had bins of 50 each. What that means is the entire range from 0 to 1,000, was divided into 50 different bins. Each bin then contains the range between 0 to 20, 20 to 40 and so on. The Y axis shows us the number of times a data value was in that given range. This is the simple concept of a histogram. Now you will also notice, that the values here are fractional values. This is because both of the histograms have been normalized. What this means, is that the area under the curve of the histogram adds up to the number 1. To achieve this, we take the original histogram and divide by the area to get the value for each of these columns. Also notice that the shape of the two histograms are quite different. What this means is that the probability of charges between 200 and 1,000 are a lot more, than the probability of payments made between 200 and 1,000. The probability being given by the integral between 200 and 1,000 of this curve. In the case here, the integrals are merely the sum of the numbers in each bin. The histogram is, in fact, an example, of a non parametric statistical model. In the following few videos, we are going to inspect what it means to have non parametric statistical models, in greater detail. We have now explored the data set and drawn a few histograms. This particular quiz is not graded. But please do follow the exercise along. Use what your learned in lesson two about the irreducible variance to treat the average values of the three variables as random variables and the standard deviation as the irreducible variance int he data. So write code to do the following. First, draw histogram of f naught of the charged amounts. Then in the same plot, make another histogram of f naught plus 1 times the standard deviation of f naught, which is another variable given in your data. In the same plot again, draw a histogram of f naught minus 1 times the standard deviation of f naught. You should see something like a histogram with a error band. Once you are done with that, click on the box here. In the last video, we plotted the distributions of f0, f1 and f2. Define the same variables above for California as g0, g1 and g2. Draw the corresponding histograms for g0 and g1, and g0 and g2. Change normed equals 1 to normed equals 0. What do you see? This is an ungraded quiz. Once you're done, click on the box here. Here we see the solution to the quiz. We have defined three new variables, g0, g1 and g2 for California. Here we are going to plot the variables. Let's first run this code. Now that the variables are defined, we are going to first make a plot of g0 and g2 with normed equals 0. You should see a histogram that looks something like this. Now we are going to make the histogram of g0 and g1. Notice we have now changed normed equals 1. We expect a normalized histogram. Upon running the code, you should see two histograms. One in red and one in green. Notice the values in the y axis have changed. In these histograms, the area under the curve here or the count in each bin summed up must result to one. So far, we have inspected the various variables of interest in the data. We have looked at the allowed amount, charged amount and the payment amounts. Now we will ask the question of correlations. In asking if the variables are correlated, we can ask if the correlation is non-linear or if the correlation is linear. We can inspect this by looking at a scatter diagram of the variables. Let us now inspect the scatter diagram of the two variables, f1 and f2. Notice that we have plotted both the variables after scaling them. Once you're done, you should have a graph that looks very much like this. The scatter plot for these two variables are very linear. What we can conclude from that is that the paid amounts are directly proportional to the allowed amounts. Similarly, we can look at the scatter plot of f0 and f1. In this case, although we have some correlation, it is not as linear as it is between f1 and f2. We want to have a quantitative measurement of the correlation between two variables. We have already observed from the scatter plot of f1 and f2 that they're linearly correlated. We can calculate what is called the Pearson's correlation coefficient. Let's first define this quantity. Let's look at the histograms of f0 and f1, and f0 and f2 again. Let's switch to the iPython notebook quickly. Here we are looking at two histograms of f0 and f2. The f0 is the average submitted charge amount shown in green, and f2 is the average Medicare allowed amount shown in blue. Notice the shape of the two histograms are quite different. Moreover, the value ranges for f0 and f2 are also quite different. Similarly, you can look at the histogram of f0 which is, again, in green. And f1, which is the Medicare payment amounts, given in red. Here also you see a difference in shape, and range of the histograms. To compare two histograms in the same range, we can scale the variables. To scale variables, we can subtract the minimum value of the variable from each of those valleys of the variables. And divide by the range of the variable which is the maximum value minus the minimum value. Let's now use this formula to scale each of our variables, and re-plot our histograms. Upon running, you will get something like this. It is very hard to see here. So in the next histogram we will make this into a logarithmic scale. To change the y scale to logarithmic, we have changed log equals 1 in the code. Let's run this part of the code. You should now have a histogram that looks like this. This is the histogram of f0 and f2 plotted in a logarithmic scale. Also notice the range and the x, or the horizontal axis. The range of the scaled variables are between 0 and 1. Of course be careful, such that you don't have only one data point, in which case you will get 0 for the denominator because your max and min values will be equal. Scaling the variables makes them computationally easier to deal with, and easier to visualize. Now we are going to look at correlations between two variables, but before we do so, let's look at where we are in our questioning and model-building phase. So far, we have inspected the various variables of interest in the data. We have looked at the allowed amount, charged amount and the payment amounts. Now we will ask the correlations. In asking if the variables are correlated, we can ask if the correlation in non-linear, or if the correlation is linear. We can inspect this by looking at a scatter diagram of the variables. Let us now inspect the scattered diagram of the two variables f1 and f2. Notice that we have plotted both the variables after scaling them. Once you are done, you should have a graph that looks very much like this. The scatter plot of these two variables are very linear. What we can conclude from that, is that the paid amounts are directly proportional to the allowed amounts. Similarly we can look at the scatter plot of f0 and f1. In this case, although we have some correlation, it is not as linear as it is between f1 and f2. We want to have a quantitative measurement of the correlation between two variables. We have already observed from the scatter plot of f1 and f2, that they're linearly correlated. We can calculate what is called the Pearson's Correlation Coefficient. Let's first define this quantity. First, let's look at the quantity covariance. First we take the expectation value of x minus the mean of x and y minus the mean of y multiplied together. This quantity is the covariance between the variables x and the variable y. The Pearson's correlation coefficient, is given as the ratio between the covariance and the standard deviations of x and y multiplied together. Scipy happens to have a built in library called pearsonr, that calculates the Pearson correlation coefficient between two variables. In our case, we want to calculate the relation coefficient between f1 and f2. Let's run this code. We get the values of 0.99. This shows a direct linear correlation between the variables f1 and the variable f2. This sort of makes sense. What that means, is that the average MediCare payment amount, is directly proportional to the average MediCare allowed amounts. You can also use the core plot method from c born to quickly give you a correlation matrix of all your variables. Let's run this code. Upon running you will see a matrix like this. It calculates the correlation between two variables that can be calculated as they need to be numerical values. In this case, we have a heat map that denotes all the correlations between the different variables in our data set. This is a nice little tool, to look at all the variables in their data set, and see how they are correlated with each other. If you want to learn more details on correlation coefficients, please look in the referenced book in the instructor's notes. Calculate the Pearson's correlation coefficient row after scaling the variables f1 and f2. Does scaling change the value of row? If yes, tick here. If no, tick here. For your answer, give the new value in this box. If you answered no, you were right. Scaling does not change the value of the Pearson's correlation coefficient. The value is still 0.9989. Specifically, scaling features can make the feature better, both computationally and visually. And you won't lose the correlation between variables or features. So far we have seen that the variables f1 and f2 are linearly correlated. In this case, we decided to use f0 and f1 as the variables of interest for building our model. Now we want to form a general model that involves f0 and f1. What are our choices? We could build a parametric model we could build a non-parametric model which would both be statistical models or we could build some mathematical model. We could also try to build some. Machine learning models we using clustering and so on. In our case, we'll choose the simple non-parametric model. We will use the method of kernal density estimates to build a non-parametric model. We choose this because in a lot of statistics classes, you would have probably seen parametric. Models already. Since we are going to introduce kernel density estimates here we are going to stick to single variable approach. To do so we are going to use a derived feature. The derived feature we will use will be defined as x, which is the absolute value of the difference between f naught and f1. Divided by the value of f naught. This feature by itself is scaled to the values of f naught. Thus we expect the range of the variable x to be between zero and one. So now, we are going to look at non-parametric models formally. We defined a derived feature which is x equal to the absolute value of the difference between f0 and f1 divided by f0. What this means in practice is that the ratio of the submitted charge, f0, to the difference between the submitted charge and the payment amount by medicare. When you're making a derived feature, make sure what that represent, makes sense in your given problem. Now that we have a derived feature x, before we go on to making Kernel density estimates, let's look at a simple histogram of this feature x. Let's switch to our IPython notebooks. In te block of code here in the IPython notebook, we have defined the variable x. We are going to bin it between zero and one, with 100 bins. We are not going to normalize it so we can look at the number of entries. Let's run this piece of code. Once the code has finished running, you will see a histogram that looks like this. This is our variable of interest x. So now we are going to look at non-parametric models formally. We defined a derived feature which is x equal to the absolute value of the difference between f-nought and f 1 divided by f-nought. What this means in practice is that the ratio of the submitted charge, f-nought, to the difference between the submitted charge, and the payment amount by Medicare. When you're making a derived feature, make sure what that represents makes sense in your given problem. Next, we're going to look at a distribution of values of x. Now that we have a derived feature x, before we go on to making kernel density estimates let's look at a simple histogram, of this feature x. Let's switch to our [INAUDIBLE] in this block off code here, in the [INAUDIBLE] we have defined the variable x. We're going to bend it between 0 and 1 with 100 bins. We're not going to normalize it, so we can look at the number of entries. Let's run this piece of code. Once the code has finished running, you will see a histogram that looks like this. This is our variable of interest x. It ranges between 0 and 1. It also has two definite peaks. One at the value of zero, the other between 0.8 and 1. Somewhere around 0.89 or 9. Notice that we had two variables in our model, but we have changed that to a single variable x, which is a combination derived from the two variables here. This will let us use a single variable kernel density estimate, to build a model of x. Form the derived variable corresponding to x, for the state of California. Name it y. Once you're done click on this box. You should have found y as the absolute difference between g0 and g1 divided by g0. Now plot a histogram of X with color green, and histogram of Y with color red, both normalized. Once you are done, click on this box. You can use the iPad phone notebook to see the results. The histogram from your quiz should look like this. The range for both the histograms would be somewhere between zero and one. The variable x here is shown in green, and the variable y for California is shown in red. Notice the two variables have somewhat different shapes in different regions. You will also notice that we have 200 bins between zero and one with very jaggedy points for each bin. In the next video we are going to learn about kernel density estimates. You will see how using kernel densities. We can smooth out this histogram into a probability density function. As we mentioned before, we are now going to form a kernel density estimate from our model for x. This will formalize the concept of the histogram as a non-parametric model. And in the next few videos we are going to see how to do this. Non-parametric models are extremely useful to get from discrete data, to probability to density functions or distributions. For example, it is useful in signal processing, in online data collection to build quick models, and in several other fields. In our case, we are going to use a single variable x. If x one, x two to xn are data points. We're after a probability density function P of x. That best describes the distribution from which x one, x two, so on till x n could have been drawn from. How do we go about doing this? Lets start with some data point x. Now imagine a cubic box with sides h. And the point x inside it. We can write the volume of this box of side h, given by volume V equals h cubed. Now if we are in a D dimensional space the volume would be h raised to the power of D. Let's define a function, K of u. The value of K equals 1 when all values of u lies between minus half and half, or the absolute value of u is less than half. The value of K is 0 otherwise. This is, a D dimensional hypercube centered around zero. Now similarly, let's define the function K of x minus xn divided by h. This is equal to 1. If the point Xn is inside the box with side h, around X. And it's equal to 0, otherwise. In this case, we have made the variable u into X minus Xn over h. And we have centered the box around the point X instead of zero. Let's look through this a little bit more. Let us try to understand this expression a little bit further. Let's look at the first point, x 1. We form a box of side h around the point, x1. Now, the kernel evaluates to 1. Since the point x is inside this box. Let's look at x2. Similarly a box with side h around x2, contains the value of x. So the kernel evaluates to 1. Again, look at x3. The same case we have, a box of side h and x lies within this box. So we call the value of this function, the kernel function, as 1. In the case of x4, we have the box of side h, but the value of x lies outside this box. We call the value of the kernel to be 0. As you may have guessed through this explanation, that this function that we have defined, is called a kernel function. In statistics, we use the kernel function in general, for many different cases. In this case, we're going to use it to estimate densities. Now we have the kernel function given as K of x minus xn over h. Let's say we have total number of points, capital N, then the density estimate is given as a function of x. Here's the normalization factor given by the number of points inside the box with side h and dimension D. And we sum all the kernels from one to N for each xn. Here we can get the expectation value from the kernel density estimate. It's all right if you can't follow along the entire math here. The idea is to take the expectation value of the probability density derived from the kernel. Which is one over Ndh to the d. And the sum of the kernel and the expectation function has been taken inside the sum. Once we work this out, you can see that the expectation value is a convolution of the kernel, with the original probability density. We won't get into more details about the expectation value here. But we can discuss in the forum why this form is important. The kernel function we have defined here is known as the Parzen window kernel. This method is used in signal processing problems. You will also notice that the smoothness of this PDF depends on the value of h. We will now look at another kernel known as the Gaussian kernel. In the last lesson, we saw the uniform kernel or the Parzen window kernel. Another kernel that is used often is called the Gaussian kernel. The Gaussian kernel can be written in this form, where uT is the transpose of the vector of values of u. D being the dimension of the space. The kernel density estimate is then given by this expression here. It is very similar to our previous expression, except now we have used the Gaussian kernel, instead of the Parzen, Render, or the Uniform kernel, to make the density estimate. Remember, the kernel density, is an estimate of the shape of the distribution, using now here, the sum of Gaussians surrounding each data point. We are now going to investigate what happens if we change this parameter h. The h in this case is called the bandwidth for the kernel density estimate. Let's now switch to the IPython notebook, to see what happens when we change the bandwidth. Before we get into an investigation of how the bandwidth choice affects our kernel density estimate, lets form some kernel density estimator using our iPython notebook and some python code. Look at the block off code here. We are going to use the stats marginal from si pi. Here we are defining a function that chooses various bandwidth factors. This part of the code defines a function that gives us a kernel density estimate from our data. Let's run both these pieces of code, as they were both simply functions that didn't give us any results yet. But let's now call the function, and plot them with a bandwidth factor of 0.1. Let's see what happens, round this piece of code. Upon running the code, you will get a plot that looks something like this. It might have taken you a while to run, depending on the computer you're using, so have patience and let it finish. Here, the bandwidth is very, very small, and we see a density estimate that is quite jaggedy. We are now going to increase the bandwidth factor, by two. Let's see what happens. Again, once it finishes running, you will see a kernel density estimate. The green line is still a little bit jaggedy, but smoother then the last one. Let's keep doing this. For each of the next three lines of code, we have changed the bandwidth factor to 0.7, 0.9, and 0.99. Run all three. For a factor of 0.7, you see a more smooth density estimate. Here's the density obtained for the factor of 0.09. Here's the density estimate with the largest bandwidth selected. Now we have a much smoother kernel density estimate. So you see as we vary h from a small value to a large value. The variations in the kernel density estimate itself becomes smaller, and the smoothness of the curve increases. The jaggedness of the variation is in fact called the variance. The smoothness is part of what is called the bias. We can trade off between variance and bias by choosing a higher or lower bandwidth. We will now investigate if there is a way to find an optimal bandwidth. So up to now, we have built a univariate kernel density estimator, which gives us our model. Now we ask the question, is the model optimal? We discussed the bandwidth and how that plays a role in determining the quality of our model. To validate our model, we will need some validation techniques for estimating probability density. One way to do this is to minimize the MISE, or the AMISE. The MISE stands for the mean integrated standard error. And the AMISE stands for asymptotic limit of the mean integrated standard error. We can also minimise the KL Divergence, which is called the Kulberk-Liebler divergence. We're, we're going to look into some of these validation techniques and derive some rules of thumb of how to get an optimal bandwidth. The mean integrated standard error is the expectation values of the square of the difference between the estimated PDF and the true PDF. We will see this expression a lot in lesson four, when we're validating any model in general. This is a loss function that is used quite often, in model building. The AMISE is the asymptotic limit of the mean integrated standard error. It is often difficult to calculate this integral. And we take some asymptotic value for the integral as an approximation. Breaking the mean integrated standard error into its component, we get some irreducible variance that comes from the data set itself. The bias square determines the smoothness of our kernel density estimate for different bandwidths. The variance determines the jaggediness of the kernel density estimate for a different bandwidths. So the choosing the right bandwidth is a tradeoff between the Bias square and the variance. We will learn a lot more about these two quantities in the next lesson, where we emphasize validation of models. So we can technically use the mean integrated square error or AMISE to select the optimal bandwidth for a kernel density estimation. However, for general kernels, this is very difficult to estimate. So the first rule of thumb is given by Silverman's Rule of Thumb. This works only for one dimensional data. The optimal bandwidth is given by H, which is four times sigma raised to the power of five, divided by 3n, the entire thing to the one fifth power. In this case sigma is the standard deviation of the distribution. In this case, the kernel is also assumed to be Gaussian. A more generalized way to obtain a bandwidth is given by Scott's Rule. This shows you the formula to obtain the general bandwidth in D dimensions. For a one dimensional case, we put D equals one. In this case, the Sigma here shows the covariance matrix in the D dimensions of the data. In fact, when we determine the kernel density estimate of our data here, we used the Scott's rule to find our bandwidth. All the different factors we obtained was simply some fraction of the bandwidth obtained using Scott's rule. Notice we also used the Gaussian kernel here to get the KD. We're now going to look at The Kullback-Liebler Divergence or Information. It's a general method to compare two probability density functions. Now let's assume we have two models, a model f which is given by a pdf and a model g, which is given by another pdf. We can calculate the information of the Kullback-Liebler Divergence between the model f and g using the following formula. Here we have taken the logarithm with the natural base of the ratio of f and g. The model g, can be estimated even by some parameter theta, and it could be the model that best estimates the model f. We can use this K-L information or K-L Divergence to measure the distance or difference between two models as well. We can think of the K-L Divergence as the information lost when g is used to approximate f. Sometimes the K-L Divergence is described as a distance. However, it's not strictly a distance, since the K-L Divergence between f and g is not the same as the K-L Divergence between g and f. So it is more of a directed distance. This function recognized that it's not symmetric. In statistical information theory, the K-L distance is considered one of the most fundamental of all information measures, in the sense of being derived from minimal of assumptions. In fact, you have seen the K-L distance before as relative entropy between two measurements in lesson two. We will not go into any more details of how we use the K-L Divergence or similar metrics. This subject is vast and we invite you to look into the reference material in the instructor's notes and discuss this in our forum. We'll now move on to estimating the kernel densities, for the various bandwidth we have introduced before. Take a look at the function called getAllKDE in the IPython note book. This function is used to generate various kernel density estimates. We use the Silverman's rule as well as various factors of Scott's Rule. We plot all of them together in the same plot. Once we have run the code above, we are going to use that function with our variable x to draw all of the kernel density estimated plots. Let's run this piece of code. In general, we have used various different bandwidths to make different kernel density estimates for variable x. The actual PDF, has been put in the red dashed lines using a histogram. In each of these cases, we have used the Gaussian kernel to do our density estimation. Now with a kernel density estimate, we have a general parameter free model of our data. We can now ask questions about probability. For example, we know values close to 0 are cases when medicare paid the health care provider the same, or close to the same amount that was charged. The rising peak here, to the right, shows that claims made by most providers, the faction of the total number of providers is given by the integral under this curve are denied by Medicare. So now we have formed a single variable kernel density estimate. We have looked at various ways to compare our estimates and make sure the values we are getting are optimal. We have also validated our univariate kernel density estimation model. We introduced mean integrated square error, as well as K-L Divergences to look for differences in PDFs. Instead of just making a one variable kernel density estimate, we could use multivariate density estimates. Multivariate kernel density estimates will give us joint probability density functions from which we can draw conditional probabilities. In this case, we can use another second variable denoted as x bar which could be the difference between f0 and f2 divided by f0. This will then be the difference of the submitted charge amount and the Medicare allowed amount, divided by the submitted charge amount. Let's take a look at what the kernel density estimate for the joint distribution of x and x bar looks like. Back to your iPython notebook. We have defined a new variable called xbar which is the absolute value of the difference between f0 and f2 divided by f0. We can use a very convenient joint plot function included in to make the joint kernel density estimate of the variables x and x bar. Let's run this piece of code. This might take a while to run. The star here indicates that the computer is still working on your plot. Have patience and you will get a nice looking figure. Once that code finishes running, you will get a two-dimensional plot looking like this. On each axis we have plotted the one dimensional PDFs. The joint distribution is given as this heat map here. Looking at the joint PDF, you can see that there is a cluster around the point 00. There is also another one around 0.8. There could be another cluster somewhere around 0.6. You can ask several different questions after you have a joint KDE. Notice that the multivariate kernel density estimation was made behind the scene, and all you see is the plot of the probability density. You can use multivariate KDEs to do regressions. You can also use them to do pattern classification using Bayes' decision rules. The usefulness of KDEs is also to determine the shape of the data very quickly without having to collect a lot of data and can be used for online models and high velocity data. In the next lessons, we're going to look at some ways to determine if we have outliers in the data. We covered a little bit of kernel density estimates as part of nonparametric methods in statistics. However, the subject is really vast. We recommend the book All of Nonparametric Statistics by Larry Wasserman if you want to learn more about the subject. A link to the book is given in the instructors notes. In the next lesson, we're going to look at a simple way to see if there are outliers in our data. We have so far built multivariate non-parametric models using kernel density estimation. We can also ask the question, are there outliers in the data? The Mahalanobis distance for a single variable x, can be defined this way. Where Mu x is the mean value for x. And S is the covariance matrix. We use the inverse of S here to get this product matrix. Similarly for two identically distributed variables x and y, we take the difference transposed. And use the inverse of the covariance matrix and then the difference and multiply them together. In both cases the values are always positive definite. Since it's a true distance function. The Mahalanobis distance gives us a simple way to look for outliers in the data. The square of the Mahalanobis distance in two dimensions or with two variables is equal to the chi-square with two degrees of freedom. Thus that distance directly give us how far these values of data are from their central tendencies in the probability distributions. In our case, we have two distributions of F1 and F2. F1 was the average Medicare payment amount and F2 was the average Medicare Allowed amount. We had inspected these two distributions and they seemed to be quite identically distributed. Assuming they were identically distributed, given that F1 and F2 were quite identically distributed. We had the derived variables x and x bar identically distributed. Can we look for outliers in a joint distribution of x and x bar? Moreover if we do find outliers using the Mahalanobis distance we can still ask the question what do these outliers mean? Well, we will leave it up to you to find what the meaning of these outliers could be. Let's go back to the i Python notebook and implement the code for the Mahalanobis distance and see if we can find some outliers in our variables x and x bar. In your iPython notebook you'll find a function that says MahalanobisDist. It takes in two variable, x and y. It calculates the covariant matrix as well as the difference arrays for each of the variables from their means. It returns the Mahalanobis distance. Lets execute the scope. Once this is done running, we can use the MahalanobisDist function to calculate the distances between all the points in x and x bar. Let's run this line of code. The next function implements an outlier detector. In this case, we call the previous MahalanobisDist function between the two variables x and y. We set a threshold of -2 times log of 1-p. Here p is the probability with which I believe that a point is in the distribution of x and y together. Let's run this code. Now that we have defined the function FindOutliers, we can call it with our variables. We use x,xbar and a probability of 2.43 times 10 to the -6. This corresponds to a five sigma significance in a normal distribution. Let's see if we can find outliers with five sigma significance. The FindOutliers function returns the outliers in an array. We want to plot the outliers and see what the indices of these outliers are. Use the PlotOutliers function to do that. Once it's done running, you will see the following output. Total Outliers found : 4. The indices of the variables for the outliers are given here. The plot here shows the values where the outliers lie in the x x-bar PDF. You already formed the variable y which is the absolute value of the difference between g0 and g1 divided by g0. Now, form another variable, y bar, which is g0- g2 divided by g0. Can you find outliers using y and y bar using the Mahalanobis distance? Choose the answer that shows the correct indices for potential outliers in the data for y, y bar. For the correct answer, check on the corresponding box. We are going to now leave you with some food for thought on outlier detection. The first question we want to ask you is, is Mahalanobis distance a good method to detect outliers in the Medicare data? To answer that, let's try to ask another question. Are Outliers in bivariate data indicative of true outliers when the data has more dimensions? Think about these questions for a while. These questions are not graded quiz but just food for thought. Share your thoughts with other students in the forum and we will post a link for you in the instructor's comments. Just to give you a simple answer, the Mahalanobis distance is ideal for samples drawn from identical distributions. It is useful to use more advanced clustering techniques like k-means or others to find outliers in multidimensional data. Ideally we should inspect all variables that are uncorrelated to find true outliers in the entire dataset. However, if you want to explore more advanced techniques, discuss it in the forum and try them out on your own. We have travelled quite a bit in our QMV diagrams here. We started by asking questions as to how one should detect anomalies in data. We thought we could detect outliers in distributions. We inspected distributions and looked at correlations between them. We derived certain variables. We used two variables to form some model. The models we looked at could have been parametric, non-parametric, or mathematical. We concentrated on the non-parametric models and derive some single variable kernel density estimates. >From the single variate kernel density estimate, we look for optimal models. We used mean integrated square error to look for optimal bandwidths. We introduced the KL divergence. We also found some multivariate kernel density estimators and looked at joint probability distribution functions. For the multivariate methods, we looked at outliers in the models. We used Mahalanobis distance to find the outliers and we left it up to you to find out what these outliers may mean in our data. This completes this part of our lesson. At this point, we will leave behind this diagram. However, you can see there are many points we could have further enhanced this diagram. In fact, this is always the case with our Q and V diagrams to find the flow of logic in model building process. As we discussed in the beginning of the lesson, we wanted to introduce you to the method of building models. The kernel density estimate example covered one of the most important concepts behind models. That is to understand the probability distributions corresponding to processes and phenomena. We did not go through parametric or machine learning models in this lesson, hoping you have learned how to build a regression or mathematical model if you need to by asking the right questions and selecting your features well to answer these questions. The next part of this process is to validate and verify the results that we get from our models. We got a little taste of that in this and the previous lessons. In the next lesson, we will build classification models. Through that example, you will learn about testing and validating your models, and ways to select between models based on performance. In examples in previous lessons we have introduced to you various ways of building models. We investigated applicability and merits of the various models and went through some of the steps of questioning and validation. Iterating between these steps towards building the models. In this lesson, we will take a deeper dive into the model validation process. Before getting into this, let us think of a situation where we want to teach a six year old how to multiply two numbers. To do so, we show her how to multiply by using many pairs of numbers. Let's say we use 100 pairs of numbers. After a while we are convinced that she can multiply two numbers and want to test her skill. Now if we test her multiplication skills on the same numbers that we gave her to learn multiplication with there is always the possibility that she did not figure out the underlying algorithms or patterns for multiplying, but has simply memorized all the values. A good test will be to ask her to multiply numbers she has not seen before. We want to test how well she learned by giving her data which she has never seen before. A similar approach is required in testing models. How well the model learns needs to be verified on data that the model has not seen before. It is like giving the model an exam or a test. And the test scores give us a way of estimating how well the model will fare. Much like we test children's ability to use the knowledge they learn in school through tests and use the test scores to evaluate and rank their abilities. So now that we know that we will need some data to test our models we can either go and collect more data to test them. Or simply set aside a part of the data that we have already collected for the purpose of testing. Most of us know the story of the Shepard boy that cried wolf. The moral is that, of building trust. Model building is no different. For any model to be useful, it needs to be verifiable. In fact, we have already looked through several examples, where we have iterated through the questioning, modeling, and verification steps, to build robust models. In this lesson, we will take a closer look at what it means to verify a model, and how you, the model builder, can go about doing so. We will go through several examples, of how to make a classification model and validate it. We will touch upon, how we will obtain the best performance from a model and define, what are the measures of performance. We will also introduce to you, the quantitative tools to compare different models and how to choose between them. To illustrate this, we will use location data collected by AT&T's connected cars and try to identify these cars based on the driving patterns over time. So, let's get started Before we can identify the cars, and the connected car's data. Let's start with a question. What is the data collected by the connected car? To answer this question, we will take a look at some aspects of the data. As in the previous examples, we will be using the iPython Notebook. >From the instructor's notes, please find the download links to the iPython Notebook and the data set that we will use in this lesson and make sure you have a file in it named connected cars.cvs. Launch the iPython Notebook, and click on Lesson 4 Handout. You should see something like this on your web browser screen. Let's look at the first block of code. Here, the first thing we do is import the pandas library. And then we use the Pandas library to read the CSV file, connected_cars.csv. For the entirety of this lesson, to run the piece of code in that block, we will use the Play button here. You can also use the Enter and Shift button together to do the same. Let's run this piece of code. Now the data has already been loaded in a pandas data frame. The data frame is called events. We will use the describe method for the data frame to see what it loaded. Here, you will see a summary of the data in a table. This summary shows that we have 128,561 events or entries. You can take a look at the actual data by using the head method on the events. The head method shows you what is inside the data. In this case we have 5 columns. In the first column we have. A driver ID. The second column gives us some values of x. The third some values of y. The fourth are dates on which these values were taken. And the fifth, the time at which these values were taken. We only see here the top 5 events. Similarly, we can try to use the tail method on events. Let's see what happens. We see the same. Sort of data except now the driver IDs are G. You can see these are the last 5 rows of the data set. We will quickly summarize the data. We saw that the data has IDs A to G. It has some x and y location, or coordinates. It also has a date and time. Here, we have a scattered diagram to show you what the data actually contains. These are coordinates of people driving around in a connected car. You see there are many different features we could have selected to build this model. For example, we could have looked at locations at different times. For example, at 2 a.m., or 10 p.m., where a person is more like likely to be at their home. We could also have used the (X, Y) co-ordinates instead of the distances. Or we could have used a different distance metric. Let's explore this a little bit further. We could have taken the distance between point one and point two as the absolute value of the differences between the X and Y co-ordinates of the two points. Think about this distance function for a second. Within cities, only crows and pigeons and mostly birds can move in a straight line from point A to point B. The rest of us, non-flying people, have to cover this particular distance. This distance is called the Manhattan distance or the taxicab distance. What features would you have chosen to solve this problem? Pick a couple, then use some of the techniques you have learned in lesson two to analyze them. For example, you could calculate the information gain both for the Euclidean distance we chose earlier and the Manhattan distance denoted here. Check the box when you have done this. This quiz is not graded. Thank you for your response. Please discuss your ideas about using different metrics on the forums. Before we move on to the modeling section, I would like to give you some food for thought. As you have seen, distance metrics are very useful. In our case, we have used the distance metric quite literally to measure distance between two points on the surface of the earth. However, when we measure difference between two PDFs or functions, we can use distances as well. In the following sections, we are going to introduce what are called the risk and loss functions. You will see that the lost function is essentially a distance measurement. As we move on to build and verify the model, feel free to use features you chose in the quiz instead or in addition to our features that we have used so far in the analysis. And see how the model performs. Now that we have looked at the data can we ask some more questions? So the first question we want to ask is can we identify people based on their location patterns? We have discussed in the previous video that we could use some distance functions measured at different times to determine a person's location pattern. To answer this question, we can start using simply x and y coordinates for their location. Or use something like a taxicab distance or a Euclidean distance. We have already discussed the formulas for the taxi cab and the Euclidean distances. As you see, we will continue using the flow diagrams to map our progress through the iterative model building process. After looking at the data we can quickly draw the following conclusions about it. First, the data is labeled. Secondly, the labels are categories and not numerical. So we can cast this as a classification problem. Of course, we can use location and distance over time to make this classification. The next thing we need to decide is whether we want to build a model that is linear or not. We can use some nonlinear methods. For example, kernels or artificial neural networks. However, for this problem we will not go down this route. We will go down the linear route first. Before diving more deeply into the modeling and verification process of our connected cars data, let us take a detour and think about the various models that we have talked about in the previous lessons. In models in natural sciences, many simplifying assumptions are made. About the system under investigation, so that it is possible to arrive at some mathematical expression of the phenomenon. This gives us a way of understanding the phenomenon and modeling the observed data in light of this fundamental understanding. Complexity is often introduced as additional layers in terms of perturbations or dissipative effects. We will start looking at how to split the data before we do so. Let's investigate what are the characteristics of models. We just talked about mathematical models, such as what are used in the natural sciences, economics, medicine, etc. Part of what we do in natural sciences or mathematical models, involves point estimation. That is to calculate the value of certain quantity very, very well. Sometimes, we might have a multi parameter model, and we want to make a good fit to the parameters. Often, the generalization of the model is inherently built into the model. The immediate goal of such a model is to fit the data as best as possible, with the underlying assumption that the mathematical structure of the model, in itself, helps to make the model generally applicable. On the other hand we have data driven models. These are models used in statistical and machine learning techniques. And they have very wide applicability in sciences engineering computer science etc. In this case we train on data from a very complex phenomenon. And the generalization to new data from similar phenomenon needs to be explicit, understanding. So you see, the simplifying assumptions in your model for the data driven case aids in computation, whereas, the simplified assumptions in a mathematical model aids in understanding of the basic phenomenon. The goal in the data driven models is to learn the underlying patterns of the phenomenon directly from the data using appropriate algorithms. These algorithms are then tuned so that they are useful in general toward solving similar problems. In most cases we want to make sure that the model is not over trained on the data and does not overfit the training data but also generalize the new data In this lesson, we'll learn how to split the data. First, we have the training data, this is the data that we use to train a model. Secondly, we have validation data, this data is used to select a model. Third, we have the test data, this is the data we use to report the performance of a model. Now, training is error is used to tune the performance of the model on the training set. A validation error is used to select the model that works best among many models. We then use the test error to calculate the performance of the model that we have already selected. In the next few videos, we will introduce each of these errors and how we use them to make the decision to eventually use a specific model to solve our problem, and to identify cars in the connected cars data set. There are no general rules how best one should split the data for each of the three purposes. Depending on the size or the volume of the data set, one would decide to split in proportion of 50 for training, 25 to validate and 25 to test. Often, one might need more for training or validation. There are also constraints from the model itself and the complexity at which we build the model. We will talk about model complexity shortly. It is important to note that the test data needs to be set aside until a particular validated model has been selected. This gives us an unbiased estimate of the performance of the model. More specifically, when we are building the model, we do not examine the test data at all. This is to avoid biasing our results on the test set. This is an essential part of the validation process. Now that we have already seen how to split the data, we will do some data cleaning. Remember we have already decided that the location can be represented as the distance from a point of reference. We will go into depth a little bit later. As to which distance function among these three might work best. We will also need to decide on the times when we calculate these distances. The distances at each time is a feature in our model. We will also need to decide at what time intervals we choose these distances, so that we can have adequate number of features. We have now switched to the iPython Notebook again. We're going to run the code to clean the data. The code in block five takes the date and time, combines it into one timestamp quantity, and makes the value reflective of US specific time. It also makes sure that if some values of date or time is absent or is equals, equals none those data points are not used in the analysis. So let us run the code again by pressing the play buttons. After you have finished running through lines six, seven, and eight you will see an output. The output has 128,520 entries. This is down from 128,561 entries we started with. The reduced number of events is a result of cleaning the data. We are now at the point to choose the features we want to include in our model, to identify the cars by their driving pattern. Remember we have the choice of selecting different distances. We also cleaned the data. Now we are going to select features using distances sampled over time. As we know from previous examples, this can be broken down again into several iterations investigating what works best. We will go through only a few steps to ensure we are selecting features that can give us reasonable results. Let's now head back to our iPython Notebook again. And, see how we can identify our features. Take a look at the code in this block. Here, the get_sampling_intervals method takes the intervals of time at which we are collecting the data. We also divide the seconds by 60 to get our answers in minutes. The method returns an array named sampling intervals. Let's run this part of the code. This part does not give you any output, since this merely defines the method. In this part of the code, we are simply going to see how many different unique driver ID's there are in our data. Let's run this part of the code again. You see we get an output of seven. We have seven different unique drivers driving around in a given patterns. Our goal is to look at these patterns and see if we can distinguish between each driver. Before we do so. We want to make sure we have enough data for the sampling intervals that we have in the data. In the next line of code we make an array for all the sampling intervals for all the people. Next we want to make a scatter-plot of all the sampling intervals. Let's first make the plot and then we will see what the plot is showing us. Again, use the play button to run that piece of code. If everything goes well. You would get a plot that looks like this. Let's take some time to investigate what this plot is showing us. In the x axis we actually have the index of all the data points in our array. In the y axis we are plotting the inter-sampling time now calculated in minutes. So what this means is there are some data points that have inter-sampling times that are quite high in time. In this case 750 minutes. In this case around 800 minutes. However, notice most of the points have very short intersampling times. In fact if we take about 60 minutes as our intersampling time we will capture most of the data. When building your features it is important to make sure that after selecting the features, you have enough data to do your analysis. So, by looking at the data, we concluded that a 60 minute or one hour inter-sampling time will leave us with enough data. This is an important point. Whenever we have a time series data. It is important to know at what intervals we are making these measurements. Now we come to another question. How many points or features should we select? We have actually 24 choices. If we take one hour inter-sampling times. The 24 choices being location at the first hour of the day. Second hour of the day. Third hour of the day. And so on and so forth. Up to the 24th hour of the day. Let's say we select four features. That means we will collect the location of a person at four different hours of the day, and use each of these locations as a feature. Now, another question rises. Will we have enough data for the chosen features? We return to our IPython Notebook to answer the question if we will have enough data for the chosen features. In this block of code, we draw some histograms. We have already seen what histograms represent. Let's first run the code, and then we will analyze the plots. Once it finishes running, you will see a histogram that looks like this. In this histogram, we're plotting the inter-sampling time in minutes in the X axis. The Y axis shows the frequency for each of these inter-sampling times. The bins here are bins of 60 minutes. You see the value between 0 and 60 has the maximum number of data points. If you scroll down, you will see the same histograms now drawn for each of the persons. In this case, this is the histogram for person A, and we see that most of person A's samples are also between 0 and 60 minutes. You can keep scrolling down and examine the histograms for each of these persons. In this case, this is person B, person C, person D, and so on. We ask the question, if we have enough data for the inter-sampling time of 60 minutes? The answer, in this case, is yes. So we asked a question and then validated it by showing that there's adequate frequency of data. Then we come to the next question. Is there enough data to distinguish pattern? This is a very important question because we want features with the most discriminating power. What that means is that the features that we use should be able to separate each individual in the best way possible. We have already seen in lesson two how we can select features that give us the most discriminating power. Here, we are going to do a simple visual inspection of the data patterns and not dive into any of the methods previously used to decorrelate features. However, we must verify that the selected traces that we are using are distinct. Before visually inspecting which selected features have the most discriminating power, what kind of transformations do you think will improve the discriminating power of the selected features? 1. Principal component decomposition. 2. Using a bootstrap method to generate more points. 3. Scaling the features. 4. Nothing can improve on what we already have. Select all the answers you think are correct. The correct answers are number one and number three. Using a principle component analysis, we could probably find the features that are more important that others. We could also scale the features such that the numerical values of the variations they have are similar. And thus, it is easier for the model to make a distinction. The question we're asking now is, can we distinguish the patterns in the data? First of all, do we have enough data to distinguish the patterns? And are the features good enough to see a pattern? To do so, we will now visually inspect the patterns in the data. In fact, human beings are incredibly good at distinguishing patterns. In problems where we are trying to recognize patterns, it is often useful to visually inspect the data. Often it is not possible to look at their entire data set. In that case, take a subset of the data and take a look at the scatter plots. When the data is multi-dimensional, this becomes difficult to do. However, sometimes it is useful to take a look at the data two dimensions at a time. Let's go to our iPython Notebook, and make some scatter plots. We had previously plotted a histogram of the inter-sampling times. Now, let's make a scatter plot of the x and y coordinates of all the data. Depending on your computer, this might take a little while. However, at the end you would get a graph that looks like this. This is a simple scatter plot of x and y for all the data. This image, however, does not give us very much information about an individual's driving patterns. Moreover, you will see there are regions where the data is clustered around. What we will do here is look at the region between minus 1 and plus 1. Most of the data seems to be concentrated around in this region. In the next part of the code we do exactly that. After we have the restricted regions, we want to make a scatter plot with the restrictions in place. Let's go ahead and run this part of the code. Again, depending on your computer, it might take a while to produce this plot. However, now you see that we are in a much different scale then we were before. We can see a lot more points distributed between minus 0.5 and 1. Next, we want to plot the location of each person on a separate plot. We want to see if the patterns of their locations are distinct. To do so, let's run this line of the code. Running the above line should give you graphs that look like this. We can try and scroll through the graph to see what each individual person's location patterns looked like. In this case, we're looking at the location pattern of person A. We can try to scroll down to look at the location pattern of the other people in the data. This is the location pattern of person B, person C, D, E, F and G. Notice for person E, the location patterns are clustered around two very disjoint points. In this case, we're certain the scaling will work to reveal to us, a much better pattern in this region. In this part of the code, we're going to take the location of each person within their restricted region. We have run the code, and we have the following scatter diagrams. Let's try to scroll down and look if the patterns for E are better. You see, by restricting the scale of the plot within a certain region, a pattern for the location of person E has emerged. We now have distinct location patterns we can use to build our model. Let's do a quick recap of what we have done so far. We started with the question, can we identify people based on their location patterns? Then we decided to use linear methods to do classification. We wanted to use a Euclidean distance measured over different times of the day as out features. We cleaned the data. We selected the features. And we made sure we have enough data to build our model. Let's come back to the question of using the Euclidean distance. First, let's define what this distance is. The Euclidean distance is the distance we can measure between two points. Let's say these two points are point 1 given by the coordinates x1 and y1. And a point 2, given by the coordinates x2 and y2. Our goal is to measure the distance of the line joining point 1 and point 2. This distance between point 1 and point 2 can be written as the square root of the sum of the squares of the difference of the coordinates. So in this case, it's x1 minus x2 squared plus y1 minus y2 squared. In the data, we started with x and y coordinates. >From the x and y coordinates, we made the transformation to calculate a distance between the x and y coordinates for each position at a given time and some arbitrary origin point, which we denote here as 0,0. Notice we took two variables, x and y, and transformed it into 1 variable which was the distance function. We did not necessarily have to do this. We could have used x and y independently as features and still done the analysis. On the other hand we could have also used a different distance function. In the next video we will investigate using a different distance function. But before we do so, let's get back to our IPython notebooks, calculate these distances to be used as our features in our model. Getting back to the IPython notebook. Here we have the code that calculates the Euclidean distance between the two points. We do this for all the points in our dataset. Running this part of the code sets the distance metric to be our Euclidean distance. This is what we will use for the classification problem. In the next part of the code we'll calculate each person's distance from the origin at 4, 5, 6, and 7 p.m. every day. These are the four different hours we have chosen to get our locations. The people, or the driver ID, will be our classes y. We will use the values of x to learn values of y. In the function here, we're filling out two areas of x and y. In x, we append the distances. And in y, we append the person_id's. The method, in the end, returns us to areas of x and y. After running the above code we now want to check what the data holds for each of the persons. Running this part of the code should result in a list of the persons and the number of events we are left with for each of these persons. So after running this we now have a list of the name of the person with number of events. The number of data points or events have reduced significantly because we have chosen to use only four different times of day when we collect the location information. We do not use the data from the other times. We are ready to try to build various models. But before we get into the next steps, let's learn what it means to evaluate the model. As you must have noticed, we are constantly iterating to and fro from the validation phase and the questioning phase and the modeling phase. So what it is that we use to validate these models? We'll take a short digression from our example that we have been following to introduce some key concepts involved with the model validation techniques. We won't end up using all these techniques in our specific example, but this material is very important to clearly understand the concepts behind model validation. When we build models and train them, we need some way to measure how far out model is from the actual values. The loss function is just such a concept. It measures how far an estimated value of a quantity is from the true value. Let's take an example to see what the loss function is. Let's take the special case of parametric model. Let's say, the parameter is theta, which is the true value of a quantity. And then theta hat is an estimated value for the same quantity. Remember, theta hat is a function of the data that we collected. The idea is that we are fitting a parametric model to the data. And we always need a measure of how well we perform. We accomplish this by defining a loss function. It is up to you to choose how you define the loss function. The choice depends on the specific problem you're trying to solve. Here are some examples of loss functions you may encounter. I'm using the notation of theta and theta hat for a parametric model. One example is the squared error loss. In this case, you take the difference between theta and theta hat and square them. Another is the absolute error loss. In this case, you take the difference between theta and theta hat and the absolute value of that difference. An Lp Loss is essentially the Absolute Error Loss raised to the power p. Another very interesting quantity is the Kullback-Leibler Loss. It's a complicated formula and it's an information theoretic loss calculated for two different distributions. You will often encounter squared error loss in statistical learning. The square error loss is sensitive to outliers. For values of theta hat that are far from the values of theta, it contributes a large quantity to L. Also, notice this is always a positive quantity given it's a square of difference. The Absolute Error Loss is not so sensitive to outliers. However, it is not smooth at the value where theta hat is exactly equal to theta and thus, it's difficult to differentiate it at that point. We mentioned that the loss function depends on the selected data. Remember, theta-hat is a function of the data. We want a measurement that is averaged over many possible datasets. The risk is such a function. It is an average measure of the loss. And we calculate this by taking the expected value of the loss function. So as you see here, the risk is denoted by R which is the expected value of the loss function and it's calculated under the integral. Now, we are going to look at the mean square error and the bias variants trade off. Remember, we had the risk function given as this expression. The integral expression here denotes the expectation value of the loss function. Now, let's define y as the true value and f hat of x knot as the estimated value evaluated at x knot for this value of y which is the true value. We can now calculate the expectation value of the square error loss. The expectation value of the square error loss. Is given as the expectation value of this expression in here, which is the true value minus the estimated value at x not squared, evaluated at x not. We can now work out this expression. And you will get the following expression. This term here is the irreducible error. This term here is the square of the bias. And this term here is the variance. Now let us inspect each of these terms separately. Remember the irreducible error is this first term. It comes from the variance around the true mean inherent in the data. It is very difficult to get rid off the irreducible error, hence the name, through the modeling process. We have also seen how bias and variance, plays into how one can select a model. For non parametric models, like histograms and kernel density estimators. In lesson three, we discussed the role the bin size or bandwidth plays in determining the variance and bias of the model. Our goal is to minimize the square error loss. This, in turn, means that we are minimizing the bias term of the variance term. The problem is, minimizing both is not always possible due to the complexity of the model. We will look at this problem, thus we have a situation where we have to have a trade-off, between minimizing the bias square, or the variance turn. This is what is refered to as the bias variance trade-off. The goal in statistical and machine learning models is to minimize the loss function, and in turn, the risk. You may recall an introduction to machine learning class, how the cost function was defined and minimized to converge to a model. Notice it is simply the use of the least square loss function. Although we often choose the square loss in statistical learning, you must understand that you are in complete control of the way you choose your model through the choice of the loss function. Our decision is also based on how well we want the model to fit the data measured by the variance, versus how well we want the model to generalize to new data, which is measured by the bias. Remember, there's always the bias variance straight off. The cost function can be calculated. On the training data set, the validation data set, or the test data set. Minimizing each of these cost functions serve to optimize the model for different purposes. Let's take a look at the cost function calculated on the training set versus this test set. The following example is taken from a book named The Elements of Statistical Learning. Please see the instructor's notes for more details on the book. Let's take a look at the following diagram. What we have on the X-axis here, is model complexity. Think of it as the number of variables in your model. On the Y axis we have the prediction error. The red lines here, represent the error, calculated on the test data set. The blue lines here, represent the prediction errors calculated on the training data set. Notice how the error on the training data set reduces as the complexity of the model goes up. However, this is not the case for the prediction error calculated on the test data set. The solid curves here represent the expected values of all the prediction error curves. The model complexity can be thought of as the number of features in the model being used. Intuitively you may already have an idea. Of chow the number of feature can affect the way your generalization or prediction error can be affected. A less complex model with a low number of features will have low variance and high bias and we might end up under fitting the model. In contrast, a more complex model will have high number of features, resulting in high variance, a low bias. And thus, will be overfit. Remember, always, the tradeoff between the bias and the variance. In the next video, we will try to quantify the complexity of a model. And look at bounds on the training and testers that we can calculate on the samples. Now we will look at the Vapnik Chervonenkis dimension. This gives us a way to measure the complexity of the model. Remember the complexity of the model determines the performance of the cost on both the training and the test sets. There are various techniques to determine bounds on the generalization error, on the test set. A good treatment of this is given in the reference in the instructor's notes. Let us now look at the following expression. This shows that the error calculated on the test set has an upper bound. We can quantify a degree of belief in this upper bound using a parameter eta. Such that given eta for example 0.01 we have a probability of 91% that the test error is bounded above by the expression inside the radical. Also notice the expression inside the radical depends on the quantity h. This quantity h quantifies the complexity of the model. Let's look at the following diagram. In this diagram, we're increasing the complexity of the model as we go along the horizontal axis. The cost calculated on each of the data set is on the vertical axis. Now if you look at the training term, which is the training error here, you will have curve like this. Looking at the term under the radical sign, here, is the complexity term that increases like that. So you see, the test error is upper bounded by this red curve. At some point, we can select the minimum of the test error, which is something over here. If we fix the sample size N, and the training set error is given, then we see that the expression here, depends on the quantity h which we have already introduced as the VC dimension. There are various ways one can calculate the VC dimension and the topic is covered in detail in the references and the instructor's notes. For general models, calculating the VC dimension is a challenging problem. However, for linear classifier models with m dimensions or variables, or features, we have a very simple expressions. The VC dimension is given as h which is simply m plus 1. Let's take a short digression and discuss why we're building a model. In fact, this is a very important question because the answer to this will determine how we validate our models. We have discussed various models that we've built and the ways we can quantify the errors in terms of the loss function for each model. We've also encountered various ways to calculate the loss function of each model. We have seen that the choice of the loss of the risk function is dependent on how we formulate the problem. In fact, we're free to choose our loss or risk function and a choice of a loss or risk function can result in a well performing model. Now this choice is very dependent on the problem formulation. The problem formulation itself is in turn dependent on this question of the purpose of the model itself. In natural sciences, we often build very well behaved, sometimes linear and first order in time, models with simplifying assumptions of the parameters. In this case a good fit to the model parameters, often their point values, is desired and we find the best fit of the parameters to the data. This is often referred to in fields like economics and many branches of engineering as the Model Identification Problem. In statistical learning, we accomplish more than just identification. We choose systems that are quite complex and often models with many non-linearly correlated variables to describe them. In this case, we want to recognize the patterns in the data, to not only solve the identification problem, but also to accurately predict such patterns for all possible future data where the patterns are similar but not necessarily the same. Validating a statistical or machine learning model does not only requires us to validate the model on the data we have, but also to hedge the model's prediction abilities with probabilistic bounds In this video, we are going to learn how to select between different models. Before we do that, let's do a quick recap from where we left off. With selected features using distances sampled over time, we investigated how many features we should select. We made sure we had enough data for the chosen features. We used the frequency of the data, to validate that we have enough data. We also wanted to make sure the data is good enough to distinguish patterns. We did this by visually inspecting the scatter plots and making sure that our chosen features will have a discriminating power. We have discussed how best to choose our features and how that can affect the final validity of our models, through analysis of errors and model complexity. Once we have a good idea of the features set and a clear definition of the problem statement, we can build many different types of model to identify different cars from our connected cars dataset. We are now ready to make the statement of the model. We want to use classification model to distinguish between B and F. We can do so by using several different classifiers. In this case, we have chosen three. A LOGISTIC REGRESSION, a SUPPORT VECTOR MACHINE based classifier, and a RANDOM FOREST based classifiers. Each of these learning algorithms are a topic unto itself. And we will not attempt to cover the basics of these, assuming you have used or seen most of these topics in the machine learning course. If you would like to have a refresher, please look at the material in the instructor's notes. After we build the three models, we will look at ways to measure the performance of each model and then be able to make a choice between models. In the previous video, we chose three different classifiers. Here's a quiz. Why did we use three different models to solve the same problem rather than just one? There isn't really a right or wrong answer to this question, but enter what you think the answer is in the box below Thank you for your answer. As we mentioned, there isn't a right or wrong answer to this last question. So what's the solution? We used three different classifiers because different models perform well for different problems. So we can experiment with multiple models and then compare them using inter-model validation techniques. We are now entering the modeling phase. Let's switch back to the iPython Notebook. To recall briefly we had chosen person 1 as B and person 2 as F. And we are going to classify them according to their location patterns. We did a final sanity check to ensure that the features chosen actually seem to have discriminating power. At this point let's run this piece of code. What it does is, it fills an array named classification_events with the events of person B and person F. We are now going to split the data in a training set and a test set. In this case we are going to use 60% of the data on training, and 40% on test. Let's go ahead and do that first. At this point, we have split the data into training and test sets. We are using a large portion of the data for testing, as we will be illustrating several validation matrix on the test data. Just as a sanity check. We want to plot our features within our training data. Once you have run the above code, you should get histograms for the distribution of different distances at a certain time for each person. Remember we wanted to make three different classifiers. To do so we have now split our data set. Into the training and the test set. We also asked a quick question. Are the features different enough in the training data set? We validated that again by looking at the histogram of the different features. So now we are ready to run the three classification models. Let's get back to the iPython notebook again. In this part of the code, we are setting up the three models for our classification problems. We are using the sklearns logistic regression support vector classifier and the random forest classifier. This method here simply returns the model that we want to train. So now let's run this code. After you're done with defining the three models, let's simply build the models. It's as simple as this one line of code. Let's run it. And there you have it. We have the models built. In the following videos we will use some of Sklearn's built in model alidation metrics to give us the performance of these models. Before that, let us define some of the quantities we might want to look at,when evaluating the performance of classifiers Given that we have chosen a Training and a test set, it is important to think of how the classifiers perform for a given size of the Training set. Learning curves are a plot of the model performance, usually denoted by the risk, the cost or the score versus the size of the Training set and the test set. In the case of classifiers we can always use the score or one minus score versus the Training Set and the Test Set size to draw our learning curves. The learning curves are useful in a further iteration back to determining how much data we might need to optimally train the model. Now let's go to I pod phone notebook and make some learning curve plots. Up to now, we have trained our models using the X_train and Y_train data sets. Now let us look at how to draw learning curves. In this piece of the code, which is taken from the Scikit learns libraries, we are going to draw some learning curves for the three models that we have built. Let's go ahead and run this piece of code to see what happens. The piece of code here defines how we are going to plot the learning curve. Notice if you use the side kit learn libraries directly, you will be plotting the score versus the Training examples. In our case, we are plotting one minus score. This is so that the graph resembles the same trend as that of an error that we plotted before. Let us run this piece of code first. As you see, this simply defined what we want to draw. Using this piece of code we are going to now actually draw the learning curves. Hopefully you'll have three plots that look similar to this one. What we have here is that on the X axis we have plotted the Training examples and on the Y axis we have plotted the cost which we have defined to be one minus the score. This is the learning curve for the logistic regression. The blue line shows you one minus the Training score, and the red line shows you one minus the test score. Scrolling down, you can see the learning curves for the other models that we have built. The second curve here. Is the second model in our list, the random forest classifier. Notice that the Training and the test scores don't change very much after the 25th Training example. Here, we can see that using 25 examples we can do just as well as using larger number of examples. This is the advantage of drawing a learning curve. You can see the number of Training examples after which the score does not improve very much. Now that we have looked at learning curves, let's look at another performance metric, the accuracy. In case of a classifier, the accuracy is the fraction of predictions that are correct. By correct, we mean the negative instances are classified as negative, and the positive instances are classified as positive. The loss function, in this case, is a 0, 1 loss. The risk is given by the misclassification error. Another way to look at this, is simply to count the number of times our model correctly classified our data. So, the accuracy of a classifier can be given as the sum of the true positives and the true negatives, divided by the total number of occurrences. We're now going to look at the confusion matrix. It is important to know that selecting a model is dependent on the problem being considered and the application of the model. Often for practical problems, we're interested in measures other than just accuracy. Let us consider a kind of contrived example. We have many bottles of wine, where some are poisoned. In this case, it is important that we always catch the bottles of wine that are poisoned. So labeling a good bottle of wine as poisoned, which is a false negative, implies some wasted wine. However, labeling a poisoned bottle as good, which is a false negative, can imply a far dangerous mistake. So, in this case, we can sacrifice accuracy for some other measure where we give a lot more value at minimizing the false negatives versus the false positives. Here is a quick quiz. Give an example of a classification problem where the cost of a false positive outweighs the cost of a false negative. When you have thought of the example, click on the box here. To continue, we are in fact interested in four different quantities. One is a true positive. A true negative. A false positive. And a false negative. What this picture shows us here is actually what is called the confusion matrix. This is called a confusion matrix because it makes it easy to tell whether the classifier is confusing the positives with the negatives. So here, the actual cases of negatives are on the top, or the x direction, and the predicted cases by the classifiers are in this side here. So true positive is an actual positive that has been predicted to be positive. Similarly, a true negative is an actual negative that has been predicted to be negative. Now, you can also have false positives. These are also known as type I errors. In this case, an actual negative is classified or predicted to be positive. Similarly, you can also have false negatives. They're also known as type II errors. In this case, an actual positive is predicted to be negative. In the next video, we are going to define a few performance metrics based on these concepts we just defined. We will now look at two other metrics, known as sensitivity and specificity. Let's consider this diagram. In this case, the circle represents actual positives, which is in fact a sum of the true positives and the false negatives. The sensitivity is defined as the true positives, divided by true positives plus the false negatives, which are all the actual positives. Similarly, let's look at all the actual negatives. In this case, it is a sum of true negatives and the false positives. The metric specificity is defined as the true negatives divided by the sum of the true negatives and the false positives. In our example, we were classifying person f and person b. In this case we would denote f as a positive case and b as a negative case for f. Let us denote the true positives, which is the number of actual positives that were classified or predictive to be positives, with TP. Let's denote the false negatives, which are actual positives classified as negative, with FN. Let's denote true negatives, which are the number of actual negatives that are classified to be negative, with TN. And false positives, which are actually negatives classified as positive, with FP. Also remember, in the language of hypothesis testing where events are classified as belonging to a null hypothesis or an alternate hypothesis, false positives are called type I errors, and false negatives are called type II errors. The accuracy is given as the sum of the true positive, and the true negative divided by all the cases. All the cases are sum of everything listed here. With this notation, we can write the sensitivity and specificity like this. Sometimes, you will see the specificity written as 1 minus the false positive over the sum of true negative, and the false positive. This essentially gives you the expression above. So now you're familiar with the accuracy metric, the sensitivity metric, and the specificity metric. For our poisoned wine classifier, we would want our sensitivity to be very high. That is, we want to catch almost all instances of poisoning. For the example that you showed in the quiz, this metric is less important. For our poisoned wine classifier, the specificity metric is less important, since if we falsely label a harmless wine as poisoned, it's not as dangerous. For the example that you gave, we want the specificity to be very high. We will return to the concept of sensitivity and specificity and show you how to choose between classifiers based on these metrics. We have looked at several different metrics. We've looked at the accuracy, the sensitivity and specificity. And in general looked at a confusion matrix. Let's swing back to our classification models and calculate their accuracy. Let's run the piece of code here. Upon running this code, you will see accuracy for logistic regression, for random forest classifier, and a sampled vector classifier printed. In this case, we have used the score method of the model object to calculate these accuracy values. Let's look at the next block of code. This block of code prints out the confusion matrix for you. Let's run this block of code. Upon running this block of code, you will see the following confusion matrix. This code should give you a good idea of all the metrics we have talked about so far, and how to calculate them yourself. Take some time to go through this block of code yourself. Scikit-learn also comes with a method to print the confusion matrix. In this block of code, we'll use the scikit-learn's confusion matrix printer to see if we get the same values. Upon running, we see the following output. We leave it up to you to verify that the numbers are the same as what we obtained through our own calculations. Refer to the result of the confusion matrix you just obtained. Now calculate the following for the logistic regression. The sensitivity and the specificity. Enter your answers to two decimal places in these boxes. Remember, sensitivity is the number of true positives divided by all actual positives, and specificity is the number of true negatives divided by all actual negatives. It is okay if you look back at your notes for the formula for sensitivity and specificity. Let us now look at the solution. The sensitivity is given by the true positives divided by the true positives plus false negative. In my case, the answer is 53 divided by 53 plus 1, which is 0.98. The specificity is true negatives divided by true negative plus false positive. 1 over 1 plus 33, which gives me 0.03. Now notice you might have gotten a little bit different answer for the sensitivity or even the specificity. I would like you to think why your answer may be slightly different from the answer here in the lesson. We can discuss this further in our forum. Lets revisit specificity and sensitivity, and see how it is calculated. In the language of hypothesis testing we can have the classification problem expressed as a hypothesis I or class 1, and a hypothesis II as class 2. Often one of the hypothesis is called the null hypothesis and the other the alternate hypothesis. In our example, we can say hypothesis I as the person is B, and hypothesis II as the person is F. This is the case of a binary classifier. Now recall, we had two kinds of errors defined. The type 1 error and the type 2 error. The type 2 error is the one minus the the specificity. The model inherently classifies between the two classes or hypothesis. By making a cut or a threshold. In this case, let's put the threshold as 1.5. With this cut, we can calculate the area under the probability distribution of both the hypothesis. If the Type 1 error for given cut or test disket. For a given hypothesis, is less than a pre-determined value called significance, which is expressed as a probability of the classifier, we fail to reject that class. We also claim that as failing to reject the null hypothesis. We also define the power of the test as 1 minus the sensitivity of the type 2 error. Now notice the sensitivity and the specificity is dependent on the threshold or the cut determined here. If we move this around, the values for sensitivity and specificity. With change. You can guess now, that we can draw a curve that plots the specificity versus sensitivity for different values of this cut. Such a curve is called a receiver operator characteristic curve. In the following video, we will look at an ROC curve for our classifiers. Now we're going to look at two main concepts in ROC analysis. The first one is the Iso Performance Lines. The idea here is very simple. If you draw a line through ROC space, then the points in the ROC Cuves that intersect with the straight line have the same performance and this performance is measured by the slope of this line. The gray line that I drew, intersected the ROC curves in one, two, three, four, five, points. All these points have the same performance and that's the fall on iso performance line. So you see these lines can be determined by the objective of optimization, such as the lines that can give maximum accuracy or specificity or sensitivity. Another important concept is the concept of the area under the curve. An ROC curve with a larger area under it performs better on average than and ROC curves that has a smaller area. In our case here you see the green line has a lesser area than the red line under it. Thus, the SVC performs better than the logistic regression. Since a greater AUC, or area under the curve, implies better performance. In our case we choose the random forest classifier as it performs best on average given the area under the curve is the greatest. One very interesting property of the ROC curve is that is invariant under the class distribution, and thus the results are applicable to a dataset with different proportion of class instances. What this means is that it doesn't matter, as far as the ROC analysis goes, as to which class has how many points to train upon. In the previous videos, we talked about the iso performance lines and then our c curve. We've also learned about accuracy. Now we could have a straight line or an iso performance curve that gives the best accuracy. Let's look at the following problems. We don't assign this as a quiz, but rather as a discussion that you should participate in the forum. Let the true positive rate of a classifier be p, where p is some value. What should be the slope of the line that will give isometric accuracy? Let's get back to the map of our models. We've so far built three different models and used several validation methods, including ROC curves, to choose the model that performs best. We could end our investigation here. However, we can ask a further question. Will changing the classes change the model? Let's investigate this a little bit further. To investigate this question a little bit further, we come back to our original point where we started building our models. To continue that investigation, we'll now build new classifiers by redefining the classes. Then we'll use different validation metrics and choose a new model. At this point, we have our entire map of our model-building and validation process. Let's investigate this end a little bit more to see what metrics we can use to validate models with our new classes. We're back to the precision, recall and F-Score video 28. The sensitivity and specificity of a classifier are symmetric measurements in the sense that the changes in false positives or false negatives do not affect the sensitivity or specificity respectively. Thus in our example we had two instances of classes, B and F. And we decided, thus in our example, we had two classes B and F and we classified between those two. Now we re-ask the question to see if we can choose between two classes that are asymmetric. In our new problem, we have slightly redefined the problem. We want to ask if something belongs to Class G or not Class G. This is a one versus all classifier. We're going to look at some metrics called precision, recall and F-score, that helps us in validating classification problems and models based on these kinds of classifiers. The difference being, the instances for Class G are a lot less than the instances for not Class G in the data. Let's get back to our Python notebook and see how we can build these models first. In this piece of code, we are starting to classify person1 vs all. We call that ova_person, or One Versus All person, and we choose G as that class. We now divide up the events such that we have G and not g as the two classes. Let's run this piece of code first. In the previous piece of code, we have setup the one versus all data. Let's look at what the y values are between then this is 220 to 230. Run this piece of code now. You see the ten values are Fs and Gs, with the last four values being G. Now let's see how this looks when we have the one versus all. Let's see how this looks for the one versus all data. Run this line of code. Notice, all the labels that are not G are labelled as false. All the labels that are G are labelled as true. Running the next line of code gives you the total number of false and true in each of the training and the test samples. We are now ready to build our models. Run this line of code that initializes the models and trains with x_train and y_train with the one verses all model. Now that we have built our one verses all models, let's try to print our accuracy, or score, for each of these models. Running this line gives us the accuracy for each of the LogisticRegression, RandomForest, and Support Vector Classifier models. Note, this time the classes are quite different than what we had before. Now we will define a few more metrics that are useful for selecting between these kind of classification models. We are now going to look at the definitions of precision, recall and f score and come back and calculate these for the model we just built. So now that we have built a one versus all classifier, we will introduce to you, precision, recall, and F scores, which are other performance metrics to validate these kind of models. Let us now define this other metric called precision. Precision is the ratio of the predicted positives that are actually positive or true positives over the true positives plus the false positives. The denominator here is the number of examples that are predicted to be positive. Let's quickly recall our confusion matrix. In the top we have the actual values and on the side we have the predicted values. So actual positives and negatives. Predicted positives and negatives. In this case TP. FP, FN and TN refer to true positive, false positive, false negative and true negative respectively. So the precision is now the true positive divided by the true positive plus the false positives. You see this is somewhat different from the sensitivity value which was true positive divided by true positive plus false negative. In this case we are dividing by this row of predicted positives. You see precision is a good measure for classes where the same is skewed that is the instances of positive classes is smaller than the total number of instances. Can you think of an example of a classification problem where we can use precision as a good metric to evaluate the classifier performance? One example where we want to have more precision is a keyword search. You may have given some other examples. Look in the forum to see what other students have to say. In the case of the keyword search we might want some documents retrieved that we are interested in. In this context precision corresponds directly to something you care about. How many of the documents you're showing your user are actually relevant to them? One thing to note here is that precision depends on the positive outcomes only. Thus if you are interested in the positive class, precision is a good metric to use. Just like precision, we have another quantity that is called recall. The recall is the number of true positives, divided by the number of true positives plus false negatives. Let's quickly look at our confusion matrix again. In this case we are taking the true positives and dividing by the sum of true positives and false negatives. If you remember the definition of sensitivity, you will see that recall is the same quantity as sensitivity. The reason we have two different names for the same quantity is a consequence of how the entire field of model building. Was developed through many dif, different fields and disciplines and only recently we are beginning to consolidate the terminology. So usually if you look at statistics books, you will see the terminology of sensitivity. While if you look at a machine learning book, you will probably find the word recall. Also, when we are comparing with specificity. We tend to use the word sensitivity. When we are comparing with precision, we try to use the word recall. Mathematically, they are the same quantity. Also remember as we varied our threshold, we had different values of sensitivity and specificity. In the same way we can get different values of precision and recall by changing our threshold or cut. The resulting curve is very similar to our ROC curves. In this case these curves are called precision recall curves. Let's go back to our iPython Notebook. We had already built our models and we had received some accuracy numbers for the given classifiers. Let's look at this piece of code. When you run this piece of code you will get the following table. Now we have the same kind of confusion matrix that we had before. Let's look at the next piece of code. Here we are using this method of plot precision recall curve to plot a precision recall curve. We're using the scikit learns method to actually calculate the values of precision recall for every cut. Let's run this piece of code now. The next line should actually draw the curves for you. If everything runs properly you should get a curve that looks like this. In this case the recall has been plotted on the x axis. And the precision on the y axis. Remember the values of both range from zero to one. In this case the Logistic Regression, the Radom Forest Classifier, and the Support Vector classifiers are shown in blue, green, and red respectively. We have also calculated the area under the curves for each of these precision recall curves. This gives us an average measure of performance for these classifiers. It is often useful to use the PR plot if we are interested in the proportion of positive outcomes over the negative ones. Instead of calculating precision and recall individually and looking at each to make a decision as to the performance of a classifier, we can actually use at something called the F-beta score. This is the general definition of the F-`beta score. The beta here can have various values, depending on how much weight one wants to give to the precision value over the recall value. Using a combination of precision and recall, such as the F-beta score, can be very helpful because it gives you one number to maximize rather than two. If you're trying to decide between two different models and one has a higher precision but a lower recall, which will you choose? One method would be to choose the model with the higher area under the ROC curve. Another method would be to choose the model with the higher F-beta score. Now let's try to set beta equals 1. Setting beta equals 1, you can see that the F1 score is given by this formula. This is simply the harmonic mean of precision and recall. This kind of mean protects against comparatively high values of one of these against the other. The F1 score thus weighs the precision and recall equally. One can use different values of beta that give different weights to precision and recall. Much like the loss function, the choice of beta here depends on the specific problem under consideration. We want to do a quick quiz on your understanding of F scores. In each of the following scenarios which choice of F1, F0.5, or F2 be the best choice of metric? First we look at cancer detection. If someone is falsely diagnosed, we may do some extra tests. If someone who actually has cancer is not diagnosed, they may die. In the box give your appropriate choice. The second example is convicting to prison. People are innocent until proven guilty by US law. We want to avoid false convictions, but we also want criminals to not run free. In this case, what sort of F score will we use? In the first case the answer is F2. In the second case the answer is F0.5. Now you may have also chose the second answer to be F1. We can discuss these answers further in our forum. Let's get back to calculating precision recalls and F scores for the classifiers that we've built. If you look at this line of code, the classification report class from psychic learn gives us a very nice summary of all these metrics. Let's run this piece of code. For each of the classifiers. You will get values for precision, recall, F1-score, and support. The support column, contains the total number of actual positives and negatives in the test set. The next line of code gives you a count of the false and true instances in each of the classifiers. We can also calculate the F1-score, F2-score, and F0.5 score. Run the last block of code now. Now you have different values of the F1, F2 and F0.5 scores. Using these scores you can make a decision of which classifier will work best for your needs. We have now come to the end of our map. We have now worked through several examples of the iteration between questioning, modeling and the validation process of model building. Towards the end we changed the model and change our definition of the classes. We introduced to you different performance metrics like precision, recall, and F score. We showed you also how to choose a model based on these performance metrics. Hopefully you followed along and were inspired by all the lessons in this class to continue learning about the various methods of model building and validation. Hope you enjoyed learning from us as much as we enjoyed making this class. Good luck and good bye. We have now worked through several examples of iterating between the questioning, modeling and validating process of model building. Hopefully, you followed along and were inspired by the lessons to continue learning about the various methods of model building. Congratulations on making it to the end of the class. I hope you enjoyed learning from us as much as we enjoyed making this class. Good luck.