Welcome to homework one. We're going to be breaking some code today. What you need to do for this homework is break a few incorrect implementations of the fixed-size queue we've been working with earlier in the unit. Let's go ahead and take a look at what the queue is supposed to do first. The queue class is just a fixed-size queue of integers and the constructor down here is supposed to take a single integer parameter greater than 0. That is the maximum number of elements that the queue can hold at a time, and there are four methods that we're going to need to concern ourselves with, and these method calls are all you're going to have access to during this. The empty call should return true if and only if the queue currently holds no elements and false, otherwise. The full method should do the exact opposite. It should return true if and only if the queue cannot hold anymore elements-- it's completely full and false, otherwise. You would call it like this or like this or the empty and full methods, respectively. The enqueue is the only one that takes a parameter. It takes an integer to put into the queue. It returns true if it's successfully does so and it returns false, otherwise, which would only happen or should only happen if the queue is full. The dequeue method does not take any parameters or removes an integer from the queue and returns it. If it doesn't, if it fails to return anything, which should only happen if the queue is empty, then it returns none. As an example if this code where we first create a queue of size 1, then we check if it's empty, which it should be immediately since we haven't done anything with it so it's empty, should be true then we try to enqueue the number of 10 which also should succeed since it should be able to hold one more element so succeeded should be true then we check if it's full which should also be true because we just enqueued one element we can only hold one element so it's full should be through then we dequeue that element and should return 10 the value, so value should be 10 in here to give an idea of what we should expect from the queue implementation. Now the five queues that you're going to test for are going to have bugs here and there in them, so they're not going to behave exactly like this. I very much recommend reading over this specification carefully before you start testing. Now to test the five queues you're going to do exactly what you do here. You would instantiate queue and call these four methods in sum fashion and then using assert statements, you would try to break this code and draw in it assertion error. On the back end, we'll have a test harness that runs your code against all of the buggy queue implementations and will return the incorrect output if you manage to find it and when you run submit bin on your code it will tell you how many of the buggy queue implementations you have manage to successfully catch. As an example of what your test code should do if we run my implementation against the buggy code then we see that we get in assertion error here. So your code should run assertion errors for all of the buggy code. We'll have a test harness that will run your code against all five of the incorrect implementations and when you submit your code we'll tell you how many of the implementations you've managed to successfully catch and your goal is to catch all five. It took me, I think 22 lines to catch all the bugs--so let's see if you can beat me. Let's start this solution video off by looking through all of the incorrect implementations of the queue, and then we'll see how we actually caught all of the bugs. The first queue has an error where you enqueue and it only stores integers up to 2^16 and that's roughly 32,000. When you enqueue things it's just silently wraps it back around to zero if you go over that. The second queue silently fails to create queues with more than 15 elements. So in the initializer, if the max size is greater than 15, then it just sets it to 15 and doesn't tell you about it at all. The third queue is a little tricky--it implements the empty method by trying to dequeue an element and checking if the dequeue is none. If it is, then it returns true. If it isn't, then it returns false. And because the empty method changes the state of the queue when it shouldn't, this is going to cause buggy behavior. The fourth queue has a bug in the dequeue method. When you try to dequeue from an empty queue it returns false where it should, according to the specification, return' none. The final queue just holds one less element than you tell it to hold. So it's just decrement size max when you pass it in by one and then stores that many elements instead of how many you told it to. Now let's try to catch all of these bugs. The first queue can be caught by just trying to enqueue a value greater than 2^16 and then dequeuing it and check that the value is correct. The second queue, you try to enqueue more than 15 elements-- remember that was the one that only stored 15 without telling you. As soon as you go past 15, this assertion should fail. The third queue is a little bit trickier. We create a queue with two elements and we try to enqueue 10 which should succeed fine, and we check that, then we assert that the queue is not empty which it shouldn't be, but remember that this empty function checks that it's empty by trying to dequeue, which removes 10 from the queue. Now when we try to dequeue a second time, now the queue is empty. It's not going to return 10, it's going to return none. This assertion is going to fail. The fourth queue returns false instead of none when you try to dequeue from an empty queue. To test this, we instantiate a 2-element queue, and we try to immediately dequeue from that empty queue, and we should get none, which is what we're checking. but instead we get false, so this assertion will fail. Now the final queue just holds one last item than intended, so we created 2-element queue, and we try to enqueue two elements, and this will fail for the second since this queue will only hold one element not two, so this second attempt will fail. Okay, I hope you enjoyed catching all those bugs. This is something that comes up fairly frequently when you have to do blackbox testing when you don't actually know the code that you're running against, you don't have access to it. We'll talk a bit more about that in Unit 2 and I hope to see you there. Thanks! Welcome to the first unit of CS258--How to Make Software Fail. Now, making software fail is something that I feel very strongly about, but in the big picture, it's not necessarily failures that we're interested but rather the fact that if we can find failures in software and if we can fix these failures, then eventually we're going to run out of bugs and we'll have a software that actually works reliably. Now if you look at testing software as a whole--as a large problem--it's really kind of daunting, because we can look at Microsoft's products and we can see that Microsoft didn't manage to eliminate all the bugs in their products, and we can look at Google's services and we can see that Google didn't manage to eliminate all the bugs in their services and so we might want to say to ourselves--how can we possibly get rid of all the bugs in our own software. Well, what turns out to be the case is the testing problem is not really this large monolithic problem, but rather can be broken down into a lot of smaller sub-problems, and by looking at those sub-problems, we can apply known techniques and things that people have done before, and we could sort of pattern match on these problems, and once we've become good at these smaller problems, then we can become much better testers as a whole. And this is what Unit 1 is about. Now I'm going to take quick look at the question what is testing. So it's always the case that we have some software under test. When we're doing testing, we always require a source of test inputs. This results in test outputs. The outputs are processed by an acceptability check. If the outputs are okay, then the test case succeeded. If the outputs are not okay, then the test case failed and we're going to have to debug. This is, of course, really simple. On the other hand, a couple of the things on this slide here--that is selecting a good set of test inputs, and designing good acceptability test--end up being actually really hard, and basically, these are what we are going to be spending this course talking about. I would like to give a few facts about testing--by facts of course I mean my own opinions. First of all, the goal of testing isn't so much as finding bugs, but rather it's finding bugs as early as possible. If our goal is just to find some bugs, we go ahead and give the software to our customers and let them find the bugs, but of course, there are huge cost associated with doing this. What we rather want to do is to move the time in which to find those bugs early. And the fundamental reason for that is, is that it's almost always the case that the bug that's found earlier is cheaper to fix. The second fact is that it's possible to spend a lot of time and effort on testing and still do a really bad job. Doing testing right requires some imagination and some good taste. Third, more testing is not always better. In fact, the quality of testing is all about the cost/benefit tradeoff. And fundamentally, testing is an economic activity. We're spending money or we're spending effort on testing in order to save ourselves money and effort later. Going along with this, testing methods should be evaluated about the cost per defect found. Fourth, testing can be made much easier by designing software to be testable. We have a quite a bit more to say about that later. Fifth, quality cannot be tested into software. And the corollary of that is--it is surprisingly easy to create software but it's impossible to test effectively at all. Finally, this is an important one, testing your own software is really hard and there are several reasons for this--first of all, it's pretty common for us as developers to be proud of our work. What that means is we may not really want our own software to fail. This isn't sort of blaming developers--it's rather just a fundamental fact of human nature. We're trying to shape software, we're trying to make good software, and we may fundamentally at some sort of subconscious level not really want to break it that badly. The second reason that testing our own software is hard is that as an individual developer, we tend to get our heads buried in the details. And we may not know the global context that would allows us to do really effective testing of the software that we just wrote. The third reason is we can't test the code that we left out because we didn't understand that it needed to be written. Finally, we can't write effective test cases for parts of the spec that we didn't understand correctly. So it's not uncommon of all for us to misunderstand things while reading this verification. For example, to keep aside or get something, otherwise, backwards and once we've done something like that, what we really have is a bug in our thinking. And it's mirrored by the bug in the software, but those kind of bugs are extremely hard for us to find. They rarely often takes somebody else testing it to find that kind of bug. So I want to spend a little time talking about the question-- What's going on when we test software? It turns out the answer is a lot of different things are going on. So here I'd like to start off with the output produced by a test case. It's going to pass through the acceptability check. What we're doing when we check the output of a test case for acceptability is we're running a little experiment, and this experiment can have two possible results. One result is the output is okay. In which case, we are to go run another test case. And the question is, what do we learn in that case? And the answer is unfortunately not very much. What we might have done at best is increased our confidence just a tiny, tiny bit but the software under test is correct. And as it so happens, we stand to learn a whole lot more when the output is not okay. And the process I'm going to talk about right now is if the acceptability check fails that is to say the test output is not okay, we have to discover what's happened. And so of course what we expect is much of the time we'll find a bug in the software under test. If that's the case, we're going to go fix it. And if not, there's still plenty of other possibilities. And one of the main ones is a bug in our acceptability check. If so, we're going to fix it. If not, we should consider the possibility that there might be a bug in our specification. As is often the case, the bug in the specification is a fairly large part of debugging because the specification often is not written down or it's not written down particularly formally. It's just an English document or an understanding among a group of people. And very often, we're learning what it is that we needed to implement as we're implementing the software. If that's not the case though, if the bug was not in our acceptability check, not in the software under test, and not in the specification, we still have some possibilities but they're getting more difficult to deal with. Some of the worst debugging stories that you hear of stem from a flaw in the operating system in the compiler, in run time libraries, or even in the hardware. Since fixing this kind of bugs is often not an option for us because we've purchased these things or otherwise gotten these from some other vendor, often they have to be worked around and these can be extremely painful. If that is the case, we have a hard road ahead of us but at least we know where the flaw is. If that's not the case, we're at a bit of an impasse. What is it that's really going on here? What's the big picture here? Well as I was saying, what the test is it's a tiny experiment where a failed test reveals that something is wrong. It reveals that something is flawed in either the understanding of the system or in some part of the system. And there might be a fairly elaborate process in figuring out what the problem is. These kinds of discoveries that we're making about the system that we're using are not necessarily a problem because these are things that we need to know if we're going to create correct software in the future. Okay, so why don't we take a look at some bit more famous bugs that kind of fit this basic flowchart. So let's talk about a bug that happened to the Mars Climate Orbiter. The Mars Climate Orbiter was sent off to Mars in 1998, and there were some miscommunications, let's say, between NASA and the people they contracted out to which was Lockheed Martin. And by the time the Mars Climate Orbiter actually got to Mars which was in 1999, quite a while later, there had been some problems that caused the orbiter to drift off course enough that it basically ran into a suicide mission and crashed into the Martian atmosphere and broke up and crashed in the planet. What happened was a basic unit error. See, NASA expected units in metric, for example--meters per second, and Lockheed Martin programmed in English units, for example--feet per second and neither of these are wrong. I mean there are both entirely valid ways to program a rocket and the underlying code was actually correct at least in terms of this bug didn't have anything to do with the underlying code. But because the NASA software was expecting units in meters per second, and the Lockheed Martin software was sending feet per second English units. That miscommunication caused its drift off that caused the Mars Climate Orbiter to crash into the Martian atmosphere and into the planet. So why don't we go ahead and turn this into a quiz. Is this a bug in the software under test that is the actual software of the Mars Climate Orbiter? Was it a bug in the acceptability test when they tested the actual software earlier on? Was there a bug in the specification, was there actually an underlying bug, and how they plan on operating the Mars Climate Orbiter? Or is there a bug in the underlying hardware or libraries or compiler or the operating system of the rocket? So go ahead and mark your answer here. You could perhaps argue some of these--you might have some difference of opinion here, but I wouldn't say that the bug was in the actual software under test because the actual software did what it was supposed to do according to specification correctly. And was there a bug in the acceptability test--well in that the acceptability test didn't check this then yes but if you're going to use that as a metric, then you could argue that all bugs are bugs in acceptability test because they didn't catch all of the bugs which is of course infeasible. I would say this is a bug in the specification because this was a miscommunication between the engineers at NASA and Lockheed Martin. And both of their ideas were correct but they didn't communicate those ideas properly. It was assumed on each of their ends. So if they had put that information explicitly in this specification then we wouldn't really have had the issues and the Mars Climate Orbiter wouldn't have had at least that bug may be something else would cropped up or maybe not. I also wouldn't say that there is a bug in the underlying OS, compiler, libraries, or hardware. At least this bug isn't an example of that. At least we don't have any evidence of that. There might have been other bugs again but this one I would say is definitely a bug in the specification--so let's check one more example and then move on. What I'm going to do now is go over a fixed-sized queue data structure that's going to serve as a running example for some of this lecture, and we're also going to keep using for a couple of our exercises for the next several units. And the way this data structure works is it supports an enqueue operation, a dequeue operation, an enqueue elements to dequeue and FIFO order. The FIFO is first in first out--so if we enqueue 7 and then also 8, the first thing that we dequeue. will be 7 then 8, and if we try to dequeue again--if we try do dequeue an element with an empty queue, we're going to get some sort of an error. Okay, so that's all fixed-sized queue. Now let's look at some code. So in the implementation site--what we have here is a Python object so it's called queue and the constructor for that object is going to set up the data structure because structure for that object is going to take a size max argument--that's going to determine the largest number of objects that can be stored in our queue and it's going to set up the data structure. So first it's going to make sure that size max is greater than zero. It's going to save a temporary copy of that, initialize some head and tail pointers, a size variable which stores the number of objects currently in the queue, and finally we need to reserve some space for the queque elements themselves. But you can see here is we're allocating a Python array and so one implementation option for a queue in Python will be to just use a Python list and that will be basically trivial. Python list already is pretty much need of a support enqueue and dequeue operations and the problem with the Python list is they're dynamically allocated, they are dynamically tight and that makes them not very fast. And so by allocating a fixed-sized storage region of statically tight memory, where the "i" here means that our queue is only going to be able to store integers we can avoid some of Python's dynamic checks that makes the queue slow and so in some cases, a queue based on a Python list is perfectly fast but on the other hand in some benchmarking that I did, this statically sized, statically tight queue is more than 10 times faster than a queue based on a Python list. So the first method that queue supports is the empty method and this simply returns true if self.size equals zero--so of course the empty queue is the one that currently contains zero elements. Very similarly, the full queue method returns true if the current size of the queue is equal to the maximum size of the queue. So now let's look at a couple of routines that are slightly trickier. The enqueue method is going to take an argument x. X is an integer that we want to try add to the queue. The first thing this method is going to do is check if the current size of the queue is the maximum size--if so the queue is full, then we're going to return false. If we passed this test, of course, the queue is not full and we have room. So the next thing we're going to do is put the argument data item into the queue at the location pointed to by the tail. And so now let me show you a little bit of about how our representation works. So for demonstration purposes, we're going to look at a 3-element queue and initially it's going to have a head and a tail according to the first queue element-- that is the queue element with index zero and also its size is going to be zero. To enqueue an item, the first check will be useful--no it's not because its size is zero. We go ahead and put the item--let's say it's the number 7 in the queue element pointed to by the tail We're never going to increment the tail, and now the last thing we have to do to enqueue an element is increase the size of the queue to be 1. Okay, now let's go look back at the code. Seeing here at the code, we can see that we put the element in the queue, we increased the size of the queue, we moved the tail to point to the next element and the only thing that's left the only bit of logic that's sort of tricky here is if the tail of the queue point passed the end of the queue--that is to say if it's equal to the max and so remember what the zero index array, the maximum way in is going to be one pass at the end of the queue--we're going to reset the tail to point at the zero element of the queue-- that is to stay at the beginning. Now the dequeue operation is very similar--first if the size of the queue is zero then the queue is empty, we're not going to be able to dequeue an item. And so what we're going to do in this case is return Python to none type. So none of the special data types supported by Python we can often use to indicate that we don't have anything--we don't have any actual value. So if we pass that test, then there is something to return. So what we're going to do is store the item from the head of the queue in a temporary variable. So x is going to get 7. We're going to decrement the size of the queue. We're going to move the tail point out to point the next element and then using logic very similar to the tail pointer in the enqueue function, we're going to wrap the head pointer around if it's gone passed the end of the queue. So let's go back to the drawing and look out and see how this plays out. So we're going to return 7, decrement the size, and make the head element point to the next element of the list, and we're not going to bother erasing the 7 we returned but we're going to have to make sure that our queue logic never returns this dead element. Oh! So let's take a very quick quiz just to make sure that you understood all of that. First I'm going to make a 2-element queue,. I'm going to set temporary variable r1 to be the return value of enqueuing an element and set r2 to be the returned value of enqueuing another element, r3--a third element, r4 to be the value returned by dequeuing an element, r5 to be the returned value of dequeuing another element, and finally r6 to be the returned value of dequeuing yet another element. Now the question that I'm asking is what are the values of these 6 variables. Choice A is going to be 6, 7, 8, 6, 7, 8. Choice as B is going to be True, True, True, 6, 7, None. Choice C is going to be True, True, False, 6, 7, 8. Choice D is going to be True, True, False, 6, 7, None. Go ahead and write your answer here. The answer of the quiz is D. The first enqueue operation returns true because in a 2-element queue where we haven't inserted many elements there's space to insert an element. The second enqueue call succeeds for the same reason. The third enqueue call fails and returns false, because there was no room left in a 2-element queue after 2 enqueue operations had already succeeded. Because we have a FIFO queue, the first dequeue operation returns the first element that we'd added to the queue--that is to say 6, the second dequeue operation returns 7, and the third dequeue operation, which is now called on an empty queue, returns none. Okay, so now that we know that while sometimes we can be clever a lot of the time we just can't test everything we would like. How useful is any individual test case? Is there any way to make a test case that is a little bit more useful in practice? Let's look at this real quick--we just enqueued 7 into our fixed-size queue and then immediately dequeue it. So we should get 7 back out and if that's the case then we print success and otherwise we have an error. How useful is this? Let's go ahead and turn this into a quiz. The question is if we passed this test case, what else have we learned about our code or have we learned anything? And your options are our code passes this one test case, our code passes any test case where we replaced 7 with a different integer, or our code passes many test cases where we replaced 7 with a different integer. Check all that apply. Okay, so if we passed this test case, then obviously, yes, we've learned at least that much. It isn't completely useless. Does our code pass any test case where we replaced 7 with a different integer? Well, maybe not--if you replace 7 with a gigantic integer-- something you can't even store on your computer it's so big-- then you're probably not going to get the same behavior out of these test cases with something simple like 7. We can't necessarily say this. I would say that as well. Now, our code passes many test cases where we replaced 7 with a different integer. This is true. If we were to replace 7 with say 20 and replaced it down here as well, then yes--this should behave essentially the same. And it should behave essentially the same for many, many integers. Certainly not integers that have very large stored space requirements, or maybe it doesn't store integers past a certain static typing. For example, if you're storing a type in your queue that is only able to store very small integers, let's say, shorts which are typically much smaller, then yes, this won't work. So the question is--what exactly do we learn, and how do we make test cases that represent large a number of other potential test cases so we don't have to write a whole lot of very small simple test cases? The question I'm asking here is given this test case that we were to talk about, is it possible that I can conclude without trying it that this test case will succeed? What this test case does is enqueues 7, sleeps for a while, dequeues an element, and then checks if it was 7, and the answer again is yes. And again, it depends on making an argument about the actual form of the code for our queue. If you recall that code, the code didn't contain any dependencies on the time at which queue request are out. Just to be clear, we're trying to make arguments that a single test case is a representative of a whole class of actual executions of the system that we're testing. And in the general case, we can't make those arguments. We can only make these arguments for the specific queue code that we're talking about because we know that, for example, it doesn't depend on the integer value that we're adding. It doesn't depend on the time at which operations are made and that's why these arguments actually work. What we're actually getting at here is a fairly deep idea. The idea is we have a single test case that represents a point in the input space for the system that we're testing. By running the code, we mapped that point in the input space to a single point in the output space. But the problem is, there is of extremely large input space and we can't test it all. What we're trying to do here is build up an intuition for when we can make an argument that, in fact, we haven't made an argument about the mapping of a single input to a single output. But rather, some class of inputs that are closely related, that are somehow equivalent for purposes of the system under test, and that if the system executes correctly for the single input that we've tried, it's going to execute correctly for all of the inputs in this particular bit of the input space. Let's look at a few ways in which the arguments that we've made here could've gone wrong. If it's ever the case that our queue could work for some integer, but not work for general integers, and certainly, it is the case. For example, if the Python runtime contains some sort of a bug where large integers were truncated, that it would obviously be the case that if we put one of those large integers into our queue, it will get truncated somewhere in the queue processing here, and what would come out wouldn't be what we had tried to put in. Similarly, if the Python runtime contained some sort of a garbage collection bug, that corrupted heap memory--it might be the case that sleeping would give the Python interpreter time to corrupt our value, time to corrupt our queue, and we wouldn't get something out. But in both of these cases here, we're making arguments about the broader runtime system, and not the actual code under test and remember that the arguments that we made here they're really only about the actual code that we were testing. So in summary, we can make these kind of arguments but we have to be extremely careful that the assumptions that those arguments are based on are good ones. What I'd like to do now is give you a little quiz about these concepts. Here's the quiz I'd like to give you--what I've provided is a queue code we've already seen and also three test functions. The first test function is the one that I wrote and this is put here really quickly. It creates a three-element queue, checks that it's empty that is to save. It calls the empty method, and if it's not empty, that is to save. If the empty method doesn't return true, we print test1 not okay and return. Otherwise, we keep going. So we enqueue a 10, make sure that that succeeds. Enqueue the value 11, make sure that that succeeds. Call dequeue method which takes the first thing out of the queue that we put in which is 10, so we make sure that it's 10. Dequeue the next element, make sure that it is 11, and now we check again that the queue is empty. If all of these are true, then we print test1 okay. So, that's my test function. You're test function are test2 and 3--so just make sure that everything works here. We run the code and we see the test1 okay, test2 okay, and test3 okay. So you're assignment is to write functions test2 and test3. So just a little bit previously, I gave some examples of several different tasks for the queue that were equivalent to each other in the sense that if we run one of them then it would just about as good as running the other one. So, if you remember, for example, we can enqueue some numbers and dequeue them and check if they are correct. The queue code would behave exactly the same if we do the same exact operations but with sleeps in between them. So, you're going to do here is the opposite of what I was showing. What I want you to do is write test functions two and three so they're not testing equivalent functionality in the queue. This should be pretty easy but let me just go up and look at the code and we'll get some ideas about how that might work. So, what kind of behaviors in the code could you trigger that I didn't trigger? Well, it's pretty obvious. For example, I never triggered the four case for enqueue. I never triggered the wrap around case, I never triggered the empty case for dequeue, nor did I trigger the wrap around case with the head point. So, given those ideas, right functions test2 and test3 such that they are not testing equivalent queue functionality as the test1 function that I wrote. Okay, so let's go ahead and take a look at clients real quick. So, this is the original test function that you had test1 and remember that you said that some of the behaviors that that doesn't test for is basically all of these if statements here. So, I'm going to go ahead and just try test2 to test these two main ones that is if the size is equal to max, then we will return false, and if the tail is equal to max, then we will reset the tail to zero. So let's go ahead and build that. Okay. So in test2, we create a queue of size two and we make sure that it is empty to begin with just as a good start, and we enqueue1 and enqueue2. We make sure that these both returned true. And then we enqueue 3 or we attempt to enqueue 3, which shouldn't work because remember it's only a queue of size 2--it can only store two elements. So, we should be able to check that, and the tail, queue.tail, does not equal to zero. If it doesn't, then we haven't reset it to zero like we said we did right up here, remember? So, that would be a problem. And we check that right there and, otherwise, we print out test2 as okay. So, we go ahead and run this. Okay. Now we see that we print out test2 as okay, so everything is good. So now, let's go ahead and check out test3. So for the third test, remember let's take a look at dequeue function really quick. So, we want to test that when you dequeue an empty queue, you should return none since there is nothing to return. Then, we also want to check that if we dequeue and we have gone pass the end of the queue that is the head pointer has stepped out into a larger element and the total length of the queue, the size of the queue, then we want to reset it to zero. So, if we go forward or pass the end then we want to wrap it that around. So, let's go ahead and take a look at test3. Okay, so the first thing we do is we initialize a queue of one element. So, it can only store one element. It's not a terribly exciting queue. So first, we try to empty it, which shouldn't work. If it does, then we got a problem. Then, after that, we try to dequeue and so remember this should return none. If it doesn't return none, then again we got a problem. If it does, then we continue. Now enqueue one element, so at this point, the queue is full and head should be at one, which is the max size of the queue. So now, when we try to dequeue, we should reset queue.head to zero. If we don't, then we got a problem. And we go ahead and check that the element that we get backed out from dequeue is also one--just to be complete. So, if we get through all of these, then we print out test3 okay. So, let's go ahead and run that and we see that, yes, test3 is okay. So, you might have tested different things. You also might have tested just some of these and that's fine. The goal was just to try to figure out different places in the code that you could test against that we had already given you. And that's a really important thing to learn how to do as a tester. So, okay, let's go ahead and move on. Now we're going to talk about something that's pretty important in practice, and that's creating software that can be tested effectively. And we're going to start off with things that are kind of maybe a little bit fuzzy and unquantifiable. And we'll get towards more very specific recommendations a little bit later. And so I'm going to start off with some sort of generic recommendations, and first, it's to write clean code, and, of course, we all know that's pretty much impossible. So when it's not possible to write clean code in the first place, we need to refactor code whenever necessary. So third, of any software module that we're writing, we should always be able to clearly describe what it is that this module does and exactly how it interacts with other code. If we find ourselves in a situation where we can't clearly describe all of the dependencies that our module has with other modules in the system, then we're probably in a situation where we've created software that can't be tested effectively. Code should never have any more threads than it absolutely needs to get its work done. The software should not be a swamp of global variables. Global variables make software really hard to test, because global variables are implicitly inputs to any software that reads the global variables. And so, in addition to testing software on its explicit inputs, if we have a lot of global variables, we're going to be forced to test a lot of values of those, and this can really make testing difficult. Our code-- and this applies probably more to C and C++ and Java than to Python--shouldn't be a soup of references and pointers, because these are another way to create unintentional connections between different parts of the code, breaking modularity and again making it really hard to test our code. Most of the time, the modules that we'll create should have unit tests, and when applicable-- that is to say when our modules are interfacing with other modules that might fail-- we should add support for fault injection. Finally, testable software often contains a large number of assertions, and so now we're going to expand on that topic. Assertions are an important enough topic that we're going to want to cover them separately. What an assertion is is an executable check for a property that must be true of your code. For the square root example that we have been using, we're defining a function called "sqrt," which takes an argument. We're going to use that argument to compute our result, and here we're going to assert that the result is greater than or equal to zero. So let's return to the square root example we've been using. Here we're going to define a Python function called sqrt, which takes an argument. There's going to be some code at the top which computes the result And now what we're going to assume is that if the argument to the square root was negative--this bailed out with some sort of exception--so if we reach this line in the code, then we successfully computed a square root result, and what we're going to do is assert that that result is greater than or equal to 0 and then return it. Why did we assert that the square root was always greater than or equal to 0? Because we know, by definition, our square root function is returning the positive square root of its argument. At this point in the code, we know that we must've computed a positive number. So it's good to go ahead and assert that. So let's look at some basic guidelines for putting assertions into code. The first rule is that assertions are not for error handling. So, for example, it would've been a mistake at the beginning of this routine probably to assert that (arg) was greater than or equal to 0. That would reflect a condition that we want to handle with an exception-- that is to say with Python's established error handling mechanism-- and not with an assertion. The assertion that we put into the code asserts the result of our logic that we wrote as seen. It's not asserting something about the behavior of some other entity in the program. That would generally be more in the domain of error checking. Rule 2--and this is really important-- is never make assertion calls that have side effects. An example of a side affecting assertion would be asserting that a function foo returns the value of 0 but where foo changes the value of global variable. And the problem with side effects and assertions are when we turn on optimization in Python, it's going to drop all of the assertions in our program. So what we're going to have is a program which happens to work correctly, because the assertion changes a value in some way that's needed. But, on the other hand, when we turn on optimizations and drop all of the asserts, then the program's going to stop working. And this makes the assertions worse than useless. This is almost the worst possible thing you can do with an assertion is have an assertion in your code that changes the global state in some observable fashion. So we definitely don't want to do this. Now rule 3 is we don't want to put silly assertions into our code. And let me give an example. So here we're asserting that 1 + 1 = 2 Now is it conceivable that in some Python program, 1 + 1 was not equal to 2? Sure, it's conceivable if the Python interpreter is incredibly broken. But, on the other hand, if the Python interpreter is that broken, nothing is going to work. Our program isn't going to run at all, so in any case, there's no point whatsoever in doing a silly assertion like this. The best assertions are those which check a nontrivial property that could be wrong but only if we actually made a mistake in our logic. It's not something that could be wrong if the user did something wrong, and it's not something that's wrong that's just completely silly to check. Now I'm going to let you guys write some assertions. Okay. So here we are, looking at the queue code from unit 1, and what we're going to do is try to figure out what kind of assertions we can add to this queue code that would make it more robust with respect to mistakes. So we go through the code--and this is the same code we already looked at--and what you can see is at the end, I've added a check rep function. And what check rep is is a function that stands for check representation, and this is a function that we commonly add to a data structure or to other functions that checks the variables in the program for self-consistency. And so what it's going to do is basically try and terminate the program if some invariant that we know should hold over the program's data structures fails to hold. What I first want to do, though, is run this program. And when I run it, what's going to happen is some testing code that I haven't shown you is going to create a queue and call its methods in a loop, and it's going to look at the output of the queu and try to find any mistakes. So the test function has the test oracle built into it that I wrote that's going to try and see if our queue is correct. What we hope is that it's correct, and, also, we know that the check rep is going to succeed, because it contains no assertions right now, so let's run. Okay. Now that we've seen that our queue passes the test harness, let's see if we can harden up the queue a little bit by thinking of a good assertion to add to it. And so I think that the first assertion that we're going to add is going to be over the size variable. And if you recall from earlier, the size variable tracks the number of elements currently in the queue. So let's go ahead and add an assertion about the size. One thing we know about the size variable is it should never be negative, because we can't possibly ever have a negative number of elements in a queue, so let's assert that self.size is greater than or equal to zero. And the other thing we can assert about self.size is that it never exceeds self.max, which is the constant variable, which counts the most elements that could ever be in our queue. So here we're asserting self.size is not negative and doesn't exceed self.max, and so now when I run the test harness again, we're going to again check the queue for functional correctness and also call the check rep. Let's see what happens, so it printed a finished over the old finished, and so that's good. Okay. That invariant was good. So what I want to do next is I'm going to break our queue, so I'm going to break our queue by making the enqueue fail to properly reset the tail variable when it overflows. What we have here is logic that when self.tail is equal to self.max, we reset it back to 0. And I'm going to set it to 1 instead. So let's see what happens when we run the queue now. It failed, and what it did is it failed in an assertion inside the test harness. This was in my test function down here on line 90. See, we only have 40 lines of this code. So way below is my test function, and we failed there. What this means is we've messed with our data structure in such a way that it actually returns a wrong result, so this is the result not be equal to the expected result assertion. And so what we've done is we've created a data structure that's broken, and when it returns wrong results, this is going to be deeply buried in some program that we care about, so it's going to return the wrong results to some deeply buried part of the program, and probably that program is going to keep running for a while. And it's going to start to do very bad things. It's going to lose track of webpages or it's going to start to do other things that we don't like. And so what we want is to write a tighter set of assertions for our queue so that our check rep can catch this before we actually return the wrong thing to the user. And so this is your API quiz. Your API quiz is write an assertion that goes under mine in the check rep which catches the bug in the queue before it can actually misbehave. The trick you're going to have to do is since our buggy code sets self-tail to 1, that's in range, so another invariant besides saying self.size is between 0 and self.max inclusive is we could've asserted the same things for self.head and self.tail, but the bug here is not going to violate those particular checks. Rather, what's going to violate is a relative check between the values of head, tail and size. And so that's what you should be thinking about to add to your assertion here, and so what I want you to do is come up with the correct data structure invariant to assert here, so that that assertion fails when we run our queue tester before we actually return a wrong result. And additionally what we're going to do is we're going to check your assertion against a correct version of the queue, so we're going to take your check rep code and put it in the correct version of the queue and check that it doesn't fail spuriously. So what we want to do is write an assertion that's tight enough that it catches the error that we've introduced here but not so tight that it always fails. Okay. I hope you'll have fun with this. I'm not sure what assertions you added, but let's look at some of the ones that I decided to add. One condition we can do is if self.tail is greater than self.head, then the difference between them has to be the size. That is to say, the distance between the head and tail has to be the number of elements currently in the queue. On the other hand, if the tail is less than the head, then that means that the tail has already wrapped around, and in this case the difference between head and tail has to be equal to max minus the size of the queue. The third case that we have here is that if the head and tail are at the same place, then either the queue is empty or the queue is full. Let's go ahead and run this on the bugging and see what happens. So, we violated an assertion, and better yet, we violated an assertion in the checkRep function. So, that's good. What this means is by violating an assertion in a checkRep function, we caught the bug, we're terminating the program, and we can debug the code, using sort of localized debugging inside this data structure. We don't have to do some huge tracking down of where the bad values came from. So, we want to do one final check here to make sure that my assertions are right. I want to go to my buggy key code, fix it, and run the code again. What we want is all of these assertions should succeed during the test cases. Okay, good. So, that was the case. Now we're going to talk about the question why assertions? What are assertions really buying us? This boils down to three or four basic reasons. First, assertions make code self-checking. That is, they make the code that write fail proactively when somethings wrong, and this can lead to more effective testing. Second, assertions make code fail early at a location that is closer to the actual bug, since this is more relevant to debugging than to testing, but it's generally the case that debugging is a different backwards problem where you're reasoning in a backwards sort of way about the behavior of the software that you wrote, and the fewer steps there are in one of these backwards chains, the better. Assertions really can help with that. Third, assertions can help us assign blame. It's often the case that assertions live at interfaces between different modules, and in many cases it's possible to look at where the bad values came from that lead to the assertion failure and use that information to assign blame to one of the modules involved in the call or to the other one. Real software testing exercises often lead to different finger-pointing problems, and any tools that we have that allow us to accurately assign blame to one or the other of our software modules can be valuable. The final reason we like assertions is because they allow us to create executable documentation about assumptions we've made, about preconditions in our code, and so by writing down executable checks on our preconditions, we not only get to check these preconditions when the code runs, but also we're documenting for other developers the things that need to be true for that code to execute correctly. Post conditions are things that are supposed to be true after our code runs. Again, in addition to creating executable checks for these, we're also telling other developers what we think should be true. This should allow other people to rely on this postconditions. Invariants are closely related to preconditions and postconditions. These are things that need to be true, for example, at every iteration of a loop. Something you might be wondering is do real developers put assertions into the code that they write? It turns out that often they do. I took a quick look at the GCC and the LLVM code bases. GCC is the GNU compiler collection. It contains not only the well-known C compiler and C++ compiler, but also a Java compiler, and an Ada compiler and a number of other tools, and the GCC source code base contains more than 9000 assertions. Now, the LLVM compiler suite contains about 13,000 assertions, and that's over about 1.4 billion lines of code for a total of about 1 assertion per 110 lines of code. Here I'm just counting raw lines of code, not source lines of code. This includes blanks, comments, and everything. What this tells us is that the LLVM and GCC developers have made a pretty serious commitment to checking assumptions, preconditions, post conditions, and invariant in the code that they wrote. One thing that I've done personally is reported a lot of compiler bugs to both the GCC and LLVM products, and one thing I've learned is that much of the time these bugs show up as assertion violations. Not always--sometimes these compilers seg fall to have other problems. But most of the time they're assertion violations. These assertions are actually succeeding at accomplishing the goals that we talked about on the left side of the screen here. One further thin that I want to talk about is do we disable assertions in production code? For example, when we run the Python interpreter with the "-O" option, which causes Python to enable some optimizations, it disables assertions in order to make our code run faster. The question we're asking here is is that a good thing? It turns out that there are arguments we can make in both directions. Let's first look at reasons why we might want to disable assertions. One of the main advantages of disabling assertions would be that it lets our code run faster. Another advantage--and here we're starting to get into something, which really depends on what we're trying to do with our system-- is that code running without assertions is less likely to stop running. The code keeps going even after some sort of condition is found within the code that would have triggered an assertion violation if assertions had been enabled. Here we really have to ask ourselves the question what is it that we're trying to do with our system? Is it better to keep going or is it better to stop? Remember that keeping going after some condition is true that will lead to an assertion violation may lead to a completely erroneous execution. On the other hand, possibly, that's better than actually stopping. Now let's move to disadvantages of disabling assertions in production code. If our code happens to rely on some side effect performed by an assertion-- that is to say, if whoever wrote the code that we're using has violated one of the rules for assertions that I gave you-- then turning off assertions is going to break the code. Not only is it going to break the code, but it's going to break the code in an extremely confusing way, because lacking assertions, we're going to have a very hard time detecting the error except that it's probably going to cause our system to crash in some confusing way or just to do completely the wrong thing. This is a real risk of turning off assertions in large systems. The second reason is even in production code it may be better to fail early rather than keep going. Really, it just depends on what our system is doing. The question is do our users want the system to die or do they want the system to give them some completely wrong result? What I'm going to do now is read a short quote from Julian Seward, the author of the Valgrind Tool. If you haven't used Valgrind and if you're a serious C++ developer, you really should. It's an amazing tool that runs dynamic checks on your running program. and it looks for errors such as null pointer references, out-of-bounds array accesses, and other things. He said, "This code is absolutely loaded with assertions, and these are permanently enabled. As Valgrind has become more widely used, they have shown they worth, pulling up various bugs which would otherwise have appeared as hard-to-find segmentation faults. I am of the view that it's acceptable to spend 5% of the total running time doing assertion checks." So, what Julian is saying here is that he's going to cost everybody 5% of total running time in order to make testing and debugging of Valgrind easier. I've included this somewhat long quote because I really agree with Julian here. I think it's generally worth our time to try to make our code fail early, fail fast, rather than keeping going and failing in confusing ways later or even providing people with completely wrong results. On the other hand, let's imagine we're working at NASA or someplace like that, and we're writing software that's going to be controlling a spaceship as it lands on Mars or some other planet. We have to ask ourselves the question do we want to enable assertions checks for this kind of a mission? I actually talked to a NASA engineer some time ago who had worked on the software for one of the Mars missions. What he told me was for most of the mission-- that is to say for the launch phase, for the cruise phase, for orbiting Mars-- they had plenty of assertions enabled in the actual mission software-- that is to say, in the software running on the spacecraft. On the other hand, for a short period of time during which the spacecraft was actually landing they disabled all of the assertions on the idea that they got only one shot to land on the planet and that if anything went wrong with the software during the landing process it was better to just try to keep going, because an assertion violation leading to a system reset could cause the lander to become unstable in such a way that it wouldn't have been recoverable. That gives us a summary. If you're doing something so critical that it keeps going that it resembles landing a spaceship on Mars, then go ahead and turn off assertions in your production software. Otherwise, you're probably better off leaving them enabled. So there's always some software under test. And this might be something huge like the Linux kernel, it might be something really small like a new square root function that we wrote, but, in any case, what we want to be able to do is draw a box around the software that we're testing. In general, the reason that we implemented the software is to provide some sort of a service that we're going to use in other software that we're going to provide to a user. And what that means is the software under test is providing some set of APIs-- that just stands for application programming interfaces-- and all this means is the software under test is going to provide a set of function calls that can be called by other software. And of course, that's what we want to be testing. So for the next little while, let's assume a really simple piece of software under test. Let's assume that it's just a square root routine that we have implemented, and we want to make sure that it works. So to test this code, all we're going to need to do is pass values into the software-- so here we're calling square root routine with 9 as an input--and see what it returns, see if it returns the thing that we expected. Let's just take a quick quiz here. If I call a square root routine--and here let's assume that we're not talking about the Python square root; we're just talking about a general square root function that we're implementing for some hypothetical piece of software. So we invoke the square root routine with 9. And what should it return? Here we're going to make a multiple choice exam, and you should enter into the text box all of the answers that apply. So answer a) is 3, answer b) is -3. Go ahead and please write your answer in the text box. The answer to this quiz is any answer is acceptable. It's perfectly possible that the answer we expected from square root of 9 is 3. In fact, that's what we would expect most of the time. But also, -3 is a perfectly correct answer for the square root of 9. The issue that we're getting at here is what's the specification for the software under test, for our square root routine? Is it defined to return the positive value? the negative value? Can it return either of them? And this is what we're getting at here is that software always operates under some sort of a specification. Of course, most of the time, specification isn't actually available to us in some sort of a clear technical document. Often the specification is vague or it's written down in English. Part of our job as a tester is to help refine the specification by figuring these things out, by figuring out what the acceptable answers really are. Let's continue with our example. We're testing the square root routine, and now we're going to test it with the input value of -1. So we call square root of -1, and the question is, what should it return? Let's make this into another little quiz. Our possible answers are a) 1; b) i; c) not a number; d) throw some sort of an exception; e) -1; and f) just crash the machine. And so here's a text box for you, and I'd like you to type in all of the letters that apply as reasonable results for square root of -1. Let's go through the answers. a) can't be right. b) could be right. It's perfectly acceptable to return i for a question about the square root of -1, assuming that we're working in a domain of complex numbers. c) is probably an acceptable result as well. Not a number is often returned for numerical queries in the floating-point domain that have no good actual answer. d) is also most likely a good answer, throwing an exception. e) is not a good answer. -1 can never be the square root of -1. And crashing the machine is probably never a good response. So the answer is some combination of b, c, and d. And so again here, what's happening is just running a simple test case is forcing us to think about the specification for the software under test. And in fact, this is really, really common that as soon as we start testing a piece of software, we start to really have to think about what the software is actually supposed to be doing. And this is a good thing. Often when we're testing software, we're not so much just looking for bugs in the software, but we're helping to refine the specification for the software under test. What I'd like to do now is look into this issue in a little bit more detail. If we think about a piece of software as a mathematical object-- and of course that's very natural for something like a square root function, but we can do this for any piece of software-- if we think of a piece of software as a mathematical object, what we'll find is the software has a domain of values. The domain of values is just a set of possible values that constitutes the set of inputs that the software is designed to handle. Correspondingly, every piece of software also has a range. This is the set of possible outputs for the software. And so let's look at what the domain and range are for the square root function. Earlier we said that we probably would like to return the positive answer for a square root, and we also said that we were probably not going to be able to compute the square root of a negative number. We can account for both of those facts in the square root's domain and range by making it a function from non-negative real-valued numbers to non-negative real-valued numbers. Alternatively, if we're implementing some sort of mathematical software, we might want to define square root for all real-valued numbers, in which case the domain is the full set of real numbers, and then the range is the set of complex numbers. Actual computer code is not going to be able to deal with a full set of real numbers. Most real-valued numbers are not representable on a computer at all. So instead of dealing with the reals, we're going to worry about floating-point numbers. So for example, we might want to make square root a function from non-negative floating-point numbers to the set of non-negative floating-point numbers. But this isn't really very convenient. What should we do if somebody calls square root with a negative number? What languages like Python often do is declare square root over the full range of floating-point numbers and give outputs that are the positive ranged floating-point numbers unioned with a different behavior, which is to throw an exception. And so this bottom domain and range for floating point is the situation we actually get in many real programming languages. So now given the specification, let's get back to software testing and ask the question, should we test this square root function with negative inputs? Now let's bring the discussion back to testing and take a little quiz. The question is, given this specification here where square root maps the full set of floating-point numbers onto either a floating-point number or a math exception, should we test this code with negative inputs? And the answers are either yes, we should test it with negative inputs, or no, we should not. So take a minute now to mark down your answer. The answer is that yes, we should test it with negative inputs. Negative inputs are part of the domain of a function, and one of the basic principles of testing is we should test computer programs with values sampled from their entire domain. Sometimes as a software tester, you'll test code with an input that looks like it should be part of the domain, and the code will malfunction, will crash with some sort of a bad error, perhaps maybe not throw an exception but rather actually exit abnormally. And what you'll do is you'll go to the developer of the function, you'll go to the developer of the software under test, and you'll say, "I think I found a bug," and they'll tell you something like, "Well, the input that you tried "is not part of the domain for the software." That's usually perfectly legitimate. As long as that kind of fact is documented, as long as it's clear to the users of the function, then restrictions on the domain of functions are actually a very valuable tool in practice because otherwise, without restrictions on domain, every function that we implement, every piece of software that we implement, has to contain maximal defensive code against illegal inputs. And in practice, this kind of defensive coding is not generally possible for all internal interfaces. For one thing, it makes code extremely slow, and for another thing, it clutters up code with error checks that make it completely impossible to find the code that's actually supposed to be doing something. So to reiterate, domain researches are a perfectly valid technique for keeping code simple in the case where these kind of assumptions about input domain are actually reasonable. In contrast with software libraries and software components where the domain is clearly defined and might be a subset of all the values that you can construct using the programming language, it's often the case that higher level software artifacts don't have the luxury of limiting their domains. And so one good example of this is an operating system. So here let's say we're testing a UNIX platform such as Linux or Mac OS X. I know we really haven't discussed this case yet, but I want to take a really quick quiz in order to start building up our intuition about testing. This example will be a very slightly simplified version of the UNIX read system call, and this is a system call supported by all UNIX platforms, so by Mac OS and Linux and others. All that read system call does is takes a file that's already open and reads some bytes out of it into the address space of the process that calls a read. And so the read call takes 3 parameters. It takes an integer called fd, which is just a small integer referencing a file that's already open. It takes a second current parameter called buf, which is a pointer to a memory region-- that is to say, a valid memory region in the process's address space. And finally, it takes a number of bytes. And so the quiz that we're going to take is which of the following test cases constitutes a useful test case for the read system call? Our 4 test cases are first, reading from file descriptor number 1-- this is always a valid file descriptor for a UNIX process-- from the address of b, and let's assume that refers to a valid memory region, and we're going to read 10 bytes. The second test case is also reading into variable b, 10 bytes, but it's specifying file descriptor -99999. The third test case is reading into file descriptor 1 using a pointer to an address which is almost certainly a bad one--this is just a random hex constant I just made up-- also reading 10 bytes into there. And the final test case reads into file descriptor 1 and to a valid address b but reads -33333 bytes. So go ahead and write your answer. Write all of the test cases--write the letters indicated-- all of the test cases that you think are good ones for a UNIX kernel in the space provided. So the answer is all of the above. And the reason is when we're testing something like an operating system kernel, we want to test it basically with all possible values of the parameters to the calls that it supports. And it turns out that there's a really good reason for this. The operating system boundary is a trust boundary. On 1 side of the operating system boundary we have us. We're the kernel implementers, and our job is to keep the machine running to provide isolation between users and basically to enforce all of the security policies that operating systems are designed to enforce. On the other side of the boundary we have them. These are our users. Our users might not actually be malicious. They might only be writing buggy codes. But the point is, regardless of whether the users of the operating system are malicious or just writing buggy codes, they're going to invoke system calls like read with crazy arguments, with all sorts of invalid arguments, and it better not be the case the operating system malfunctions when this happens. Therefore, if we're testing the operating system interface, we really, really, really need to be issuing calls like read with garbage. And in fact, this is one of the ways that we actually test operating systems. So the Linux kernel developers-- or actually, this tool works for multiple versions of UNIX--have a tool called crashme. Here we have the OS kernel, and here we have this program called crashme. What it does is it allocates a block of memory, writes totally random garbage into it, then it masks off all signal handlers--that is to say, system level exception handlers-- and it just jumps into the block of garbage-- that is to say, it starts executing completely garbage bytes. And what happens when you start executing garbage bytes is often the instructions are illegal, and the operating system kernel has to handle this. So over time what we end up doing is exploring--that is to say, testing-- a lot of operating system system calls, a lot of calls into the operating system, that contain really weird and unexpected values. And if the kernel goes ahead and keeps running properly and keeps trying to kill the crashme process, then the operating system kernel is succeeding. If the kernel ever falls over, if it crashes, then we've discovered a bug. We have to go ahead and fix the operating system. And the experience is that if you take a real operating system and you test it with a tool like crashme, you'll end up crashing the kernel unless the operating system has already been hardened with a tool like this. And so what we're getting at here is something very fundamental about testing. We're working towards another testing principle, and the principle is that if you have an interface or an API that represents a trust boundary, that is a boundary between part of the system that we trust and users on the other side who we can't trust to stay within the domain of the API that we're implementing, then we have to test that API with all possible representative values, not just ones that the developers happen to think are in the domain. So there's our principle. And it turns out that in practice, people are pretty bad at this-- that is to say, people aren't empirically very good at testing these interfaces with a full range of values. And this lies at the core of a lot of the security vulnerables that we see in real software today. So let's look at just one more example. We're going to be testing some sort of a graphical application. So we have a GUI application that's our software under test. And so we have to ask ourselves what's the domain and range of a graphical user application? The domain--that is to say, the set of legal inputs-- is basically just going to be the set of all possible GUI actions, so mouse clicks, keyboard events, swipes, etc. And the range is going to be possible states of the GUI application. So now it should go without saying that the size of the domain here-- that is to say, the size of the set of all possible GUI inputs-- is really pretty gigantic. There are a lot of possible combinations of inputs. And so now I have a quiz question for you. The question is what constitutes a set of good test inputs for a GUI application? Answer a) is just use the application normally. Answer b) is let a 4-year-old use the application for a while. Answer c) is use some sort of a tool to inject a stream of fake GUI events-- that is to say, a stream of automatically generated keyboard events, mouse clicks, and similar. And answer d) is reproduce GUI inputs--that is to say, mouse clicks and such-- that crashed our previous version of our software. So go ahead and write all of the answers that apply in the text box here. The answer is, of course, all of the above. All of these would be good ways to test a GUI application because a GUI interface, similar to an operating system interface, usually represents a trust boundary. And what I mean here is just the fact that it's usually the case that the GUI can't rely on a user to always present valid inputs. Therefore, a GUI application needs to be ready for basically anything. And it turns out that when my children were younger, one of them was really good at crashing Windows machines. You would just set him down at a Windows machine, and he could make it unusable within a couple of minutes of just pounding on the keyboard. Let's talk about a final implication of the interaction between testing domains and trust relationships between different entities. Let's say we have some sort of an interface here, and on both sides of it we have people we trust. Here's me, here are some of my teammates. And so the question we have to ask is can I trust my teammates, and can my teammates trust me to always generate inputs when using the various APIs that remain within the domain of those inputs? And of course the answer is generally no. In fact, I probably can't even trust myself to always generate inputs that are within the domain of acceptable inputs for APIs. And so what this brings us to is the idea of defensive coding-- that is to say, error checking--for its own sake to detect internal inconsistencies, and this is something that we'll get to a little bit later during this unit. So overall, testing software by calling into the APIs that it provides is fairly straightforward. We just make calls into the API and look at the results. But something inconvenient about real software is that software doesn't just provide APIs, it also uses them. What I mean here is that the software under test is going to be issuing calls into libraries and getting return values into the operating system and into virtual machines such as the Python runtime. So let's take, for example, just for the next couple of minutes the idea that the software under test is something like a web browser. One thing we can do is test the web browser using the APIs that it provides-- that is to say, using its graphical user interface-- and not worry about testing it with respect to the APIs that it uses. And so what kind of APIs is the web browser using? For one thing, it's talking to the network, it's talking to the hard disk through the operating system, it's talking to all sorts of lower level interfaces. And sometimes those APIs don't act as we would expect. Just as a simple example, let's take the case where our web browser is storing cookies-- here I'm trying to draw a chocolate chip cookie-- onto the hard drive of the local computer. Most of the time during testing, we expect the storage and retrieval of cookies to operate perfectly normally. But what happens if, for example, the hard disk is full when the web browser tries to store a cookie? Does the web browser crash? Does it mangle its internal state in some fashion and become impossible to use? Or does it gracefully stop storing cookies for that session and, for example, wait until there's more disk space free before it starts to store cookies again? Of course we'd like our web browser to do the right thing, whatever it is, but on the other hand, we need to actually test this. If we just hope that the software does the right thing, then one of the golden rules of testing is we shouldn't ever just hope that it does something; we need to actually check this. So the problem is that we have this fairly awkward problem where we don't actually control how the operating system responds to calls that we make. And what I mean by that is the awkward thing here is that we don't actually have direct control over how the operating system responds to calls that we make. So we can't easily just make storage of a cookie file fail. Rather, we would have to do something like create a full disk partition, arrange for our web browser to store cookies there, and then see how it responds. And so in this particular case, creating a full disk partition is awkward, but we could do it, but there are plenty of cases where lower level software has failure modes that we really can't easily simulate, regardless of how hard we work. So what we're going to do now is go back to our friend the UNIX read system call. Let's take another quick look at the UNIX read system call. And so this is our UNIX process's read from files, and so of course real UNIX programs are issuing calls to read constantly, maybe hundreds of times per second. And so earlier, we were concerned with the domain of the read system call-- that is to say, the set of possible valid arguments to the read system call-- and now we're concerned with the range because now we're not testing the UNIX operating system anymore; we're testing a program that runs on top of the UNIX operating system. And so it's the response of the operating system back to the process that concerns us here. We can see here that read returns the number of bytes read from a file, but there's an interesting fact here that read is allowed to read less bytes than you actually asked for. So it's going to return some number between 0 and count, but we don't know what number it's going to return. Another thing that read can do is just fail outright-- that is to say, it can return -1 to the application-- but it turns out that there are a whole lot of different reasons for that kind of a failure. We can see here that there are at least 9 different error conditions that read can return. We have EAGAIN, EWOULDBLOCK, EBADF, EFAULT, etc. And the point is for the application that's calling read, the operating system can return any of these values, and these values aren't all semantically equivalent. The application might have to do different things depending on which of these values it gets. And the point is it might be very hard as people testing the web browser to actually make the operating system read call return all of those different values. And until we've tested it with all of those different values, we're left with software whose behavior we probably don't understand, and, therefore, it's software that hasn't been tested very well. And so I'd like to tell you that there's some really great solution to this problem, but there really isn't. And the fact is that a lot of real programs that run on operating systems aren't prepared for some of these odder, stranger responses that the operating systems can deliver, and, consequently, when those things happen, the software malfunctions. So what we have is a fairly difficult testing problem. And in practice, there are a couple of different ways to deal with it. The first way doesn't actually have anything to do with testing, but it's such good advice that I can't resist giving it. And the advice is that you should always try to use low level programming languages or interfaces that are predictable and that return friendly error codes. What I mean is basically given a choice between using the UNIX system call interface which has all of these unfriendly features that we were just talking about and using the Python libraries, you'd almost always choose the Python libraries because they have far friendlier behavior and they have far fewer obscure error codes. Of course we don't always have the option of doing this. We probably can't implement a full size web browser in Python and expect it to run really quickly and be fully featured, so we're forced to use these bad style APIs sometimes. From a testing point of view, we can often use the technique called fault injection to deal with these kind of problems. Let's assume for the moment that we're using the Python library to create a file. We're going to be issuing a call like open /tmp/foo-- that's a path to the file we're trying to open-- and w specifies that we're opening that file with write permissions. So now if our Python application issues this call, we might have a fairly hard time testing the case where the open call fails because on most machines there's going to be a directory called /tmp, and so this call might almost always succeed. So what we can do instead is call a different function, my_open here in this case, which has an identical interface to the open call. And in fact, not only is the interface identical, but its implementation is also almost identical. So most of the time what my_open is going to do is simply call open. So what we have is a stub function that's almost functionally equivalent to open, but the key difference is we wrote the code for my_open, and what we can do with this is conditionally inside the my_open code we can sometimes cause the open system call to fail. And again, this is called fault injection, and it's a fairly common testing technique. So in practice, you have to be pretty careful with fault injection. One thing that can happen is if you make my_open fail too often, like, for example, it fails 50% of the time, then a program that's using it probably will never get off the ground. A large sized program that opens a lot of files will almost never succeed in opening all of them, and so we're not actually going to get to executing the interesting part of the file. So one thing we might do in my_open is have it initially succeed maybe for the first 100 calls, and after that it might do something like fail 1% of the time. So that's just an example. In practice, we would have to experiment with what kind of failure rates are good at testing the software that's actually under test. And so let's just take a quick quiz over this material. The quiz is faults injected into some software under test should be first, all possible faults; second, none; and third, faults that we want our code to be robust with respect to. The answer to the quiz is the third one. Generally, we don't need to inject all possible faults but rather just the ones that we expect might happen in practice and that we would like our system to be robust with respect to. To recap a little bit, we have our software under test, and it provides some APIs. Each API's collection of functions and most of the work that we have during testing is going to be calling these functions with various values and looking at the results. Additionally, the software under test is going to use APIs provided by, for example, the operating system or the programming language runtime, and a separate kind of testing is looking at how the software under test responds to return codes and similar given to it by the APIs that it uses. That would be great if both of these kinds of testing represented a complete picture of everything that we need to test, but in fact, that's not the case, and there are some added complications that we're going to talk about now. The issue is that the overt explicit interfaces that we see here don't represent all the possible inputs and outputs that we might care about. For example, on the input side it's completely possible that our software under test cares about the time at which inputs arrive. So it might be the case that our software responds differently if 2 inputs arrive very close together than it does if 2 inputs arrive separated by a large amount of time. And if this seems silly, just consider, for example, the software that processes mouse clicks. Two clicks very close together represent a double click, and that's interpreted very differently from 2 mouse clicks that occur farther apart in time which count as 2 single mouse clicks. Another example is something like a web browser where if the data corresponding to a web page is returned in a short window of time, this data will get rendered as a web page. But if the data that comes from the network is scattered across too much time, this is going to result in some sort of a time-out-- that is to say, the software under test, which is our web browser, will render some sort of an error page instead of actually rendering the data if it comes too slowly. Both of these examples that we just looked at are fairly easy to test because in each case, we have this sort of binary distinction between, in one case, the data arriving quickly-- that is to say, a double click or a complete web page arriving before the time-out-- and in the other case, we have data arriving too slowly-- that is to say, 2 single clicks or a web page that takes so long to arrive that we time out. In other cases, the timing-dependent input can make our lives significantly more complicated. So now we're going to look at kind of an extreme example of timing-dependent input being difficult to deal with. In the 1980s, there was a machine called a Therac-25. And what the Therac-25 was was a radiation therapy machine, and it was used to deliver a highly concentrated beam of radiation to a part of a human body where that beam could be used to destroy cancerous tissue without harming tissue that's nearby. And as you can see, this is not, obviously, an inherently safe technology. It's going to depend on skilled operators and also highly safe software in order to safely dose patients at the cancer site without harming the patients. So what happened with the Therac-25 was a tragic series of mistakes where 6 people were subjected to massive radiation overdoses and several of these people died. I'll make sure to include a link about these occurrences in the supplementary material for this course. If you actually take a look at it, you'll find that it's really quite terrifying reading. It's really a very scary series of accidents. The Therac-25 had a number of serious issues with its software, and we're just going to talk about 1 of them here. The Therac-25 was a largely software-controlled device, and it had, at the time, a fairly sophisticated controller. It turned out that the people developing the software put a number of bugs into it. The particular bug that I'm talking about here was a software bug called a race condition. And what a race condition is is a scenario where different threads of execution fail to be properly synchronized, with the result being that the software containing the race conditions can actually make mistakes. This particular race condition in the Therac-25 software involved the keyboard input to the radiation therapy machine, which is what the person operating the machine used to tell the machine how to treat the patient. And what happened was if the operator of the machine typed slowly, the bug was very unlikely to be triggered. And of course while the machine was being tested, the people testing the machine weren't very good at using it. They hadn't used it a lot, and so they didn't type very fast. But unfortunately, as operators in hospitals became more and more familiar with this machine, as they treated hundreds and hundreds of patients, what happened was these people got very good at operating the machine, they typed faster and faster, and eventually they started triggering this bug. And the effect of this bug, unfortunately, was to deliver massive radiation overdoses to patients. And, as I said, this led to several fatalities. And so the kind of quandary that this scenario raises for us as software testers is do we have to care about the time at which inputs arrive at our software under test, or can we not worry about that? And so obviously, for the Therac-25 and obviously, also for something like a Linux kernel, the time at which inputs arrive is relevant. On the other hand, unless we've been extremely sloppy, the square root function that we've been talking about won't care about the time at which its inputs arrive. Now we're going to take a really quick quiz, and the question is would you consider it likely that the timing of inputs is important for testing: a, software under test that interacts directly with hardware devices, b, software under test that interfaces with other machines on the internet, c, software under test that is multi-threaded, or d, software under test that prints time values into a log file. Go ahead and write all of the answers that you think are good ones into the text box here. I think to some extent the answers here are debatable, but the best answer that I would give to this question is definitely a software under test that interacts directly with hardware devices would consider the timing of inputs important. Also definitely software that interfaces with other machines on a network might consider the timing of inputs important. Software under test that's multi-threaded won't necessarily depend on the time at which inputs are delivered, but it certainly might, and finally, I would not consider software under test that prints time values into a log file to necessarily depend on the timing that its inputs are delivered. It's this kind of software--of which there are many examples-- a software that does care about time values because it's printing them but probably its logic doesn't depend on these time values because they're just being output to a device, and they're not being depended on in any meaningful way. Again, your mileage may vary on this, but my opinion is that a, b, and c are the best answers. What you'll notice here is that I gave you a quiz before telling you how to actually determine whether the timing of inputs is important for a particular software under test, and that was deliberate because I wanted to get you to think about the issues but also because there aren't really any great hard and fast answers here, so let's talk about this a little bit more. Basically what you want to do to figure out if timing matters for the inputs for the software under test is think about its specification. Think about what it's trying to do. Think about its requirements, and also look at the source code for the software, and you want to look for things like timeouts, things like timed sleeps, and basically values or computations that depend on the time at which things happen. If you see any of that, then it's strongly possible that you're going to need to take time into account while testing the software, and that means, for example, delivering some inputs to the software under test very quickly, delivering other inputs rapidly, and seeing how the software responds. Okay, so what we're doing here is continuing to dig into some of the nuances of testing software, and we're going to keep looking at things that can make testing hard. The final issue that I want to talk about in this general vein of things that can complicate testing is what I like to call non-functional inputs, and these are inputs that affect the operation of a software under test that have nothing to do with the APIs provided by the software that we're testing and nothing to do with the APIs that are used by the software that we're testing, and so what are these non-functional inputs? Well, they include things like context switches. What context switches are are switches between different threads of execution in a multi-threaded software under test. Now, of course, you shouldn't have to worry about context switches at all if your software under test only has a single thread, and the assignments we're going to be working on in this course are single threaded, but in general it's very common for complex software systems to convey multiple threads of execution. And so the issue is that these multiple threads of execution are scheduled along different processors on the physical machine that we're running on at different times, and it's the operating system that makes the decisions about what thread goes on what processor at what time, and depending on the scheduling of these threads bugs in the software under test can either be concealed or revealed, and the problem is that the timing of these context switches is completely not under the control of our application. It's under the control of the operating system, which provides these non-functional inputs, and this makes testing multi-threaded software actually really, really quite difficult. Let me give you another example of non-functional inputs. Some years ago, in the late 1990s, I spent a summer working at a company that made very, very fast networking hardware, and this hardware, for example, would let 2 PCs talk at multi-gigabit speeds using a switch, and so sort of the interesting thing about the software that we're developing that ran not only on the PC but also on a network card was that it was completely independent of the TCP/IP stack that normally provides reliable delivery for machines connected by networks but also it was supposed to provide reliable delivery of data even when we had errors in the network. And so a problem that we faced was this network in fact was extremely reliable. It would introduce bit errors into the data being transmitted maybe only once every several hours or maybe even only every several days, and so what we faced was a real difficulty in testing the end-to-end reliability software running on the PCs because the network was so reliable. And so what we often ended up doing was we would open up a switch, exposing all of the electrical contacts inside, and then we would take a key, a metal key, and run it across the contacts that were exposed from some of the chips on the inside of the switch. And what this would do is introduce a mass of number of very short-lived short circuits inside the switch, causing a huge number of bit errors causing the software running on the PCs to have to cope with all of these bit errors. And of course, either the network would glitch for a moment and then when it started up the data transfer would resume, or else it would fail to resume properly, in which case possibly the software crashed or possibly it delivered erroneous data, in which case we had some debugging to do. And so what I like about this example is we're able to access using this very crude mechanism of running a key across contacts sort of a deep level of the system and introduce errors that we weren't able to introduce at any other--at least conveniently-- at any other level of the software stack, and so this, again, was another kind of non-functional input to the system under test that by getting control over we were able to perform better testing of the software. What I want to do now is look at some of the different kinds of testing that exist, and so what this really is is a very, very brief survey. It's not intended to be exhaustive, and it's not intended to give you an orthogonal set of kinds of testing because there's going to be a bunch of overlap between these different categories. What kind of testing you often hear about is white box testing, and what white box testing refers to is the fact that the tester is using detailed knowledge about the internals of the system in order to construct better test cases. And in the next unit we're going to really look at white box testing in quite a bit of detail. The complement to white box testing is black box testing, and black box testing refers to the fact that we're not using detailed knowledge of the system's internals. We're rather testing the system based on what we know about how it's supposed to respond to our test cases. Unit testing is something we're going to spend a lot of time in this course looking at, and unit testing means looking at some small software module at a time and testing it in an isolated fashion, and so this is what we were doing with the bounded buffer example from the last unit and with the square root function that we were talking about earlier in this unit. The main thing that distinguishes unit testing from other kinds of testing is that we're testing a smaller amount of software. Often the person performing the unit testing is the same person who implemented the module, and in that case we may well be doing white box testing. But unit testing can also be black box testing. The goal of unit testing is to find defects in the internal logic of the software under test as early as possible in order to create more robust software modules that we can compose later with other modules and end up with a system that actually works. Besides the size of the software under test the other thing that distinguishes unit testing from other kinds of testing is that generally at this level we have no hypothesis about the patterns of usage of the software under test. In other words, we're going to try to test the unit with inputs from all different parts of its domain. Remember, the domain is the set of possible inputs for this piece of software. Unit testing is also a kind of testing that enjoys a great deal of good tool support, and so Python, in fact, has a number of frameworks for unit testing. It also has a number of frameworks for what are called mock objects, and what mock objects are are fake objects that we can bolt under the software under test that mock up the behavior of the larger software system in which this unit lives. And what we'll do is we'll include links to some of these tools in the supplemental material for this course. The next kind of software testing that I want to talk about is integration testing. Integration testing refers to taking multiple software modules that have already been unit tested and testing them in combination with each other. At this point often what we're really testing are the interfaces between modules, and the question is did we define them tightly enough and specify them tightly enough that the different people or the different groups of people implementing the different modules were able to make compatible assumptions, which are necessary for the software modules to all actually work together. And it often turns out that during integration testing we find substantial differences of assumption that have to be resolved before we can create a reliable integration of the individual parts. And so as a computer science professor, what I find is that groups of students are almost always successful at creating well-tested, reliable software modules, but that separate groups of students who individually create software modules, even when implementing to a fairly tight specification, often create software modules that fail under integration testing. It's really something that's quite a lot harder to do. Coming up with a software module that survives integration testing is really quite a lot harder often than creating reliable small units. The next kind of testing is system testing, and here we're asking a different question than we were asking with integration testing and unit testing. Here we're asking the question does the system as a whole meet its goals? And often at this point we're doing black box testing, and that's for a couple of reasons. First of all, the system is probably large enough by now that a detailed knowledge of its internals may not be very much help in creating good test cases. The second reason is really at this point we're not so much concerned with what's going on inside the system as with whether the system actually meets its goals or not, and so another thing that differentiates system testing from unit testing is the fact that at this level we are often concerned with how the system will be used. And the fact is, at this level we may not care about making the system work for all possible use cases. Rather, we would simply like to make sure that it performs acceptably for the important use cases. Somewhat orthogonal to unit testing, integration testing, and system testing is a testing technique called differential testing. In differential testing what we're doing is taking the same test input, delivering it to 2 different implementations of the software under test, and comparing them for equality. Differential testing is extremely powerful in the case where we have multiple versions. Stress testing is a kind of testing where a system is tested at or beyond its normal usage limits, and it's probably best described through a couple of examples. For example, with the square root function that we were talking about earlier we might test it with very large numbers or very tiny numbers. For an operating system we might test it by making a huge number of system calls or by making very large memory allocations or by creating extremely large files. For a web server we could stress test it by making many requests, or even better, by making many requests, all of which require the database to communicate with its back end. Stress testing is typically done to assess the robustness and reliability of the software under test. And finally, we have what might be my favorite kind of testing, random testing. In random testing we use the results of a pseudo-random number generator to randomly create test inputs, and we deliver those to the system under test. And sometimes we're just feeding raw, random bits to the software under test, but very often this can be much more subtle and more sophisticated than just throwing random bits at the software. And random testing is very often useful for finding corner cases in software systems, and the crashme program for Linux and other Unix kernels that we talked about earlier is a great example of a random tester, and we'll talk about more examples later on in this course. Now I'd like you to take a short quiz. I'm going to describe a kind of testing that somebody is doing, and you're going to tell me which of the kinds of testing that we've discussed that it's most like. We have a piece of software under test, and this is some sort of a really critical embedded system, maybe, for example, it's controlling airbags on a vehicle. And this vehicle is going to be out in the field for many years, and so cars get subjected to fairly extreme conditions. They have a lot of heating and cooling cycles. They get exposed to salt and mud and water, and so the cars are in a fairly harsh environment. And one of the things we're going to want to know is how the software responds to defects that get introduced into the automobile's wiring as this vehicle ages in the field, that is to say as people are driving it around the salty roads in Michigan or whatever. And so what we're going to do is using maybe a simulator or something we're going to make it look like a wire that's used for this processor that's used for software running on this processor to talk with other modules, so cause a really big distributed system, so the software under test here is just one of many pieces of software running on the car. We're going to use some sort of a simulation module to simulate a noisy wire, and that basically means, for example, a wire whose insulation has rubbed off and that is not perfectly conducting signals anymore. It's getting junk injected into it because it's banging against the car's frame or something. The question is what kind of testing are we doing? And your choices are regression testing, unit testing, stress testing, or white box testing. Please right now click the box that best describes what we're doing. As far as I'm concerned, the best answer here is stress testing. We're not doing regression testing because regression testing always involves taking inputs that previously made the system fail and replaying them against the system, and the way I described it that wasn't the case here. We're probably not doing unit testing because the software under test running on this processor, even if it's just one part of all of the software running on a car, still has many modules. It has an operating system. It has some communication software. It has the actual application functionality, whatever it's doing, controlling airbags, and so we're most likely not doing unit testing, although if you answered unit testing that's not a bad answer. But my personal judgement is that it's not the best one, and we're certainly not doing white box testing because we haven't considered at all the structure of the software under test here. This is really a kind of black box testing. All right, in this next example software under test is some sort of a web service, and what I'm going to do is I'm just going to go ahead and roll this out onto one of the machines in our cluster, but I'm not going to expose this to most of our users. What I'm going to do is expose this machine to some small fraction of our users who have been selected, for example, based on their willingness and desire to use new features. The question is what kind of testing is this most like? Is this integration testing? Is it differential testing? Is it random testing? Or is this system/validation testing? Okay, so please take a second to go ahead and mark what you believe is the kind of testing that's the best match for what it is that we're doing here. What I think the best answer is is that we're doing system or validation testing. That is to say we're testing the whole system as a unit, and we're trying it on some small subset of our users to try to validate the fact that the changes we've made are good and that they work and they're not going to cause any sort of major problems when we roll it out for the entire user base. We're not doing random testing because there's no random element here. We're not doing differential testing because we don't have multiple implementations of anything. We have a single system that we're looking at. We're not doing integration testing because the character of the testing that we're doing here does not have the flavor of putting multiple modules together that haven't been tested together previously. Rather, we're rolling something out and seeing what happens. Okay, let's take a quiz. In this scenario we have some large software under test, and part of it is a library that's been flaky and has been giving us problems. Let's say this library is implementing numerical functions, and these numerical functions have been sometimes throwing floating point exceptions and crashing our system. What happens is the vendor has given us a new version, but what we don't want to do is just roll that directly into our production system because in the past we've gotten burned by that sort of thing where we have deployed a new version and it contains flaws that were worse than the flaws in the previous version. What we're going to do is we're going to spend some time checking this thing out and trying to make sure that even if it's not that great it's at least not worse than the previous version. The question I have for you is what kind of testing is this most like? Is it unit testing? Is it white box testing? Is it black box testing? Or, on the other hand, is it stress testing? Okay, please take a minute to mark what you think is the best answer. The answer is it could have been any of these, but I think the best answer is unit testing because what we're doing is we're taking a software module, testing it in isolation to see if it's up to our standards before we integrate it with the rest of our software. Now, the type of testing that we're actually doing could have been white box testing, could have been black box testing, and could have been stress testing. But I didn't give you enough details about what we were doing to allow you, I believe, to conclude that any of those was definitely what we were doing, so what I think is the best answer here is unit testing. Now I'd like to talk about being great at testing software, and this involves a number of different elements. First of all, we need to recognize that testing and development are fundamentally different roles, even if they're often played by the same exact people. A developer's primary attitude is I want this code to succeed. A tester's main attitude is I want this code to fail, and of course, the reason the tester wants the code to fail is that the tester's end goal is creating stronger code which later on doesn't fail. If we look at these requirements a little bit we can see that for the same person to be a great tester and a great developer there might be a little bit of doublethink involved, and so those of you who read Orwell will know that the doublethink is the ability to hold 2 contradictory beliefs in one's mind simultaneously, and there is a bit of that required in order to be a great tester and a great developer. And of course, really the contradictory nature of these 2 beliefs is only apparent because, of course, the developers and the testers in the end want the same thing, which is to create a great software. The second element of being great at testing is to learn to test creatively, and I showed the example earlier of testing an ARM assembler and disassembler by exploiting the fact that they were inverses of each other and also exploiting the fact that the ARM instruction space could be fully enumerated, and that's something that I consider to be a great example of creative testing, and if we can think of creative ways to test we often do a much better job than rote testing, and furthermore, do a much better job than just rote testing with the most obvious inputs. The third thing is that great testers never ignore weird things that they notice. It turns out that at least in my experience it's very often the case that we get little hints of things wrong with the software that lead to threads that if tugged on would have led us to discover problems that were really quite serious. On the other hand, if we see these little things wrong and we paper over them, we ignore them, we end up not finding those problems until later. I'm going to end up today with the claim that great testing is a lot of fun because it's fun to break software, and it's very satisfying to produce software that's really great because it's been well tested, and great testing can also be profitable. And what I mean here is that testing is a separate career at many companies like Microsoft and that companies like Google and Mozilla offer bug bounties, and what this means is if you find a security critical bug in, for example, Google Chrome, they'll pay you up to $20,000, and this isn't at all theoretical. I was recently talking to a software testing researcher who wrote an automatic tester and applied it to Chrome and to Mozilla, and their research group, over the course of a few months, made about $50,000. You can usually think of problems in released software as coming from things that people forgot to test and what that means is testing was inadequate and the developers were unaware of that fact. What we're going to look at this week is a collection of techniques called code coverage where automated tools can tell us places where our testing strategy is not doing a good job and that's a very powerful and useful-- One of the trickiest things about testing software is that it's hard to know when you've done enough testing and the fact is, it is really easy to spend a lot of time testing and to start to believe that you did a good job and then to have some really nasty bug show up that are triggered by parts of the input space that we just didn't think to test. So let's look what's really going on, so we're going to start with the domain range diagrams that we known off. We're going to have some test cases into domain and after a while we're going to start pretty good about what we're been doing. What's going to turn out is without really knowing it, our testing is being compliant to some small part of the input domain and the problem, is that even the small part of the domain may contain an infinite number of test cases or else a finite numbers of test cases that is large enough that its for practical purposes. It's not distinguishable from infinity and of course what's going on is we're going to do test cases other parts of the domain we didn't think to test, putting the results and outputs that are not okay. It really depends on how we've broken up the input domain. For example, let's think about the case that we're testing Therac-25 radiation therapy machine that I used an example in the last lecture. It's not might be the case but all of the inputs that we are going to test, the once in this region are the once where we happened to type the input to the machines slowly. We simply didn't realize that there's a whole other part of the input space that has to be treated differently that happens when we type the input fast and of course its in that region where we triggered the raised conditions that we're leading to massive overdoses. Similarly, if you remember from lecture one, we have a small Python program called the Python runtime to crash. The distinguishing feature of it seem to be a large number of cascaded if statements. It's pretty easy if we're testing Python programs to remain in the part of this space where for example we have <5 nested in these statements. Over here is another region containing programs with 5 of more if statement nested and these are ones that cause for example, the Python virtual machine to crash. To take an even more extreme example, let's say that we're testing some software that somebody has inserted in the back door in. Well in that case, there's going to be an absolute infinite test of all part of the input domain. Maybe wherever here that triggers the back door cause remember if your putting a backdoor in code you don't want to trigger it accidentally and that's going to lead to something extremely bad happening over here. We didn't tested inputs triggering the back door because we just didn't know it was there. So what we'd really like is some sort of a tool or some sort of a methodology that if we are in fact testing only a small part of the input domain for a system what we really like is some sort of an automated scoring system that looks at our testing effort and says to us something like, your score is 14 out of 100. Your not doing a good job testing the system. Keep trying. And that's what today's lecture is going to be about. It turns out there's a lot of reasons we do not want to know the sign of score to our testing effort. Probably the main one is that this helps us find part of the input domain that need more testing. So for example, if we can increase our score by testing this part of the domain, we're naturally going to be led to do that and our testing efforts will improve. Other reasons to assign a score to a testing effort are that we might be able to argue to a boss which is some sort of a certifying authority that we've done enough testing. Similarly, we might want to argue that the system we're developing has not been tested well enough but its not yet safe to deploy and that it needs more testing and a testing score can provide a numerical justification of that sort of way. Probably, it would be nicer if we could take a large test suite one maybe that takes several days to run and identify parts of the test suite that are completely redundant. That is to say parts of the test suite that tests exactly the same parts of the input domain. Even though they occupy parts of the input domain have roughly the same testing effect on the system to assigning a score to our testing efforts and let us do that as well. This time, let's talk about some they called partitioning the input domain. So, we are going to start with some software under test and it’s going to have a set of possible inputs, an input domain, and of course, this input domain usually consist of so many possible test cases, but there’s no way we can possibly test them all. Speaking historically, well people would've often been interested in his ways to partition the input domain for piece of software under test into a number of different classes so that all of the points within each class are treated the same but the system under task. And while constructing these classes, we are allowed to look at the implementation of the software, we are allowed to look at the specification, we are allowed to use even our vague suspicions that we have. We can use anything we want in order to create these partitions. So, for example, we will have some subset of the input domain. For purposes of finding defects in the system under test, any point within that subdomain is as good as any other point within that subdomain. So, basically, when testing the system, which pick an orbitary point, execute the system on it. Look at the output, and if it is acceptable, then we’re done testing that class of inputs. So, obviously, in practice, sometimes this partition is going to fail, and by fail, I mean that the thing that we thought was are class of inputs that are all equivalent with respect to the system under test isn’t really, and in fact, there is a different class hiding within this class which triggers a bug even though the original test case didn’t. And so, when that happens, we can sort of blame the partitioning scheme. We can say that we improperly partition the input. The problem with this sort of scheme is that we can always blame the partitioning, and the unfortunate fact is the original definition of this partitioning scheme didn’t really give us extremely good guidance in how to actually do the partitioning. All that settle as we had to do it. In fact, this sort of scheme hasn’t worked out for large systems under test. We’re talking complex software like real time imbedded systems or operating systems or other things. And so in practice what we ended up with is not this idea of coming up with a good partitioning for the input domain rather than a notion of test coverage. What the test coverage is doing is to try to accomplish exact the same thing that partitioning was accomplishing, but it goes about in a different way. Test coverage is an automatic way of partitioning the input domain with some observed features of the source code so let me say what I mean. So one particular kind of test coverage that we might aim for that is sort of an easy kind of test coverage is called function coverage, and function coverage is achieved if we managed to test our system in such a way that every function in our source code is executed during testing. We will be dividing our input domain in chunks where any test case in this part of the input space is going to result, for example, in a call to foo. So now, there's going to be some different subset of our input domain and any point in this subset of the input domain when used as a test input is going to result in a different function in the system under test--let's say bar being called. We can keep subdividing the input domain for the software under test until we have split it into parts that results in every function being called. So now, of course, doing this in theory is easy. In practice, we start with a set of test of cases, and we run them all through the software under test. We see which functions are called, and then we're going to end up with some sort of a score. So, for example, some sort of a tool that is watching our software execute can say-- we called 181/250 functions. And so what this kind of score is called a test coverage metric. It means that our test cases so far have covered 181 of the functions out of the 250 that we implemented. And so now that we have achieved this goal that we had, which is assigning a score to a collection of test cases. The next thing we have to ask is this score any good? Is that a good test coverage to have executed 181/250 functions? With this example, it's probably not. So, we can do is for each of the functions that wasn't covered, we can go and look at it. And we can try to come up with the test input that causes that function to execute. There is some function baz here, and we can't seem to devise an input that causes it to execute, then there are a couple of possibilities. One possibility is that it can't be called at all. It's dead code. Another possibility is that we simply don't understand our system well enough to be able to trigger it. Either way, there is something possibly suspicious or wrong. Test coverage let’s us assign a score to a collection of test cases and so let's be a little bit more rigorous about it. So test coverage is really is a measure of the proportion of a program exercised during testing. So, for example, we’ve just talked about measuring a number of functions out of the total number of functions or exercise by some test that we had. What’s good about test coverage is it gives us a score, gives us something objective that we can use to try figure out how well we’re doing. Additionally, when coverage is less than 100%, that is to say, as in our example, where we had failed to execute all of the functions in the software under test, we know what we need to do to get full coverage. We know what the functions are that we need to execute. Now, we simply need to construct test cases and execute those functions. So, these are the good things about test coverage. On the other hand, there are also some disadvantages. First of all, test coverage, because it is a white box metric that is derived from the source code for our system is not good at helping us find bugs of omission, that is to say bugs were simply left out something that we should have implemented. The second drawback is it could be really hard to know what a test coverage score less 100% means and in safety critical software development would sometimes done is requiring 100% test coverage of a certain coverage metrics and that sort of removes this problem, it means that we don’t have to interpret scores less than 100% because we’re not allowed to ship our product until we get 100% test coverage. For a larger, more complex software systems where the standards are correct and not as high as they are for safety critical systems, that’s often the case, but it’s difficult or impossible to achieve 100% test coverage. Leaving us with this problem we’re trying to figure out what that actually means about the software. The third disadvantage is even 100% coverage doesn’t mean that all bugs are found and you could see that sort of easily by thinking about the example why we’re measuring our coverage by looking at the number of functions we executed. Just we executed some function, of course, it doesn’t mean that we found all the bugs in that function. We may not have executed very much of it or may not have somehow found very many of the interesting behaviors inside that function. We'll that's enough abstract ideas for now without coverage. So, let's take a concrete look of what a coverage can do for us in practice. How it can help us do better testing. What are we going to do here is look at some random open source Python codes that implements a splay tree and before we go into the details let me briefly explain what a splay tree is. And broadly speaking a splay tree is just a kind of binary search tree. And a binary search tree is a tree where every node has two leaves and it supports operations such as insert, delete, and lookup. The main important thing about a binary search tree, the keys have to support an order in relation. And so an order in relation is anything that assigns a total ordering to all the keys in the space. Just a simple example if we're using integers for keys then we can use less than for order in relation. Or for example, if we're using words as our keys, then we can use dictionary order. And so again, it doesn't matter what kind of a data type the key is in a binary search tree. All that really matters is the keys we're going to use to look up elements in the tree have this order and relation. The way the binary search tree is going to work is, we're going to build up a tree under the invariant that the left child of any node always has a key that's ordered before the key of the parent node and the right child is always ordered after the parent node using the ordering. And so, hopefully which we can see now, is that if we build up some sort of a large tree with this kind of shape, we have a procedure for fast lookup. And the way that lookup works is, when we're searching for a particular key in the binary search tree, we only have to walk one path from the root down to the leaves. We always know which subtree might contain the key that we're looking for, and of course, we have to actually go down into that subtree to see if it's there, but the point is we only have to walk one path to the tree. And the upside is that generally, operations on this kind of a search tree, we're going to require a number of tree operations logarithmic in the number of tree nodes. So for example, we if we have a tree with a million nodes and if that tree that we build ends up being relatively balanced, then usually, we're going to expect to do about log base two of a million tree operations as part of a lookup, or insert, or delete and so we're going to end up doing roughly 20 operations. So, you can see some speed number of operations is far lower than a number of nodes in a tree that this is generally considered to be an efficient kind of a data structure. Looking at that, there's a few extra things you need to know about splay trees, and the first is that it's a self-balancing tree which means that if you insert nodes in sort of order, remember we have a binary search tree but we call this stability tree that looks something like this. And so as you can see, this is a very bad kind of a binary search tree because here to do a lookup, we're going to have to walk all of the nodes in a tree and we'd lost this nice logarithmic property that made lookups and search and deletes extremely fast. The way a self-balancing binary search tree works is as you add elements to the tree that has some sort of a balancing procedure that keeps the thing approximately balanced so that tree operations remain very fast. And if you look at the literature, it turns out there are tons and tons of different data structures that offer self-balancing binary search trees and the fast ones of these end up being somewhat complicated and the splay tree is really one of the simplest examples of a self-balancing binary search tree and the implementation that we're going to look at in the minute contains something like a 100 lines of code. So the other thing you need to know about splay tree before you get into the code is that it has a really cool property that when we access the nodes, let's say we do a lookup of this node here which contains 7, what's going to happen is as a side-effect of the lookup that node is going to get migrated up to the root and then whatever was previously at the root is going to be pushed down and possibly some sort of a balancing operation is going to happen. But the point is, is that frequently accessed elements end up being pushed towards the root of a tree and therefore, future accesses to these elements become even faster. So this is sort of a really nifty feature of the splay tree. So what we're going to do now is look at an open source of Pythons splay tree, specifically a random-based structure that I found on the web and that happens to implement a splay tree--it comes with its own test suite and we're going to look at what kind of code coverage that this test suite gets on the splay tree. So here is what the splay tree code in an editor called Emaxx, and this is my personal editor of choice. It's doing syntax highlighting and indenting, very much likely Udacity online IDE would do for you. But here what I'm doing is running this on my local PC here. And so we can see is, we have a splay tree class, it's a port and insert method. The remove method, this takes a key out of the tree or couple of other operations. So insert, remove, and look out for the basic operations supported by any binary search tree, but many implementations support additional operations. So find the look out for operation for this tree. And then there's the splay operation and what the splay does is, it moves a particular key up to the root of the binary search tree up to the root of the splay tree. This serves as both the balancing operation for the splay tree and also the look up. So, the splay routine involves a couple of screenfuls of codes but it's still fairly simple. And now, let's look quickly at the test suite first. So the author of this open source splay tree for the test suite. In this test suite, you can see imports the Python module unit test And what unit test does is basically provides a little bit of infrastructure for running unit tests. And so, what it's going to do is automatically call these different functions, which are defined for an object inherited from the case test framework And you can see here, that what we do is for example, we have a setup routine that initializes a list of keys. We make ourselves a new splay tree and insert those 10 elements into the tree. We have a test insert function which loops over all of the elements of the tree and asserts that they're found. We have a remove test function which is going to be called last in our test suite. The functions beginning with a string TEST are called in alphabetical order, and this happens through the last one. This is going to remove all of the keys from the tree. Then finally, we have a function which is going to insert a large number of elements in the tree. It's going to insert 40,000 elements into the tree. It's going to make sure that they get staggered by a gap of 307. This is basically going to end up stress testing our splay tree a little bit. Finally, we have a routine that tests whether the tree correctly knows that it's empty. Okay, so this is our sort of minimal unit test. What we can do is, we can run this and this is going to take just a couple of seconds to run. Okay, so all of those tests took about 1.6 seconds to run and what I'm going to do now is run the same test under a code coverage string. So, I'm doing here is erasing code coverage date which has been previously stored in this directory. Running the splay test under the coverage framework which is basically going to run an instrument and version of the code and we'll talk about how this all works a little bit later and what it means, and then we're going to generate some HTML. So, what you can see is it ran quite a bit slower this time because instrumenting the code to measure a coverage has a lot of instructions to the code and this takes extra time. Okay, now we're going to look at the output. Here we are looking at the output of the code coverage tool and what it's telling us is that when we run the splay tree on it's own unit test, out of the 98 statements in the file, 89 of them got run, 9 of them failed to run, and we'll talk about this excluded business later. So, the language didn't get run are marked in red. And so this first one we're going to skip over for now and let's look at the second example. So, the second line that didn't get run is in the splay trees insert function, but what we can see is that the test suite failed to test the case where we inserted an element into the tree and it was already there. And if you look at this, this looks fairly harmless, we're just erroring out but we're not going to worry about this one right now. So, let's move on a little bit. So, here we got something a little bit more interesting. So here we're in the splay tree's removing function. So, we're going to remove an element from the tree. And so what you can see is that the first thing this function does is, splays the tree based on the key to be removed, and so this is intended to draw that key up to the root note of the tree. If the root note of the tree does not have the key that we're looking for, then we're going to raise an exemption saying that this key wasn't found. But the thing's noticed here is that this wasn't tested. If we look below here and go on to the body of the delete function, we see a pretty significant chunk of code that wasn't tested, probably want to go back and revisit this. And so, let's take a step back and think about what coverage tool is telling us here. It's basically showing us what we didn't think to test with the unit test that we wrote so far. So, what we're going to do is go ahead and fix the unit test a little bit in order to test this case. In order to test the case of removing an element from the tree where it's not there, so let's go ahead and do that. We're going to the test suite, we're going to go to the test removed. After we've removed everything from a tree, we're going to remove an element that we know is not on the tree and of course after we removed everything from the tree, anything we choose should not be on the tree so minus 999. We'll work this well as any, so we're going to go ahead and save that and run the coverage tool again. So this time something interesting happens. What happened is the removed method for this spaly tree on removal of minus 999 so on the line that we just added causes an exception re-thrown in this splay function and so let's go back and look at those splay tree code. So when we removed an element from the tree but wasn't there, it's suppose to raise the exception key not found in tree. On the other hand, what its actually doing is failing quite a bit below here in the middle of the splay function when the code does a comparison against an element of type none and so that's probably not what the developer intended by adding just a little bit to our test suite we seem to have find a bug not anticipated by the developer of the splay tree and I think this example is illustrative for a couple of reasons. First of all the coverage tool, the very first time we run it told us something that we didn't know and this my experience in general that this is what happens when you use a coverage tool. Its basically a very similar to the first time you run a profiling tool on a piece of code where it turns out that usually this functions are using up CPU time are not the ones that you necessarily thought were using up CPU time--well, coverage tools are very similar. It often turns out that the stuff that you thought was going to run might not be running only some of it but often turns out that some of the stuff that you thought was going to run doesn't get run. So it told us something interesting and that's nice. Now on the other hand if the coverage tool haven't told us anything interesting that is to say if it told us that everything that we hoped was executing when we run the unit test was executing well then that's good too--we get to sleep a little easier. The second thing you noticed is a much more settle point and this point is that we're out of the test case to execute this one of code but it turned out that the bug wasn't right here. The bug was somewhere completely different buried in the splay routine and if you go back and look at the coverage information, it's going to turn out that the splay routine is entirely covered--that is to say every line of the splay routine was executed during the execution of the unit test for the splay tree. This serves to make a couple of points--first of all just because some code was covered especially at the statement level this does not mean anything about whether it contains bug in it. It just means that it run at least once. The other thing is that and the second thing is we have to ask the question, "What do we want to really read into the fact that we failed to cover something?" The thing to not read into it is a bit of failed piece of coverage is a mandate to write a test that covers this test case. That's what we did, that's not a good general lesson rather the way we should think about this is that the coverage tool is giving us a bit of evidence. It has given an example suggesting that our test suite is poorly thought out. That is to say that our test suite is failing to exercise functionality that is present in our code and what that means is we haven't thought about this problem very well and we need to rethink the test suite. So to summarize that, when coverage fails its better to try to think about why we went wrong rather than just blindly writing a test case and just exercise the code which wasn't covered. We just looked to an example where measuring coverage was useful in finding a bug in a piece of code. What I want to show you now is another piece of code. This would be a really quick example. The coverage is not particularly useful in spotting the bug. And so, what I have here on the screening is a broken function. The job is to determine whether a number is prime. That is to say, it doesn't correctly do its job of determining whether the number is prime. So, let's look at the logic in this function. So, this prime takes a number. If the number is less than or equal to 1 or divisible by 2, then it's not prime. Now, once the number passes that test, what we are going to do is loop over every number between 3 and the square root of the number and check if the original input number divides evenly by that number. If it does, then of course, we don't have a prime number. And if all of those tests failed to find a divisor, then we have a prime number. And so, as can you see, there is a little bit of test code here that checks how this function responds for the numbers 1 through 5 and 23, 24. And so, what to do is let's just run this code. This code has correctly computed that 1 is not prime, 2 is not prime, 3 is prime, 4 is not, but 5 is. Similarly, 20, 21, and 22 are not prime numbers. They all have divisors 23 is prime, and 24 is not. So, for this 10 examples, this prime function has successfully identified whether the input is prime or not--so, let's run the code coverage tool. We got the same output. Now, let's look at the results. So, we can see out of the 20 statements in the file, all of them run, and none of them failed to be covered. Statement coverage gives us a perfect result for this particular code and yet another set is wrong. Here of the same code and ID, so let's run out. The output is what we expect, and what I would like you to do is do 2 things. First of all, at least 1 test case, where you check a value for prime and the code here returns the wrong result. And then second, create a new function is prime 2 that fixes the error. That is to say a correctly identifies for any natural number input. That is to say any input 1 or larger whether it's prime. So, here we are back with our broken formality test. And what I have done is I've changed the input a little bit so let's run these new inputs. And what we can see is the function has not made a couple of mistakes. For example, it is indicated that 9 is prime, and it is indicated that 25 is prime or of course neither of those numbers is prime. So, let's go back to the code and look the mistake. The algorithm here is generally a valid one. It's fine to first of all special case the even numbers, and it's fine then to loop between 3 and the square root of the number. So, the problem is that Python's range function does not loop through 3 and the square of the input number rather it loops between 3 and 1 less than the square of the input number. So, we can fix this, of course, easily by adding 1 to the square root causing it to actually test against the full range that it needs to test in order to have a successful formality test. So, let us run this code and see what happens. So, 6 is not prime, 7 is. None of the rest of these numbers are prime except for 29. So, the question is what happened here? Why did test coverage fail to identify the bug? And the answer is sort of simple--statement coverage is a rather crude metric that only checks whether each statement executes once. Each statement executes at least once that lets a lot of bugs slip through. For example, we can have a statement execute, but it computes the wrong numerical answer or what happened here a loop executed, but it executed for the wrong number of times computing the wrong result. The lesson here is we should not let complete coverage plus a number of successful test cases fool as into thinking that a piece of code of right. It's often the case that deeper analysis is necessary. Now, we’re finally ready to take a detailed look at several coverage metrics, and the first thing to keep in mind, there is a large number of test coverage metrics out there. In the supplemental material for this class, what I’m going to do is link to a nice article, which list 101 test coverage metrics. At first, that sounds a little bit of a joke, but if you take a look at the list, what you see is that it’s not too hard to imagine situations in which most of these metrics are actually useful. And if you read it, you can see that these things would probably actually find bugs in real software systems. So, what I’m going to do here is talk about a fairly small number of coverage metrics that matter for everyday programming life and I’m going to talk about a few more that are interesting for one reason or another because it can get inside into software testing for which you probably want actually use in practice. So, the first metric we’re going to worked out is called statement coverage and this is in fact the one that is measured by the fault by the Python test coverage total where we looked at. And so, you already had a pretty good idea of what it does, so let’s just go through it in a little bit more detail. So, let’s just use this very simple four line codes that I put as an example and let’s try to measure its’ statement coverage. So, let’s say we call this code with x=0 and y=-1. Well, in that case, this x is 0 test is going to pass and so y is going to be incremented by 1 making y=0. Now, if y=0 will also passes in sort of an increment acts. The four statements in this code fragment, if we enter this code fragment with x=0 and y=-1, all four will be executed. So, this will give us a statement coverage of 100%. So, that’s pretty obvious. It’s really quick, so let’s sort of call it with different values. So now, we call this code with x=20 and y=20. Both tests will fail, and so, we'll end up executing both of the test but neither of them branches of the test, and so, we will end up with a code coverage or 2/4 statements or 50%. While we’re on the subject of statement coverage, I would like to also mention line coverage. This is very similar to statement coverage, but the metric is tied to actual physical lines in the source code. And so, in this case, there is only one statement for each line, so statement coverage and line coverage would be exactly identical. But on the other hand, if we decided to write some code that had multiple statements per line, line coverage would conflate them where a statement coverage would consider them individual statements, for most practical purposes, these are very similar. So, statement coverage has a slightly finer granularity. So on the subject of statement coverage, we will take a very short quiz. The quiz is going to have two parts, both of which stem from a little statistics module that we have here. The function stats is defined to take a list of inputs and what the function does is computes the smallest element in the list, the largest element in the list, the median element of the list-- that is to say if we order the list numerically the middle element where the average of the two middle elements if the list has an even number of elements and finally, it computes the mode of the list where the mode is the element which occurs the most frequently in the list. In the case where the list is bimodal--meaning it has two modes or multimodal--more than two modes--we'll print all of it. What we do here is test the stats function extremely badly, so I'm going to create a list containing only the #31 and I'm going to call stats on the list and so let's look at the coverage that we get. We can see here that even my fairly bad test managed to cover 29 statements in the stats function but these statements are uncovered and it's shown right here. Your assignment for the first part of this API quiz is to--early collection of test cases for it which is 100% statement coverage. What I mean by that specifically is that your job is to construct a several lists, calls stats on them, and cover all statements. What I'd like you to do is think about it a little bit, try to visualize the effect that your inputs are having on the code, try to come up with the corruption of inputs that gets false statement coverage on the first try but then, of course, check your answer using the coverage tool. And the key really was simply calling the stats function with two list. One which had an even number of elements, the other with an odd number of elements and then furthermore at least one of the list had to not read the degenerate list containing only one element. So if you do something like what I did here, you'll achieve full statement coverage and let's just look at that quickly. So here we can see that out of the 34 statements in the program, 34 will run and none were missing--so we achieved full statement coverage. Well, let's take another quick quiz and this one like the broken prime number function I showed you is designed to pry into the limitations of coverage metrics. So your job now is to insert a bug into the stats module, that is to say make it wrong but in a way that's undetectable by test cases that get full statement coverage. So test 1 is a function that you write which contains either the test cases you just submitted or different ones. These test cases together need to achieve 100% statement coverage for the stats function but we must not reveal the bug--that is to say you're broken stats function needs to return the correct answer for the test cases presented here. The second thing I want you to do is define a function test 2, which also calls the stats function, and this one should reveal the bug. Your assignment is to break the stats function in such a way that the flaw is undetectable by test cases that you design would get a 100% test coverage, but also you need to show us what the flaw is by supplying a second test case. We''ll I hope you enjoyed trying to fool the test coverage metrics. We'll it turns out there's essentially an infinite number of ways to do that. The way I chose was to insert a call to the absolute value function, so here we're iterating over the values and the list of numbers looking for the largest one, the smallest one, and competing frequency leading to the mode. What I did here is so that i is equal to the absolute value of i. Now of course when the inputs are positive as they are in the first two test cases that achieved 100% statement coverage, everything is fine. On the other hand in the other test cases, what I do is call them with the list containing negative values and of course, it's going to compute the wrong thing for them--right. So the list contains -33 and -34, and clearly the minimum value in that list is not 33 and the max is not 34. So what you can see here is I've met the requirements of the assignment. As I said, really there are a lot of different ways you could've accomplished this effect and realistically, for purposes of this assignment which was to get you to think about the limitations of coverage a little bit, any of them would be just as good as the other. Back to coverage metrics. We just talked about statement coverage, which is closely related to line coverage, but it's a bit more fine-grained, and now let's talk about what is probably the only other test coverage metric that will matter in your day-to-day life unless you go build avionics software. Branch coverage is a metric where a branch in a code is covered, if it executes both ways. For example, to get 100% branch coverage for the statement that tests whether x = 0, it would be needed to be executed in a state where x was zero and also in a state where x was not equal to zero. And so, in many cases, branch coverage and statement coverage have the same effect. For example, if our code only contained if-then-else loops, the metrics would be equivalent. On the other hand for code like this that's missing the else branches they're not quite equivalent. We can take as inputs to this code x =0 and y is -1. These inputs were sufficient to get 100% statement coverage. On the other hand, these are not sufficient to get 100% branch coverage. What happens is these cause the taken branch of the if to be executed but not the else branch. Then the taken branch of the second if to be executed, but not the else branch. There are different ways to score branch coverage, but one way we can do it is there are two ways to take this branch, two ways to take this branch, so we could say this is 50% branch coverage. The other thing we could do, however, it do what our Python module for coverage is going to do, we could say that both of these branches were partially executed. That is to say, one of their possibilities was realized during testing. No branches were completely missed, and no branches were totally covered. So, let's go ahead and see how our coverage module tells us this. Right, so as you see here, I've typed into an editor window the same code that I wrote down by hand on the left part of the slide. And so we're going to invoke the function foo with x being 0 and y being -1. Let's see what happens. We're going run this under the coverage tool, but this time we're going to give the coverage run command, and argument--branch. That simply tells it to measure branch coverage instead of just measuring statement coverage. It's again going to render some HTML as a result, and if we look at the output, it's going to tell us that out of six statements all six of them were run, and there were zero missing statements. So, at the statement level, we've achieved 100% coverage. On the other hand, at the branch level, we had two branches that were partially executed-- that is to say, only one of their two possibilities was realized during execution. Now, if we change this a little bit by calling foo a second time with 0 and -2, run the coverage tool again, what we'll see is that the second branch, the test of y now is executed both ways. On the other hand, the first branch, the one that tests x still is partially executed. Now we're going to take a quick programming quiz on branch coverage. The quiz is going to involve some Python code that simulates some adders. Adders are very simple hardware modules that perform addition. Here I'm just going to draw a 1-bit adder to show you how it works. The code that you're going to test is going to be a cascading series of 8 of these. The way the adder works is it takes two inputs, A and B, and A and B are bits--so they're valued at either 0 or 1, and in Python we're going to represent the 1 bit as true and the 0 bit as false. So, we're going to use Boolean logic to implement this, and there's a carry bit coming in. The output is a single-bit sum of A and B plus a carry bit out. The function implemented by the adder can be described like this: [S = A ⊕ B ⊕ Cin] The sum is the A input XORed with the B input XORed with the C input. To do an XOR on two booleans in Python, we can simply use the not equals operator [!=]. The carry out is going to equal to (A · B) +(Cin · (A⊕B)). And so the "and" and "or" operators in Python are, of course, logical and and or. What we have here is a couple of boolean equations that together implement a full 1-bit adder. If we change these adders together, we can build up something extremely exciting like an actual adder that corresponds to the add instruction that you would find in an instruction set for a real computer. Now let's look at the code. Here I have some Python code implementing an 8-bit adder. What you can see is it takes a0 through a7. That is to say 8 bits of input that constitute parts of a where a0 is the low order bit, and a7 is the highest bit. Then b0 through b7 indicate the bits of the second input where again B0 is the lowest order bit of B and B7 is the highest. It takes in an initial carry-in bit. The chain of logic here is a cascading series of full adders for the individual bits. As you can see, it's a little bit long. And so your problem for this API quiz is to come up with a series of call to this 8-bit add function which get 100% branch coverage. And let me give you a couple hints. First of all, the way to approach this is probably to think about how you would test an actual adder. Most likely, if we're testing an adder-- that is to say let's say this code is part of some sort of a circuit simulation, and we want to test the validity of our circuit, we'd pass it actual numbers. How do you pass it actual numbers? Well, you can write in some little support functions for yourself that take integers and convert them into the bit format. That's probably how I would test this, and that's certainly not the only way, but what you probably don't want to do is treat this as a black box. What's I'm deliberately doing here is giving you a function that's going to be a difficult test if you don't understand this logic at all. What you need to do is think of this as an adder and then it really becomes fairly easier to test. Let's go through a couple of solutions to this programming quiz where we're trying to get 100% branch coverage for an 8-bit adder. And so they way I solved this is using exhaustive testing. The insight is that 8-bit inputs characterize the full range of values from 0 to 255. What I can do is write a function test_exhaustive which lets i loop over the range 0-255, j loop over the range 0 to 255, and then define a my_add function which is going to invoke the 8-bit adder with those inputs. Then what we're going to do is print the output and assert that the result is equal to the actual i + j output. This goes a little bit beyond what I asked you to do for the quiz, but it's a good idea to make sure that the code is right. So, now this my_add function is kind of where the magic happens. What my_add does is takes an integer a and splits that into the bits, splits it into 8 bits that represent the same value as the integer, do the same thing for b. Then we're going to call the addEightBits function with the bits of a and the bits of b, resulting in a set of bits representing the sum. Then, we can glue those back into an integer and that's going to be returned to check our assertion. Let's look at the split and glue functions. Split is pretty simple. It just takes an integer, and if n bit-wise and with 1 is true, then the lower the bit must have been set, and so we'll return a true value for the low order bit position. Then we'll go to comparing with 2, 4, 8, 16, 32, 64, and 128. Together these tests serve to split the input integer into a sequence of true and false values that together represent that integer. The glue function does exactly the opposite thing. It takes a series of boolean values and glues them into an integer. The way we do this is by keeping a running total. If b 0--that is to say, if the low-order bit of the input is set, we increment a running total by 1. If the next bit if set, we increment it by two. If the third bit is set, we increment it by 4, then 8, 16, 32, 64, 128, and 256. And so together, this lets us reconstruct an integer value from its set of bits. So, when we call these things together, we can verify that the code actually implements an adder and incidentally, because we're testing it on all possible values, we're going to get full branch coverage. Let's just make sure that that's the case. Okay, so we just ran the code. Now let's look at the coverage output. Here we're looking at the coverage output, and what we can see is that of the 85 statements present in the adder, all 85 of them are ran, so none of them are missing, and there were 9 partially executed comparison statements. That is to say 0 comparison statements, which only went one way. If we look through the code we can see that indeed, the coverage tool believes that it's all covered. So, now let's look at an alternative solution. Instead of our exhaustive test, we could have written a much smaller test that gets 100% branch coverage. This is based on the observation that it we look at the cascading series of tests in the branch coverage, basically all that makers are whether the input bits are 0 or 1. So, if we call the adder with all 0 bits, we call it with all 1 bits, and then we need 1 more test case, we can get 100% branch coverage this way, so let's just make sure that that's, indeed, the case. I'm going to run the adder again. This time it's not printing out anything. Let's go back to the webpage. This time we have 81 statements, and they all ran, and we have 0 mission statements and 0 partially executed statements. So, as you can see, the coverage tool believes that just these three really simple test cases were sufficient to get 100% branch coverage of our adder. We looked a couple common coverage metrics that come up in practice. So, we've looked at statement coverage, which is close relative to line coverage. We looked at branch coverage, also called decision coverage, and for most of you out there, these are the coverage metrics that are going to matter for everyday life. Now, as I said before, there are many other coverage metrics, and we're going to just look at a few of them. The reason these are interesting is not because we're going to go out and obsessively get 100% coverage on our code on all these metrics. But rather because they form part of the answer to the question how shall we come up with good test inputs in order to effectively find bugs in our software? Loop coverage is very easy. It simply specifies that we execute each loop 0 times, once, and more than once. The insight here is that loop boundary conditions are an extremely frequent source of bugs in real codes. For example, if we had this loop in our Python code, so for a line in open file--that is to say we want to read every line from the file and then process it--to get full loop coverage we would need to test this code using a file that contains no lines, using a file that contains just one line, and using a file that contains multiple lines. Now, let's look at a fairly heavy-weight coverage metric called modified condition decision coverage or MC/DC. If that seems like kind of a big mouthful, there's a reason for that, which is that MC/DC coverage is required for certain kinds of avionics software. That is, if you're going to write safety critical software, where if the software fails, airplanes can fall out of the sky, then one of the things you'll need to do to show that you're software is correct is get a high-degree of MD/DC coverage. It's designed to be pretty rigorous but still without blowing up into an exponential number of tests. MC/DC coverage basically starts off with a branch coverage, so I'm going to simplify here a little bit, but MC/DC coverage basically starts off with branch coverage. It additionally states that every condition involved in a decision takes on every possible outcome. Conditions are simply boolean-valued variables found in tests, and decisions are just the kind of tests that we see in an if statement or a while loop, or something like that. Finally, every condition used in a decision independently affects its outcome. So, that's kind of a mouthful. This is going to be hard to grasp unless we go through an example, so let's do that. So, we're going to start off with the Python statement "if A or B and C:", so let's make sure to nail down the precedents so we don't have to remember that. If A is true, or else both B and C are true, then we're going to execute some code. And so now let's look what it takes to get full MC/DC coverage of this bit of code. The first thing we can see is that we're going to need to test each of the variables with both to their true and false values, because the conditions, that is to say, the conditions are A, B, and C here, need to take on all possible values. We can see that each of the conditions is going to need to be assigned both the true value and the false value during the test that we run. Now, the other part of MC/DC coverage--that is, does every condition independently affect the outcome of a decision--is going to be a little harder to deal with. Let's take a look. Let's first consider the input where A is true, B is false, and C is true. We don't even need to look at the B and C part of the expression, because we know that if A is true, then the entire condition succeeds. This maps to a true value. What we want to verify here is that we can come up with a test case where every condition independently affects that outcome of a decision. Since our top-level operator here is an or, let's see how we can make the whole thing come out false. While the B and C clause already came out to false, because if B is false, then the whole thing comes out to false. If we make A false, then the entire decision will come out to be false, and if we've changed only A and we haven't changed B and C, then we've shown that A independently affects the outcome. So, let's write another test case. Our second test case with A being input as false, B input as false, and C as true, leads the overall decision to come out as false. We've shown now that A independently affects the outcome. So let's try to do the same thing for B and C. If we want to continue trying to leave everything the same and only change the value of one variable in order to establish this independence condition, let's this time try flipping the value of B. We're going to have A being false, B being true, and C being true. If we look at the overall value of the decision, what now happens is that B and C evaluates to true, so it doesn't matter that A evaluates to false. Overall, decision evaluates to true this time. By flipping the value of only B we've satisfied this condition for the input B. That is to say we've shown that B independently affects the outcome, because when we change B, the overall value of the decision went from false to true. Now let's see if we can do the same thing for C. We're going to leave A and B the same, and we're going to pass in C as false. Now let's look what happens. B and C evaluates to false, and then also A is false, so the entire value of the boolean decision comes out to be false. By only changing C and by seeing that the overall decision changes value, we've now shown that C independently affects the outcome of the decision. So, what I believe we have here is a minimal--of if not minimal, at least fairly small-- set of test cases that together gets 100% MC/DC coverage for this particular conditional statement in Python. You can see here that this isn't a particularly complicated conditional. We could've written one much more complicated, and if we had, we probably would've had a fairly hard time reasoning this stuff out by hand, and what we would've needed to do in that case is probably draw out a full truth table. So, let's look at the idea behind MC/DC coverage. Why would this be a good thing at all? What it's done is taken a statement that was really very easy to cover using either branch coverage or statement coverage. That is to say it's pretty easy to take this and make it either come out to be true or false overall on the force testing of the individual components of the boolean logic. Basically, the idea is that when we have complicated boolean expressions, they're truth tables become rather large. What that means is there's a lot of complexity hiding in those truth tables. When there's complexity, there are probably things we don't understand, and that means they're probably bugs. It turns out that the domain of interest for MC/DC coverage-- that is to say embedded control systems that happen to be embedded in avionics systems end up having generally lots of complicated conditionals. It's definitely desirable when people's lives depend on the correctness of these complicated conditional expressions to force people to test them rather thoroughly. The other idea behind MC/DC coverage is that as part of establishing that every condition independently affects the outcome of a decision we're going to figure out when we have conditionals that don't independently affect the outcome of a decision. If you think about it, why would you have a conditional involved in a conditional expression which doesn't affect the outcome. What that basically means is there's a programming mistake, and it may be a harmless mistake. That is to say, the extra conditional being part of a decision may not affect the correctness of the overall system, but what it means is somebody didn't understand what was going on, and probably there's something that we need to look at more closely and probably even change. Another thing you can see, looking at MC/DC coverage, is that getting it on a very large piece of software is going to take an enormous amount of work. This is why it's a specialized coverage metric for the avionics industry where the critical parts of the codes end up being fairly small. That's MC/DC coverage, and we're not going to take a programming quiz on that, since first of all, as you saw, it gets to be a pain pretty quickly, and second we lack good tool support for MC/DC coverage in Python. The next coverage metric we want to look at is called path coverage. Path coverage is a little bit different than previous metrics that we've looked at, because it cares about how you got to a certain piece of code. In general, things like statement coverage and branch coverage, and even to a large extent MC/DC coverage and loop coverage, don't really care how you got somewhere as long as you executed the code in such a way that you met the conditions. So, path coverage cares how you got somewhere, so let's look at what that means. A path through a program is a sequence of decisions made by operators in the program. Okay, so let's look at the function foo, which takes two parameters, x and y, and does something x times and does something else once if y is true. What we're going to try to do is visualize the decisions made by the Python language as it goes through this program. Let's first of all look at the execution if x is 0 and y is true. Range 0 is the empty list, so we're not going to do anything here. And then y is true, we're going to execute something else, and we'll leave. So, this is one sequence of decisions that we made. We made the decision to execute something 0 times and something else once. So, now if we come in with x1 and y true, we execute something, we execute something else, and we leave. This is a separate path through the code, because we made a different set of decisions. Now, if we executed a third time with x coming in as 2, we're going to go execute something, go back, execute it again, execute something else and leave. This, again, is a distinct path through the code, because we made a different set of decisions when we got to branch points in the code. As x increases in value, we get more and more and more paths. One thing you might ask is, just by changing x how many paths can we get through the code? The answer is that it's unlimited. You can see that achieving path coverage is going to be impossible for all real code. What it does is gives us something to think about when we're generating test cases, because, of course, every possible path through the code might have a distinct behavior, so it is the case that we'd like to test lots of paths through the code. We can't test them all. Now, there's going to be a similar family of paths for y is false. We're going to get a path where x is 0, so we're going to skip doing something. And we're also not going to execute something else. We essentially have 2 times infinity paths through this code. So, path coverage is basically an ideal that we'd like to approach if we want to do a good job testing. It's not going to be something that we can actually achieve. Now, let's take a really quick quiz on path coverage. I'm going to write a function here called foo again, and it's going to take three boolean parameters--x, y, and z. What the code is going to do is it's going to use each of them as a conditional in a test. So, if x is true, we do something. Otherwise, we do nothing here. If y is true, we do something. If z is true, we do something. The question I pose to you is how many paths through this code are there? Go ahead and write your answer in the text box. The answer to the quiz is 8, and let me show you why that's the case. We came into the program, the paths fork based on the value of x. So, now we have two paths at this point. Assuming that this code doesn't have any paths that we care about, We now come to another decision point--if y is true with fork execution. But we also do it on the other path. So, now we come down here and test on z, and we fork again. This illustrates, in addition to loops, y conditionals make path coverage hard, because we get an exponential number of paths on the number of conditional tests. Of course this is going to be completely impractical but on the other hand it could be that a bug lurks only here. We need path coverage to find a certain bug. This is a valuable weapon to have in our testing arsenal, even if we're not going to be achieving path coverage in practice. So next kind of coverage I want to talk about is boundary value coverage. Boundary value coverage unlike some of the other coverage measures to be looked out doesn't have any specially take technical definition. Could be use it to me in different things. We're going to look at in a broad sense. Wihat boundary value coverage basically says is when a program depends on some numerical range and when the program has different behaviors based on numbers within that range then we should test numbers close to the boundary. So let's take the example where we're writing a program to determine whether somebody who lives in the USA is permitted to drink alcohol. So we want is the 21 years of age or more is fine for them to drink alcohol and if they are less than 21 that is to say 20 or less then it is not legal for them to drink alcohol. We just want to get the boundary value coverage on this program, we want to include the ages of 20 and 21 in our test input and possibly also to 19 and 22 simply close enough to the boundary values that there maybe interesting behaviors looking good as well. The insight here is that one of the most prominent kinds of errors that we made while creating software is off by one error. Off by one error can almost always be triggered by a value with the boundary and so that's what we're trying to do here. What I've done here is frame the boundary value coverage as a function of only one variable and also I've framed it in terms of the program specification not in terms of the implementation. So let's look at those two issue and term. So let's consider a program with two inputs. It's assume for the sake of arguments that these inputs are treated independently by the software that we are running. So the first input is going to be the age of your car and where insurance company here and we're going to decline to insure cars more than 20 years old. The other parameter is the age of the driver and here we're going to decline to insure drivers who are less than 18 years old. If the software treats this values independently will probably okay testing values around 18 independently of the age of the car and testing car ages around 20 years old independently of the age of the driver. Now on the other hand, if we had specific knowledge of our implementation consider these variables together then we probably also need to test this combinations of inputs put in the other boundaries and you could see the problem here is that of course as the number of inputs of the program goes up the number of test cases can grow very large because we have to consider the interaction between all possible combinations of variables that are dependent. On the other hand, if our variables are independent then we can test this separately. So that was briefly to visit the issue of whether we are doing boundary value coverage with respect to the requirements of this specification purpose of software or whether we are doing it with respect to the implementation. Let's go and revisit the program we have a little bit earlier where I inserted a bug into our stats function which causes it to misbehave for some inputs and not for others. So if you recall, we have the function here stats which computes the minimum, the maximum, median, and the mode of the list of numbers and the problem I post you in a quiz was to break this function in such a way that a collection of test cases getting good coverage on it wouldn't discover the bug. And so the way I accomplished this was by taking the absolute value of the inputs and so here is a bug, what we're going to do is it's only going to be discoverable when we pass numbers into the function which contain negative values. So if we think about what it takes to get boundary value coverage on a function like this and now we're talking about rounded boundary value coverage considering the implementation not just the specification what will happen is a function like absolute value would change its behavior around zero and so what we need to do get boundary value coverage of the absolute value function is call it with a negative argument and a positive argument. So to get good boundary value coverage on this function, we would have been forced to call it with a list containing at least one negative number and we most likely in that case would have discover the bug. Let's look at this function in its broader context. We have a lot of code here. For example, i is less than mean, i is greater than max. We have a lot of different operators in here but I'll have different behaviors around certain boundaries and so to get good boundary value coverage on this function over all would be probably extremely difficult. I don't know a good tools automating this on Python and there are techniques such as mutation testing that can automate boundary value coverage at least in some forms in general. Now, let's talk about a slightly different issue, which is, "What we should do about test coverage for concurrent software?" Now in general in this class, we haven't been dealing at all with testing of concurrent code and this mainly it is a difficult and specialized skill. Let's talk briefly about what coverage of concurrent software would actually mean. First of all, hopefully it's clear that while applying sequential code coverage metrics to concurrent software is a fine idea. Probably, these aren't going to give us any confidence with the code lacks concurrency here such as race condition and the deadlocks. So let's talk about how we would figure out if we've done a good job testing concurrent software. So let's take for example this function x fer, which transfer some amount of money between bank account one and bank account two. This particular function is designed to be called from different threads. So what I've done here is mark a1 and a2 in red and these variables are representing the different bank accounts and then red in order to indicate that these are shared between different calls to x fer. And so the transfer function is going to be called from one thread so some thread is going to transfer money between accounts and also several other threads are going to do the same thing. So we have is multiple threads calling this transfer function and as long as the threads are moving money between different accounts, probably everything is all right. On the other hand, since the transfer function does not synchronize-- that is it hasn't taken any sort of a lock while it manipulates the accounts. If these threads are operating on the same accounts concurrently, then it's going to be a problem. We're going to mess up the balances of the accounts that are involved. And so I ask the question, "What sort of coverage what we be looking for while testing this function in order to detect this kind of bug?" And the answer is some sort of a coverage metric to make sure that threads T1 and T2 both call this function at the same time while transferring a money between the same accounts. So the coverage would essentially be T1 gets part way into the function and then let's say it stops running, the processor then starts to run T2 for which it operates on the accounts and then completes and then this interleaving of actions between the different threads would be what would constitute a unit of test coverage so that is to say we want to make sure we tested the case where the transfer account is concurrently call. So that's one kind of coverage that we might look for when testing concurrent software. Another kind of coverage that we might look for is going to be inspired by what happened after we fix this x fer function and so the likely fixed for the bug that we had in this x fer function is to lock both of the bank accounts that we're processing, transfer the balance between them, and then unlock the accounts. And what this will do is make sure that no other threads in the system are messing with the two accounts that we're updating while in the middle of the messing with them. Now one thing that people are found is while testing this concurrent software. If this often the case, you can delete all of the locks out of a code, run it through some tests, and often it passes. This is really a sort of a scary and depressing thing because what this means is that during the test suite the locks weren't doing anything. This is in spite of the test coverage metric called synchronization coverage. What synchronization coverage is going to do is ensure that during testing this lock actually does something. In other words, during testing, the x fer function is going to be called to transfer money between accounts when the accounts are already locked and what this does is ensure that we're stressing the system to a level that the synchronization code is actually firing. It's actually having some effect on the execution and if that's happening what that means is we're probably doing a reasonable job stress testing the system. So that was really quick and like I said we're not getting into some detail. In summary, if we're testing concurrent software, we want to be looking at concurrency-specific coverage metrics such as interleaving coverage which meant if you recall functions which accessed shared data are actually called and in a truly concurrent fashion that is by multiple threads at the same time. And also synchronization coverage which ensures that the locks that we put into our code actually do something. How do we boost a number of different coverage metrics? I’d like to return to a topic that I discussed at the beginning of this unit which is the input domain. What coverage is letting us do is divide up the input domain into regions in such a way, for any given region, any test input in that region, will accomplish some specific coverage task. Any input that we select within this particular region of the input domain will cause a particular statement or line to execute, cause the branch to execute in some specific direction, execute a loop zero times, one time, or more, etc. And so, the obvious problem is, if we partition the input domain this way and we go ahead and test, it is easier for me to reach the region in the input domain, we’re going to get good coverage, but we’re not going to be able to do is find areas of omission. That is to say, nothing about this process of selecting points in the input domain and testing them in order to achieve good coverage is going to let us discover of what we haven’t implement. So, let me give a quick example--so, typically, as we talk about unit too, the software under test is using APIs provided by the system. Perhaps, the system under test is creating files on the hard disc. One extremely possible kind of bug that we would put in to the system under test is failing to check error codes that could be returned from file creation operations that happen when the disc is full or when there is a hard disc failure or something like that. And so, what I really mean here is that, for example, we got a full branch coverage of the system under test but there just isn’t a branch in the test that should have been taken when the hard disc returns an error code. And so, if the branch that should be there isn’t there, when the disc does return an error code, something weird is going to happen, the software is going to fail. So, the fundamental fact here is that coverage metrics are not particularly good at discovering areas of omission like missing error checks. To discover those, we need to use other methods. So, for example, we discussed fault injection where we make the disc fail, we will make it send something bad up to the system and we see what happens, and in that case, if we’re missing an error check, then the system should actually do the wrong thing and will be able to discover this by watching the system misbehave. Another thing we could have done is partition the input domain in a different way. That is to say not partition the input domain using an automated coverage metrics rather partition the input domain using the specification. So, if we partitioning input domain based on the specification and the specification mentions the need for our system to respond gracefully to disc errors, there is going to be some little corner of the input space that is triggered only when disc fail and while we test that. So the point that I’m getting to here is that there are multiple ways of partitioning the input domain for purposes of testing. Partitioning the input domain based on features of the code is one way and it happens to be quite effective in general, but since I can’t find the areas of omission, we also want sample the input domain in different ways. We also want to sample the input domain in other ways and that’s the team that we will return to a full force in units four, five, and six. All right, so that wraps up our quick survey of test coverage metrics. Now, I'm going to talk about the question--What does it mean when we have code that doesn't get covered--for example, if we're using statement coverage, what happens when we have some statements that we haven't been able to cover? There're basically 3 possibilities. The first possibility is that the uncovered code was simply infeasible. An infeasible code means exactly what it sounds like. It's a code that can't be executed under any circumstances. Let's say that we're writing the check Rep function for a balance tree-data structure At some point in the check Rep, we're going to assert that the tree is balanced then in the implementation of the balanced method, we're going to check if the left subtree and right subtree have the same height, and if not, we're going to return false. If the tree becomes unbalance, we're going to return false causing an assertion evaluation. Assuming that the code is not bugged and assuming that we're testing a correct tree, we'll never going to be able to return false from a balanced function. And of course, a coverage tool is going to tell us, we failed to achieve code coverage for this particular statement in the code. We have to ask ourselves, is that a bad thing? Is that bad that we failed to execute this line of code, and of course, it's not bad because that code only can execute if we made a mistake somewhere. So, the proper response to this kind of situation is, we just need to tell our coverage tool that we believe this line to be infeasible and then the tool won't count this line against us when we're measuring code coverage, and so let's look an example of this--we have here is an open source AVL tree. An AVL tree is somewhat analogous to the splay tree that we already looked at. in the sense that it's a self-balancing binary search tree. As you can see, there's quite a lot of code here. AVL tree has a reputation for being fairly complicated as these things go. The code comes with its own test suite so let's run it under the coverage monitor. Okay. Now, let's look at the output. You could see here that out of the 389 statements in the AVL tree we failed to run 61 of them and 20 branches were partially executed. So, you can see that there're some various minor failures of the test, which exercise interesting behaviors. So, here is the AVL tree standard recheck function and what it does is check a bunch of properties of the AVL tree and if any of them fail, raises an exception. Every statement here that raises an exception has failed to have been covered, and then the branches which test the condition, which leads to raising the exception only partially covered. So superficially, we haven't gotten very good coverage of this function, but actually, of course, what we hope is that this AVL tree code is correct and these are truly infeasible. And if we really believe that, what we can start doing is go ahead and start telling the coverage tool to ignore this and the way we do that is to go back to the source code and left some of this with a comment that has a special form pragma, no cover. And then we go ahead do this for the rest of these and we should see that it does sort of thing. All right. So, now we run the coverage tool again and see if things look a little bit better. Okay. Good. We'll, it's starting to get all of this code. But we can see now that the coverage tool has marked these in sort of a light gray color indicating that it's ignoring the fact that they weren't coverage because we told it to. So case 1, the code that didn't cover is infeasible code. The second case is the code that we believe to be feasible but which isn't worth covering. A code might not be worth covering because it's very hard to trigger and it's very simple. So let me give a quick example. The res variable is the result of the command to format a disc. And if that operation fails, we'll just going to abort the program. And so what might be the case is that we lack an appropriate fault injection tool that will let us easily simulate the failure of a format disc call. And in furthermore, the response of the code in this case is to simply abort the execution of the program. If those two things are the case, then we might be perfectly happy not to test this code branch, and the reason is the abort code, which is going to terminate the entire application, is presumably something that was tested elsewhere, so we don't have any real suspicion that it might fail and there's no reason to think that calling it from a location like this would act any differently or will probably be okay. The third reason the code might not have been covered is that the test suite might simply just be inadequate, and if the test suite is inadequate, we have to decide what to do about it. So one option of course is to improve the test suites so that the code gets covered. The other option is not to worry about it, and of course, since it should in good coverage could be difficult, it can easily be the case that we'll just decide to ship our software without achieving a 100% code coverage, and there is nothing in here to be wrong with that because coverage testing, like all forms of testing, is basically just a cost-benefit tradeoff. The question we have to ask is are we getting enough value out of doing the testing to offset the cost of doing the testing. And so if we do ships software with uncovered code, we basically have to do is admit to ourselves that we're perfectly okay with the fact that the customers who are on the receiving end of the software might be the first people who will actually run it. So, what I'd like to do now is talk quickly about an example of testing done right. The example that I'm going to use is an open source database called SQLite. And you can read about it more at the URL here. And so what SQLite is, this small open source database which is designed to be easily embeddable in other applications. It's really very widely used by companies like Airbus and Dropbox. It's used in various Apple products, Android phones run SQLite inside and really a lot more examples. And by the way, SQLite has good Python bindings if you want to use it yourself. So, this database is implemented in about 78,000 lines of code. So, you can see it's a sort of a medium to small size project as software projects go. Now, on the other hand, the test suite for SQLite comes out to more than 91 million lines of codes. So another way to think about that is, it's more than 1000 times more tests than code. So, if we have SQLite's code over here, it can be dwarfed by this gigantic mountain of test code. And so we might ask as well, what's in this test code? How is SQLite tested? What it turns out is the author of SQLite has achieved a 100 percent branch coverage and 100% MCDC coverage. The author has done a large amount of random testing and that'll be the subject of units 4, 5, and 6 of this class. Counter value testing has been used. The code contains a ton of assertions. It's one of the val render tool I mentioned earlier that is good for finding bugs in programs written in C. SQLIte is subjected to integer over flow checks and a large amount of false injection test are performed. So, what you can see here is that almost everything I've told you about in this course so far. Almost every single testing technique and many of the coverage metrics have been applied to SQLite, and the result is generally, a really solid piece of software. One reaction you might have when you see a thousand times more tests than code is that it's pretty extreme. On the other hand, when you consider the large number and wide variety of users that SQLite has, it's pretty great that it has been tested so thoroughly. Of course, what I'm not doing here is advocating that you go out and write a thousand times more lines of tests than you have code. On the other hand, it's nice to see that certain people who have extremely widely used codes actually do this kind of thing. So the next topic that we're going to cover is called the "Automated Whitebox Testing." And this isn't in the form of code coverage but what rather is way to get software tools to automatically generate test for your code, so you wrote some code and the questions were going to ask is "How to generate good test cases for it?" And of course one answer is we can use the kind of techniques that I've been talking about for this entire class. We can think about the code. We can make up inputs. We can basically just work hard to get a good test coverage but another answer is we can run one of this automated whitebox testing tools and so let's see how that works. For this tools goal is to generate good path coverage of the code that you wrote. So let's start of basically just making up random values for your code. Let's say one on one for the inputs. So now let's just go ahead and execute the code. The first question, is this a prime number? And so it's not. It wasn't prime the first time but it's still not prime so we're going to return zero. So now that the automated testing tool has seen a path through the code that didn't take both of the if branches so we will try to construct the new set of inputs for the function but take a different path. So the most obvious choice point to start with is the first if. So the question the tool is going to ask is, "How can a generated input that's prime?" And so to do that of course, it's going to have to look at the code, the test for formality so it's going to end up with this sets of constraints on the value of a which are going to be pass to a constraint solving tool and the answer if the solver succeeds is going to be a new value of a that passes the formality test, so let's say a is three. Though automated whitebox testing tools come up with a new set of input dysfunction its going to go ahead and run in again. So this time the first test is going to succeed a is prime, we're going to increment b by 3 decrement a by 10 and now a is going to fail the formality test since let's assume our formality check one at a time detect positive. Now the new value of a minus 7 is going fail the formality test and we're going again return zero. So the question we have is, was the tool learned? What is learned is one execution that falls straight through. Another execution that takes the first def badge, so now what its going to do is try to build on that knowledge to generate inputs that also take the second branch. So its going to take the first set of constraints that is the constraints of course a to be prime. Its going to add another number set of constraints that force the updated value of a that is to say a value of 10 lasts than the original value of a to be prime. So its going to turn that all one to a set of constraints pass it to the solver and the solver is either going to succeed in coming up with a new value of a or possibly it will fail but let's assume it succeeded and so let's say that the value of a that comes with this time is 13. We're going to execute the function again, 13 is prime, so we're going to add 3 to b subtract 10 from a giving 3, 3 is prime, so now we're going to ask if b is an even multiple of 20. If so we would return 7 but its not so we're going to return zero. The third time through the function, its going to add a new constraint. So not only are we keeping all our constraints on a but writing a new constrain on b the b mod 20 has to come out to be zero and so this time the solver, let's say, comes up with a is 13 and b is 20. Now its going to execute the function another time, this time returning 7. And so by iterating this process multiple times that is to say by running the code and then using what it learned about the code build up a set of constraints to explore different pass what we can do is generate a set of test inputs that taken together that use good coverage for the code under test. Unfortunately, I don't know of any automated whitebox testing tools that exists for Python to receive programer is a tool called Klee that you should try out which implements this techniques and I encourage you to do this, Klee is really an interesting tool. And so as you might expect, in real situations of tool like this might fail to be able to come up with a useful system of constraints or to solve them for really big codes and in fact that's absolutely the case These tools blow up and fail on very large codes but for smaller codes like the kind of thing I'm showing you here and actually they're are really nice job of automatically generating good test inputs and as it turns out these techniques are used fairly heavily by Microsoft to test their products in the last several years for the finding of a very large number of bugs in real products. We're going to finish up this unit with the summary of how to use coverage to help us build better sotfware. We're going to have to start off doing a good job testing and there's no way around that. The next step is to measure the coverage of the test using some coverage metric that is appropriate for the software that you're testing. If the coverage was pretty high, let's say 80% or 90%, then what we should do is use the feedback from the coverage tool this feedback to improve our test suite then measure coverage again and if the coverage results were poor, let's say maybe we've only covered 20% of the statements in our code base, that's the signal that we need to rethink our testing charting. Of course, if should be the case that we're perfectly happy with poor coverage. There are plenty of scenarios and for example, web application with element. We don't need good coverage because our users are going to test the code for us, and for breaks, we'll detect it by looking at error logs and we'll be able to fix it on the fly. On the other hand, if we have poor coverage results for some sort of Avionics software, or automotive software, it's going to be deployed and that's extremely hard to update, we probably need to rethink our plan in a really serious way and try to come up with a much better test suite in order to get a higher level of coverage. If coverage is used in the fashion that I've outlined here, it can give us a pretty significant amount of bang for the buck, regardless of the result, regardless of whether it's giving us a little bit of feedback to get the last 5% or 10% of improvement and coverage or whether it's saying that our testing strategy really isn't very good, it's telling us a useful information. It's telling us a useful information we probably need to know if we're going to create a high quality software. And finally, I just want to finish up with a reminder, so we strongly believe that if we have a good test suite, and we measure it's coverage, the coverage will be good. We do not believe, on the other hand, but if we have a test suite that gets good coverage, it must be a good test suite. And the reason that, that it's not true, is that it's really easy to take a test suite and tweak it until it gets good coverage without actually improving the quality of it very much. To finish up, used in the right way, coverage can be a relatively low cost way to improve the testing that we do for a piece of software. Used incorrectly, it can waste our time, and perhaps worst, lead to a false sense of security. So let's go ahead and take a look at my test code for the Q class. So it doesn't actually take a whole lot to get full coverage on the Q class, and that's, this is a good point, we haven't tested the Q a huge amount here. It shows a little bit of the problems with S, especially just normal statement coverage, so let's go ahead and get started. So we start off with just a Q of size 2, we assert that it does actually get made and then we call check rep, we're going to be calling check rep after every single test that we do, just as a further test. And really, any of the test that we do that call the internals of the q class should probably be in check wrap, because we don't necessarily want our test code to have to worry about the internals of it, we want the internal testing to be taken care of by the class itself. Preferably all though you dont always get that. But after that we check the que it starts off as empty. Which it should be. And that the que is not full. Which it also should not be. And then we try the que from the empty que. And this should return none according to specification. Then we're going to que ten in to the que. And we should get a true. Result back and enqueue returns true if you successfully enqueue something, false, if it doesn't correctly enqueue for some reason or another. Then, we enqueue 20 and again, it should be true. Now, we check that it's empty again. It should not be empty and we check that it's full. Since it's a queue of size two, the full method should return true. And that's what we asset here. Now, we further try to enqueue thirty, but since the queue should already be full, this should return false. And we go ahead and assert that. Now we try to dequeue from that, and we should get 10 out since this is a q. It's a first in, first out data structure. Then we try to dequeue again, and We should get 20 out. And, again, after each of these, we call tech rep. If I go ahead and submit this, we see that I got it right. Okay. Os, that's one way you could have done it. There are a number of different possibilities here. And like I said, it doesn't really take a huge amount of code to get full coverage of the queue class. If fact, what I did here is probably a bit much. But again. It doesn't necessarily tell us a whole lot about whether the Q class is, in fact, correct in any meaningful way. So, great. I hope you got it right and had fun with it. I hope it also wasn't too challenging. But I hope it made you think. Now the second homework problem is a bit more involved. Still not too bad but it requires a bit more code. So we start off by initializing the SplayTree and we have a few variables to keep track of the current min and max as we are going through. And we check whether the empty SplayTree is actually empty, that is, isEmpty method returns true. And similarly for findMin and findax, and sent either can't be any min or max. And that when we search an empty SplayTree that we also return none. Now we insert 100 in here, and we set the current min and max to be 100, just to initialize the tree to something, and now we go through, and, for 10 examples we check whether the tree is empty and it shouldn't be. Then we insert it, insert a new key into the display tree for range 10 through 20 twice, just to see if that works okay. Update current_min and current_max. Now we checked that that has been inserted by trying to find it in the SplayTree, and we assert that that should be found and it should, find should return the current of the for loop as found, and this is what we assert here. Now we check the min and the max; the max shouldn't change, since we're going from 10 to 20 and we've already inserted 100, but the min should change each time. So this should test, the findMin function right here. And now we do the same thing except we try to remove. And we just try to remove these, assert that it's not empty, and that we don't find, after removing the key, the key in the SplayTree, because it shouldn't be in there anymore. And then just as a little, aside here, we, try to insert 373 and remove it, to see what happens. This is really not too involved. You could have gotten quite a bit more advanced with this. This should do everything you need to do. So, if we run this, we see we got it right. So, I hope you had fun with this. Sorry about the bug that was in there initially. And thank you to everyone who caught that, my fault entirely. So, other than that, I hope you enjoyed this. I hope it showed you what coverage can and can't do, and after this we'll be moving on to random testing which I hope you'll also really enjoy. Thanks a lot. For this first problem, we're going to be writing a program that checks Sudoku grids to make sure that they are valid sub-grids, and if you haven't played Sudoku before we'll go over briefly or you can go check the Wikipedia page, which probably has a far more in-depth explanation than we could ever give here. Sudoku is a game played by on a 9x9 grid, which will be representing as a list of lists that contains 81 elements and each one of this 81 elements has to be a number between 0 and 9 including 0 and 9 and if either of this doesn't hold, then we'll be returning none right away before we do anything else. If we do pass those any checks, then we're going to return true if all of these hold, which are the basic checks that go on in a Sudoku game. Each number 1 through 9 occurs only once in each row. It also occurs only once in each column and in each 3x3 sub-grid on the box on the board, and it also occurs only once in each 3x3 sub-grid in the over all grid and it might be clear with a bit of a picture. Here you can see all nine 3x3 sub-grids. Each of this should only have the number 1 through 9 once for each sub-grid. Now, you can have zero in a valid sub-grid, it means it hasn't been encoded yet. We don't have any solution, for instance, that has those numbers in it. Your program would have to account for that, and I will give you a few examples before we expect. First, we get an L form board where, see that this row doesn't have mine elements. It only has eight. We should return none immediately. This is a valid board with no zeros and it shouldt return true. This is an invalid board that should return false, because it has the elements 1 through 9 either in more than once in a row or a column or a 3x3 sub-grid. We need to check that is indeed the case for this one, and then we have two that aren't finish. One that's a fairly easy Sudoku puzzle appeared going solid by hand and one that is actually quite difficult to solve by hand at least for me and both of these should return true. You can solve those piece with a valid different answers. You're going to write this in the check_Sudoku function, and give you these different statements to check them. Now these are the only test cases we're going to be testing against for this problem set. That's not really the point of this assignment though. What I'd like you to do is go to the forums and post the code that you used to solve this in the forums and we'll have a link to a specific forum post in the instructor comments down below the video, so please check that. I'd also like you to look at other people's code and see if you can think of some fuzzers that would break these people's code. Either a specific instances in those code or if you have a general purpose fuzzer that would run well against general Sudoku checkers then go ahead and post that in the forums. I'd really like you to think about what kind of inputs and what kind of testing you would have to do in order to do this well. I hope you had fun with this. I hope there's a lot of great discussion on the forum about it, and I look forward to seeing all of your solutions. Okay, great. Have fun. Okay. I hope you enjoyed that quite a bit. I hope there's a lot of great discussion going on in the forums over your solutions, and we can go ahead and take a look at mine real quick, which feel free to break this apart. There are issues with it and I invite you to find them. This is a very non-clever solution. First, it checks the type of the grid to make sure that it is a list and that the length of the list of list is 9. And for each row, we check the type of that and we check the length for that as well, and again, for all of these. This is basically just type checking and length checking over and over and over again for each different part that we need to make sure about first off. Now, check each piece of the sudoku puzzle separately, we don't try to check them all once in any kind of clever way. If you did that, then congratulations. First, we check the rows from 0 to 9 and we instantiate a dictionary and we check all the values. If the value isn't 0 and the value is already in d, in the dictionary, then we return false immediately because we've already found this in the row, so it shouldn't be there, we should only have this in there once, and then, we put the value in d if we don't return and move on. And we do the same thing for the columns. Okay, for checking the subgrids, we basically just do additional 4 loops. We do the same dictionary checking as we did for the rows and the columns, but now, we use i j, i i and j j, as a shifter to check the row and column spot for each of these, 3 * i + i i shifts over to the appropriate subgrid and similarly for the columns with j and j j, and after that, the code is essentially identical. We just check the dictionary to see if we already have the relevant value in that subgrid. If we passed through all this without returning false, then we return true. Okay, and when we run the solution, we see that we get the same output that we're suppose to get for the five test cases that were given. So, go ahead and look at that. Like I said, there are issues with this code and I invite you to check it out on the forums and discuss how you would break this, particularly how you would break this with a random buzzer and what kind of problems a Sudoku checker might be vulnerable to with a good buzzing test. So, okay, great! See you in the forums. The challenged problem for this problem set is essentially an extension of what's already done in the first problem. Instead of checking whether a Sudoku board is valid, we're actually going to be solving the Sudoku board and coming up with a valid solution assuming that it is valid. So, we're going to be extending new check Sudoku function. Since I don't know what you wrote, then I invite you to paste this in or paste it into something else, run it locally, and then give your findings on the forum and the code for people to handle has end and hopefully break. Now, our Sudoku solver is going to be essentially an extension, so it should return None for broken input just like check_Sudoku does. False for inputs with no valid solutions and a valid 9/9 Sudoku grid containing no zero elements at the end. Any 0 elements that are in board that's pass N assuming it's valid should be solved and changed into numbers one through nine that give a valid solution. There are a lot of ways you can do that. There are a lot of clever ways to solve a partially completed Sudoku board. We could always just go with brute-force recursive solution may be not the best idea if people end up testing against very, very difficult puzzles, and you can come up with some really pathologically evil Sudoku puzzles if you don't have fast algorithm. The one I'll be showing you in the solution video is essentially just a brute-force algorithm. Please give some example of some Sudoku puzzles that maybe it wouldn't fair so well against or maybe it would take a very, very long time to complete. We're going to be running this in the equator against the same Sudoku puzzles. Hardest commented out by default because for a more naive Sudoku solver, it will take a little bit of time maybe five to 10 seconds or so depending on exactly how you plotted it out to actually solve this one and there are going to be multiple different solutions depending on exactly how you code this up. We are going to be checking that. We would simply be checking that it's a valid Sudoku board with no zeros. And again, I invite you to post your solution code in the forums, how you coded it out, what you think it might be vulnerable to, and to break other people's code and see how you would test against a Sudoku solver, which is a bit more complicated of a piece of code than the checker itself. There are a few more ways that things can go wrong. Okay, I hope you had fun with this, and I hope you enjoy breaking everyone else's code. Like I said in the problem, I'm going to show you a fairly naive brute-force solution to this that is definitely vulnerable to some things. First of, we start up with the check Sudoku function from the first problem. Exact same function as the base for solving this real board. After that solve Sudoku is actually fairly short if not exactly pretty or copy in sys here and copy is the only needed here, sys is used down below for something that is entirely for testing purposes. First we check the grid then check Sudoku and if check Sudoku returns none or false, we returned the result. You could also assert here, I wanted to be able to continue on for testing purposes so I just return the result instead of asserting that, although asserting would definitely be a good idea here as well and we used deep copy to copy this over and not going to get into what exactly the copy package and deep copy is. Surprise to say that we made a copy of the initial input into another. Now, we check through the grid for zero elements and we change them to each of 1 through 9 and then recursively call solve Sudoku on the results. This is a very very slow brute-force process but its also fairly easy to call that as well. Each element in the grid, which I put zero and then for all and in the range 1 through 10 so all elements 1 through 9 then we set that grid position to that number and then we call solve Sudoku again. If this new grid isn't false, then we return it. Otherwise, we backtrack and that it's it. It's all there is too bad. It's really pretty simple. There are a lot of ways you could really really mess with this if you wanted it to take a very very long time for instance and right here break it into forms. Just to check real quick, let's check that my solution actually works. and this is for all of the inputs that we gave as example test cases. You can see that it does verify that the first two, the L form and the invalid ones didn't work and then for the valid one and easy, we didn't do ALG simply because it would take this out on a little bit longer to do that and I didn't want to wait so that it shows that the valid and already solved board is the exactly the same and the easy board is filled out with a valid Sudoku board solution. I hope you enjoyed that and I hope you looking at everyone else's code and seeing how everyone thought about the problem and how you might think about testing the problem itself. I hope you had a lot of fun with that. See you in the next unit. Okay, we finally got into my favorite testing topic, which is random testing. And random testing just means that test cases are created at least in part using input from a random number generator. This became my favorite testing method a few years ago, when I noticed though without even realizing it, I've written a random tester for every piece of software I ever wrote where I actually cared about its correctness. So, I've written at least a dozen random testers. So, let's look at how this works. We have a random case generator. The test case generator takes its input pseudorandom numbers, so PRNG here stands for pseudorandom number generator. Of course, on computers, we almost never get real random number generators. What we rather get are pseudorandom numbers which are algorithmically generated but for purposes of creating test cases, they're good enough. The nice thing about pseudorandom numbers is they're repeatable. So, when we start using a pseudorandom number generator, we give it a seed which completely determines the sequence of random numbers it's going to generate. So, if we ever want to generate the same random numbers, all we have to do is remember the seed. This is actually nice because we don't have to save of voluminous random tests. We can rather just remember what seed we started with if we ever wanted to get back a particular set of random tests. The other thing that goes into a random test case generator, usually to make a good one, you needed a significant amount of domain knowledge. By domain knowledge, I just mean that you have to understand some properties of the software under test. We're talking about quite a bit of detail about what form this domain knowledge might take and how you encode this as part of the random test case generator. Generated test cases come out of the random case generator and they go into the software under test. The software under test executes and produces some output. The output is inspected by a test oracle. The oracle, as we've already learned, makes a determination whether the output of the software under test is either good or bad. If the output is good--that is to say, if it passes whatever checks we have, we just go back and do it again. On the other hand, if it the output is not okay, we save the test case somewhere for later inspection and we go back and do more random testing. And so, the key to making this all work is, wrap the entire random testing tool chain and some sort of a driver script which runs it automatically. What we do is we start the random tester on some otherwise unused machine and we go and do other things or we go home. And while we're doing other things, the random testing loop executes hundreds, thousands, or millions of times. The next time we feel like seeing what's going on, maybe coming to work in the morning and we basically just see what kind of test cases have been saved. If anything interesting turned out, we have some followup work to do, like creating reportable test case and debugging. If nothing interesting happened, then that's good. We didn't introduce any new bugs and we can rebuild the latest version of a software under test and start the testing loop again. Generally, the way random testing works is, is not necessarily part of the test suite for the software under test, but rather it's a separate testing loop that gets run independently, acts to the separate or an external quality assurance mechanism. If the random test generator is well done, and if we give us a sufficient amount of CPU resources to the testing loop, and if it's not finding any problems, random testing can significantly increase our confidence that the software under test is working as intended. And it turns out that in general, there are only a couple of things that are hard about making this work. First of all, it can be tricky to come up with a good random test case generator, and second, they can be tricky to come up with good oracle. And of course, we've already said that these are the hard things about testing in general, making test cases, and determining if outputs are correct. Basically, the same thing is the case here, but the character of the problems that we run into while doing random testing are a little bit different and that's where we're going to spend the next several units of this course looking at. What I'd like to do now is go over a couple of real examples. One of them involves a very large random tester testing quite complicated pieces of software. The other one is almost trivial, it's a tiny almost one line random tester. This testing a small function, by the way, so let's take a look at this. I would like to show you a little bit about how a tool that my research group created called Csmith works, and so Csmith is just a random test case generator. We use it to try break C compilers. Let's just take a little bit of a look and see how it works. What I'm going to do here is shell into a machine belonging to my group. And we're going to look at an in-progress Csmith run that's been going for a couple of days. So let's see the results. Let's see what happened. Okay. So here's what happened over the last couple of days with this testing run. So what I'm doing is making a program using Csmith and then using the latest version of GCC and the latest version of clang that is LLVM C front-end to compile each test case with a variety of optimization levels. And I'm not using any kind of weird optimization levels. I'm basically just compiling it. -00, -01, -02. And it doesn't matter if you're not a C compiler user. These are basically the sort of vanilla compiler optimization levels that any compiler user would probably be using so what the bottom of this output shows us is that 150 thousand tests have ran since I started this testing run and during this testing run GCC has failed a half dozen times or so, clang has failed a few times, and also we see a few Csmith failures. It could be that the Csmith failures are actual bugs but historically speaking most of the time these are timeouts and so generally all of these tools are ran in their timeouts when we use a random testing loop because random test tend to be really good at finding performance pathologies but the tool runs for a really long time. And that wastes time so what we do is we just kill any tool after it has ran for several minutes including our random test case generator itself so let's see what we can learn here. So what I'm going to do is look and see if we found any interesting LLVM failures. And so when LLVM crashes it always tells us how it crashed using an assertion violation. And so as I've discussed a couple of units ago, many real programs contain a lot of assertions and these compilers certainly have that property. Okay. So what we can see is that LLVM has crashed it looks like 4 times here. One time with a message about wrong topological sorting. And 3 times with a message about unknown live-in to the entry block. And it turns out that I happen to know that this wrong topological sorting bug is not new. This is the one that I reported a couple of weeks ago. On the other hand, this unknown live-in to the entry block bug is probably new. So one thing we could do is check in LLVM's Bugzilla if this actually is new, but what I'm going to do now is just assume that it is and let's take a little bit of a look at this bug. So here we are in the actual log of Csmith's output. And that it's telling us is what command line arguments it used when it invoked Csmith to generate the program that killed clang. And so what you can see is it contains a bunch of options but also it contains the random number seed that Csmith uses so we're going to need to cut and paste this, and we can pass this to a different script, which is going to generate this program again. And let's edit it. Let's look at it. What does the random program looked like that killed clang? Here we have a bunch of preamble stuff at the top and let's get down to the random code. Okay. Here's the random code. That's all. And so what you can see is just basically it's just a bunch or random junk. And so what are we doing here? We're making function calls. We have a little integer constant. We have some relational operators and so this is the largest program. Let's see how big it is. This program isÃ¢Â€Â”oh it's not that big. It's 1600 lines or 37 kilobytes of code. But right now we don't know what about that program was that caused clang to crash. All we know is that crashed it so let's just look and make sure that that works. Okay. Good. So here you can see that clang did indeed die in that input. We can see thatÃ¢Â€Â”here we see the assertion failure, unknown live-in to the entry block. And then the rest of this is just a bunch of sort of other information that clang gives when it crashes. For example, the stack trace or functions or actually the functions that we're executing when the compiler crashed. It's giving us a detailed version of the arguments and some other stuff. This is all the stuff that we would include in the bug report when I report this bug to the LLVM developers, which I will do tonight after I finish recording lectures. So let's see now if we found anything about GCC that's interesting. And so here we started at LLVM, which includes this assertion string when it crashes. And GCC includes a different message all the time. It's an internal compiler error. Okay. And so what we can see here is that several times GCC responded with a message saying that it was killed and these do not represent compiler bugs that we're interested in. These are just performance pathologies where the compiler ran for quite a while on some random input. And those tend to be hard to report as bugs so we're not going to worry about that. We can see that there was one internal compiler error where verify SSA failed. I believe that's a known bug so we're not going to worry about it. There's a segmentation fault where GCC crashed, and most often that's due to dereferencing a null pointer. We're not going to worry about that either. There's an error in the get loop body that happens to be a known bug. However, that bug has been reported, and then we have an error here which looks to me to be new. We have internal compiler error in get initial def for induction at tree vect and so tree vect means that error was in the loop vectorizer which is a module of GCC that has been getting quite a bit of development and quite a bit of attention in recent years. This is the place where we'd sort of expect to find bugs. And so I'm not going to go generate program that caused this crash since we already saw that example for clang but basically what we've seen here how a random testing loop works. And so if we actually look at the activity on this machine, we can see that lots of programs are running here involving evaluate program as one of the driver scripts of the random tester. Csmith of course is a random test case generator. CC1 and clang are compiler processes. These compilers are sitting here using all 8 cores on a rather fast modern machine in order to break compilers. Just go back to the diagram--what we just saw was exactly an example of this general outline that I showed you, so CCNet was the random test case generator. It was given seeds by a driver's script--the driver's script run the tools in a loop and the oracle in this case was just simply looking for compiler crashes, so it wasn't even running the compiler output. Of course, when the compiler runs, it generates code. We weren't even running that code or even looking at it--all we we're doing is waiting for the compiler to tell us that it fail, and the reason the compiler could tell us that it failed is because the compiler developers have conveniently included lots of assertion violations. If those compiler developers hadn't included all of those assertions, then these compilers wouldn't have failed in such an obvious way, but they still had encountered bugs and they probably would've failed in different ways later and it might have failed in the worse possible way which is where the compiler generates code and that code actually executes improperly at runtime. And that's bad because we, as compiler consumers, don't want the compiler giving us wrong code. So while those assertions can be annoying as compiler uses because the compiler crashes actually very good and we wanted them to be there. So the driver's script are then checking the output, taking the test cases wherein this case the seed in the log file and then go back and do it again. And so if you remember, we got about 2 days of testing time, this loop would've executed about 150,000 times on a fast day core machine, and of course, we we're testing are simpler systems and compilers. We might have executed a much larger number of test than that and if we're executing the very slowest software under test, then we might've only execute a much smaller number of iterations The example we just saw using Csmith to test C compilers represented several years of work by my group--it's really sort of a large effort for us. What I want to show now is a different example of random testing. We're going to test the tiny UNIX utility function using the very tiny random tester. And so if you remember a couple of units ago we talked about the UNIX read system call, so let's look at the read system call works. Before I get going into this example, I should apologize a bit for dropping in to see here. I try to work this example into Python and it really just didn't come out right-- there just didn't feel right and it sort of inherently a C example. So it's not going to matter if you don't know C or shouldn't like C--I'll go through the code. Really it would be rather simple. The UNIX read system call as called from the C program tips the arguments. It takes fd which is a file descriptor, it takes a buf which is a 0.02, a block of memory into which data going to be read, and it takes a number of bytes to read from the file represented by the file descriptor into the memory blocked pointed to by buf. And now usually what the read system call does is it reads the number of bytes that you expected into the memory block that you provided and all was good. The return value of read is going to be the number of bytes read, so that's what usually happens, but there a couple of other things that can happen. A second possibility is the read system call then returns 0, indicating that you've reached the end of the file. Another thing that can happen is it can return -1 if it failed. But the fourth possibility and this is the one that sort of pernicious and its one that a lot of programmers get wrong is that read can return the number of bytes less than the number you asked for and this isn't a failure--this just doesn't represent any sort of out of memory condition or end of file or anything like that--it just means we need to try again. And so the little UNIX utility function that I'm going to test here is a different version of read. It's a version of read called read all that acts just like read except that it' not going to have this property of returning partial reads ever. So what read always going to do is in the case of a partial read is going to just issue another read call that picks up where the first call left off and it's going to repeatedly do that until the read is complete or until some sort of an error or end of file condition occurs. So if anything bad happens, it will just return a -1 value which is the sort of standard UNIX error value. If the read calls a succeeding but returning a partial results, we're just going to keep issuing read calls until we're finished, and so let's look at the code that does this. Here's my read all function--this is the code I just took a few minutes to write, and what you can see is that I've included a couple of assertions. The first thing that I've asserted that the file descriptor passed 10 is greater than or equal to 0. These file descriptors are integers usually small ones and they have to be non-negative. The second thing we're going to assert is that buf that is to say pointing to the block of memory into which we're going to read data can't be the null pointer and just asserting buf like this is implicitly a test that the pointers not null in C. And the third thing we're going to assert is that the number of bytes to read is also non-negative. So, given that setup and we're just going little sanity checking here, we can get to the main logic. So the first thing we're going to do is initialize a left variable which is going to be the number of bytes left to read to initially be the total number of bytes to read. Now we're going to enter our while loop--the while loop is going to operate until either the read system call returns something less than 1 that is to say it returns 0 indicating an end of file condition or -1 indicating an error. And if either of those things happens, we're just going to return that value to our caller. If read work that is to say it read some positive number of bytes, we're going to increment the buffer pointer by that number of bytes, reduce the amount of bytes left by that number of bytes, assert the number of bytes left hasn't gone negative due to some sort of a logic error, and now finally if there are no bytes left, we're going to return the number of bytes read which has to be just the original number of bytes but these asked to read. So this is pretty simple code and I would expect or at least I would hope to be able to get it right but I know from vast prior experience that I never get this stuffed right the first time, so what I want to do is be able to test it. So what I've done is written a little test runners, and so what the test runners does is opens a program and so here we're just using a splay tree from Python that happens to be in this directory and allocating a buffer and we don't need to worry about the details here and then what we're going to do is 100 different times we're going to issue the read all command to read the contents of the file into the buffer and then we're going to assert that read all--always read the full number of bytes we're trying to read and then we're also going to assert using a memory comparison function that the contents of that file are the contents that we expected. Its definitive variables initializes some code I didn't really talked about to contain the data from the file. And so what you're going to say if we do this 100 times, then our new read all call passes. And so let's look at what happens. Okay, so a 100 test cases passed in that tiny amount of times it took us to run. And so one thing we could conclude is that since a 100 of test passed over read all that the logic is solid but it turns out that this conclusion isn't really warranted. Let's see if we can get the program to tell us why. So I'm going to do here is print out a value indicating the number of bytes that was actually read by the raw read system call and then we'll run our testing loop. And what happens is every single time we called read, it read the full size of the file. So our splay.py here--we can see that's 3121 bytes and the actual Unix read system call always read that many bytes every time we call it. So it ends being the case even though it's illegal for read to return less bytes than we asked it to read is not actually doing this, so what we're going to need to do in order to test our logic in any meaningful way is make it do that, and so the question is how to do that. Well, one way to do that would be to go hack Mac OS so that sometimes doesn't read all the file bytes that we asked and that is not very fun, so we're going to do something a little easier. We're going to insert faults ourselves by calling a read fault injection file. By calling a read function with fault injection is the function I already wrote here, so what you can see that read with fault injection does is it has exactly the same interface as read just a different filename, but what it's going to do is instead of reading the nbytes that we asked for it's going to set the number of bytes to read to be a random number between 1 and the number of fault bytes inclusive. And so the +1 there means that instead of going from 0 to the number of bytes minus 1, we;re going to test the range from 1 to the number of bytes, and that's what we want to do. We don't want to return 0 because that indicates an end of file condition. So now we're ready to run some tests. This is what we got this time--so we can see is that our fault injection version of read is indeed reading less bytes than the full number of bytes we asked for and what you can see is the first time it's called, you get the range from 1-3121, so it's going to pick something in the middle usually, but then success of read calls have a narrow, narrow range of random numbers that can return and so that's what explains this kind of progressional numbers, that's what explains this sort of progression of numbers towards smaller numbers, and so there should be a 100 of these little sequences due to fact that we let the test loop 100 times. So the question we can ask is, is our confidence in the software now are higher. Well, probably there are other tests we should do--like one thing we could do is use a read fault injection function. Instead of being random just reads 1 byte every time-- that might end up being a reasonable stress test. Another thing we might do is simulate random end of file conditions when random errors that read is allowed to return. So during our testing, the read system call never actually returned an error value. It always read the file successful, but if we want to use fault injection to make it do that, then we have to modify our program a little but, but I'm not going to do that right now. In the meantime, I think we've established that the logic here is fairly solid. It can do with a wide variety of conditions and one thing we might want to do just before our leaving is out of the 1,000,000 times instead of 100 times. Okay, so we compile with optimizations, we run it, it's running a 1,000,000 iterations of the test sequence, so it's doing a 1,000,000 iterations of this little sequence here, and let's see how long this takes, hopefully, it's not very long. Okay, there we go--it took like 15 or 20 seconds, so that was a little bit painful but not that bad overall. And so the question is now do we have reasonable confidence that our logic is solid and except for the error handling condition that we talked about, I would say that this is probably pretty solid--this is probably pretty solid code now. We go back to our overall master diagram. What we have is a driver's script which in this case was just the c program. We have a random test case generator which is basically two lines of code--one of them we had computed that used a random number to compute the number of bytes to read and the other one, actually called the read function. The software under test was the other function--the read all function that we wrote. The oracle was implemented by memory comparison function--that is to say did the read all command actually read the right bytes from a file--and so as we saw, the oracle never detect an error since we got the function right and everything work out. This case of implementing a read all call which is stand in for read that behaves better. It has been on my mind lately because I assigned this is as one problem on the final exam for my operating systems class that my students finished a couple of weeks ago. And it turned out that many of the students didn't implement the right code. A lot of times they forgot in the read all command, for example, to increment buffer so that subsequent calls to read wouldn't overwrite the original call to read. I saw so many of those that I decided I write the function myself and see if it was hard, and it was a little bit tricky but it worked out and that's why I thought it might make a good example for you all. Hopefully what you learned from the previous example is that building a random test case generator doesn't have to be very difficult. But realistically, it's usually a little bit more involved than the one we just saw. And the key problem is generating inputs that are valid. Or another way to say that is generating inputs that are part of the input domain for the software under test. What I wanted to do here is look at the entire space of random inputs. And what I mean by entire space of random inputs is we haven't ruled anything out. So what we get is random 1s and 0s. And so remember that request to constructing random inputs, we're going to be feeding those into some software under test and we're looking for outputs in the range of the software under test. And let's say for the sake of argument that we're testing a web browser. And so the question we have to ask ourselves is how much of the space of totally random 1s and 0s constitutes a valid input for a web browser and when I say the word testing, they're rendering it in part of a web browser so we're testing what happens when data comes over the web to the web browser in response to an HTTP request and now the web browser wants to render that. So we have total random 1s and 0s. And as you might expect almost all arbitrary combinations of 1s and 0s fail to create valid web pages so there's going to be some small subset of the set of random inputs that constitutes valid inputs to the web browser and if we take one of these other inputs and hand it to the web browser there's going to mapped to a part of the output space that corresponds to a malformed HTML. One thing we might ask is, "Isn't it a good thing to test web browsers with completely invalid inputs to see if for example instead of returning a page error it's easily possible that some subset of this space corresponds to browser crashes?" The answer is yes. We definitely do want to test our web browser with completely random inputs. But on the other hand, the amount of code in the web browser that serves to classify codes as renderable versus not renderable there's some very tiny fraction of the total amount of code in the web browser. And if the fraction of valid HTML is very low, we're going to spend almost all of our time testing this very small part of the web browser. What we probably want to do is take some tests from this broader set of completely random inputs but take most of our tests from a set of valid webpages. These are the ones which when we select a random input in here are going to with some probability end up exposing something like cross-site scripting bugs. Now I'm just going to ignore the little details like the fact that we probably would need to be generating JavaScript as well in order to make these kind of bugs happen. We're mostly trying to look at the abstract problem. Let's take a little bit different view of the same problem. So, what I want to do is draw a graph here, so in the level of code coverage with a random test cases are inducing on the system under test. So again, we're testing a web browser, and on the x axis, this is going to be a little bit fuzzy unfortunately. But what I want to show is how far into the web browser we execute. Which is just checking, for example, if the incoming data is even valid HTTP. Once we get valid HTTP, the browser is going to scan and make sure it got valid HTML. It's going to be doing lexical analysis and checking of HTML. If the input fails to be rejected by this kind of code, it's going to go on the rendering engine and finally, it might have sort of some more advanced processing which is dealing with things like forms, cookies, scripting and such. Okay so, we have this graph. So, now let's see what happens when we fuzz the web browser using totally random bits. Well, what's kind of most likely happen is most of those bits that come in, are now even going to be valid HTTP responses. So, we're going to get coverage rapidly drops off and what's left is almost always going to fail somewhere else. What we're going to see is, we're going to spend the bulk of our testing effort rejecting random sequences of bits very early on and very little of our testing effort, testing code here. Again, as I said, if that's what we're trying to do, if we really want to be stressing the early parts of the web browser code, then that's great. And random testing is perfectly good at that, but on the other hand, if we are interested in is for the broad coverage of the software under test, then we're going to fail. The red color indicates random bits. The next thing we can do is we could go random input generator totally respects the constraints of the HTTP protocol. Furthermore, we can adapt it, so the text that it generates contains valid lexical elements of HTML. That is to say it's composed of things like races with tags in them, other kinds of tags so this was the directive, but this isn't too hard to do. So, if we do something like that, I think I'm going to call that is, use green to represent protocol credit code. I'm using sort of fuzzy terms of LAN devising here, I'm not trying to use any kind of standard terms. So now what's going to happen is, hopefully we'll get pretty good coverage of the protocol code still. To get quite good coverage of lexical HTML processing and we're going to fall off the cliff again. Because as soon as we get to the render, it's going to become apparent, and we didn't try hard enough to generate valid HTML and we're going to get something to render very often. So now, what we've accomplished here while we pushed the coverage that we're getting on the software under test farther into the system, it is farther into the HTML processing chain, but still haven't pushed it very far. So, the next thing that we can do is use some sort of a grammar or some sort of a structural definition of HTML to generate random but valid HTML. The next thing is valid HTML. And so what's going to happen is, a coverage of the protocol code and the lexer may decrease, while on the other hand, we're going to be able to push into the HTML processing code quite deeply before falling off a cliff. So what've we done? We've traded off coverage in the early parts of the web browser which may well be so simple that we don't care about much about them for coverage farther in. And so finally, what we could do is, generate random code that includes elements of scripting, forms, whatever else that we're interested in testing and we can run that through and now we can start randomly testing our browser with this. What's going to happen now is, our coverage might decrease even a little bit more in the early parts because we're spending more time doing other things, but we're probably not going to fall off a cliff at all. And so, you can see that in most cases when we do random testing, what we're looking for is something like this kind of flat line, and what this flat line indicates is that we're covering all parts of the software under test roughly equally. What we're going to see is as we look through more random testing examples, is getting sort of a coverage curve like this often requires quite a lot of work, quite a lot of sensitivity to the structure of the input demand, but on the other hand, we get paid back for that work with random tests that can exercise the entire software under test and that's going to be a valuable thing in many cases. Okay. Now that we've introduced this input validity problem, let's work toward a situation, which is going to lead towards a programming problem for you where you have to solve this kind of problem. The specific problem that we're going to look at is generating valid credit card numbers. And so you might ask the question, "Why would we care about generating valid credit card numbers?" and the assumption here is that you are working on some software under test whose input domain is valid credit card numbers. And so your software under test is probably doing something like processing payments and its output is going to be something like completed transactions. So now how are you going to test this? Well, one thing you're going to do is collect some credit cards that you have and that your friends have. And so you'll have a variety of Mastercard and American Express and other kinds of credit cards. You're going to run it through your payment processing system and you're going to see if it works. After you're finished doing that, the question you have to ask is "Have you shaken all the bugs out of your code?" and the answer is probably not. So what we're going to want to do is randomly generate valid credit card numbers and use that to test the payment processing system. To get any further here, what we're going to have to do is take a look at the structure of a credit card number so here's how those work. So there are some number of digits at the start of a credit card number, so let's say 6, that constitute the issuer identifier. What this means is that for example all of the American Express cards issued by a certain company are going to share an issuer identifier. The next set of digits, let's say 8 of them, is going to correspond to the account number for this credit card and then finally at the end is a check digit. And this is computed algorithmically from the preceding numbers, and the function of the check digit is to serve as a validity check for a credit card number. So if I'm entering my credit card into a website or if I'm swiping my credit card and the magnetic strip has gotten corrupted it's fairly likely that if any of these digits are wrong the check digit is going to catch it and the credit card number can be rejected rapidly and algorithmically other than actually having to submit this thing to somebody who can process the payments and forcing them to reject it when the account number or the issuer identifier fails to look up correctly. The check digit is computed using something called Luhn's algorithm. So this fellow Luhn was a guy who worked for IBM during the middle part of the 20th century. And in 1960, he patented Luhn's algorithm which was simply a method for calculating this check digit and it has been used ever since in applications like this. Let's look at how to use Luhn's algorithm to calculate a check digit. So what we're going to have is a sequence of numbers, and this will work for any number. And now there are two cases. If there's an odd number of digits, we're going to do one thing. And if there's an even number, there'll be a slight variation. If there's an odd number of digits, what we're going to do is go through the digits and take every even-numbered digit and multiply it times 2. So the odd digits we're going to leave alone and the even ones we're going to multiply times 2. And the next thing we're going to have to do is see if any of the resulting numbers are greater than 9. If so, we're going to have to subtract 9 and some number is going to result. Now what we do is we sum up all of these digits that we've derived at--that is to say the odd number of digits we just summed up, the even number of digits we summed up, the results of these calculations, and then we compute the value Mod 10--that is to say we just took the 1s digit of the sum and if that value was equal to 0, you have a valid credit number. If the value does not come out to be 0, then it's invalid. Okay. So that was the case for the odd number of digits. Now, let's briefly look at the case for an even number of digits and that's almost the same. So if we have an even number of digits, we go ahead and take the odd number of digits, double them, and we'll try it now.. If the doubled number ends up being 10 or higher, then after that the process is the same. We add them up, take the 1s digit or equivalently take the sum Mod 10 and see if the result is 0. So that is Luhn's algorithm, and the reason that we need to go over that in a bit of painstaking detail is because if you want to generate valid credit car numbers, then you're going to need to implement Luhn's algorithm in order to do that, and so there's one little detail that I left off. And so what I just told you is way to check whether the number is valid and what you're going to need to do is create a valid credit card number. So you're going to be given the issue identifier, you're going to make up the account number randomly, and then what you're going to do is put a 0 at the end of the credit card number, use Luhn's algorithm to compute the check sum for it and the result of that is unlikely to be 0. If the check sum comes out to be 0, then you've already generated a valid credit number. If it hasn't, then you're going to have to take 10 to track the check sum from 10 and put whatever number you arrived at in the place of the check sum digit where you had 0 and now you have a valid credit card number, and I suggest that as you're implementing this you check it--so you compute the Luhn number. If it's not 0, you subtract it from 10, put that in the check sum position, and now you can go ahead and compute the check sum of the entire credit card number--it need to come out 0 Here's a look at what you're going to do in a little bit more detail. Remember that what you're writing is a random tester for the credit card transaction processing system. The core of this random tester is going to be a function called generate, which is what you're going to implement, so generate is going to take two parameters. The first one is the prefix, which is going to correspond to the issuer identifier. And this is a sequence of digits which is given to you, which has to appear at the start of the credit card number that you generate. The second parameter is the length. That is to say, an integer parameter that determines the total length in digits of the credit card number that you're going to generate. So your job then is to construct your credit card number, which has the given issuer identifier or prefix, has the required total length, has completely random integers in the middle here, and has a valid checksum digit as computed using Luhn's algorithm plus this little procedure for turning the Luhn checksum into a digit, which makes an overall credit card number's checksum come out to be 0 and therefore be valid. So let's just look at the code quick. So what you're writing is a function called generate. It takes the prefix and the length and so you're code is going to go here. And so we might call it like this. We might say the prefix is 372542. This happens to be the prefix for a credit card that I have in my pocket. and a 15-digit credit card number and now you're going to create a valid credit card number with the required properties. To do this, you're probably going to want to implement a function called Lahn checksum which computes the checksum of a number and this axillary function is-luhn-valid simply checks if the Lahn checksum or a given input is 0. what I'd like you to do is implement that now. If you get totally stuck and you like kind of a hint, there's something you can do which is look up Lahn's algorithm on Wikipedia. The reason that's kind of cheating is because on this Wikipedia page for Luhn's algorithm they actually will give you a Python code that works. But don't go there unless you actually want to see the real answer. Okay. Take a few minutes now, write the code, and what our API will do is use it to generate a number of credit cards and check if they're valid. If they're not valid, it will give you some indication on that the problem is. And if they are valid, then it will tell you what the N and you can move on to the solution. Before I let you go hack on this, I just want to give you a quick reminder about something that should save you some trouble so in the discussion I did here about Luhn's algorithm I assumed that the digits were numbered starting out 1. For example, #1 here is an odd-numbered digit. This is digit #2 which is an even-numbered digit, #3 is odd, etc. Now on the other hand, Python strings are index starting at 0. That's something you're going to have to watch out for. Or else you're going to go a little off by 1 error in your code that makes it a pain to debug. We've still got the code the I wrote for the Luhn checksum. And this is kind of a redundant bad code but I wrote it quickly and I think it works. So we have the Luhn checksum function. And the first thing it does is looks at the length of it's argument. That is to say, it looks at the length of the string over which it's going to compute a checksum. It's also going to initialize the checksum variable to be 0. Now, if we're computing a checksum over an even number of digits, the remainder mod 2 is going to come out to 0. So in this case, we're going to loop over the digits and the checksum. That is the range(l) goes from 0 to length -1. And here what I'm going to do is add 1 to that number so that we can get a 1-base numbering system like I showed you in the explanation of Luhns algorithm. So here now, if we're at digit 0 in Python we're at digit number 1 in the 1-base human number system and so if that comes out to be 0 when moded with 2 now with an even number digit and so in that case we're going to add the value of that digit into the sum; otherwise, we're going to call this Luhn digit function on the same value and the Luhn digit function is just sort of the obvious thing. It multiplies the number by 2. If it's greater than 9, it returns 9 less than the number. Otherwise, it just returns the number. Now, the other case is that we're checksumming an odd number of digits. In that case, we have exactly the same logic with the even and odd cases flipped around. So here we're taking the Python zero-based digit adding 1 to it to get a human 1-based numbering system moding with 2. If it comes out to be even, we're doing the Luhn digit computation. If it's odd, then we're just adding in the value of the number. And finally, we get a checksum that we've accumulated here. And we take that mod 10 and that gives us our actual checksum value that we're going to use. So now, here's the algorithm for generating a valid random credit card number. So the inputs are remember a prefix and a total length in digits. So the number of random digits we're going to generate is the total length minus the length of the prefix minus 1 for the checksum digit. Now we're just going to assert that that's greater than 0 just to be careful. And now we're going to set N as our credit card number that's in progress. So we're going to initialize that to be just the prefix. Now for the number of random digits we have, we're just going to make up a random digit and append it to the string. Finally, we're going to add a zero checksum to it and compute the Luhn checksum. If that Luhn checksum comes out to be zero, we're done. If that Luhn checksum comes out to be non-zero, we have to do the little inversion that I talked about where we take the checksum where we subtract the checksum from 10 and now what this little bit of logic does is it strips off the last character. That's the zero we added from the credit card number and adds on the checksum number. So that's how easy it is to generate a valid credit card number. And now there is one valid function just to check if the Luhn checksum is 0. Now here we have below a code from Wikipedia that does the same thing. And I'm not going to go through the logic here. But what you can see is this is quite a bit more idiomatic Python. It's actually quite a bit nicer than the code that I wrote. So if you like that better then use this as a model instead of the code that I wrote. The code that I wrote is pretty kind of dumb and obvious. We have equivalently a Luhn valid check sum using the Wikipedia algorithm, which just does the obvious thing. And then what I have here is a random tester, which generates a random credit card number with a certain prefix and 15 digits and then ensures is that it's valid. The validity checking function for credit card numbers simply makes sure that it has the right length, that it has the right prefix, and that the checksum comes out to be 0. That is to say, the is-Luhn-valid function returns true. So that's all there is to it. But what I want to do finally is take a look at one other issue. I'm going to comment out my code here and comment in some different code. What we're doing here is generating completely random 15-digit numbers. What I'm doing is generating a random integer between 0 and the smallest 16-digit number. The largest number that could be generated here is the largest 15-digit number. And then we're going to zero-fill, convert that to a string, and do a zero-fill operation, which adds leading zeros. Now we have a completely random number that is 15 digits long. And if that checks out to be a valid credit card number, we're going to increment our validity checker and then finally after doing this 100 thousand times we're going to print the number of valid credit card numbers that we got. So let's run this and see what happens. Okay. We got no valid credit card numbers out of 100 thousand. So the problem is the prefix was too long. With a 6-digit prefix, the chance is one in a million that we'll generate just this prefix and then it goes down to one in 10 million that will meet the prefix and the checksum requirement. So if we start off with a much smaller prefix like just 37 and this is basically anything in the American Express system I think, now let's see what happens. We're going to generate 100 thousand credit card numbers and 104 of them came out to be valid. So even with just a two-digit prefix, it's pretty unlikely that we generate valid credit card numbers. And so what that means is if we're generating lots of invalid credit card numbers of course we're stressing only a very small bit of a transaction processing logic that checks for valid credit card numbers and we're not stressing the rest of it. So what I hoped I accomplished here is first of all motivated the fact that this generation of valid data is a real one and second of all, to give you a little bit of a feel for what code looks like that we usually have to write to generate valid inputs. And so, if we go back to our web browser example, you can see that we will be doing a similar exercise but it'd just be quite a bit more sophisticated to generate for example a valid HTML or a valid HTML with scripts and other things. That it would take to actually do meaningful fuzzing of a web browser as shown by the blue line here. And so now to do this, instead of spending half an hour or however long you spend on the PR quiz now maybe you're going to be spending many weeks. But on the other hand, what we're going to get out of this if we do it right is a lot of high-value bugs including security bugs in our web browser and strongly possible but of course not guaranteed that the value we get out of those bugs that we find in a web browser but the effort would've been worth it. So in the two examples that we just looked at, there was a vast space of inputs and some small part of that constituted the actual input domain for the software under test. And what happened was when we generated invalid inputs, which was extremely easy to do, they were executing a boring part of the software under test and getting them out to input rejection errors. And those rejections of invalid input were basically forcing us to generate another value and submit it and see what happens. So what's going to happen is the random testing loop is going to spend a lot of time spinning. That is to say, stressing on only a small part of the software under test. But there's actually something different that can happen that's much worse than that. So the other thing that can happen is the input isn't rejected. And the reason that an invalid input might not be rejected is that the software under test failed to contain sufficient validity checking logic in order to distinguish valid inputs from invalid inputs. And so what we're getting now is something potentially much worse. We get strange misbehavior, crashes, or something else very bad. And the problem is that when we get misbehavior, crashes, and other bad behavior from the software under test, these behaviors instead of being rejected by the software under test and letting our random testing loop proceed, these behaviors look like bugs. So normally if we crash the software under test, this is something that we have to track down manually and now we're taking a developer's time. And once we start taking human's time, what we're essentially doing is mounting a denial of service attack on whoever is processing bugs that come out of the random testing loop after looking at about 10 bugs that are caused by invalid inputs to the software under test the person who is looking at that input is going to get tired of it and isn't going to look at them any more and now we have a problem. Now we have a random testing loop producing no actionable output. And so our random testing effort has basically failed. And so let's talk a little bit about why software under test would lack a validity checker. Well, as we saw I believe in Unit 1, internal interfaces, that is to say interfaces within a product and interfaces that don't spend trust boundaries often lack full validity checking. And they do that for performance reasons for code maintainability reasons basically because we have to trust somebody sometimes. Otherwise, we can't get a new software written. Another reason is, there exists software for which it is impossible to construct a validity checker. And so earlier in this unit, I was talking about the example of using Csmith. That is to say, my random test generator that generate C programs for testing compilers and it turns out that C compilers don't contain a good validity checker for C. And the reasons for that are kind of involved but the quick answer is that if you have a C program it's undecidable whether that program dereferences a null pointer, accesses an out-of-bounds array element, or do something else that violates the rules of C. So compilers do do quite a lot of validity checking-- that is to say, they look for syntactically invalidacy. What they fail to do is check for the dynamic validity properties that are required for certain kinds of miscompilation books. So in summary, the input validity problem bites us as a performance problem almost every time we write a random tester. It bites us in a much more serious way when we have some software under test that lacks a reliable validity checker. So now that you've seen how easy it is to build a random tester, I hope that you build many of them in the future and most of the rest of this course is going to be spent going into more details about how we build effective random testers. But before we do that, what I'd like to do is address an issue that eventually confronts almost everybody who works on random testing, which is that random testing gets no respect. And what I mean by that is people often think that it's a really stupid way to test software. For example, one of the classic references on software testing is a book called The Art of Software Testing. In this book, the author talks about random testing and here's what he has to say. "Probably the poorest methodology of all is random random input testing... What we look for is a set of thought processes that allow one to select a set of test data more intelligently." So basically what Myers is saying is random input testing is not a way that allows one to select a set of test data intelligently. And so my opinion and this is after writing at least a dozen testers what he should've said is something like what we look for in testing is a set of thought processes that allow one to randomly generate a set of test data more intelligently. And so what we can do is get around this fact that testing requires thought about the software under test. But my experience and the experience of a lot of other people is that if we think hard about a random test case generator that's as good or better in many cases than thinking hard about generating the test cases themselves. So another classic piece of work is a book chapter called Random Testing by Richard Hamlet. And in this chapter he says, "Most criticism of random testing is really objection to misapplication of the method using inappropriate input distribution." And so what Hamlet is really talking about here is the input validity problem that we've just been discussing and what he means is that if you ignore the input validity problem and you just test randomly using completely random garbage, you're going to get a bad impression of the method because it's not going to work very well. You're going to experience a phenomenon that we talked about a little bit ago where all of the test cases get rejected by very early parts of the software under test. And so I would actually say what Hamlet said a little bit differently. Most criticism in random testing is really objection to random testing done badly. So let's take a look at one more quote. So what the author of Testing for Zero Bugs has to say is, "The introduction of random testing practically eliminated user bug reports on released back ends. To our amazement, RIG (and this is their test case generator) was able to find over half of all bugs reported by customers on the code generator in just one night of run time." And so this clearly was written by somebody who did a really good job creating a random tester. And also probably applied in the domain where random testing happened to work really well. The rest of this Testing for Zero Bugs is worth reading as well, and we will include a link to that in the supplemental material for the course. Well, I'd like to talk a little bit about how random testing differs from fuzzing and the short answer is they're the same thing. The long answer is going to require a bit of explanation. So let's go back in time to 1990 when a professor called Bart Miller and his students published a paper called An Empirical Study of the Reliability of Unix Utilities. And so what they did as part of the fuzzing effort was provide a completely random data to a bunch of Unix command-line utilities. These were things like editors, terminal programs, text processing utilities, and other similar Unix tools that you can basically think of as being tools predating the era of graphical use or interfaces on Unix systems. And what they found is using this incredibly simple technique, that is doing random testing without worrying at all about the input validity problem they were able to crash a quarter to a third of these utilities on basically any version of Unix that they tested. And so what you have here is a pretty strong result. They were able to crash lots of applications with minimal effort. What that means is that the quality of the input validation done by these sorts of programs at the time was really rather bad. A few years later in 1995, the same group repeated the effort and wrote another paper about this. This time they not only tested the same kind of utilities that they tested 5 years earlier but they extended the work to testing network applications and GUI applications and basically they got very similar results. Now, in another 5 years in 2000, the same people did another study and this time they fuzzed Windows applications. And what they found was basically more of the same. They can crash most of the applications that they tested. And then finally in 2006, the most recent installment of a fuzzing study by this group was published. This time they attacked Mac OS X. And this time they found something a little bit different. The command-line utilities on Max OS X would hardly crash. They found a much lower rate of crashes than they have found earlier. But on the other hand, of the 30 GUI apps that they tested, 22 could be crashed. It's worth mentioning that as this group evolved their fuzzing work, they kept having to write new tools. For example, to fuzz the Windows applications they had to generate Windows events to GUI applications and they had to do something similar for Mac OS and previously for X Windows applications. So they had to keep evolving their tools but the input generation methodology that they used, that is to say basically generating random garbage and not really worrying about the input validity problem remained the same across all of these studies. So now what I've covered so far was this particular random testing effort by this one research group. But something interesting happened I believe sometime around 2000 or a little after is the term fuzzing took on another use. Something interesting happened and I believe it was not too long after about 2000 is that the term fuzzing took on another connotation and in this connotation, fuzzing basically penetration testing, and what I mean by that is fuzzing is used for very specific purpose which is finding security vulnerabilities in applications. And usually the connotation is--is that the fuzzing is being done on applications that the person doing the fuzzing didn't write. So for example what we might do is find some sort of a machine on the Internet that offers a service and we'll do random testing over the Internet of that service with the goal of finding bugs in the service that are going to let us mount an attack such as a denial of service or some sort of an exploitable vulnerability that will let us mount an intrusion on that Internet service. The sort of an interesting thing about this newer connotation of fuzzing is really gotten rather big. So for an example, plenty of special purpose tools are available both commercial and open source for performing fuzzing over the network. The best of these contains a lot of canned knowledge about protocols due to a lot of relatively unskilled user to find bugs in services that are available over the network. This course is mostly not going to be about this particular kind of fuzzing. Rather, the emphasis of the course is more on random testing in the original sense of fuzzing which is to say trying to find flaws in software rather than this very specific meaning of fuzzing for penetration testing purposes. And now one of the challenge problems for this unit is going to involve writing something with more like this kind of fuzzer, but basically mostly we're concerned with random testing sort of in the general software robustness. Mainly we're concerned random testing as what would be applied to our own software or to software developed by other people in our own organization. Alternate history is a genre of fiction where we explore what would have happened to the world had some historical event turned out differently. For example, we might have an alternate history now exploring what would have happened if the Allies hadn't won World War II. What we're going to do is look at a few alternate history examples where we're going to try to explore the question, "What would've happened if certain groups of people in the past had used random testing in addition to whatever other kinds of testing they were doing?" And we're going to sort of specifically focus on the question, "Would random testing have been likely to alter the outcome of some famous software bugs but would have not made much difference? Would it have not been a very useful technique?" And so the first case study we're going to look at is the FDIV bug in Intel Pentium chips. And so in 1994, it came to the world's attention that the FDIV instruction supported by Intel's Pentium processor was flawed and so FDIV is an instruction that you would use to perform floating point division. If we're writing an assembly language for a Pentium, we would issue the FDIV instruction with two operands and what do we do is compute the quotient of a divided by b. And so what happened is there's a bug in FDIV. And what had happened was the implementation of FDIV, that is to say the hardware implementation of this machine instruction was intended to be very fast. The Pentium was at that time a very high performance processor. And the way FDIV was fast, at least in part, was because it did part of its work using table lookups. So there was this table of values stored in the Pentium's hardware. What had happened was somewhere on the way to fabrication some of the values had not been loading correctly into the table. So part of the lookup table, I believe it was 5 entries, contained the wrong results. And for some values of A and B passed to the FDIV instruction, looking up these wrong results in the lookup table would cause it to do the wrong thing. This wasn't an extremely major error in the sense that it would return a million instead of 0 but rather it was off in sort of some number of places after the decimal point. The reason that the flaws in FDIV went undetected for sometime was that only about 1 in 9 Billion randomly selected inputs would actually trigger the bug. And so now the question we need to ask ourselves is given this relatively low observed failure rate on random inputs, would random testing have been a good way to find the Pentium FDIV bug? And the answer is almost certainly yes. So what we're going to do now is try to figure out about how long it would've taken Intel to find this bug using random testing. And so the first thing we're going to need to do is make an assumption about how many tests per second they can run. And so it's not as if in 1994 they have modern multigigahertz hardware available but rather these Pentiums ran at around 60 MHz. And so what we're going to say for the sake of argument is that it took 10 Ã‚Ä¾s to perform an FDIV and verify its result. So in other words, at 60 MHz, I'm assuming that it would take about 600 cycles to check a particular input for FDIV. Now let's try to work out the math. All right. Let's try to work out the math. So what we have is, by assumption, we can run 1 test every 10 microseconds. Now, there are a million microseconds in a second, 60 seconds in a minute, and 60 minutes in an hour, and 24 hours in a day. So now we're going to do some unit cancellation. We can kill microseconds, we can kill minutes, we can kill seconds, and we can kill hours. So if we do the multiplication, we can get tests per day. And if we do that, we get 8,640,000,000 tests per day. If we multiply this testing throughput by the failure rate we're going to get 1 failure per 9 billion tests. We can cancel tests, do the division, and arrive at 0.96 expected failures per day. So under what I think are fairly modest assumptions here, if we perform completely random testing of the input space for fdiv, we should be able to find this bug in about a day. And so now this kind of testing is going to need some sort of an oracle. So we're going to need a way to tell if our particular output from fdiv is correct or not. And the way this is going to work is IEEE floating point, which is what fdiv is implementing, is specified very tightly. That is to say, one implementation of IEEE floating point division has to return the same bit pattern as another implementation. That's one of the nice things about that particular specification is that it's fairly tight. So we ask ourselves, what would have been the oracle for fdiv? And probably it would have been Intel's existing 487 floating point unit, which had been around for some years by the time they were developing the Pentium. So what I think this shows, unless I've messed up sort of egregiously on the math somewhere, is that random testing would have been a perfectly good way to find the Intel Pentium fdiv flaw, presuming, of course, that we could have found a way to rig up a Pentium in concert with a 487 in such a way that they could have cooperated to do the testing. So now let's talk about the 1988 Internet worm. There are several interesting things about this Internet worm. Probably the main one is that it was one of the first worms that actually got widespread attention. It got this attention for good reason. If you remember 1988, the Internet was not particularly well known to the general public, and it had a relatively small number of users. And even so, this worm infected an estimated 6,000 machines. And while this is a really tiny number compared to a modern worm or a modern botnet or something like this, this was a substantial fraction of the number of machines connected to the Internet at the time. The way this worm spread is it used computers' Internet connections to exploit known vulnerabilities in the UNIX hosts of the time. Of course, at the time, the existence of a remotely exploitable bug wasn't considered nearly as serious as it would be considered today because, of course, the 1988 worm and all of the subsequent ones hadn't happened yet. One of these bugs was a buffer overflow exploit in the finger daemon, and this was a service that would run on UNIX machines of the time, and the finger daemon would let you query a remote machine to learn about whether a user was logged in to that machine and some other stuff. And so now let's ask the question, would random testing have changed the outcome? Well, it seems extremely likely not because these bugs were known at the time. On the other hand, let's ask a little bit different question. Could this bug in finger daemon and lots of other bugs like it have been found by random testing? And the answer to the question is probably yes. In fact, if we go back to the original fuzzing paper, one of the bugs that was found was caused by the same programming practice that provided one of the security holes to the Internet worm. So basically, even in its original fairly weak form where fuzzing was done with completely random data, it was finding the kind of bugs that were causing security holes. This remains true to this day, but fuzzers are used to find a lot of exploitable vulnerabilities in hosts that have Internet-facing services. So in summary, it could have found the kind of bugs that the worm exploited and others like it had people been running fuzzers a couple of years earlier. What we've been doing in this unit is looking at a progression of random testers, from the simpler ones to the more complicated ones. And as we've seen, one of the major things creating difficulties for us is that structure is required in inputs. What we're going to do now is look at the next level of structured inputs, and that level of structure is required for random testing of APIs. So if you remember from our pictures earlier in the course, we have some software under test, and it's providing an API or several APIs for other code to use. And this is the kind of APIs that we're going to focus on testing. What an API is is basically just a collection of function calls that can be invoked. And what we're going to find when we test APIs is that a single random test is not going to be something extremely simple like a credit card number or a couple of inputs to a single API call but rather, a single random test is going to constitute a string of API calls. There are a couple of things that make this situation more complicated than what we've seen so far. The first possibility is that there are dependencies among API calls. What this means is there might be certain orderings of API calls, perhaps even combined with dependencies on the arguments to those API calls, that are either illegal or undesirable, and we're going to need to teach our random tester to avoid those. So let's just for the moment take the example of randomly testing a file system. What file systems are are pieces of code that take the raw abstraction exported by a hard disk driver--so here's the disk itself. The disk is going to talk to a driver. The driver is a piece of code running in the operating system that understands the details of interacting with whatever kind of particular hard drive we're dealing with, and what it exports is the abstraction of a block array. A block array just means that from the point of view of the file system, what the hard disk looks like is an array of blocks where blocks are numbered 0, 1, 2 all the way up to some very large number for a modern hard drive. And so the purpose of the file system is to take this low level interface of a disk block array and build on top of it a hierarchical namespace--that is to say, a directory structure provided by file systems in UNIX or Windows-- and then those directories, of course, contain files and files are just variable sized bags of bits that we can create and delete dynamically. And so the file system has to manage all of that structure just to manage the free space; it has to efficiently respond to all sorts of calls that perform file system operations. And so let's look at what the file system operations are. The question is, what are the contents of the file system API? We have things like mount and unmount, and these are the calls that invoke the file system code in the first place. So if a file system is not mounted, it's just sitting there on disk. It's not being used at all. Once it's mounted, its contents are available to the operating system. We have calls like make directory. Mkdir creates a directory, and remove directory deletes one. Open can be used to create or just open an existing file. Unlink deletes one. I'm talking about the UNIX style file system interface. And then there are a bunch more calls. So if we want to do random testing of a file system, we're going to be issuing sequences of calls in the file system API. Before we get into the details, let's just think about why we might want to do random testing of a file system. First of all, file systems are fairly long and complicated, and so I looked at the size of 5 of the file systems commonly used on Linux, and they're all between 15,000 and about 70,000 lines of code. File systems end up being substantially large chunks of code, and what's more, the integrity of our data depends on the correctness of that file system code. So for example, if we save some critical data to the disk and the file system messes it up--that is to say, it saves it in some wrong place or corrupts it in some other way--then we're not going to get that data back ever. And so it's really, really important the file systems work well. Now let's look at these calls and try to figure out what the dependencies are among them. First and most obviously, nothing is going to succeed, no file system call is going to succeed, until we've mounted the file system. Similarly, no file system call will succeed after we've unmounted it. So we want to be careful with mount and unmount. We don't want to just freely mix those into our stream of random API calls that we're making because if we do that, we're going to be mounting and unmounting the thing so often that the file system code is never going to get into the kind of steady state that we're going to need it to be in in order to do effective random testing of it. So similarly, some of the other calls in a file system, like reading and writing to a file, are only going to succeed if the file has been opened. And so if we do something silly like providing completely random arguments to open read and write, then the odds of us actually successfully reading or writing to an open file are going to be extremely low. So we're going to have to do something more systematic. We're probably going to have to do something like open a file and keep track of the fact that it's open so we can randomly generate read and write calls into it. There are probably some other dependencies as well. So what we're seeing is that if we want to do effective random testing of a file system, we're going to need to track these dependencies, at least in some rudimentary fashion, in order to issue a sequence of API calls that's going to do reasonably effective random testing of a file system. The second issue that starts to come up as we perform random testing of APIs is that our tests--that is to say, a sequence of file system operations in this example-- are going to become quite large, and that's going to beg the question, how are those tests represented? In the examples we've seen so far, test cases haven't had any sort of official existence of their own. What we've rather done is mix the software under test and the test driver code in a single piece of Python code. So now if we do this, what's the actual form taken by the test cases? And the answer has to be that those test cases only exist as ephemeral objects in memory. For most testing purposes, that's fine. The driver code creates the test cases, passes them to the software under test, looks at the results, and keeps going until it finishes. There are a couple cases in which that's not so good. One of them is where we find a particular test case that makes our system fail and we'd like to save off that test case for later use in regression testing. And so what we end up having to do to make that work is writing some separate code which also lives in the Python program to load and save test cases. And so now this module is going to be doing file AO in order to load and save test cases, and most likely those test cases are going to be saved as text files or something. So for example, if we wanted to represent a sequence of file system calls as a text file, we could just have the word mkdir followed by whatever the arguments to mkdir are, and then the loading and saving code would need to either parse those files in order to create a test case in memory or unparse them-- that is to say, pretty print them to disk in order to save a test case. And so there's a couple advantages of doing this. First of all, as I was saying, it facilitates regression testing. And the second reason is that it's often the case that when random tests become extremely large, we need to turn them into first class objects--that is to say, objects living on disk using some sort of a save and load routine--in order to perform test case reduction. Test case reduction is a piece of technology that's very often combined with random testing, and what it is and what it does is takes a large test case that makes the software under test fail and turns it into a smaller test case that still makes the software under test fail. We'll look at more detail and we'll look at test case reduction in more detail later on in this course. Now what I'd like to do... ...is investigate in more detail random testing of a specific API, and that's the bounded queue that we've already looked at a couple times in this class. So let's take a minute and review the code for the bounded queue. So what the queue is is an abstract data type. It's providing several methods. So for example, we have a constructor, an empty call full. Enqueue adds something, dequeue removes something, and checkRep is our assertion checker that does sanity checking over the internal state of the queue. So now let's look at the simple test routine that I wrote. What it does is it creates a queue of 2 elements, adds some stuff to it, asserts that the proper values are returned, and removes those things from the queue, again asserting that the proper things were returned. And notice also that we're calling checkRep after every queue operation. So this is a non-random test for the queue. What I'd like to do now is, using the Udacity API, write a random tester for the queue. So let's just talk about this a little bit--how should it work? It's clear that the random test, like the deterministic test here, is going to need to start off by creating a queue. It's also clear that after performing any queue operation, the random test should call checkRep. The thing that's not clear is how do we decide what API function to call, and how do we know whether the result is correct? One thing you can do is randomly call functions in the queue's API with random arguments and just keep asserting checkRep and not really care what comes out. That is to say, you can just randomly invoke queue operations in a loop and wait for checkRep to fail. On the other hand, it's not that hard to do better. The first thing is we know how many elements the queue holds, and we also know whether any particular addition or removal from the queue succeeds or fails. And so it's pretty easy using an integer to keep track of how many elements are currently used in the queue. And what we'd like to do in that case is assert that any enqueue operation to a non-full queue succeeds and any enqueue operation to a full queue fails. Similarly, any dequeue operation from an empty queue needs to fail, as we see a little bit below, and any dequeue operation from a non-empty queue should succeed. So the third thing you can do--and you don't need to do this to pass the quiz, but it's something that would be nice-- is actually keep track of what values should be coming out of dequeue operations. And so let's just take a minute to see what that would look like. The bounded queue that you're testing is based on a Python array, but it turns out that it's really easy to emulate the operation of that queue using native Python data structures. So basically, if you wanted to do this, what you would do is you would declare a Python list that's initially empty, and every time I want to enqueue something in our bounded queue we also append it to the list. We have to be a little bit more careful than this because if the enqueueing of an item fails, we don't want to append sums to our list. Similarly, when we dequeue something for the bounded queue, if the dequeue operation succeeds, we want to issue l.pop(0). The way this works is when something gets appended to a list, it gets appended at the end, and l.pop(0) takes something off the beginning. So basically, we're emulating a queue using native Python data structures, and our emulated queue isn't going to be as fast as the native queue which is based on the array, but that doesn't matter for testing purposes. And so if we do these kind of operations and we do them in a random testing loop and we insert the appropriate checkReps, then we can build really a quite effective random tester for the bounded queue using not that many lines of code. So what I'd like you to do now is go to the API and write a small random tester for the queue. Now let's really quickly look at mine. It doesn't necessarily need to be the case that yours looks very much like mine at all as long as it accomplishes an effect that's somewhat similar. So what we're going to do here is set N, the size of the queue, to 4, create a new queue of that size, and immediately call checkRep because basically, every time we do a queue operation, we'd like to do the checkRep. So if anything goes wrong with the queue, we detect it as early as possible. We also initialize an l to be empty. So this l and the q are always going to contain the same number of elements in the same order, so l and q are always going to have the same contents. So now we're going to do 10,000 random tests, and each test is going to be randomly either an enqueue or a dequeue. And if it's an enqueue, we're going to enqueue a random integer. After enqueueing, we call checkRep, and if the enqueue succeeded, we're going to append the item that we enqueued to our list which is mirroring the queue's contents and also increment a variable saying how many times this operation succeeded. On the other hand, if the enqueue doesn't succeed, I want to assert that the length of our list is equal to N-- that is to say, the list has the same number of elements as a full queue-- assert that the queue is full, call checkRep again, and also increment a counter indicating we performed an add operation on a full queue. We're not going to talk about why these counters matter just quite yet, but we're going to look at this a little bit later. The dequeue structure is exactly analogous. We're going to issue a dequeue operation with 50% probability, checkRep, and then we're going to do sort of the analogous empty checking operations that we did and empty checking operations if the dequeue operation failed. And if the dequeue succeeded, we're going to pop off an element off our list and also ensure that it's exactly the value that we expect. So once that loop terminates, we're going to go into a loop whose purpose is to empty out the elements of the queue and finally, after we've drained the queue of all elements, we're going to assert that our list which mirrors the queue is also empty. So that's my little random tester for the queue. What I'd like to talk about now is a little bit more detail on some of the different ways that we have to generate random inputs to our software under test. So let's start off with our diagram showing the random test case generator. If you recall from the diagram a little bit earlier in this unit, this test case generator has 2 inputs. It takes pseudo-random numbers from a pseudo-random number generator, and it's also predicated on some amount of knowledge of the input domain, and that's supplied by a human. We already looked at one example--testing the outer-- where essentially no domain knowledge was needed. That is to say, the random test case generator pretty much took its pseudo-random numbers and used them directly as test cases. We looked at a different example--that is to say, the credit card number generator-- where pseudo-random numbers, part of the input, and they weren't directly parroted to the output but rather they parameterized an algorithm which created valid credit card numbers. And of course it's possible to carry that kind of logic farther, as in the example of where we're testing a file system and we need to generate valid sequences of API calls or testing a web browser where we need to actually generate well-formed HTML in order to test certain parts of the browser. But it turns out all of these ways of generating input are variations on a single theme which we call generative random testing. Generative random testing simply means that inputs are created from scratch. There's an entirely different approach called mutation-based random testing. Inputs are created by randomly modifying existing non-randomly created inputs to the software under test. And so what that does is changes our diagram a little bit so that we have a separate source of input which is the existing test case. One thing we might ask ourselves is, which approach is better? As far as I know, there is no cut-and-dried answer to that. Sometimes generative works really well; sometimes mutation-based testing works really well, and it really just depends on the situation and how much time you have to spend. In general, my view is that generative testing might be a little bit better at ferreting out really weird errors, but it's a lot more work to create entire random tests-- at least if there are sophisticated constraints on their form--from scratch. Mutation testing, on the other hand, has a different strength, so let's take a look at that. So here we have the figure that we know and love of the input domain for some software under test. If we think about what the generative random tester is going to do, what it's probably really going to do is cluster them in some part because it's very hard to cover the actual full space of inputs. But in any case, they're going to be spread out over some part of the region. What a mutation-based random tester is going to do is start with some known input, and what it's going to do is randomly modify it, and so it's going to end up with test cases that in some sense are kind of in the same neighborhood as the original input. And so our mutation-based random tester is going to be able to access points like this. If we start with a different input, it's going to be able to get points like this. And so what's happening is we're exploring interesting parts of the input domain, and it could be that we could have never reached this part of the input domain using any kind of a generative random test case generator that we have time to make. So this approach is extremely useful and extremely valid. But on the other hand, it's generally limited to exploring sort of a region of the input space that's close to the starting point. One thing that you should be asking yourself right now is, what are the operators that we use to mutate test cases to create new random test cases? And so let's look at that a little bit. The most obvious thing to do is start flipping bits. So for example, we take a Word document that we actually saved out of Microsoft Word-- maybe it's a couple megabytes long; it's some huge file that we've created-- and we flip maybe 10, maybe 100, maybe a couple thousand bits in it, we load it up in Microsoft Word, and we see if we can find a part of the range for Microsoft Word that causes it to crash. Another thing we can do--and this is one of the techniques often used by penetration testing tools based on fuzzing-- is modify selected fields. And the model here is that our test case has some known structure-- that is to say, it's a valid HTTP request. What we're going to do is target parts of the HTTP request that are known to frequently lead to vulnerabilities in servers and we're going to selectively modify those. So we might, for example, take the request size and just replace it with some random number and see if that triggers a buffer overflow in the web server. Another thing we can do if we have a parser for our test case is modify it using its grammar. So we can, for example, add or remove or replace tokens in a test case or also subtrees of the abstract syntax tree. So let's finish up by looking at a short mutational fuzzer. What I have here is a 5-line Python program that was made kind of famous by Charlie Miller's talk, "Babysitting an Army of Monkeys." This talk was pretty fun to watch, so I recommend that you Google that and look at it on YouTube. What Charlie Miller claims in this talk is that he found an enormous number of bugs with this 5 lines of Python. And so what it turns out is that the 5 lines of Python are missing quite a bit of code that you need to make this work in practice, and I've added comments sort of explaining what these are. So what we would first need to do is load a file into a buffer in memory. And so the file that we're going to load is going to be a PDF document, a PowerPoint document, a Word document--whatever it is that we want to mutate for purposes of creating a random test case. What this code does is first assigns into this numWrites variable, which is basically deciding how many places inside the file that we've loaded we're going to mutate. And so now we're going to loop over that range, and for every iteration of the loop we're going to make up a random byte then make up a random location within our buffer and then mod whatever value was there with our random byte. So what does that mean? We're basically totally randomly picking some places in the buffer to mess with and messing with them, then we're going to save the buffer back to disk, run whatever our application is, so run Windows Media Player, PowerPoint, Acrobat Reader--whatever it is we're trying to fuzz-- and look at its exit code. And so its exit code from the operating system is going to tell us whether it died or whether it didn't die. And if it doesn't die, then we're going to have to wait a little bit and then just kill it. So that's a failed test case. If it does die, then we're going to want to save the buffer off into some sort of a location where we can examine it later, and in either case, then, we're going to start over. So hopefully what would happen if we made this code real by writing the rest of it-- and this is going to be a challenge program for your assignment at the end of this unit-- is we would basically have a large pool of documents or files or whatever that we can use for fuzzing. We'd start this thing up on some sort of a fast machine-- ideally, we'd start up a copy for every core on the machine-- and we'd go on vacation, we'd take a week. And when we came back, ideally, it would have found a bunch of vulnerabilities. It turns out that a lot of people have been using this sort of tool on the common utility programs like Acrobat Reader and PowerPoint for a number of years now. So it may be the case that if you do this, you're not going to find anything on the latest version of Acrobat Reader. And in fact, if you do find something, it's actually pretty interesting, and I hope you'll share it on the forums. On the other hand, if we want to actually get some easy successes-- and this isn't for purposes of finding real bugs; it's for purposes of understanding random testing better-- what you should do is find some old versions of Acrobat Reader or PowerPoint or whatever, get those on your machine and fuzz those. And almost certainly, using some sort of an infrastructure like this, if you wait long enough, you'll be able to find some sort of a problem in those applications. Now we're going to talk about oracles. We really probably have deferred this topic for too long. Oracles are extremely important for random testing because if you don't have an automated oracle--that is to say, if you don't have an automated way to tell if a test case did something interesting-- then you've got nothing. And in fact, Richard Hamlet in his sort of well-known article on random testing said that "random testing has only a specialized niche in practice "because an effective oracle is seldom available." And this is something that I don't actually agree with. What we're going to see is that sometimes you have to use some imagination, but really, there are potentially quite a few oracles available and that almost always we can make something work even if it's just a weak oracle like watching out for crashes. So what I'm going to do is organize the oracles that are suitable for random testing into a collection of categories. So we're going to start off with weak oracles. Weak oracles are some of the ones that are most useful in practice, but I'm calling them weak because they can only enforce fairly generic properties about the software under test. The most important weak oracle is just detecting whether or not the application crashed. What a crash usually really means is that the system under test violated some rule that the hardware imposed like, for example, memory accesses have to be aligned, or that the operating system imposed like, for example, the application isn't allowed to try to write to memory that's owned by the kernel, and in response to this violation, the operating system has decided to terminate the process in which the software under test is contained. The second most useful weak oracle is violation of a programming language rule. In Python, if you try to access an element of a list that's out of bounds and if you don't catch the resulting exception, then your Python program will get terminated. That's an example of the violation of a language rule killing an application and serving as an oracle that tells us basically that something went wrong. And basically, most any programming language has a number of rules like this and the stronger those rules are, the more effective of an oracle the programming language runtime can serve as. And one of the reasons why C and C++ applications crash so much is because the language is an extremely weak enforcer of runtime rules, and so basically, all of the enforcement falls upon the operating system and the hardware itself. Another form of oracle comes in the form of enhanced execution environments. I've already used the example several times, but in C or C++ we could run the compiled program under a runtime monitor like Valgrind which provides an enhanced execution environment which checks a lot of rules that aren't checked by the regular C and C++ runtime or by the operating system. So for example, Valgrind will terminate our process if we access beyond the end of an array, and in practice, that's an extremely effective form of oracle. If that kind of a rule violation happens during random testing, we absolutely want to know about it, and Valgrind provides that service. Another example of an enhanced execution environment would be one that checks for, for example, integer overflow of problems or floating point problems. So a medium oracle is going to vary in its degree of checking power between the weak oracles we've already said and the strong oracles that I'll talk about in a few minutes. And so far in my list, and I could be missing something, but so far I only have one example of a medium power oracle, and this is assertion checks that the programmer has put into the software. The reason I call this a medium power oracle is it provides quite a lot more application-specific kind of checking than do these weak oracles. But on the other hand, it doesn't guarantee anything even remotely close to actual correct operation, and that's generally what the strong oracles are going to do. So let's look at those. Strong oracles are extremely useful and you should always use one if you can possibly find one. We're going to have a bunch of examples here. Let's start off with one of the important ones which is having an alternate implementation of the same specification, and if you think about it, this is what my random tester for the bounded queue did and perhaps what yours also did, and what I mean by that is my queue tester included a second implementation of the same queue abstraction. This one was implemented with a Python list and this was the second implementation of the same abstraction. We could use to actually check that the queue that we're testing gave the right answers. That's a very strong kind of a check. Another example that we use in my compiler testing work-- we do what's called differential testing of compilers, and what that basically means is that if we have multiple implementations of the same compiler specification--that is to say, for example, multiple C compilers-- we expect them to behave the same given equivalent inputs. Another way that we might get an alternative implementation is simply looking of an older version of the software that we're testing. This is checking not necessarily whether the software is correct but just whether we've broken it. And so remember for example that I said that in S.U.T probably could have tested the Pentium floating-point unit against the 487 floating-point unit. Another kind of old version oracle that tends to be extremely effective is after a refactoring change-- that is to say, the change of our source code base that isn't intended to have any functional effect-- we could use the version before and after refactoring. We can do differential testing of the version before and after refactoring and in that way try to get a pretty good idea that the refactoring did not actually break the code. The best kind of alternate implementations you could have is a reference implementation. That is to say, some sort of implementation of the specification, which you are after that you can trust. For example, following to implement an extremely high performance Python compiler, what I would do is I would use the regular C Python implementation as the reference implementation, and that would be treated as correct. The next kind of strong oracle that I'd like to talk about is what I would call a function inverse pair. We have available to us some sort of function and also its inverse, and we can use these as a pair to do strong checking of correct behavior of the software under test. If we remember, a couple of units ago I gave the example of where you could test an assembler by pairing it with a disassembler and also an exhaustive enumerator for instruction encodings. And then what we would do is take all valid instruction encodings, disassemble them into assembly language, reassemble the assembly language into machine code, and do a comparison on the output. So that's what I'm talking about when I mention function inverse pairs. If we think about it, we can find these function inverse pairs a lot of places. Another example is encryption and decryption, compression and decompression, saving and loading of files, and this isn't as trivial as it sounds. I'm not talking about an example where we take some sort of a bitwise representation in memory and dump it literally to disk and then load it again. That's unlikely to go wrong. On the other hand, many times when a program saves its state to disk this is a pretty non-trivial operation. We might, for example, be replacing machine pointers with some sort of an on-disk representation, and then the load operation is going to contain the inverse, but it's pretty easy to get that wrong. Or maybe perhaps we forgot to save part of the program state and then when we load it we'll end up in a different state, but we'll be able to catch that by treating save and load as a function inverse pair. Transmit and receive across some sort of a loop back interface-- that is to say, a network interface that connects a machine to itself-- can serve as a useful oracle because in some cases there are non-trivial transformations of data representation that happen as part of the transmit and receive operation. And finally, encoding and decoding of, for example, media formats serves as a final example of a function inverse pair. I know there must be a bunch of these that I'm missing, but these are the ones that I could come up with just sort of sitting down and brainstorming for a couple of minutes. If you have a function inverse pair, use them together and try to see if you arrived at the original data. And of course, this may not always be as easy as it seems. For example, when we save pointers to disk and we swizzle them into some sort of on-disk representation, when we load them, the pointer values may have changed, but the shape of the data shouldn't. So some sort of abstraction is going to be required to compare the saved and loaded version. Similarly, the encode and decode of media files often involves a lossy encoding step. So if we encode something as a JPEG and decode it, then of course the bits that we get back are not going to be the bits we started with. But on the other hand, they shouldn't be too different. If we can somehow quantify how different they are, then we might be able to use even a lossy encode/decode pair as a test oracle for something like a JPEG encoder and decoder. The final strong oracle I want to talk about is what I'm going to call a null space transformation. This is where we take a random test case or any test case, of course, and we make some sort of a change to it that shouldn't affect how it's treated by the software under test. And so one thing we could do just as a trivial example is we could start with a simple Python function. This is the input to the null space transformation. So here we have foo of a and b. It returns the sum of a and b. And one possible null space transformation on this would be to add a level of parentheses or maybe several. This is a null space transformation because we've transformed the test case, possibly in a non-trivial way, although here we've done something pretty trivial, but we've transformed it in such a way that we know that the meaning of the test case shouldn't have changed. So now we can do something very much like differential testing, but instead of taking the same program and running it through 2 implementations of Python we're going to take 2 programs which are semantically identical and run it through the same implementation of Python. And if that implementation of Python treats this differently-- that is to say, it interprets this code in such a way that it returns some different answer-- then we've found a bug in it. So of course we could always do things even more elaborate. Here we're calling some sort of an identity function on a-- that is to say, some function that is guaranteed to return the same value of a that it's called with, although of course that's not going to be apparent to the interpreter-- and then instead of adding b to a, we're going to subtract -b from a. So another little null space transformation. I've heard of these kind of things. I don't have any personal experience using null space transformations to find bugs, but I've heard of people doing this kind of thing, and apparently it can be useful. So what have we done here? What we've done is seen that in realistic situations, there are a lot of potential oracles available for performing random testing. And so the conventional wisdom--at least as it's expressed in that article or that chapter on random testing--that oracles are almost never available I think is probably not right. It's often the case that if we think about it for a while, that if we're potentially willing to invest some time in creating a good oracle, then we can very often find something to use for random testing. Welcome to problem set 4. In this problem set, I'm going to write a random tester or fuzzer. In our example, we're going to be fuzzing PDF files and our application is a couple of PDF readers. This example is taken from Charlie Miller's fuzzer "Babysitting an Army of Monkeys." I encourage you all to look at these sites. To start this fuzzer, we begin by choosing a random file and choosing a random PDF application. We then read in all the bytes from our file and store it in the buffer. Next, we run a random function to determine the number of writes that we are doing based on FuzzFactor. Read up on this FuzzFactor to be 250, and you can see as the FuzzFactor increases, the number of writes decreases. Based on the number of writes, we create a byte, a random byte, and choose from our buffer one of the bytes that we're going to overwrite. We then overwrite that byte with a random one and we continue to do that until we get to the number of writes. Finally, we write all our new bytes to a file, a new file, so we don't overwrite our old one, and we then uses a subprocess margin to open our application with our new file. Now, when you run the script, you might see your program go crazy, and if that happens, then you've done it correctly. Now, there are few things I want to point out. This type of code is the heart of the fuzzer This is all that you really need for this program to work. A couple of other things noticed that we don't do any logging. This is bad practice for all programmers. We should always do logging in your applications to make the bugging easier. What I want you to do is write a fuzzer based on the one that we gave you for real world applications. In our example, we fuzzed has a PDF files. After you bring your fuzzer, I want you to go to the forums and link to your fuzzer, show what you fuzzed, describe any bugs that you found, and explain how you would improve your fuzzer in the future. Post all these things to the forum, and when you're finish, check this box. Now, of course, you can check this box without actually having done the problem set, but then, what will be the point. Good luck on problem set 4. We're going to talk about more advanced issues on random testing. And so we're going to talk about things like test oracles. This is always a big issue when we're using random testing. We're going to talk about how random testing fits into the bigger picture--that is to say how ideally a random tester that you develop will sort of co-evolve with the software that is testing. They can both strengthen each other over time. We're also going to talk about perhaps the trickiest topic of all in random testing which is tweaking and tuning the rules and probabilities involved in making a random test case so that they'll actually do a good job. And so together these issues and some others that we're going to cover, constitute what I would sort of think of as kind of advanced random testing issues. What these are going to do is give you a basis for creating a really strong random test case generators. Now I'd like to talk a little bit about how random testing fits into the larger software development process. So first of all, I just want to start by asking a question, why is it that random testing works at all? And I don't think the answer to this question is actually known in any kind of a detailed sense. But we do know parts of the answer. The big part is that random testing is based on very weak hypotheses, about where bugs live. And what I mean by this is every test case that we run is a little experiment. The input to the experiment is a test case and the outcome of the experiment is a bit of information about whether the system worked or it didn't work. What random testing does is it makes the conditions, under which we are running the experiment weaker. We don't have to guess about some particular thing that might fail. What we're rather doing is guessing about a whole class of things that might possibly fail. This turns out to be powerful because, given the very complex behavior of modern software, people don't seem to be very good about forming good hypotheses about where bugs lie. Second reason is people tend to make the same mistakes while they're coding and while they're testing. For example, if Iforget to take into account some sort of a special case while I'm implementing my system then I'd probably also forget to test it. What I mean by this is that if I forgot to implement some feature or if I miss-implemented on feature, I'm not going to test the stuff that I did wrong because if I was able to think of the test case, I probably would have got the feature right in the first place. Random testing to some extent get us out of this problem because it can construct test cases to test things that we don't actually understand or know very well. The third reason is there's a gigantic asymmetry between how fast computers are and how fast people are. Evidenceen if the random tester is mostly generating stupid test cases, if it can generate a clever test case, maybe one in a million times just by getting lucky, then that still might be a more effective use of our testing resources than writing test cases by hand. And there has been a similar finding in other areas. For example, the game Go has an extremely large state space and it was traditionally thought that computer Go players were just never going to be very good. The game was just too hard. What turned out to be the case is, is that today's computer go players are quite a bit stronger than previous ones, and they're based on what are called Monte Carlo methods, which are simply randomized methods for exploring small parts of the Go state space and it turns out that this is often good enough to create respectively strong players. They're still not nearly as good as the best human players, but they do pretty well. And I feel this is exploiting sort of a very similar insight to random testing where we probabilistically explore these spaces that are very large and even if we can't achieve any kind of meaningful coverage, we can still often find interesting things, especially if we have this extremely fast 4- and 8-core machines that are really quite cheap today. Now, I'd like to ask a slightly different question, which is why random testing is so incredibly effective on commercial software, at least sometimes? What I don't want to do here is start some sort of a debate about whether random testing is or isn't effective. I think there's pretty ample evidence, so for example the fuzzer papers that we discussed yesterday or the talk that you can watch online, babysitting an army of monkeys. People shown that random testing really is effective on commercial software. Why is that? I'm going to give some opinions, and of course, you should feel free to disagree with these. The first reason, I think, is that it's because the developers of the commercial software systems aren't doing it or at least not doing it enough and I think it's pretty clear that the kind of bugs found, for example, in the fuzzing papers, or in Charlie Miler's talk would'nt have been there in the software if the developers of Adobe Acrobat or the Unix utilities have had a reasonably aggressive random testing program, and remember that some of those bugs Charlie Norris was talking about were secured liabilities. These are things that they really don't want in their software, and so what I'd argue-- Software development efforts that don't make proper use of random testing are flawed, and the reason that these efforts are flawed is because modern software systems are so large and so complicated that test cases produced by nonrandom processes are simply unlikely to find the kind of bugs that are lurking in these software systems. What that leaves us with is a question: what should they have done? How is random testing supposed to work? Let me give some ideas about that. What I'm going to do here is show sort of a rough software development timeline with a releasing software over here and early development stages over here. What we've looked at mainly so far in this course is random unit testing. We're developing these software units and what we're trying to do is make sure that they are robust enough. Then we start composing them together later, they'll be a solid foundation. We looked at several cases of, for example, with a queue--bounded queue here. We looked at fuzzing the interfaces that it provides. We also looked at an example of random fault injection and that was for the read_all function, and if you remember, that was the function that was supposed to cope with the fact that the Unix read system call can display partial success. They were doing fault injections that were fuzzing the interface used by the read_all call not the interface that it provides. And so, like I said, what we want to do is ensure as we're developing the modules that we're creating robust pieces of software whose interfaces we understand and that are going to be solid foundation for future work, so I would start developing more elaborate software stacks. It's going to be the case that some of our random testers become useless. For example, if we have a Q instantiated here that is used by some more sophisticated piece of software, we no longer are interested in the ability to randomly test the interface provided by the Q because it's simply being used by the rest of the software. On the other hand, other kinds of random testers such as those that come in at the top level and those that perform system-level fault injection are absolutely still useful. In fact, fault injection of things like erroneous responses to system calls are really important things to test larger pieces of software with because typically, those kind of errors can result in failures propagating all the way through our software stack and we'd really like to understand how our system responds to that sort of thing. To be part of something that's more of a complete product our focus should be on external interfaces provided, so this is going to be things like file I/O and the graphical user interface, and so, if you recall those fuzzing papers, we're fuzzing exactly these sorts of things. Here, we're delivering random bits to the file interface and they were delivering random gooey events to applications and knocking over a pretty large proportion of applications that they tested. What we want is to build this system-level random testers as early as possible in the development process, and there are a number of reason for this. First of all what we'd like to do is start off with a simple version of our system that doesn't implement very much functionality and then we want to use sort of a weak fuzzer. That is to say, we're going to test it with values that are maybe perhaps not that interesting. And the good thing about this combination is these weak random tests probably are going to find some flaws. What they are not going to do is flood our developers with huge numbers of bugs like might happen when we use an extremely strong random tester. There is sort of no easier way to demoralized software developers than to hand them a really big pile of bugs--nobody wants that. What they're gong to start to do is ignore those bugs and get back to getting work done. What you want to do is give people a slow but steady stream of important bug reports and let them fix this as they go on, and that's another reason why it's nice to give people with continuous stream of bug reports, which is that what these do is help show us flaws in our software. They help show us interfaces that we don't understand. They help show us modules that end up being extremely weak for one reason or another, and it basically helps us better understand where our software development effort is going wrong. Now, if instead of giving people maybe a couple of bug reports a week for a year, we give them a hundred bug reports in the very end All they're going to do is triage to find the five most critical bugs, and fix them using hacks. So nobody learns anything. Everybody is angry. Nobody is happy. Rather what we like to have done is have been doing random testing all along and using it to spot weakness in our software. The other thing that happens is, as our software evolves to be more robust as we move toward releasing it, we're evolving our random tester to be stronger and stronger. That is to say, maybe this week we have a feature where we generate a new kind of random input that we haven't generated before. Also it's going to generate some bug report, and we'll fix them, and our software evolves to be more robust. If we keep doing them not just over weeks but over years, what we'll end up with is a random tester and a system that have sort of gone through this co-evolution process where they both become much stronger. That is to say, we've evolved in extremely sophisticated random tester, and we've also evolved the system that gives robust with respect to the kind of vaults it can be triggered by that random tester. What I firmly believe is, and of course I can't prove this and you're free to disagree, is that if, for example, Microsoft had done this from the beginning, Adobe had done this from the beginning, and these other companies that end up with lots of security vulnerabilities had fuzzed their products all the way through the development chain they'd end up with far fewer of this sort of nasty crashes and critical security vulnerabilities that they are always scrambling to patch, and that anybody with a fuzzer seems to be able to find without a whole lot of effort. Or at least that's been the case in the past, and it's possible now that with more widespread user of more aggressive fuzzers with that kind of error of easy security bugs in popular products maybe is hopefully starting to kind of tail off. We just talked about random testing in this larger context as part of the software process. What I'd like to talk about now is one of the more advanced and frankly difficult parts of building a really good random test case generally. As I was saying a little while ago, the best way to start random testing started early in the software process and starts simple, and what's going to come out of a simple random test case generator is often a collection of testing results that are not particularly great. That is to say we get shallow coverage, we violate many constraints on valid inputs and overall it doesn't work all that well, and probably we doesn't find that all many bugs and that's for testing a particularly weak system like for example, the UNIX command line utilities from around 1990. The thing to do with these results, look hard at the test cases. Think hard about how they relate to the overall shape of the input domain for the system. For example, it maybe that we are generating a super set of the input domain that is to say we're generating invalid inputs, but it's also likely that in many cases we're generating a subset of the domain. That is to say we're generating test cases that fail to explore some parts of the behavior space of the software under test, and that's going to lead to missed bugs. You should definitely look at the fact that the random test cases have on code coverage. Often this is extremely revealing cause as we've seen, it's really easy to write a random test case generator. It gets very shallow coverage of the software under test. The next step, here's what you've learn to adjust the rules that are in the fact for generating random test cases and to tweak the probabilities. If that sounds very vague at this point, then that's good because it is very vague. What I'm going to do a little bit is going to some specific examples. Of course, we can also add functionality to our random tester. As I was saying earlier, it's often good to do this in a very slow and incremental fashion so we avoid overloading people with bugs that they don't have time to fix, and this is an iterative process that doesn't stop as long as we're using the random tester. We should sort of kind of keep looking at it and trying to evaluate how we can better adjust the random tester to find interesting things out about the software that we're testing, and I don't mean to say here that this has to take a lot of time. Something you do is just anytime you have time every couple a weeks or whenever, just take a look at the test cases, take a look at the coverage, think about things a little bit, see if anything has changed in the software under test that might make you want to tweak the random tester, and basically you just to put it back into production. What I'd like to do now is look a few examples. The first is file system testing and we've already talked about this a little bit, so I'm not going to reintroduce this subject. If we start with a simple file system tester, what we're probably going to do is make a list of all the API calls that we'd like to test. We can mount and unmount a file system, we can open and close a file, we can create a directory and remove a directory, make list of all these kinds of functions, and basically just call them randomly with random arguments. Then, we'll look at the results and think about them and we will see is that probably our testing is highly suboptimal because, for example, if we're throwing unmount calls randomly into the max, we're going to end up on average operating on an unmount file system at least 50% of the time. Similarly, if there is no correlation in randomly chosen file names between different calls to open-close and read and write and such, we may effectively never actually perform a read or write call on an open file, of course, that's obviously undesirable. We're going to do then is, for example, first is special case mount and unmount. We're also going to need to, almost certainly, special case open and close, and namely, one thing that we might want to do is keep track in the random tester of the set or currently open files. And just these couple of simple things is probably enough to get a file system fuzzer off the ground. But there's more, we probably want to limit the size of files so we don't waste a lot of time writing many, many, many gigabytes to files and we could be using that time better and perhaps we also want to limit the height of the directory hierarchy in order that we don't generate incredibly deep directory hierarchies that aren't interesting. On the other hand, we may want to do exactly that. we want to know exactly test extreme deep directory hierarchies or extremely large files, but it might be the case that these are special cases that we want to test separately from the main body of our fuzzer. Okay, so what I hope is the general pattern is clear here. We start off doing something really simple. We probably observed this doesn't work very well and then we start special casing things in order to remove limitations of our random tester and, over time, so maybe over weeks or months or even years, This process ends up with a random tester that will be extremely strong. Let's get another example and this time will be quite a bit more specific. We already wrote a random tester for the little bounded line queue data structure and one thing we might want to ask ourselves is "Did that further do a good job at all?" Well, if you wrote a queue and find all the type of the bugs that I seeded in the queued data structure, then you probably did a pretty job but let's take a look again. We're going to do is we're going to look at the queue as a finite state machine. We're going to see what kind of states we can drive that machine into using the random tester. We're going to start off with an empty queue and 50% of the time, at this point, we're going to make a dequeue call, which is going to fail, 50% of the time, we're going to enqueue something resulting in a queue containing one element. Okay, from here, it should be pretty obvious what could happen. We can dequeue something going back to the empty state or enqueue something going to the two element state unless it is assumed that that's full. And so, the dynamic process that we're going to get when we run a random tester is some sort of a random walk through this finite state machine. And so, what we want to ask ourselves is "Does this random walk have a reasonable probability of executing all the cases?" And so here, probably the most interesting cases are dequeuing from an empty queue, enqueuing to a full queue, and then walking around the rest of the states base. And so, what I've done is, I rig the random tester with some extra static skipping, so you can tell us about this a little bit--so let's go take a look at that. We saw this earlier, although I was trying not to call attention to it, but what happened is when we add something to a queue that is not full, I'm going to increment the counter. Similarly, I'm going to increment the counter when we add something to a full queue when we remove something from an empty queue and when we remove something from a non-empty queue. Let's see. We want to make a two element queue to match my drawing. Now, let's run the random tester. Okay, here's what it tells us. It did 33,000 in sum adds to a non-full queue, 16,000 in sum adds to a full queue, 33,000 removes from a non-full queue, and 16,000 in sum removes from an empty queue. We can see is that at least thus far as got really crude coverage metric goes, and of course, we could have got similar information using a branch coverage tool, but it is a little easier and more general purpose, we can get coverage of our queue, and now, let's try something different. Let's imagine that instead of a state machine having just three states. Let's say that we had a queue containing 100 elements. The dynamics are going to be exactly the same, sometime leaving a lot of nodes in the middle. Okay, so we're still going to start at the empty state. We're still going to randomly walk around our finite state machine and the question is "Are we going to get good behavioral coverage of our queue in this case?" So, let's take a look. Now, let's look at a very similar case, where I'm going to visualize the execution of the queue as a finite state machine, but now we're going to have a much larger queue. We're going to have a queue that stores a thousand elements instead of two. We're going to have exactly the same kind of state machine, but its shape is changed. Instead of containing 3 nodes, it contains 1001 and so, I left out a vast area of state in the middle, and so when we randomly test this queue, we still are going to start at the empty state. It looks like with the dynamic side at this time. Here I have a queue with N = 1000 and nothing else has change When we run the random testing, this time we've done around 50,000 adds to a non-full queue, and we haven't done any adds to a full queue. We've done almost 50,000 removes from a non-empty queue and 10 removes from an empty queue. So, let's try to understand what happened here. So, what happened is and I'm going to come up with another view of that state machine. So, here we have the empty queue and here we have the full queue. and here we have the number of times we visit a particular state. and what's happening is with 50% probability we're randomly walking back or forward and we're going to be getting a situation where the probability of visiting queue states farther away from where we started drops off exponentially and actually they're good close form equations for being the probability of getting to any particular point, but as we saw using 100,000 tests, I believe, we never actually manage to make to 1000. Although, we did perfectly easily make it to a queue size of two. So, 1000 is particularly a large size for a data structure, we could easily have some sort of a queue to have 10,000 or 100,000 elements and the chances of randomly walking all the way out to the end become even more negligible and so the question we're going to have ourselves is, What do we have to do differently to a random tester to make sure to test this situation as thoroughly as tested to this situation and so there is no common answer to this kind of question. These kind of questions definitely are hard in practice and when something changes about the software under test, we might have to adjust the probabilities to compensate. Let's just look at one possible solution. One possible solution would be to bias the probabilities towards enqueueing. If the random number generator returns a number less than 0.6 instead of 0.5, we're going to enqueue and the other 40% of the time, we're going to dequeue. So, let's see what effect that has. So this time, we attempted to add a list to full queue a large number of times, but this time, we seem to have not particularly well tested the case of removing from an empty queue and let's run it one more time and see what happens. Yeah--so this time we didn't remove it from an empty queue anytime so we biased the probabilities too far towards adding, and what's going to happen is in any kind of random walk is as long as the probabilities are respectably balance it's going to take a long time to get somewhere unless we unbalance the probabilities significantly like we did with the 60-40 distribution. What they're probably doing in this case is make a configurable biased probability and now add a completely new random testing loop. We're now going to set that bias variable to be something sort of large unless x is odd. If x is odd, we're going to set the bias variable to be something less. Okay, so let's look at what we did. We took a random testing loop and we enclose it in a larger random testing loop and in that larger to random testing loop, we sort of made qualitative change to one of the probabilities that is to say we bias execution towards one of our API call in favor of the other and on even-numbered calls, we biased it the other way. So, what we're hoping to do now is create a situation where start of in the empty state, we migrate with pretty high probability towards full and bounce around there, and at the end of that particular configuration of the testing run,we're going to walk back We're going to do that, I believe, 20 times. And so what we hope is that this will test even for the fairly large queue a lot of its possibilities so let's see if that actually happens. So of course, this is going to take longer to run this time. All right. There we go, so I screwed up the even-odd test. Okay, so xmod2=1, that is to say if we had an odd numbered run through our outer testing loop, we're going to bias the probability towards removing from the queue. Otherwise, we're going to bias towards adding to a queue. Okay, good. This time, we added to a non-full queue 600,000 times added to a full queue almost 400,000 times removed from a non-empty queue 600,000 times and removed from an empty queue close to 400,000 times. This time, even with 1,000 element queue, by adding this bouncing back and forth behavior, we managed to do a good random testing. If you think about it, this idea of adding a new outer testing loop to a random tester is often a pretty good one. If you think about it, that's exactly probably what we would end up doing from the file system example. For special case, mount and unmount, probably what that's going to mean is hard coating are call to mount at the start of a random file system test, then executing a bunch of API calls, that is to say opens, closes, make directories and stuff, and at the end, we're going to hard coat and unmount call. What that fails to do is interesting stress testing of the mount and unmount logic, and so remember, if we call mount and unmount with equal probability as the rest of the calls that was too many times. But if we hard coat a call to those at the beginning and end of our entire random testing run, that's probably too few. What we can do is mount the file system, do a bunch of stuff, unmount it, and then enclose that in an outer testing loop to make sure that we sort of stress all the parts of the system that we intend to stress. We can imagine that the state machine from the file system case is considerably more complicated than the state machine for queue, but on the other hand, we could still do this diagram or something like it and we find that we need to adjust our probabilities. This is a sort of a deep topic and it's a difficult one. Let's take at one more example. We're going to look at the testing a couple of bitwise functions. What we're going to want to do is write two Python functions called high common bits and low common bits. Given two inputs (a) and (b) which are integers that have the same size, let's say 32 or 64 bit integers. Return integer with the same bit width with the higher the bits that the two inputs have in common are included in the return value. The first bit where the two inputs differ at a bit position starting from the high end is set and all remaining bits are cleared. Okay, so that's probably completely unclear. This is the list of requirements, so we're going to do an example. If (a) is 10101 and (b) is 10011, then the output is going to be the higher bits that they have in common. The 1 is common in sudden both inputs, so we return that. Zero is in common, so we've copy that to the output. This bit is different. The third bit is different so we want to set that bit and clear all remaining bits. Hopefully, that's good. So we copied bits that are in common until we find a difference. We set that bit and everything else is clear. We also want to implement a function called low common bits, that does the same thing except starting from the bottom end. We take the same example and we do the low common bits. Let's start at the bottom and see that 1 is common, so we copy it between the two. The next bit, the second bit from the bottom, is different so we set that bit and clear all the rest. And so what you might be saying to yourself at this point is, this functions look really silly. But this look like a contrived example that a professor would come up with just to mess with the students but does not actually the case. It's functions come from an optimized trie implementation. What a trie is, it's a kind of a balanced ordered tree, not incredibly unlike the splay tree that we already looked at and some of it's branch, but tries rely on bitwise operations to find the descendants of nodes in order to get really high performance and really low-memory footprints. All of that was just introducing these two functions, and so the deal is, is this really easy to write straightforward code for this? Let's take a quick look. So, here's highcommonbits. All we're going to do is walk through the bits in the input here. I'm assuming a 64-bit input. We're going to walk through the input, reversed, so from 63 to 0. And every time we see 2 bits being the same in a and b, we copy that bits to the output. As soon as we see 2 bits at the same position that are different in a and b, we set that bit position in the output and return it, and lowcommonbits are just the same. The only thing that is different is we looped from 0 to 63 instead of 63 to 0, but we do exactly the same thing we're copying bits in the output as long as they are common between a and b. And as soon as we see a difference, setting the bit and return it. What we have here is a really slow implementation. Even if we translate this implementation straightforwardly to see, it's still going to be really slow compared to optimized implementation. Optimized implementations of this are going to be potentially more complicated and potentially harder to get it right, but it might execute in just a handful of clock cycles as opposed to maybe a hundred clock cycles for the optimized C versions and probably thousands of clock cycles for the Python. Of course, performances are really going to be our concern here. We're just using this to illustrate initially that comes up and extremely optimize chart codes. The factor we're trying so hard to optimize these codes means we're going to have some concerns about whether they are correct or not. Let's talk about how we're going to deal with that. What we're probably going to do is write a random tester. The reason we're going to write a random tester is with 64-bit inputs, the input demand for either of these functions is going to contain 220 elements so it's far too big, too exhaustible test. We're going to be forced to do some sort of either systematic testing or random testing. There's is no way out of partial testing of these functions. The most obvious kind of random tester array would be make a random 64-bit integer, make another one. Now assert that our super super optimized highcommonbits function called with a and b returned the same results as a reference implementation that showed through Python version but sort of simple incredibly obvious code and maybe reference implementation at C, it doesn't really matter. We're just going to randomly test all sorts of configuration of the function. The question we're going to ask is, is this a good random tester? And what we might do is run the code coverage tool so let's go ahead and do that. Okay, so this is what a random test is going to look like, and now let's look at the implementation in Python. And we're going to see is that since all I have is the reference implementation, not actually going to be testing two implementations against each other, rather all I'm going to be doing is making up two random 64-bit integers and just running the high common bits and low common bits function on them. We're going to do is we're going to be able to see code coverage, but I'm not actually checking the output for correctness, and I'm running a 100,000 test cases, so let's run this under the coverage harness. As you can see, I'm asking here for branch coverage. So, that's a lot of output, and the coverage should have been computed, and we see here is out of 25 statements around 23 of them a couple of statements were missing, and we had couple of partial branch coverage results. What you can see here is we've executed the body of the loops, and we've returned out of this path, but we failed to take the exit branch from the loop, and we failed to execute the return statement. The same thing is happened over here. So, let's asked ourselves why completely random testing with valid inputs a hundred thousand times did not manage to cover this. Well, of course, this is pretty easily to see, so we're generating two totally independent random 64-bit numbers, and the only way we can reach this case is fairly the same. So what are the odds of two randomly generating 64-bit integers being the same? They're extremely low compared to a number of test cases running. We're never going to test this case, so for testing optimized implementations of these functions in such matters quite a bit more. When we do optimize implementations of these functions, we're going to use specialized instructions that modern architecture support with providing bit counts. This is going to boil down to at least before using GCC as compiler functions like this builtin clz and builtin ctz, which as you can see, clz returned a number of leading 0-bits in x starting at the most significant bit position, and we can use this to implement one of our bit functions by XORing to the other the two operands to turn common bits into leading 0 bits, and so, we can do is using XOR operation to turn high order common bits in our arguments into leading 0 bits then we can use this builtin clz to implement an extremely fast version of the high common bits function and similarly, this builtin ctz can be used to build an extremely fast low common bits. If you see here, if x is 0, that is to say, if the two inputs are equal, the result is undefined. The implementation here has allowed to do something really weird in the case where we passed in two operands that are the same, but as we saw a little random testing here has only a negligible chance of actually generating two arguments that are actually the same. It's really a bad random test for this case. Let's go back and try to do better. A better random tester for these functions would set off making a completely random and then randomly flipping a random number of bits, so here we're going to make a random number between 0 and 63 and flip that many bits. We can make a much better random tester for this particular code by making a random and then making b a mutated function of a, so it's like that how to do that. We're going to still make a totally random. All right. So what we want to do is change our random number generator and here we're going to be slowing it down by adding a nested loop, so I've changed the number of total random test to 10,000 instead of 100,000, and so what we want to do is a is still a completely randomly regenerated 64-bit number and now what we're going to do is initialize b to be a and now for j in 0 to 63. For j in 0 to 63, so that is for some random number between 0 and 63, we're going to flip the sense using Python's XOR operator with one random bit at the input. And now our new test case is going to be a and a changed version of a with the idea that that's going to be a better test case. Let's see if that is indeed the case. So now we've completely covered our code. There are 27 statements. All of them ran with no partial executions. Due to their structure, we only executed further into the for loops and implemented these functions when bits were the same. If every bit was independently different, then the probability of executing further into these functions dropped off exponentially, and so at 64 bits, the probability dropped off too fast for us to ever execute the case where a and b were the same. Based on that knowledge, based on some domain specific knowledge, we made a new random tester whereby flipping a random number of bits did a much more even exploration of the iteration spaces of those loops including reaching the ending state, which is what we saw on that GCC documentation was the state that we actually wanted to test. This has been a pretty elaborate exercise for what in the end are two pretty simple functions and what this just shows is that you can do okay with a really simple random tester and not understanding what's going on, but if you wanted to do a better job we really need to think about what it is that we're testing, how the code is structured, and how we're going to execute all the way through it, and this is just sort of a fundamental limitation of random testing. Okay, now, I would like to go back to the drawing of a software under test that I feel like I've drawn about 30 times already and recall that this provides APIs and most of the time that's we're fuzzing. Now, on the other hand, the software under test uses APIs like the UNIX Recall, we could do fuzzing at this level as well and this most often called fault injection when we do that and run a fault injection, as we discussed, is extremely powerful. We've looked at examples of both of these and both of them are incredibly useful. What I like to talk about now is fuzzing of the implicit inputs, we talked about unit too, where we have non-API inputs to the software under test that affect it's behavior that turns out that this is often pretty important, and so, how we'd do this? Well, as we mentioned earlier, possibly the most important implicit input is the timing at which different threads are on different processors. This is to say the thread scheduler provides a very important form of implicit input to multithreaded software under test. An extremely important technique is perturbing the schedule. And as far as I know, there isn't a single best way to do this, and so we find in practices, people do a lot of different things. One thing you can do is just generate load on the machine. That is to say, make sure that your testing is not happening on the complete idle machine. It is often the case that if you test software on a completely idle single core machine, even if it has multiple threads, those threads will be scheduled in a quite deterministic way. Similarly, if you have a multicore machine that has more processors, then your application of threads, there is again a strong possibility it will be scheduled on an extremely deterministic fashion while you're doing testing. And if they are being scheduled extremely deterministically, that means that the scheduler is exploring only a very tiny fraction of the full set of possible thread schedules and what that mean is concurrency bugs are going to go unfound during testing. So, generating load by running apps--generating load by running other application is important. Generating network activity on a machine can also be valuable. And network activity is interesting because incoming network fax machine caused interrupt handlers to fire and they cause lots of kernel routines to run It caused cache lines to be stolen away from your application and this causes some perturbation of the schedule. There also exist specialized tool for stress testing of multithreaded applications. And if you have one that is available too, it would be an extremely good idea to use it to help task your multithreaded code. Okay, so I discovered is what I will call external pertubation to the threads Also possible, previous software to introduce perturbations internally and one way to do that is to do things like concerting delays before and after acquiring locks and also around accesses to shared variables. This entails a fairly high cost--this isn't so easy, but on the other hand, it might be useful. And I heard a story one time, there was about a group of researchers who are working on formal methods based tools for finding bugs and threaded codes and these researchers were having some sort of a competition where each of them try to find the most bugs in the software under test that they could, and what happened was, somebody showed up and just put a bunch of sleeps inside the application and showed that they could find just as many bugs as this super heavy weight, super complicated formal methods tool. So, the final thing you can try is what I'm going to call "unfriendly emulators." Put that in qoutes, I kind of just making up that name. But these are--it's kind of special virtual machine or other run time that's especially designed to stress test your application by doing things like invalidating cache lines, invoking thread schedules in odd ways, and other things. And so, there are some these kind of things generally available, and if one of them fits the needs of whatever your software under test is, you should definitely use it. The next issue we're going to explore is basically purely selling my own views and opinions, but the next issue we're going to explore is the question, Can random testing inspire confidence? And so what I said earlier in this course is the purpose of testing is to maximize the number of bugs found per amount of effort spent testing, and so that's really what testing is all about. On the other hand, when we start to do a better and better job testing especially using random testing, we start to get tempted to use those results as justification to believe that we're creating good software. What I want to talk about now is, is that ever justified, is that an inference that we can make? What I think I can give is sort of a highly qualified yes answer to this question. Let's go through the qualifications. If we have an API that we understand really well and we have a relatively small piece of code For example we're doing unit testing of a data structure like a balanced tree and we have a strong and well-chosen collection of assertion embedded in that code and we have a mature, well-tuned random tester, and we've measured coverage and shown that coverage is good then at least in some cases, I think we can conclude that the software is pretty good. What I mean basically by this is that I could or any of you could take the splay tree that we looked at for the queue or red ??? or something. Do all of these things and develop a reasonably high degree of confidence as described. If any of these conditions isn't met, I would strongly doubt whether random testing can inspire confidence. If you don't have a small code, let's say that instead of small code we have would be acrobat reader, there's no way, even if we have all these other things that we could possibly get any confidence in the quality of that software using random testing. For example, if we don't have a small code, if we have for example adobe acrobat reader then even all of these other things were true, there is no way that random testing would inspire the confidence in the quality of the product. The thing we should keep in mind is since these conditions for inspiring confidence are quite restrictive, we never just do random testing. We always use it to augment other testing methods that might be even better at showing up some of the weakness of random testing. Let's look at some tradeoffs in random testing, and what I really mean here is advantages and disadvantages of random testing that might lead you to spend more or less time doing it and so on this side I'm going to put advantages. Over here, I'm going to put some disadvantages. One of the main disadvantages of random testing is of course this something you spend a lot of time already dealing with is that the input validity problem can be really hard, and a lot of people would give up on random testing. In my opinion, give up on it too early before they've gotten good results and often it's this input validity problem that stops them. The fact is there might be some creativity required to get around this. For example, mutating existing inputs or getting a good enough understanding of the structure of the input that we can actually generate valid or random inputs from oracle. Oracles are hard too--so again, we need a substantial amount of creativity to deal with the lack of oracles. One of the people often like when doing testing is they like to have a fix test suite, and they like to run it and they like to know when it's done, but if they didn't get any failures, they can feel good. Random testing on the other hand has no stopping criterion, and so for example, if you run a random tester on eight course for a day without finding any problems that implies nothing about what you'll find in the future. You might find six or seven problems in the next hour of testing, and in the process of doing this compiler testing that I've been doing for several years now, I found this to really be the case. I can start a compiler testing run. For example the weight exertion of GCC and all of them. I won't find anything for a long time, and I think to myself-- gee, they've really sort of nailed the issues and then just as the probability is working out, how they do, a bunch of problems will come out after a day or two. So another problem and this one we haven't really talked about yet is that random testing may find unimportant bugs and what I mean by this is that since random testing explores parts of the input demand they may not be often explored by real users of the system. It could be that the people developing this software under test are not remotely interested in fixing the bugs that are triggered by the strange inputs and this can be frustrating, but it is really just very situational. Some kind of bugs such as buffer overflows in a web server or in a pdf reader or something like that almost mandatory to fix because they are so often exploitable. So those kind of bugs really can't be labeled unimportant by right thinking developers On the other hand, if we're talking about problems that really might not come up in practice so let's say that with my C compiler testing, I generate a C function that's 100,000 lines long and this fails to compile. Well, the compiler developers all and certainly going to ignore me because people just don't like code like that, and similarly, if I make an identifier let's just say a variable name that's 800 characters long and that crushes the compiler and they may not care about that either. and so the general flavor of the solution to this problem is first of all to generate random inputs that are more like inputs of interest. So for example, when we're generating random C-codes, we don't make really long functions, we don't make really long identifier names, we don't nest parenthesis really deeply, and it could easily be of these things would find bugs, but we just don't really have any confidence that if we find these bugs that anybody would care about fixing them so we'd rather just not even find them. The second part of the answer to this problem is to do a good job is that as the person running the random tester--you need to do a pretty good job filtering the bugs that you find so that you only pass the ones that look important. The ones that look like there might be critical or security problems or something onto the developers, and if you find a bunch of bugs that they never see, this isn't a problem. If you find a bunch of bugs that they're never going to see, this isn't necessarily a problem but it uses your time. If you find a bunch of bugs that you never pass on to developers, this isn't necessarily a problem. You've learned interesting things about the software under test and if you make the decision to not pass this on then that's probably okay. It's not uncommon for a simple random tester to spend all its time performing boring tests. For example, if we do a bad job generating html then maybe all our test result in rejection by the html cursor and we never find anything interesting. I have two responses to this kind of thing. One response is to simply put more resources into the random tester. For example, we can use more course. We can use machines with more memory or maybe we can just make random tester faster by reusing the size of the inputs through a generator. The other response as we've discussed several times is to make the random test case generator smarter. That is to say to use our human expertise or human knowledge of the domain of interest in order to generate manual test cases that aren't boring. Random tester may find the same bug many many many times and if we're going to automatically reject repeated instances of a bug, this isn't a problem. On the other hand, if we don't have an automated way to do triage of issues that we found, then this can really waste somebody's time and this issue, we'll talk about explicitly a little bit later. Another problem is that it could be very hard to do debugging from the test case. These are very large and or nonsensical, and so what I mean here is that random test case that triggers a failure in some software under test might be many bytes long, and it may have no usable human interpretation. In any of those cases debugging software under test can be really hard and there's the solution for this. There's is a pretty good solution for this problem but I'll talk about it a little bit later. A final disadvantage of random testing is that every fuzzer and the final disadvantage of random testing is that people experience is that every time you write a new fuzzer, it finds different bugs. And what this basically means is little incidental design decisions in the random test case generator explores suddenly different parts of the system under test input to many and that results in finding different bugs, and this shouldn't be a surprise given the extreme difficulty of the testing problem, but it's still a little demoralizing when you spent a lot of time writing a fuzzer. It finds a lot of bug and you think the system is getting pretty robust. and then somebody else writes a little simple fuzzer and then it finds a whole bunch of bugs that you can find and this is very common for that to actually happen. That's the longest of drawbacks. Any drawbacks are real. Let's go back and look at the other side of trails. In the advantage side random testing has less tester bias, means the testing is less influenced by what I know about the systems implementation and random testing also has weaker hypothesis about where bugs are. Maybe that's just another way to say the same thing. Besides this issue of bias and hypothesis, perhaps the real killer advantage of random testing is once you implement a random tester and once you automate the testing process the human task of random testing is basically zero, and this is surely been the case with the C compiler fuzzing effort that I have mentioned several times where we really spend a long time developing this random tester. We worked really hard on it, but since we more or less finished it up a couple of years ago its been finding on average a bug that too are weak in real compilers would almost no efforts from us. The machine sitting in dark room somewhere, it's doing random testing but it's running by itself. It needs almost no oversight, all we have to do is look at the results and write up some bug reports and this process of turning random test into a bug report is something that will come back to you a little bit later. An extremely interesting when your talking about random testers is they often surprise us. I have the personal experience of being surprise by random testers many times. And also, in the course of reporting 450 compiler bugs, I found the compiler developers are often really quite surprised about the behavior of their tool. Random testing sort of consistently has this ability to tell us something we didn't know and that's extremely valuable. And another advantage is that every fuzzer finds different bugs and so of course, I included that under disadvantage as well. This is sort of depressing at some variable but on the other hand, this is really cool. What it means is, we have several fuzz tools, might as well just bought a couple of course in each of them and more of them likely because now your going to be that over lap when the bugs are found by the different random tester. And finally due perhaps to some combination of this other advantages are at least find random tester to be really fun. It's fun to have self-automated. It's fun to be surprised about the system. It's fun to have shortcomings and a logic pointed out by a process that doesn't have a human involved. It's just really interesting to have this happen and it's just really interesting that this kind of staff happens. Now we finished up with random testing, we're going to talk about some more advanced testing issues, such as what to do when a false tester overloads you with bugs. And it sounds kind of silly but it can really happen in practice. We're also going to talk about how to take a large test case that makes software fail and turn it into a very small test case. Finally, we're going to talk about how to report bugs in such a way the developers are more likely to pay attention to them. One issue that can come up if we write a really good random test-case generator is that we can be overwhelmed by the success of our own bug-finding effort. On some level we should be so lucky to have this problem, but on the other hand if you take a large software project that's never been subjected to random testing, and hit it with a sophisticated random tester it's not at all unexpected that you'll be overwhelmed by the number of bugs. What'll happen is you'll start a random testing run. You will leave for the weekend and next time you come in there might be 250 bugs. The way we want to visualize the situation is like this. We have the input domain of course and all I'm going to show here are inputs that found bugs. There's a bunch of them. What we have is a fundamental quandary, which is the question do we have 250 test inputs that all, for one reason or another, happen to map to the same bugging output, or on the other hand, do we have a situation where we found, perhaps, 250 different bugs. Without looking in more detail at what's going on there's no way we can tell. Most likely--almost certainly--the truth is in the middle. Almost certainly we found 10 bugs or maybe 50 bugs, but we probably didn't get so lucky as to find 250 bugs. We probably weren't so unlucky as to trigger the same bug 250 times, although, of course, that kind of thing does happen. There are basically two ways out of this problem. The first solution--pick a bug and report it. It's totally irrelevant which bug it is. It could be a random bug. Most likely what we'd want to do is eyeball them a little bit somehow and see which one seems the most serious. It doesn't matter. Let's say we report this test case here, so that goes to our developers. What they're going to do is fix it, of course. This buggy part of the output space goes away in the next version of the system. Now we don't have range of the system. We have range prime, which is just the range of the next version. So, we still have exactly the same input space but the behavior of the software under test has changed a little bit. Now something interesting happens. Not only did that particular failure go away, but perhaps some of the other ones did. Perhaps all of these over here stopped being inputs that trigger failures anymore. Of course that's great, because it's nice to see that they fixed a bug that covers such a large part of the input space. On the other hand, another possibility is all of the remaining bug-triggering inputs that we found still trigger bugs, and so what do we do? Well, of course, we just go ahead and report another bug. This strategy will keep working. As soon as we get a new version of the system that fixes a bug we've reported, we can just do another one. This isn't a bad mode of operation. What this works for is basically for smallish systems where bugs can be fixed rather quickly, there's no problem at all reporting one bug at a time. On the other hand, if the people fixing bugs have a slow fix cycle-- let's say we're only testing actual released versions of Microsoft Word, for example-- we're only getting a new version every couple of years. If they fix one bug in Microsoft Word every couple of years, the question is how long do we think that'll take for them to arrive at a correct version of Microsoft Word? The answer, of course, is they never will, because they're putting bugs in faster than we would take them out, using a one-bug-at-a-time model. This can easily be the case for real software products. If that's the situation, if reporting one bug at a time doesn't work, we're forced to use a different strategy. I call that bug triage. A bug triage is the process by which the severity of different bugs is determined, and we start to disambiguate between different bugs in order to basically try to get a handle on which bugs we can report in parallel. The deal is that any inputs that trigger separate bugs can be reported in parallel, but if we report all of the bug-triggering inputs that we found, what we're going to do is be causing a lot of duplicate bug reports. One of the golden rules of reporting bugs--and I'll go over this a little bit later in more detail-- is that you don't report duplicate bugs. This wears on developers incredibly much, and they'll soon get tired of you and start ignoring you. I say this from experience. I've reported quite a few duplicate bugs myself. It's not that fun. People don't like it. The question now is what does bug triage mean. How do we start getting a handle on which bug-triggering inputs map to different bugs and which ones map to the same bugs. Basically, there is no silver bullet here. What we have is we're going to see there is a number of different weapons we can use to disambiguate bugs. In an easy-as-possible case the bugs in the system are causing assertion violation messages. One thing we can do is disambiguate based on assertion violations. What I basically mean here is that you do what I gave a demonstration of doing a little bit earlier, which is you just look at the text of the assertion violation messages, and you go ahead and make the assumption that distinct assertion violation messages are caused by distinct bugs in the software under test, and that doesn't need to be true. Not too long ago I reported something like six bugs in the LLVM compiler that had completely distinct symptoms-- that is to say, completely distinct assertion violation messages-- but they were all traced back to a single error in the source code. Of course that kind of thing can happen, and that's especially common if the underlying bug in the software under test is some sort of a memory safety violation. That is to say, we have a C or C++ program that starts corrupting it's own internal memory. In that case, a single bug can map to any number of symptoms. One possibility is that we have a single root cause that is a defect in the software under test. That can map multiple outputs that looks different. This is one possibility, and this happens, although maybe not that often. Another possibility is that we could have multiple defects that all cause the same symptom. This is actually really common. For example, in a big C or C++ program like a web browser it's very common for multiple memory safety defects-- that is to say, multiple bugs that include things like buffer overruns and null pointer references-- to all have the same symptom, which is segmentation fault. That happens really commonly. In contrast with these slightly pathological cases, what we hope happens--and this is also relatively common-- is that a single defect maps to a single symptom. By reporting a test case leading to that symptom, we can get that defect fixed. So, in this case we have many defects, all mapping to their own symptoms in the output space, we can report these in parallel, and that's really nice. Like I was saying, with large, complicated software that's evolving rapidly, reporting bugs in parallel may be our only option, if we want to actually get the number of defects in the software under control. Unfortunately, not all bugs resolve to nice assertion violation messages, and bug disambiguation can be trickier when all we have is something like a core dump of a stack trace. What these things are going to give is--at least if we have good debugging symbols-- probably some sort of an indication of what part of the code failed, and then it's going to give an indication of the stack frames leading up to the failure. Sometimes these are going to be useful for doing disambiguation and sometimes not. They provide qualitatively at some level similar information to assertion violation messages, and if they're giving us some clues relating the symptom to the source code but is not going to be as easy to find and it's not going to be as readable. A third weapon we have in the arsenal when doing bug triage is actually pretty interesting, which is that we can do a search over the revision history of the software under test-- that is to say, of course, if we have access to the revision control symptom we can do this. Let's go back to our example here where we have all these bugs. If it's the case that all of these bugs here appear just one revision back, and these other ones appeared in different revisions maybe weeks or months ago then it's a good bet that all of these new inputs are triggering the same error-- that is to say, an error that somebody just committed the other day-- and these probably trigger older bugs. This is a pretty powerful weapon and some automated tools for revision control even build in functionality to do this search over version history. For example, git has a git_bisect command that can be used to do a binary search over revision history in an automated way. Even if that kind of thing isn't available it's not too hard to script this up yourself given an automated way to detect if a particular version has a particular bug. Our last way we can triage bugs is to examine a test case. Often it's the case that test cases that trigger the same bugs have similar features. In the compiler bug finding example that I've been using it's often the case that I'll look at a bunch of failure-triggering programs-- that is to say, inputs to the compiler that cause it to crash or otherwise fail-- and contains some sort of a smoking gun like maybe 64-bit integer divides or something like that. There would be some sort of common feature that tips us off that some collection of test inputs all maps to the same bug. Then if we see another group of test inputs that contain some totally different feature- maybe really deeply nested parentheses or really long identifiers-- then it's a good bet that those map to a different bug. The problem is looking over large, randomly-generated test cases is really painful. This is going to directly lead to our next topic, which is a really important one, which is test-case reduction or test-case minimization. That's an automated process for taking a large failure-inducing input and turning it into a small one, and we're going to cover that next. Test-case reduction is the process of taking some large input that triggers a failure and turning it into a small input. It's usually the case that many bugs can be triggered by small inputs. On the other hand, it's often the case that, for example, we discover a Firefox crash, and the web page that causes the Firefox crash might be giant. It's probably huge. We discover this bug in the wild, and statistically speaking, the pattern in this input that triggers the Firefox crash just probably isn't going to be found on some small web page. What we can do is, by hand, figure out what part of the input causes the test case. One thing we can do is eliminate part of the input. Sometimes we do this in a smart way. For example, we might just know that some part of it is unlikely to trigger a crash, or we might just chop some of it out blindly and see if the smaller input triggers the test case. If it doesn't, then we got back to our original test case and try again. If it does, we proceed. Now maybe it's this part. If we're really lucky, at the very end of this process, we'll end up with a really small test case. That's the thing we'd like to report to the people developing the software under test. Of course, even if we're not reporting bugs to someone else-- that is to say, we're finding bugs in software that we wrote-- it's still really nice to have a minimized test case, because these make it much easier to track down failures. Option one--manual reduction. This has been done by people debugging for probably about as long as computer science has been around. The second option is really cool. It's a really, really nice technique. It's called "delta debugging," and it automates this process. If you can write a program that can tell automatically if a particular input triggers a failure-- that is to say, you load up the webpage in Firefox and you'll see if it crashes by looking at the exit code that it provides to the operating system. Then delta debugging is a framework that takes your script and takes the test input and automates this process in a loop, and this loop terminates when the delta debugger, which has a bunch of heuristics built in for eliminating parts of the input, it terminates when this delta debugger can't reduce the input anymore. What I don't want to do is going into this technique in a ton of detail, because the guy who developed the delta debugging is going to be teaching a Udacity class sometime soon, and it's probably going to be really interesting. What I hope is that I've intrigued you enough that you'll really serious consider taking his class. It's going to be about debugging. What I'd rather do now is show you an example. Okay, here we are back on one of my work machines, and we're going to look at a test case which makes the latest version of GCC crash. That's not to say the latest-released version, but the version from their subversion repository that I just built recently. We can see that this is kind of big, and if we look at it's size, it's about 40 kb, so actually that's not that big as random test cases go. Sometimes the bugs that trigger compiler crashes are much huger like hundreds and hundreds of kilobytes. Anyway, this one is pretty small. That's nice. What we're going to do now is run a delta debugger on it. First, let's make sure we know that it crashes, so current--gcc is what I call the GCCs that I build from the repository. All right, it seems to have croaked in getinitialdefforinduction at tree-vect-loop 3222. An assertion violated inside the compiler, and we're happy that it did, because otherwise this might have turned into a bug that causes GCC to generate wrong code, and we never want that to happen. The next thing we need is an automatic script that detects if a particular program has this bug. I've already made the script. I've called it test1.sh. What it does is compiles a program at the particular optimization level 033. and then searches the output for this particular string getinitialdefforinduction. The reason for that is that we want to be sure not only that we can crash GCC, but we crash GCC with the particular bug we're interested in right now. This test script will return 0 to the operating system if it succeeds, and it will return something else if it fails. Now what we're going to do is invoke the godelta debugger. This is a little script here. It makes that work, and it's going to go ahead and try to minimize this test program. Let's see how it does. The delta debugger just finished, and no doubt Udacity's excellent editors have cut out the several minutes I sat there waiting for that to finish, but it took about maybe 4 minutes. The point is I didn't have to sit there waiting. Let's see how big the program is. Okay, it's not that small, but it's about 7 kb. The delta debugger got rid of a bunch of the junk. It's still going to be a little bit bad. It's not too bad. We'll be able to cut down that 7 kb by hand if we want to fairly rapidly-- much faster than we would have been able to deal with the original nearly 40 kb. One thing you might be saying to yourself is that's all well and good. John can do test-case reduction for compiler inputs using delta debugging, but the really nice thing about delta debugging is it works for any text-based input format. That's really nice. If we're trying to reduce an HTML document that causes the web browser to crash, we can use delta debugger. We can use delta debugger if we're trying reduce the JavaScript program that causes the JavaScript interpreter to crash. We can use delta debugging, basically--anything that's text we can use it for. If we think back to the bounded queue data structure that we developed a random tester for, those particular random tests didn't have any external manifestation. Those only existed as data structures internal to Python. We can't do delta debugging on those, because there's no text file, but what we could have done is before running the random test through the queue, we could have saved those to a text file-- that is to say just translating the operations that we were doing into strings-- and then once that is saved on disk we can run the delta debugger on it and come up with a minimal set of queue API operations that cause the queue to do something wrong. That would probably be a really good idea. So, delta debugging is an extremely powerful technique and test-case reduction is something that we really need to have in practice, especially since this ties in with the triage idea that I talked about a little bit earlier. Remember that one of our triage methods is looking at the test case. Well, looking at big test cases is really hard. There's a bunch of junk that's sitting in the way, obscuring the actual features that are causing the system to crash or otherwise misbehave. If we can do test-case reduction before doing the bug triage then the whole process becomes really a lot easier. But now I would like to talk a little bit about the art of reporting bugs. So let's assume that you're successful in creating some sort of really nice random tester that starts crashing utilities. Let's say for example that you want to do your own fuzzing study. So if you recall a fuzzing paper was written in 1990, one was written in 1995, then another in 2000, another in 2006. So now it's 2012 and is basically time for a new one. You do it and so what you do is go take whatever operating system you choose, and you start crashing software and hopefully instead of just being happy about what a successful effort you've had, you'll report the bugs to the maintainers so they can start improving the robustness of the software. Let's talk about how to do that. The first rule and it's really simple is don't report a duplicate bug. To avoid reporting duplicate bugs is to go to a bug reporting systems that most open source projects maintain and search on some symptom of the bug that you've found and see if it's been reported. Let's say we've found a way to get a segmentation fault out of all the m and we'll see if anybody else has that way. Well okay, so we only have one reported here and so it's really nice. What we would do then if we want to report a new segmentation fault to an LVM is we'll go ahead and look at this and see if the new one that we're going to report seems to be sufficiently different and if it is, we'll probably go ahead and report it. The next golden rule for reporting bugs is to respect whatever the local conventions are of the community that you're reporting to and so the best thing to do here is just look for some bugs that have been reported and see what kind of discussions they generate and try to take sort of a similar tone to what people have already done. The third thing is to report a small test case. I'm going to say here a small stand alone test case. The test case manipulation stuff that we have talked about a little bit earlier is really important for purposes of bug reporting and that kind of a process should always be a part of your bug reporting and the second cause here a stand alone test case is also really important. This means for example, don't report a bug that depends on some other file that only exists in your machine because that's not very useful to the people who are going to try to reproduce the bug because of course they don't have the other file. Do what you can to make a bug report stand alone. The next rule is when we report valid test cases and so this referring back exactly to the input validity problem that we've been talking about that I'd talked about a number of times in the material in random testing and if the software to which your reporting bugs isn't supposed to programmatically reject invalid inputs then you shouldn't report any valid inputs and so an example of this system is for example a C compiler isn't suppose to reject invalid inputs and another example might be something like internal communication between the JavaScript engine and a web browser there may not be total input checking on some sort of an internal interface like that, and so basically, we just want to be sensitive to the fact that some systems can't do or aren't equipped to do full validity check of their input and for those systems we often need to make sure the input is valid ourselves by hand. Now of course other systems and we've talked about things like Acrobat reader or whatever. These are suppose to reject invalid input and they are not suppose to crash in some sort a buffer overflow even on invalid inputs. For those kind of programs, this issue doesn't really exist. The next thing you need to do is tell the people who are the recipients for your bug report what the expected output of the system was that is what you hope would happen if the system wasn't buggy and what the actual output was. That is to say what really happen on your system and so the temptation is to not supply these things because you think it was insulting or you think that's really obvious, but in general it's a good idea to err on this side of caution and include these things. So you might include a bug report to the Python maintainers saying that when I added 1 + 1, I was really expected to but then this implementation, this particular version of the implementation printed 3. The next thing you need is to make the failure reproducible because nothing will get a bug report ignored faster than the incompletely irreducible and so you need to include things like platform details. For example this probably crucial information that your running Firefox on windows as oppose to Linux if you are trying to report some sort of Firefox crash. The exact version of the software under test is important and of course, if this is some sort of an open source project where you've hacked yourself before compiling you will definitely need to include details about that in the bug report. But if this little bug is your fault, then you don't want to waste people's time trying to track in down. So in summary, if we're able to come up with good solid answers to all of these requirements, that's a great idea to report a bug and so let's go ahead and do that right now. What I want to do is go back to my terminal. Instead of looking at a GCC bug, we're going to look at an LVM bug. I believe this is one we already looked at a little bit. We say clang -02 small.c. It's going to die with an assertion violation about isa(Val) and Unknown live-in to the entry block, and that's assertion failed. What we need to do first is check of the lvm people already know about this one. We're going to go here to their bugzilla and search for this exact string. All right. They don't know about this bug, so that's nice. If we know about a bug and they don't know about it, we can report it. Now we have to go back and make a reduced test case. What I'm going to do here is instead of invoking the delta debugger, which is this nice extremely general purpose powerful tool, I'm going to invoke a different tool that's called C reduce, and it's one produced by my group. What it is is an extremely special-purposed delta debugger, so it operates on exactly the same delta debugging ideas that the other tool operates by, and it just has extra knowledge embedded in it about how to reduce C programs. This is going to take a little while, so you don't have to wait for it like I do. Let's run a time command and just see how long it takes. That took about 11 minutes. Not incredibly quick, but not too shabby either. Remember, this wasn't time that I had to be attending to the computer. The computer was just doing an automated search algorithm. Here is the output of the reduced test case. It's pretty small, so let's check its byte count. It has 274 bytes. That's nice. What I'm going to do here is make a bug report. What I'm going to first do is make sure we know the version of clang. It's version 156970. I'm going to make sure the test case gets included. I'm going to show clang crashing on it. That's a bug report. If I haven't appeared to go through all of the steps that I've narrated to you earlier, it's because I've reported a lot of compiler bugs, and I know what I can get away with. There we go. This includes, I believe, enough information for the LVM people to reproduce the bug. It should be good. I've marked that we're reporting its chunk. We just need a name for this bug report. The bugzilla is trying to help us avoid a duplicate, and I don't think it's told us about anything that we didn't know. In fact, all these are marked as fixed anyway, so we're good. Now let us ship this off. Now the LVM developers, when they get up in the morning, have a new bug report to look at. That concludes our data on bug reporting. Alright. So I'd like to talk just really briefly about building a test suite for a piece of software. So test suite is just a collection of tests. And it's often the case that the test suite can be run automatically. It's also often the case that test suite gets run periodically. So for example, perhaps nightly, on every commit. Although in many cases that's infeasible, since, if commits are frequent and the test case is slow. The goal of a test suite is to show that some software under test has some desired properties, namely passing all of the tests. Although it's very common for real software to almost always be in a state of partial failure. But what we hope is that most of the time, most of these failures are not particularly severe ones. So the question is, if you're maintaining a software project, what goes into the test suite? And to a large extent, this is a matter of taste and preference, but on the other hand there's some pretty common features of nearly all test suites. It's very common, first of all, to have a lot of unit tests or at least feature-specific tests that are small tests that exercise very specialized behaviors. So, for example, if we're developing some sort of a web browser, we might have tested effect that different HTML elements render correctly and that sort of thing. It's also very common for a test suite to contain large realistic inputs. So, for example, if we're testing some sort of a microprocessor, maybe we'd boot up Linux and run it for a couple of hours, and the purpose of these kind of inputs is to provide realistic stresses on the system. And to exercise a lot of features and combination. And make sure that things are up to scuff. It's nearly, always a good idea to include regression tests in a test suite. Now, regression tests is basically any input that's cause any version in the software in the test, to fail at any time. And, there are several reasons regression tests exist. The main one of which, is we want to make sure that the software in your test doesn't regress. That is to say that it doesn't go back into a state in which it fails on a bug that we already fixed. There are a number of reasons why that could happen. First of all, regression tests are useful because whatever the defect was in the software that caused the bug in the first place, we might not have gotten rid of all the instances of that defect in a source code. So, for example, a buggy piece of code might have been cut and pasted to several other places, and those other locations might not be causing our system to fail currently, but some other change might enable the buggy code to fire, and make the bug happen again. Another reason is it is pretty easy through, for example, mess ups with the revision control system. To accidently go back to an old version of a file, before we fix the bug; if that happens we want to catch it as soon as possible, because, because some regression test trips. Third reasons is that defect in software come from errors in people's thinking, it's pretty often the case, that the person who introduce the defect into the software; didn't actually correct the error that they had in their thinking. Rather maybe somebody else fix the defect. And the person retains their mistaken assumption about some sort of an API or something. And due to this latent error in somebody's head, they can go ahead and start adding similar defects to the system later on. And if we have good regression tests, we stand more of a chance of catching those kind of things. And something that usually doesn't go into a test suite is a random tester. And for whatever reason I'm not totally sure that I understand all the reasons even. Random testing is often treated as a separate activity. This is related to the fact that random tests often are non deterministic unless we're being careful to preserve the same seed. They don't have a clear correctness criterion. And perhaps more importantly, random tests always have a possibility of showing us something new. That is to say, they have the possibility of introducing a test case that we haven't seen before. In fact, that's what we hope will happen. And the reason that might be undesirable, is the test suite is supposed to be predictable. It's supposed to consist of things that we know to test for. Now, if all of a sudden, the test starts containing new and different tests, then that's not necessarily good. So, for whatever combination of these reasons, random testing is often a separate activity. So, sometimes we find ourselves faced with, really hard software testing problems. Let me go over some of the characteristics of this problem. Lack of a specification is common, or perhaps only, lack of a good specification. If there are no comparable implementations of the kind of system we're implementing, that is to say of the first system of its sort, that makes testing quite hard. Because what it probably means is we are debugging the specification or even developing the specification as we go. Big systems are hard to test, large highly structured input spaces, like testing quite hard and so if you want to imagine sort of a hard testing problem with a large highly structured input space. Consider for example, the flight control computer is on a space craft or an airplane, these things take sort of an enormous variety of input from all sort of different redundant sensors. The time at which these inputs arrive is significant. The spacecraft or the airplane has all sorts of physical properties likes its altitude, its attitude, the positions of various control surfaces that all affected dynamics. These kind of systems are really, really, sort of truly hard to test. Non-determinism can make a system very hard to test. And, the issue here is that, we'll play a test case against the system once, and it succeeds. But then, at some later time, some variable that's not under our control will cause the system to fail on that same input. Lots of hidden statement make systems hard to test. Some systems that I've heard of either extremely hard to test or for example, Java Virtual Machine. That are run by, for example, financial organizations on lots of cores with huge amounts of memory. These things have so, so much internal state, that when something goes wrong, it's almost impossible to make any inferences about what was going on inside of it. And, you end up needing to try to reproduce the problem. But, of course, this is also extremely hard because the problem probably happened Three hours into some sort of a massive processing task. Finally, if we lack strong oracles testing can be really hard. And so, for example, something like a large molecular simulation might be very hard to test. So ,we have some sort of a new simulation code, we have no idea what the right answer is supposed to be probably. It's running on some sort of a large parallel machine, so we have a very hard time reproducing problems. Problems might take a long time to occur, something like an autopilot might be extremely hard to test, because the response of the thing is going to be inherently in terms of the stability. And good behavior of the airplane. And of course this is an incredibly large complex physical object that's very hard to model and simulate in a reliable fashion. If you think about picking a strong test for an autopilot, it's almost inconceivable. Okay, like I said, some sort of a giant JVM running with lots of cores for a long time using a huge amount of heat. Very, very hard to test the behavior of something in that kind of a state. So, the question we want to ask ourselves is, how do we get a handle on these situations? How should we test these things? And often, there aren't really any easy answers. But what we can do is, we can, leverage weak oracles to the maximum extent possible if any of these things, the simulation, the auto-pilot or the JVM actually crashes in a bad fashion. Then we definitely know something's gone wrong. We can try to bootstrap some degree of confidence in the software under test by taking small test inputs. For which we've hand-checked the output and trying to argue that, for example, somehow, the auto-pilot if it responds well for these inputs also responds well for other test inputs. And in the end, if we're thwarted in our attempts to do really good testing, we probably have to rely on non-testing methods. And so, of course we, we should be doing code inspections and using formal methods on our systems in any case, if we care about the reliability. But what's happening here, if we really can't test the system effectively, we might have to rely on these things more than we would've liked. Okay, so that's just a quick survey of, things that can make testing really hard in practice. All right, we've come nearly to the end of our course. What I'd like to do now is summarize what I think are the high points, that is the most important testing principles that I've tried to convey in this course, and put them all in one place. So, let's go through these. First of all, testers must want software to fail. Second, testers are like detectives who are hunting down bugs. And as detectives, testers will have to be observant to all sorts of suspicious behaviors and anomalies in the software of your test. My guess is that a number of really serious bugs that have occurred over the years. I think these had already been noticed by people and swept under the rug because people were busy or they just wanted to ship the product or maybe they were users who didn't know what the bugs meant. So regular users have the luxury of ignoring bugs, but testers don't, and so it's really important not to sweep things under the rug. All available test oracles should be used as a basis for testing. And so, you might be tempted to think from the language that I was using, that is, strong oracles versus weak ones, that if we had a couple of good strong oracles available, maybe we wouldn't need the weak ones, maybe we wouldn't just use them at all. And as I hope I've convinced you by now, that's not the case at all. All of the oracles should be used because they can all detect different kinds of faults, and even if they detect the same faults, weak oracles might be much cheaper to use. Test cases should contain values selected from the entire input domain, and if there's doubt about what exactly that domain is, this is something that will be good to hash out with the developers. Interfaces that cross a trust boundary need to be tested with all representable values, not just those from the ostensible input domain. If we recall some of the examples we've looked at, if we are writing a web server we might hope everybody submits data that's well-formatted, but as its most likely the case, they won't and the reason they won't is because they will be trying to break into our Web site. Server. So we need to test on that kind of data, to ensure that we can correctly reject it. Similarly, a piece of software like the Linux kernel has the trust boundary at the system call interface, that is to say the interface between user mode applications and the Linux kernel. Linux kernel like the web server, can't trust that its clients are going to make well formed requests all the time. It should expect that its clients if not actually hostile, are at least buggy, and are going to do all kinds of, it needs to catch that instead of crashing or violating its security policy. A little brute force goes a long ways when we're testing. And what I mean in particular is in certain restricted circumstances, we can do exhaustive testing and almost anything else can be randomly tested. Quality cannot be tested into bad software. We saw the Therac 25 example, where the control software for the radiation therapy machine was probably so broken, that almost an, almost no amount of testing would have been sufficient to make it right. It needed to be thrown away I think I needed to start over. I'm sure we've all seen software that looked like that. So in contrast with examples like a fair at 25, testable software has a few of the following qualities. No hidden coupling between modules and side channels where modules can share information without being visible to the system developers. A few variables but are shared between threads. Few global variables shared between modules. And no pointer soup, and that is to say, no huge data structures with pointers going everywhere where it can't possibly keep track of who's changing what and what's valid and what's not. Codes should be self checking whenever possible using plenty of assertions. However, these assertions are never used for error checking, rather they're used to check for logically impossible conditions. It implies some sort of an internal consistency violation. Assertions must never be side effecting, because if they are, and you turn them off, the system behavior will change, and this leads to madness among developers. Finally, these assertions should never be silly or trivial ones, because first of all, those serve no purpose, second, they clutter up code, third, they make things slower, and finally, they fail to communicate useful information to the next person who looks at the code When appropriate, all 3 sources of input to a piece of software under test should be used a a basis for testing. And those include the obvious API's, provided by the software on your test which can be tested directly. API's used by the software under test can be tested using fault injections techniques. So recall that these are things like substituting the library that provides these API's with a different library the inject faults, well perhaps just hacking the layer underneath to inject faults. Finally, non-functional inputs such as thread schedules need to be tested using whatever method you can get that will work to actually test these things. And finally, the last principle for testing is that failed code coverage do not provide a mandate to cover the failed items, no matter how tempting that might be, but rather, they give clues to ways in which the test we did is inadequate. So, blindly coding to the coverage metric is going to destroy those clues, but it's going to do it in such a way that doesn't improve the quality of the test suite very much. So taken together, this list of items I've just given you constitutes pretty much all that I know about testing. And the detailed version of these has been the content of this course. This is material that I have never taught before, so I hope it came out in sort of a fairly coherent fashion. It's stuff that's been brewing in my mind for a long time. I've wanted to teach it because I've noticed for years that we don't seem to be doing a very good job teaching CS students to test. What happens instead is they write small test cases in response to assignments, they debug them until they pass the test cases we give them, hand them in and never look at them again. And it's hard to think of anything less like the real world of software development than the environment we create in classes. So what I have tried to do is structure this course a bit little differently, tried to bring out thing that I feel are really important that we often don't do a very good job with. And this has been really enjoyable for me, it's been great to actually try and set this material down in a coherent fashion and I very much hope that this material has been useful for you and that the class has been enjoyable. Thank you.