Welcome to Statistics 101, taught by me, Sebastian Thrun and by Adam Sherwin, who is our assistant instructor doing a lot of work in the background. I figure I'll start with giving you a teaser, a challenging teaser. I'm going to provoke you. So, this is you. I believe you should be unhappy, not because our class is so bad, but you and I will prove in a second that you are unpopular. The reason why I show this is to show you how deep statistics is and how we can easily fool ourselves. So let's dive in. Let's say there are two types of people for simplicity—type A and type B. Type A are the popular ones. They have 80 friends. And type B are less popular. They only have 20 friends. You might now say that I don't know which type you are. I will compute what's called the expected or average number of friends. To do so, I assume that half of the people are of type A and half of the people are of type B. Here's your very first quiz in this class. Into this box enter what you think is the expected or average number of friends if you have a 50% chance of being a type A and a 50% chance of being a type B. Type in here when you're done. There is a submit button somewhere down here. You can see if your answer is correct. Regardless of whether you finish this or not, at some point just hit the next button, and you'll see my answer. My answer is 50 friends. The way I get there is I don't know what type you are with a 50% chance or 1/2 you're type A, in which case you have 80 friends, and a 50% chance you're type B and have 20 friends. That gives me this equation over here as the expected number of friends. Working this out means you have 40 + 10 = 50 friends. So far, so good, but why are you unpopular? Here is your Facebook or G+ page, and of course you're smiling. On it is your list of friends, and we already know it's either 80 or 20 friends. In expectation it's 50 friends. Let's pick a random one of your friends, like this one. This person will also have a Facebook or a G+ page. Before I raise the question how many friends this person has, let's consider that this might be either a type A or a type B person. Keep in mind that type A have 80 friends and type B have 20 friends. The question I have for you is what are the chances you picked a type A friend? This should be a number between 0 and 1. I'm also going to ask you for the opposite. What are the chances you picked a type B? Please enter both numbers, and these numbers should sum up to 1. Submit and then next. I should warn you this is a challenging question. If you don't get this right, don't worry. This is the type of stuff you'll know when you've taken the class. I just want to tease you a little bit in the beginning. Here's the interesting finding. Because type As are so much more popular, your chances of linking to a type A is 0.8. To see, let's take the extreme view. Suppose type Bs had 0 friends. Then you'd never link to a type B. You link to type A and to type B in a proportion of 80 to 20. Type B would be 0.2. That means most of your friends you link to are type A. They're the type of people that happen to be popular. Let's now go back and ask the real question. In expectation, how many friends does this friend of yours have? Please put your number right here, and again, it's a challenging question. Don't get disturbed if you don't know the answer. This type of stuff we'll study in and out. Here is my surprising answer. It is 68 friends. Who would have thought? The way to get there is with 0.8 chance you'll pick a type A who has 80 friends and with 0.2 chance you'll pick a type B who has 20 friends. If you work this out this is 64 + 4, makes 68. Your friend in expectation wold have 68 friends where you would only have 50 in expectation. So, sorry, I think you are unpopular in expectation. Let's talk about this class. Most of the material is very basic. It's the first class you would have in college if you're not a statistics major. We teach you things like how to visualize data, how to summarize it, how to run test, and even find trends. But there are also a few nuggets in there that are challenges. These are optional, and they're clearly marked as optional, but I will let you prove some theorems along the way using little games that I play with you. Most important, you'll be afforded the possibility to program the things you've learned. Again, this is optional, because I don't expect you to have a programming background. But give it a try. I believe that through programming you learn the material much better than any other way. It's optional. You can assimilate all the material without it. But I would recommend to give it a try if you know how to program. So this unit is a tough one. We're going to talk about perhaps the holy grail of probabilistic inference. It's called Bayes Rule. Bayes Rule is based on Reverend Thomas Bayes, who used this principle to infer the existence of God, but in doing so, he created a new family of methods that has vastly influenced artificial intelligence and statistics. So let's dive in! Let's use the cancer example from my last unit. There's a specific cancer that occurs in 1% of the population, and a test for this cancer and with 90% chance it is positive if they have this cancer, C. That's usually called the sensitivity. But the test sometimes is positive, even if you don't have C. Let's say with another 90% chance it's negative if we don't have C. That's usually called the specificity. So here's my question. Without further symptoms, you take the test, and the test comes back positive. What do you think is the probability of having that specific type of cancer? To answer this, let's draw a diagram. Suppose these are all of the people, and some of them, exactly 1%, have cancer. 99% is cancer free. We know there's a test that if you have cancer, correctly diagnose it with 90% chance. So if we draw the area where the test is positive, cancer and test positive, then this area over here is 90% of the cancer circle. However, this isn't the full truth. The test sent out as positive even if the person doesn't have cancer. In fact, in our case, it happened to be in 10% of all cases. So we have to add more area, because as big as 10% of this large area is as big as 10% of this large area where the test might go positive, but the person doesn't have cancer. So this blue area is 10% of all the area over here minus the little small cancer circle. And clearly, all the area outside these circles corresponds a situation of no cancer, and the test is negative. So let me ask you again. Suppose we have a positive test, what do you think? Would a prior probability of cancer of 1%, a sensitivity and specificity of 90%, Do you think your new chances are now 90% or 8% or still just 1%? And I would argue it's about 8%. In fact, as we see, it will come out at 8 1/3% mathematically. And the way to see this in this diagram is this is the region that should test as positive. By having a positive test, you know you're in this region, and nothing else matters. You know you're in this circle. But within this circle, the ratio of the cancerous region relative to the entire region is still pretty small. It increase, obviously, having a positive test changes your cancer probability, but it only increases by a factor of about 8, as we will see in a second. So this is the essence of Bayes Rule, which I'll give to you to you in a second. There's some sort of a prior, of which we mean the probability before you run a test, and then you get some evidence from the test itself. and that all leads you to what's called a posterior probability. Now this is not really a plus operation. In fact, in reality, it's more like a multiplication, but semantically, what Bayes Rule does is it incorporates some evidence from the test into your prior probability to arrive at a posterior probability. So let's make this specific. In our cancer example, we know that the prior probability of cancer is 0.01, which is the same as 1%. The posterior of the probability of cancer given that our test is positive, abbreviate here as positive, is the product of the prior times our test sensitivity, which is what is the chance of a positive result given that I have cancer? And you might remember, this was 0.9, or 90%. Now just to warn you, this isn't quite correct. To make this correct, we also have to compute the posterior for the non cancer option, which there is no cancer given a positive test. And using the prior, we know that P of not C is 0.99. It's minus P of C Times the probability of getting a positive test result given not C. Realize these 2 equations are the same, but I exchanged C for not C. And this one over here takes a moment to computer. We know that our test gives us a negative result if it's cancer free, 0.9 chance As a result, it gives us a positive result in the cancer free case, with 10% chance. Now what's interesting is this is about the correct equation except the probabilities don't add up to 1. To see I'm going to ask you to compute those, so please give me the exact numbers for the first expression and the second expression written over here using our example up there. Obviously, P(C) is 0.01 0.9 is 0.009, whereas 0.99 0.1, this guy over here, is 0.099. What we've computed is here is the absolute area in here which is 0.009 in the absolute area in here which is 0.099. The normalization proceeds in two steps. We just normalized these guys to keep ratio the same but make sure they add up to 1. So let's first compute the sum of these two guys. Please let me know what it is. And, yes, the answer is 0.108. Technically, what this really means is the probability of a positive test result-- that's the area in the circle that I just marked. By virtue of what we learned last, it's just the sum of two things over here, which gives us 0.108. And now finally, we come up with the actual posterior, whereas this one over here is often called the joint probability of two events. And the posterior is obtained by dividing this guy over here with this normalizer. So let's do this over here--let's divide this guy over here by this normalizer to get my percent distribution of having cancer given that I received the positive test result. So divide this number by this. The answer is 0.0833. Let's do the same for the non-cancer version, pick the number over here to divide and divide it by this same normalizer. The answer is 0.9167 approximately. Why don't you for a second add these two numbers and give me the result? And the answer is 1 as you will expect. Now this was really challenging. You can see a lot of math in this slide. Let me just go over this again and make it much, much easier for you. Well, we really said that we had a situation where the prior P(C), a test with a certain sensitivity (Pos/C), and a certain specificity (Neg/₇C). When you receive, say, a positive test result, what you do is, you take your prior P(C) you multiply in the probability of this test result, given C, and you multiply in the probability of the test result given (Neg/₇C). So, this is your branch for the consideration that you have cancer. This is your branch for the consideration of no cancer. When you're done with this, you arrive at a number that now combines the cancer hypothesis with the test result. Look for the cancer hypothesis and the no cancer hypothesis. Now, what you do, you add those up and then normally don't add up to one. You get a certain quantity which happens to be the total probability that the test is what it was in this case positive. And all you do next is divide or normalize this thing over here by the sum over here and the same on the right side. The divider is the same for both cases because this is your cancer branch, your non-cancer branch, but this score does not depend on the cancer variable anymore. What you now get out is the desired posterior probability, and those add up to 1 if you did everything correct, as shown over here. This is the algorithm for Bayes Rule. Now, the same algorithm works if your test says negative. We'll practice this in just 1 second. Suppose your test result says negative. You could still ask the same question: Now, what's my probability having cancer or not? But now all the positives in here become negatives. The sum is the total probability of negative test results, and we may now divide by this score, you now get the posterior probability for cancer and non-cancer assuming you had a negative test result, which of course to be much, much more favorable for you because none of us wants to have cancer. So, look at this for a while and let's now do the calculation for the negative case using the same numbers I gave you before, and with the step by step this time around so it can really guide you through the process. We begin with our prior probability, our sensitivity and our specifitivity, and I want you to begin by filling in all the missing values. So, there's the probability of no cancer, probability of negative, which is negation of positive, given C, and probability of negative-positive given not C. And obviously this is still 0.99 as before 0.1 and 0.1. I hope you got this correct. Now assume the test comes back negative, the same logic applies as before. So please give me the combined probability of cancer given the negative test result and the combined probability of being cancer-free given the negative test result. The number here is 0.001 and it's the product of my prior for cancer which is 0.01, and the probability of getting a negative result in the case of cancer which is right over here, 0.1. If I multiply these two things together, I get 0.001. The probability here is 0.891. And when I'm multiplying is the prior probability of not having cancer which is 0.99 with the probability of seeing a negative result in the case of not having cancer, and that is the one right over here, 0.9. So, we'll multiply 0.99 with 0.9, I actually get 0.891. Let's compute the normalizer. You now remember what this was. And the answer is 0.892. You just add up these two values over here. And now finally tell me what is posterior probability of cancer given that we know we had a negative test result and the probability of negative cancer given there is a negative test result. Please give me the numbers here. This is approximately 0.0011, which we get by dividing 0.001 by the normalizer 0.892, and the posterior probability of being cancer-free after the test is approximately 0.9989, and that's obtained by dividing this probability over here by the normalizer and not surprisingly, these two values indeed add up to 1. Now, what's remarkable about this outcome is really what it means. Before the test, we had a 1% chance of having cancer, now, we have about a 0.9% chance of having cancer. So, a cancer probability went down by about a factor of 9. So, the test really helped us gaining confidence that we are cancer-free. Conversely, before we had a 99% chance of being cancer free, now it's 99.89%. So, all the numbers are working exactly how we expect them to work. Let me now make your life harder. Suppose our probability of a certain other kind of disease is 0.1, so 10% of the population has it. Our test in the positive case is really informative, but there's a 0.5 chance that if I'm cancer-free the test, indeed, says the same thing. So the sensitivity is high, the specificity is lower. And let's start by filling in the first 3 of them. Obviously, these are just 1 minus those: 0.9, 0.1, and 0.5. Notice that these two numbers may very well be different. There is no contradiction here. These guys have to add up to 1, so given ¬C, the probability of positive and negative have to add up to 1, but these guys don't. It takes a lot of practice to understand which numbers have to add up to 1. But I set it up in a way that you should have gotten it right. Now comes the hard part: What is P(C, Neg)? And the answer is 0.01. P(C) = 0.1, and P(Neg│C) is also 0.1, so if you multiply those two they are 0.01. And what's the same for P(¬C, Neg). And the answer is 0.45. P(¬C) is 0.9, and P(Neg│¬C) is 0.5. So 0.9 * 0.5 = 0.45. What's the score over here? Well, you just add up these two numbers to get 0.46. So tell me what the final two numbers are. The first one is 0.01 divided by normalized 0.46 and that gives us 0.0217, and the second one is called over here 0.45 divided by 0.46 and that gives us 0.9783 and uses the correct posteriors, restarted our chance of 10% of having cancer. We had a negative result. We're down now to about 2% of having cancer. Let's now consider the case that the test result is positive, and I want you to just give me the two numbers over here and not the other ones. So once again, we have 0.9, 0.1, and 0.5 over here. Very quickly multiplying this guy with this girl over here 0.09. This guy with this girl over here 0.45. Adding them up gives us 0.54, and dividing those correspondingly 0.9 divided by 0.54 gives us 0.166 and so on and 0.833 and so on for dividing 0.45 by 0.54. And with this means, with the positive test result, our chance of cancer increased from 0.1 to 0.16. Obviously, our chance of having no cancer decreased accordingly. You got this, so let's just summarize. In Bayes rule, we have a hidden variable we care about--whether they have cancer or not. But we can't measure it directly and instead we have a test. We have a prior of how frequent this variable is true and the test is generally characterized by how often it says positive when the variable is true and how often it is negative and the variable is false. Bayes rule takes a prior, multiplies in the measurement, which in this case we assume to be the positive measurement to give us a new variable and does the same for all actual measurement, given the opposite assumption about our hidden variable of cancer and that multiplication gives us this guy over here. We add those two things up and then it gives us a new variable and then we divide these guys to arrive the best estimate of the hidden variable c given our test result. And this example, I used the positive example is a test result but it might do the same with a negative example. This was exactly the same as in our diagram in the beginning. There was a prior of our case, we have this specific variable to be true. We noticed inside this prior, it can cover the region for which our test result applies. We noticed that test result also apply when the condition is not fulfilled. So, this expression over here and this expression over here corresponds exactly to the red area over here and the green area over here. But then we noticed that these two areas don't add up to 1. The reason is that's lots of stuff outside, so we calculated the total area which was this expression over here, pPos. And then we normalized these two things over here by the total area to get the relative area that is assigned the red thing versus the green thing and at this time by just dividing by the total area in this region over here; thereby, getting rid of any of the other cases. Now, I should say if we got this, you don't find any immediate significant about statistics and probability. This is totally nontrivial, but it comes in very handy. So, I'm going to practice this with you using a second example. In this case, you are a robot. This robot lives in a world of exactly two places. There is a red place and a green place, R and G. Now, I say initially, this robot has no clue where it is, so the prior probability for either place, red or green, is 0.5. It also has a sensor as it can see through its eyes, but his sensor seems to be somewhat unreliable. So, the probability of seeing red at the red grid cell is 0.8, and the probability of seeing green at the green cell is also 0.8. Now, I suppose the robot sees red. What are now the posterior probabilities that the robot is at the red cell given that it just saw red and conversely what's the probability that it's at the green cell even though it saw red. Now, you can apply Bayes Rule and figure that out. In this example, it gives us funny numbers. It was 3 for red as 0.8 and one for the green as 0.2. And it's all to do with the fact that in the beginning where there had no clue where it is. The joint for red after seeing red is 0.4. The same for green is 0.1. 0.4+0.1, S to 0.5. If you normalized 0.4 divided by 0.5, you get 0.8, and if you normalized 0.1 by 0.5, you get 0.2. If I now change some parameters--say the robot knows the probability that it's red, and therefore, the probability 1 is under the green cell as a prior. Please calculate once again using Bayes rule these posteriors. I have to warn you--this is a bit of a tricky case. And the answer is, the prior isn't affected by the measurement, so the probability of 0 is at red, and the probability of 1 at green, despite the fact that it's all red. To see this, you find the joint of seeing it red and seeing red is 0 times 0.8, that's 0. That's the same join for green is 1 times 0.2. So you have to normalize 0 and 0.2. The sum of those is 0.2. So let's divide 0 by 0.2, gives us 0, and 0.2 divided by 0.2 gives us 1. These are exactly the numbers over here. To change this example even further. Let's make this over here a 0.5 and revert back to a uniform prior. Please go ahead and calculate the posterior probability. Now the answer is about 0.615 or 0.385. These are approximate. Once again, 0.5 times 0.8 is 0.4. 0.5 minus this guy is again 0.5. 0.25, add those up, 0.65, normalizing 0.4 divided by 0.65 gives approximately 0.615. 0.25 divided by 0.65 is approximately 0.385, so now you you've got it. I will now make your life really hard. Suppose there are 3 places in the world, not just 2. There are a red one and 2 green ones. And for simplicity, we'll call them A, B, and C. Let's assume that all of them have the same prior probability of 1/3 or 0.333, so on. Let's say the robot sees red, and as before, the probability of seeing red in Cell A is 0.9. The probability of seeing green in Cell B 0.9. Probability of seeing green in Cell C is also 0.9. So what I've changed is, I've given the hidden variable, kind of like the cancer/non cancer variable, 3 states. There's not just 2 as before, A or B. It's now A, B, or C. Let's solve this problem together, because it follows exactly the same recipe as before, even though it might not be obvious. So let me ask you, what is the joint of being in Cell A after having seen the red color? This is the joint as before. And just like before, we multiply the prior, this guy over here, that gives you 0.3. What's the joined for Cell B? Well, the answer is you multiply our prior of 1/3 with the probability of seeing red in Cell B, as seeing green at 0.9 probability, so red is 0.1. So 0.1 times this guy over here gives 0.033. Finally, probability of C and Red. What is that? And the answer is exactly the same as this over here, because the prior is the same for B and C, and those probabilities are the same for B and C, so they should be exactly the same. So here's the $100,000 question. What is our normalizer? And the answer is, you just add those up. And now we calculate the desired posterior probability for all 3 possible outcomes. So please plug them in over here. As usual, we divide this guy over here by the normalizer, which gives us 0.818. Realize all these numbers are a little bit approximate here. Same for this guy, it's approximately 0.091. And this is completely symmetrical, 0.091. And surprise, these guys all add up to 1. So what have you learned? In Bayes Rule, there will be more than just 2 underlying causes of cancer/non cancer. There might be 3, 4, or 5, any number. We can apply exactly the same math, but we have to keep track of more values. In fact, the robot might also have more than just 2 test outcomes. Here was red or green, but it could be red, green, or blue. And this means that our measurement probability will be more elaborate. I have to give you more information, but the math remains exactly the same. We can now deal with very large problems that have many possible hidden causes of where the world might be, and we can still apply Bayes Rule to find all of these numbers. Let me give you one final test. This test is actually directly taken from my life and you'll smile when you see my problem. I used to travel a lot. It was so bad for a while. I would find myself in a bed not knowing what country I'm in. I kid you not. So let's say, I'm gone 60% of my time and I'm at home only 40% of my time. Now at summer, I live in California and it truly doesn't rain in the summer. Whereas in many of the countries I have traveled to, there's a much higher chance of rain. So let's now say, I lie in my bed, here I am lying in bed, and I wake up and I open the window and I see it's raining. Let's now apply Bayes rule--What do you think is the probability I'm home now that I see it's raining--just give me this one number. And I get 0.0217, which is a really small thing. And the way I get there is what taking home times the probability of rain at home normalizing it using the same number of a year plus the calculation for the same probability of being gone is 0.6 times the rain I've been gone has a probability of 0.3 and that results is 0.0217 or the better of 2%--did you get this? If so, you now understand something that's really interesting. You're able to look at a hidden variable, understand how a test can give you information back about this hidden variable and that's really cool because it allows you to apply the same scheme to great many practical problems in the world--congratulations! In our next unit, which is optional, I like you to program all of this so you can try the same thing in an actual program interface and writes software that implements things such as Bayes rule. But not to worry, this is optional. If you don't know how to program just skip the next unit. So this unit is optional, but I find it very empowering to program what we've learned on the computer, because once you've programmed it, you can apply it forever. So, let me show you what I mean. So, here's our programming environment again, and I've given you a very, very simple program. You can process it, it's called print 0.3 and in the next window, I just want you to hit the run button to see what happens. And not surprisingly, when you hit run, you should get 0.3 in the output window. We will now give a slightly more complicated problem of the form we're going to be using we're going to be using and I hope you won't be too confused. So, this is it. There's two parts to it. There's the print command as before but rather than printing 0.3 directly, we're going to print the function that computes something from 0.3. Right now it's identity is going to print out exactly the same value but we're doing this is to set ourselves up to print something else. And here is how f is defined. We define f with the argument p that will be set to 0.3, to be just a function that returns exactly the same value. Why do we do this? Well, to practice programming. So, hit the run button and see what happens. And once again, we get 0.3 and the reason is, as we go up, 0.3 is being funneled into the function f. F starts over here and p is now 0.3. We return then the value 0.3 straight from the input and then the return of this is being printed. Sounds complex--well, from now on all I want you to do is to modify what's inside this function. So as the first exercise, say this is the probability, let's print the probability of the inverse event. Let's make the function over here that takes p but returns 1 - p. So please go ahead and modify this code such that the return value is 1 minus p and not p. This modification just replaces p by 1-p in the return function and when I run it I get 0.7. So the nice thing about our complimentary probability we can now plug in a different value over here, say 0.1--with 0.1, I get as an output 0.9. So congratulations, you've implemented the very first example of probability where the event probability is 0.1 and the complementary event and negation of it is encapsulated in this function over here. Here is my next quiz for you. Suppose we have a coin with probability p. For example, p might be 0.5. You flip the coin twice and I want to compute the probability that this coin comes up head and heads in these 2 flips--obviously that's 0.5 times 0.5. But I want to do in a way that I can use any arbitrary value for p using the same style of code as before. So all you're going to modify is the 1-p into something that if I give a probability p returns to me the probability of seeing heads twice in this coin--that is the probability of heads. And here is one way to implement this, just return p p and for 0.5, it gives me 0.25. If I make this a loaded coin of probability of heads a 0.1, then the outcome is 0.01 and I hit the run button. So let's up the ante and say we have a coin that has a certain probability of coming up with heads--again, it might be 0.5. Just like before it will be an input to the function f and now I'm going to flip the coin 3 times and I want you to calculate the probability that the heads comes up exactly once. Three is not a variable so you could only works for 3 flips not for 2 or 4 but the only input variable is going to be the coin probability 0.5. So please change this code to express that number. You might remember for P = 0.5 then you go to the truth table, you’ll find the answer is 0.375. If you set P to 0.8, the number actually goes down at 0.096. So, you can check the implementation to see if you get the exact the same numbers. So, here’s my result--when you build the truth table, you’ll find that exact the 3 possible outcomes have had exactly once; it’s H T T, T H T, and T T H. So, of the 8 possible outcomes of the coin flips, those 3 are the ones you want to count. Now, each has exactly the same probability of P for heads x (1 - P) x (1 - P). So, to get all 3 of them together, we just multiply these by 3. And this is how it looks in the source code 3 x P x (1 - P) x (1 - P) if, for example, I give this input 0.8, then I get 0.096 as an answer. But if you never programmed before and you got this fight, then congratulations! You might actually be a programmer. Obviously, if you programmed before, this should relatively straightforward but it’s fun to practice. Let’s now go to a case where we have 2 coins. So coin 1 has a probability of heads equals P₁ and coin 2 has a probability of heads equals P₂ and this might not be different probabilities. In my programming environment, I can account this by making 2 arguments separated by a comma, for example, 0.5 and 0.8, and then the function takes as an input, 2 arguments, P₁ and P₂, and then I can use both of these variables in the return assignment. Let’s now flip both coins and write the code that computes the probability that coin 1 equals heads and coin 2 equals heads for example of 0.5 and 0.8, this would be? Yes, 0.4 is the product of these two values over here. So, in reality the solution is just to apply the product p1 p2, and hitting the 1 button gives me indeed 0.4. I can now go and change this probability to 0.1 and 0.8. You probably already figured out that the answer is now 0.08, and indeed, my code gives me the following result, 0.08. So two coins again, C1, C2. And let's say each coin has its own probability of coming up heads. For the first coin, we're going to call it P1, and for the second, P2. And for reasons that should be clear later, we write it as a conditional. So that means, if the coin you're flipping is C1, then the probability of heads equals P1. If the coin we're flipping is C2, then the probability of heads will be P2. Now, this alludes to the fact that I really want you to pick a coin here. You're going to pick one coin, and the probability of you pick coin one, C1, is P0. And logically, it follows the probability of picking coin two, the other coin, is 1 minus P0. And I'm interested in the probability that heads come up under the scenario where you first pick a coin at random and then flip the coin. And in this exercise, I give you some very concrete numbers. P0 is 0.3, P1 is 0.5, and P2 is 0.9. And the answer is 0.78. And the way I got this, you might have picked point C1. That happens with 0.3 probability, and then we have a 0.5 chance to find heads. Or we might have picked coin two, which has a probability of 1 minus 0.3, 0.7, and then chance of seeing head is 0.9. We work this all out, we get 0.78. So the task for you now is to implement the function with three input arguments that it computes this number over here so that it can vary any of those and still get the absolute correct answer for this function over here. If you've never programmed before, this is tricky. You have to add one more argument and you have to change the returned function to implement a formula just like this but this using p0, p1, p2 as arguments not just the fixed numerical numbers here. And here is my answer. You can really read off the formula that I just gave you. If p0 we pick coin one and it comes up with heads p1 and with the 1-p0 we pick coin two and it comes up with heads with probablity p2. So you can now given three arguments p0, p1, and p2 such as 0.3, 0.5, 0.9 and it gets us 0.78 if I hit the run button. Interestingly, if I change this numbers, for example the first one to 0.1 and the last one to 0.2. I now get a different result of 0.23. Let's go the cancer example. These are prior probability of cancer we should call P₀. This is a probability of a positive test given cancer. I call this P₁ and careful, there's a probability of a negative test result for don't have cancer and I call this P₂. Just to check suppose probability of cancer is 0.1, the sensitivity is 0.9, specificity is 0.8. Given the probability that a test will come out positive. It's not Bayes rule yet, it's a simpler calculation and you should know exactly how to do this. The answer is 0.27. We first consider the possibility we have actually have cancer, in which our tests will give us a positive result of 0.9 chance, and then we add the possibility of not having cancer that's 1-0.1 or 0.9, and then when one gives us a positive result with 0.2 chance or 1-0.8. Resolving this gives us something like this that is 0.09+0.18 adds up to 0.27. So now, I want you to write the computer code that accepts arbitrary P₀, P₁, P₂ and calculates the resulting probability of a positive test result and here's my answer. My code does exactly what I have shown you before. It first considers the possibility of cancer multiply this with the test sensitivity P₁ and then it absorbs the opposite possibility, and of course, the specificity over here refers to a negative test results, so we take 1 minus that to get the +1. Adding these two parts up give us the desired results so let's try this. Here's my function f with the prime that we just assumed, and if I hit, run I get 0.27. Obviously, I can change the prime with this. So suppose we make it much less likely to have cancer in the prior from 0.1 to 0.01 then my 0.27 changes to 0.207. Now realize it is not the posterior in Bayes' rule. This is just the probability of getting a positive test result. You can see this if you change the prior probability of cancer to zero, which means we don't have cancer no matter what the test result says, but there's 0.2 chance of getting a positive test result, and the reason is our test has a specificity of 0.8 that is even in the absence of cancer, there's 0.2 chance of getting a positive test result. So now I want you to write the computer code that accepts arbitrary P₀, P₁, P₂ and calculates the resulting probability of a positive test result. Here's my answer. My code does exactly what I've shown you before. It first considers the possibility of cancer, multiplies it with the test sensitivity p1 and then it observes the opposite possibility and of the course the specitivity over here refers to a negative test result so we take 1 minus this to get the positive one. Adding these two products up gives us the desired result. So let's try this. It gives me a function f with the parameters we just assumed and if I hit run, I get 0.27. Obviously I can change these parameters, so, suppose I make it much less likely to have cancer in the prior from 0.1 to 0.01 then my 0.27 changes to 0.207. Now I realise it's not the posterior in Bayes' Rule. It's just the probability of getting a positive test result. You can see this if you change the prior probability of cancer to 0 which means we don't have cancer, no matter what the test result says. But there still is 0.2 chance of getting a positive test result and the reason is our test has a specitivity of 0.8 that is, even in the absence of cancer, there is a 0.2 chance of getting a positive test result. Now, let's go to the holy grail and implement today's work. Let's look at the posterior probability of cancer given that we received the positive test result, and let's first do this manually for the example given up here. So what do you think it is? And the answer is 0.0333 or a 1/3 and now we're going to apply the entire arsenal of inference we just learned about. The joint probability of cancer and positive is 0.1 0.9. That's the joint that's not normalized. So let's normalize it and we normalize it by the sum of the joint for cancer and the joint for non-cancer. Joint for cancer we just computed but the joint for non-cancer assumes the opposite prior 1-0.1 and it applies the positive result of a non-cancer case. Now because the specificity first is negative, we have to do the same trick as before and multiply it with 1-0.8. When you worked this out, you find this to be 0 to 0.9 divided 0 to 0.9 + 0.9 0.2 that is 0.18 So if you put these all of this together, you get exactly a third. So I want you to program this in the IDE where there are three input parameters P⁰, P¹ and P². For those values, you should get a 1/3 and for those values over here, 0.01 as a prior 0.7 as sensitivity and 0.9 as specificity, you'll get 0.066 approximately. So write this code and check whether these examples work for you. And here's my code, this implements Bayes rule. You take p0 a prior times a probability of seeing a positive test result and divided by the sum of the same plus the expression for not having cancer, which is the inverse prior and the inverse of this specificity is shown over here. When I plug in my reference numbers, the ones from over here, I indeed get 0.33333. So, this is the correct code and we can plug in our return numbers. It's fun if we give it a zero probability prior to have cancer and guess what, no matter what the test is, you still don't have cancer. That's the beauty of Bayes' rule, it takes the prior very seriously. Now, let's do one last modification and let's write this procedure assuming you observed a negative test result. This means the posterior of having cancer under a negative result is 0.0137 for those numbers over here and about 0.00336 for those numbers over here. In both cases, the posterior is significantly smaller than the prior because we received negative test results. So, go ahead and modify your procedure accordingly. And here's my implementation for the cancerous case. You don't have to plug in the measurement probability to see a negative test result, which is one minus the sensitivity and in the normalizer, we copy the first term over in the second term of the noncancer hypothesize. we just put in the specificity and when you put this all together and run the procedure, we indeed get 0.013698 and so on. That's the number I gave you for the first example. So I really hope you enjoyed all this, and the reason is I really want you to do a program basic statistics by yourself. So, for the rest of your life, you know how to program it. When you know how to program it, you know how to do it, and it will empower you to solve basic problems in probability. I should tell you in my own life, I've built a lot of robots, and I've applied Bayes rule like crazy. My job talk at Stanford was all about Bayes' rule applied to robotics. It's a very powerful paradigm and I hope you enjoyed it. If you want to dive deeper, there's always Udacity CS373, which talks about programming and about half of the classes variations of Baye's rule. I'll spare you with the details here because it's a basic introduction class, but stay tuned for the next unit, I have another great surprise in store for you. So today I want to talk about a little bit more of probability distributions and specifically talk about what ??? continuous distribution is because there is some ??? that you should be aware of ??? look forward in this class. So here's a new prop. It's a ??? gun. They actually shoot like this and we have a lot of those at Udacity and I want to know doing experiment in which I shoot it projectile and some way hits the ground. And the answer is zero, nothing else but zero. Because any specific value over here is just really, really unlikely – so unlikely we can give it a probability. This is a more common example; it’s called the wheel of fortune. We’re twisting an object like a bottle, or like a pen and then it arrives at a specific angle. We all know that angles go between zero and 360 degree. So let’s take a specific angle. What is the probability that our object has the very precise angle 180 degrees? And the answer is zero. It’s zero for any other number. 179.99, 179.98 and so on, each angle has probability zero. That’s because getting that exact angle right with exact the right decibels, you just won’t be able to get there. So now here is the conundrum. Does this mean that the object stops nowhere, yes or no? Say, yes, if you believe it stops nowhere. And of course it stops. So, no, is the right answer. But the bizarre thing is, it’s going to have an angle, like this one over here, this might be 101.374819 and so on. And this number truly has probability zero. So whatever the outcome is, that specific outcome will be unlikely, so unlikely as probability zero. And this is one of the bizarre things about probability when you go to continue spaces. In continuous distributions, every outcome has probability zero. And that might sound entirely counterintuitive and it is, but it is important to understand. So let’s now redefine how to treat continuous distributions, here is our circle again and here is the bottle that we are spinning. Let’s call the outcome X, that’s the final angle at which the bottle arrives. Tell me what’s the probability that X is between zero and 180 degrees? Put your answer here. And you guys correctly, it’s half. There is a 50% chance of being between zero and 180 and the 50% chance of between 180 and 360. How about the interval 260 degree all the way to 290 degrees, what’s the probability? That’s actually one twelfth: 0.0833 and so on. Okay, let’s make this interval really, really small. Let’s say we go from 179 degrees to 180 degrees. So, it’s specific up to a single degree but is still an ever so slight range in the spectrum over here. What’s the probability? It’s 0.002777 which is the same as 1 over 360. So, obviously, the probability for any interval, defined by A and B, is the size of the interval divided by 360. And, if you got those right, you understand all this. So, I’m now going to teach you a concept that is very deep, called the density of a probability. And it’s kind of like a probability for continuous spaces but not exactly. It’s just a good motivator to think of a density as a – of a probability for continuous spaces. Let’s take the bottle example. We know that the outcome of turning the bottle lies between 0 and 360 degrees. So, given what we learnt, you want to assign 0 to anything outside that range but inside this range, we’d love to give a value and say, look, there is a function here that renders every outcome in this interval equally likely. So, you’re going to construct this function for So, you’re going to construct this function for c number one, each outcome in the range of 0 to 360 has an equal value, and, constraint number two, the area under this function sums up or integrates to 1 which means this area over here is just 1. So, tell me what is this function for X’s that are between 0 and 360 degrees. There is a single numerical value that goes in here and you have to guess it. And, the answer is, again, 0.002777 and so on, which is the same as 1 over 360. And, the way to see this is that the width of this interval over here is 360. We didn’t know the height which is what I asked you to guess. But, if we multiply 360 by 1 over 360 then the area of this rectangle over here becomes 1. So, let’s do this again. Suppose, you look at the date and time you were born, and then specifically look not at the date , not even the time, but just the seconds of the time. So, it could be that we were born exactly as the minute began, in which case, it was zero ; or exactly at second 59.282, I don’t know. Let’s assume it’s completely uniform. There is no preference to be born early in the minute or late in the minute. I’m going to ask you two questions now. For any such thing, X, where X is the seconds of the timestamp , what’s the probability of that specific X ? And, what’s the density of that specific X? It might mean that these are actually different questions. And, the correct answer here is 0. Any specific continuous thing, 0. For uniform distribution, we find that the interval we’re talking about skates between 0 and 60 so the correct answer is 1 over 60 and that is 0.01666. So, here’s a new quiz and in this quiz you’ll be looking into densities that are non-uniform. In particular, I’m looking at the time of day when people are born. And, let’s assume for this exercise that it’s twice as likely to be born before noon than in the afternoon or the evening. So, if you look at the time of day and the density for the time of day, I wonder if the shape is like this. This is zero and this is midnight, right at 24. Perhaps like this or more like that? Pick the one that you think is most plausible. This is the one I would pick and the reason, if it’s twice as likely to be born before noon, it means the density in the morning, before noon, should be twice as high as the density in the afternoon. And that’s best depicted by this diagram over here. This one would be uniform so ignoring the fact that it’s twice as likely. And, this is even worse; it emphasizes the afternoon over the morning. And, now, comes the hard part. I want you to calculate the actual density. We know that the density for the birth time to be before noon is twice as large as the density for being born after noon. So, here is my density function, time of day X, Nf of X and you’ve already identified the shape. And, there’s now two parameters, A and B. I want you to figure out for me what A is and what B is, assuming that the basic unit here is in hours. Put differently, there is 24 hours horizontally in this interval . As a hint, make sure that the area underneath sums up to 1. And, here’s how I calculate it. These spheres over here are all of the same volume. Since they have to add up to 1, each of those has exactly a third. Now, let’s start with the right one. If this is a third in terms of area and we have to cover 12 hours, then B equals to A third times 12 which is 0.0277. The one on the left is exactly twice the size, 0.0555 which is two-thirds times 12, it’s 1/18th. if we multiply A by 12 because the density over here applies to 12 hours, plus B by times 12, tell me what the resulting number is. And, I hope you got this right as, of course, 1 because the surface area has to add up to 1. This was to cross-check over here, whereby 1/18th times 12 is two-thirds and 1/36 times 12 is one-third, and two-thirds plus one-thirds equals 1. So, this was an example of a non-uniform density that you were able to compute. As you can tell, each specific birth time has exactly a probability of zero and this is just a density. Here’s another quiz. You’re dropping a package out of an aircraft, and the package goes down and we happen to know it takes between 3 and 3.5 minutes to reach the ground. And, let’s say, in between the probability density is uniform. Asking the same question again. There’s something to be learnt here. Here is 3. Here is 3.5. Obviously, the density outside is zero, is uniform, and there’s a value here A. And, I want you to tell me what is A. And in my calculation A is 2. And, the reason is the normalizer over here is a half, from three to three and a half. To multiply a half with 2 gives us the area of 1 which is what we want to have for density. Now, what’s interesting here? A is larger than 1. Probabilities are never larger than 1. They are bound to the above by 1. But, densities can be larger than 1. In fact, if this package were to arrive between 3 minutes and 3 minutes and 1 second, then this would be 60 because there are 60 seconds in a minute. So, it is possible that densities become larger than 1; that’s a bit confusing if you think it was probability. But , you should know that this is the case. Let me ask another trick question and that’s somewhat academic. For function f to qualify as an entity which of the conditions have to be fulfilled: has to be positive, non-negative, continuous, and smaller or equal to 1? Check any or all that apply including the ‘none of above’ if you believe none of those apply. And, this is a really tricky question. So, clearly, densities don’t have to be positive everywhere. It suffices to be non-negative and I gave you examples where outside of certain interval density was zero. Zero is not positive. Zero is just non-negative. Now, densities don’t have to be continuous. I gave you an example of a density like this. And, over here where the density jumped, you had a point of discontinuity. So, continuous is not correct. And, if you got this wrong, no big deal. The word continuous might not even mean much to you. But, it’s one of these things to know about densities. Now, smaller or equal to 1, that’s a really tricky one. And, I would submit that’s not correct and here’s why. Suppose you have a density that assigns uniform probability only to values between 0 and 0.1, then the height of this density could be 10; it’s 1 over 0.1. And, as a result, it’s possible that densities can exceed 1. And that is a key point that makes them different from probabilities. Probabilities are always smaller than or equal to 1. Densities can be larger than 1. So you just learned about the concept of a probability density and that’s very cool. You now know what a uniform density is, you’ve encountered a new type of density that have a step in the middle and later in this class we will see densities that are very funny like this one over here called the Gaussian. We get back to this after we talked about large numbers and something funky called the central limit theorem or CLT. Awhile back I've told you about the Simpson's paradox and it was surprising how easy it was to draw the false conclusion from data. Today I will give you a deep insight of the common mistakes that's being made in interpreting statistical data by confusing correlation with causation. I'll show you example where data is correlated and why it's tempted to confuse correlation with causation. So both of those are words that start with a C and very frequently I read newspaper articles that deeply confuse both the relationship of correlation and causation--so let's dive in. Suppose you are sick, and you wake up with a strong pain in the middle of the night. You so sick that you fear you might die, but you're not sick enough not to apply the lessons of my Statistics 101 class to make a rational decision whether to go to the hospital. And in doing so, you consult the titer. You find that in your town, over the last year, 40 people were hospitalized of which 4 passed away. Whereas the vast part of the population of your town never went to the hospital, and of those, 20 passed away at home. So compute for me the percentages of the people who died in the hospital and the percentage of the people who died at home. And the answer is quite obviously 10% of the people in the hospital died, 4 over 40, whereas only 2.5% of the people at home passed away. Now I offer these as a fictitious example – these are relatively large numbers. But what’s important to notice is the chances of dying in a hospital are 40 times as large than dying at home. That means whether you die or not is correlated to whether or not you are in a hospital. So the chances of dying in a hospital are indeed 40 times larger than at home. So let me ask the critical question. Shall you now stay at home, given that you are a really smart statistics student, can you resist the temptation to go to the hospital because indeed it might increase your chances of passing away. And realize both answers count, but clearly the correct answer is no. You should go to the hospital. Hospitals don’t cause sick people to die. I know that there has been lots of studies that show if a perfectly healthy person goes to a hospital, they might actually catch a disease there but hospitals have a reason, they want to cure people. Why is this interesting? Because based on the correlation data, it seems that being in a hospital makes you 40 times as likely to die than being at home but that doesn’t mean by staying at home, you reduce your chances of dying So this is a statement of correlation. Being in a hospital, that fact alone, increases your probability of dying by a factor 40 is a causal statement. It says the hospital causes you to die. Not just it coincides with the fact that you die and very frequently people in the public get this wrong. People observe there is a correlation but they suggest the correlation is causal in attempting to make you understand the statistic as a call of action. Now, to understand why this could be wrong let’s dive in a little bit deeper into the same example Let's say of the 40 people in the hospital, 36 were actually sick and passed away, and some were healthy, 4 of them, and they all survived. Let's further assume, for the people at home, 40 were indeed sick, and 50 of them passed away, whereas the remaining 7,960, they were healthy, also inquired a total death of 20, perhaps because of accidents. These statistics are consistent with the statistics I gave you before. We just added another variable, whether the person's sick or healthy. Please now fill out once again, in percent, what is the percentage of people that passed away in each of of those 4 groups? When you divide 4 by 36, you get 0.111, or in percent, 11.1%. That's the mortality rate of sick people in the hospital. It's 0 for the healthy people. The mortality rate at home is 50%--half of them pass away-- and 0.25% approximately for healthy people. Now, if we look at this, we realize that you are likely sick. If you fall into the sick category, your chances of dying at home are 50% and it's just about 11% in the hospital. So, you should really go to the hospital very quickly Let's observe in more detail why the hospital example gives us such a wrong conclusion. We study two variables--in-hospital and dying or passing away. We rightfully observe that these two things are correlated. If we were to do a scatter plot where we have two categories-- whether or not we're in the hospital and whether or not a person passed away-- you find there's an increased occurrence of data over here and of data over here relative to the other to data points over here. That means the data correlates. What does correlation mean? In any plot, data is correlated if knowledge about one variables tells us something about the other. This is a correlated data plot. Here's another data plot. Correlated or not? Yes or no? The answer is no. No matter where I am in A, B seems to be the same. And now the data sits, a square in which data is uniformly arranged, correlated, yes or no? The answer is negative. No matter where I am in A, the range for B is the same, as is the mean estimate. Another data set--there's boomerang over here. Correlated? Yes or not. The answer is yes--clearly for different values of A, I get different values of B. Another linear correlation yet still a correlation. So clearly in our example, whether or not you're in a hospital correlated with whether or not you died, but the truth is, the example omitted and important variable, the sickness, the disease itself. And in fact, the sickness did cause you to die, and also effected your decision of whether you go to a hospital or not. So if you draw acts of causation, you find sickness causes death, and sickness causes you to go to the hospital, and if anything at all, once you knew you were sick, being in the hospital negatively correlated with you dying; that is, being in a hospital made it less likely for you to pass away given that you were sick. In statistics, we call this a confounding variable. It's very tempting to just omit this from your data, and if you do, you might find correlations; in this case, a positive correlation between the hospital and death, that have nothing to do with the way things are being caused, and as a result, those correlations don't relate at all to what you should do. So let's study another example. Suppose you observed a number of different fires and you graph the number of firefighters versus the size of the fire. And for the sake of the argument, let's assume we studied four fires with 10, 40, 200, and 70 firefighters involved and the sizes of the fires were given as follows: 100, 400, 2000, and 700 in terms of the surface area that the fire occupied. Putting this into a diagram, you get pretty much the following. Put the number of fighters. In fact, you've already learned this looks very linear. So, let me ask a question--is the number of the firefighters correlated with the size of the fire? Yes or no? Check one of the two boxes. And obviously it is because there's a strong linear correlation. Now the real question I'm bringing up to is, "Do firefighters cause fire?" or more extremely, "If you're going to get rid of our firefighters, will you get rid of all the fire?" Obviously, this seems to be in the data. And the answer is no. This is a case of what we call reverse causation. You can argue that the size of the fire causes the number of firefighters that is being destroyed, and that's because the bigger the fire the more firefighters the fire department will send. Now our graph, which shows the correlation between these two variables is oblivious to the direction of this arc. You could conclude size causes this and fire than the firefighters. You could conclude the number of firefighters causes this size. In both cases you could use exact the same data. But when I put it this way, it's pretty obvious that the right answer should be the size of the fire causes the number of firefighters to grow up and it's not from the data itself. It's because we know there's something about fire and firefighters. It's impossible to deduce from this data that it causes a relationship. It could be just coincidental or that cause a relationship could go either way. So here's my assignment for you. Go online and check old news articles and find me one that takes data that show a correlation and from the data suggests causation or differently it tells what you what to do. I argue with the news is full of this abuses of statistics and we will talk later about how to set up a study to avoid this trap, but in your assignment if you find an article that has this property extract that text and post it to the discussion forum. I will be monitoring and comment on those and I want all of us to enjoy what kind of hilarious funny misinterpretations of statistics you can find that people confuse correlation and causation So go ahead and find me interesting articles. Thank you! I will now teach you about estimators. You will learn exciting terms like maximum likelihood estimator and laplacian estimator. And what this will empower you to do is to derive the probability from things such as coin flips. Let's get started. In this class, there's the subtitle to fake or not to fake. And without giving it away, the outcome will surprise you. So let's get started. Suppose I flip a coin 6 times and get this as an output 100101, where 1 represents heads and 0 represents tails. The problem I'm addressing is I'd like to find out what should we think about this coin. In particular, if someone asks us for this coin what is its probability to come up heads, what would you say? And obviously, this looks like a fair coin. It comes up heads 3 times and tails 3 times. So how about 0.5? Different coin. Now flip it 5 times. What do you think the probability of heads should be? And the most obvious answer is what's called the empirical frequency where the word empirical means the same as observations, what you actually saw. So we have 4 of the 5 coin flips gives us heads. So the obvious answer is 0.8. Let me do a third quiz--this coin was flipped seven times and it came up tails every single time. What do think we should see now using the same method. And you'll probably say 0. Now look I had a fixed formula in all of those. If you call the data X₁, X₂ all the way to Xn where n is the number of coin flips, I wonder which of the following formula do you think best capture as what we we're doing the sum/Xi, 1/N<i>sum/Xi, the product of Xi or 1/N<i>the product of Xi.</i></i> And I'm sure you guessed this correctly--we're taking the sum of the outcomes of the normalized by the total number of experiments and because the outcomes is 0 and 1, the sum over here cannot exceed the value n; hence, this expression over here will always be between 0 and 1, which is the value of probability. For reasons that should be obvious in a minute, we call this the maximum-likelihood estimator. Keep this formula in mind--it is a really good way to guess an underlying probability that might've produced any given data set. Let's generalize this to more than two outcomes. Here is a die with six possible outcomes. Let's assume you've got the following data sequence for 10 different experiments, and now we're going to apply the same method to compute the probabilities of the six different outcomes. Please enter them the six cubes over there. Using the maximum-likelihood estimator MLE that we just discussed and here are my answers. 1 was observed in 1 out of 10 experiments, gives us 0.1--1 divided 10. 2 was observed twice 0.2, 3, 4 and 5 was observed once each, and 6 was observed four times, one, two, three and four, is a 0.4. Now, these are the most likely settings for the underlying probabilities. Just for safety check, let's add them up and tell me what the sum of those probabilities would be. It's 1. All these probabilities always have to add up to 1, so you can easily check this. Let's now dive in and understand how to use this cryptic name maximum likelihood estimator. The reason is because the estimation problem is given data, find the probability p which now a simple example was the probability of heads. Given a fixed p we started in probability, what's the probability of the data. So let's begin with the second and find a good solution for the first problem-- the estimation problem, and again I'm going to ask you a quiz. Supposed my data looks like this--I want you now for the different values of p to calculate for me what's the probability of the data. Remember this is exactly the problem we discussed earlier when we introduced probabilities of multiple independent events. The answer for the first one is 0.125--that is the half times the half times the half, makes an 8th. Let's now change p to be 1/3. With the exact same data, what is now the probability of that data? And the answer is 0.074. The first one would give us a probability of 1/3 · 2/3 · 1/3. And the model that heads come is a probability of 1/3. These are the individual probabilities. You multiply them together. And when you do this, you get 0.074, which is the same as 2/27. Let's keep searching. P = 2/3. What do we get? We get 0.148. This is the largest number so far. It's the product of 2/3 for the first one times 1/3 times 2/3 or 4/27. So obviously the larger the value of p the better the result. So let's go to extremes. P = 1. What's the probability of data now? Give me your best answer. And bummer--it's is 0. These two guys contributed 1. But the middle guy has literally a probability of 0. So 1 x 0 x 1 is 0. If we graph this with the probability as the horizontal axis and P(data) as the vertical axis. Then, for 1/2 we get a certain value. For 2/3, we get a larger value. For 1/3, you might be down here. And for the extremes of 1 and 0, you end up down here in 0. So we effectively are getting a cliff but as we find out P is exactly at 2/3 and goes to 0 in both extreme so this point, 2/3, is the one that maximizes the likelihood of the data. Therefore, it's called the maximum likelihood data estimator or MLE. Now let's expose the weakness of the maximum likelihood estimator. Let's be wicked. Suppose I flip the coin exactly once and it comes up heads. What do you think is the probability of heads for this coin under the maximum likelihood estimator? And the answer is 1. 1 heads over 1 observation. That's 1. Let's say you flip another coin once and you get 0. It's a complete set of experiment. What do we now think the probability of heads is? And the answer is 0. 0/1. Now does this mean from a single coin flip, the maximum likelihood estimator will always assume a loaded or biased, yes or no? In particular, what if I use an unbiased coin like this one? Will the maximum likelihood estimator always be doomed to be wrong? And very certainly, the answer is yes. From a single coin flip, the maximum likelihood estimator will always assume that all future coin flips come from the same way. Let me ask you again. Suppose you make 111 coin flips. Will the maximum likelihood estimator always assume that the coin is loaded? And very certainly, the answer is again yes. An unloaded coin has a probability of 0.5. So whatever the ratio the top, the observations of heads must be exactly half the total number of observations. If the total number of observations is 111, that's just not possible. So the coin will look ever so small loaded even if it's a fair coin. That is frustrating. Somehow the maximum likelihood estimator seems reckless. We have almost no data, and it comes up with these really extreme guesses of what the underlying probability is. I would submit this as unintuitive. If you and I flip a coin and it comes up heads, we would not say obviously this coin always comes up heads. We would say maybe a little bit more frequently but let's flip it more often. You're not yet convinced there might not be tails as there is in this case. There is a solution. The solution is to fake it or more precisely to add fake data. So what I'll ask you to do is to take the original data for the coin, and add two fake data points—one heads and one tails. So if my data was a single observation of heads, give me first the maximum likelihood estimate. And in the second box, give me an estimator that has one fake data point. And on this fake data, apply its maximum likelihood. So this is the one without fake data. This is the one with the fake data. As before, without it'd be 1, but with it'd be 2/3 or approximately 0.667. The reason is without fake data, one out of one coins flips gave me heads, but with the fake data we had three experiments, of which two came up heads, so 2/3 is 0.667. The same for the data 001, please. Without fake data, with fake data. It'll be 0.33, because 1 out of 3 experiments gave me heads, but when I add the fake data, we get 2/5 or 0.4. Let's take a case where both events are equally likely. Given me the number for the original data set and for the original fake data set. It happens to be 0.5 in both cases. In 2 out of 4 experiments, we saw 1. With fake data, it's 3/6, which is still a half. Just for the fun of it, let's assume we don't just have 1 heads, but we also have heads twice. What's going to happen for the original data? What do you get for the fake data? For the original data, it's once again 1--2 out of 2 experiments ended up heads. But for the fake data, we know that 3 out of 4 experiments come up heads and that's 0.75. There are a couple of things to observe here. One is in general the fake data pulls everything towards 0.5. Where you go to extremes over here, we are less extreme in this case. 0.33 is further away from 0.5 than 0.4. So, all these numbers get moved towards 0.5. This is somewhat smoother. We also see that these two outcomes--the first and the last-- on the division model gives us the same extreme estimate, but the more data we get in our new estimator, the more we are willing to move away from 0.5. One observation of heads gave us 0.667, two of them 0. 75. I can promise you in the limit, as you only see heads for infinitely many, we will finally approach 1. Now, this is really cool. We added fake data, and I will tell you that I generally think these are better estimates in practice. The reason why is it's really reckless after a single coin flip to assume that all coins come up positive. I think it's much more moderate to say, well, we already have some evidence that heads might be more likely, but we're not quite convinced yet. The not quite convinced is the same as having a prior. There's an entire literature that talks about these priors. They have a very cryptic name. They're called Dirichlet priors. But, more importantly, the method of adding fake data is called a Laplacian estimator. When there is plenty data, Laplacian estimator gives about the same results as the maximum likelihood estimator. But when data is scarce, this works usually much, much, much better than the maximum likelihood estimator. It's a really important lesson in statistics. We study again our die. You observe the following sequence: 1, 2, 3, 2. I'll ask you know for the two different estimators what is the maximum like estimator and what is the laplacian estimator for our estimate of the probability of the outcome 1 for this die, based on that data. Let's start with the MLE. Obviously, it's 1 out of 4. It's 0.25. Now, let's move to the Laplace estimator. The way I defined the fake data is that I added a data point for each possible of the 6 outcomes for the die. The answer should be 0.2 with 6 fake data points--1, 2, 3, 4, 5, 6. We get to observe the outcome 1 twice and that gives us 2 over 10 or 0.2. Please do the same for the probability of 2, and please give me both numbers. 2 shows up twice in the original data out of 4 times, which makes 0.5. In the new data, we're observing it 3 times of 10, which is 0.3. So, in summary, we talked about the maximum likelihood estimator, and we even derived it mathematically for those of you who had the patience to stay on. It's a really simple formula. It's what's called the empirical count. We talked about the Laplace estimator, which added k fake data points, 1 for each possible outcome, and that resulted in the slightly more complex out, as shown in this formula over here. and we identified cases where the Laplace estimator is much better than the maximum likelihood estimator, specifically when there isn't much data. When there isn't much data, fake it by adding more data. I'll see you in class in the next unit. So we learned something really cool and we ask ourselves, shall we fake or not fake our data? I'm not so sure. When the data is scarse, I believe faking it gives you a better result. It is unreasonable to assume that after a single coin flip you know the exact probability and it's either 0 or 1. It's much more reasonable to say we don't know yet and take a more caution 2/3 or 1/3 as a probability. So obvioulsy I've shown to you that even under very very very simple methods faking it might be justified and in fact give you better results. That blows my mind. Now you will learn about the three Ms in statistics: the mean, the median, and the mode. And these are terms that are really important to know. They are useful in looking at data, and they are often confused. So let me bring clarity to the story of the three Ms. Let's talk about the first. We talked quite a bit about house prices, and here is a list of possible house prices. The mean is using the exact same formula as in the previous unit. It's the average of all those prices over here. Do me a favor and quickly compute it for those numbers over here. When you put in the number, don't worry about the thousands. It's something between 165 and 190. When you add those up, you get 870,000. This is the total sum of all the house prices. We have to divide by the number of houses, N=5. 870 divided by 5 is 174. That's the mean. Why is this useful? The prices I just gave you are really the type house prices you find in the region over here, which is a small section of Pittsburgh, Pennsylvania, where I lived for many years. You will see they go between $165,000 and $190,000. By computing the mean of $174,000, you can really characterize this section of this neighborhood of Pittsburgh. When we look over here, we find most house prices are actually a little bit cheaper-- 125, 148, 110, 160--but there are two outliers. One is 325K, but the big one is the one over here: 2.4 million. So do me a favor and compute the mean for this section of town over here. And remember, 2.4 million is the same as 2,400K. And the answer is approximately 492.6K. Aren't we suspicious of this number? Most of the homes here are in the $100K range. There is one of $325K and one of $2.4 million. The mean doesn't really reflect this. It makes you feel that the average home in the neighborhood is $492,000.00. But that's not a good description of what's happening. Instead, we have a situation where we can graph the price and the frequency, and we have a peak around $130K. But then there is a really, really long tail that gets us all the way to $2.4 million. And the sad part is rather than finding the peak over here, we find ourselves somewhere in the middle. And the reason is this one number of $2.4 million drags us to the right side and really has a strong impact on the mean, which ends up to be just below $500K. This is exactly where we get the second M, the so-called median. The median is a different statistic that seeks to find the typical house price in this list--the one in the middle. Put differently, if you sorted those, the median picks the one in the middle. I'm sure you can do this in your head, so please enter the median house price over here. To solve this, let us sort the house prices by their increasing numbers-- 110K, 125, 148, 160, 180, 325, all the way up to 2400-- and that makes it obvious the one in the middle is 160K. So $160,000 is the median house price. If you look at this neighborhood again, I would say the 160K is a really good characterization of the typical house price you find in this neighborhood. It's much, much, much more reasonable than the nearly 500K that the mean would provide us. So we've learned about two of the three Ms: the mean, the median, but not the third, so let's focus on the third, the mode. I will use as an example a children's birthday party. If you've ever been to one of those, there's usually kids and some of their parents. This one happens to have 5 kids and 4 parents, or 9 people in total. Let me write down the ages of the kids and their parents: 4, 3, 32, 33, 4 again, 32 again, 3, 38, and 4. So one of the things we can compute for this is the mean age of all the people at the party. As I'm sure you figured out, the mean is 17. If you sum all these things up, you get 153, divided by 9 gives us 17. Now, obviously, the mean isn't particularly informative. There are really no teenagers at this party. There's either toddlers or small children or parents. This is one of these cases where the mean is giving you bad statistics. So let's move on to the median. What's the median? To find this out, we sort the list. Here are the ages of the kids, and here are the ones of the parents. Now we see the median is 4. Is this a useful statistic? Not really. Suppose another two parents show up of ages 35 and 36. What would the median now be? And the answer is if we add 35 and 36 to the sequence, our median shifts here, and it becomes 32. So with a small modification of the people at the party, our median shifted from 4 to 32. That's kind of odd. The mode is the age that's most frequently represented at the party. We already encountered modes when we looked at bar charts and talked about this a little bit to find the most frequent bar. Here, because our ages are discrete, you can give me the most frequent age at this party. The answer is 4. It is irrespective of whether these two parents show up or not. Four is the most frequent age at the party. So the mode is a useful statistic if your distribution of data is what is called multimodal. In multimodal distributions, you have a curve that has multiple bumps. This one is called bimodal because it has exactly 2 bumps. The mode is the highest value of the largest bump, which now occurs here with the value of 4. So in a situation like this, the mean is usually irrelevant, It falls just between the data. as we showed with our house prices example. The median may be in the middle or flipped from left to right and really doesn't say much either. The mode, too, can flip from left to right. At the very least, it corresponds to something that is very frequent in the data. So for bimodal data, or for multimodal data, which is data with even more than two bumps, the mode is something you should really calculate. So let's practice. So let me give you a list of numbers: 5, 9, 100, 9, 97, 6, 9, 98, and 9. Compute for me the three Ms: mean, median, and mode. For the mean, we add up those numbers. The sum is 342 divided by 9, because it has 9 numbers, gives us 38. For the median, we re-sort them, and we find that 9 is the number in the middle. And for the mode, it's easy to see that there are 4 occurrences of 9. Every other number only occurs once. Obviously, sometimes there could be ties. There could be 2 elements in the center or 2 numbers that could be for the same mode, and we just assume that ties are broken at random. So pick either number; it doesn't really matter. Here is one final sequence: 3, 9, 3, 8, 2, 9, 1, 9, 2, and 4. Give me once again the mean, the median, and the mode. If we sum all of those up, you get 50, so the mean is 5. The mean is always precisely determined. The median is a bit more difficult to compute. If you sort the numbers, you get this sequence here. Both the 3 and the 4 sit in the center, so I accept both as valid answers, either 3 or 4. The mode in this case happens to be 9. But realize had there been one fewer 9, there would have been other candidates for the mode as well. So, with one less 9, 2, 3, and 9 would have all been valid answers for the mode. This finishes my class on the 3 Ms. In the next one, we'll learn about variance and standard deviation. These are actually fun topics relating to the spread of data. We learn the difference between data like this and data like this that is much more spread out, even though the 3 Ms might agree in all of these data. And we're going to sharpen our skills to really understand how to look at data. This unit is all about variance and one of its very close cousins, the standard deviation. Say you are a beginning college student, so your friends might be of the following ages: 17, 19, 18, 17, and 19. And you also have 5 close family members of ages 7, 38, 4, 23, and 18. Now for both you can compute the mean. Please enter the mean on the right side. And in both cases it's 18, as I'm sure you had figured out. There's something really surprising about these age distributions, which is these ones are clustered very close to 18 whereas these are all over the place really far from 18 in most cases. So can we calculate this somehow? I would say the mode, the median, or the mean don't really capture the spread of the data. What does calculate the spread is called the variance. Now we will compute the variance of the data, and the very first trick you apply is, we normalized these sequences by subtracting the mean from each data item. So in the case of the friends, if we subtract 18 from 17 we get -1, 1, 0, -1, 1. If we subtract 18 from these data sequence, we get -11, 20, -14, 5, and 0. If you compute the mean of these two sequences, then you find the mean in both cases is 0 and that's bigger the work and the reason is by subtracting the mean from every single data item we took it out of the equation and we arrived at 0. But what's now really interesting is, that these numbers are much closer to zero than these numbers over here. So, the spread over here is much larger. Now you might think in computing the variance, you just add up all these values and measure up the spread, but the truth is it doesn't work. If you add them up, they are actually 0. So we have to add up something else. What's commonly added up are the squares of those values. The squares here are 1, 1, 0, 1, 1. Whereas over here, they're 121, 400, 196, 25, 0. For the variance, we sum the squares of all these numbers and normalize that. But the squares are not taken of the original data but taken off the original data minus the mean and we're going to write that as Greek letter over here, µ. Do me a favor and try to compute the variance for the first sequence, which you obtained by adding those squares over here and dividing them by 5. The answer is 0.8, the reason being that the sum of those is 4/5 is 0.8. Let's do the same for the second sequence. Add them up and divide them by 5. Now the answer is very different. It's 148.4. And that's interesting. The variance is a measure of how far the data is spread. It's really small if the data is centered around the mean, and it's really large if the data falls far away from the mean. Here are two trees, and both of these trees have a couple of apples Of course, the apples at some point fall down. When they're on the ground, then all of them are centered equally around the stem of the tree, so if you computed the mean you would get back exactly where the stem of the tree is. But on the wide tree, the spread is much larger, whereas for the small tree, the spread is much smaller. You can think of variance as a measure of how wide the tree is from which the apples fell down. So coming back to our example, here are the variances again. Let me give you another word that's really important called standard deviation. The variance by its very nature computes in the quadratic. It's the quadratic deviation from the mean. In fact, it's the average quadratic deviation from the mean. If you don't want something quadratic, then you can take the square root of it, and you get what's called the standard deviation. In the first case √0.8 is 0.8944, and second case the √148.4 is about 12.18. This is how much you expect the age of an individual family member to deviate from the mean by about 12 years. Whereas here you expect the deviation to be just below 1 year. You can see why this is a good approximation. Here 4 of the 5 kids differ in age by 1 yr whereas here people differ as much as 20 years in age from the mean. The variance and the standard deviation are much larger. Let's practice this. I'll give you a couple of quizzes now. I'll give you data. I want you to complete the mean for me, the variance and the standard deviation. 3, 4, 5, 6, 7--please fill in all three of those values. Obviously, the mean is 5. When you sum up the squares, here the difference is -2² would be 4. It'd be 1 over here--0, 1, 4. You get 10, but you're going to compute the average distance, which is 10/5--that's 2. The square root of this is about 1.414. 8, 9, 10, 11, 12--what's the mean, the variance, and the standard deviation? The mean is now 10, and the variance and standard deviation stay the same. The reason being once you subtract the mean from the sequence, it's exactly the same as this data sequence over here. 15, 20, 25, 30, and 35-- what's the mean, variance, and standard deviation. The mean is obviously 25. In fact, every element over here is 5 times as large as any element over here. Now, how does this affect the variance? Well, when we compute the differences from the mean, they're going to be 5 times as large. When we square them, they will be 25 times as large. As a result, the variance is 25 times as large as over here--it's 50. And we can quickly check this. Obviously, there is no difference for 25 in the middle. 20 and 25 are 5 apart--the square of this is 25. Same for 30. 15 and 25 are 10 apart. The square is 100. We add those up--we get 250--and now we divide by 5 to get back our 50. The standard deviation is 7.071, and that's exactly 5 times as large as the standard deviation over here. If this is 25 times as large as this one, then the square root of it will be just 5 times as large as this one. You can easily verify that this is the square root of 50. Here is an interesting data sequence--3, 3, 3, 3, 3. What is the mean, and what's the variance and the standard deviation? The mean is obviously 3, the variance will be 0, and the standard deviation will be 0. The reason is when you subtract 3 from the sequence you have 0s left. The squares of those are 0s, the sum of those is 0. If you then complete the variance, it's 0. The square root of that is 0 as well, which means there is no spread at all in the data. In particular, consider a single data item as our sequence. Just 4--what do you think is the mean, the variance, and the standard deviation? The mean is 4, and the variance and the standard deviation are 0. For any single data item, you won't find variance or standard deviation not be different from 0. You got it? So, if your data falls onto, say, a curve like this, and this here is your mean, then you can think of the spread--this one over here-- as your standard deviation. The formula that gives you this as the mean is the now-familiar formula of the sum of the data points divided by the size of the data set. The variance--often written σ² is the sum of our data points normalized by the mean to the square. And then from that it follows that the standard deviation is just the square root of σ². These are the formulas that you just applied, and I hope you recognize them. Here are our equations again for the mean and the variance. They should now look very familiar. The problem with that is they require two passes through the data. First, I have to go through all the data and compute the mean. I do this by summing up all the data and dividing it by the total number of data items. For that, I maintain two things. I maintain the total number of data items, which I increment every time I see a new item. I maintain the sum of all Xᵢs, which I can easily add up as I go through the data. Once I've done this, I know µ, and then I can finally plug it in here, but now I have to go through the data again and compute this guy, so we can finally get to my covariance. Now, I want to teach you a trick now for which we don't maintain this guy but instead maintain just this guy. The nice thing about this is if you maintain these three things you only need a single pass. To see, as we go through the data, we can maintain the number of data items that we have. We can maintain the sum over here. We can maintain the sum of the data themselves. You can maintain simultaneously the sum of the square of the data items, but it's not obvious that from these three things alone you can figure out what σ² is. Let's play a game. The way the game works is that I put a number of mathematical expressions here on the right side. I'll start the derivation and leave little gaps and ask you to pick one of those and plug it into the equation. You can only use each of those ones but you won't use all of them, just a subset. Let me do the first step by hand and rewrite σ² by factoring out the square. I can rewrite this by multiplying things out. So, help me. What do you think goes in the middle? Which of those expressions? The correct will be this one. And this is the correct one, -2Xᵢµ, which is the mixed term in this multiplication, so let me just take it and plug it in right here, and we'll remove it from the list on the right side. Now, I break these sums into individual sums-- a sum for this guy, a sum for this guy, and a sum for this guy. That gives me 1/N Σ Xᵢ² minus something, and you pick on the right side, times Σ Xᵢ. What do you think it will be when I break out this term over here-- and individual component in the equation down here. Just look at this guy here. The answer is you have to obey the 1/N, the 2, and the µ, so it's 2µ/N. This is the one. Let me remove it. And now let's look at the final one. Which one of these then fits the right side over here? And it's µ²--this one being the 1/N over here but there's N of µ² and the Σ. So the N<i>µ² inside the Σ divided by N gives us µ² because we moved this guy over here.</i> Rewriting this gives me an expression like this. I'm leaving an exactly one expression over here. Can you find it? This one require some thought. So you have to be really careful. If we look what's missing here, it's 1/NΣXi. Have we known what this is--this is the mean. So what's missing is µ. List that µ and move it over here. Minus 1/NÂ˛ times another expression you find on the right side. And the expression here is this guy. This is the square of µ minus the numerization factor which I put over here. So the final formula gives us the following, =1/NΣXi²-1/N²(ΣXi)². And noticed we didn't use any of these elements over here so let me just take them off. Notice how the calculation of σ² which we're going to rewrite a little bit as follows. This obviously the same. Noticed how this calculation doesn't rely on this term here anymore. It solely uses the terms over here. In particular, we have the ΣXi², we have N. N² is easily calculated from N, and we have ΣXi²--sum them up and when they all summed up, we find the square of the sum of it--so here the square is outside and here the square is inside as we sum the elements. So the counter statistics of ΣXi, ΣXi² and N are sufficient to compute the variance and the single pass through the data, let's just calculate the variance. Here's our formulas again from µ² and σ², and for the data sequence, 3, 4, 5, 6, 7, give me the three statistics that we need to compute it in these boxes here. Although the N is 5, the Σ is 25 and the Σ² is 135, this time we'll not set tracking of the mean as we did before. So it's 9 plus 16 all the way plus 49. We will now plot this into the formulas above. Well, giddy-up. And the answer is the mean is 5 as before. It's 25/5=5. σ² is a bit more difficult to compute. 135/N, which is 5, that happens to 27 - 1/N² which is 25 of this expression X². That's 25²--625 and of course, 25² divided by 25 is 25. The difference between 27 and 25 is 2. It's the same variance we had before, just computed a little bit differently. So, wow, you now understand a lot. You understand the variance which is the spread of the data squared. You understand standard deviation which is the same without the square, and the other way to compute all of those in a single pass to the data using only these running counters. Let's try it profound. You're really deep into statistics now. So, let me ask you two tricky questions. This is a secret, please don't tell anybody, but I'm actually considering, at least hypothetically, to give everybody a raise, and we're lucky to ask is what the effect of the raise is on the mean and standard deviation of the distribution of salaries in the company. I'm considering two types of raises, a fixed amount of $1000 and what's called of relative raise of 20%. Now, that changes the mean and/or the standard deviation to potentially different values they call µ’ and σ’ and the changes either multiplicative, which would be the factor over here, or additive. If there is no multiplicative change, just put one over here. Can you guess what the effect is of adding $1000 to your salary on the mean and the standard deviation? The mean shift upwards by $1000 and we add this to the salary, so we put a $1000 over here and 1 over here because it's the old mean plus $1000. For the standard deviation, that's interesting. Let's look at the variance, which is defined as follows. And now let's add for the new variance, a $1000 to each individual salary and a $1000 to the mean. As you can see, these $1000 cancels them out and you have the formula for the old variance. Hence, the old standard deviation, which means you put 1 over here and 0 over here. Let's do the same for the relative raise we're trying to present and see how that effects the mean and the standard deviation. Please give your best guesses over here. And obviously the new mean is 1.2 times the old mean because we're multiplying everything up by 20% raise that makes a factor of 1.2 over here and zero over here because we don't really add a constant amount of money per person its variable. It's a multiplicative change. For the variance it's more interesting. In our new variance, we multiply each salary by a factor of 1.2 and the same is through for the mean. When we look at this, we can bring the 1.2 outside these brackets and it becomes (1.2)². The reason why I squared this, we can bring it outside Xi and u but there's a square over here and we can move it outside this sum as a factor. Now this is true for the variance but not the standard deviation. The standard deviation is cleared of the variance. The standard deviation was up by factor of 1.2 only, with a constant offset of zero. I finally want to teach you about the concept of a standard score. The basic idea is that for any Gaussian no matter what the mean and the covariance is, you can state how far inner out a point axis, so let me give you an example. Suppose this is a point x and now I will give you a different Gaussian that's much wider with a different mean and the different center of deviation. If I ask you about the right Gaussian with the corresponding point x in the right Gaussian, tell me is it here, here, or here? And the answer you probably picked is this one over here. Even though in total distance this guy is about as far as this guy over here because its Gaussian is wider. This seems to be the more logic of point in relative to the mean and the variance of this Gaussian. This point corresponds pretty much to this point over here. Now that's interesting. That's called the standard score. So given the point x, you subtract the mean and you divide it by the variance and it gets me this standard score. Got it. Let's see. Here's a data set 3, 4, 5, 7, and 6. The mean is 5, the standard deviation is √2. If I now ask you for the number 2 for this specific x, what do you think is the standard score relative to the Gaussian that fits those data points over here. And the answer is -2.12 in approximation. The way we get this is applying this formula here--2-5/√2. And when you punch this into a calculator, this is the number you get. Let's do this again. Now, I gave you 5 estimated data point x. What do you think is z--the standard score. And this was an easy one--the answer is 0. 5 data point minus 5 mean gives 0, divided by anything else, will remain 0. We talked about standard deviation, variance, and standard score. I guess this makes this lesson somewhat standard. Never mind. Let's move to the next unit. So this is an optional unit. I want you to program what you just learned in the previous units, and to me, this is an enormous amount of fun. It's challenging, especially if you don't have programming experience, but it's also the moment where you can really exercise and deeply understand some of the very basic concepts we talked about before. Of course these concepts weren't particularly hard. Really important is this is optional. This is not required for completing the class. This is really just a fun exercise, so feel free to go to the next unit if you don't want to program. So in the first exercise, we will calculate the mean of data. So we'll define what "mean" means. This means define mean, and mean is computed of something. So it's computed of data, and there's this funny notion of return, where you put the mathematic expression for the mean. So when you want to print, say, the mean of this specific data set here-- let's call this "data 1"-- you would say print mean of data 1. Now this is a little bit more complicated than the kind of instructions we did before. I'm actually defining what's called a function, and the reason why I do this is it allows us to test your function with different data examples to make sure it's really correct. But the key thing is you have to return the correct thing over here. I'll give you a hint: In Python, there are special commands. One is called "sum." Sum applies to lists like this one over here. It gives you the sum of all the elements. In this case, sum of data should give you the sum of those numbers over here, which will be 2 if you add them all up. The other convenient function that is part of the programming system of Python that you should just know it exists is called "len," short for length. How long is this thing? This thing here is 5 elements. One, two, three, four, five. So that'll give you 5. So let's dive in. Here's our programming environment with a data sequence. I'm setting up the mean function right over here. You are to return something. This is where you put your code. And then for testing, I just say let's run this function and print up what it returns. That's the syntax. So let me give you an example: Suppose you put a fixed value over in here, like 12 in this case. Now we hit the run button. Then the output would be 12, which is not the correct answer, but you can play with that. If you say return sum of data, which is the command I've just given you, and hit the run button, then for this specific data sequence you get 8 and not 16.0 as the answer. Now the job is yours to plug in the right answer over in here. And of course, the correct answer for the mean is to sum up all the data and divide it by the number of data items. So you use the command sum(data) divided by len(data)--realize that inside the function this thing is called data. Then if we plug in data 1, the specific data sequence, into the function, and we go and hit the run button, we get 54.4 as an answer. That's the correct mean. So if you got this right, you wrote an interesting piece of software already. Now if you have lots of programming experience, this was trivial, but if you're new to programming, it's actually quite remarkable. Slightly more challenging to program is the function median. Now the median of this list is the middle element of the sorted list, which in this case will be 2. Of course, if there is an even number of elements-- let me just add one-- the median isn't exactly defined. So let's say I pick either one of the two center ones. All the examples I'm testing with will have an odd number of elements. Okay? So let's not worry about the case where it is of even length. Let's just make sure our code runs for an odd length. So this one is more complicated to program, and there are two hints. First, there is a function called "sorted." You can give a data, and the output of the function gives you a sorted list. That's built into Python, so we don't have to worry about how to sort things. Luckily not. It will be used to assign this to a new list, and the way you do this, you just give it a name. Say sorted data is data. That gives you a sorted list. The second thing to know is if you want to access any element in a structure like this or in a structure like this, you can do this using notation like this. Now this is the tricky thing. This doesn't give you the third element in the list. It gives you the fourth, and the reason is each list is indexed starting with zero. So this list has five elements, and the indices go from zero to four. So to go to the center element, you would have to use index number two, and that gives you effectively the third element in the list. I apologize for this. This is all over computer science, that the indices tend to run from zero on, not from number one on, whereas in the English language we use one as the first index. Some programming languages use one, some use zero. Python uses zero, so you have to know this. With those hints, you should be able to fill in the gaps, write your code over here, return something, and if you print the median off this data sequence over here, then it should output a 2, which is the median. So here is the code. To solve this, first make a new sorted list of the data and then find the corresponding element in the list to return. Let's assume there is an odd number of items in the list so things should always be fine. So here is my solution. I created a new data structure called sdata, and I get it by sorting it using the sorted command applied to data. So that's Python notation. If you've never programmed before, get the notation by saying we get this thing data, we run it through the sorter, out comes something new, and we assign it to the left side over here--this new thing called sdata. I can just make these things up. And then I ask myself, "What's the right index?" So if sdata is of length 5, the index I want is not number 3, but number 2 because the indexing starts at zero. So say len(data) returns 5. I subtract 1. It gives me 4. And I divide the 4 by 2. That gives me the index number 2, and that works for any data length. So if I had 7, for example, 7 minus 1 is 6. Divided by 2 is 3. It always gives me the middle element. And then we just return with this indexing over here the center element. This return with this command over here, the element of the sorted list that's right in the middle. And that returns the median. So if I hit the run button, I get back 2. Now things should become challenging. I ask you to program the mode of a data set. Of course, data sets can have multiple modes. I just extended mine to have 3 times the element 5. Let's assume for now if you have multiple modes you can return either one; I don't care. But if it's unique, I want the correct mode. So this is entirely nontrivial for programming, and it's a real challenge. This is what makes this class fun. When you hit print mode of this data set then I want it to return 5. Now some hints: There are many different ways to implement it using complex data structures such as sets, but at the minimum for a simple solution, you should know what a for loop is, what an if statement is, and then there's a beautiful function called "data.count." You give it an argument--like this case the number 5-- and it returns to you how often this specific number occurs in the data over here--three times for number 5. If you were to give it 6 as an argument, it doesn't occur at all, the result would now be zero. So my solution has a funny "for" notation. For variable "i" in the range of the length of data. Len(data), we know, is the number of elements. There's 7 over here, so it would give me 7. Range turns this into a list from zero to six of indices into the data, and then the for loop goes to these indices. That's one way to access each element in the data set sequentially. Now the count thing allows me to count how often each element occurs. So if you took my data--took the i'th data item-- then what this thing gives me-- this funny thing it gives me. How often inside this function do I see that specific number that is the i'th number in the original list? I leave it at this. This is not a trivial question. If you get stuck, you might go to the web and read up on for, on count, on if statements. So here is the code, and good luck programming it. My solution, I admit, is not the most elegant, but it's the simplest that I could find without going into more complicated data structures. The key thing is that I'm going to go through all my data items, there's a for loop over here. As I explained before, this gets me the length of the data sequence; this gives me a list of all the indices from zero to six, in this case. We're going to go through these indices one after another in this variable "i." Now comes the tricky part. I pick the i'th data item, and I count how often does this occur in the entire list. For the first data item, 1-- the argument here would be 1-- and the count will give me 1 because there's exactly 1 occurrence of 1. But for the third item-- as we know, "i" will be 2 now, going from zero on-- then this thing over here will give me the number 5. I'm going to hit count. I get back 3 because there's 3 occurrences of 5. Now I need to find the maximum count, and specifically the data item that maximizes the count. For that, I've implemented a variable called "modecnt" that I've set to zero. If my current count exceeds modecnt, then I've found a new winner. So as a new winner, I'm going to set the new data item to the winner--the mode-- and I update modecnt to reflect the fact that this new winner has a higher count in the data set than my initial zero. I iterate this. Out comes the mode. When I run it for this data set, I get 5. If you got this right, then you know a lot about programming. This was really a nontrivial programming quiz. Now things will become even more complicated. I'm going to give you a list of numbers slightly different than the one from before-- those are actually floating numbers. They have a decimal point. And I want you to implement the function variance. It takes our data and returns a single number, which is the variance of the data. So for the data set I will be giving you, it so turns out that the variance is 62.572. Now some hints. First you're going to use the function mean, that you have already programmed, so it's in your code. Just use it. And then the trick that I want to play is: We have our list here inside the function data. We're going to transform this into a new list called "ndata." It is the normalized data, which effectively is the data minus the mean, which I'll call "mu." So you compute the mean, called it "mu," perhaps, and then subtract from the data--from each data item--the mu. And this subtraction is not entirely trivial to make the new data set. The commands I have been using is I iteratively construct with a for loop the data set. First I set it to an empty list. Then there's this function called "append"--"dot-append," and whatever's inside the function--you've got to figure this out-- will be appended to this list over here. So with an initial assignment of an empty list and a for loop, we'll go through all the data items and I'll append the appropriate thing to this new data.I get out this new data list. Then I can apply the same mean function to the new data list, and that gives me the variance. You got it? If not, listen to the video again. So here's the coding environment. Our data set. I've given you the function mean that we've programmed before, and now you are to program the function variance. So if you print variance of data set, you get the desired 62.572884. And here's my solution. First I compute the mean using the mean function. Assign it to a new variable that I make up called mu. So this is now inside the variance; I now know the mean. Here's how I construct my new lists--my ndata lists. I first set it to the empty lists. Then I go through every index in the original data structure. So len(data) as we know gives me the number of elements. Range constructs a list of indices from zero to one minus that. Then I go through it sequentially, and that allows me to index the i'th item in my data structure. Now I take the i'th item. I subtract the mean. This is a very short notation for squaring it. I could've just done the same times itself. But this is my new i'th element that I append into my ndata set. So we're doing this with a for loop. I get a parallel data set called ndata that is data minus the mean, squared. Now I compute the mean of that new ndata and return it; that's my variance. It turns out there's an even more elegant version to do this in Python. It requires a good amount of programming skills that are probably way beyond most beginners' programming skills. Here is how it goes: You can do this in two lines. You can compute the mean, as we've done before with our function mean that we programmed ourselves. And here is the tricky part. We're now going to compute the mean of a new list. We can construct this new list in a single line--that's the key thing here. The way we construct it is we go through all the data items for x in the data and we construct a new list out of it where the math for constructing the new list is x minus mu to the square. So this loops through all the data items, applies this equation to each data item, constructs the new lists, passes these lists on to the mean function, and returns the correct value. So this is one of the compact ways of programming the variance. So as my last exercise for you, I want you to program the standard deviation. I won't say much other than you might want to use the function "sqrt" that returns the square root of its argument. So here's the coding environment. I'm actually importing the function sqrt for you from the math library. Otherwise it doesn't work. Data structure. Mean. Our compact version of variance, and I want you to program standard deviation so that when you hit "stddev" for data2, you get 7.9103 and so on for the data set I'm giving you. So just enter the right thing at standard deviation. And this was an easy one. All we take is the function variance(data) and pass it into the square root and return the result in standard deviation, and this gets us the correct solution. So thank you, and congratulations for getting this far! Some of these programming exercises were somewhat nontrivial, and if you've never programmed before and got them right, I'm really impressed. So this completes this optional unit. Let's move on to the next unit. Let's move on to the next unit. Which of the following people routinely ignore data? Politicians, leaders of cults, statisticians, or none of the above? Check any or all that apply. We know about politicians, we know about certain cult leaders, but did you know about statisticians? Did you know that statisticians routinely ignore data? Here is an example. In your favorite sports club you get a list of the members, and it might look as follows. You are given a name and an age in years. If you look at this, is it prudent--or should you--ignore data when, for example, computing the mean age? And the answer is positive. There seems to be a 211-year-old Tom. Unless you are writing the Guinness Book of Records here of having found the oldest-living person, this is likely just a typo. If asked, what do you think is an appropriate mean? And yes the answer is 22, which is the mean of the remaining numbers. And what's really helping is--is that the real age of each club member. There is a relationship to the age in the database, but sometimes the age in the database gets corrupted. Something as simple as a typo and perhaps there's a certain chance of a typo, may be 10% but then any number in the database can be explained either by the real age or by the typo. If you have a sequence that reads 20, 21, 22, 23, 24 and 211, I would submit given what we know about the real ages the score over here is a result of a typo. And therefore we should not consider it in the calculation of the mean. The easiest way to ignore outliers is called quartiles or their closely related cousin, percentiles. Suppose we have a data set the following form and here the data is shown in order--there's exactly 11 items. Then quartiles partition this data into four regions and look carefully, there's gaps in between. The element in the center that you've encountered--you might remember what is the mean, median, or the mode--please check one of those. And of course, this was the median. These two elements over here are called the lower quartile and guess what--the upper quartile. And in fact this range in between the upper quartile minus the lower one is called the interquartile range. So this range is the data we used to calculate things such as the mean and data outside this range are the north. This is a simple but often very effective outlier removal technique that gives results extremely that often are attributed to things other than what you're trying to understand. Obviously this works well because there's exactly 11 items in our data. If there were only 10 items, you would've to shift this element around and you'll break a little bit of the symmetry but that's not a big deal because most of the results are very large. Now assuming it works for 11, give me the next number up that also works. And this will be a 15 from reads 19, 23. The formula is we have to four quartiles, 4 times n plus three separating elements-- the lower quartile, the median and the upper quartile. Any number that satisfies this formula over here has an exact definition of quartiles and anything else, you just pick something in between. It really shouldn't matter much in most cases. Here's our age distribution again and they come at certain frequencies. Two of the people in the database are 19 years old, one is 20, one is 21, three at 22 and so on--there's 11 individuals. When I ask you the obvious question now--give me the lower quartile, the median and the upper quartile in these three boxes over here. And this is easiest seen by providing all the extra data. Let me do this here, there's two of age 19, 3 times the age 22, two aged 23, 24 and 25 and now we can call this at exactly 11. This is where you'll find the lower quartile, the median and the upper quartile. That's 20, 22 and 23. Now assuming you'll use the method I just told you about, and you compute the mean after outlier removal, what do you get? Obviously, these are the numbers with which to compute the mean. With my calculator, I get the total sum of 153 divided by 7 gives me 21.85. Here it is. Come in. I'm making a class. Very nice. All right, now you know the answer, Tom. 1489. Okay. So what's the mean of those numbers if you just say all these numbers? With me just now is Tom Mitchell professor at Carnegie Mellon University and Tom says a 1000. What do you say? The answer is about 300--290.6. I actually cheated. I used my calculator over here. What's interesting from now is I applied the quartile removal method. After cleaning up and removing the points outside the quartile range, what do you think the new mean will be? Professor Mitchell, what do you think? You throw away the extremes, the mean is 20. Okay, so you throw away the extremes, you arranged all points as before minus 99, 13, 17, 33, 1489, we find that these are the lower quartile and upper quartile, this happens to be the median--these are the numbers that are average--13, 17 and 33. You have one more chance Tom to revise your estimate. Okay. I'll go with 21. 21! You did it. Professor Mitchell from Carnegie Mellon University with us today computed the quartile of this data sequence. Thanks so much! Thank you. Thank you. Let's talk about percentile. Percentile is kind of the same thing. The kth percentile is so to speak k% through the data. There's many ways to split. The one I'd adopt is, let's say, for the 10th percentile--this is 10% and this is 90%. The kth we applied on one side could be applied on both sides. For our age example, we removed the upper 20 percentile, we removed or exit data items here. Check the ones that we removed. The answer is this guy. This data item is exactly 20% of the data on the upper range. Statisticians ignore data, and it is with pride. So, do statisticians lie? Yes, no, or maybe? Well, that's kind of a fun question I didn't mean seriously. Based on what I've shown you here, I wouldn't call this a lie. But whether I can vouch that no statistician ever lied? Of course not. I'm sure some of them do. Let's move onto the next unit. So we talked about coin flips, and we flipped some coins. Now, I want to flip many coins including this 2-dollar coin whose country of origin I just don't remember. Perhaps you can post on the forum where this 2-dollar coin might be from. So let's ask an easy quiz. Suppose we do 2 coin flips. I would like to know how many outcomes of these 2 coin flips are there and which number of heads equals the number of tails which in this example means you would have head exactly once and tails exactly once. Give me that number. And the answer is 2. If you look at the truth table— head-head, head-tail, tail-head, and tail-tail—these are the four possible outcomes. Those two outcomes over here yield an equal number of heads and tails. Let's now go to 4 coins and ask the same question. In going through the truth table, there's a bit more info. So let's find the one where the number of heads and tails are the same. Those three on the left side and those three on the right side for a total of 6. Now. Let's go to 5 coin flips. How many outcomes have the same number of heads and tails? This is a trick question. And the answer is 0. With an odd number of coin flips, one has to be more than the other. There's no other way. Okay. I tricked you a little bit. With 5 coin flips, how many outcomes will have exactly 1 heads, hence 4 tails. The answer is 5. There's 5 different ways in these 5 outcomes. To place heads--could be first, second, third, fourth or fifth. So these are 5 different ways. Let's now make you think really hard. In 5 coin flips, how many outcomes will you have 2 heads. This is a serious and non-trivial question. And the answer is 10 and this is a non-trivial answer. So you could go and place the first heads anywhere in these five elements--say here-- and there's five different ways to place heads. You can now place the second heads among the remaining four--for example, you could place it over here and it gives you a factor of four different ways of placing the second heads. But again you do this, you over count--you over count exactly by a factor of 2 in the business. You place the first heads over here, the second over here, but if it was chosen to place the first head on the right side and the second head over here on the left side, and the outcome would've been exactly the same, so you counted the one twice which means we have to divide it by 2--5 times 4/2 is 10 Let's now go to 3 heads if you think the result is no. And the log is 10 again. There's two proofs. One is I can just flip heads into heads. So three heads means two tails. I can give the exact same game as before where I placed tails as opposed to heads and it gives me the same equation as before, but let's do it the new way, three heads. I can place 543--the first heads, the second and the third.** For the first, I have five positions, for the second--four, and for the third--three are left. This gives me the common networks for those heads, but now I'm over counting. How much am I over counting? Well, suppose I'm committed to put the three heads into the three slots over here and that's not given. And I just wonder in which order I've put them in, so I might put the first one here, the first one here, the first one here. Then for the first one placed in here, there's now three different ways of placing it. For the second one, there's two different ways of placing it. For the third one, it's not deterministic--there's just one slot left. So I over count this by a factor of 6--there are 6 different ways of placing these three heads into these three slots, so the result is 543/321 producing the 5*2=10.* And that is insightful. Let's say I have 10 coins. You just get 10 of these shiny silver coins here. So we got 10 coins. And I got about 4 heads in these 10 coins, and just apply the same logic to find an answer. The first time I'll do it for you. I can place those heads in 10<i>9<i>8<i>7 different slots if they were counted.</i></i></i> Now that I'm committed to having chosen these slots, the implementations of those are 4<i>3<i>2<i>1.</i></i></i> That gives me 5,040/24, also known as 210, so that's 210 outcomes out of 2 to 10th outcomes which is 1024 in which exactly 4 heads are observed and 5 tails. Now it's your turn. 5 heads out of 10 coins. One of them coming up to heads. I was given 10<i>9<i>8<i>7<i>6 which is 30,240 whereas in picking those five heads</i></i></i></i> and we're over counting by 5<i>4<i>3<i>2<i>1 which is 120 and this gives us as a quotient 252.</i></i></i></i> So let me give you the mathematical formula that you might be familiar with. n! for any number n is the same as n<i>(n-1)<i>(n-2) all the way to times 1.</i></i> Let's call it factorial. So 10 factorial for example will be 10<i>9<i>8<i>7<i>6 and so on.</i></i></i></i> If you look at this equation over here, I'll give you a couple of choices how to write it. It could be n!; it could be n!/k! where this over here is k which is 5 and this over here is n. It could be n!/k!<i>k! or it could be n!/k!<i>(n-k)!</i></i> These are four choices. One of this is actually correct for the formula that we've computed before. Pick the correct one. And this one is the correct one and I have to admit I must like you a little bit by taking a symmetric example where k is exactly half of n. The key observation is that this thing over here is n!/(n-k)! and to see this let's go back to the case where we had 4 heads. In the 4 heads case, we multiplied all the way to times 5 over here. We only multiplied all the way to 7. These are the 4 heads that we placed. So n! goes from 10 all the way to 1. (n-k)! goes from 10-4 and that's 6 all the way to 1. So this blue expression over here will go from 10 x 9 x 8 and so on all the way to 1 over now n-k, 10-4 is 6. 6<i>5<i>4 and so on.</i></i> When you look at this, the 6<i>5<i>4 occurs on top as well.</i></i> So we can cut those out and what remains is 10<i>9<i>8<i>7. This one over here.</i></i></i> Now once we place these 4 heads, we have to divide by a 4 factorial which is different ways of placing those 4 points into the predefined bins. So put differently, this is k! so if we put all this together you get n!/(n-k)!<i>k!</i> This is the expression over here. Let's practice this one last time. Say you have 125 coins and you ask how many ways exists in which 3 coins come up heads. What is the resulting outcome? Be careful when you use your calculator. It might result in an overflow, but the answer is easy to compute. So, I get 317,750, and the logic is I just plug in these numbers, 125!/122! 3!. When I evaluate this, I find that these guys can be easily reduced to 125<i>124<i>123.</i></i> From 122 on, these factors exactly cancel. 3 expands to 3<i>2<i>1 also known as 6.</i></i> When you divide these two things, you get 317,750. And let's really ask about probabilities. Let's say for now, we have a fair coin with a probability of heads is 0.5. If I flip a coin five times, what's the probability the number of heads is exactly 1. You should be able to compute this. Now, we know from our previous consideration that there are five ways in which the number of heads could be one. 5!/4! 1! happens to be 5. We also know that there are 32 possible outcomes. It is 2⁵=32 outcomes. This is the size of a truth table. So, 5 out of 32 outcomes has exactly 1 head. So, I would suggest that the answer is 5/32, and my calculator tells me this is 0.15625. So, that is actually interesting. If you flip a coin five times, there's a chance that it only comes at head exactly once and that is the probability 0.15625. Let's now modify it and ask, what are the chances it becomes a head three times? What's the probability for that to happen? And the answer is 5!/5-3 is 2!/3! and that gives us 10. 10/32 gives us the probability of 0.3125. Now, I'm going to make it really difficult. I'm going to give you a coin--let's call it loaded. So, the probability for heads will now be 0.8 and therefore the probability for tails is 0.2. To make it easier, assume only a 3 coin flips and ask the probability of heads coming up exactly once. What is that probability for the loaded coin without giving you a head. I recommend answering it using a truth table. So here is my truth table of these eight different outcomes. The ones that has head exactly once are this one, this one, and this one, but they're not all equally likely. Heads, heads, heads is much more likely than say tails, tails, tails because heads has a probability of 0.8. This one here has an outcome probability of 0.8³ while this one has an outcome probability of 0.2³. All of the green once have the same outcome probability. They all have exactly one head 0.8 and two tails. So as before, we took the probability to be one of these has the truth table. This time each of those has a probability of 0.032. That's each amount of the three. Now, we have to consider all of these three outcomes, which means you're going to add 0.032 for each one of those three guys over here and this gives me as an answer 0.096. That's a nontrivial question. Let's now say for the same loaded coin, we flip that coin five times with that same load of probability over here and the key about the out of five times heads comes exactly four times. Now, I want you to compute this probability over here, It's a completely nontrivial question. If it's too hard, hit the next button. So interestingly enough, we can do the trick as before 5!/4!<i>1!,</i> which is the number of outcomes at exactly 4 heads and we know that's 5. Four heads means one tails. There's five ways took place and one tails and this one over here. The question now what's the probability of those? Well, they have heads four times, (0.8)⁴ and tail once to (0.2)¹. So we do is we multiply the total number of outcomes that have this property with each of probability, which happens to be the same because we get exactly four times heads and 1 times tail and multiplying these things together gives us 0.4096. This over here is indeed 0.4096 and the 5<i>0.2 cancels the others out,</i> so that's the result in this specific case. The cancellation doesn't always happen. It's a rare circumstance. So, this does not go and get the same for 3 heads. I leave this here, but obviously, the numbers aren't correct anymore. And here is my answer 5!/3!*2! is 10, and we have 3 heads, so we put 3 in here and 2 tails, so we put 2 in here. Putting these all together gives us 0.2048, which is half the probability of the previous question. Now, you're ready for the real challenge. You flip the coin 12 times in the care about how likely it is to get heads 9 times out of the 12. This is not a trivia question, but you should be able to get it right. And the approximate answer is 0.236. That's the probability of exactly 9 heads out of 12 coin flips for this heavily loaded coin that mostly gives you heads. And again, the answer is 12!/12-9. This is 3! 9!. Then we have to compute the probabilities being (0.8)⁹ and 1-0.8 is 0.2³ and that is the number over here. What have you learned? Well, to flip a coin n times one for k small or equal to n. We ask the probability--what are the chance it comes up heads k times. For any coin, with the probability of heads equals to all caps P,. we now get the following formula: n!/(n-k)!<i>k!.</i> These are the total number of outcomes that have this property. And then this one has the following probability: P to the k, this was the (0.8)⁹ before times (1-p) to the n-k, which is the remaining 3 over here in this example. So, this formula is the probability of what's call the binomial distribution and really was this is the accumulated outcome of many identical coin flips, and it leads us beautifully to our next lesson when we talk about very large experiments and the normal distribution. What you should have learned and understand now is you can take very large experiments with large numbers of coin flips and compute the probability that heads comes a certain number of times using the formula that you should now fully and wholly understand. You had asked why did I drag you through all of these binomial distribution stuff and flipped so many coins, the reason is you're going to move now towards what's perhaps the most deep insight in all of statistics. It's called the central limit theorem. And the way I want you to get there is through a programming exercise. Now, I told you that all the programming is optional and you can totally skip this one but I beg you to stay with me. What you're about to see is perhaps the most interesting way to understand the central limit theorem and statistics of large numbers. In assignment #1, I literally want you to flip a coin 1000 times. And once you've done this, I want you to compute the mean of the outcome and the standard deviation. Flipping a coin is a random event. It gives us things like 0 or 1s as outcomes. Like this thing over here. If we were to do this 1000 for a fair coin, you expect the outcome of the mean to be 0.5. You probably have no clue what to expect for the standard deviation. There's a couple of things I want to give you. In your programming environment, you'll find the function mean, the function variance, and the function standard deviation as you've practiced it before. And what if I did a little bit to make sure that whatever you do is of type float what you need for computing the mean. Otherwise, it might be of type integer and then these calculations all go wrong. Same over here in the computation of the variance but ignore the float type conversion. Other than that, it's exactly what I've shown you before. I now want you to implement the function flip that takes this as an argument the number of coin flips you want to do, 1000 in this case, and then use as a function mean and then standard deviation to compute the mean and the standard deviation of the resulting sequence of outcomes. This will be a list filled with 0s or 1s. The thing to know is that with the function random.random, there's two of them with a dot in the middle, gives you a random value that sits between 0 and 1. Every time you call this function, you get a different random value, which is nice because you just have to call this 1000 times to get you 1000 samples. But then in the interval of 0 and 1 and you want to put them back into coin flips, so what you have to do is to call this expression here. And this expression over here will give you true or false, which is the same for the purpose here of 1 and 0 . It gives you true if the random value happens to be larger than 0.5 and false if it's smaller. So the assignment is to call this thing 1000 times and make a list of these 1000 outcomes and put that code in and the function flip that returns that list and then you're done. So here's a typical outcome for this code. If I run it, the mean might be 0.484. There's the standard deviation. If I run it again, I get a different mean of 0.51 and a different standard deviation And here's my answer. It's a one-liner. A bid in an array of 1000 things, and this is the beauty of Python. There's ways to make it more complicated as it is before in the variance case. Now this is a little bit more compact. So I ran the test random.random larger than 0.5. And this thing over here gives me the true or false and I want to do this 1000 times. And doing this 1000 times invokes this command for x in range (N) where N is 1000 and range N becomes a list of 0 to 999. This will go 1000 times of different x's. The x's that we've used here because the random coin flip doesn't understand what the order of the coin flip is. They're the same every single time but this just means I ran this procedure over here 1000 times, collected the results in the bracketed list, and returned it. Specifically, if we were to print out f and hit the run button then what I get is the stuff down here. A list of 1000 items of false, true, and false. It makes for a beautiful wallpaper, doesn't it. Now with this in place, now here comes the really interesting question. It’s assignment number two. And again it’s a programming assignment – free to skip. Now that we have a function of flip, it gives me this list of a thousand outcomes from which I cannot derive things like the mean. Run this thing itself a thousand times and each time you get a different mean, so this means zero, mean one and so on all the way to a mean nine, nine,nine. And these means are continuous values obviously, between zero and one and give you the same function as before, mean, variance, standard deviation, and flip and as I scroll down, I find this function sample, I want you to put in code over here so that when I sample with the same n, I run the flip experiment a thousand times and every single time I compute the mean and now I assemble a list of all the means into this thing called outcomes. The means will be continuous, I can do a history plot, it’ll be better with many bins, so this notation over here gives me 30 bins. And to give you a feel for what to expect, this is a typical histogram I get out as a result. It’s really beautiful. If I increase n to 2000, I get this histogram over here. Apologize some numbers are a little illegible over here but the center of it is 0.5 and it falls off to smaller number to the left, to the right. You can think of it as a distribution over the means outcomes of large numbers of coin flips and has an interesting shape. So go ahead and program it and see if you can reproduce these results. And my answer is quite simple--again, it runs my experiment a 1000 times using the for x in range it summons a new list and this on the list is the mean of the list produced by flip. Flip itself every single time I run it will give me a 1000 0s or 1s or truths or false and using the function mean, I compute the mean of that. But now I have an outer loop where I do this calculation of the mean a 1000 times. It summed them into a new list and that's my new list and that list is continuous valued. These are printed out by saying print outcomes after generating it. What I see is a list of 1000 numbers--it all have around some by 0.5, some of it is 0.046, some is 0.515. These are the empirical means for these sequences of a 1000 coin flips. See how it flip effectively 1,000,000 coins here, and this is the corresponding histogram plotting the frequency of the coins. [#000000] The thing that is suspicious is that in this binomial distribution, it seems that the frequency of outcomes is centered around the expected outcome of 0.5 that falls off according to a funny looking curve--often called a bell curve and the reason is this could be the world's largest church bell. The significance of this bell curves in the relationship to what's called the central limit theorem will be discussed in the next unit. So today, you’re in for a treat. You’re going to learn a theorem and I won't prove anything to you, you learn it by doing but this theorem is really important for all of statistics. So let’s dive in. And what this theorem really is about many coin flips. Almost generally it’s about the distribution of the sum of many things. So let’s study many things. So let’s look at a coin. It has outcome 0-1 and we’ve already learned that the probability of the sum equals exactly K for N coin flips; ; looks like this cryptic formula over here: N factorial divided by N minus K factorial and K factorial. And then we had some other term here, if the coin is fair, it is like 2 to the minus N. Right now, I’m just going to care about these coins over here. I won't care about the probability for the time being. I want you to help me construct a table for these counts as N gets larger. So we start with N equals 1, go all the way to N equals 4, obviously outcomes range from zero all the way to 4 at 4 coin flips. So here’s our table. It’s also quite obvious that these guys over here can never occur: if you flip the coin only once, the outcome can’t be 2. So we’re going to fill out this table over here together for the coin over here. Tell me, for N equals 1, what are the two values that go right over here, just those two values. And the answer is 1 and 1, N factorial over N minus K factorial, K factorial - they are all ones, just 1, 1, 1. That was easy. Let’s go to equal 2 and ask for the sum, what are these coefficients for an equal 2? And the answer is 1, 211. Let’s plug-in zero. 2 factorial is 2 divided by 2 factorial gives us 1 and zero factorial is 1. So that’s 1 over here. But if you plug-in 1SK we get 2 factorial over here which is 2 divided by and another 1 stays 2. So this is the correct answer. Let’s go N equals 3. Now, it gets interesting. The correct answer for this formula is 1, 3, 3, 1 and you plug them in, you can see it’s correct. This is actually called a Pascal Triangle. If you have never seen this before, you can check it on Wikipedia. It’s actually quite easily calculated. Each element over here is the sum of the value above plus the value on the left. This guy over here, sum of the value above, be zero plus value on the left. So let’s fill in the same for N equals 4. The answer is 1, 4, 6, 4, 1. I used the Pascal trick but when you plug it in, you get exactly the same number over here. So let’s look at these distributions over here. They are really interesting. In the first case, you get something like this: just the counts . In the second case, it looks more like this. In the third case and in the fourth case, we get something that looks really interesting, like this form over here. So here is a question for you. What happens when N equals 10,000? When I draw it like a diagram like this, will it look like that, like that, like that? Check the one that looks most plausible. And it so turns out it looks like this over here. You can see it over here. It looks pretty much like a function that’s very high in the center and flattens out to the sides. So how does this relate to anything of interest? So let’s look at a more complicated case. Say we have a three-sided die and it can come up with the outcomes 0, 1 or 2. And again I just want to count, I don’t want to work in probabilities just like before, I’m going to help you a little bit. First, I’d like to know what’s the possible largest sum, the largest number I put over here for anything that’s four tosses if each outcome can be between 0, 1 and 2. Just write in the box over here. And if that outcome is eight, if I toss it four times, the largest value is two every single time. Two times four is eight. So, let’s now go all the way to eight and compute those numbers, and it just the counts, not the probabilities. Give me the first three numbers right over here. When we throw this die exactly once, what do you get is one, one and one. Now the tricky part starts at n equals two and I will construct this for you, you can take over for three and four. For the one of the left to have the sum equal to one, we stay at one, which is we add up the value above and the two on the left because there’s no three outcomes. This guy over here is two, it’s the one above and the two in the left, there’s nothing over here but there is something over here. This guy here is three, it’s a sum over the one above and the two on the left gives us three. We go down to two, which is a one above zero and the two on the left, all the way back to one. And this already looks very much like the type distribution we talked about like this. Now, it’s your turn. Give me all those values over here. There’s six of them and just to help you a little I give you one of them. This over here is a six. Please fill them in. And we would do just like before, we take the value from above, plus the two left ones. So one, two plus one is three, three plus two plus one is six, two, three and two with seven, six, three and one. So now for the final question, there’s nine total values to be filled in, I give you two of them, four over here and there is a sixteen over here, so please fill in the seven remaining values. And as before we take the one above plus the two left, one here, these two are four, six, three, one is ten, six, seven, three is sixteen, six, seven, six is nineteen, sixteen, back to ten, four and one. And when we graph this, it looks about like this. It’s a big function that has the highest value at the center, nineteen and then falls down in some interesting way that looks awfully much like the way the previous exercise fell down. So even for this different experiment with the die, we get about the same phenomenon that the sum of those seems to have a distinct high probability for what’s the most likely outcome four and then it falls off in some interesting way. So this is effectively what’s called the central limit theorem. And this sounds complicated and in fact it is, but here is the simple version to remember. Suppose you’re doing many experiments. Each of these experiments over here have some distribution, we don’t care, it has to just be reasonable and you sum those all up for some very large number and if N is large then the joint distribution of the sum of those with approach a function it looks like this. That’s called a Gaussian. We discuss the Gaussian in depth. It has its own formula and typically if N is as big as thirty that is maybe your test thirty people whether they have cancer or not, that’s usually good enough to approximate this really complicated binomial over here with this relatively simple Gaussian function. We’ll learn more about this later, but the key thing to remember is we take many experiments, you add them up and outcomes a function just like this. That’s called the central limit theorem. Welcome to my online class on statistics. The basics of statitics is that the world is full of data, and we the people have to make decisions. Statistics comes to our rescue. It takes data and turns it into information that we the people can use to make decisions. Whether you are in social sciences, medicine, engineering, public policy, psychology, climatology, robotics, even archaeology, health sciences, finance, business and marketing, or pretty much any other discipline that you can study. All of those are now driven by data, including unlikely fields like biology or physics and so many others. Statistics is an amazing discipline to know. It is universal, useful, and fun, as I hope you're going to see in the class I'm just about to teach. One of the standard problems that people study in statistics has to do with purchasing decisions. Suppose you wish to buy a house. There are small houses and big houses, but you really like this one special house build by a famous designer. This house has a certain price. Say in US dollars it's $92,000.00. The question you'd like to ask yourself--is this okay? Is it too much--you should pay less--or too little? Let's go an find out. In statistics, the way we find out is by looking at data. Let's assume there is a database of previous house sales of homes in the same neighborhood. Just for simplicity, let's assume we know about two things-- the size of the home and the cost at which it was sold. There is a house with 1400 square feet that sold at $112,000, a much larger one with 2400 square feet sold for $192,000, and so on for an entire number of other houses. Now it's a statistics problem. You have past data, and here is your very first quiz. Say the house you wish to purchase has 1300 square feet in size. How much money should you expect to pay? Well, in our very first quiz we're just going to look it up. It turns out there was other sold at the same size, and it brought in $104,000. In the interest of statistics, the answer is $104,000. This is not the game theory class. Obviously, you wouldn't want to bid that much, but in the interest of statistics, that's what you'd expect to pay. Same question now with 1800 square feet. And yes, the answer is $144,000. A more tricky question--what about if the house you're trying to purchase has 2100 square feet? That one is tricky. I'm going to answer it the following way-- 21 is just halfway between 1800 and 2400. If you take halfway between $144,000 and $192,000, we get the mean of 144 and 192 thousand, and that is $168,000. Now, that isn't always correct. I assume that this data has certain properties, which I'll talk about later, but let's move on and assume we can use the trick of finding prices just in between existing prices to price other sizes of houses like 1500 square feet. Please put your answer right here. Yes the answer is $120. By our logic, 1500 lies between 1400 and 1800. In fact, it's a quarter away from 1400. We'd say it's $112,000 plus 1/4 of the way from 1400 to 1800. That's a difference of the price of an 1800 square foot home and a 1400 square foot home, adding a quarter of this gets us from $112,000 to just $120,000. I guess by now you've probably figured it out, but let me ask you just to make sure you understand the logic behind this specific data set. What is the cost of the home per square foot? Here's my square foot, and please answer in the box over here. The answer is $80 per square foot, and we get this by just dividing $112,000 by 1400. It turns out that this data set has this amazing property that the cost per square foot is constant. That allows us to interpolate the way we just did. In statistics that's often not the case, but I want to congratulate you. You did your very first unit of statistics. Congratulations. You completed Unit 1. But as we go forward, we're going to look into data where the cost might not just be a constant factor times the size of a home. See you in the next unit. What you are about to see is one of the most transformative parts of modern statistics and it uses things if like we've never seen before. We start out with the binomial distribution that you're familiar with from our last unit and then we move into the central limit theorem which basically means we've taken a number of coin flips to infinity. From that, we arrived with the normal distribution which is basis to so much in statistics-- all of testing and confidence in results are defined though the normal distribution. And the reason why this matters is much of what we've done in coin flips had one or two coin flips but in statistics experiments, you often have 1000 of patients or 1000 of data points and then starting the normal distribution as an approximation to the binomial distribution is much more practical. So let's start--so when we start with our now well established formula for binomial distributions where N is the number of coin flips, k is how often it comes up with heads and p is the probability that the coin comes up with heads, usually 0.5. And I want to graph for fix n this function here. K can go from 0 all the way to 20. Instead of letting you compute this thing over here for all those different values of k, I'm going to ask a different question--suppose you have a fill coin, what do you think this value takes on its maximum value, which value of k maximizes our expression. I know you can't really know this but with some thought, I believe you'll arrive with the correct answer. I just programmed and ran the experiment, and the answer is 10. The reason why the answer is 10 is because the number of combinations to place 10 positives and 10 negatives into our list of 20 is larger than any other number. This term over here is maximized when k is exactly half of N--so 10. The other interesting thing is things fall down in the interesting fashion as you deviate from 10, 11, 12, 13, 14 all the way to 0 or 20. Obviously we got a curve that looks a bit like this. This curves is often called a bell curve because it's quite feasible to think of it as a church bell-- that's move left and right and rings the bells. I did a related experiment--actually I bought a piece of software to flip a coin 1000 times and if you did the last optional units on programming, you wrote a piece of software to flip a coin a 1000 times and from that, I looked at the empirical frequency which is the same as that count of heads divided by a 1000, but this one scales between 0 and 1. I called this thing an experiment--I flip the coin a 1000 times, out comes a one singular number which is the ratio of heads to the total number of experiments. It should be 0.5 in the ideal case but often it's a little bit off 0.5. I repeated this experiment 1000 times and that means I've got 1000 samples of this ratio over here. When I do this, I got a whole bunch of means. When I run a histogram over those, I might see a curve. What shape do you think the curve has--is it going to be like this, is it going to be like this, like this or like this, all are focused on 0.5. Which one do you think is it? And it's this one. Let me show you. Here's a typical one, and I apologize the axis over here can't really be read, but you can take with faith that the center is 0.5, and you can see the characteristic bell curve for this simple coin-flipping experiment. For this run the mean was 0.50006. If I run it again, I get a different sample. And there is some randomness involved as this bar over here illustrates, but over the randomness you can clearly see the bell-shaped curve that flattens off on the sides. The question really is can we find a better formula for this bell-shaped curve. The answer is, well, take your guess. The answer is a resounding yes. Even more so, what I'm going to show you doesn't just apply to binomial distributions with fair coins. It applies to almost any distribution that is sampled many, many times, which is a very deep statistical result. I will construct for you the formula that is being used. I will define for you a normal distribution with a specific mean that's often called µ, Greek letter µ, and a variance that's often called σ². We already know that variance is a quadratic expression. In normal land we often use µ and σ². Let's do this. The very first element is that for any outcome x we write the quadratic difference between this outcome x and µ. This is indeed a function in x. So, look at this. Here are four possible hypotheses of what this function might look like. Each case is µ is on the right side some where. The horizontal axis is x, and we're graphing f(x). The first I'll give you is a triangular function. The second is a quadratic function. The third one is a negative quadratic function. And the fourth one is a quadratic function that doesn't quite touch µ. So, which one in your opinion best describes this formula over here? I would submit that it's this one over here. The reason is this expression is 0 when x = µ. As a result, it can't be the fourth of these choices. It's strictly non-negative, so it can't go down into the negative area, so this one is being out ruled. It's quadratic. Hence, a function like this doesn't make sense, so it must be this one over here. The next thing I'll do is I'll divide it by σ². Without telling you why I'm doing this, I want to see what the effect is. Suppose σ² = 4. That means we have a variance of 4 and a standard deviation of 2. I've given you already the quadratic function when it isn't divided by σ². It's the same as saying σ² = 1. What I'd like to know is whether our new version where σ² = 4 makes this quadratic wider or whether it makes it narrower, assuming that this is our new function f(x). So, pick one of the two, or perhaps it stays the same. Then pick the third. The answer is it makes it wider. To see why is this affects the vertical dimension--the output and scales it down by a factor of 4. Whatever we said before gets dragged down by a factor of 4. That means this point over here finds itself here. This one over here finds itself here. That means we'll widen out the quadratic. Observe that large variances yield wide quadratics. Small or tight variances yield sharp quadratics. In particular, if we now look at the quadratic over here, which is much tighter, which of the following potential σ² would you this is best representative of this narrow function over here, provided that this is the quadratic that corresponds to σ² = 1. Check one of those four--4, 1, ¼, and 0. It follows it's got to be something like a quarter. We already learned that 4 widens the quadratic, so that can't be it. One is already shown over here. Zero makes no sense because we have division by 0 which you can think of as a quadratic that shoots up almost like a straight line, but honestly it doesn't make any sense. A quarter is the one that really describes this particular quadratic the best. Now, that's great. Now we understand this expression over here. Let's go further, and let's now take this function and multiply it by -½. Again, I ask you what the affect it. If this is your original quadratic, then what do we get? We already know that it's going to flatten it, because you are dividing the f value by half, but are we going to get something like this or perhaps something like this? Pick one of those two choices. And quite interestingly, we inverted the sign, so all of a sudden the function is negative. Quite obviously, green is the correct answer. So, now we have a quadratic that points down into the negative space whose maximum value is 0 and otherwise, it's strictly negative. That's this function f over here. And now I'm going to take the most extreme of all steps. I'm going to make this the exponent of the e function. Remember, the inner argument is a quadratic that points down. This a bit does depend on σ. This mean is µ so I call this f(x) where f(x) maximize. And I'll give you several choices for x=µ, x=0, x=-infinity, or x=+infinity. Where will this thing be the largest? to understand the solution, it's useful to draw the exponential function. e⁰ is 1 and then it goes up exponentially to really large numbers. If you've ever heard Ray Kurzweil talk about the future of society, you've seen these curves--everything goes up exponentially. Everything is just exponential. And further, if you go back in time to negative values, this thing slowly drifts down to 0. Of course, that's not very exciting, so we never talk about exponential in the negative space. However, it turns out that all the arguments of the exponential are at best 0 and otherwise are negative, because the exponential is monotonic-- that is the larger its argument, the larger its exponential value. It ought to be optimized where this thing over here is the largest, and where is that the case? Well, it's exactly where µ hits 0. Let me ask you another question. What is the value of this function if we go to the point where it's maximum, which is x = µ? That's the way to write this. Compute for me in your head this what this thing will be when x = µ. Quite interestingly, even though this formula looks complex, it a really easy answer, which is when x = µ this thing here is 0. That makes the entire thing 0. e⁰--any value to the 0--is going to be 1. Next I'd like to know where is f(x) minimized? For what value of x would we get the possible smallest value of this entire expression over here? Again, pick one or more of those choices over here. The answer is now ± ∞. If you look at this, if you put a really large positive or negative value in, the difference to any µ will be enormous. The square will be even more enormous. Therefore, this entire expression on the right side will be huge. Put a minus sign in front of it, and you have a hugely negative number. You have e^-∞. e^-∞ drive the e curve all the way to the left where this just ends up to be minimized. In fact, what do you think is the value of this where x = âˆž? The answer is 0. As x goes to infinity, this expression goes to negative infinity. The exponential of negative infinity converges to 0. So, now we've got basically the normal distribution function. We have a function f that assumes the value 1 when x = µ that goes to 0 when x goes to ±∞. It so happens that it looks like a bell curve. The fact that it looks like a bell curve is not entirely obvious, but you have to take my word for it. This--what I would consider a relatively simple formula-- describes the limit of making infinitely many coin flips. In fact, it describes the limit of computing a mean over any set of experiments. This is a very powerful result. No matter what you do when you drive n to very large numbers you get a bell curve like this. There is one flaw here, and I'll tell you about the flaw without going into detail. That is the area underneath this curve doesn't always add up to 1. In fact, without proof, it adds up to √2πσ². The reason why this matters is deeply buried in probability theory. But it turns out we want all these areas to add up to 1 just as much as we wanted a coin flip and its complement to add up to 1. The true normal distribution is normalized by just the inverse of this thing over here--1/√2πσ². So, that is the normal distribution of any value x indexed by the parameter µ and σ². So, this is a very deep piece of mathematics. Now, we will apply it a little bit for you to practice how the normal distribution looks in the field. So, here is our normal distribution again. I'm going to write it as "exp" for exponential {-½ (x - µ)²/σ²}. The truth is when you're new to this this looks really cryptic. When you're with statistics for many years as I have been, you wake up in the middle of the night and you can recite this formula. It's as normal as getting breakfast in the morning or having a beer after dinner. What I want to get into your brains is not the complexity of the formula. I want you to really understand how this formula is constructed. I you to understand the quadratic penalty term of deviations from the expectation of the mean of this expression. Then the exponential that squeezes it back into the curves. That's basically what it is. We can draw values from this normal distribution just the same way as we flipped coins before. The way to look at this is any value x has this probability up here. This is nothing else but a notation of the probability of x for a normal distribution with µ and σ². So, a value x that has twice the high bar than some other value x' will have twice as much of a probability of being drawn. Now, obviously, the normal has an entire continuous space of outcomes. And obviously that renders each individual outcome of probability 0. But, in essence, you can think of the height of this thing over here as being proportional to the probability that this value is being drawn. So let's look for a second at different ways to compute probabilities for coin flips. You have a coin that has probability P of coming up heads. I will give you now 3 very different formulas. Here's a single probability, here's our common [UNKNOWN] formula and here's our Gaussian exponential. So what I want to know from you is which one of these formulas is best suited to compute a probability for a single coin flip, a few coin flips, or many coin flips, possibly even infinity coin flips as a limit value? So click exactly three of those buttons, one each row and one each column for the best correspondence of the formula to the case on the left As I'm sure you've guessed this is the probability for a single coin flip. This is the formula we can use for multiple coin flips and as we go to very large numbers, the [UNKNOWN] is often a good approximation for the outcome of many coin flips. I should warn you, if the coin flip is zero to one, then the mean is always in the positive. This can assume negative values, but it's an approximation What I've shown you in the beginning of class have from a coin flip to a binomial distribution all the way to a normal distribution, and you might think that this was challenging and indeed it was. As it turns out, you can treat all this things about the same. In fact, if you're a medical doctor and you have one patient, you might think of it as a coin flip. If you have 10 patients, you might think of it as binomial distribution. If you do what's normally done, when test say a new drug and you have, we say 10,000 patients then this thing over here is a beautiful and very compact representation of it. Otherwise, it would be almost impossible to compute. That's the purpose of normal distribution for the sake of this class. As you go forward and look into hypotheses testing and confidence then develops. You don't do this for this relatively complicated expressions over here. We just do it for the normal distribution that I think several could be easy to compute. Welcome to the world of normal distributions. So welcome back! You just made it to one of the hairy parts of this class when we talk about normal distributions. Now we've come back into something intuitive, which is how to manipulate normal distributions. Like I said in the previous class, you can flip coins up to better to the values from the normal distribution just the same way you flip coins. So now I'd like to understand what happens if I manipulate normal distributions. So my very first example will be the following. Suppose all the salaries in my company Udacity are normally distributed. That is to say, If I were to look at the salaries, there is a well-defined mean of course. Fewer and fewer people deviate from the mean by a larger amount where the frequency of deviations captured really well by this normal distributions as most people are really near the mean and there's a small number of people are farther from mean. And just for the sake of the argument, suppose the mean is $60,000 per year and the standard deviation is 10,000--these are obviously not accurate numbers. Suppose now, I give everyone a raise of say $10,000, what do think is the new mean and the new standard deviation following µ' and σ'--can you answer these questions? And the answer for the µ‘ is 70,000 of this given salary increase and the new σ stays the same. We already talked about this example earlier in class, but now it's in the content of normal distribution so let me take a second to show this to you. In our normal, ignoring normalization constant, we know that everyone's salary is drawn from a distribution that looks like this. This is plugging in the mean and the variance as that defined it, and now we know, we set the new salary x' to be the old salary plus 10,000 That is the same as saying that the old salary was the new salary minus 10,000. We now substitute this x over here with this expression over here--what we got is something that looks looks smallest, so you can see I took out the x and replaced it by x' minus 10,000 and if you look at this carefully, you'll find this is the same as (x'-70,000)²/10,000². So that proves to you that the variance of the same deviation doesn't change and the mean has effect it by just increasing it to 70,000 but definitely the µ‘ is now 70,000 and the σ' remains 10,000. Now let's say in the same company now with a mean salary of 70,000 and a standard deviation of 10,000, I decide to double everyone salary, and please don't tell my people about that--I'm contemplating with. Don't discuss this in the forums. That's kind of secret, but what does this mean? What do you think is the new mean and what's the new standard deviation important? I'm not asking about the variance but just the square of this just the standard deviation. And the answer is the mean will double and so with the standard deviation. To see put this again into our formula using our original mean, and now we realize that the new salaries x' are twice the x salaries or put differently x will be a half of x'. If I substitute this into the formula over here, I get the same exponential expression, but now with half x' where the x used to be. Now the trick is to deal with the half and that's more complicated than our case before. The very first thing is I have to bring the half out of the parenthesis over here and there's a trick whereby I rewrite 70,000 as a half of a 140,000 and you can see where this is going. With that, I can now bring the half out of the square over here, which gives me a quarter, you have to square the half with x'-140,000)²/10,000² and now, all I have to do is I have to bring the quarter into the 10,000. So quarter up here is the same as a 4 down here. Now, you have to bring it inside the square, so we have to take the square root so it ends up to be the same as the new 2, the square of 2 into the square over here. So this transformation shows that doubling the salary doubles the µ but also doubles the σ. It doubles the standard deviation of those salaries--I've proven it to you. Let's ask you a different question--suppose we're on the fields and we learn how to throw ball. On average, we're able to throw it 30 m, but because there's randomness in the ball, we have a standard deviation of 5 m. And now we trained, we improved our performance by 10%. We didn't just step forward by 2 more meters before we throw and that gives us an additional improvement of 2 m. For Gaussian outcome like this, how does it affect all µ' and σ' after both of these improvements are materialized. And the answer is relatively straightforward a 10% boost in performance goes from 30 to 33 m and a subsequent 2 m gives us 35 m as our new mean. Our σ increases in proportion to the first term to 5.5. The fixed offset of 2 m doesn't affect the spread so this all just stays the same. Here's another challenge--this time you are a golfer. You hit the ball really hard, it goes flying and it lands with some uncertainty that's Gaussian where the distance in expectation is a 100 m and your variance is 30 m². And now you do exactly the same thing again--you hit the ball really hard, it goes flying and has a final position. Again, you expect it to fly a 100 m with a variance of 40 m². Without diving into the mathematics, I'd like you to answer the question first. For the combined stroke, what do you expect the distance to be? And the answer is 200 m. Those just add up. What's the combined variance? And the answer is 60 and it turns out the uncertainties from both of these experiments just add up. The second one is very non-trivial. I'm not going to prove it to you. There's a proof in my book that takes marginal pages, but it is interesting to see when you add Gaussian variables, the means add up and the variance add up. The important to this standard deviation is don't add up. Let me modify my question and let me say this time I expressed things in terms of standard deviations and not variances. Each of these has a standard deviation of 10 m. You ought to know that the combined µ will be 200 m, but what about the new standard deviation. You can actually calculate this and give me at least an approximate answer. And the answer is 14.14 m, which is the same as 10 m * √2. And you see, we know that the old variance would be a 100 m², the µ thinks therefore, it's 200 m² because I told you that just add up. If we go to the standard deviation, we take the square root of this. This factors into 2*100 m², so we take the square root of this, we get back the 10 m on the right side and you get the √2 on the left side, which gives me the 14.14 m for the standard deviation. That's interesting--standard deviations don't add up, variances add up and that was a non-trivial question. So at first we draw values from a normal distribution with µ and σ² parameters and say it equals a value A, then what's the mean and the variance of aA+b where a and b are constants. I'll give you possible answers down here from 1 to 7 and you have to plug an expression to either one of those by giving me the corresponding number. So if you think the expressions over here would be for example σ²+b², you type in 3 over here. If you think it's over here aσ², you type in 7, so either one of those, you have a number in mind 7 and as a hint, once an expression is used up, you can use it again. This was a trick question, which was totally nontrivial. Multiplying something with A+b modifies the mean in a very straightforward way. So this is the valid expression here, aµ+b. So 6 would've been the right answer over here. You already know that adding a constant doesn't change the spread of a distribution. So the result should not have a b inside. That takes out this term and all those terms. We also know that the spread has nothing to do with the mean so this term is out. So the question really is, is it aσ² or a²σ²? And if you remember correctly when we did the salary raise by a factor of 10%, we noticed that the standard deviation goes up by this factor. Therefore, the variance goes up by the square of this factor and 1 is the right answer here and not 7. And that's interesting because in doing arithmetic with the mean and the variance, you have to be very conscientious as to whether you use the variance or the standard deviation. If you get that wrong, then you've got the wrong answer. Let me do a final quiz. This is related to playing golf. We have a normal distribution with µ and σ² from which we draw A. We do the same with B using the same µ and σ². Now we combine A+B and you just add them up. Now I would like to know how the new µ and the new σ² looks like. And as before, I give you a number of possibilities. So there are seven options in total from 2µ all the way to 0. Please put your green number into these fields over here. So this will be a number within 1 and 7, and it indicates that the corresponding term is the correct answer for µ‘ and σ‘². You already learned that the means add up. So we get 2µ, which is 1 for the µ‘. And we learned that variances add up in our golf example. This is just 2σ² or 3 over here. That's the correct answer. And now, let me play a trick on you. I want to take A-B. So I'm subtracting A from B. Think of it as playing golf all the way over, picking up the ball, and playing the ball back to where you started. What do you think the µ‘ will be and what do you think the σ‘² will be? I'm appealing to intuition here. And let's say in expectation, when you go forward 100 m and come back 100 m, you are to be where you started. So 7 will be the correct mean. Now σ‘² is more difficult. It turns out the variances still add up. And the way to see this is if we started over here at a well-defined zero position, and you go to over here and you have some uncertainty as to where you might be. And then you hit the ball back over here, your uncertainty will increase. It won't decrease. That makes 2σ² the correct answer. So even though we substituted the plus sign by a minus sign, the uncertainty that comes in for either of these steps over here will add up just the way it did before. And I have to grant you, this wasn't an easy question and I hope you got it right. If not, think about it and perhaps do the same quiz again. There's some deep insight here, which is for the means the arithmetic I'm giving you always apply straight, but for the variances things are counter-intuitive. When we add to random variables, the variances always just add up no matter what the operation is plus or minus. So this completes this unit. Congratulations! I'd say this was nontrivial. I didn't derive every piece of math for you. But you've learned a number of things. If X is normal with parameters µ and σ², then aX+b will yield parameters aµ+b and a²σ². You also learned if you combined X and Y, that the combined thing adds up the means in this case two µ's and adds up the variances as well. So we did some very basic math on normals. We got a feel for how they changed as we turned them into problems and manipulate them. I'll leave it at this. This was possibly the most insightful and deep probability consideration. You now go back to statistics, and we worry about confidence and other things. We now have a series of statistical myths. I'd like to play once against your styth buster that is I'm going to resolve your belief you might have hold close to your heart. So, here we go. It is a known fact that most drivers believe they drive better than the average driver. Now, you might have heard this most people have a higher IQ than the average person. Most people can run faster than the average person. Do you believe that such a statement can be true? Yes or No. And the things to pay attention to is most drivers drive better than the average driver. Do you believe this can be the case? Yes or No. And surprisingly, the answer is yes. Before I explain, let me ask a second related quiz. Is it possible that 50%, 90% or even 99% of all the people in the world are smarter than the average person. You now check any or all of those or none of those, so you have four choices. Check any or all but at least one of those. And once again the answer is it's even possible that 99% of all people are smarter than the average person and that blows people's mind. Often, there seems to be a contradiction. Here is my hand and as I draw my fingers, I find that I have 5 fingers on my hand. Please check your hands, I bet in all likelihood, each of your hands has 5 fingers, but not always. So if we go and measure the number of fingers that people have on the left hand, you'll probably find that almost everybody has 5 fingers, and in all honesty, there is the rare case of a person who has fewer fingers, like 4 or 3, but that's really rare. So if we look at that sequence, we have 20 hands, 19 of which have 5 fingers. So what do you think now is the average number of fingers for this specific data sequence. Please enter your answer here. [This may be all that stands between us and the enveloping darkness. - Carl Sagan] And the answer is 4.95. When we add these numbers up, we get 99 with 20 data points--99 by 20 is 4.95. Let me ask you how many hands exceed this specific average and I"m asking you for the percent of hands that does this and not the total number. And the answer is 95%, 19 out of the 20 hands have more fingers than the average of 4.95. So 95% of the people could claim that their number of fingers per hand exceeds the average. So it's pretty obvious to see that any percentages below 100 can be obtained-- that is making a sequence of which the majority has one number 5 and very well exceptions such as the 4 over here have fewer fingers. This is one in 20 that is 5% but as you draw more 5s, you can make the percentage of these occurrences as small as you want. So next time you see a statement like most people think they are smarter than the average person, will you say, "This is impossible" or will you say, "This is indeed possible." And I would argue that this is possible. It sounds arrogant to say that most people are smarter than the average person. I'm sure it's not based on scientific evidence but statistically this is very much possible. When people say this is a contradiction--it can't be that most people are smarter than average. Just smile and say--you took Sebastian's stats 101 and you understand. In this unit here, we'll have fun. Somehow in the last couple of days on Facebook, a discussion brought out what Sebastian's weight is. And I decided rather than telling people how much I weigh I turned this into statistics. And upfront I want you to put everything together what we've done so far using programming and since programming has been optional in this class consider this unit optional but it'd be great if you had a chance to try it. It's not that hard and at the end of the day you'll know something about me that I rarely discuss in public. Through a comment I made in class on Facebook a discussion erupted in our Facebook STATS 101 discussion group what my actual weight is. And here is the form that I posted. They were asked to submit their best estimate how much I weight in kilograms, and also to submit how much they thought I weighed a year ago. And within a few hours, there was a good number of guesses including this one over here that's about as much as the planet Pluto weighs and also some negative guesses. These are both the negative weight of Pluto each. But other than that, there were lots of really good guesses. And you can see in kilograms, some people think I weigh 80 or 65, others think I weigh 250. I took the good guesses and added them into a large list called weight. That's just below 100 of those and now I want to do a statistics on those. The very first thing I did is I printed the mean estimate and it turns out to be negative. It's -2.10x10²⁰, and it's a typical situation in statistics. When you look at those numbers, most of them are actually pretty good guesses. But these extreme guesses of 10²² or -10²² over here completely affect and screw up the actual statistics. Now, you've learned how to deal with this. You know everything about statistics. What I want you to do is to now code a piece of software called calculate<u>weight</u> that has 3 things, and I think you can do all three of them yourself. First, I want you to remove the outliers by only extracting data between the lower and upper quartile. It turns out the number of data points make it well defined what the lower and upper quartile is. And all test cases we run through have a well-defined number of data points. And all the test cases we'll be using will have the property that the lower and upper quartile are well-defined elements. Then, I want you to fit a Gaussian using the maximum likelihood estimator. And from there, I want you to compute the value x that corresponds to the standard score z, so I'll be giving you not just the weight statistics or the weight data but also where my extra weight is. If you plug in the standard score of -2, which is two standard deviations below the mean of the data that we will estimate, you'll find out my extra weight that I took this morning. It's amazingly accurate. But definitely, the data that you guys provided for this was overestimating my weight and I'm happy to report by two standard deviations. All these formulas are known, and I think you have all the coding skills necessary from the past to fill these gaps. Obviously, the first step is the hardest. And when you're done with it, this command over here will give you the correct answer. I won't give you a number, but here is my solution. The hardest part is to get the outlier removal correct. For that I sorted the data and I computed the lower and upper quartile. And these are the exact formulas I used in the exact case. It turns out even if it doesn't factor nicely, those give me good numbers. And then I compute a new data set where I just extract data in the range from the lower quartile to the upper quartile using this simple command over here. There's many different ways to implement this. Then, the maximum likelihood estimator is very simple. You've already developed the functions mean and standard deviation. I just use those. And they were supplied for to you in the code we gave you. And finally, this is the inverse of the standard score z that I've told you. If the standard score z is this expression over here, then you can easily solve it for x by bringing x to the left side. With this formula, we get this kind of code over here and we return the corresponding x value. Now when you hit "run" you get the answer to the fascinating question of how much I weigh but I won't tell you. There are all kinds of other cool stuff you can do with the data. This is a cleaned up data set where I took these extreme values out manually. And then I kind of want to scatterplot off today's weight versus last year's weight. And when I do this, I get an interesting graph. It is approximately linear, but there are some very funny outliers. There's a point over here. Obviously someone thought I went from 0 to 200 within a year. And there are also points over here that indicate massive weight loss or massive weight gain within the span of a year. And I have no clue why people believe I massively lost or gained weight. But as a statistician, looking at the data is really informative. It tells you that even in the spectrum of good guesses, there are some that are suspicious or people likely didn't take the question very seriously. As a statistician, your skill will be to look at these data points more deeply and possibly remove them based on what you see in this scatterplot over here. That goes way beyond the idea of quartiles but in the estimation of the mean those phenomena have a massive impact as to what your final outcome looks like. Thank you everybody for supplying the data that led to this programming unit. It's proof time again! 3 AM in the recording studio. And as always, this is entirely optional. This goes way beyond what will ever be taught in an introduction to statistics class. It's only for those who love hard-to-prove challenges. And as previously, we're going to prove the correctness of the maximum likelihood estimator or MLE but this time for Gaussians. Our Xi's can now be arbitrary values not just 0s or 1s. And the mean and the variance are computed using these two now familiar formula. How can we prove this to be correct? Again, last chance to skip. As before, there's the proof on the left side with a couple of blanks five in total. And there's seven expressions that you can put in and the way you put them in is if for example the seventh expression in your opinion fits over here you put the number 7 in here. You only use each expression on the right side once, and there's obviously more expressions on the right side than the holes on the left side. Let me quickly go over the proof. In maximizing the probability of the data, we first state the probability of these data items and then send over here using a normal distribution with prime of µ and σ². But that left something unknown on the left that you have to pick from the right side. Then we take the logarithm of this and here's the logarithm with two gaps in sight. We take the first derivative of the logarithm that you have to compute by picking the appropriate term on the right and set it to 0, which is where the maximum is obtained. From there, we can transform the expression to this expression over here where I left out a term and it finally gives you the desired proof for the maximum likelihood estimator for the mean. Good luck! The first thing you have to do is figure out that this is actually a product. It's #5. Because you have multiple data points, the probabilities multiply. Going to the logarithm, we replace it with a sum that is #6. And the logarithm of the expression in the exponent is just its argument which means #3 is the solution here. You simply just copy those argument over. The first derivative loses all of these terms over here. They don't depend on µ. But it would change this one over here. The answer ends up being 4, Xi-µ/σ² and the reason is the way the derivative works. We take the derivative of this expression. There's a 2 over here that gets multipled left and a minus sign in front of the µ. Those get multiplied in and then canceled out with -1/2. Now from here to here, I just multiplied with σ²and now I bring the µ out and import this and the resulting expression is Nµ. The N comes in because we have N additions of µ in the formula over here. That completes the entire proof. Now we talked about the easy thing, which is the mean. We're now going to move on to a much more challenging proof which is the σ². I once again started with the probability of data which is the very familiar product expression over here. And let me lay out the entire proof for you. As before, we start with the probability of data and as before we take the logarithm of it. And since I've already done this, I'll just restate what we previously derived. This would look familiar. If not, go back and watch the previous video. Now the key thing is that we take the derivative of spectral sigma. Let me lay out the entire proof for you. So here's how it looks. We take the logarithm this time with respect to sigma of all expression to arrive with this thing over here. We simplify this by multiplying something into here, factor out something over here, and we arrive at our desired result. Plug in those numbers as before. There's a total of 10 of those for seven open expressions on the left. Let's start with the first expression. This one over here doesn't depend on sigma at all. Hence, the derivative 0. The derivative of logσ is 1/σ. And this expression is funky because you have a σ⁻² in the denominator. That results to 1/σ³ and then -2 multiplied in cancels the minus 1/2 over here very conveniently to with we have all these sigmas in the denominator. We multiply up σ³ which gives σ² over here and leaves only this expression on the right side. Taking out σ² to the left side, we observe that we have Nσ² in the sum. We get Nσ² and bringing N to the right gives us σ. This is the full proof that the empirical observed variance over the data is the maximum likelihood estimator for finding the parameters of a Gaussian. If you got this far, I am impressed. We left basic statistics way behind us and got into fairly heavy math. Thanks for taking this unit. If you just clicked forward and hit the next button, never mind. You can complete the do you understand basic statistics without understanding how it's being derived Let's move on and use the Gaussian for something practical. Today, we'll be turning things upside down and apply what we learned so far about estimation and probability to solve some really basic statistical problems that occur everyday. Today, we talk about confidence intervals and in the next class we talk about testing hypothesis. These are fundamental concepts that every applied statistician has to used pretty much everyday. You'd better listen. This is my first example. Imagine it is just about election day, and I know not all countries have election days but I wish they did and I wish they were fair. In the United States, they have 700 million people that are eligible to vote over those a section go and vote, but as a transition you often wish to understand what the outcome of election day is before the actual vote happens. One thing you could is to ask every voter and then just report the results. Is this a good idea? And of course not--the amount of effort involved from asking every voter is paramount to running the entire election itself, which in this small group of people maybe five or ten is feasible but in a group of hundreds of millions of people isn't feasible. Here's what statistician do. They choose a random sample. The sample is the selection of hopefully random withdrawn people from this pool that are representative of the pool at large. In this sample, they might derive an estimate on how people are likely to vote. They might just report that as estimate, but in practice, what they do is they report that the estimate plus a margin of error. In this margin of error, means what's really been given back is not singular number like 60%, but an interval--we call this the confidence interval. Look at these numbers for the blue party or party A and tell me what do you think is the lower bound and upper bound in the conference interval in percent. And the answer is simple. You take 60%. You subtract 3 to get 57 as the lower bound and 63 as the upper bound. The confidence interval of 57-63 means we are fairly confident that given our current sample, election day will give party A an outcome within this range. This is a very fundamental concept in statistics. It applies to coin flips and many of the other examples we discussed before. A candidate in election might have a true chance that any given voter votes for him. Let's call this chance P, the same as the coin flip, and of course if P larger than 0.5 in a 2% run off that person will not initially win in most cases, not always, but as a statistician we can't assess the two chances, so what we do is to form a sample. In coin flipping, we flip n coins. In elections, we ask n randomly chosen people. As we know, this gives us an estimated mean. It gives us also a variance and therefore, standard deviation. What's new now is the confidence interval, and the confidence interval is not the same as the variance. In fact, drop the consideration of a variance. What this says is based on the outcome, we believe for the parameter µ. This is the best guess we can get, which is usually maximum value estimate, but we are not quite certain, so you're going to asses a wider range, in which we believe with high probability that whatever the outcome will be that the outcome will be high probability within this range. Very often this range is defined as 95% chance that the final outcome falls into this range. That will take some work to compute. So let's dive in. Let's say it's election day coming up again. In this example, I would like you to get an intuition how confidence intervals behave. Here's again all the voters and there are multiple institutions. Institution 1, Institution 2, and Institution 3 that all samp of the voters to derive a prediction of what the outcome of election day is. For simplicity, there are only 2 parties A and B and say institution 1 finds in a sample of 20 with 11 say A and 9 say B. What do you think the maximum of your estimate is going to be base on the information? I'm asking here for the probability that party A wins. And the answer is 0.55. Now let's say Institution B has the same probability, just more rigorous, and it asks 5 times as many people. So 55 say A and 45 say B. Once again, what is the probability for A using the maximum likelihood estimator? And the answer is 0.55 as before. But there is a difference. Institute 1 used many fewer data points than Institute 2. In fact, if someone when all the way to ask 2000 potential voters, yet derived the exact same 0.55 then all the predictions look alike, but if you were to trust one of the three the most, which one would you choose? Choose exactly one of the three. And I would say it's the third one because they asked the most people. To see, let's make an extreme example of an institute that only asked one person. Now, it's kind of chance. A is a bit more likely than B. They will be forced to say that all voters will either vote A or all voters would vote B, depending on the outcome of this one question. In this case because they encountered someone saying A, the probability for anybody else voting A will be 1. Would you trust this company that only asked one person? Of course not. The more data that's sampled, assuming that the sample is fair and independent, the more trust you have, and more trust means a smaller confidence interval. Suppose you beta sample and we now increase the sample size N, will the size after confidence interval grow, shrink or stay the same? Pick one of the three. And the answer is it will shrink. The confidence interval become smaller. If you have a probability here you're trying to estimate--and here is the empirical mean-- with few data points--you might have a very right confidence interval--but as data increases, the boundary of the confidence interval will move inward. Let me contrast to this with a concept that we encountered previously, which is the standard deviation. Suppose you increase the sample size N, how would you expect the standard deviation of the sample to change--will it grow, shrink or stay about the same? And here the answer is still about the same. It turns out that standard deviation of distribution is not dependent on the sample, so whether you throw a coin 100 times or a 1000 times when you graph the outcomes the standard deviation is not affected by the sample size but the confidence interval is. That's a really important difference. If there's one thing you want to take home from this class that is that as you get more data as a statistician, your confidence increases--so your confidence interval shrinks, whereas of course more data doesn't change the standard deviation. In fact, if the standard deviation is σ, then the confidence interval is effectively 1/√Nσ. This is proportional, depends on number of other factors. There could be a constant in front of it. But there is an interesting relationship. As you get more data, the standard deviation will become more accurate, but it's not going to change that much. Your confidence will shrink, and we're going to look into this a more detail. Let's go back to probability and go back to our simplest of all examples--that of a coin flip. Assume that p is the probability that the coin comes up heads and assume we flip the coin N times-- that is the same as saying that a sample size of N. It gives us N outcomes and in the past, we computed the empirical mean, the variance, but now I will let you compute the confidence interval. Let's start--I'm going to ask you a series of questions, most of them are fairly challenging, and at the end you'll be able to compute the confidence interval for a sample size N of coin flips. Let me begin. What is the expected mean outcome when you flip this coin? Suppose one is heads, 0 is tails--either comes up, it's is 0.5 chance. This could be a number where the number scales between 0 and 1. It would be 0 if the expected outcome is always tails and be 1 if the expected outcome is always heads. And the answer is 0.5. To see, we find that the probability p was 0.5 we get an outcome of 1, heads, and with probability of 1-p, again 0.5, we get an outcome of tails. Adding this up gives us 0.5. A more tricky question--what do you think for this coin flip is the variance of the outcome? That's the question you never really asked, and it's okay to get it wrong or punt on it, but perhaps you can give it a try. The math is very similar to the math that I provided here. Well, you know that the variance is defined as the quadratic difference of the actual outcomes from the mean. There's two possible actual outcomes--there's outcome 1 and outcome 0, both have probability of 0.5. If 1 is the outcome, here is our difference from the mean--we square this. If 0 is the outcome, then this is our deviation from the mean. This guy is about 0.25 after I'm planning to square, so we get in total 0.25. You should check if the math is correct. This is all basic probability and doesn't get us into confidence intervals, but let's do it now. Let's say we have a sample of size 1, 2 and 10. We know that the variance of each of those individual coin flips is 0.25, but I want to add up all the outcomes. What is the mean when I just sum up the outcomes--I've not yet computed the average-- and what is the variance when I sum up all the outcomes for all the samples? Let's do the means first. Hopefully, you guessed this correct. For a single coin flip, it's 0.5 as written up here. For 2 it's a 1. So in average, the outcome may be 1 and for 10, it'll be 5. Let's now do the variances. Fill in those three numbers. Let's now do the variances. The first one we can copy over, but what is the second number? Well, you might vaguely remember that the sum of two coin flips or sum of any two of anythings you do, the variances just add up. With this formula in mind, we find that the variance of two coin flips is 0.5, and the variance of the sum of 10 coin flips which is 2.5, which is 10 times the number over here. And now I'm going to be really demanding. I would like to know what is the variance of the mean, not the sum, but the mean. I'm going to put 1/N over here--the same sum as before. This is what you do, for example, when you compute the mean outcome of an election from a sample of size N. You take the sum of the things you saw and you compute the mean. That's the formula of the mean, but now I'm asking what's the variance of the mean. So I'm not asking for this one over here--we asking for this one over here. And as a hint, at some point we talked about something like the following-- what if you multiple a variable by a constant A, what happens to its variance? It was something times variance X. Let's do the first. What goes in there. Well, this one is surprisingly easy because N=1, so this thing falls out so it's the same as this thing over here--0.25. The second gets the higher one. Give it a try. It's okay if you fail. And this is the one thing that's really important--variance is a quadratic expression. A final multiplier of the outcome goes out quadratically, and we talked about this, you might have forgotten it--it's not important. I think now that you see it again, you might hopefully remember it. That means if you multiply 1/N inside the variance, the 1/N² outside, N²=4 in this case, 0.5/4 is 0.125 This is logic. I hope you'll be able to do the fifth one over here. The answer will be 0.025. If you look at this, the variance of the sum increased with N. But now we're dividing 2.5 by 100, 10²--so 2.5/100 gives 0.025, and there's really an interesting insight here, which is the spread of the mean-- what you compute from the sample--goes down as N increases. For small N, it's 0.25, but for larger N like N=10, it's now 0.025. We are now two steps away from the confidence interval. We're almost there. The first thing we do is we go back from the quadratic expression down to the non-quadratic expression--the standard deviation that corresponds to the variance over here. You're ready now to compute this. Just give me those three values over here. 0.5 is the square root of 0.25, 0.3536 is the square root over here, and the variance over here is approximately 0.16. And now we get to the confidence interval. And without further ado just multiply this value over here with 1.96. The magic number. That should give us a size of the confidence interval. Yes the first one is 0.98, 0.69, and 0.31. Now I should argue this 1.96 trick isn't mathematically correct for very small sample sizes. It usually assumes we have a least 30 samples, but let's just ignore this for a second. What you've done is for the coin flip example, you've actually computed confidence intervals yourself. That's a nontrivial thing to do, and because we like this so much, let's assume that we have a sample size of a 100 and give me all of those numbers over here. Now the mean is obviously 50, 0.5<i>100, the variance will be 25, 100<i>0.25.</i></i> But as we divide the variance by now 10,000, we get 0.0025. The square root of that happens to be 0.05 and multiplying with this magic number 1.96 gives us 0.098. The way we read this is if after a 100 coin flips we observed, perhaps empirically, that the probability of heads is 50%, there is still an approximate ±10% confidence interval. So we are really certain it'll fall between 40% and 60%, but in between we're not that certain. Let's pick a second exercise, we have now a loaded coin that mostly comes up tails with 0.1 chance it comes up heads. I gave you the formulas for how to compute the mean of the outcome. The mean is just the probability and the variance. This comes without proof, but in the next optional units, we'll prove this expression together. We studied the special case of a fair coin, p=0.5, and now I want to do the same thing for our arbitrary p. It turns out that the real challenge is not to compute the confidence interval. The real challenge is to compute the variance of a coin flip where the outcome is p. In particular, let's do a quick check. Suppose p=0, what do you think the variance should be? Do this by thinking principally not by trying to solve complex mathematical equations. And of course the answers is 0, if p=0, the coin comes up tail all the time and as a result it won't vary, therefore there is no variance. The same as to for p=1. Once again we know the variance is 0. So if we were to draw the variance as a function of p, we already know it's 0.25 when the count is most random, and it goes down in someway all the way to 0. Here's a game I would like to play with you. I drew on the right side six different expressions, and I'm going to derive this for you, leaving out those six expressions and whenever I stop, you have to pick one on the right side, Click the appropriate radio button--or maybe two of them--and if they're correct, we'll move on. To calculate the variance of x, we first notice that p is the mean of x, and if you look at the two possible outcomes starting with heads and then tails, you find that the probability for heads is p and if heads is chosen then the variance is the quadratic difference of the actual outcome, which is 1 for heads minus the mean which equals p. Complete the same expression for the tails case. What's the probability of tails, and if tails actually occurs, the outcome is 0. What will be the quadratic deviation we putt into this formula over here? And this is the correct one, tails comes up with probability 1 - p, and the difference between 0, the outcome, and the actual mean is p, and we have to square it for the variance. So I take this expression away. We multiply this out and then we're going to leave certain expressions open. In particular, I'll multiply this square root expression first but leave the center expression open, and on the right side, only the open term--so two of those fit exactly in here. Please click the two appropriate boxes. And obviously this is -2p--the guy over here. So let me put this in. And this one is here -p³ which sits over here. So let me just put this in and take those options away. The next step is now straightforward--we multiply this guy out over here p - 2p² + p³ leaving this guy over here intact. And that simplifies to one of the expressions on the right side. Obviously the p³ cancel out plus -2p² over here plus 1p² over here, which makes -1p², and we can leave the p intact--we get the expression on the bottom. We can factor p out to get p(1 - p). That's the variance of X for arbitrary p. Let's do another exercise. I'll give you a specific p. Calculate for me the variance. That was p(1-p). Obviously, that's 0.09. For different values of N, I'd like to know what is the width of the confidence interval using the same formula we used before. Let's start with N=1 and this involve different steps, you compute the standard deviation as the square root of the variance you divide it by the square root of N and multiply with this funny factor 1.96--p(N-p) is 0.09--the square root is 0.3. N=1 so it's 0.3<i>1.96 reveals a 0.588.</i> I trust that you'll probably do the same now for N = 10 and N = 100 using this exact formula that just the same that we've done before. 0.186 for N=10 and not surprisingly for 100, it'd be exactly a factor of 10-- smaller than the confidence interval over here. Now that's really cool which means for any coin flip in any sample size, you can compute for me how confident you are. If in the coin flip, you came back to saying, wow I believe it's the 0.44, you can now say, well, given the number of same as the head, it's not 0.44. It's a confidence interval. It's a certain way of maybe ±0.07 that gets added to the 0.44 that they can be confident about according to our method study so far. In practice, in which you were given a sample and you don't know the exact value of p. Suppose this is the sample you observed, N=4 and we're going to apply the formula even though we understand it should only apply when N is equal or larger than 30. Just for the exercise of applying the formula, compute for me the mean and the variance. The variance, I don't need the formula variance but the variance, I mean the actual variance from the data sequence. Obviously, the mean is 0.75 and for the variance, you subtract 0.75 from the data sequence and you square it and so on. You get those results, compute the mean and I hope it also gives us 0.1875. You know a magic formula we can compute the ± term of the confidence level. And here's how it looks like, where we've replaced the analytic variance by the empirical variance over here, but otherwise, it's the same as before. I just calculated this, and here's what I get. For this sample, that means, we would say between the probability 0 and 1, we believe most likely the coin was biased and has a 0.754 probability. We are quite sure we have a confidence interval that's 0.42 wide on both sides. In fact, it reaches beyond 1, which is a funny side effect of this in the normal assumption. This is the interval in which we are uncertain about what the correct value of the coin is. In fact, this one does include 0.5--it could be that this coin is perfectly unbiased. 0.75-0.42 is much smaller than 0.5. Obviously, it's a small sample. Let's do the same thing for a larger sample, and this one is a quiz for you. Here's the sample. I've ordered it. There's three tails and seven heads. I want you to just compute for me the mean plus/minus this confidence interval with the term. Please give me both numbers. Now for the mean is 0.7 and we get 0.28 for the term in the right. In my calculation, σ² is 0.21 and when I apply my magic formula, plugging in N=10, I get this guy right over here. What's interesting is this CI term is now smaller because they have a larger sample. Let me give you one last quiz. Now, we have here 400 times tail gives us 600 times heads. We flip the coin a 1000 times which is a lot. Let's observe what happens to the mean and our much beloved width of the confidence interval and just follow this--use our formula again. As easily seen, the mean is 0.6. Hope you got that right. The variance ends at 0.24, surprisingly it's 0.6<i>1-0.6</i> and determine the right here is 0.0304 in approximation. And that means with a thousand samples, we are really confident that the mean is 0.6 and we are only willing to consider a deviation of 0.03. Now I ask you, do you think this gets evidence that this is a fair coin? You'd say absolutely no because the confidence interval is so small. This is where the square root of N really helps us really drives the size of the confidence interval down. The more the sample size, the better for us. The more confident we are in our result. So congratulations! You've learned about confidence intervals and this is a really cool tool to use when making conclusions from sample data. It's much better to say here's the mean and here is how confident I am about indicating the interval than just giving the mean, and you learned that the interval size shrinks as the sample size increases--so eventually, if you have lots of data, we get more and more confident. In this short unit, I want to address the magic number that came up in the last unit. Do you remember which one the magic number was? And of course, it wasn't Ď€ or e--it was 1.96. So we defined the confidence interval to be mean ±1.96 of the observed standard deviation over the data said size. So the question is where did the 1.96 come from? Remember the Central Limit Theorem? Remember what it said? For large sample sizes N, the mean outcome of the sum of N independently drawn excised becomes more and more normal. Now it turns out the Central Limit Theorem is directly related to the value of 1.96 and it isn't entirely obvious so I didn't make it a quiz. With the Central Limit Theorem, things become really easy. Take a coin flip. There is a true probability p for the coin. Now we know that we sampled the coin N times and compute from it our empirical mean using the formula you well know. That it's well true that it could easily happen that p doesn't end from µ and that's because flipping the coin number of times is a stress and estimate of a true probability p. Now whole likely our estimates, so suppose this is a true p and I gave you three possible µ. This one, this one, and one out here, and I put a little check marks underneath. Check the box that is the most likely µ you might observed, if p is a true probability using our statistical estimator. And of course, it's near this one. If it the most likely one will repeat itself and the further out you are, the less likely is to have a sample that it gets you this far out. And now, we'll be using the Central Limit Theorem. For large N, say N large N=30, we know that the distribution of µ you might observe is Gaussian. For any value here, let's call this one a. The chances that we observed in µ that's smaller than a is the same as the surface area and then the Gaussian over here. There's a trick you haven't seen but the probability is driving any µ is at least in the limit given by the height of this Gaussian. So for any value a, we can compute a probability that µ becomes smaller than a. And by symmetry, for any value of b, it's is equally away from p as a as on the left side of p, they can do exactly the same thing. Now, where does the 1.96 come from? and here's the trick, 1.96 marks the 95% confidence interval. So we plug in the observed μ instead of the p which we don't know under the Central Limit Theorem, we get the same Gaussian and then we compute the x which μ-x or μ+x give us these decision boundaries such as the total area in these two tails over here is exactly 0.05. That's the same as 5%. In fact, you placed half of this on the left side and half of it on the right side. When that's the case, the confidence interval itself carries 95% of all possible values for p, so the chances that p lies outside the green confidence interval is 5%, hence, we call it a 95% confidence interval. Let me ask you a quiz. Suppose you go from 95% to 90%, does this make x smaller, larger, or will there be no change. Which one is it? I would argue it makes it smaller and here is why. Now, you can afford placing 5% on each side or 0.05 and that shrinks the size of the confidence interval, hence, the smaller x. What if we go to 99%? Let's provide the answer now. And it was right, it's larger. If you only allow it half of this end on each side, the confidence interval becomes noticeably larger. A typical confidence is from 90% to 99%, you can find these values in tables. They are called quantiles. There might be numbers like 2.58, 1.64, 1.96. In this quiz, I just want you to tell me which of these numbers corresponds to what confidence intervals and I'll give you that to correct each of those match exactly one of those over here. So check exactly three boxes. And here we go, we already know 95% is 1.96. We know as you want to increase our confidence, we have to go outbound which means a larger value. So it's 2.58 as the only choice for larger value, then it's 1.64 the final choice for the 90% confidence interval. Let me ask you a trick question. This is always correct. Say you have a sample, you run a statistic, you plug in one of these factors, will you always get the correct answer when finishing them up? If you listened very carefully I already told you--yes or no. And the answer is no. There's many reasons but the reason I've given you as I said before we need a large sample of this study. So here's a t-table that applies to statistical estimates with fewer than 30 samples. And the way to read through this on the left you see the degrees of freedom which is the number of samples minus 1. So if you have 10 samples it would be 9, if there are 15 samples it would be 14. On the top, you'll see 1 minus the confidence level. So if you want 95% confidence, you go to 0.05. If you want 98% confidence, you go to 0.02. Now there's a slight distinction here. There's a 1-tail and a -2tails. So far, we've talked about 2-tails in which, we always cut off tails left and right of our confidence interval. There are occasions, which we'll talk about later, where we should just cut off one side. And those often occur in the context of testing hypotheses. Now for the time being, please just look at the 2-tails number over here. And ignore all those 1-tail numbers in this table. Okay? So just to make sure you know how to read this table. Suppose we have n=8 samples and we look for a 90% confidence interval. What number would you find in this table to be the magic number? And the answer is 90% translates to 0.1, so it's going to be over here in this column. N=8 translates to 7--I told you data point minus 1. So this will be the correct number over here, 1.895. Let's ask you another question, suppose you care about 95% probability confidence and you want a factor that is no larger than 2.145. How many data points do you have to collect at a minimum to reach a factor that isn't larger than 2.145? You can read this off the table. And again the answer is simple 95 means you're going to look into this column over here. 2.145 happens to be the value over here, which translates to 14. Now, we do know that the number of data points is the number degrees of freedom plus 1. So 15 is the correct answer, not 14. Let's now apply what we just learned in the context of computing an actual confidence interval. Suppose I care about 90% confidence and from my coin flips, I get a data sequence of 5 elements 11000 and let's for now assume you're going to use the T table. This might not be the world's most accurate method but for now we do this. Now we've computed the variance many times. I'm just going to give it to you. This 0.24 and if I knew this five takes the square root of the variance over n is about 0.219. You compute this so many times, these are just the numbers, but I want you to give me is the confidence intervals, which is the mean plus or minus the term on the right. Obviously you can do this by using our table over here. Remember to use the two the tail not the one tail and by multiplying in the appropriate magic number. The mean is easy. It's 0.4 and this thing over here, we'll find the appropriate element in the table. For any puts five, we're going to look at the row four and for 90% we look at the 0.92 tails, this one over here and they intersect at 2.132 and if we multiply this to 0.219, we get 0.467 approximately--would have been the correct answer. So now we talk about one of my favorite topics hypothesis testing. And the reason why I love this topic is because it's really about decision making. We talked a lot about data or often called the sample and we used statistics to extract information and now we use more statistics to make decisions and the decisions to be binary, be it a yes or no. And the reason why I like this, I think life is all about decision making. I think there is no meaning in doing statistics without eventually at least making a decision. This is the first time in this class, you're going to make an actual decision. So in this box of pills for weight loss and particularly it says that 90% probability, taking these pills, a substantial weight loss is being guaranteed. Now suppose your job is not to make a decision whether to buy this box or not. I can promise you in all likelihood why those pills don't work but whether this claim is correct or not so you really care about why the company that sells this product made a correct thing. And to do so, you talk to people who have taken this medicine and you ask question--did you lose weight? Yes or no and say this is a magical medication. And 11 out of 15 people tell you yes, but 4 out 15 people tell you no. Obviously, this is not 90%. In fact, what do you think it is in percent? It is only 73.3, which is 11/15. But that could just be a sample error. It could actually be 90% as advertised and you were just unlucky, but we believe that high probability of this is correct or is the data actually contradicting this claim and you should complain that the claim is incorrect. That's the essence of hypothesis testing. We make decisions whether a hypothesis is correct. So this is the hypothesis. It's often called the null hypothesis or H₀. Null is kind of a funny word but that's what people commonly call it. And there's a contrary hypothesis that evaluates this hypothesis-- we'll just call it the alternate hypothesis or H₁. So on the right side, I'm going to write examples for what this hypothesis could mean in this case. So here are five different choices and if you listen carefully, can you tell me which of these five choices and there's exactly one would correspond to the null hypothesis that this medication works exactly as advertised where p is the probability of losing weight. And yes, this is the one. Now, the tricky question is--what do you think is the alternate hypothesis if I suspect that the medication doesn't work as well as advertised in terms of losing weight. It's a bit of a trick question because we haven't talked about it yet, so don't worry if we don't understand if I get it wrong. I would say your counter hypothesis--the alternate hypothesis is p is smaller than 0.09, and the reason why is I said the counter hypothesis if the medication doesn't work as well as advertised. If I'd said it doesn't work as advertised and dropped the words as well, then this might be the correct counter hypothesis, but those are hard as you analyze. In reality, you often have a suspicion that you're in a specific side of the null hypothesis. So this is the one to pick over here. So now, we have two hypotheses and we'd like to test them. The null hypothesis is the basic hypothesis that something is correct and unless proven wrong, we always believe null hypothesis is actually correct, we trust the manufacturer or do we have sufficient evidence to be very high likelihood the null hypothesis is wrong, the alternate hypothesis is correct then, we reject the null hypothesis and we tell the manufacturer, please print the label on the box because the number there isn't quite correct. So let's do it. To recap, we have sample 11 yeses and 4 nos. And our null hypothesis is equals p=0.9 and our alternate hypothesis, if it doesn't work, as well as the null hypothesis, so p is smaller than 0.9. This is a very exact to the setup you're going to study when we make hypothesis test. If you look at the sample and it's obvious this is binomial which is the fancy word for coin flips, binary experiments. With 15 different experiments and under the null hypothesis probably goes 0.9. So obviously, in this binomial distribution, there are about 16 different outcomes from 0 to 15. The most likely is 13, 14 is also pretty likely if the null hypothesis is correct, and then the probability goes back to be down. The key is assume that H₀ is actually correct, so the probability is 0.9, then FIND was called the critical region under which 5% or less of the total probability of this binomial designs for different lane. You will place a mark over here that separates outcomes that invalidate the hypothesis and outcomes that validate the hypothesis, so that all the outcomes, together, left off this marker collect in totality at most the confidence level, in this case 5%. So that if you find an outcome that sits to the left of this point, you can say wow this outcome was so insanely unlikely under the null hypothesis, I just don't buy it. If conversely the outcome is somewhere in this green box and it isn't that unlikely after all. Realize that each outcome itself might have a small likelihood, but you don't care about the probability of the outcome itself. Even the maximum of the outcome has a small probability in this model. All you care about is putting a separator within and except this region and what's called the critical region and again the logic is the critical region collects many possible outcomes, but in totality, these probabilities don't exceed the 5%. So if the final outcome is in the critical region, you can confidently say, wow, I'm really surprised I don't buy that H₀ is correct. Let me write down for any possible outcome of the binomial, the corresponding probability. So just for our little Python program implementing the familiar expression of binomial distribution and with this program, I'll get the following. Practically speaking, all the first ones have zero probability. This is about something times 10 to the minus 5 and then over here, very small probability has come up, and they go all the way to 0.34 for 14, somewhere between 13 and 14 is the most likely outcome, but even those numbers aren't huge because there are so many of those up to 15. So you've build these tables before, I spared to the work this time around and the very basic logic here is to not draw a line such that the total probability contained on top of the line and it's this, doesn't exceed 5%. So here's a question for you, where would you draw the line? If you draw it here, for example, then give me the number just on top of the line, 3. If you're going to draw over here, it will be 13. Neither of those are correct. Draw the line where the probability contained above the line doesn't exceed 5%. And the correct answer is after 10--the sum of all these things over here up to the 10th are 0.012 approximately. There's a slight truncation issue here that gets some work with number and is far away from 5%, which will be 0.05, but if you're now were to add this guy over here, you'll go beyond 0.05. So this is where we draw the line, any outcome over here is critical. That means so any of those outcomes, you'll be so surprised that we reject the null hypothesis. Whereas any outcome in the region over here is okay, so we accept it. Notice subtlety here. Even outcome 11 is okay. Despite the fact that 11 is very surprising, it comes to the probability less than 5%, but that's not the way to look at this. The more frequently your sample, the more unlikely each individual outcome is. Look at the outcome as blocks. When you implement hypothesis testing, there'll be something like a most likely outcome. Somewhere between 13 and 14, I guess it's 14. That outcome has to be okay obviously. If you see that, you'll be happy and then you push this in one direction towards H₁, the alternate hypothesis until all the probabilities remaining cover 5% or less and that's the critical region. So let's give this another try. Last Saturday, I went to the magic store and bought a loaded coin. I paid a lot of money for it and the wizard that sold me the coin told me the probability of head is equals 0.3. In fact, there are many buckets of coins, the fair ones the slightly loaded, all the way to the fully loaded, and those guys are cheap and those guys are cheap but this one over here was really expensive. So I wanted to make sure that I really got a loaded coin. Let's first ask, what's our null hypothesis. Here are our choices for the probability of heads. Pick the one that's best describes the null hypothesis. And yes it was a difficult question--if at all it confused you by the frequent occurrence of 0.7 which makes no sense whatsoever. This is the correct one, p=0.03. Now my suspicion is they just sold me a fair coin or something closer to a fair coin-- that's a one-sided suspicion. Which of those best describes the alternative hypothesis? Which one? Check one of the remaining seven boxes. And in this case which showed p>0.3--that means it's more like a fair coin than this one over here. It could be unfair in the other direction. This test just short for the number over here is correct. So now I do my sample--tails, heads, and I get the following 11 outcomes. What is the value for p? Is 0.45 which is 5/11. So again now the question with 5% confidence, tell me should we return the coin because the given one is more fair than the one I wanted or should I accept it. To make it a little easier, I will calculate for you the binomial probabilities and of the null hypothesis 0.3 for the 12 possible outcomes from 0 heads all the way to 11 heads. So here are the approximate values in this table and before you make your decision, let me ask you a couple of easier questions--what is the most likely outcome for 11 coin flips assuming that the H₀ is correct and the answer is 3. Technically, 0.3 times 11 is 3.3, but 3.3 cannot be the outcome--it has to be an integer number. 3 is the highest probability as seen in the table over here. Next question. The critical region--is it to the left or to the right of 3 in this table? And the answer is to the right. We're not asking the question of whether the coin unequals 0.3, at least not for now. So now the $100 question--what is the smallest outcome in the critical region? Put your answer right here. The answer is 7. These probabilities up here are smaller than 0.05. If you go to move to the left, you crossed the 5% confidence boundary. I'm going to ask you, should you return the coin. Yes or no. I would argue you're safe. There are five heads in the sequence. That's your actual outcome. It's well within the safe region. We'd only return it if we seen heads seven times or more. Now, let's look at one more example and this time we go to a bank and hope to get a fair coin This going coin over here and our alternate hypothesis is a two-sided hypothesis with a probability either is smaller than 0.5 or larger than 0.5 which provide us p≠0.5. The way to know where this concept will lead, in all we've done so far, we assume that H₀ is correct and we've computed some sort of distribution and then we cut out a critical region such as the volume underneath with smaller input of 5%, assuming you a 5% confidence. Now in the two-sided test, what we do is cover the smaller region on the left but also one on the right, such that the area on the left doesn't exceed half of 5% that is 2.5% and the same for the area on the right. Now, we've moved the critical region into two areas. We have a two-sided test now that's called a two-tailed test and in totality, the two critical regions on the left and right don't exceed 5%. This looks awfully like a confidence interval, right? Let's flip the coin. In 14 experiments, heads comes out exactly 3 times and 11 times it comes out tails. Let's do the analysis. So in this table, I've graphed to you the probabilities under the binomial distribution for each possible outcome, from 0 to 14 as before. We go 0, 0.005, 0.22, 0.06, 0.12, 0.18, 0.21--this is obviously the most likely outcome for the null hypothesis and then it goes down exactly the way it went up over here. I've also added check boxes. I want you to check exactly those that define the critical region, so the total probability in the critical region does not exceed 5% and remember this is a two-tailed test. This one was tricky--you're going to count start on the left and on the right and these points over here defined the critical region. If you were to move it inbound, you'd find that 0.022 plus 0.005 becomes larger than 0.025 with just half of the confidence that you have to exclude over here. Now since we know that these must smaller or equal to 2.5% each, we have to exclude 3 and 11 from the critical region, and 3 shall be fine for these two sample tests. You only worry about the following--is p smaller than 0.5 as the alternate hypothesis? Then these shortest times over here would now define the critical region? And here is my answer--the sum of those guys over here is about 0.287 and that's smaller than our 5%, and in this case, you would reject the number of heads equals 3, but what about the coin. I often don't have a one-sided hypothesis--I have a two-sided alternate hypothesis. Now you're going to put everything together for me, and I'll give you a new problem, and you'll answer all the questions for me to demonstrate to me that you fully understood what I'm talking about here. A treatment against cancer is advertised by the manufacturer to work in 80% of all cases. Your suspicion is that the drug doesn't work as well as advertised. Suppose 10 patients have been treated for this drug, what's the most likely outcome, so to speak, of healthy people? That's a simple question. And obviously the answer is 8. 8 out of 10 is 80%. Now, we suspect the drug doesn't work as well as advertised, here is the hard question. Where does the credibility region end? Put differently, what is the largest number of healthy people in the critical region or that define the critical region? What's the largest number of healthy people out of the 10 where you'd be suspicious and you wouldn't be 95% confident using our 95/5% confidence threshold. You'd really reject the claim made by the manufacturer of this drug. The first thing we do is we notice it's a one-sided test--that is, if this drug was even better than advertised we're not really that concerned, to be honest. Then we graph the binomial just like before. Here are the outcomes and here are the probabilities. Obviously, 8 is the most likely outcome, assuming the null hypothesis is correct. The critical region is not defined from the left side, which is fewer people are healthy and the treatment doesn't work as well as advertised. Let's add up until we reach 0.05, which is right over here. The sum happens to be 0.032, which is below 5%. So 5 is the correct answer over here. Now in our final question, I now give you the observed outcomes. Five people were healthy and five remained sick without a change of the cancer in the N = 10 experiments. My question now: do you reject the claim that this treatment will heal 80% of the people? Yes or no? And the answer is yes. Five healthy--that's the 5 to look at, not the other one--falls into the critical region. In fact, it's included in the critical region that goes from 0 all the way to 5, and therefore we reject this claim over here at a 95% confidence level or 5% error probability. This is cool! You made it! You now understand the very basics of hypothesis testing. We talked about critical regions. We talked about null hypothesis. We defined the critical region in the one-sided test and in the two-sided test. And assuming that the null hypothesis was correct is the actual outcome fell into the critical region, we become suspicious and reject the hypothesis. Whereas, if it was 95% consistent with the null hypothesis, we'd be okay and that's the essence of hypothesis testing. Next time we'll tie this concept together with confidence intervals and some of the more advanced statistic concepts we've learned when sampling from data. We now come to the hypothesis test unit number 2 and we just put together what we've learned in the previous hypothesis test unit and in the unit on confidence intervals. This ends up to be the dominant way of doing hypothesis testing. We arrive at a very simple formula that we can even program. According to Wikipedia in the 2007-2008 season, the average NBA basketball player was 6 feet and 6.98 inches tall. That is the same as 200.66 cm or for the sake of this exercise, just 200 cm. Let me question this and see if this is actually acceptable. Go to a training game for NBA players. Here they are training, and I drag all of the players in the side and I take their height. As I take those measurements, I find, here the sizes I've observed. In fact, I got everything from 199 cm to 206 in 1 cm increment. And now, I want to ask the two-sided hypothesis test question whether I shall reject the claim that the average player in the NBA is 200 cm at a 95% confidence level. The trick goes as follows. You learned about confidence intervals, which is the mean plus/minus a factor taken from the T-table times a square root of the empirical variance over N. That formula you've not seen many times in practice. When I plug this in here with the appropriate (a), I get for the mean 202.5 ± 1.916. If you want to check this, I'm using 2.365 for (a). Graphically, what this means is if here is the 200 and here are my data points, the mean measurement of the sample sits over here and the 95% confidence level stops at 200.58, just short of 200, and that means we reject the hypothesis of 200 cm based on our sample of 8 people, because the confidence interval doesn't quite make it here. Now, I should argue that this isn't quite correct in practice depending on what town I go to, I might find a different sample and a different town. We say we didn't pick these people completely independently and it isn't accounted here. That's just a word of caution, but I think you get the principle. It's really simple. Use your confidence interval and check whether the outcome lies within or outside the confidence interval. Let me illustrate this again. For any sample, you know how to compute a confidence interval. If the null hypothesis is inside the confidence interval of the observed sample, everything is okay and you believe null hypothesis. Conversely, if the null hypothesis falls outside, you don't believe null hypothesis and you accept the alternate hypothesis. So simple is the hypothesis testing and you use confidence intervals instead of, for example, the binomial distribution and that's really the common case. Let me just briefly summarize what we're doing. Given the sample of data, you compute the mean, you compute the variance, you get the t-value at some desired error probability p and you have to make sure you pick the correct one if it's one-sided, and pick a different one for the two-sided. Then the plus/minus term in the confidence interval is simply this number over here times the square root of the empirical variance for N. Then I would have to be really cryptic but I've just given it to you, but by now, it should make a lot of sense and because we practiced every element of it. The mean, the empirical variance, getting a number from a table, and then compute the size of the confidence interval which we then put into the plus/minus terms. So µ minus this is the lower bound µ plus is the upper bound of the confidence interval according to the confidence level specified for the error probability p. Let's put this into action--a dance club operator advertised the fact that the average age of its client is 26. You walk in and encounter the following people--four people are 21, six people are 24, seven people are 26, 11 people are 29, and two people are actually 40 years old for a total of N=30. Now, compute for mean whether you know to trust the statement that the average was 26. And let's do so. Obviously, the fact that there is two 40-year-olds is a little bit disturbing. Let's do so in stages and I want to guide you through this. First, let's calculate the mean. And the answer is 26.97. When you work it all out--that shows the answer. The variance--you've done this before. 19.57--that's, of course, truncated. What value we will be using for a two-sided 95% confidence interval. This is from the counter table with very large interval here, so we can use a Gaussian approximation. Well, for 95%, we know the number--it's 1.96. We've seen it many times. Now finally, what is the plus/minus term in the confidence interval that results. I get the 1.91, so if we take 26.97 ± 1.91. We find that this claim over here is well within this confidence level. There is no reason to doubt that this would be a wrong claim based on the sample that we've drawn. Now again, just to pay attention here. In any given night there might be a reason that particularly young people come or particularly old people to come--maybe on the style of music played. Just taking a sample on one night is not fair as a statistician. It should be really independent, so you should go and in on random nights and take a random person each night. But leaving this aside, I hope you were able to follow this. If you are, you're now able to test based on data hypothesis and accept or rejection. This is a very powerful technique. In the next final exercise, you get to program all this. Here's our optional unit. It involves programming and what I want is really simple. I want to input a sample and a hypothesis and out should come a simple yes or no whether whether I should accept it. This piece of code should just do it, and for simplicity I assume 95% confidence and two-sided-tests. Here's what I'll do. I'm going to give you functions you already know. You've already programmed with the mean function. Here it is again. Notice the use of the word float, in case the list doesn't have a float happens to be an artifact of type conversions in Python, but never mind. Then we have a variance function that you programmed before, and here's the variance function in a very compact way. And then, I also should really implement the T table but I was lazy so I'll just give 1.96, even though we know that's not quite correct. I'm going to give you a list. This was the list of height in my basketball club and I print the mean. That is hard to see but I should indeed get 201.5. Give it a try. What I want you to implement now is the function conf, which is the plus/minus term in the confidence interval below and above the mean. If I run it for this data sequence over here from 199 to 204, I indeed get 1.366544. So back in your code, we left the function conf open and please insert your code right here. You can do this in one line. Here's my answer--we extract the factor and then just take the square root of the variance divided by the length of the list. This is exactly the formula I gave you. When you run this you get the desired answer. Finally, I will add the hypothesis for example h equals 200. I want you to write a function called test that accepts as input a sample l and hypothesis h and it returns true if we believe the null hypothesis in a two-sided test. Otherwise, it returns false. Insert your code right here. Here is my solution. Test is a function of the data l and the hypothesis h. We first compute the mean and then the plus/minus term of the confidence intervals, and if our actual hypothesis differs from the mean by less than the confidence interval size, we return true, and that is a two-sided test, hence the absolute involving the difference between h and m. If we look at this, this logical expression at the bottom will only trigger, if h deviates way too much more than c from m, and so it goes. Down here is the output of our example from class. 201.5 was the mean. 1.366 was the size of the interval, and we got a false. We don't believe h zero. If I change my hypothesis to 201 and one will test again, we now get a true as a result of this test. Great. You programmed confidence intervals and you programmed a hypothesis test. That is actually really cool. If you programmed it, I bet you now really know how it works. So welcome back to statistics 101. Today, we're going to learn something insanely cool. You might remember from the very beginning of this class that we had data sets that involve more than one dimension. For example, the size of a house is relative to its price, and initially, we talked about scatter plots, we talked about bar charts but we didn't talk about what's the holy grail of statistics, and that is fitting a line through the data points. After this unit, you'll be able to fit a line to data points like those and you'll even be able to tell me what the residual error is in that fit, and that allows us not just to understand the data but also to make predictions about points we've never seen before. So let's dive straight in--so let's talk about lines and let's talk about the technology to fit lines to data called linear regression. It has two dimensional data such as the age of a person and the person's income and this is obviously made up data. Then linear regression tries to fit a line that best describes the data. So how do we specify a line--suppose we call the horizontal x as x and suppose we call the vertical axis y then the lines commonly described with a functional relationship between x and y of the following form y=bx+a. So let us look at lines. Let's take this simple example y=2x. Let's add units to the axes of coordinate system and let me draw 3 different examples of lines. The blue line, the green line, and the red line and for this example y=2x. Can you imagine which of the three lines would be described for this equation. Please check one of these three boxes. And let's look at this--if x=0 then y=0. So he has to go through the origin. Let's takes out the red hypothesis over here. Now if x=2, then y is 4. That means the line goes to the point 2, 4 and of course, 1, 2. That makes the green line the correct choice here. And just for kicks, let's try this again with a different equation-- a and b are now -1 for b and 4 for a that describes a different line. I'm going to give a couple of choices. Here's one, here's another one, and here's the third. Which one is best reflective of this equation over here? And I hope you got this right. When x=0, then y should be 4, which is the point over here. If x=4, then -4 + 4 cancels out. Y = 0--it's the point over here. So red would have been the correct answer. A final quiz, now we get to determine what a and b is in the line of 1 where x=4 assumes the value y=2. Now if x=0, it assumes the value of y equals what? What's a and what's b for the blue line? Well, you always start with a, because you can get a by looking at the case of x equals 0 and then b doesn't matter. So if x equals 0, so it means you write over here, then y equals a that falls us directly from this question because x equals 0. So what's the y value over here? It's 1 so that means a is to be 1. Now that we know this, you can look at b. We know that if x = 4 then the expression on the right over here gets us a 2 in the y dimension. We really know a is you can put the a in which gives us 4b+1=2. We bring 1 to the right side. Therefore, 4b=1 and then we divided by 4, which means b equals to fourth or quarter and 0.25 and 1 other correct answers here. So now you understand about lines. Let's talk about linear regression. In linear regression, we are given data and data has still has more than one dimension. We have just studied for two dimensions throughout this class. So this might be the data and we're looking for the best line that fits the data. We'll put differently the parameters a and b that we discussed before that defines the line. The word best is interesting. Obviously, it's impossible to put a line through the data points over here. These are what's called nonlinear data that go up and down. Data often looks like this even if the relationship between x and y is linear-- that's usually what's called noise in the data, some amount of randomness you can't explain. So in finding the best fit, we're trying to find a line that minimizes the difference between the data and the line in the y direction and that's somewhat counter intuitive. You'd normally think the distance is given by these lines over here in red and that is the shortest distance irrespective of x and y. But in linear regression, it turns out that we're minimizing the distance just in the y direction. And there's a lot of theory why this is a good idea. In summary, we assume that our data is the result of this blank some unknown linear function, bx+a+noise and if this noise is assumed to be Gaussian, they're minimizing the quadratic deviation between the data points and the line happens to be the correct mathematical answer. But leaving this aside, what we're doing is we are adding over all data points the difference between our function and the y value of the data point with the square and that distance is the distance we're minimizing. So let's look into this with examples--for the following four data points, which of these lines has the smallest quadratic error and would be the best regression line--pick one of the three. In this case, it's easy--it's this one, because it seems to describe the data almost perfectly whereas the red one has substantially an error as does the green curve even more so. Let's look at more data, which line would you pick? The one in the middle, the red one, the blue one, or perhaps any of those. Perhaps, it is all equally good. It turns out green is the right answer. Let's just look at this. The blue one suffers no loss for the three points over here but a fairly substantial loss for the other three data points. Let's call this loss c. So, the blue curve would have an error of 3c²--if this distance here is called c. The reason why it's squared is because we have three of those and we're using the quadratic distance. Similarly, the red one has the same problem, 3<i>c² for the red one.</i> Now, how about the green one? For the green one, we have errors for all six points. But now, the amount of the error itself is half as big as c. So, we're going to write (c/2)². And when you work this out, you'll find that this is 6/4 c² is the same as 3/2 c². So, the total error for the green one, the quadratic error, is half as big as it is for the blue one and that's a surprise. It has to do with the fact that in a quadratic version of the error, large errors count much, much more than small errors. So, green is the best regression line we can find for this data over here among the choices I've given you. Let's look at another data set reaching x and y, and now I'm going to ask you a different question, which is the parameter b, is it negative or positive? Assuming that this is 0, 0, and say this is 10, 10. I ask you the same for the parameter a. Is this negative of positive? In either case choose exactly one answer. And once again, a must be positive. If we draw the line, a is the value of y intersection, where x=0. Remember, all linear functions are performed bx+a, so if x is equals 0, and this term falls out, and we have y=a, so this is the value of y. It must be positive, it's above 0 and b happens to be negative. For any graph that's from left to right that goes down, b is negative. It's easily seen as x increases, as you make x larger, you make y smaller. That's a negative correlation as we'll call it in the future unit but the graph goes down. That's good. Now we understand a lot about this formula over here. Y = bx + a, the key holy grail in linear regression and in much of statistics is how to use data to determine the value of b and the value of a. So if you can do this with data, then we solve the problem of fitting the best line and that's once again called linear regression. So I won't give you the derivation but I'll give you the formula. Let's start with b. Assuming your data comes in pairs for x and y as indicated here and the formula for b might look really complex at first but I promise you it isn't. Just in the calculations of the variance, we take the difference between x and the mean of x, but rather than squaring this as in the variance case, we now do a product with the same term for the y's. So important here, this notation x-bar is the mean of the Xi and x-bar is the mean of the Yi. Previously, we would have called x-bar µ but now that we have two variables we're going to use the bar notation. Let's go back and look at this formula here. When we computed the variance, we would have taken (Xi - X-bar)² but here we're taking the product of Xi - X-bar multiplied by the y direction of the same thing. So it's similar to computing a variance. The last thing we do is normalize this thing. Before we often normalize with n but now we're going to normalize with something else. We normalize with the term that very much reminds us of the variance. Here's our formula again. Let's assume I have the following the data. There's an x variable, y variable, and we have four data points. 6 goes to 7, 2 to 3, 1 to 2, -1 to 0, and very quickly we can just look at the data diagram. In this case, a scatter plot. So here's the x axis and here's the y axis. And if I plug the data, 6 goes to 7, just about here, 2 to 3, 1 to 2, and 1- to 0, not surprisingly this data is in fact, linear. Whatever nonlinear you see is because of my poor drawing skills. And if you analyze carefully, you already saw that y is always respond larger than x. So before we apply the formula, let's just quickly guess what b and a would be, if y is always found larger than x. What are the coefficients of b and a? And it's 1 and 1. So, the way I get these numbers, 1 and 1, is really easy. I just write down that y is always larger than x by 1. When you look at this, that just means a = 1 and b = 1, because you can think over this 1x instead of just x over here, and this gives us b equals 1. But, I'm now going to do this with the formula up here. In particular, we have four things we add in the denominator and also four things we add in the numerator. It's four because there are four data points over here. Before we fill in this data, I'm going to ask you a different question, one that is easy for you to answer, and that is what are the means for x and for y in this data set. You will need them when you fill in these blanks over here. So, please give me just the two numbers here in green. The axes add up to 8 and then 8/4 is 2; hence, the mean for x is 2, and for y, you find it to be 3. They add up to 12/4 is 3. So these are the means. Let's now fill in the very first value on the left over here, which is for the first data point. This expression shown over here. Please go ahead and fill in the first value. And my solution is 16 and the reason why is 6-2 is 4. So the first expression results to 4. And the second expression, the y expression also results to 4, 7-3 is 4, so 4<i>4 make 16.</i> Let's now go and add the three next values over here. To me, the answer is 0 for the first one, 2-2=0. So the first expression is 0 and the second one is going to be 0 as well. The next one is 1, 1-2=-1. For the second expression, 2-3=-1. -1<i>-1 gives us a 1.</i> The last one would be 9, -1-2 is -3 for the first expression. 0-3 is also -3 so we get a -3<i>-3=9.</i> Now, I want you to add the values down here and those would look really familiar because you've done so many versions of it when completing variances from data. Go ahead fill them in. And surprisingly, they happen to be exactly the same y as in the top--y. However, this specific data set, you get for example that 6-2 this expression over here is the same as 7-3. If we look at this carefully and work these things out, we find that 16, 0, 1, 9 set are answers. What do we get for B? Please enter the answer here. And the answer is one1 as we expected. Now that happens to be a trivial example of a two reasons. One is b=1 is the most simple case of the linear relationship, but secondly, there is no noise. This equation of b = 1 will fit to data exactly, but we did it to exercise the use of the formula. The final thing I want to teach you is how to calculate a. There is a simple formula as well, and the insight here is you already know b and we know that bx+a results to y for all linear function. Now, what happens to be true is if we take the average x and the average y, then this equation is still correct. Now, we can resort the terms--a=y-bar - b<i>-bar, so that's the formula for a.</i> Now, the formula for b and we have the formula for a. That's really cool. Let's apply this here and see what your value for a would be. Just give me the answer based on plugging the means into the formula over here using the appropriate b. And the answer is 1. The mean of x is 2, the mean of y is 3, b=1 3-2 makes 1. So we've recovered using these formulas, but we kind of already knew about the data, mainly that y is x+1--that is a=1 and b=1 because there's exactly 1x over here in this equation, so let's make this more complicated. And you now can give me easily the value for a in this box. And the value here is 15. Welcome to Unit 2. Today we talk about the most important aspect of any statistics person. I want to quiz you what it is, knowing that you can't know it. But I want you to ask you first before I tell you. What is the most important thing a statistics person does? Look at data? Program computers? Run statistics of the type we'll discuss in the future obviously? Or eat tons of pizza? Just check one of those boxes. Now I can tell you I'm tempted to check mark "eat pizza," but that isn't the most important thing a statistician does. The most important thing is to look at the data. I can't over emphasize how important this is. A great statistician differs from an okay statistician by the ability to spend a lot of time just looking at data. So, here is another data set. We have the sizes in square feet listed on the left, and the corresponding prices in dollars listed on the right. and I want you to look at the data. Particularly I want you to tell me is there a fixed dollar amount per square foot? Yes or no? This data set makes it easy. The answer is no. There are two houses of the same size that sold at different prices, so it cannot be that there is a fixed dollar amount per square foot. How about I change the house over here from 1400 square feet to 1300 square feet. What is the answer now? It turns out now the answer is yes, and as I'm sure you verified, the cost per square feet is 70 dollars. Obviously, the data carries a lot of information. I'll now teach you how to visualize the data, using a simple trick called a scatter plot. For that, I want you to take a piece of paper and a pen or a pencil and arrange the data in a graph where the x axis is the size, and the y axis is the price. In a scatter plot, each data item becomes a dot. If we graph different sizes horizontally and prices vertically, the very first data item can be graphed as follows. You draw a line that's vertical at size 1400. Every house on this dotted like is of the same size--1400. You draw a horizontal line at the price of $98,000, just below $100,000. Where these two lines meet you have your very first data point. This dot corresponds to the very first house in your list of data. Let's do the same for the second house. And again, the size is 2400. The price is $168,000. This is the right answer. Let's do this for a third time for the third house on the list. Which of the boxes represent best the third house? For a house of size 1800 square feet we pay about $126,000. This is this the dot over here. I think you get the story. In a scatter plot, each data item becomes a point. We conveniently chose a 2-dimensional list to make for a 2-dimensional scatter plot. These are the most popular scatter plots because surfaces like paper are 2-dimensional. It's really hard to do it in like 125 dimensions. Being as it is, when I draw in all the six data points, I get a scatter plot like this. That's a nice scatter plot, because I can draw a line straight through all of the data points. Now when this happens and there's a relationship that's governed by a straight line, we call the data linear. Now, linearity is a rare concept in statistics. Very often you'll find deviations, and that's because the size of the house is not the only determinant in the cost or perhaps because most of us are bad negotiators. But when a data set is linear, it's really easy to predict prices of houses in between. For example, a house of this size ought to cost this much. You can read out of a scatter plot the dependence of the size of a house and its price. Here we are--we're actually looking at the data. We're doing what a statistician ought to do. I'll now ask you to make your own scatter plot. Please graph this new data and tell me based on the graph-- please really draw it on a piece of paper-- whether the function between price and size is linear-- that is, can you draw a line through all the scatter plot data on your diagram? Is this linear? Yes or no? To answer this, let me make the scatter plot. Here is my x-axis, which measures size. Here is my y-axis, that depicts price. If I graph the different sizes, I get those 6 data points over here. If I graph the different prices, I get the lines over here. Sure enough, connecting those things get's me a data set that happens to be exactly linear. So, just checking do you believe there’s a fixed price per square foot.Answer, yes or no? And yes, it’s US$30 per square foot. Let's do this again for different prices. I just changed all the prices of those homes. Let me ask you the same question. Do we believe there is a fixed price per square foot? Check yes or no. The answer is no. If you look at the first data point and divide the price by size, you get approximately 31.1765. Compare this to the next data point, which gets you 30.9524. They're about the same but not quite. In fact, they're all different for the different data points, as you can see over here. Let's make a scatter plot. Please take a piece of paper and graph the size relative to the price as you are now used to in scatter plots. My question for you is when you graph this is the data linear. That is, can you fit a line through the scatter plot data? Yes or no? Please graph it very carefully and then make your judgment. It's a non-trivial question. Amazingly the answer is positive. Let me graph the data--first data point, second, third, fourth, fifth, and sixth. While my hand drawing isn't perfect, if you're careful you'll find out that they're just linear. That's amazing--even though there is not a fixed dollar price per square foot, the relationship is linear. Here is a a really challenging question. In this data, the prize is linear in the size plus or minus a constant dollar amount. Can you fill in those values? I should say this is a nontrivial question, and I don't expect you to get this right, but it's a wonderful, wonderful preview of what I'll teach you to do in this class very automatically using really amazing pieces of statistics. The answer happens to be the square foot costs $30, but there is a constant cost added of $2000. If you plug this in, you'll find, for example, that 1700 times 30 gives $51,000, but if you add the $2000, you'll correctly get $53,000. Do it again with 2100 square feet over here multiplied by $30, add your $2000. You get $65,000 and so on. Now, again, arriving at those numbers is nontrivial. I wouldn't be surprised if you weren't able to complete this exercise. But if you did, you're quite amazing, because that is the relationship between the price and the size in my example. In a final exercise, I'd like you to scatter plot the data that is modified, by modifying two of those prices, and please again, plot the data, and answer for me the question, whether you can fit a line, or for differently, "is this linear?" I really want you to go with a piece of paper to draw those points. The answer happens to be no. If I draw the original blue data points, the data is indeed linear, but this data point over here of a house of size 2100 square feet sold remarkably cheap, so we get the point right over here. Similarly, the house of size 1300 square feet sold for a lot. Both of these data points are outliers. We'll talk about outliers later. There is clearly no way to fit a linear function through all those data points. I would even question that this wicked kind of function that I drew is a good accurate representation of the relationship between size and price. You would not expect that as you go south of 1500 square feet that the price would rapidly increase and then decrease again because there is likely nothing special about small houses than justifies the price over here. You finished Unit #2, and you now know a lot about scatter plots. They tend to be 2-dimensional, and real-world scatter plots look often just like this where a simple eyeballing tells us something about the relationship of one variable to another. Now, scatter plots aren't great when there is what is called "noise." That is, the data deviates from the exportation in some random, noisy way. In the next unit, we talk about a simple plot called "bar chart" that addresses the issue of noise in data by pooling data points into a single cumulative bar. So stay tuned. In this unit, I'll teach you about a term called correlation. It's an important statistical term and in the end of this unit, you'll be able to use it yourself. Here's the fundamental problem. Sometimes, there are lines done very nicely like these points over here. Other times, two variables seem to be utterly unrelated. Correlation is a measure that lies within -1 all the way to 1. That tells us how far the data is described by a line. In both cases, you can fit a line but in one case it would be a really good description of the data whereas in the others don't. So the correlation coefficient of what we call r is 1 if the data is perfectly aligned for the line. It's 0 if there seems to be no relation between the two different axes and the data, and it can also be -1. In the case where the data in fact is still perfectly aligned but there's a negative relationship between one variable and the other variable. Let me see if you got this. Here are three data sets--in one, we have a strongly positive correlation; in another, it's above zero, and in the third one, it will be a negative correlation. Which of these cases best describe those conditions and use each condition exactly once here. And the answers goes like this--the reason being that here we have a clear line. In fact, r will probably be one and it's positive. Both variables grow at the same time. With this U shape, there might be a dependence but it's not linear. We fit the best line, it's going to be flat, and in this flat line there is really no dependence between the x and the y variable. So definitely knowing x does not tell you about y and vice versa that tends to be r=0 and in this last case, there is a negative line. The best line fit will go something like this and r might be as small as -0.2, but it is negative, the reason being that the line down here goes down negatively. Let me ask a different quiz--suppose you run linear regression, and you found that b=4 and a=-3. To remind you, this describes the following linear relationship between x and y. My question is about the correlation coefficient r. Will r be positive, negative, zero, or can't we tell as in can't tell?Check exactly one box. I should say this is not an easy question. Any answer is positive. If you look at whatever the data was, this is roughly how this line looks like, and that means in the data, there is a tendency as you increase x there would be an increase of y, so that means the correlation will be positive. We can't tell what value it is. If the data fits exactly onto the line, then it would be one. It could also be that the data has enormous deviation from this line, and this is the best fitting line, in which case the correlation coefficient will be still larger than zero but it might be much closer to zero, depending on the amount of deviation of those points from the line. To summarize the correlation coefficient, which you're just about to learn about, is a value between -1 and 1, tells us how related or correlated two variables are and both 1 and -1 stand for perfectly linear data. In the case of +1, we know that this line increases in x and y simultaneously whereas for -1, we have the inverse effect. Let's compute r--my favorite way to compute it is very similar to the way we computed b in linear regression. It looks like the sum of all data points and takes the product of (xi-x-bar) and multiplies for each data point this with (yi - y-bar) and then, we have to normalize. This could be any value. It isn't between ±1. We normalize by a √(x-x-bar)² sum of all i's times the same expression for y. Now this looks a little bit wild, and this is probably the worst formula you've encountered in this class in terms of complexity but once you dived in, you'll realize this is really related to a lot of stuff you've seen before such as variances and similar. This is the quintessential term that occurs in the variance calculation of x. All that's missing is the normalizer. Same over here, this is the variance of y--this thing is the normalizer. We take the product of the variance of x and the variance of y modally with the normalizers and we get something quadratic even in variance space. This over here is kind of a mixed variance. This is often called the covariance. But notice that there is also a normalizer in this thing over here. In fact, the missing normalizers on top and bottom of this bar can't see each other out; hence, I just omitted them. But this one is just like the variance calculation but it mixes x's and y's whereas these are x² and y². This is called often covariance if you've normalized, because it is the variance calculation of a two co-occuring variables. These are the variances. So what this really tells you is kind of the ratio how much these two things co-evolve, how much the errors correspond versus normalized by the multitudes of errors individually and whether the ratio becomes 1, we have a perfect correlation. When the ratio becomes 0, then the numerator is 0 which means our errors cancel each other out. That is very different for x and for y under any linear model. So this complicated formula is what's called the correlation coefficient r. So let's try this out. Let me give you a data set. x=3, 4, 5 and for those x's we get 7, 8, and 9. The first data item would be 3, 7. Second 4, 8. Third 5, 9. It's easy to see that the mean x-bar is 4, mean y-bar is 8 and this gives us x - x-bar and y - y-bar, the new numbers -1, 0, 1 and -1, 0, 1 here again. So these are three mean and normalized data points. Let's now compute these three values over here for this example. Give me the first one. The answer is 2--if we multiply the data point the first expression and the second expression get -1<i>-1 which is 1, 0, and 1 again that adds up to 2.</i> Please calculate the expression over here and put it in this box. And the answer is 2 again. -1² +1² =2. And the third one will give you the same exact 2 as before. I just do this for you. And we now work it out. What do you think is the answer? And yes the answer is 1. 2*2 is 4. The square root of this is 2 again. 2/2 gives us 1.* Let's now work with a different data set. We write 2, 5, 8 for y, which gives us the mean for y. Before doing any calculations, let's take a guess what r might be. Is r is going to be 1, r going to be 3, r going to be 2, or is r going to be 0. Check one of these boxes over here. One is actually correct, and by virtue of what I told you before, you can figure it out that r has to be 1. The reason is we know that r is between -1 and +1 so it can't be 3 or 2. And 0 is kind of this pessimistic case where there's no relationship whatsoever between the data but clearly this data fits in the line. In fact, when it fits on the line no matter what the steepness of the line is if it isn't flat, if there's a positive relationship no matter how small or how large, it's going to be 1--so this is the correct answer. Let's see if you can find this. I filled out this table for you. I know you can use this table to calculate the numerator of the fraction over here. And the answer is 6. -1*-3 is 3. Add to it 0. Add to it another 3. It ends up to be 6.* What's the value over here? Clearly, it's 2. -1Â˛ is 1, 0, another 1. Add those together and you have 2. And how about the value over here? It's 18. The reason being -3Â˛ is 9. Add to it 0. Add to it another 9. We get 18. So what do we get down here? Well, 18 2 makes 36. Square root of this is 6. 6/6 is 1. We've just proven to ourselves that 1 is the correlation coefficient even for this data set over here. Now, how about we switch the order of the y from 3, 5, 8 to 8, 5, 3? The mean stays the same, but several of these values over here change. And before I compute it, let me again test your intuition. Is r larger than 0? r = 0? Or r smaller than 0? Pick one. And the answer is smaller than 0. There's a negative correlation. When x goes up, y goes down. I think if you draw out the data, you get something like this for x and y. And that data perfectly fits in line which makes me believe r = -1 is the perfect correlation down. Let me just fill in the table over here. 3, 0, -3. In our equation, this term doesn't change because it only depends on x and I haven't changed x at all. But let's compute the numerator over here. The answer is -6. -1*3 is -3 add to it 0. Add to it another -3. You get -6.* What's the value over here? And just like before, it's 18. We view all the numbers, but the sum is still 9+9. So what do...what do we get over here? It's -1. Again, 2<i>18 gives us 36. Square root is 6. -6/6 gives us -1.</i> This is the correct correlation. Now, let's do something tricky. Let's use 8, 5, 8 for y, which gives us a mean of 7 for y. And the following table down here 1, -2, 1. Let me test your intuition. Is r larger than 0? r = 0? Or r smaller than 0? Which means positive correlation, no correlation, negative correlation. Pick one. And you could do the intuition thing which is you can look at this and arrive at the point that there's no correlation and this is correct. It's a little bit tricky, but you can see x go up from 3 to 4 and y shrinks. Then it goes from 4 to 5--the opposite happens to y, and it increases by the same amount. That means in our data set, it will look as follows--up, down and up, and then they happens we already saw the best fit is the horizontal line, and the horizontal line just means x and y are completely independent for this best linear fit. Knowing x does nothing about y and knowing y does nothing about x, and this kind of independence leads to a coefficient of 0. Let's check it. What's the field over here? It's 0. It's 0 because -1+ 0+1=0, That's interesting even though this field over here ends up to be 6, 1²+2²+1². Zero of anything gives us a 0 at the end. So let's look at one final case. This is our data. Now y goes from 8 to 3 up to 7. It look something like this--8, 3 back to 7. Clearly, it doesn't look very correlated. Check your intuition. Is r large than 0, equals 0 or smaller than 0. And the answer ends up being smaller than 0. You can see from the data points if the blue one guy over here was up here, then the line horizontal line would be the best fit, but the blue is a little bit lower so it's slightly downward tilted line, and they end up being better like this one over here. So let's look at this and compute the mean y value, which is 6. I fill in this table for you 2, -3, 1 which is this row over here minus 6, and now give me the first value over here. It's -1. -2+0+1=-1. Give me this value over here. Now, I think it's 14--2Â˛ is 4 plus 9 is 13 plus 1 is 14. Let's go to the final value over here. What is it? And now we need a calculator--you have a square root of 28 and you divide -1 by that square root and gets approximately -0.189 when I work this out. Today's negative correlation but it's weak. The data really in't well-described by a linear function. If the data instead were to lie exactly on this line then r will be -1 with the negative correlation. In summary, you really learned about correlation coefficients. It's larger than 0. If there's a positive relationship between x and y. It's smaller than 0 if the relationship is negative. It's equal to 0 if there is no relationship. The magnitude of r goes to 1 as the relationship becomes increasingly linear without any noise or any deviation from the line. This is a powerful measure. For any data set with multiple variables, you can now tell how much variables relate to each other. If someone, for example, shows you the salary of a person and the age of a person, you can say they're really correlated, or you could say they're not correlated at all. Whatever you say, with this--what I believe to be a very simple formula, the formula right over here--you can now compute for any data set how much x and y relate. That's called the correlation, and it's really an important lesson in statistics. I use it all the time to inspect data to make a statement how much two variables relate to each other in a linear way. Thank you. Let's have some fun. This gentleman you probably don't know. His name is Monty Hall. Starting in 1963, for many years Monty Hall ran a game show called Let's Make a Deal. In the center of the game show was a puzzle that puzzles statisticians to the present day. In this unit, which is optional in the statistics class, I'll let you solve the Monty Hall problem, and you'll get a chance to program it to verify that the assertion, which is not obvious, is actually correct. Here we go. The essence of the game are three doors. Behind one of those is a beautiful car, but Monty won't tell you where the car is. Instead, all the doors are covered by curtains. The subject of the game is as follows. You get to place a bet. Say you pick door 2. If behind the bet you place you'll win the car. Otherwise, you'll win nothing. So far, so good. Here's the caveat. Obviously, between the two remaining doors there'd be one that doesn't contain the car. Monty is going to be mean. He's going to show you. He's going to lift the curtain from one of the other doors, and there is clearly no car. After now closing the curtain again, he'll ask you do you want to switch. That is, do you want to change your vote to the remaining curtain in the hopes of having an increased chance of winning the car. The reason why this problem is interesting is because when Monty Hall lifted the curtain he really didn't tell you anything. You knew in advance that one of the two curtains wouldn't contain the car. The mere fact that he lifted the curtain should give you zero information relative to where between the remaining doors the car is. All you know now is that door 3 doesn't contain the car, but why should door 1 now be more likely than door 2. Here's the question. What are the probabilities of the car being behind door 1, door 2, and door 3, provided that your initial pick was door 2 and Monty then lifted door 3 to show you there's no car? Let's start with door 3, and here the answer is 0. You already saw when the curtain was lifted that there is no car. How about door 1, and that is a really tricky question. For reasons that I'll explain in a minute, it's actually 2/3 or 0.667. It's entirely non-intuitive. Given that this is the case, what do you think is the number for door 2? Of course, this is 1 - 2.3, which gives me 1/3 or 0.333. When I now ask you, with these numbers in mind that you might believe or not believe should we switch. Yes or no? Should we go from door 2 to door 1 to increase our chances of winning? Of course, the answer is yes. That's an interesting phenomenon. You placed a random bet. You had a third chance. Monty would lift another curtain and it feels like he's giving you absolutely no information, because you know such a curtain exists. He just shows you the obvious. Yet in doing so you will always switch--every single time. You won't even think. You'll blindly switch to the remaining curtain. By doing this, you'll double your chances of winning. Why is this so? Well, let's build the truth table. For the true location we have three choices, and because this is a truth table, I write them multiple times. We happen to know that any of those has exactly 1/3 probability, and we'll be using this 1/3 in a minute. Now say you guess door number 2. With this guess Monty will show me an empty door. He might show me door 1, 2, or 3. Now I have all the combinations of the true location and the door I'm being shown. Let's talk about the probabilities of each of these entries. If the true location is 1, which has 1/3 chance, Monty hall will not show me location 1--probability 0. He won't show me location number 2, because that's the one I guessed. The only location he'll show to me will be automatically location 3. It's not the true one. It's not the one I guessed. It's the only empty one that remains. The probability of showing 3 given that the true location is 1 is certain, but the true location is 1 only with a 1/3 chance. This specific outcome that the true location is 1 and Monty Hall shows me 3 has exactly 1/3 of a chance. Now, but analogy, you can now fill out for me the three probabilities that correspond to the true location being 3 and Monty showing me 1, 2, or 3 into this these three things over here. The answer is 1/3, 0, 0. I guess you can write this as 0.33. The analogy is exactly as before. Clearly he won't show me door number 2, because that's the one I placed by bet on. Three he won't show me because it actually contains the car. So he shows me 1. With absolutely certainty, he'll show me 1. Yet, this case is only 1/3 likely, because it's only 1/3 likely that the car is behind door number 3. Gives us 1/3. Now comes the tricky part, and I want you to answer it for me. What is the joint probability that the true location is 2 and Monty chooses to show me 1? What is the joint probability for the location be 2 and Monty shows me 2? Two and three, clearly, the true location in this case is 2 and I've picked 2, so in this specific situation Monty has a chance to choose from the two other doors and say he picks them at 50% chance randomly. Here is the answer--a 6, 0, and a 6. The way the 6 is calculated is by a product. We are in this position the 2 is the true answer with 1/3 chance. When 2 is the correct answer, he'll never pick 2, because he'll just show me that my guess is correct, so he'll pick 1 or 3. Each of those he picks with half chance. As a result, for the joint of these two things to occur, you multiple 1/3 by 1/2 and get 1/6, and the same over here. This is the truth table for any combination of the true location and what Monty shows me, provided that I pick door number 2. Now let's say Monty picks a door and he picks door number 3 as indicated by these bars over here. Put differently, because Monty picks door number 3 all the other cases of picking door number 1 or 2 now have 0 probability. Let me just cross them out. The only events that remain in the truth table are this one, this one, and this one. Here the true location of the car is door 1, door 2, and door 3. Unfortunately, these numbers don't add up to 1. They are a 1/3, 1/6, and 0. I want you to tell me what the true posterior probability is for the true location to be 1, 2, or 3. Please write those numbers down over here. The answer is 2/3 or 0.667, 1/3, and 0. This is a mere matter of normalization. If you add 1/3 and 1/6, there are 2/6 and 1/6. We get 3/6 which is 1/2. These two things over here add up to 1/5. To make them probabilities, we have to multiply everything by 2, and we get those numbers of here. What this suggests is by simple logic of the truth table door 1 has no a 2/3 chance of being correct. Whereas door 2 has only a total of 1/3 probability of being correct. Door 3, of course, because that's the one that Monty Hall lifted, has 0 chance of being correct. That's actually a proof. There's an easier version of the proof, which is more intuitive. It says before you saw the empty curtain the chances were 1/3 each. Once you see the empty curtain, you learn nothing. The chance of the bet you placed is still 1/3. Therefore, the chance that the other door must have gone to 2/3. This is actually correct, but when I read this first, I didn't think it was plausible. I really had to go through the truth table. Let's now go and program it. I want you to program this now. I want you to run 1,000 simulations using a function simulate that you have to fill in. In this simulation, you count how often you succeed in the variable k. You pick a random door using the function randint. If you give it the parameter 1,3 it picks a number between 1 and 3 at random. You make your own choice using the same function randint, and then you have to emulate Monty. Sometimes it picks it deterministically. Sometimes it picks it stochastically. Write that code and then flip your choice of the remaining door. If that remaining door is correct, implement k. Otherwise, don't increment k. When you run this 1,000 times, your output should be approximately 2/3. This program requirement requires real knowledge of Python. If you're new to all this, feel free to skip it. It is a big challenging. Here's my solution. First, I use randint to assign the true location of the car. My initial guess, called guess1, is also chosen at random. You see these independent random choices with equal probability. Now there are two possibilities. Either truth is equivalent to guess or it isn't. When truth is equivalent to guess, Monty will pick one of the two other doors. Let's just implement this. I will let Monty pick a random door using randint. Of course, he might have picked the correct door, which contains the car, which is also my guess, and when that's the case I just rerun the random number generator until it finally finds a door that is not the true door or equivalently not the guess. That this pick. If we look carefully, this might run a number of times. But when it returns, he picked one of the remaining doors at 50% chance. In the else clause, the truth and my guess are not the same. So, Monty now has to pick the remaining door with probability 1. How do we get get Monty's choice? Now I'm going to do a little cute trick. I know that the sum of all doors--1 + 2 + 3--equals 6. If I subtract from 6 the truth and my guess, I get the third door. This equation--6 - truth - guess--will give us the number of the door that isn't truth and isn't guess 1. To flip my door, I can now do the same trick I did over here. I pick the one that wasn't my initial guess, which is subtract guess1 from 6, pick the one that isn't Monty. That gives me guess 2. If that guess is the same as the 2 door, I increment k by 1. Then I return the final number over here. This gives me a single simulation. I run through this n times. So, I added a loop for i in range n, move everything to the right a little bit, and then this simulation is run n times, returning my desired ratio of the successes over the total number of experiments. Let's run it. When I run it 1000 times, I get a number like 0.672. That's not exactly 2.3, but obviously there is some randomness involved in the simulation. If I increase N to 10,000, I get a number slightly closer to 0.66. Do it 400,000, I get 0.66748. That is awfully close to 2/3. It's really interesting to see that the switching helps. Put differently, say I use guess1 as my final guess to compare to the truth as opposed to guess2 and run this. I get 0.331. The interesting thing about guess1 is--the reason why it works is-- all the code from if truth all the way to here--guess2--is now irrelevant. I can comment all of this out here in the middle, which I've just done, and just retain the assignment of the truth, which is a random number, the assignment of guess1, which is a random number, and then I just check where these two agree. Clearly, the probability must be a 1/3. All this story about showing the open door has no effect on this. That little thing here that I simulated and when I retained the original guess then all the story about opening the door has no effect on the chances of my guess, prove that the chance is to be 1/3 for the original door. Therefore, switching goes up to 2/3. Isn't that cool? This is the story of Monty Hall and the very famous quiz. Welcome back to Statistics 101. Today, we do a case study. The case study is based on data that you, the students, submitted where on Facebook you all discussed my weight, and you've all see me in this class and a whopping 163 of you followed this very hidden Facebook discussion and submitted your guesses. Thanks to all of you, I now have a data set. In my first study, I will treat your guesses as my data or sample and somewhat abusing the nature of my own weight. I treat my own weight as the hypothesis H0. Technically speaking, this is a bit of an abuse. We're not really testing whether my own weight is incorrect, but you can still use the same math and have a lot of fun. Towards the end of this exercise, you will actually find out my weight. Please don't tell anybody. Here is the actual data that was submitted by all of you. One of the really interesting findings here--some of you had some fun. In the data, you find a guess 1 10²², which is about the weight of the planet Pluto in kilograms. Then there are three counter guesses with -1, which makes me suspect that these numbers were submitted by the same person. Some of you think I weigh 0 whereas others think I weigh 1000 kg--as much as a car. Thank you so much. Very kind of you. Let's take this data, and I'll ask you the first quiz. We have 163 samples. Remember the outlier removal method using quartiles? How many samples survive? The answer is 83. If you remember this method, you realize there were four quartiles, and individual elements separate those. This one's called the median, the lower, and the upper quartile, and these guys are the ones being picked. Now, 163 - 3 special elements makes 160, divided by 4, makes each of these 40. We pick the two center 40s, plus these three special elements and get 83. For these 83 elements, I'll tell you that the sum of the values is 6618.47. The sum of the square is 528,679. Now, you can work this out yourself, but then you have to go through a awful lot of numbers, so I just did this for you. Give me the mean. The answer is 79.74. The way you get that is divide the 6618 by 83. How about the variance? The variance is about 11.1 in approximation. You might remember that we can calculate that is exportation or the mean of the x² minus the mean of the x to the square, which I just did here. Now, give me the standard deviation. That is 3.33, the square root of the variance. Now I want the plus/minus term, the two-sided confidence interval at a confidence level of 95%. This is the number we can add or subtract to the mean, so that at 95% confidence the true mean lies within the interval. Please enter it here. The answer is 0.72. The way we get there is to take the variance and divide it by the square root of n--83. Then factor in the magic number 1.96, which is the two-sided 95% confidence interval number for the normal distribution. That gives us 0.72. Here is now finally the revelation. My weight is obtained if you assume as standard score of 2. It so turns out. Can you give me a good guess what my weight will be if the standard score is 2 for me? I should add that the mean over here is the heavier of the two numbers. So, I don't quite weight as much as here. The answer is about 73 kg. 73.086 would be the exact number, but I'm happy with 73. Let's do something that isn't quite correct semantically but we can do anyhow. Suppose the null hypothesis is 73, and with a two-sided test, using a 95% confidence threshold would we accept or reject the null hypothesis? Just pick one. The answer is reject, and you can see it easily from the data shown here. 79.74 is the mean. This is the 95% confidence interval--less than a kilogram. The reason why it's so small is because we have so many data samples, really interesting. As a result, we don't even come close to 73. So, if all the guesses that you guys voiced online were actual measurements using the actual scale of Sebastian, and this is what I told you and the question really was H0. Is Sebastian telling the truth or is he lying? The, using statistical techniques, you should conclude he's lying. This is somewhat contrived. This is the only data point, and everything else is just guesses. But I hope you had fun using actual data that you students derived in answering this question. Welcome back to statistics. Today we do a case study in regression. In this case study, I will use again the data that you all submitted about my current weight called L and my previous weight from last year as you guessed it all in kilograms called P. Here is my question for you as a well-trained statistician. What is the very first thing I'll do? Compute the mean, the variance, and similar simple statistics? Run a scatterplot? Or go straight to the end and run my regression methods to see what's the best line that fits the data? Pick one. The best answer is to run scatterplots, and the reason is I taught you that you should always look at the data first before doing anything with the data. So, here is the scatterplot for those data points. It's really interesting. If I look at this, it looks like almost all the guesses are the same. That's the case until I realize that my vertical axis goes from 1e22 all the way down to -1e22. Obviously there are outliers in that data. This plot makes it completely obvious to me that this isn't good data. Let's take this plot away, and let's look at the data. As before, we notice that there are these extreme cases where we have 1E+22--the same over here. I have to ask myself how to remove outliers. I have to ask myself how to remove outliers. Let me ask you a question. Can we use the quartiles method? Yes or no? And I'm not asking whether there is in principle a variant of the method that can be used, but the way you've learned the method--can we just go through both lists and independently in both lists remove the lower and the upper quartile? Yes or no? The answer is no. I mean, of course you can do it, but it'll give you really bad data. The reason is the data is paired. These two guesses correspond to each other. The same person guessed 78 over here and 80 over here. The same is true with the next two guesses. If we just remove the lowest quartiles in the first list and the second list, we'll remove different elements, and the surviving elements don't fit together, and we can't really run regression anymore. We have to keep that correspondence alive. I removed all the data points where either of the two components would guess a value under 50. If it said 0 or 1 over here, I'd remove the entire data point in both lists. The same is true for weights over 120. I figure I really don't look like a 200 kg guy or like a 1,000 kg person. I figured 120 is fine, but beyond 120 is really not a reasonable guess. The reason why I use these brackets is I really have to remove data pairs together. That was the easiest way for me to remove outliers. Let's look at the data again. Here is that data after outlier removal. I shifted by subtracting 50 from either dimension. This goes from 50 to 120 even though it says 0 to 70. The same is true over here. Now we can see the guesses are more rational. They seem to line up a little bit in linear fashion. They are certainly increasing, which means people who guessed a high weight for me today as in the vertical axis likely also guessed a high H for me last year. The guesses aren't entirely random. What I'm going to ask is how did the guesses this from last year in the horizontal axis correlate with the guesses from this year. I'm going to ask you what line best describes this data, and we're going to compute this using the formulas we learned about in class. Here is a quick quiz for you. If you look at that data between axes x and y, I will assume that you calculate the correlation r, but for now I want you to look at this and tell me your best guess of r. I'll give you a couple of hypotheses— -1, 0.7, 0, +0.7, and +1. None of those is absolutely correct, but one of them is quite close. I think you can figure out which one it is. Please check one of these five boxes--exactly one of those. The answer happens to be 0.7, and the reason is we can see that there is a positive correlation between x and y. It is not a 1. That would mean there would be no spread on the data. They'd all line up nicely on the line. But there is spread around this data, so it can't be 1. It's not zero, because there would be no correlation. It's not negative, because the line doesn't go down. It goes up. So, 0.7 is the best guess for the correlation. It's always good to look at the data and guess it in advance, because then you can see if your math is actually correct. Let's dive in and calculate the actual correlation. For last year's weight, or the x axis, your mean estimate was 81.2. I'm very flattered to report for this year on average after outlier removal you believe my weight went down. The standard deviation for x was 10.6, and the same for y is 9.34. Finally, I'll give you the mean of the following mixed product, the difference from the original axis and the x bar, the mean, times the same for y. This looks an awful lot like a variance, but it's between two different variables. We call this a covariance or "cov" between two variables x and y. In our example, the numerical value for the covariance is 75.36. Using the covariance it is amazingly easy now to calculate the correlation r. We just divide it by the two standard deviations for x and y. Please give me that number. The answer for me is 0.76. It's between ±1. It's positive. We knew there was positive correlation. It's not 1, but it suggests a very strong correlation. Now, b is also easily expressed using the covariance, but now we're normalizing by the variance of x. What do you think that is? The answer is 0.67. The caveat here was that the variance is the square of the standard deviation. So, you have to divide 75.36 by the square of 10.6, which is just a little over 100 and gives us 0.67. a, you might remember is the mean of y minus b times the mean of x. I'm sure you can compute this for me. The answer is 25.2. The line of 25.2 + 0.67x = y. This line over here is linear regression to the data. Pictorially, this is the line just like this. It might be hard to see as my shift of units, this is really 50 and this is 120. When you work this out, this line hits the y axis at 25.2, and it's steepness when it goes one step right, it goes 0.67 steps up vertically, which is off the line. We just did the following: we inspected a data sample with two dimensions, and we saw it had massive outliers. We removed the outliers using a very context-dependent method of just thresholding. We calculated some basic statistics, such as the mean, the standard deviation, and the covariance. Then we came to the meat of the unit. We computed the correlation and the regression, and those two things were the primary result of the step, but they would not have been possible without the initial steps of cleaning up the data. I hope we got those right. If you did, you are a very capable statistician at this point. This is a very typical question that arises in statistics and getting it correct means you have a fairly deep understanding of something really nontrivial that comes up in statistics. Congratulations. So why don’t you in this unit look at a field example, if we are applying statistical techniques for problems of important significance in stock trading. So here is a stock price for a specific day in 2010 and the company we’re discussing is Apple, which on the day started trading just above $250 but in the afternoon the stock went all of a sudden down to $200, back to the original price. As a statistician, we are concerned about this, because if we make trading decisions based on what’s happening, we might lose millions or billions of dollars. This is a one in a decade abnormal behavior and as a statistician, we might care about writing software that finds these kind of behaviors, so we stop trading and we don’t fall into the trap of selling our stock at a unfortunate rate just because something really, really strange is happening. And that’s a real example, in which have lost millions if not billions of dollars. So on a typical trading day, Apple stock traded about 70,000 times, giving us 70,000 data points of prices during the day. What we’d be looking at is the percentage change between adjacent trades, which we call Delta T and this defined as the difference between two consecutive trading points, XT plus one minus XT normalized by XT. So normally if XT and XT plus one is the same, this will be zero. If there is a big difference then this is a larger value and we know for the day in question what the mean and the standard deviation is for this percentage change. The mean is as small as 0.00074 negative in percent, so if we leave off the percent it’s even a hundred times smaller and this is the value for the standard deviation, 0.01344 percent. My first question for you is, to compute the confidence interval, assuming that we can use a confidence interval to detect outliers in the data. If you assume a symmetrical confidence interval with five percent confidence, in our magic factor that percentile is 1.96; compute for me in percent the confidence interval for the variable Delta T. And the correct answer is minus 0.02708 and 0.0256 and this is obtained by taking the mean over here and adding in the standard deviation plus minus up multiplied by 1.96. So here is the real question, suppose you use this confidence interval to detect abnormal behavior and abnormal behavior mean, you go to your boss and you complain, he’ll stop trading then how often during a day would you expect to detect abnormal behavior in your trading, or differently how frequently does this confidence internal trigger on any given single day. And the answer is way too frequently, 3,500 on just a single day you’d be taking your boss all the time and the reason is very simple. You take the 70,000 over here and you apply a five percent internal over here and if everything is normal you distribute it what you believe it is, you get 3,500 outliers in expectation every day. The trick is to change these numbers over here to numbers that trigger once in a decade and this is as simple as changing the 1.96 to 6.5 which is really kind of a minor change, but this makes all the difference. The percentage now becomes so small, you get one trigger per decade. So let’s quickly compute what the numbers look like for 6.5. And these are the numbers I get: minus 0.0881, 0.08662. And this is a wider confidence internval than before, that means we trigger less frequently and we find less frequently problems with our trading statistics. So let’s now dive in on May 6, 2010, when the Apple stock crashed and see if we could have detected that crash and saved ourselves from risky trades. So, let me give you examples for two consecutive prices. So, here’s one example where the stock changed from 286.85, 286.83. First you calculate the Delta T, which to remind you was the difference of those two things, Xt+1 - Xt, normalized by Xt. So, why don’t you give me your calculation of Delta T, into this box over here. I get 0.00697 when I plug this in. And now, here’s the important question, is this abnormal, yes or no? And I would say, no, because this value falls well into – between this confidence interval from here to here. I give you three more examples from that day. Because at some point, we got from 247.6 to 247.55, some other time 242.5 to 240.0 and then we got from 205.71 to 201: all within a fraction of a second. I’m just going to ask you the same question as before, do you consider these abnormal, relative to this confidence interval over here? And it turns out, the first one is still okay, but the last two would have been abnormal. And the reason is when you work out the Delta T in percent, you find that this one over here is a 0.104% change, but it’s just too much for a single trade. And this one is an amazing 2.3% change within a single trade. And those clearly trigger the abnormality behavior. Let’s look at the data. And this here is our Delta T, over time, for Apple’s stock on May 6, 2010. And you’ll find during the day it trades very normally and all of a sudden you get this amazing oscillation here of the stock first going down and then going up again and that is really indicative of something bizarre happening. And our statistic would have found it and might have saved you some money. So, that’s my example from financial data. In the next few minutes we’re going to pick a different example. Here’s another real world data set, this is the Space Shuttle Challenger and some of you might remember on January 28, 1986, it exploded and was destroyed and seven astronauts on board passed away. So, this example of would be one where the question is whether a good set decision, could have predicted this disaster and saved the lives of the astronauts on board. On the day of this tragic launch, the temperature was unusually low, at 36 degrees Fahrenheit. So, the NASA engineers asked themselves, do we have data from past flights to see if this temperature could cause problems. They graphed the temperature Xi versus the number of O-rings that had failed in the past, because those were known to be affected by temperature. In seven flights there were failures; in fact, sometimes they failed twice and sometimes only one of them failed. And the temperatures were all over the map, 70, 57, 63, 70, 53, 75 and 58. Now, these were all the cases where a problem occurred and the question was, would the lower temperature increase the chances of a problem occurring? Now, as a good statistician, what do we do? Linear regression? Do we compute confidence intervals? Or do we find a maximum likelihood estimator. Check exactly one box of the most appropriate term. And I would argue this is a clear linear regression case, you have two variables, so this is what you’re going to do. For linear regression, we compute the quotation B and A, and I give you the formulas B is calculated as a quotient of these terms that I’ve given you before divided by those guys here. A as you might remember is given by this formula. And to compute them, let’s first compute the various components here. Let’s start with N, what is N? N was easy at 7 you just count the number of data points. What is the sum of the Xis? And the answer is 446, you just add these number over here. The sum of the Yis, is a particularly challenging question. The answer is 9, you just add the Yis. Now, here is a harder one, sum of the Xi-squares which of course is in the formula of b over here. It’s 28,816. And finally the term we need is the sum of the Xis, Yis. What is that? 574. So now we can take all these five things and plug them into the formula for b please do so, use the formula as shown over here and calculate for me what b would be. And the answer for the data is 0.00143. Let’s do the same for a. 1.194. So this kind of suggests that as the temperature gets warmer, the chances of failure go up, because b is a positive slope. So you had all reasons to believe that a cold temperature is actually good for the space shuttle and we should launch. Let’s just for kicks, compute the actual expected number of O-ring failures or the Yis for the temperature of 36 degrees. And here’s the formula, it’s 36 times b plus a What is that? And the answer is 1.25. That’s still a significant number, but NASA had now all reason to believe cold temperatures are good for you. I should say there’s another kind of flaw here, one we will discuss in a second, but one that’s kind of often is the only looked at data with failures, so that would give us a number that is by definition larger than 1 But most flights had no failures. So the actual expectation should be even lower. So NASA was even more confident that this thing could be flown safely. How wrong they were. So in the full dataset there were 23 launches and many of which had no failures. The sum of Xi was 1,600, sum of Yi is 9 as before. The Xi square is 112,400 and the crossover here is 574. Now this is all extracted from the data, we have done it before, and now please calculate for me b. So this is some work but please calculate for me b again and here is the formula as before. And the answer is -0.0475. And what I now want is a, here’s the same formula as before. 1 over N sum Yi minus b times the average Xi. And the answer is 3.698. So let’s now plug this in and see for the temperature of 36 degrees what our predicted failure is. The formula now is simple, 36 times b plus a, what do we get? And the answer is 1.99 which is we expect 2 O-rings to fail because of temperature and those happen to be enough to destroy Challenger and cause the disaster. In Unit #3, we'll talk about bar charts, which is a common statistical data visualization tool. Let's look at our housing data again. This time I'll order the houses by size increasing. Here are the associated house prices from $88,000 all the way to $98,000 for these different sizes. Just as a warmup, I'll ask you a quiz that really belongs to the last unit. Is this data linear? Without plotting it, I suggest the answer is no. The size goes up monotonically, but the cost jumps up and down. You won't find a way to drive a linear line through the data when the size is increasing by the cost is bumping up and down. I'll leave the drawing of a scatter plot to you as an exercise, but it looks about like that. If I now ask you how much to pay for a 2200 square feet house, and you use the interpolation method that we learned in the very first class where we looked at the two nearest data items and interpolated linearly, what would you get, and in a minute I'll ask you, do you have trust in that number? What you get is the halfway point between 2100 and 2300. That is, 105,000. Let me ask you, do you have trust in this $105,000 number? I really want you to say, no, so please take your vote. And, yes, I don't have trust in this. The reason is from a 2100 square foot home that's clearly small than this one the price has decreased. It'd gone down. Do we believe that in general between a 2100 square foot home and a 2300 square foot home the price should go down? Do we actually believe that in a scatter plot like this the functional relationship between size and cost goes like this? Or do we instead believe it should go like this? The deviations from that linear graph is what's called "noise." Now, noise might not be the best term, but it's the term that statisticians use. It might be that one of them has a great view. The next one has an old house. This one has coastal access, which makes it more expensive. This one really requires a new kitchen. There might be factors that really effect the house price beyond the size. But if those factors are unincluded, to a statistician that's called "random noise." But coming back to my original question, I think we don't believe the red curve. Let's talk about bar charts as one way to alleviate the problem. In a bar chart, we take our raw data and pull it together. For example, we might say all the data that falls into this interval over here should be summarized by a single value. Such a value would lie halfway in between these two data points and form what's called a bar. Similarly, we might pull together this data over here into a single bar and so on. Let me ask you a question. What is the height in terms of the dollar figure for the very first bar? The answer is 80,000. It's the halfway point between 88,000 and 72,000, which is 80,000. Let me just redo this for the second bar. Please put your number right here. And the answer is 90,000, which is just the halfway point between 94,000 and 86,000, and these are the two data points that fall under the second bar. I'm sure you get it to give me the number for the third bar. These are the two points that fall into the third bar. The mean value here is 105,000. If you look at the bar graph, what you'll find is it's a much finer representation of the data. By pooling together multiple data points into an individual bar, you can see that there is a much better way to really understand the dependence of cost to data. While the bar doesn't give you the linear relationship-- in fact, in this case, happens to be nonlinear-- it really gives you a sense as you go up in house sizes the cost increases, which wasn't obvious from looking at the individual data points. What the bar chart does is it really helps you to pool together groups of data into a single bar and understand global trends. Such global trends might not be that important if you only have six data points, but imagine you have 60,000. With 60,000 data points, you're scatter plot may look like this. I can tell you my hand isn't really able to draw 60,000 points. If you go to look at this data set, the individual data tells you very little, jumping in x parameter by a tiny bit might make a jump in y from here to here down to here to here and at some point down to here. Yet a bar graph can really help you understand the data. Clearly, one of the things that a statistician does is to use cumulative tools, such as bar graphs, to gain an understanding of the underlying data. Let me ask you. Are bar charts cool? Just check one of the two answers. Honestly, if you check no, perhaps this class isn't for you. Perhaps you don't share my excitement about looking at data and using simple tools like bar charts to really understand what's going on. But if you checked yes, you're on your way to become a statistician. I want to talk briefly about histograms as a special case of a bar chart. The key difference is whereas the bar charts we discussed so far were defined over 2D data, the histograms look at 1D data. That is, there is only one dimension of data that is being plotted. Let me start with an example. Here is a fictitious data set about annual income. Suppose at some company I asked software engineers how much annual income they make. Again, this data set is contrived. Of the nine people I asked, here is the survey of different annual salaries. In the histogram case, I make a bar chart that concerns itself with only one thing, which is called "frequency," which is short for "count," that will group these salaries into three different buckets-- from $120,000 on, $130,000 on, and $140,000 on. What the bar chart plots is the frequency at which people asked fall into the different categories. Specifically, I am asking you what is the count for the salaries that fall into the $120,000 to $130,000 bucket. Please answer it here. The answer is 5, because all the salaries marked here fall between $120,000 and $130,000. So the bar in the histogram plot for this interval would be 5 high. Give me the same number for the next interval. Yes, the answer is 2. There are exactly 2 elements that fall between $130,000 and $140,000. Obviously the final bar is of size 2 again, because there are exactly 2 elements. Now, this histogram differs from the bar chart in that the vertical axis is just a frequency count whereas before it might have been a median home sales value. In 1-dimensional data sets that are numerical, this could be informative. You can say, for example the majority of workers in this company are in this salary bracket where as a much smaller number are in higher salary brackets. A famous histogram can be obtained by looking at the age distribution. For the USA, the distribution looks about as follows. Again, going to the statistics is an endless number of actual ages and so on. Let's make a histogram that is somewhat simplified that only looks at people from age 0 to 40. Here is the data set from 21, 17, 9, 27, and so on. I'm asking you for all these four graphs in a histogram how high are the bars for each of those four ranges as shown over here. Again, the horizontal axis depicts age, and the vertical axis the count. Please, enter your answers here. From 0 to 10, we have 5 individuals. From 11 to 20 it's 7. From 21 to 30, it's 1, 2, 3, 4, 5. And finally from 31 to 40, it is the remaining 5 data points. In this example, our chart would look like this. This is a histogram that depicts the count in this data set as the function of the range. In this unit you learned about bar charts and histograms. They both use vertical bars, and they both aggregate data. The big difference was that the bar chart is defined over 2D data. The one dimension applies to the x axis and the other to the y axis whereas histograms only apply to 1D data where the y axis becomes the count of that data. In the next unit we'll encounter another plot. Without giving it away, it is some how related to this birthday pi. So stay tuned. Let's now talk about pie charts. You all know what a pie is. Here's a birthday pie, and if you look at this pie from above it looks just like this. It's a circle. Now here comes Sebastian and cuts out his first piece of the pie. What results is a pie with a missing piece, and then my wife comes, and she gets just a small piece, but my brother eats a very big piece, so only the following pie is left. We've just made a pie chart, and in statistics you use pie charts to visualize data, specifically relative data, and I'll tell you in a second what that means. Let's start with an exercise. Suppose we are in an election, and there are two parties--party A and party B. And suppose it's a toss up, so both parties are getting the same number of votes--or 50%. Here are three pie charts. Will any of those reflect the outcome of the election? Perhaps the first, the second, or the third, or none of the above. Please check exactly one box. The answer is the second is actually a good representation of this outcome. When I color the pieces of the pie, we'll see that 50% of the pie falls into one class and the other 50% in the other class. Compare this to this pie over here, which seems to be a 75 to 25 split, or the one over here, which is 25 to 75. This is the one I would've chosen. Now, I said that pie charts are good for relative data. To illustrate this, suppose party A go 724,000 votes and party B got 181,000 votes. What is the percentage of votes that part A got? What's the percentage of votes for party B? Enter your numbers here. Well, with a little bit of math, we realize that this is 80%. The way to calculate this is 724,000 is the votes that party A received, but the total number of votes was 724,000 plus 181,000, which is 724,000 over 905,000. That's exactly 0.8, which is the same as 80%. It follows that party B got 20%. You get to that number when you replace 724,000 with 181,000--the number of party B. It turns out 181,000 divided by 905,000 is exactly 0.2. This is exactly 5 times as large as this number over here. Now, given all this I will now draw a number of pie charts. I want you to select the one that most closely resembles the distribution over here. Just for clarity, party A is depicted in red and party B in blue. Please select exactly one of those pie charts. To me the best answer is the last one. The reason being that this area of the pie most closely resembles 20% of the pie. Now, closely related is the first one where we cut out a quarter of the pie. But a quarter is 25%. So this is slightly smaller than a quarter. I think it's the best one to correspond to 20/80 in this pie chart. Here comes a tricky question. Given that we have a pie chart with distribution 80% and 20%, I'm now changing the total number of voter. I'm telling you there were 23,000 people voting for party B, and I'm asking you how many voted for party A such that the pie chart over here is exactly the correct one with an 80 to 20 distribution. The answer is 92,000. You can see this because 80 is exactly 4 times as much as 20. If you took 23,000 and divided it by 20% and multiplied the result by 80%, That is the same as just multiplying 23,000 by 4. You get that number over here. What's remarkable about this chart is it's invariant to the total number of votes. What it really depicts is the relative number of votes. It shows that A got many, many more votes than B. It shows it graphically, so you can see this without even studying the numbers. Let's practice this one more time. Suppose you're taking a Udacity class. And I guess you're taking one right now, so let's drop the suppose. Among the students that take the class with you, you find the following age distribution. From 13 to 19 there are 12,000 students. From 20 to 32 there are 96,000 students. And from 33 on there are 36,000 students. I now want to construct the pie chart with you. Here's my pie. I want you to place the separator for the very first class of the age 13 to 19, which is the blue class. Please check the box on the perimeter of the pie chart that best places the separator. For example if you check the box over here, This is not that easily solved. What ratio is 12,000 to the total number of students? Well, 12,000 over the total number of students ends up being the same as 12 over 144, and that's the same as 1/12. The correct answer would have been the check box over here. This area over here corresponds to the age group of 13 to 19. Moving on to our dominant age group, please check on the perimeter of the box that best represents the separator for the second class. The answer is the box over here. Some of you might have chosen this box, but I'll tell you in a second why this is the better box. This is the resulting pie chart where the red class is 20-32, and the remaining black class is the age group 33 and older. Now let's do the math. To understand what area the red curve will occupy, we're going to divide 96,000 by the total number of students, which is 96/144. It happens to be the same as 8/12. Now, the 8 mark is the one over here, but I chose the 9 mark. This is the 9 mark--1, 2, 3, 4, 5, 6, 7, 8, 9. The reason is one mark has already been used up by the first class, and now we incrementally add 8 to those to arrive over here. The surface area in the pie shown in red really corresponds to 8/12 of the total surface area. If we now plug in the final class of 36,000 students, which is the same as 36/144 or 3/12, you'll find this area over here occupies exactly 3/12 of the total area. Hence this is the correct pie chart. Let's do this again,using once again our election example. This time with four parties--A, B, C, and D. And here the election outcomes party A received 175,000 votes, party B 50,000, party C 25,000, party D 50,000. In most democratic parties, you don't find such a distribution that one party takes the vast majority, but say that's the case for our country. If I now draw a pie chart, let us assume that we try to graph party A first, then B, C, and D--as indicated over here. Please check exactly those boxes that define the separator from one party to the next. And What we'll find is that party A got 7/12 of the vote, which is the majority, whereas the other ones only received 2/12, 1/12, and 2/12. So if we go forward 7 pieces in this diagram 1, 2, 3, 4, 5, 6, 7-- we check mark this box. Another 2 gives us this box. Another 1 gives us this box. and these are the final two. So here’s how this chart will look like, Obviously A takes the majority outcome, B reaps this slice over here, C is the smallest party, and D is the one over here. As a final question, I will now tell you that in a different election where the same pie chart is correct we had a total of 240,000 voters, which is the sum of all votes cast. Assuming that this pie chart here is correct, can you tell me how many votes are cast for each of the parties? The answer lies in the numbers I just wiped out. If A got 7/12 in total, we know that 1/12 is 20,000. So A got 140,000, which is 7 times 20,000. Party B got 2/12 or a 6th, which is 40,000. Party C--a disappointing 20,000. And party D the same as party B. If we look at the diagram, it tell you nothing about the absolute numbers. In fact, I can change the absolute numbers. As long as the relative percentages stay the same, it does however tell you a lot about the distribution of the data. It shows you that A is the dominant party that got more than 50% of the outcome whereas B, C, and D occupies smaller slices with a slice for C being half the size of B or D, respectively. Again, this is called a pie chart. Pie charts are really, really powerful to represent things like election outcomes. In any data set we just care about the relative outcomes and perhaps have more than just 2 classes. Congratulations. You just learned about pie charts. They're great for relative data, and they're wonderful for comparing which slice of the pie is bigger. So in the next class we'll look at relative data again, and we'll pick up the touchy issue of gender discrimination in college admissions, using a study originally performed at UC Berkeley here in California. This'll be a really deep statistical question, and I promise you you'll be surprised by the result. Now comes the moment of truth. I have been waiting for this moment for quite a while to let you experience first hand how to visualize data, and in this unit, you will do everything entirely by yourself. I will give you data and you will plot those data and answer simple questions. But there is one thing to know--this unit is strictly optional. The reason is there is programming involved in this unit and programming was not a prerequisite. However, I will make sure what you have to do is extremely simple, is very well explained, and a whole lot of fun, so if you choose to take this unit, you will visualize some interesting data using a very simple programming tool. This over here is the Udacity programming interface. Don't freak out--all I'll do is give instructions over here, and I'll hit the run button. As I scroll down, you can see a histogram of the data. So this has been computed automatically now. There are five data categories from 2.0 to 6.0, and you can see the frequency of data ions in the vertical dimension--1, 4, 3, 1, and 1 again. Now this is a familiar histogram. Just look out how easy it was to generate this. Here are 3 lines of things that the computer has done for me. The very first one you should just ignore. It'll always be there. Just ignore it. It tells the computer that we wish to plot things. The second and the third one were the important ones. I define a data set--a data set is a list of 10 elements--3, 4, 2, 4, 3, 5, 3, 6, 4, 3 that I made up and with this line, I tell the computer that this over here is my data. Then I tell the computer, "Please histogram plot my data"--and then this is the result I just shown you. Here are the 3 things that we have to tell the computer. If you ignore the cryptic "from plotting import *," then what we do is we define the data. We list it, we give it a name called data, and we generate a histogram plot of the data. Then we then hit run in the small box down here--you'll get exactly the plot that I've shown you where the frequency of the data is plotted accordingly. Obviously, that range was 2.8 with 3.6 as the most frequent and if we go back up to the data. We find yet 3 occurrences of 3 with 4 into this range. This is the most frequent range. What I want you to do next is look at a more complex data set. In this data set, we study the height of people in inches from a data set I got from the web. Here is the data set of people and their individual heights--all ranging between about 65 and 72 inches, and I want you to do a histogram plot following the example I've given you. Remember histplot is the command for histogram plot, and I didn't call the data "data." I actually called it height--so try to assemble the right command to plot the data and look at it because I'll ask you a question about it. If you got this correct, then your histogram will look pretty much like this thing over here. There's a big peak in the middle and very short people and very long people are much less likely than medium-sized people. I'm covering up the ranges because I'd like to ask you a question here, but I will tell you that the one command that had to enter was histplot with height as an argument, and if you play with it, you'll realize you have to match the uppercases of the height, but you can plug it into histplot just the same way we used data before. Let me ask you a quiz. For our data set, what is the most frequent height? Is it 63-65 inches, 65-66, 66-68, 68-70, or 70-71? These are the ranges that your histogram is sure to pick. And the answer is a resounding 66-68 so this is the correct number. Now if you look at these ranges very carefully, they aren't all the same size it seems. This seems to be more like 2 but this range seems to be just 1, and the reason is these numbers are truncated. They actually have decimal points that are not displayed but regardless, this category wins hands down in the statistic. Let's test this again, and this time we study the weight of people. I'll give you a data set of weight, I ask you to do the histogram and answer a question about the most common weight category. Here's a data set called the weight that you can look at, and I want you to do a histogram for the weight that will look as follows. Please go ahead to tell the computer to make this graph and hit the run button. As before, the command is histplot with the variable of weight as its argument. This generates the plot I told you, and I'm going to ask you a question about the plot. What is the most frequent weight in this histogram-- 97-108 pounds, 108-119, 119 all the way to 130, 130-142, or Sebastian's weight, which is a mystery? Please check the exact line box. The answer, if you program histplot the way I did, is 119-130 pounds. This is the correct answer, and no I won't tell you what my weight is at this point. But I can tell you it's much more than 153. Let's now look at the combined data set of height and weight. It turns out both lists have the same elements, and the very first element in the height list is the same person as the weight in this list over here. The same is true for the second--so these data pair up conveniently to plot the height of a specific person and his or her weight. I now want you to run a scatter plot. Remember how a scatter plot looked like. Particularly, I want you to graph the height versus the weight, and the command to use is called scatterplot and accepts these two variables as input. I'd like you to try these out for the data I've given you. All you have to do is to enter the scatterplot command the way I've given it to you, but let me now ask a question about the plot that you've seen. Is this data exactly linear without any deviation from linearity, approximately linear--that is can you see a linear trend, or at the other extreme are height and weight pretty much unrelated--that is you can't really see a dependance between height and weight? I know there are other possibilities here but I just want to narrow it down to these three possibilities to derive a very crisp and clear answer. To answer this question, let's look at the scatter plot--you see there're certain sizes of people and then there are certain weights associated with these people, and it's hard to make out but I think there's a dependance whereby the taller a person is the heavier the person is. Now, there's lots of exceptions--this person is a remarkably heavy person for size, this is a remarkably light person for about the same size, and we all know that some people are heavier and thinner and thicker for these sizes, but if you'll expectively, you can see a slight upper trend that appears to be approximately linear. We can't tell this right now--we don't understand how to analyze where the data depends as a linear but you get the juice of it. By looking at the data alone, you can tell that weight increases with size. So for now, I'll check the second box over here because the other ones are obviously incorrect. As a final exercise, I'd like you to replace scatter plot by bar chart again with the same two arguments as before--so please go ahead and tell the computer to generate a bar chart. Here's the command barchart, Height and Weight, and as I scroll down, I will find my bar chart that shows pretty much how larger sizes was that in larger weights. Now this chart would question whether it's exactly linear. You can see the distance over here to be significantly smaller than the distance over here but it's all approximate, and in reality, I can tell you the ratio between height and weight is not linear. But for the sake of the exercise, that's a better statement than any of the other statements that I've given you. Let's now look at the data set that relates age to wage--how much money you make. Most of you out there are young, so your preference might be that the world pays young people the best, but in many countries wage goes up with age until perhaps you hit retirement age. Which one is it? Let's plot the data. Here is a very elaborate data set of people of certain ages and their wages that they receive. I want you to make a scatter plot for this data. You'll realize that the highest wage is $267,000, and I want you read off the scatter plot what is the youngest age in this data set where this wage is being realized. Here's the scatter plot command. And without looking at the result, I now ask you about the question. What is the youngest age at which a person earns $267,000? What is the youngest person to earn $267,000 in our data set? Look at your scatter plot and you can read it straight off. As a hint, because it is the largest wage, it'll intersect with the vertical box of the diagram. The answer is 30--as I go to the scatter plot that graphs age versus income, you'll find that up here is the very first time someone had the maximum wage. That specific number occurs 4 times--here, here, here, and here among the older crowds of 35-pluses might made it--my own age isn't even on the diagram anymore. But the correct answer would have been 30, and it's really easy to see from the scatterplot. Now do me a favor and please make a bar chart of the same data. And after you've done it, I'll ask you the same question of whether there's an approximate linear relationship between age and wage. Here's the command for making a bar chart. Let me ask you the question. Is the relationship between age and wage exactly linear, approximately linear, or there seems to be no relationship? Exactly one of those boxes is correct. I would say approximately linear--if you go to the barchart, you'll find that these bars kind of nicely lined up almost like a line with an exception that over here it levels off a little bit. Linear seems to be a good guess as to what's happening--wage increases linearly with age. The reason why it's not exact is more obvious from this scatter plot which shows the raw data. You can see even with higher ages around 40, they're still individuals that earned relatively little money--so for the data to be linear, we get these to line up exactly on the straight line which is not the case in this data. On average, the relationship might be linear, but when it comes to the exact data points, it's clearly not linear. Finally, I'd like to ask you a question of what age group is most frequent? Remember we have a specific plot to look into this and I gave you five options--18-22, 22-26, 26-31, 31-35, and 35-40, so please play with the data. Enter the appropriate command over here, look at the output, and then after you're done, come back to my question about what age group is most frequent. The correct plot is, of course, histplot over age. We're going to ignore wage for this question. Now here's my question--look at your output and answer which of these age groups happens to be the most frequent. The answer on our data is 35-40. Here is the actual histogram plot and if you look at this, the biggest bar is the bar on the right, which corresponds to the age of 35-40--so this would have been the correct answer. Congratulations! You've finished something really complex. You've made your own bar charts, your own histograms, and your own scatter plots. Isn't this a lot of fun? Isn't statistics really great? Did you learn that by just looking at data, you can already understand a lot. So the purpose of this unit was to empower you to instruct the computer to plot data, and the reason why I told you this is because the very first step every statistician does is to look at the data if you're given data. Now on the next unit, I will give you something that'll bend your mind. I will give you data that depending on how you look at it will get you a very different conclusion. Stay tuned and check out the next unit where I will bend your mind. In this unit, I want to show you a problem that will illustrate how deep statistics can actually be. Statistics is not just a superficial field. In fact, in this unit I will show you a problem that will blow your mind. I promise you will think about this for a long time to come. So, let's just dive in. The problem I'd like to tell you about is motivated by an actual study the University of California Berkeley, which many years back wanted to know whether it's admissions procedure is gender biased. I looked at various admission statistics to understand whether than admissions policies had a preference for a certain gender. And while the numbers I'll be giving you are not the exact same that UC Berkeley found, the paradox is indeed the same and is often called, "Simpson's Paradox." I'm just giving you a simplified version of the problem. Here is the data. Among male students, we find that from 900 applicants in major A 450 are admitted. Please tell me what the acceptance rate is in percent. Obviously, it's 50%. In a second major B 100 students applied, of which 10 were admitted. What is the acceptance rate? And the answer is 10%. The same statistic was run for female students. Again, I made up the data to illustrate the effect. Females tended to apply predominantly for major B with 900 applications for major B and just 100 for major A. The university accepted 80 out of 100 applications in major A and 180 out of 900 in major B. Please tell me the rate of acceptance in percent for major A for the females student population. Of course it's 80%--80/100. Please do the same for the major B in the female population over here. 180 is 20% of 900. So, just looking at these numbers for the two different majors, would we believe--in terms of the acceptance rate-- is there a gender bias? Yes or no? And I would say yes, in part because the acceptance rate is so different for the different student populations, even though the numbers are relatively large. So, it doesn't seem just like random deviations. But the thing that will blow your mind away is a different question. Who is being favored--the male students or the female students? And looking at the data alone, it makes sense to say the female students are favored because for both majors, they have a better admission rate than the corresponding male students. But now, let's do the trick. Let's look at the admission statistics independent of the major. So, let's talk about both majors, and I would wonder how many male students applied. And of course, the answer is 1000. How many were admitted? And the answer is 460. So, what is the admissions rate for male students across both majors in percent? And the answer is, of course, 46%. It's 460/1000 x 100%. Now, do the same for the female student population, and we had a 1000 applicants, same number as in the male case, and 260 students admitted. So, what's the percentage rate for admission? The answer is 26%. So, across both majors, I'm asking you the same question again now. Who is actually being favored? Males or females? And surprisingly, when you look at both majors together, you find that males have a much higher admissions rate than females. I'm not making this up. These numbers might be fake, but that specific affect was observed at the University of California at Berkley many years ago. But when you look at majors individually, then you find in each major individually the acceptance rate for females trumps that of males, both in the first major and the second major. Going from the individual major statistics to the total statistics, we haven't added anything. We just regrouped the data. So how come, when you do this, what looks like an admissions bias in favor of females switches into admissions bias in favor of males? I showed you this example to illustrate how ambiguous statistics really is. In choosing how to graph your data, you can majorly impact what people believe to be the case. In fact, a famous saying goes, "I never believe in statistics I didn't doctor myself." I'll let you guess here who this is being attributed to-- Mark Twain, Oscar Wilde, or Winston Churchill. Check the Web, and see who invented this famous quote. And even though I don't think Winston Churchill invented it, in the course of World War II the Germans tried to associate this quote with him to make him less credible. Be it as it is, the key lesson here is statistics is deep and often manipulated. One of the tricks I'd like to teach you is to be skeptical of statistics, of your own results, of other people's results, and really understand how to turn raw data into decisions or conclusions. I hope that this simple example made up think. Stay tuned when we dive into the basics of statistics, which is probability theory, in the next number of units. In this and the following units, we will talk about probability. Probability is just the opposite of statistics, and there's a yin-yang relationship between both. Put differently, in statistics we are given data and try to infer possible causes that relate to the data, whereas in probability we are given the description of the causes and we'd like to predict the data. The reason why we now study probability and not statistics is because it gives us a language to describe the relationship between data and the underlying causes. So, enough of the theory; let's dive in. I have here a U.S. dollar coin. It has two sides, one showing a head and one showing what's called tails. In probability, I'm giving a description of this coin, and I'm making data. We just make data. [sound of coin spinning] So if we look at the coin, it came up heads. So I just made a data point of flipping the coin once, and it came up heads. Let me do it again. [sound of coin spinning] And--wow! It came up heads again. So my new data is {heads, heads}. And you can see how it relates to the data we studied before when we talked about histograms and pie charts and so on. Let me give it a third try. [sound of coin spinning] And, unbelievably, it comes up once again heads. So let me ask a statistical question to test your intuition. Do you think if I twist this coin more frequently will it always come up heads? And say I try to twist it as fairly as I possibly can. And you can debate it, but I think the best answer is no. This is what's called a fair coin, and that means it really has a 50% chance of coming up tails. So let me spin it again. [sound of coin spinning] And, not surprisingly, it actually came up tails this time. So probability is a method of describing the anticipated outcome of these coin flips. Let's talk about a fair coin. The probability of the coin coming up heads is written in this P notation. This reads probability of the coin coming up heads. And in a fair coin, the chances are 50%. That is, in half the coin flips, the coin should come up heads. In probability we often write 0.5, which is half of 1. So a probability of 1 means it always occurs. A probability of 0.5 means it occurs half the time. And let me just ask you what do you think, for this coin, is the probability of tails? And I would say the answer is 0.5. Let me now go to a coin that is what is called "loaded." A loaded coin is one that comes up with one of the two much more frequently than the other. So, for example, suppose I have a coin that always comes up heads. What probability would I assess for this coin to come up heads? What would be the right number over here? And the number is 1. That's the same as 100%. 1 just means it always comes up in heads. And, given that, what number would you now assess the probability of tails to be? And, yes, the answer is zero. And we find a little law here we just want to point out, which is the probability of heads plus the probability of tails equals 1. And the reason why that's the case is the coin either comes up heads or tails. There is no other a choice. So no matter what happens, if I look at heads and tails combined the chances of either of those occurring is 1, because we know it's going to happen. So we can use this law to compute the probability of tails for other examples. So suppose the probability of heads is 0.75, that is, 3 out of 4 times we're going to get heads. What is the probability of tails? And the answer is 0.25, which is 1 - 0.75 using the law down here. As you can verify, 0.75 + 0.25 =1. So we just learned something important. There's a probability for an outcome; I'm going to call it A, for now. And we learned that the probability of the opposite outcome, which we're going to call ¬A (this over here just means "not") is 1 minus the probability as expressed right over here. That's a very basic law of probability, which will become handy as we go forward, so please remember it. In our example, we observed heads twice. So now I want to ask you a really tricky question: What's the probability of observing heads and heads if you flip the same unbiased coin twice? This means in each flip we assume the probability of heads is 0.5. Please answer here. That was a tricky question, and you couldn't really know the answer if you've never seen probability before, but the answer is 0.25. And I will derive it for you using something called a truth table. In a truth table, you draw out every possible outcome of the experiment that you conducted. There were two coin flips--flip 1 and flip 2--and each had a possible outcome of heads, heads; heads, tails; tail, heads; and tail, tail. So when we look at this table, you can see every possible outcome of these two coin flips. There happens to be four of them. And I would argue because heads and tails are equally likely, each of these outcomes is equally likely. Because we know that the probability of all outcomes has to add up to 1, we find that each outcome has a chance of a quarter, or, 0.25. Another way to look at this is the probability of heads followed by heads is the product . What are the chances of the first outcome to be heads multiplied by the probability of the second outcome to be heads? The first is 0.5, as is the second. And if you multiply these two numbers, it's 0.25, or, a quarter. Let me now challenge you and give you a loaded coin I flipped twice. And for this loaded coin, I assumed the probability of heads is 0.6. That really changes all of the numbers in the table so far, but you can apply the same method of truth tables to arrive at an answer for what is the probability of seeing heads twice under the assumption that the probability of heads equals 0.6? And I want to do this in steps, so rather than asking the question directly, let me help you derive it by first asking: What's the probability of tails? And the answer is 0.4 because heads comes up 0.6, and 1 - 0.6 = 0.4. And now please fill out the entire truth table. There are four values over here, so please compute them for me. And the answer using our product rule is heads, heads comes out to 0.6 0.6, which is 0.36. Heads followed by tails is 0.6 0.4, which is 0.24. Tails followed by heads is, again, 0.24. And tails followed by tails is 0.16, which is 0.4 0.4. If you add up these numbers over here-- please go ahead and add them up and tell me what the sum of those numbers is. And, not surprisingly, it's 1. That is, the truth table always has a probability that adds up to 1 because it considers all possible cases, and all possible cases together have a probability of 1. So we just check this and make sure it's correct. Reading from this table, we find that the probability of (H,H) is 0.36. And you can do the same over here. 0.6 0.6 = 0.36 So that's our correct answer. Let's now go to the extreme, and this is a challenging probability question. Suppose the probability of heads is 1, so my coin always comes up with heads. What is the probability of (H,H)? And the answer is 1. To see this, we know that the probability of tails is 0. All the probability goes to heads. 1 1 = 1 1 0 = 0 0 1 = 0 And 0 0 = 0. And it's easy to verify that all these things add up to 1. Our (H,H) is just 1. The truth table gets more interesting when we ask different questions. Suppose we flip our coin twice. What we care about is that exactly one of the two things is heads, and thereby exactly the other one is tails. For a fair coin, what do you think the probability would be that if I flip it twice we would see heads exactly once? And the answer shall be 0.5. And this is a nontrivial question. Let's do the truth table. So, for flip-1, we have the outcomes of heads, heads, tails, tails. For flip-2, heads and tails and heads and tails. These are all possible outcomes. And we know for the fair coin each outcome was equally likely. That is, exactly one quarter. Given that, we now have to associate a truth table with the question we're asking. So where exactly is, in the outcome, heads represented once? Please check the corresponding cases. And, yes, it's in the second case and in the third case. The extreme cases of heads, heads and tails, tails don't satisfy this condition. So the trick now has been to take the 0.25 probability of these two cases and add them up, which gives us 0.25 + 0.25 = 0.5. This is the number which is correct for this inquiry. Let me now make it really, really challenging for you. I take a fair coin and flip it 3 times, and I want to know the probability that exactly 1 of those 3 flips comes up heads. And this answer is tricky. We will derive it through the truth table. Now there's eight possible cases. Flip one can come of heads or tail; same for flip two, heads, tail, heads, tail; and the same for flip three and if you look at this every possible combination is represented. For example, these are heads, tail, tail. Now each of those outcomes has the same probability of an eighth, because it's eight cases. So 8 x 1/8 sums up to 1. In how many cases do we have exactly one H? It turns out that it's true for only three cases. The H could be in the first position, in the second position, or in the third position. So three out of eight cases have a single H. Each of those carries a probability so we sum those cases up to carry a total of 3/8 of a probability. These are the same as 0.375. Now that was a challenging question. I'm going to make it even more challenging for you now. I'll give you a loaded coin--the probability for H is 0.6. I expect this will take you awhile on a piece of paper to really calculate this probability over here. But you can do exactly the same thing. You go through the truth table. You apply the multiplication I showed you before to calculate the probability of each outcome; they're not the same anymore. H, H, H is clearly more likely than T, T, T. And when you've done this, add the corresponding figures up, and tell me what the answer is. And my answer is 0.288. How do I get that? Let's look at the three critical cases. H T T is 0.6 for H times 0.4 for tails times another 0.4 for tails again and it gives me 0.096. Now it turns out this case over here has the same probability because all we do is we order it 0.4 x 0.6 x 0.4 and we know that in multiplication the order doesn't matter, so you get the same 0.096, and by the same logic, if third one also gets me 0.096. So adding this 0.096s together, if we get them, gives me 0.288. So I did not have to fill the entire truth table, which you might have done in the derivation. I only have to fill out the cases I care about, yet they give me their correct result. So let's do one final exercise. Now I am throwing dice. The difference between dice and coins is that there are now 6 possible outcomes. Let me just draw them, and say it's a fair die, which means each of the different sides comes up with a probability over 6 for any of the numbers you can plug in over here. What do you think the probability is the die comes up with an even number? I'm going to write this as the outcome of the die is even. And you can once again use a truth table to calculate that number. In truth table-speak, there are 6 outcomes, 1 to 6. Each has the same probability over six. Half of those numbers are even--2, 4, and 6, so if we add those up, we get 3 1/6--the same as a half. The outcomes is 0.5. Now I'm finally going to make, as my final quiz, a really challenging question for you. Suppose we throw a fair die twice. What do you think the probability of a double is? Double means both outcomes are identical with the same number regardless of what that number is. The actually an important number because in many games involving two dice, have different rules when these come up with the same number. So, it might be important to know what the probability is. And once again, we can answer this using a truth table. Now the truth table will have 36 different entries, six for the first throw times six for the second throw, and there isn't enough space on this tablet to draw all the 36 entries. So, let me just draw the ones that really matter, one-one, two-two, and so on all the way to a six-six. So, each one of those is a probability of 1/6 for the first outcome times 1/6 for the second, which gives me 1/36, and the same logic applies everywhere. So, for all of these six outcomes, I have 1/36 of a chance this outcome would materialize. Adding them all up gives me 1/6, why? Because, I get 6 times 36 and I can simply this back to 1/6 that's just the same as 0.16667. So, 1/6 times, you will get a double Now, when you're play a game like backgammon, which is played with two dice, it might not feel like this, I can swear I don't get a double of 1/6 moves, but it's actually true that that's the right--that's the correct probability. So let me summarize, you've actually learned quite a bit. You learned about probability of an event, such as the outcome of a coin flip. You learned that the probability of the opposite event is 1 minus the probability of the event. And you learned about the probability of a composite event, which was in the form P P <i>….</i> P. Technically speaking, this thing over here is called independence, which means nothing else, but that the outcome of the second coin flip didn't really depend on the outcome of the first coin flip. In our next unit, we will talk about dependence where there are bizarre dependencies between different outcomes. But for the time being, you really managed to get a very basic understanding of probability. So, let's take the next unit and let's jointly dive much deeper into the rabbit hole of probability. In real life, things depend on each other. Say you can be born smart or dumb and for the sake of simplicity, let's assume whether you're smart or dumb is just nature's flip of a coin. Now whether you become a professor at Standford is non-entirely independent. I would argue becoming a professor in Standford is generally not very likely, so probability might be 0.001 but it also depends on whether you're born smart or dumb. If you are born smart the probability might be larger, whereas if you're born dumb, the probability might be marked more smaller. Now this just is an example, but if you can think of the most two consecutive coin flips. The first is whether you are born smart or dumb. The second is whether you get a job on a certain time. And now if we take them in these two coin flips, they are not independent anymore. So whereas in our last unit, we assumed that the coin flips were independent, that is, the outcome of the first didn't affect the outcome of the second. From now on, we're going to study the more interesting cases where the outcome of the first does impact the outcome of the second, and to do so you need to use more variables to express these cases. To do so, let's study a medical example--supposed there's a patient in the hospital who might suffer from a medical condition like cancer. Let's say the probability of having this cancer is 0.1. Thatt means you can tell me what's the probability of being cancer free. And the answer is 0.9 with just 1 minus the cancer. Of course, in reality, we don't know whether a person suffers cancer, but we can run a test like a blood test. The outcome of it blood test may be positive or negative, but like any good test, it tells me something about the thing I really care about--whether the person has cancer or not. Let's say, if the person has the cancer, the test comes up positive with the probability of 0.9, and that implies if the person has cancer, the negative outcome will have 0.1 probability and that's because these two things have to add to 1. I've just given you a fairly complicated notation that says the outcome of the test depends on whether the person has cancer or not. That is more complicated than everything else we've talked about so far. We call this thing over here a conditional probability, and the way to understand this is a very funny notation. There's a bar in the middle, and the bar says what's the probability of the stuff on the left given that we assume the stuff on the right is actually the case. Now, in reality, we don't know whether the person has cancer or not, and in a later unit, we're going to reason about whether the person has cancer given a certain data set, but for now, we assume we have god-like capabilities. We can tell with absolute certainty that the person has cancer, and we can determine what the outcome of the test is. This is a test that isn't exactly deterministic--it makes mistakes, but it only makes a mistake in 10% of the cases, as illustrated by the 0.1 down here. Now, it turns out, I haven't fully specified the test. The same test might also be applied to a situation where the person does not have cancer. So this little thing over here is my shortcut of not having cancer. And now, let me say the probability of the test giving me a positive results--a false positive result when there's no cancer is 0.2. You can now tell me what's the probability of a negative outcome in case we know for a fact the person doesn't have cancer, so please tell me. And the answer is 0.8. As I'm sure you noticed in the case where there is cancer, the possible test outcomes add up to 1. In the where there isn't cancer, the possible test outcomes add up to 1. So 1 - 0.2 = 0.8. Look at this, this is very nontrivial but armed with this, we can now build up the truth table for all the cases of the two different variables, cancer and non-cancer and positive and negative tests outcome. So, let me write down cancer and test and let me go through different possibilities. We could have cancer or not, and the test may come up positive or negative. So, please give me the probability of the combination of those for the very first one, and as a hint, it's kind of the same as before where we multiply two things, but you have to find the right things to multiple in this table over here. This is not an easy question. And the answer is probability of cancer is 0.1, probability of test being positive given that he has cancer is the one over here--0.9, multiplying those two together gives us 0.09. Moving to the next case--what do you think the probability is that the person does have cancer but the test comes back negative? What's the combined probability of these two cases? And once again, we'd like to refer the corresponding numbers over here on the right side 0.1 for the cancer times the probability of getting a negative result conditioned on having cancer and that is 0.1 0.1, which is 0.01. Moving on to the next case. What do you think the answer is? And here the answer is 0.18 by multiplying the probability of not having cancer, which is 0.9, with the probability of getting a positive test result for a non-cancer patient 0.2. Multiplying 0.9 with 0.2 gives me 0.18. Let's just quickly do the final one, because it's the most likely one. Here you get 0.72, which is the product of not having cancer in the first place 0.9 and the probability of getting a negative test result under the condition of not having cancer. Now quickly, do me a favor and add all of those up. What do you get? And as usual, the answer is 1. That is, we study in the truth table all possible cases. and when we add up the probabilities, you should always get the answer of 1. Now let me ask you a really tricky question. What is the probability of a positive test result? Can you sum or determine, irrespective of whether there's cancer or not, what is the probability you get a positive test result? And the result, once again, is found in the truth table, which is why this table is so powerful. Let's look at where in the truth table we get a positive test result. I would say it is right here, right here. If you take corresponding probabilities of 0.09 and 0.18, and add them up, we get 0.27, and that's the correct answer for getting a positive result. Putting all of this into mathematical notation we've given the probability of having cancer and from there, it follows the probability of not having cancer. And they give me 2 conditional probability that are the test being positive. If we have have cancer, from which we can now predict the probability of the test being negative of having cancer. And the probability of the test being positive can be cancer free which can complete the probability of a negative test result in the cancer-free case. So these things are just easily inferred by the 1 minus rule. Then when we read this, you complete the probability of a positive test result as the sum of a positive test result given cancer times the probability of cancer, which is our truth table entry for the combination of P and C plus the same given we don't have of cancer. Now this notation is confusing and complicated if we ever dive deep into probability, that's called total probability, but it's useful to know that this is very, very intuitive and to further develop intuition let me just give you another exercise of exactly the same type. This time around, we have a bag, and in the bag are 2 coins,coin 1 and coin 2. And in advance, we know that coin 1 is fair. So P of coin 1 of coming up heads is 0.5 whereas coin 2 is loaded, that is, P of coin 2 coming up heads is 0.9. Quickly, give me the following numbers of the probability of coming up tails for coin 1 and for coin 2. And the answer is 0.5 for coin 1and 0.1 for coin 2, because these things have to add up to 1 for each of the coins. So now what happens is, I'm going to remove one of the coins from this bag, and each coin, coin 1 or coin 2, is being picked with equal probability. Let me now flip that coin once, and I want you to tell me, what's the probability that this coin which could be 50% chance fair coin and 50% chance a loaded coin. What's the probability that this coin comes up heads? Again, this is an exercise in conditional probability. And let’s do the truth table. You have a pick event followed by a flip event We can pick coin 1 or coin 2. There is a 0.5 chance for each of the coins. Then we can flip and get heads or tails for the coin we've chosen. Now what are the probabilities? I'd argue picking 1 at 0.5 and once I pick the fair coin, I know that the probability of heads is, once again, 0.5 which makes it 0.25 The same is true for picking the fair coin and expecting tails but as we pick the unfair coin with a 0.5 chance we get a 0.9 chance of heads So 0.5 times 0.95 gives you 0.45 whereas the unfair coin, the probability of tails is 0.1 multiply by the probability of picking it at 0.5 gives us 0.05 Now when they ask you, what's the probability of heads we'll find that 2 of those cases indeed come up with heads so if you add 0.25 and 0.45 and we get 0.7. So this example is a 0.7 chance that we might generate heads. Now let me up the ante by flipping this coin twice. Once again, I'm drawing a coin from this bag, and I pick one at 50% chance. I don't know which one I have picked. It might be fair or loaded. And in flipping it twice, I get first heads, and then tails. What's the probability that if I do the following, I draw a coin at random with the probabilities shown, and then I flip it twice, that same coin. I just draw it once and then flip it twice. What's the probability of seeing heads first and then tails? Again, you might derive this using truth tables. This is a non-trivial question, and the right way to do this is to go through the truth table, which I've drawn over here. There's 3 different things happening. We've taken initial pick of the coin, which can take coin 1 or coin 2 with equal probability, and then you go flip it for the first time, and there's heads or tails outcomes, and we flip it for the second time with the second outcome. So these different cases summarize my truth table. I now need to observe just the cases where head is followed by tail. This one right here and over here. Then we compute the probability for those 2 cases. The probability of picking coin 1 is 0.5. For the fair coin, we get 0.5 for heads, followed by 0.5 for tails. They're together is 0.125. Let's do it with the second case. There's a 0.5 chance of taking coin 2. Now that one comes up with heads at 0.9. It comes up with tails at 0.1. So multiply these together, gives us 0.045, a smaller number than up here. Adding these 2 things together results in 0.17, which is the right answer to the question over here. That was really non-trivial, and I'd be amazed if you got this correct. Let me do this once again. There are 2 coins in the bag, coin 1 and coin 2. And as before, taking coin 1 at 0.5 probability. But now I'm telling you that coin 1 is loaded, so give you heads with probability of 1. Think of it as a coin that only has heads. And coin 2 is also loaded. It gives you heads with 0.6 probability. Now work out for me into this experiment, what's the probability of seeing tails twice? And the answer is depressing. If you, once again, draw the truth table, you find, for the different combinations, that if you've drawn coin 1, you'd never see tails. So this case over here, which indeed has tails, tails. We have 0 probability. We can work this out probability of drawing the first coin at 0.5, but the probability of tails given the first coin must be 0, because the probability of heads is 1, so 0.5 times 0 times 0, that is 0. So the only case where you might see tails/tails is when you actually drew coin 2, and this has a probability of 0.5 times the probability of tails given that we drew the second coin, which is 0.4 times 0.4 again, and that's the same as 0.08 would have been the correct answer. So there're important lessons in what we just learned, the key thing is we talked about conditional probabilities. We said that the outcome in a variable, like a test is actually not like the random coin flip but it depends on something else, like a disease. When we looked at this, we were able to predict what's the probability of a test outcome even if we don't know whether the person has a disease or not. And we did this using the truth table, and in the truth table, we summarized multiple lines. For example, we multiplied the probability of a test outcome condition on this unknown variable, whether the person is diseased multiplied by the probability of the disease being present. Then we added a second row of the truth table, where our unobserved disease variable took the opposite value of not diseased. Written this way, it looks really clumsy, but that's effectively what we did when we went to the truth table. So we now understand that certain coin flips are dependent on other coin flips, so if god, for example, flips the coin of us having a disease or not, then the medical test again has a random outcome, but its probability really depends on whether we have the disease or not. We have to consider this when we do probabilistic inference. In the next unit, we're going to ask the real question. Say we really care about whether we have a disease like cancer or not. What do you think the probability is, given that our doctor just gave us a positive test result? And I can tell you, you will be in for a surprise. Is the relationship expressed by this scatter plot linear and is it exact? Please check all that apply. For this data, tell me is the correlation coefficient < 0, 0, > 0? Select the right answer. Consider a coin that has a probability of landing on heads of 0.7. What is the probability of it landing on tails? If we flip one coin with the probability of heads of 0.7 followed by a fair coin, what is the probability of flipping heads followed by a tails? What is the probability of having heads on the first flip or on the second flip if we have one coin with a probability of heads of 0.7 and a second coin with a probability of heads of 0.5? What is the probability that one of the two flips will result in heads. We have two dice. One has 6 sides and one has 8 sides. We select which die to roll based on whether the flip of this coin is heads, in which case was roll the 6-sided die, or tails, in which case we roll the 8-sided die. What is the probability of rolling a 6? You can assume that the coins and dice are fair. What is the probability that the coin flip was heads given that the roll was a 6? Given that we have a probability of rain on any given day of 0.2, what is the probability that over the course of a week it rains on exactly 2 days? Given that we have a probability of rain of 0.2 on a given day, what is the probability of having rain on at least two days during the week? A common measure of intelligence, IQ, is distributed with a mean of 100 and a standard deviation of 15. What is the standard score of an IQ of 130? Consider a ball that is kicked by a mean of 10 feet in this direction and with a standard deviation of 1 foot. It is then kicked back in the opposite direction towards where it started by 5 feet, but this time with a standard deviation of 0.5. What are the mean and standard deviation of this distribution of the distance between the initial position and the final position? In a population with a mean height of 70 inches and a variance of 25, what are the mean and variance of the distribution of heights in centimeters? Recall that there are 2.54 cm per inch. For a coin that's flipped 10,000 times with 4,950 of those flips resulting in heads, what is the 90% confidence interval for the probability of the coin landing on heads? Given the following set of 10 range measurements, what is the 95% confidence interval for the actual distance? For this data set an observation of 0, 0 for x and y, of 1, 2, and of 2, 2, what is the least squares regression using the equation y = bx + a. Please provide b, the slope, and a, the intercept. Rank the following four scatter plots by their correlation coefficient. One should have the lowest coefficient. Four should have the highest. Note that you're ranking by the coefficient itself, not the absolute value, so sign does matter. Hello, my name is Adam, and I'll be your assistant instructor for Introduction to Statistics. Welcome to our first problem set covering visualization. Let's get started. First, I'd like to ask you about a scatterplot, specifically about the relationship between GPAs or grade point averages in high school and college. GPAs are a measure of academic performance and range from 0 being the worst to 4 being the best, so let's look at some simulated data that might describe such a relationship. A student who received the highest GPA in both, a 4 and a 4, would be right about here. And let's fill in the rest of the points. So looking at this relationship, I would like you to tell me whether the relationship between high school and college GPAs is linear and whether it is exact. Check all that apply. And looking at this data, it certainly appears that one can see a linear relationship, perhaps something like this line looks like it might describe how they relate. But there's a fair bit of variation around it, so I would say that the relationship is linear but not exact. Now looking at this same data, I'd like to ask you a different question. Suppose we draw a line at a 45-degree angle, This represents the relationship y = x and divides the plot into 2 regions or 3 if we count the line itself. We have a region here above the line and a region here below the line. What I'd like you tell me is what the position of a point relative to these 3 sections tells us about the given students' GPAs. So in which of these regions would a student with the same GPA in high school and college fall? Would it be above the line, on the line or below the line? Select the region in which you think it is. And such a point would be on the line, because, by definition, this line has all points where this value equals this value. Now I'd like to ask you the same question except for a student with high school GPA greater than college GPA. So for someone who did better in high school than college, would a point for that person be above the line, on the line or below the line? And a point for a student who did better in high school than in college would be below the line. So you can see here, someone in the extreme case, someone with a college GPA of 0, could have any high school GPA and would still be below the line, But as the college GPA gets higher, their high school GPA must be at least equal to the college one to fall into that region. And so that is the answer, below the line. The next question I’d like to ask you about a different scatter plot-- this one relating to web analytics. We’re comparing two measurements that are fairly commonly used. One is Click Through Rate which represents the fraction of people who visit a page that go on to do something--typically clicking an ad, clicking through to a course of Udacity, clicking through an article of a new site, etc., and the time it takes to load a web page--the page load time. Now, let’s looks some simulated data. When looking at this data, I'd like you to answer a few questions. First, do you believe these pieces of data are related? That is for different page load times, is the Click Through Rate different? And then similar to what you’ve been asked before, I’d like you to tell me if the relationship between these things is linear--if in fact there is a relationship. And then I'd like you to also tell me whether the relationship is exact. And then I’d also like you to tell me two more things. Is the relationship between these positive or negative? Just recall what a positive and negative relationship are a couple of simplified examples. A perfectly positive linear relationship would look something like this, but this and this are also positive relationships, whereas a negative linear relationship would look like this, but this and this are also negative relationships. Take a look at this data and check all that apply. Well, it seems there's definitely a relationship between these. the massive point seems to move as page load time increases. I'd say--yes, they are related. They don't really appear to be linear though. If I draw a line to try and fit this, I can do okay for one piece of this but then this piece over here doesn't appear anywhere near that line, or I could possibly draw a line like this that's consistent of a lot of range of points, but this appears to be pretty far away. Definitely not linear and not exact either. You can, say, pick this value. We have a whole bunch of different click-through rates for the same page load time. Now, is the relationship positive or negative? Well, we started off with a higher click-through rate over here. It seems the only going downward as page load time increases, so we have a negative relationship. When in fact, the relationship probably looks something like this. Now, I'd like to ask you about the relationship between bar charts and scatter plots. Both of these were used to plot two different sets of data against each other. For example, home prices and home sizes. So a bar chart might look like this. For this bar chart, I'd like you to tell me which of these scatter plots looks most similar as if it could represent the same underlying data. Is it this one, this one, this one, or this one? Please check whichever one looks closest. The answer is this one. They both display a negatively sloping somewhat linear relationship. If we look at this one, the relationship look something like this, so a bar chart would look something like-- Whereas this one would just have some equally spaced bars, and this one would actually be the same thing because there's just more noise but it's the same flat non-relationship. Whereas for this one, we can see we could draw bars kind of like that. Now I'd like to ask you about the relationship between a pie chart and a histogram. Both of which represent count data or frequency data or, if you prefer, relative sizes. For this pie chart, I'd like you to tell me which histogram looks like it came from the same data. Is it this or this or this or is it this? Select the closest answer. I would say the answer is this one, and the reason for that is it has two clearly larger groups and two smaller group that are about the same size. All the groups over here are pretty different sizes and the same here, and the all groups here are the exact same size. Now, I'd like to give you a histogram and have you tell me which pie chart looks like it could have come from the same data. Is it this or this or this or is it this? Choose whichever one seems to fit best. And I would say the answer is this one, because these quadrants were equally spaced and these bars are of equal sizes. Whereas these three seem to have slices that are significantly different in size, which would imply significantly different cuts. Let's suppose we have a fair coin and we flipped it four times. I'd like you to tell me what is the probability that there is exactly one head divided by the probability that only the very first flip is a head. Please enter your answer here. The correct answer is 4. To see this, note that for the first flip to be only a head, the following is the only sequence possible. For the numerator to be true, there are three more possibilities. Now, I'd like to tell me the ratio of the probability of exactly one head to the probability of the first flip being heads regardless of the other flips. And the answer is 0.5, 1/2. To see this, recall that there were four possibilities for the numerator--all equally likely, and if the first flip is heads and we have no other restrictions, the last three flips can be anything. So each one has two possibilities and 2<i>2<i>2=8 and 4/8=0.5.</i></i> Let's assume there are two coins equally distributed, same probability of having each. The first type of coins are fair. The second type of coins have a probability of heads of 0.9--90%. Let's assume you and I both flip a coin. When I flipped, I get the following sequence. When you flipped, you get the following sequence. So please write in this box the probability that we flipped coins of the same type. And the answer is 0.523, 52.3% To see this, first, recognize that these two sequences have the same probability. The probability with a fair coin of this sequence is 0.125, whereas with our loaded coin, the probability is 0.081. So, the probability of either of these sequences of flips is 12.5% if the coin is fair and only 8.1% if the coin is loaded. So, to figure out the probability that the coin is fair, we'll just use Bayes' Rule. Note that the 0.5 simply represents that there's an equal or 50% chance of having coin 1 versus coin 2. And this expression equals 0.6080 and so the probability that we have the same coin is simply the probability that we both have type 1 or fair coins, plus the probability of my having coin 2 times the probability of your having coin 2. And since the probabilities are the same, since the underlying probabilities were the same for each flip, we have 0.608² plus (1-0.608)² and evaluating this expression gives us our answer. Now, I'd like to ask you about what happens as the number of flips or events, in general, becomes a very large number. Check all that apply. The probability of every individual sequence becomes small The probability of every number of heads becomes small. So for example, does the probability of having one head or hundred heads or million heads, any specific number does that becomes small. Does the probability of every given proportion, for example, one head out of 100 flips, 10 heads out of 100 flips or 1500 heads out of a billion flips becomes small. Does every given range of proportions have a smaller and smaller probability. Are there some ranges of proportions for which the probability becomes small as we have many flips. Check all that apply. Also, assume that the probability of heads is neither 0 or 1. That is, we can get both heads and tails from this coin. And the answer is, the probability of every sequence becomes small. This is true because there become more and more sequences. Every time you flip a coin, you double the number of sequences. And they have varying probabilities but they are always more against the probability, always goes down and always multiplying in a number less than 1. That's why this assumption is important. The probability of every number of heads become small. This is a little trickier that this is also true because as the number of flips increases, the number of heads must increase even if we're assuming some low probability for it. So, it becomes farther and farther away from what we expect. The probability of every proportion of heads also becomes small. This is a little trickier but the thing to remember is-- every proportion of heads, corresponds to a given exact number of heads. Now that exact number of heads changes as, let's say, I double the number of flips, I also double the number of heads I need. But, that's going to always be a smaller number, since again that probability has to be divided into the chance that I could just have one more head or one less head which is going to be an ever so slightly different proportion. Now, this statement is not true. The probability of every range of proportions can't become small. The range of all proportions is arranged and we know that has to stay at once, So, that can't be true and the probability of some ranges of proportions become small. Well, this will also be true--consider something improbable like getting the proportion--a proportion below 10% on a fair coin. That's going to tend to become rare and rare and rare as you flip it more times, because the same rare or unlikely event has to keep repeating itself. Let's assume we have a fair coin and a loaded coin and the loaded coin has a probability of heads of 0.9. Let's assume that we know that the probability of actually having a loaded coin is zero. In which of these cases is the probability of being fair given the flips <0.5. Check all that apply. Note that 4 H 0 T means four head and zero tails. And in none of these cases is the probability of a fair coin given the data less than 0.5. We know this simply because the probability of having a loaded coin at all is zero. So if that doesn't make intuitive sense we can apply Bayes' Rule since P of loaded is zero P of fair is one and so P of flips given fair is always going to be equal to P of flips because that's the only way you can get the flips. The alternative is zero and so this whole expression will always be one. You could also do this with the reverse in P of loaded and you'd see it would always come out to be zero because this term will be zero. Now let me ask you if the probability of having a loaded coin is 0.1, what happens to the probability of fair given flips in each of these cases. Check all that have a probability of a fair coin of less than 0.5. We have three answers--we can quickly rule out these because these should be less likely from a loaded coin that's disproportionately heads since they each have more tails than heads. We can also say that if four heads in a row is sufficient evidence that we have a loaded coin 10 and 20 heads should be two. So let's just evaluate this for the four heads' case. So the probability of having four heads in a row from a loaded coin is going to be 0.6561. Whereas the probability for a fair coin is 0.0625 but remember this just gives us the probability of flips given fair and given loaded. What is we have to multiply each of these by the probability of fair and the probability of loaded and so those are. So we can see here the probability of having a loaded coin times the probability of the flips given the loaded coin is 0.06561 and the probability of having the same flips with a fair coin times its probability is 0.05625. Together they are equal to P of flips. So only one of these divided by that can be greater than 50% so it's going to be the bigger one that's going to be P of loaded, and if P of loaded given flips is greater than 0.5 then P of fair must be less. So this is true which means this is true and this is true. And now for our programming challenge, this is completely optional and is fairly challenging and requires a decent knowledge of programming but if you're up for a challenge and want to learn to do something fun, let's dive in. We have a bag filled with coins. Each of these coins maybe a bit different. This coin may have a probability of landing on heads of 0.5 and this coin might have a probability of 0.1 Now, I'm going to draw one coin from this bag and I'm going to flip it repeatedly and each time I flip it, I'd like you to tell me what your best guess is for the probability of it being heads on the next flip. To do this, we're going to write a program in Python. What we're going to ask you to write is a Python class called flip predictor that's going to take the set of coins in the bag. In this case, coins with probabilities of landing on heads of 0.5, 0.4, and 0.3 then we're going to flip the coins several times. In this case, the results were heads, heads, tails, and heads. After every flip, we'll tell you what the result was and then we'll ask you to tell us what the probability of heads will be the next time we flip the coin. So to solve this, remember that Bayes' Rule tells us that the probability of the coin selected being a given coin is equal to the probability of the flip being what it was that it's head or tails given coin i times the probability of coin i being the coin divided by the probability of the flip either heads or tails. So now let's look at the code you're going to have to write. So here we are back in the our editor with an outline of the program you're going to have to write. First, we just make division work the way you'd expect it to work that is 1/2=0.5 so you don't have to think about it. Then we defined this thing called a class. If you aren't familiar of what a class is, don't worry, you're not going to actually have to do anything with the class. We set up the class right here for you. A class is just a way of grouping functions and data those functions can use. So here we just set up our data and that is we're given a set of coins in our bag and we just call that self.coins and so in these functions down here you're going to be able to access the coins in your bag as self.coins and that's going to be a Python list. And then we create this new list called self.probs and that's just going to be a list with the same number of items as in coins giving the probability that each point in time of a coin being the coin selected. At first, since the coin was selected at random that probability is simply 1/n so if there are three coins it's a third, if there are 10 coins it's a 10th. So what I'd like you to do is fill in these two functions. P heads is a function that just returns the probability of heads right now. So given your best guess of the current probabilities, what's the probability of coming up with heads? And you can compute this using self.probs and self.coins. Then for the slightly trickier part, I want you to write an update function and that's going to take a result either in H or a T, and based on that result, you're going to update self.probs so that p heads will return with the correct value in the future. Good luck! This is a very challenging problem. I hope you enjoy it. And here is my answer, to return the probability of heads given a set of probabilities of heads and a set of probabilities for each coin, we just need to add up the probability of head for each coin multiplied by the probability it could be that coin. To do that, I will just use this nifty little Python feature that lets me walk through these lists together called zip. We'll just take two lists and gives me the first two items, then the next two items, then next two items and so on and I call the probability of a coin coming up heads p coin and the probability of it being that coin p. And so we just need to multiply those two things together and Python has a nice little sum functions which sums up the whole thing and we will just return that. Now the slightly trickier bit is this update piece. First to make my life easier, I'll just store the current value of the probability of heads. We need this because if you recall the denominator of Bayes' Rule is the probability of the result. Now if the result is heads, we want to update our probabilities using the same basic method of walking through these two lists together. We're going to update this by taking the probability for the given coin and multiplying it by the probability that we would have gotten that result that is p coin which is just the entry from coins which is the probability of heads for that coin and since we'd got heads that is the probability of the result given that it's the coin and we just divide it by the total probability of it being heads and if it's tails, we just need to do the reverse. The probability of having tails given that it's a certain coin is just going to be 1 minus the probability with being heads. Again, we have to multiply it by the probability that it's that coin in the first place and then we again just divide by the total probability of it being tails which is just one minus the probability of being heads. And at first, I want you and me to derive this mathematically and it is challenging if you don’t like calculus, you can skip over it. So feel free to skip. If you skip, no one is ever going to know and I promise you derivation is not essential for using this formula over here. You can use entirely without understanding why it’s correct. But if you stick with me, you get a sense of how statistics really works as a scientific discipline. So it’s a glimpse into the beauty of more advanced statistics. So last chance, you’re going to skip? Hey, thanks for staying on. So here’s my quiz and it looks complicated but let me explain it to you. I better derive for you the desired equation on here which is the maximum likelihood estimator all the way from the definition of the probability of the data. You might remember data is comprised of XIs where each XI is 1 one for heads and zero for tails. And of course, P is the desired probability of heads. I left open the derivation, however, nine steps or where these nine bars that will turn into boxes very soon, and what you have to do is you have to pick from the 13 choices over here, the ones that fit best. There is always a unique answer and you can only use each of the things on the right side once. So there will be four things left, 13 minus 9, that you’ve never used. So for example, if you believe that the right answer over here is PN, then you’re going to type 6 into the box over here and PN is now used, you can't write 6 anywhere else. So good luck with this. The way the proof works is by first taking the logarithm of this expression over here, this ends up over here and we said the first derivative is zero. So you have to complete the first derivative and plug in here and then you have to transform it all the way until you achieve the final result. Good luck. The answer to the first open question is number 9. 1 - p to the 1 - Xi. And this is actually quite remarkable. The reason why I wrote p to the Xi over here is because we already learned every time we see a heads we're going to multiply in p. So by putting this in the exponent, when Xi = 1, we multiply in p. And if Xi = 0 we multiply in 1, which has no effect. This is exactly the inverse. You're going to multiply in 1 - p for the tails probability whenever we see tails, in which case 1 - Xi will be 1. Otherwise, 1 - Xi is 0, and we just multiply in 1, which has no effect. The logarithm of this guy over here is number 1, Xi log p. The first derivative of this expression over here is number 7, and it's interesting it's not number 2. There's a minus sign over here, and the minus sign is inherited from the minus sign inside the logarithm using the chain rule of differentiation. That was really non-trivial. This one will be number 11. We're multiplying in p and 1 - p. The 1 - p stays. And likewise, this here is number 4. Multiplying this out misses exactly 1 term, -pXi, which is number 3. Now we observe that these -pXi and +pXi cancel each other out, so we get this expression over here = 0, which is number 12. When we now take p to the other side, we realize there's N additions of p, number 6. And finally, we bring N back to the left side to use number 8, 1/N And we are now done. We have a set of data with a mean of 5, a standard deviation of 4, and a variance of 16. Within the data we have a point Xi with a value of 9. What is the standard score of Xi? Please enter your answer here. Recall that the standard score is the data minus the mean divided by the standard deviation. This equals 4 divided by 4, which is 1. Now let's multiply every single piece of data by 1.5 and get some new data. We'll call them Ys. Can you tell me what is the mean--mu--of the Ys, what is their standard deviation--sigma-- what is their variance--sigma squared-- and what is the value of this point--now Yi-- and what is the standard score--z--of Yi. Fill in your answers. The mean is simply multiplied by 1.5, and that is 7.5. To see what happens with the standard deviation in the variance, recall that the standard deviation is defined as the square root of the sum of the difference between each observation and the mean. Since each of these are multiplied by 1.5 and it's then squared, that means this term below the square root gets bigger by 1.5 squared. But after we take the square root, it only gets bigger by 1.5 since the square root of 1.5 squared is equal to 1.5. So the standard deviation of the Ys is 6. Since the variance is the square of the standard deviation, it's 36. Since Yi is just the individual observation Xi multiplied by 1.5, it's 1.5 times 9, which is 13.5. And the z score or standard score remains at 1. To show ourselves this, let's just redo the calculation. 13.5 is the data point, 7.5 is the mean. Their difference is 6. The standard deviation is 6. And 6/6 is 1. Here we have a scatter plot of the height of adults versus their height as children. What I'd like you to tell me is whether the standard deviation of adult heights or the standard deviation of children's heights is larger. Please check the variable that has the greater standard deviation. And the answer is adults. To see this, let's look at how the data is spread out along this scatter plot. We can see that from the mean somewhere right around here the spread between the high and the low points is something like this, whereas for children's heights the spread looks more like-- Looking at ranges doesn't really tell you what the standard deviation is, but they are both measures of spread, and when there aren't really outliers such as in this case, they tend to tell you the same thing. And then just looking at the data, it looks pretty clear that it's spread out more in this direction than in this direction. Consider this histogram displaying counts of numbers from 1 through 11. Note that each bar is centered at the value listed, so you can assume that that's the average of the values in the bar. So you can interpret this to mean there are 3 points with a value of 3, 2 with a value of 4, 1 with a value of 6, 1 with a value of 5, and so on. What I'd like you to tell me is where the mean, median, and the mode are in this data. There's only 1 right answer for each. Check the circle for each of mean, median, and mode. And for mode the answer is clear. The answer is 3 because 3 has 3 data points and no other value has more than 2. The median is also 3. We have 1, 2, 3, 6, 9, 10, 11 data points. Median value will be the 6th data point. It has 5 above and 5 below, and that happens to be a 3. The mean is a little trickier to calculate. We could probably eyeball it as being a little above 3 since if there weren't these points over here it would be 3, but let's just work it out quickly. 1 + 4 + 9 + 8 + 5 + 6 + 11. And those equal 44. And we had figured before there were 11 data points, so it's 44/11, and that is equal to 4. Consider the case where a number can be either 0 or 1 with 0.5 probability for each. What is its variance? Note that this is the same as figuring out the variance of 2 data points, 1 that's 0 and 1 that's 1. The answer is 0.25, one quarter. To see this, recall that the variance is simply the sum of the squared differences from the mean divided by n. So in this case we have--and so this is a quarter, this is also a quarter, so this is 0.5, and then we divide by 2, which gives us 0.25. Now let's ask what we expect the variance to be when we generate a couple of data points, say by flipping coins. There are 4 possibilities: 0 and 0, 1 and 0, 0 and 1, and 1 and 1. In each case, compute the variance and then compute the average of the variances or the expected variance. In the first case the answer is 0 because they're the same. In the second case we have data identical to this. The answer is 0.25. In this case it's the same, and in this case it's 0. Expected variance is the average of these numbers or 0.125. By what factor do I need to multiply the expected variance I got using the normal variance formula to get the actual variance? Enter your answer here. And the answer is, quite simply, 2. 0.25 divided by 0.125. Now I'd like to ask you if you can find a more general pattern for the correction factor for the variance calculation on a sample. This is a challenging question. The one example you did before may not have been enough for you to figure this out, so feel free to do more examples with different size samples to see if you can figure this out. Choose one of the following as the correction factor. Is it n, n + 1 over n, n over n + 1, n divided by n - 1, n - 1 divided by n, or is it n squared - 1 over n - 1? Recall that n is equal to the number of data points in the sample. Select the right answer. And the correct answer is n over n - 1. We can see this just with looking at the single example because of the other options, the only other one that yields 2 is n. That would imply that the correction factor grows linearly with the amount of data. So if we have a million data points, our variance would need to be somehow multiplied by a million to come up with the actual variance. That doesn't correspond well with intuition because, presumably, as we have bigger and bigger samples, we should be getting closer to reality, although that's not a true proof, but you can prove it to yourself by trying this out on some larger sample sizes. Now I'd like to ask you to find a median for me, but I'd also like to introduce a little twist. In the lecture, we said that the median is an element in the set, in the middle of the set. So if there's an even number, it can be 1 of 2 things. So I'd like you to give me one of those medians here. We'll call this the in data type of median. But the problem with it is it's not unique. Many people prefer to use a unique median, and to do that, they just take the simple mean of the acceptable answers for the in data method. Tell me the unique median over here. And the answer to this question is fairly straightforward. In data we have 2 possibilities. The median could either be 9 or it could be 15. So we'll accept either of those. In order for it to be unique, we just need to take their average, and that's equal to 12. Now I'd like to ask you a couple of programming questions. These are completely optional, but if you did the programming in the unit, you might want to try these out. During the unit, you wrote a function that can compute the mean of a set of data. I'd like to now ask you to write something a little different. Write a function, mean, that can take a known mean of existing data-- in this case I called it current mean-- a count of the amount of data I have, and a new value and tell me what the mean is now. And so to call my mean function I'll just type in mean(currentmean,currentcount,new) And we can see it gives me the correct answer of 9.0. Here is the function, mean, that you'll fill in. It takes the old mean which we called current mean below, the number of observations--n--and the new value to be added--x. Insert your code right below the comment. Have fun. And here is my answer. There are a number of ways to do this, but a simple one is to take the old mean, multiply it by the number of observations, which gets us the sum, add to it the new value, which gets us the new sum, and then divide it by the new number of observations, which is just one more than the old one. Now I'd like to ask you to do a slightly more challenging programming assignment. Imagine we have a die that has an arbitrary number of sides and each side can be labeled anything we want. And the die can be fair or it can be loaded. I called that here dist. You can think of it as a distribution of values or, more formally, a probability distribution, but that doesn't really matter for this exercise because in the lecture we learned about likelihood in the context of coins. Here I'd like to ask you about in the context of a die that can have any number of sides, labeled anything, with any probability. And I'd like you to write a function called likelihood that takes the data and the distribution and returns the likelihood. I wrote one. Let's see what it produces. You can see we get a very, very small number. That's actually quite common with likelihoods because there are so many different data values, any given one likely has a pretty small value. That said, they can still be interesting. Now let's look at the code you're going to have to write. Here is the function. Likelihood that takes a distribution and data, just as we saw below. Insert your code here. Have fun. And here is my answer. To get the likelihood of a set of numbers, we just need to multiply in the probability of seeing every single data point. So we start at 1 since that's the identity. For every data element we go through and multiply by the probability for element i and then we return it. If you're interested in a real challenge, note this function can actually be written on just 1 line. So for anyone watching who is a real Python enthusiast, feel free to post your 1-liners to the forums. Have fun. In the unit we learned about quartiles. Here we have a set of 8 numbers. I'd like you to tell me between which numbers the quartile boundaries occur. That is, on this side of the boundary the number is in one quartile-- say the first quartile-- and on this side it's in the second. Please check each box at which there is a quartile boundary. We have 8 elements--one, two, three, four, five, six, seven, eight. So, each quartile, since we have four of them has exactly two elements. We go two over here, here, and here are the quartile boundaries. Consider a website with 100,000 users. A thousand of those are highly active, and you want to take a survey. Now, to simplify this, let's assume that you draw the users at random and that everyone responds. If you send out 10 surveys, I'd like you to tell me the probability that the number of highly active users in our sample is greater than or equal to 2. We have at least two of our highly actives in our sample of 10 surveys. Now, remember that sometimes it may be easier to compute the probability of something being less than a given number, and you can take 1 minus that and that is always going to be equal, since the probabilities always have to add up to 1. Enter your answer here. To answer this we're going to use this trick, and that means we need to get p(x). Let's call the highly actives in our sample x to simplify our notation. So, we need that less than 2 and that is going to be equal to p(x = 0) + p(x = 1). Each of these is a binomial variable. Recall what we need to figure out is the probability of the result being true-- the probability of getting a highly-active user--p(x) = 1000/100,000, which is 0.01. The probability of x = 0--since this can only be arranged one way we don't even need to know about binomials--is 0.99¹⁰. The probability of x equaling 1 is going to be 10!/9!1!. 0.99⁹ 0.01¹. So, if we total this all together, we get [0.099573], and then we'll recall from this formula over here we wanted the opposite thing, so we just take 1 minus, this equals our answer. Now, I'd like to ask you about random walks. These are really interesting things that show up in everything from finance to physics, but amazingly they're the exact same thing you learned about when you learned about when you learned how to manipulate normal distributions. Let's assume we have some object--it could be me, it could be a point, it could be an insect, it could be a particle in a fluid-- sitting right here at position 0. Now, let's assume that it moves every second in a normally distributed way. Its movement has a mean of 0 and a standard deviation of 1. Note that in this case the variance and the standard deviation are actually the same since they're both 1. What I'd like to ask you is after 1 second tell me what the mean and standard deviation of the object's position is. Please tell me the object's mean position and the variance of that position. Since on average it didn't move, we just add the mean of its starting position to the no movement, and we get a mean of 0. But it has a standard deviation or variance of 1. Now could you tell me what is the mean and variance of its position after 2 seconds? Its mean is still at 0, since it's on average not moving. To get its variance, we have to add up two normally distributed random variables. Recall that variances add, so the answer is simply 2. Now let me ask you a somewhat more challenging question. What is the distribution of the object's position after 10 seconds? The mean is still at 0, but for the variance we now need to add up 10 individual variances. There is one each time. Since all of these are equal to 1, adding them is the same as saying 10 1, which just equals 10. Now that you see how this works when µ = 0. I'd like to change it. When µ is not equal to 0 we call this a random walk with drift, since it tends to drift in one direction or the other. In this case, we're going to have it drift to the right, specifically, we'll say that it now has a mean of 1 and a variance of 2. Apologize in advance. I'm sure my drawing isn't quite to scale. Now, to really test your understanding, I'd like you to tell me at 1 second and 10 seconds what are the mean and variance of the object's position? And at t = 1 it is fairly straightforward. It moves on average by 1 to the right so the mean is going to be 1. The variance is going to be 2. At t = 2, it's now moved 1 more to the right no average, so the mean is 2, and the variance is going to be 4. At 10 they also just both multiply. The mean will be 10, and the variance will be 20. Now I'd like to ask you one more question about these random walks. What is the standard deviation of its position at each point in time. Please fill in your answers here. This is fairly straight forward. We just take the square roots of each of these numbers. The square root of 2 is approximately 1.414. The square root of 4 is simply 2. The square root of 20 is approximately 4.472. What's interesting about this is that these tend to grow more slowly than does the mean or the variance. They don't grow linearly with time. Now let's divide this line into several regions. What I'd like you to tell me is in which of these regions could our object be at t = 2. Here are boxes for each of these regions. Check all that apply. The answer is that it can be in any of these regions. All of the boxes should be checked. To see this, remember that our probability at each point is this formula, and the key thing to remember about this formula from the lecture is it becomes very close to 0 in both directions, and the probability of it being here or here is very small. It's not 0, so it could be here. In opinion polls, we often report a margin of error. Something like this margin of error is the same as the confidence interval width. Usually, the margin of error in polls is stated at the 95% level. Consider two candidates. In our poll, candidate A has 55 supporters. Candidate B has only 45 supporters. Please tell me what we would report as the results of the poll. Percentage of support for candidate A plus or minus the margin of error at 95% confidence. Of course the percentage support for candidate A is 55%. To compute the margin of error, we will use the following formula. 1.96 the quartile from the normal at 97.5% times the square root of P times 1 minus P equals n and substituting n and evaluating this expression the square root of 0.55<i>0.45/100<i>1.96 is equal to 9.75%.</i></i> Let' assume we want a lower margin of error to say 5%. How many people should we ask? Now, you might say that the answer will depend in part on what we see the response is being but to simplify this let's assume that 55% still will support candidate A. Well that won't quite be right. So to answer this question, we'll need to solve this formula for n. Let's call the margin of error e. We can actually simplify this. If we just square and divide, we get e²/1.96² is equal to the old variance 0.55<i>0.45/n.</i> We actually know e. We specify that is going to be 0.05. That gives us, if we multiply through by n, so if we calculate this out we get 380.3. Now since it's pretty hard to ask a fraction of a person, we actually have to ask a bit more. We have to ask 381 people. In answering how many people we actually need to poll, we made an assumption. And the assumption more or less is that small changes in p don't matter. So we don't have to worry about getting it exactly right if we're trying to estimate a sample size. To prove this to ourselves, let's redo this calculation with p = 0.5 and p = 0.6. Please enter your values for n here. Looking at our solution over here, we can see that the only place this matters is p(1-p) flows right through to down here. So we need to change these to numbers. So this is going to be 0.50.51.96²/0.05² and for this it will simply be 0.6 times 0.4 times the same thing. For p = 0.5, this evaluates to 384.16 and for 0.6 it evaluates to 369.79. Since again we can't have fractional people, the answer is 385 and the answer for 0.6 is 369. A quick way to see why this changed so little is simply that this is the same. N 0.50.5 is 0.25. N 0.60.4 is 0.24. Often we find in reality that the margin of error is wrong. Which of these reasons could make us understate the true margin of error? If the poll is taken before the election at a different time, does that change the margin of error? If some voters are under-represented in our sample, could that increase error? Similarly, if some in our sample are less likely to vote than others, does that affect our error? Now, assume that we haven't somehow perfectly matched up the frequency of voting and their relative representation in sample. And if the responses differ from what someone intends to actually do, could that cause more error? Each of these things can cause more errors. The answer is all of them marked. The poll being taken before the election can certainly affect its accuracy because sentiments shift over time. Some being under-represented in the samples. Certainly, if voters who favor one candidate versus another are under or over-represented that will change the results. Similarly if we tend to have people who are more likely to vote represented the same as those who are less likely to vote, they're not being over-represented enough. So these two things ideally should be linked. And in a well-designed poll, they try to do this although it's hard to do this perfectly. And responses that differ of course will cause additional error. Let's assume we have two different ways to lose weight, and we want to figure out which one is the most effective. We have 10,000 people who received Treatment A. Their average loss is 10 pounds. The standard deviation of their loss is also 10 pounds. Let's consider a second treatment, Treatment B. We also applied it to 10,000 people. Our average loss in this case was 20 pounds. And we also have a standard deviation of 20 pounds. Our null hypothesis of course is that weight loss from Treatment A equals the average weight loss from Treatment B. Our alternative hypothesis is if we are perhaps the providers of Treatment B is that WB is greater than WA or alternatively that the difference is positive. With a 5% allowable false positive rate, which hypothesis do we choose to accept? To answer this, our variable of interest is the difference of these two things that has a mean of 10 pounds and its standard deviation is going to be the sum of this standard deviation and this standard deviation. Now we need to find out the standard deviation of this. Since variances add, let's work with those. So the variance of Treatment B is 400/10,000 and the variance of Treatment A is 100. So if we evaluate this, this will give us the standard deviation of this number. And it's 0.2236. So we can see the ratio of this to this. This is much, much greater than the critical value of 1.645. And so in this case we reject the null hypothesis and therefore accept the alternative. And there's one particularly interesting feature of these two treatments as they relate to each other. Just consider how many people lost weight in each case, and post your thoughts to the forums. Now, I'd like to give you a number of situations where you might be able to use hypothesis tests. In which of these situations would it be helpful to use a hypothesis test? Comparing crash rates of two different airlines with 100 flights each. In case you don't know but I assume most of you do, planes don't crash very often. Comparing five-year auto repair rates on 100 vehicle samples. Comparing the heights of the tallest buildings in two different cities. And determining if a course improved test scores in some sample of students. And I would say the answers are comparing auto repair rates. Those are simply proportions that can be compared to each other as we've done for many things. And determining if a course improve test scores. We would argue that we can't compare crash rates of two airlines with 100 flights because in general there aren't crashes. Now in theory, you could say you can use a hypothesis test if you recognize that it's not normally distributed and do the appropriate binomial test and figure out that you can't determine anything but it's generally not adding anything over intuition. And comparing heights of the tallest building in two cities, that's a deterministic number. It's either their taller or it's not. There are no random errors. There's no need for a hypothesis test. That again a more creative application could be that you'd want to use it to the measurements of the buildings in each of the two cities to determine whether one was taller but that's only going to be useful if the measurements have the error on the same order of the difference in buildings which is unusual. So in practice you would not use a hypothesis test there. As the sample size approaches infinity, what happens to the confidence interval width? As the sample size approaches infinity, the confidence interval width approaches-- 0. Some constant that varies based on the problem. The answer is 0. See this. Remember that the width of the confidence interval decreases with the √n. But as n goes to infinity, the √n goes to infinity and any number divided by infinity is 0. Consider two marketing campaigns. Campaign A has a response rate of 1%. And we have a second marketing campaign, Campaign B, that has a response rate of 1.01%. For each of these, we have 10 million data points. At 95% confidence, is this significant? And the answer is yes. You might guess this just from the number of observations, but let's go through the math. So we have the difference of 0.01% or 0.0001, divide that by the total variance which is-- And this is equal to 2.24. This is significant. Let's consider this set of data. We have our x's and our y's. Y = bx + a. Please tell me what b is. Enter your answer here. Before you start computing, just take a look at the data. Of course, since y is always 0, b is going to be 0. Let's say we get 1 new data point. We have 1000 for x and 1000 for y. Now tell me what b should be with this new data. Please enter your answer here. To compute this, recall that b equals the sum of the product of the difference of the x's from the mean and the y's from the mean divided by the square difference between x and the mean So, the mean of the x's is 104, and the mean of the y's is 100. Now let's write out deviations for each term. Now for the differences of the y's. If we add up the product of these, we get a fairly large number--896,000. The denominator is equal to 892,100. Taking the ratio of these two things gives us our answer of 1.004. This is interesting, because it suggests when we have data like this but add one single point all the way over here, our best fit line goes from being this to being this. Now, note this isn't quite to scale. This point would really be all the way over here. Now let's look at the relationship between correlation and regression as we make changes to our data. Let's consider the following simple data set. We have x of 1, 2, and 3 and y of 4, 7, and 13. Consider the regression coefficient for the equation y = bx + a and the correlation coefficient. Please compute b and r and enter your answers here. To answer this we'll apply these two formulas. First we'll compute x-bar and y-bar. X-bar is 2. Y-bar is 8. Now we'll compute our differences-- -1, 0, 1 for the x's, -4, -1, and 5 for the y's. The numerator of each of these is 4 + 0 + 5 = 9. The denominator of b is 2, so b is equal to 4.5. Now, to sum up the square differences and the y's, we have 16 + 1 + 25 = 42. The product of these is 84, so we have 9 / √84, which 0.982. Now, I wonder what happens to b and r if we double our y's. Fill in your answer here. Again we'll compute our x-bars. X-bar equals 2. Y-bar is 16. Now we'll take the differences. They're the same for the x's as they were before. The numerator for each of these is denominator for b, so b is 9-- interesting, exactly double what it was before. This always holds true. If we double our y's, we always double b. Now let's see what happens to the correlation coefficient. We take 2 64 + 4 + 100 = 168. The correlation coefficient is 18 / √336, which is 0.982-- exactly the same as it was before. Now what happens if we double our x's as well? Please enter your answer in the boxes over here. I won't bore you with the arithmetic again. It's the same as it was the last two times, and we wind up with the same answers we had the first time. B is equal to 4.5, and r is equal to 0.982. What I hope you've noticed is that r doesn't change when we scale x or y. B doesn't change as long as we scale x and y by the same amount. I'd like to challenge you to find an algebraic relationship between b and r. Can you tell me what is b / r? Is it 1 / Σ(y - y-bar)²? Is it Σ(x - x-bar)² / Σ(y - y-bar)²? Is it the square root of Σ(x - x-bar)² / Σ(y - y-bar)² Or perhaps is it--choose the right answer. The answer is this. It is equal to the square root of the sum of the deviations in the y's over the deviations of the x's. Note that this is exactly the same as the standard deviation of y divided by the standard deviation of x, because the number of observations divides out. To see that this is, in fact, true, when we divide b/r we can see the numerators cancel, so we have b/r equal to the square root of the squared differences in the y's times the sums of the square differences in the x's over the sums of the square differences in the x's. The key thing to recognize is we can break this into two pieces. The denominator can be written as the square of the square root. Now we can just cancel this and we have our answer. Let's now consider linear regressions of standard scores. Recall the standard score for x, zₓ is equal to xᵢ - x-bar divided by the standard deviation of x. Let's assume we standardize both x and y, so we now have a set of zₓ's computed this way and zᵧ's for the equation zᵧ = b zₓ + a. I'd like you to tell me about the values of a and b. Specifically, tell me what is their minimum value on any given data set and what is the maximum. Please fill in your answers here. For a the answer is 0 and 0. A can only ever be 0. To see this, recall that a is equal to y-bar - b x-bar, but for a standard score, zₓ always has a mean of 0 and zᵧ has a mean of 0, because we subtracted out their means. A must always be 0. B must always be between -1 and 1. To see this, recall that--as you just discovered--b is equal to r times times the standard deviation of y over the standard deviation of x. Since these are each 1, their ratio is 1, so b must have the same range as r from -1 to 1. Now I'd like to ask you a tricky question about regression that will illustrate why regression got it's name. Why is this thing that gets us the best fit of a line? What does that have to do with regression--going backwards? I'd like you to tell me for any two variables x and y, if they're not perfectly related--that is if the absolute value of their correlation is not 1--how extreme is our y prediction compared to x? By extreme, to be a little more precise, I mean this. We have x. We have some observation in the x's--say right here. We have a total probability that's denoted by this little shaded region that a given xi is either at that value above--farther away from the mean or here farther away in this direction. We can map that to a y using our regression equation, and we'll get a value in the y distribution, and it will have, again, some probability of being that value or more extreme. The smallness of that probability is what we mean by extremity. One percent chance is more extreme than a 40% chance. One in a million is more extreme than 1 in 1,000. What I'd like you to tell me is will y be more extreme? Will y be as extreme? Will y be less extreme? Or it depends on the data. Please check the right answer. Y will always be less extreme. The key to understanding this simply is this equation. This is what tells us that the standard scores have a regression coefficient â‰¤ 1. We said it's 1, and so because it's not 1 it must be less than that. That means the standard score shrinks. If the standard score shrinks that means if it's negative it moves this way. If it's positive it moves this way. That means there is more area under here, so this has to be a little more area than this. Therefore, y will always be less extreme. This is the concept of regression. That is regression to the mean. If someone is exceptional in one thing, we will always predict that person to be a bit less exceptional in that other thing if there is any error.