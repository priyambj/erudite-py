Hi, my name is Sebastian Thrun. My name is Katie Malone. And this is the machine learning class here at Udacity for everybody to learn machine learning. I am a research professor at Stanford. I am the founder of Udacity and I also did things like Google Self Learning Car, The DARPA Self Driving Car, Challenge Ways, Google Street View, Google Glass, Google X. And the secret behind my work is, it all reduces to machine learning. Machine learning is the core of what I do and machine learning enables me to build amazing products and amazing user experiences. I'm a physicist by training so I have a little bit of a different background. But in physics we use machine learning all the time too. And so what we do, is we start with gigantic data sets of lots of particles that we create in our detector. And we use machine learning to comb through those huge data sets and to find the one event. Or the ten events out of the millions and billions that help us win Nobel prizes. And doing good machine learning is just like making great wine. So I have a bottle of wine here from Italy. the, analogy goes as follows. If you want to, going to make a really great wine you have to make great grapes. And if your grapes are no good, your wine is no good. Same is true for machine learning. You need good data. You need to really understand your data really well. And then there's a technique to it to make really great wine and it's really hard to learn. Same is true for machine learning there's a technique to look at data, to understand data, to be able to apply algorithms. And if you don't master the technique you will never be a great machine learning person. So in this class, in our machine learning class we're going to start with a bunch of machine learning algorithms. But we're also going to be looking at some really interesting datasets. And we're going to be asking some really interesting and kind of hard questions. But questions that we think are going to be really cool to answer. So we're going to start from raw data, and then we're going to show a machine learning process that sort of goes from beginning to end. So that you can start with a dataset of your own. And you know how to tackle it using machine learning. We are here in Silicon Valley. And in Silicon Valley machine learning is a big deal. Companies like Netflix, Amazon, Google and many others use data on a daily basis to optimize the product experience. And I can tell you, if you're proficient in big data and understand data science really well, you will find a job in Silicon Valley, very likely. And the starting salaries are unbelievably high. They are often north of $150,000 to an $200,000. And even if you're not looking for a job in data science right now. There's still probably lots of ways that machine learning is something that you use everyday. So when I go on Netflix at night. How do I figure what, what movie I'm going to watch, Netflix makes recommendation using machine learning. When I use my credit card, to make a purchase. There's fraud protection on it, that's machine learning as well. You're get some insights into how all kinds of tools that you use everyday are working, and they're built on top of machine learning. This is a really great class. It's learning by doing. We'll see you in class. We'll see you in class So here we are in the heart of Silicon Valley at Google. Machine learning is all over Silicon Valley. Yeah, here at Google we're using machinery for, obviously for search, for ranking the little pages that you going to see, for speech recognition, for language translation and of course, for self driving cars. One of my favorite examples is a little bit more trivial than that, but I think it's really fun. There is a company in Silicon valley that I actually tell it what clothes I like, and then it'll figure out what my tastes are and it'll sell me new clothes so I don't ever have to go shopping gain. It is a machine learning algorithm? It is a machine learning algorithm. It is. So how did you have to train it? It's really good, I didn't have to train it that long. Wow. So is there a 10% bonuses? I think, there probably, there should be, right? [LAUGH] Here we go. So Day, age of big data, yeah? And, there is no company within a 10 mile radius that wouldn't use big data, including Udacity, to optimize its products. So, that's what we'll be learning about in this class, there's all kinds of interesting examples here. And, we'll go in great depth into a few questions in this class, but you should always be thinking about interesting problems that you'd like to solve, and how machine learning can help you out there. And look, if you do a great job. We do our best to find you a job here in Silicon Valley. Let me tell you a little bit more about the format for this class. We'll have a lot of lessons that are interspersed with quizzes quite liberally. So you'll have a chance to practice the ideas that we're teaching you as we're teaching them to you. In addition to that, we'll have mini projects at the end of each lesson. And in my opinion, this is where most of the learning will happen. This will be where you get your hands on the keyboard and you actually practice coding up the things that we're teaching. And to that end, it's very important that you have a computer that has Python working on it because that's what you're going to need in order to complete those mini projects. At the end of the class, we'll have a final project that ties together pieces from a lot of those mini projects into one big machine learning analysis. And this is really cool and really exciting. And this will be the way that you know that you've learned something really concrete and really fantastic. Here are a few prerequisites that will make you maximally successful in this course if you have them. The first is programming experience, especially in Python. We'll be making a lot of use of machine learning libraries that are available to us in Python. And so, when we go in and we start to look at the code, we'll be really zeroing in on the machine learning parts of it and not particularly the Python programing aspects of it. So that should be something that you're fairly familiar with already so that you don't get stuck at, at the programing aspects of this class. The second thing is that you should have a little bit of familiarity with statistics. We mostly assume that when we introduce the statistics concept, we'll, we'll give you an explanation of it and some intuition. But if you've taken something like Udacity's descriptive and inferential statistics, then you'll be right on point to, to follow along closely with this class. In addition to that, you should have a wicked sense of curiosity, and you should be really excited about solving interesting problems and, and using, getting your hands on really interesting data. Okay, we are here in the Google Self Driving Car. We're inside. It's obviously driving itself. And it's doing everything by itself. And I am here as a passenger. I'm really hoping that Sebastian did a good job in training this, because he is not driving right now, and I- I am not driving, it drives better than me. I am at the mercy of this car. That's true, it does drive better than me. So why are we talking about self driving cars then? Why are we talking about self driving cars? Well we're going to start by talking about supervised classification, and self driving cars are one big supervised classification problem. What does supervised mean? Supervised means that you have a bunch of examples. Where you know sort of the correct answer in those examples. So I know that you have example of this from the self driving cars. Yeah so I mean we, we train our car and we show the car what's the right behavior. And we did the Autobahn challenge we would take it out for a spin and it would very carefully watch us human drivers drive and would emulate our behavior. And in fact, this is sort of how people learn to drive, right? Yeah, I think so. When I was a child, I watched my parents drive. And they weren't really good drivers, but I would still copy a lot of stuff from them. Yeah, so it's kind of like, in the way the humans drive by watching lots of examples. That's what computers do when they're doing machine learnings. You give them lots of examples, and they start to figure out what's going on. That's what we'll be learning about. And it's totally true. In this unit, you're going to be learning about. Machine learning, the same way we, we program steps on a car. You're going to program data and, and test out whether you can make a car go fast and slow at the appropriate time. Using machine learning supervised learning. That's right. So we're going to be looking at a really cool terrain classification problem that was very important for Stanley. You want to introduce that? So, in Stanley's case, I was driving through the desert and the desert terrain is, like, ruts. And boredom can be very brutal. So it you drive too fast, you are running the risk of flipping over and destroying yourself. So one thing we've trained the car to do is to really slow down at the appropriate time. We did this not by writing little rules, we did it by us demonstrating to the car how we drive. And it will just emulate us. How many miles did you have to drive to train that. Oh thousands of miles. We spent thousands of miles everyday in the desert. Wow. And it took quite a while for me to copy. Smart. Your poor grad students. I can only imagine. Well, I was the guy who had the pizza for everybody, but, it was a great time because it was no email, we just had the, us and the software. And every time you got the software back it was very obvious. The car would punish us. Oh, that sounds great. So, I think we should probably get started with that. Let;s try out a few different supervised classification problems. Yeah, so the unit's all about supervised learning, so let's dig in. Sounds great. Machine learning is all about learning from examples. So I'm going to teach you something right now from example, something that you almost certainly don't already know. That is whether a horse is acerous. Do you know what acerous means? What is acerous? I'm not going to tell you. [LAUGH] I'm going to give you a bunch of. Then teach me. I'm going to give you a bunch of examples. And then the question for you is whether you think a horse is acerous or not. Okay. Okay. So here's, here's some example animals. It's made in China. Has nothing to do with it being acerous. These are some examples I'm going to tell you for each one whether it's acerous or not. An elephant is acerous. A ram is not acerous. Let's see a triceratops, the dinosaur, is non acerous. Lemurs are acerous. A dog is acerous. A giraffe is non acerous. A goat, also non acerous. A cat is going to be acerous. Parrot is acerous, and last a deer is non-acerous. And so the question for you is, which one of these groups would you put the, the horse in with? It's hard. I guess it looks a bit like a giraffe, maybe it's on that side. Doesn't look like a parrot. I put it here. Okay, so you, this is the, this is the non-acerous group. Yeah. Now how confident are you at this? I'm not at all. Okay. If I give you 100,000 examples would you be more confident do you? Yes. Please give me 100,000 examples. These are all the toys I could find. So before we go any further I'd like you to give me your answer of what you think, what you think for the horse is. Is it asorus or non-asorus. Just based on the examples here I hate to tell you this. You did not get it correct. But let- So what did I do wrong? Yeah, let me tell you. So, when you're looking at the, at the new example, you have to figure what parts of it to be paying attention to. We call these features. So a feature might be the color. It might be- Like, white. How many legs it has. Four. It might be whether it has horns or antlers. Oh. So a horse does not have horns or antlers. You're right. But these all have horns and antlers and that's what- I see. Acerous means, is that it's lacking horns or antlers, antennae, things like that. So that's why a horse would be a, an, an example of an acerous animal. Okay. And this is, in general, what machine learning is going to be doing is we give you a bunch of examples and each one has a number of features, a number of attributes that you can use to describe it. And if you can pick out the right feature and it's giving you the right information then you can, you can classify new examples. So a smart machine algorithm would have outperformed me? I think so yeah. Darn so let's, don't take this class because I don't want all you guys to outperform me. Well no I wouldn't go that far. I think the way to think about it is what's the way that we could use machine learning algorithm to get problems like this right and that's the whole point of this lesson. Okay All right, so let's have a quiz about supervised classification examples. Here are a few examples of problems and you tell me which ones sound like supervised classification would be the right tool to try to solve them with. So the first one is, using an album of tagged photographs, try and recognize someone in a picture. Or analyzing bank data for weird-looking transactions, and when you find something weird, you flag that as possible fraud. The next one is, given someone's musical choices and the features of the music that they like, say, what the tempo is or the genre, recommend a new song that they might like. And the last one is clustering Udacity students into different types based on the learning styles that they have. So, place a check next to which one of these options you think are examples of supervised classification problems. So, here's what I would say. I would say that the first example of recognizing someone's picture from a bunch of tagged photographs is definitely a supervised classification example. This is something that Facebook does all the time when it does recognition on the photos that you upload there. Another popular one is recommender systems like Netflix and Pandora. They look at the stuff that they know that you like already, and they try to figure out what the features of, of those things are, those movies or those songs, and it recommends more movies or songs that have similar features to you. Fraud is probably not an example of supervised classification and that's because we haven't really defined what a weird-looking transaction is. We don't have an example of what that might mean. So there's ways that you could still do this, but this would be an example of unsupervised learning and we'll talk about that a little bit more later in the class. Another thing that's usually unsupervised learning is clustering, so in this example, we don't know what the types of students are. We might not even know how many different types we're looking for. There's still ways that we can approach this problem, but this isn't an example where you have a big bank of training data where you already know the correct answer. So now we will talk about features and labels. And I'm going to be using Katie, as an example. See as my artistic rendering of Katie. And Katie loves listening to songs. And I know it's cliche but one of her favorite song, is called Let It Go. She really sings it all day long. Can't hold it back anymore. [LAUGH] Okay. All right so it's not about singing lessons it's really about machine learning. So in machine learning we often take as input features and we try to produce labels. So let me explain what this is. [MUSIC] Okay so we take this music and we don't input the raw music into her ears. Katie for a sec please. Okay. Instead we extract what's called features. These might be things such as intensity, or the tempo of the song, or things like genre, or perhaps the gender of the voice. And then Katie's brain processes it into one of two categories, there is like and don't like. Obviously an oversimplification of Katie's brain, but for the time being, let's assume that's what's happening So, here is how this looks to a machine learning person. We're going to produce what's called a scatter plot. And, for simplicity, I'll just use two features. There's tempo and intensity. And, tempo might go from relaxed to fast, and intensity, for the time being, will be light or soaring. So, the song, Let it Go is mid-range in tempo, but quite intense. So, it would be a data point right over here. In for those songs, you can extract those features. And, each song becomes a dot in this two dimensional diagram of tempo and intensity. This is slow song, it's very intense. And, this is a slow song that's just very light. So, I'm adding here data points for many songs. And, suppose, the green ones are the ones that Katie likes. Where as the red ones over here, are the ones that she doesn't like. Suppose we encounter a new song with features for right over here, in the spectrum. Just looking at the data itself, that Katie would like this song, or doesn't like this song. So, just check the box on the right for like, or don't like. And, yes, the answer seems to be she likes those. We can't be absolutely sure, but it looks like she likes the high, soaring intensity songs, and she's not a big fan of light songs Now let's look at different data set. Let's take myself. I have a very different taste so in my world I don't like the But here's my situation. I'm much more scattered in the way I look at songs. If I now hear this new song right over here what do you think is the best answer just looking at the data. Do I like this new song? Do I not like it? Or is this plainly unclear? Take your best guess. And I would argue it's unclear. I mean, there might be a pattern. Maybe the songs that are like fall into this set over here. And as a result this point is included. Maybe instead they fall under the set over here and as a result is excluded. I can't quite tell, it looks complex. But one thing I can tell by looking at data like this. They also called scatter plot. I can learn a whole lot about my preferences and possible results even before applying machinery. So you might remember in the beginning of this unit we talked about this example where we use supervised learning to train the speed of our robot Stanley. Now we are going to do the same thing in the simplified example. So picture a robot like the self driving car in the desert adjusting it's speed. It'll do it based on some features we can observe and the features we chose in this example are just two of them. There's the steepness of the terrain. Whether it's flat or whether it's a hill. And then there's the ruggedness of the terrain. How much the vehicle bops up and down which we can, we can actually measure inside a car. And what we do is we're going to set up an example where you're going to be the machine learning specialist and you will learn, you will, you will help Stanley decide whether to go fast or slow. Using supervised machine learning. So, let's begin. So let's talk about something called scatter plots. And I'm going to use the current example. When we drove our car we had days when the train was completely flat, let's us drive very fast. Sometimes we encountered a little bit of bumpiness or a lot of bumpiness. Which kind of forces us to slow down. So one way in which our terrain was able to vary, was at the level of bumpiness. Which could go from smooth, to really bad. Second dimension, which our terrain varied had to do with slope. Could be flat. Or, they were different levels of steepness. I'm obviously slightly exaggerating here. So it goes from flat to very steep. And, again, the steeper it is the slower we wish to drive. And, those things could co-occur, as shown in these speed graphs on the right side. So what I want to do with you is just map these training examples over here, of say a flat, smooth vote, into our diagram over here. And I give you nine possible points that correspond to all the combinations of the three bumpiness levels we encountered. And the three slope levels. And when I ask you to take a car set away from, like, this one and then define the correct of the main items that best represents the situation over here And, I'd say this is smooth and flat, so it goes right here. How about this situation here? Which one do you think best represents the situation over here? And it says that this very steep, but not that bumpy, it's medium bumpy. So very steep medium bumpiness goes right here. And just for the fun of it, let's do it with this guy over here. Where does it belong into the scatter down here? Well it's super bumpy but still flat. That would be the point right here. What I want to get you to understand is that these individual driving conditions can be plotted in a two dimensional diagram where each situation becomes a data point. That's called a scatter plot and for the three examples that are circled these three data points are kind of identical. They have a scatter plot of those three situations. So they often, instead of looking at the raw data, we plot the data in a diagram just like this. So let's look at another scatter plot. Suppose this is your data, and again it comes in two categories in two dimensions. And now we get a new data point, and that came out over here, drawing the little question mark. One of the absolutely important questions in machine learning is what can you say about a new data point you've never seen before. Given the past data and just your intuition, would you believe that this user point is more like a red x or more like a blue circle? Check one of the two boxes over here. And I would argue it's more like a blue circle because it sits within the blue circles over here and not between the red x's. So how about a data point down here? This time, I give you three choices, blue circle, a red x, or unclear. Just pick one of the three. And, and I would pick unclear, we can argue this, maybe it's a bit closer to the blue circles perhaps to the red crosses, but to me so much in the middle it's unclear What our machine learning algorithms do is, they define what's called a decision surface. They typically lies somewhere between the two different classes, like this one over here. And on one side they will predict red cross for every possible data point. Whereas on the other, they predict the opposite concept. So with the decision surface over here, it's not easy to say what the label would be for data point over here. And again, I give you two choices, red cross or blue circle This one wasn't particularly hard, it's kind of opposite, this is more of a red cross than a blue circle. But the nice thing to understand, this decision surface over here, that I drew in, we separates. One class from another class in a way that you can generalize two new never before seen data points. When the decision surface is a straight line we call it linear. And that will give you three decision surfaces, and you should click on the one and one only, that you think will best channelize the new data. And this was a tricky question. I would pick this one. This one over here is obviously not good because we even misclassifying our existing data. Is on the, these are on the wrong side of the decision surface. So let's take this one out. But the one going vertical here does correctly classify most data points. Except for this one over here. And also comes very close to the red cross over here. That's something we're going to explore later in this class. But for the time being what you should notice is that what you're machine learning algorithm does it takes in data and it transforms it into a decision surface. DS. That for all future cases. Can enable you to make a determination of wether it's a red cross or a blue circle, and that is very powerful. So anyways, what's next in this class, you're going to learn of what's an equivalent of Naïve Bayes. Bayes was a religious man trying to prove the existence of God. He wasn't very naïve. But the algorithm is kind of naïve and it's a very common algorithm to find at a certain surface, just like this Okay, so let me show you where we're going with all this. We're going to zoom ahead on supervised classification. So here we have the driving data set, 750 points now that we've drawn in our scatter plot. And the goal is to draw a decision boundary that will help us distinguish which terrain we need to go slow on and which terrain we can go really fast. So that means being able to draw a boundary that looks something like this, where we're able to divide the two classes. And here's what that's going to look like. So we have our decision boundary that we can draw in here between our two classes, and for any arbitrary point, we can immediately classify it as terrain where we have to go slow or terrain where we can drive really fast. All right so we kind of zoomed ahead to the end there. So let's take a step back and walk through what I just did but much more slowly. You're going to see all the steps that I went through when I wrote the Python code that just made that decision boundary. And by the end of the next video or two, you will be able to write this code yourself. So the place that we start [LAUGH] as always maybe is Google. So there's a Python library that I'm going to be using a lot in this lesson, and we're going to be using Google to help us use the documentation of that library to figure out how to use some of the functions that it has. The name of this library is scikit-learn, which is often abbreviated sk-learn. So let me get you used to that convention now. So I'm going to search google for sklearn and the name of the algorithm that I just used, which happens to be Naive Bayes. We'll go back in a little bit and talk about what Naive Bayes does exactly. But first I want you to have you running the code. So sklearn Naive Bayes. See what's out there. There's a page on Naive Bayes here that gives you a derivation of the Naive Bayes formula and then a bunch of use cases. Including something that says Gaussian Naive Bayes and, as it turns out, this is what I'm going to be using. So let's head back 'because I saw Gaussian Naive Bayes as one of the other results on Google. That's this one. Gaussian Naive Bayes, this is what the way that I actually wrote classifier that you just saw Okay, so now what I've done is I've gone to the Gaussian Naive Bayes documentation page. sklearn.naive_bayes.GaussianNB. This was that algorithm that I set out to find and now that I've, now I've found the SK Learn documentation page. So the first thing that I see right here, actually this is one of the things I love about the SK Learn documentation, is it's full of examples. When I was actually developing the code for this class, this was one of the first things that I would always do is I would come find the example code and I would try to just run in my Python interpreter, see if I could get it working. And almost invariably it works right out of the box. So here's something that's just very simple. There's only a few lines here that are really important. So let me point them out to you and then I'll show you the code I've actually written for the example we just saw, and you'll start to recognize some of these lines. But first let's introduce them. So the first one that's really important is this one right here. Above this it's just creating some, some training points that we can use, it's not that important. This is where the real meat starts, is with this import statement and if you've programmed in Python before, you're well acquainted with import statements. This is the way that you bring in external modules into the code that you're writing so that you don't have to completely re-implement everything every time, you can use code that someone else has already written. So we say from sklearn.naive_bayes going to import GaussianNB. Very good. The next thing that we're going to do is we're going to use that to create a classifier. So classifier equals GaussianNB. If you miss your import statement. If you forget this line for some reason, then this line is going to throw an error. So if you end up seeing some kind of error that says that it doesn't recognize this function. It's probably a problem with your import statement. So, okay, we've created our classifier. So now the code is all sort of ready to go. The next thing that we need to do is we need to fit it. And we've been using the word train interchangeably with fit. So this is where we actually give it the training data, and it learns the patterns. So we have the classifier that we just created. We're calling the fit function on it, and then the two arguments that we pass to fit are x, which in this case are the features and y which are the labels. This is always going to be true in supervised classification. Is that it's going to call this fit function and then it's going to have the features. And then the labels. And then the last thing that we do is we ask the classifier that we've just trained for some predictions. So we give it a new point. In this case the point is negative 0.8, negative 1. And we ask for this what do you think the label is for this particular point? What's the, what class does it belong to? So in this particular case it says it belongs to class number one. Or you could imagine for some other point it might say class number two. So of course you have to have already fit the classifier before you can call predict on it. Because when it's fitting the data that's where it's actually learning the patterns. Then here is where it's using those patterns to make a prediction. So, that's it. That's kind of, now you know most all there is to know to get this working in the first example that I've done. All right, so let me actually show you the code that I used to do that example that we just saw, and then you're just going to be copying this code in the quiz to make sure that you get all the details. This stuff that I've just highlighted is the code that's really important, and hopefully this looks really familiar to what you just saw in the example. So, what I've already done up to this point is I've loaded in my training data and I've made some visualizations of it. We're going to ignore that for now. But you'll be able to see the code in the quiz if, if you're interested. And then here, as I said, are the four lines that are really important. So, we have the import statement like you just saw in the example. Then we create the classifier. We fit it, using our training features and our training labels. And then the last thing that we do is we create a vector of predictions, this called pred, by using the predict function on our trained classifier. And then to that we pass the features. As part of the code, it will actually draw this scatterplot and put the decision boundary in on top of it. So, the thing that you should see when you complete this code is an image that looks like this one. If you see this image, everything's working right. All right. Fantastic. So we're almost there. There's going to be one more thing that I'm going to ask you to do before we finish with this unit and that is to evaluate our classifier. To quantify how well it's doing at classifying points. Whether they're terrain that we can drive fast on or terrain where we have to go more slowly. So the metric that we're going to use in this case to decide how well our algorithm is doing is the accuracy. Accuracy is just the number of points that are classified correctly divided by the total number of, of points in the test set. So the quiz that I want you to answer right now is to tell me what the accuracy is of this naive_bayes classifier that we've just made. And so there are two different ways that we can do this. The first is that you can actually take the the predictions that you've made here and you can compare them to the labels in your test set. Or what you can actually do is head back over to the Scikit-learn documentation and see if there's some Scikit-learn function that might be able to take care of this for you. So, computing this accuracy element by element is one way that you can do it or if you want to be a little bit more adventurous. Go check out the documentation and maybe there's a really cool one-liner that you can use from sklearn that'll take care of this for you. Okay? So the quiz question is basically, do this calculation either on your own or using sklearn and tell me how accurate our classifier is. All right. So, here's what I did. I gave you the choice of two different methods, either writing your own code to calculate the accuracy, or using sklearn. What I myself chose to do was to use the sklearn accuracy_score, which is what you'll find if you Google sklearn accuracy. And so, this is just a function that takes a bunch of predictions, it takes a bunch of labels, and then goes through and does the element by element comparison. And then will give you the accuracy that, that your classifier has on your test set. So, hopefully, what you got, and what I got for the accuracy is about 88.4%. So, 88.4% of the points are being correctly labelled by our classifier when we use our test set. Welcome back! What you just got was 88.4% correct for naive base. 88.4. Awesome guys. Well, does that mean that your, your selfdriving car only crashed 11.6% of the time? It'll crash occasionally yeah. It's not good enough for cars. Okay. But it's great for this course. It is very good. It's much better than guessing. One of the things that you did in that example, maybe you didn't even realized you were doing it was you trained and you tested on two different sets of data. And that's actually really important in machine learning we always train and test on different data. If you don't do that what can happen is that you can over fit to your training data. You can think that you know better what's going on than, than you actually know. That's really important because in machine learning you have to kind of generaliz to new data that's somewhat different. And I would just memorize all the data, test it on the same data as training data, which will always give you 100%. But I have no clue how to generalize the new data. That's right. So what you should always do, and what we'll always do in this course, is you should save maybe 10% of your data and you use it as your testing set. And that's what you use to actually tell you how much progress you're making in terms of learning your patterns in your data. So, when you report the results to yourself and, and, and to your boss or your client, use the tested result, because it's a much better, fair kind of understanding of how they are doing when you train them. So you just built an interesting classifier. And it worked! But now we're going to go into some depth. The next unit will be a little tedious. And we're really going to dissect the little bit, [INAUDIBLE] bits of the algorithm. But stick with us and you're going to learn a lot about Bayes Rule. And then eventually Naive Base So this unit is a tough one. We're going to talk about perhaps the holy grail of probabilistic inference, that's called Bayes rule. Bayes rule is based on Reverend Thomas Bayes who used this principle to infer the existence of god. But in doing so he created an entire new family of methods that has vastly influenced artificial intelligence and statistics. So let's dive in. Let's use the cancer example from my last unit. Say there's a specific cancer that occurs in one percent of the population. And the test for this cancer. And with 90% chance that it's positive if you have this cancer C. That's usually called the sensitivity But the test sometimes is positive even if you don't have C. So let's say with another 90% chance it is negative if you don't have C. This usually called the specitivity. So here's my question. Without further symptoms you take the test, and the test comes back as positive. What do you think is now the probability of having that specific type of cancer? To answer this let's draw a diagram. Suppose these are all the people and some of them; exactly 1%, have cancer 99% is cancer free. We know there's a test that if you have cancer, correctly it is diagnosed with a 90% chance. So if we draw the area where the test is positive, cancer and test positive then this area over here is 90% of the cancer circle. However that isn't the full truth. The test sentenced is positive even if the person doesn't have cancer. In fact, in our case that happened to be in 10% of all cases. So you have to add more area. It's as big as 10% of this large area. It is as big as 10% of this large area where the test might go positive. But the person doesn't have cancer. So this blue area is 10% of all the area over here minus the little small cancer circle. And clearly all the area outside these circles corresponds to derivation of no cancer, and the test is negative. So let me ask you again. Suppose you have a positive test. What do you think? Would the prior probability of cancer of 1%. A sensitivity and specificity of 90%. Do you think your new chances are now 90% or 8% or still just 1%? And I would argue, it's about 8%. In fact, as we'll see, it will come out at 8 1/3%, mathematically. And the way to see this is this diagram is, this is the region in which the test as positive. By having a positive test, you know you're in this region, and nothing else matters. You know you're in this circle. But within this circle, the ratio of the cancerous region, relative to the entire region, is still pretty small. It increased. Obviously having a positive test, changes your cancer probability, but it only increases by a factor of about eight. As we'll see in a second. So this is the essence of Bayes' rule, which I'll give to you in a second. There is some sort of a prior, by which we mean the probability before you run a test. And then you get some evidence from the test itself. And that all leads you to what's called a posterior probability. Now this is not really a plus operation. In fact in reality it's more like a multiplication. But semantically what Bayes Rule does is it incorporates some evidence from a test into your prior probability to arrive at a posterior probability. So this makes this specific. In our cancer example we know that the prior probability of cancer is 0.01 which is the same as 1%. The posterior of the probability of cancer given that our test says positive, abbreviated here as positive, is the product of the prior times our test sensitivity. Which is, what are the chance of a positive result given that I have cancer. And you might remember this one was 0.9, or 90%. Now, just to warn you, this isn't quite correct. To make this correct we also have to compute the posterior for the non-cancer option. Written here is not cancer, given a positive test. And that's using the prior. We know that P of Not C is 0.99. It's 1 minus P of C. Times the probability of getting a positive test result given Not C. Realize these two equations are the same but exchange C for not C. And this one over here takes a moment to compute. We know that our test gives us a negative result if it cancer free, 0.9 chance. And as a result it gives us a positive result in the cancer free case with 10% chance. Now, what's interesting, is that this is about the correct equation. Except the probabilities don't add up to 1. To see, I'm going to ask you to compute those. So please give us, give me the exact numbers, for the first expression, and the second expression, written over here. Using our example up here. Obviously, P of C is 0.01 times 0.9, is 0.009. Whereas 0.99 times 0.1, this guy over here, is 0.099. What we've computed here is the absolute area in here. Which is 0.009. And the absolute area in here, which is 0.099 The normalization proceeds in two steps. We just normalize these guys to keep the ratio the same, but make sure they add up to 1. So let's first compute the sum of these two guys, please let me know what it is. And yes, the answer is 0.108. Technically what this really means is the probability of a positive test result, that's the area in the circle that I just marked you. By virtue of what we learned last is just the sum of these two things over here which gives us 0.108. And now finally we come up with the extra posterior, or as this one over here is often called, the joint probability of two events. And the posterior is obtained by dividing this guy over here with this nominizer. So, let's do this over here. Let's divide this guy over here. By this normalizer to get my posterior distribution, I think cancer, given that I received the positive test results, so divide this number by this And we get 0.0833. As to another same for the then non cancer version pick the number over here to divide and divide it by the same normalizer And the answer is 0.9167 approximately. Why don't you, for a second, add these two numbers, and give me the result. And the answer is 1, as you would expect. Now with this was really challenging, you can see a lot of math from this slide. So let me just go over this again and make it much, much easier for you. What we really said that we had a situation that prior a test is a certain sensitivity and a certain specificity. When you receive say a positive test result, what you do is you take your prior, you multiply in the probability of this test result. Given C, and you multiply in the probability of the test result given not C. So this is your branch for the consideration that you have cancer. This is your branch for the consideration you have no cancer. When you're done with this, you arrive at a number that now combines the cancer hypothesis with the test result. Both for the cancer hypothesis and the not cancer hypothesis. Now what you do, you add those up. And they normally don't add up to one. You get a certain quantity, which happens to be the total probability that the test is what it was. This case positive. And all you do next is divide or normalize this thing over here by the sum over here. And the same on the right side. The divider is the same for both cases because this is your cancer range, your non cancer range. But this guy doesn't rely on the cancer variable anymore. What you now get out is the desired posterior probability, and those add up to one if you did everything correct as shown over here. This is your algorithm for Bayes Rule It's great you learn about Bayes Rule. And one of the things you use Bayes Rule a lot for is learning from documents, or text learning. And the methods they'll tell you about is often called Naive Bayes. It's actually a misnomer, it's not to be naive, not as in naive as Bayes Rule itself. But that's the name. Okay, so you're going to exercise this later in our practice exams using Enron email data sets. But I give you the gist of it. Suppose we have two people, one is called Chris and one is called Sara. And both people write a lot of email and for simplicity I'll assume that these emails only contain three words, okay? They contain the word love, the word deal, and the word life. Same is true about Sara. Obviously people use more than three words. But this area here is a little bit small for 30,000 words. The difference of these two people Chris and Sara is in the frequency at which they use those words. And just for simplicity let's say Chris loves talking about deals. So 80% of his words or 0.8 are deal and he talks about life and love for a bit with 0.1 probability so if Chris a word in email he's going to 80 percent of the time use the word deal and ten percent of the time use the word love or life. Sara talks more about love, a little bit about deals, .2 and about life .3. And again that's a simplified example. What Naive Bayes allows you to do is to determine based on a random email who's the likely person who sent this email. Suppose says an email that goes as follows love life, and you don't know who sent it but you'd like to figure that out. Then you can do the space on base form and suppose you leave a priori that it's 50% probability by Chris or by Sara. So we'll say p of Chris equals 0.5. That means the prior probability for it being Chris is fifty percent and that immediately means, because it is one of the two, that the probability of Sara is also 0.5. So if you look at this intuitively, who's more likely er, to have written this email? Chris or Sara? Just check one box And this is easy, Chris doesn't use the world love or life very much, Sara uses it with much, much more frequencies, so you should have checked Sara But suppose you see a different email. I just made my life deal. Who will know Cris or Sara And this one is actually not as easy to compute. It's actually a close call, as you will see. And we have to multiply out the numbers. So Chris uses life with probability 0.1, deal with 0.8, and our prior on Chris is 0.5. When we multiply those out, we get 0.04. Same for Sara. Life is 0.3. Deal is 0.2. And our prior here is, again, 0.5. When we multiply this one out, we get 0.03. So the correct answer here would have been Chris by a really slim margin. In the next exercise, I want you now to compute a posterior probability. And, specifically, I want you to calculate the posterior probability of it being Chris or Sara, given that we just observed the exact same phrase as before, life deal. So just add the number into the box on the right side And the answer is 0.57 over here and 0.43 over here. And the rationale is that the posteriors are normalized to 1. So we've taken 0.4, for example, and dividing 0.04 by the total, which is 0.04 plus 0.03. When you work this out, it's the same as 4 divided by 7, or 0.57. And when you divide 3 by 7, you get 0.43. That's the correct posterior answer in this example. Just to make sure we understood it, let me change the text message from life deal to love deal. It's now your job to calculate these probabilities over here. To calculate this answer we multiply for Chris. The LOVE and DEAL probability is .8 .1 just as before and multiply in the prior of .5. And product of the .1 times .8 times .5 is 0.04. The math changes for SARA. She was as would LOVE with .5 probability. DEAL with .2, and again our prior is 0.5. This is 0.05. Now, to normalize them to bring them into a form where they add up to 1, we realize that the sum right now is 0.09. So we have to divide 0.04 by 0.09 and that gives us 0.444. An approximation. And the 5 gives us 0.555. If we add them up you might get 0.99999 or 1. So let me just explain to you why this is called Naive Bayes. What we've done is we've said there's unknown concepts. These are our target labels. Let's call them label A and label B. And you don't get to see them. They're hidden. As is always the case in, in supervised learning. And what you're going to see instead. I think these things do. Like words they use. If they use them with exactly the same probability you could never figure out what it is. If they use it with different probabilities and they might use one of them or a 100 or a 1,000. But every word that you see like this one over here gives you evidence as to whether it was person A or person B. And what you do is, you just multiply all the evidences for every word you see. And it's a big product. You do this for person A and for person B. And you multiply in the prior and when this product comes out, it gives you the ratio of whether you believe it's person A or person B. That is called Naive Bayes. It lets you identify from a text source whether this label is more likely or this label is more likely. And you can do this with people, with news sources, you can ask the question was the text written by Shakespeare or by somebody else. It's a very powerful tool, that's widely used all across machine learning. And the reason why it's called Naive is because it ignores one thing, and you tell me which one it is? All the individual with words in the message, the order of your words inside a message,. Or the length of the message. Which one is being plainly ignored here? And I'd say the best answer would have been word order because this product doesn't consider the order in which the words occur. Whereas in the English language I can tell you if you randomly re-order our words sentences don't make any sense. This message doesn't pay attention to the word order. So it doesn't really understand the text it just looks at word frequencies as a way to do the classification. That's why it's called Naive. But it's good enough in many cases So, congratulations, you made it through the end of the Naive Bayes learning algorithm. That is quite amazing. That's one algorithm down, two to go. What we'll do is we'll start a list now of all the things that maybe you should be thinking about when picking a supervised classification algorithm. So that you have some basis for starting to make a choice between the different algorithms that we'll be talking about. So there are a few things that Naive Bayes does really elegantly. It's actually really easy to implement. it, it, these were with great, big feature spaces there is between 20,000 and 200,000 words in the English language. And it, it's really simple to run, it's really efficient. There are a few things that Naive Bayes doesn't do as well that you also want to be careful of. Yeah, so it can break. It breaks in funny ways. So, historically when Google first sent it out, when people searched for Chicago Bulls, which is a sports team comprised of two words, Chicago Bulls. Would show many images of bulls, animals, and of cities, like Chicago. But Chicago Bulls is something succinctly different. So phrases that encompass multiple words and have distinctive meanings don't work really well in Naïve Bayes. So depending on exactly what the problem is that you're trying to solve and the data set that you have to solve it, you shouldn't think of supervised classification algorithms as black boxes. You should be thinking about them in terms of the theoretical understanding of how the algorithm works and whether that's right for the question you're trying to answer. And you should, of course, test it, right? So we talked about train set or test set. You can check the performance in test set and see how it works. And if you don't like it might be the wrong algorithm or the wrong parameters. Exactly Great job. You've made it to the end of lesson one. Yahoo! So, next we have another supervised classification algorithm. What's it called? SVMs. And for what? Support vector machines. I always wanted more support and now it comes with its own vector machine. Right, we'll be supporting Sebastian in using vector machines. That's fantastic. See you in class. See you there Welcome to the first mini project. It's on the topic of Naive Bayes. We're going to have you do something that I think is a really interesting application of machine learning. It's identifying who was the author of a piece of text. So for a little bit of background this actually came up in the news a couple years ago. J. K. Rowling had written a book under the pseudonym Robert Galbraith. And there was some, some machine learning, some text forensic work that was done, and they were able to figure out that even though it didn't say J. K. Rowling on it, the writing style was distinctive enough, and the, and the vocabulary usage was distinctive enough that they were able to identify who wrote it anyway. We'll be doing something like that in this in this project except we'll be identifying the authors of emails. So we have a bunch of emails from someone named Chris and a bunch of emails from someone named Sara. And the question is, using Naive Bayes, can you identify who is the author of, of a new email, an unseen email? So, let's get started. So one of the most exciting developments in machine learning is learning from text. A lot of the online data is actually text data, the web, emails and so on. And companies like Google, Yahoo!, and many others are really built on the idea that you can use machine learning and apply it to text. So if you want to build the next Google, listen up. The fundamental question in learning from text has to do with what's the input feature. And I'm going to give an example and ask you a question. Suppose you have two strings, two kind of sentences. One is called nice day, and one is called a very nice day. And suppose for whatever reason, one is a positive example. And one of the negative examples is indicated by this X over here and this circle. And maybe if many of those, and you want to toss them into your favorite learning algorithm like the support vector machine to produce either output label. What do you think is the best input dimension for the support vector machine? Give you a few choices, one, two, three, four or hard to tell. Give it a best shot. And at first, first glance, it's actually, I would say it's hard to tell. Obviously, the other ones you can make work and we'll talk about how to make them work. But it's hard to tell. The reason is, there seem to be, like, two features in the first sentence, and four in the second. So is it two or is it four? It's kind of hard to tell. So in learning from text, one of the fundamental problems we have is that the length of each document that you learn from, each email or each book title, is non-standardized. So you can't just use each individual word as an input feature, because then long emails would require different input space than short emails. Instead, one of the coolest things in machine learning from text, which analyze all these approaches, is called bag of words. And the basic idea is to take any text and count the frequency of words, which you do for me in a second. So the trick is to impose a dictionary. So you have all the words you know. Obviously, they include words like nice, very, day. It might include words that don't occur on the left side, like he, she, love. So say these are the words you care about. Notice that there's one on the left side that doesn't occur on the right side. Then you can map each phrase, each email, each title, each text, into a frequency count defined over these six features over here. So let's do this. I give you six boxes for each of the six words in our dictionary. Of all the words you care about, can you for the very first sentence, nice day, fill in the frequency count of those words in this text and give me that vector? So in particular, for each word over here, you tell me how often it occurs in nice day. And it wasn't a particularly hard question now, because nice and day, of course once, all the ones, ones occur zero times If you do this again for a very nice day, then let's see what comes out. Here are the boxes again. We give it a try. And now, of course, we find that our future vector looks like this. We had four words over here. We only get a count of three because we didn't pay attention to A. A is often called a stop word. It tends to be removed from text learning. Such as the, of, and other stop words. But the key words here are nice, very, day, and you can see how these two feature vectors of the same dimension, it estimated six, but they can be applied to different text. So here's a new phrase. MR Day. Happens to be MR Day, loves a nice day. And again, do your magic and fill in the numbers. I argue vice, I argue nice occurs once. Day actually occurs twice now. So you have a dual occurrence of day. And for the word loves, you can argue. In the strict sense, the answer is zero, and this is what I asked you to do here. But advanced techniques for bag-of-words tend to just look at the stem of a word, which is love, and count it. But the way I asked the question, it should have been zero. Just be aware that later on, as you use actual tools, loves and love might end up in the same bucket on the right side. But I think you got the principle. It's just a frequency count. So let me ask you a few question about the technique of bag-of-words. And these are all yes, no questions. Question number one, does the, the word order matter inside of a phrase? Yes or no? Do long text or phrases give fundamentally different input vectors? Yes or no? Or put differently, if I just take the same email and make ten copies and put all the ten copies into a, like the same text, will I get something different, yes or no? And finally, can we really handle complex phrases involving multiple words? Classic example in the US is Chicago Bulls is actually a sports team, where Chicago itself is just a city, and Bulls is an animal. But the conjunction of these two words means something entirely different from the words insulation. Again, yes or no? Give it a try. And the key thing to know for bag of words is that the word order does not matter. It's clearly a no, because we are counting words we have no preference to where in the phrase a word occurs, which kind of make it somewhat limited compared to really understanding a text. It's kind of taking all the words and resorting them in random order. That's why it's called bag of words. Because you toss them into a bag. And gone they are. Long phrases do give different vectors. So suppose we take the same email and duplicate it. All the counts go up by a factor of two, and it's going to be a different input vector. So to some extent you're biasing yourself to also encode the length of your text not just the frequency of the words, so the answer would be clearly yes. And I would say, can we handle complex phrases. Right now we can't, unless we make extra buckets for those, which makes it a bit more difficult to preprocess. Chicago Bulls would count one for Chicago and one for Bulls. And here's a funny little insight, when Google first came out as a search engine and you typed in Chicago Bulls. You saw lots of, of, of references to cities and lots of references to animals. And so a, a bunch of smart Google engineers to change the dictionary to also included combined phrases like Chicago Bulls. So go to Google, try it out. I promise you you see very few animals and very few cities. Now that you're familiar with the Bag of Words representation, let's get your hands on the keyboard and have you using it in sklearn. In sklearn Bag of Words is called CountVectorizer, presumably because it's counting the number of times various words show up in a corpus. Now I'm going to walk you through some examples on the interpreter. And in the next few videos, you get some hands-on experience via programming quizzes. I head over to the interpreter and using the online documentation for reference, I import the CountVectorizer. CountVectorizer lives in the feature_extraction.text part of sklearn. She got wrong the first time. But I can import it just like any other object in sklearn. Then I create my vectorizer. And I need to feed it some documents that come in the form of strings. I liked Sebastian's example, but I think I want to do something that sounds a little bit more like the emails that I would actually expect to see in an inbox. So we'll make up some strings. These aren't really emails, but they kind of sound like them. The first thing I need to do to get these into CountVectorizer is I need to put them into a list. Now I'm going to create the bag-of-words, which is what I get when I put this email list into my CountVectorizer. And I actually did this a, a little bit wrong. There's two steps that I forgot here. The first is that I need to fit my data using my Vectorizer. The next thing is I need to transform it. So first, I'll do the fit. This is where it's actually figuring out what all the words in the corpus are, all the words in all the emails. And assigning say, numbers or list indices to each of them. And then the transform is where it actually takes all the words in the corpus and figures out how many counts of each word occur. My syntax isn't perfect here, but this should work. And now to start to understand this, let me print my bag-of-words. See what it looks like. What I get is a bunch of tuples and integers, and while this looks rather opaque, we can actually unpack it in a fairly straightforward way. Let's take this row as an example. The way to interpret this is that in document number one, word number seven occurs one time. Now, of course, we have to ask what word number seven is in order to interpret this as humans. But all of the information is there. Similarly, I can see in document number one, word number six occurs three times. So just out of curiosity, let's go back up to document number one and figure out what we think word number six is. So document number one is going to be string two. Because the indexing starts at zero. And I can see that there is a actually a word that's repeated three times, and it's the word great. So assuming that that's what word number seven is, things are starting to make sense. >From looking at the sklearn documentation, I see that there's an attribute of the bag-of-words called the vocabulary. And this is a way that I can test my hypothesis. So I call the function get on my vocabulary. And I pass to it an argument which is the word. And my hypothesis is that what it should return, because this line gave us the clue, that word number six in document one occurred three times. And that's what happens. So this way, for any word in the corpus, I can figure out what feature number it is in my bag-of-words. You can also go the other direction too. If you want to ask, for example, what is the word associated with feature number six, there's a way you can extract this information too. We'll get to that in the next lesson on feature selection in a really cool example. But now, I want to get your hands on the keyboard with a quiz. Now that you have bag-of-words running, you're really able to start doing high level text analysis right away. But, I want you to think about something else when you're doing text analysis as well. And, that is that, when it comes to vocabulary, not all words are equal. Another way to think about this, is that some words contain more information than others. And in order to get you thinking about this, what I'm going to do is give you a quiz. I'm going to give you a set of words. And I want you to check the words that are low information. You don't technically know what this means yet. But just think about your intuition. Which words do you think are giving you information about what's going on in the text? And what words just aren't that helpful to help you understand what's going on? Let's suppose that our corpus is the same sort of standard set of messages that we've been looking at so far. So our vocabulary contains the words, hi, Katie, Sebastian, will, the, driving, machine, learning, and great. Obviously all of these words show up in the messages, at least once. Now check the ones you think aren't really telling you that much about what might be going on in the message. This is a little bit of a judgement call to say exactly which ones you think are low information. because it kind of depends on what you're looking for. But here are a few that are classically just not going to have that much information. The word the. The word the shows up in almost every message that you're ever going to see. So the fact that the word the shows up doesn't really tell you what the message is about, because it's always going to be there. I would say the same thing is probably true of the word will. And in this context also, where we're looking at emails, I bet the word hi occurs a lot. And so seeing the word hi doesn't necessarily tell you anything very specific about what the message is about. You could've also said that Katie and Sebastian are low information words, because every message has these two words in them. All the messages are about Katie and/or Sebastian, so knowing that those words are there isn't necessarily telling you anything. But I would argue that usually names are something that you would want to be able to keep. Some words just usually don't contain a lot of information. And it can be really valuable to have an eye out for these words. And to be able to just sort of remove them from your corpus, so you don't have to to consider them. You don't allow them to become noise in your dataset. In general, this list of words is called stopwords. And the exact definition of what a stopword is can vary. But in general, it's a low information word that occurs very frequently. Some examples might include words like and, the, I, you and have. And a very common pre-processing step in text analysis, is to remove the stopwords before you do anything else with the data. Suppose that our body of stopwords is the, in, for, you, will, have and be. Say I just give these to you and say, by fiat, these are the stock words. My question for you in a quiz, is how many words will be removed when we remove the stopwords from the message, hi Katie, the machine learning class will be great. Best, Sebastian And the answer is, that when we clean this message from stop words, we'll get rid of three of the eleven words. We'll get rid of the words the, will and be. Because these all show up in the stop words. Now that you know about stop words, I'm going to show you how to get a list of stop words out of a Python package called NLTK, which stands for the natural language tool kit. In this demonstration, I'm going to be using the interpreter. At the end of this video, there will be a quiz, and I want you to do the quiz on your own laptop, not in the Udacity web browser. So this quiz is a little bit different. So, let's get started. I fire up the interpreter. To get the stopwords out, I have to import them from NLTK. I found this line of code in an example when I went to Google looking for how to get stopwords from NLTK. To get a list of stopwords, say something like sw equal stopwords.words. And then as a parameter to words, I have to tell it what language I am interested in. That's English. And [LAUGH] if you are using NLTK for the first time like I am on this computer you'll get an error because NLTK needs to have a corpus from which it gets the stopwords. The corpus is just a body of documents. So in order to fix this, I have to call this nltk.download function. You have to make sure NLTK is imported first, and then what we need is the corpora and then we hit download. This will take a few minutes. And now that that's finished, I can try again. Much better. So I can go into this sw list and I can ask what some of the stock words are. So the first one in the list, is the word i. That makes sense, people like to talk about themselves a lot. Let's find out what the tenth one is. Yours. Again, makes sense. What I want you to do in the quiz now is to repeat these steps that I went through. So, use your Python interpreter to actually download the corpus and to get the list of stop words out. And here's my question for you. How many stop words are in the list? Write your answer in this box The answer that you should have gotten is 127. Here's how I figured it out. You can repeat the steps just like I showed you in the last video. And then you just need to call length on the sw list, and that will give you the number of elements in sw. Which is the number of stopwords, 127 There's another handy trick that I'm going to teach you now, and it has to do with the idea that not all unique words are actually different, or not very different anyway. Let me show you an example of what I mean. Say in my corpus I have a bunch of different versions of the word respond, where the meaning changes ever so slightly based on the context or based on the part of speech that the word is, but they're all talking about basically the same idea, the idea of someone or something responding. The idea is that if I naively put these into a bag of words, they're all going to show up as different features, even though they're all getting at roughly the same idea. And this is going to be true for many words in a lot of languages, that they have lots of different permutations that mean only slightly different things. Luckily for us, there's a way of, sort of, bundling these up together, and representing them as a single word, and the way that that happens is using an algorithm called a stemmer. So if I were to wrap up all these words and put them into a stemmer, it would then apply a function to them that would strip them down all till they have the same sort of root, which might be something like respon. So the idea is not necessarily to make a single word out of this, because, of course, respon isn't a word, but it's kind of the root of a word, or the stem of a word that can then be used in any of our classifiers or our regressions. And we've now taken this five dimensional input space, and turned it into one dimension without losing any real information. A stemming functions can actually be kind of tricky to implement yourself. There are professional linguists and computational linguists who build these stemming functions, that best figure out what is the stem of a given word. And so, usually what we do in machine learning is we take one of these stemmers off the shelf from something like NLTK, or some other similar text-processing package, and we just make use of it, not necessarily always going into the guts of how it works. And then once we've applied the stemming, of course, we have a much cleaner body of vocabulary that we can work with. Since word stemming can be kind of an intricate process, this is something that's usually handles by special functions within natural language processing toolkits. Like NLTK, or other types of toolkits that are available in other languages. Let me give you a quick idea of what this looks like. So I fire up my Python interpreter and the first thing that I do is I import a stemmer. As it happens, there are several different stemmers that are available. I'm going to show you what the snowball stemmer does. But there are several others that you might see from time to time. So this happens to be the command for importing the snowball stemmer. So I'll go ahead and do that. I just found this, example on the Internet. And that usually takes a second or two. And then what I'm going to do is actually create the stemmer. And just like with the stop words, I have to declare a language that I want to stem in, because, of course, stemming a French word will be different from stemming an English word, we will do English. Okay, so now I have this stemmer and I can use it to stem any word that I like. So let's try one of the words from my responsive example. So I call the stem function on the stemmer that I just created, and let's use the word responsiveness. And it gives us that the stem of that is going to be respons. So something very similar to what we were guessing in the last video. Let's try another one that's similar. Same as before, respons, and now I've added a prefix to it, unresponsive. So it means the opposite of responsive, but we're still talking about the concept of whether something is responsive or not. So we should get the same stem still. We can also see that there are certain limitations to this particular stemmer. One is that it didn't strip about the un in unresponsive. And, maybe that's something that you want, maybe you want to be preserving the information that something that's responsive and something that is unresponsive are not exactly the same thing. Maybe all you care about is just the idea that something could be responsive. So, there can be a little bit of, of fine tuning here to figure out exactly how you want to run it. But in general, when you're applying a stemmer, it's going to clean up the vocabulary of your corpus a lot. It's going to take something that can be very, very complicated, you have your tens of thousands of dimensions that correspond to all the words in the language, and it makes something that's much more compact. You'll get a chance to play with this in the mini project at the end of the lesson. So now let's do a quick quiz talking about the order of operations in text processing. We've talked about two important steps that you'll use often when you're doing text processing. One is the bag of words representation, and the other is word stemming. And here's a quiz for you. Let's suppose that you want to do both these operations to your text. You want to have stemmed words in a bag of words representation. Which one of these steps should you do first? And this is another quiz where of course we haven't already given you the answer. Hopefully you had to think about it a little bit. The answer is that you want to do stemming before you do the bag of words representation and that's for two reasons. The first one is if you put it in the bag of words representation before you stem then there's kind of no point in stemming because you could get the same word repeated many times within your bag of words representation. You're not really like condensing the information in any useful way. In fact, you're probably making it noisier and worse. because you'll just have the word sponse in there six times. Also it's more technically feasible to apply stemming first and then put it in the bag of words representation. Because stemming is going to assume a string. And the bag of words representation is going to look like some kind of matrix that has many different documents and, and words within those documents. So you almost always want to do stemming as one of the first steps in your text processing. You go through and you stem each word and then put it into the representation that you'll use in your machine learning algorithm. Between stemming and the bag of words approach I think you're ready to do a lot of really interesting things in text learning. There's one more representation that I want to teach you before we quit though because it's really, really cool. It's called the Tf Idf representation. Tf stands for term frequency. Idf stands for inverse document frequency. And the rough idea is this. The term frequency part is very similar to the bag of words. What this means is that each term, each word, is going to be up-weighted by how often it occurs in a document, just like it is in bag of words. If you have a word that occurs ten times, it's going to have ten times as much weight as a word that occurs only once. The inverse document frequency part is new though. And the idea here is that the word also gets a weighting that's related to how often it occurs in the corpus as a whole, in all the documents put together. And here's a quiz to make you think about this weighting in a very careful way. The question is this. What do you thing makes more sense if you want to be extracting information from these words? Will you give a higher weight to the very common words that occur in many, many documents, or would you give a higher weight to the rare words? The words that might only occur in let's say 10% or 1% of the documents that are present in your corpus. What a TfIdf does, is it rates the rare words more highly than the common words. And that's because the rare words, in general, are going to be the ones that help you distinguish the different messages from each other. So, let's suppose you got a bunch of emails that, some of them came from me, and some of them came from Sebastian. We might have a lot of overlap. We might both be talking about machine learning and Udacity. But some of them are going to be talking about, let's say physics. Because that's what my background is in. And that's going to be comparatively rare. There's not going to be tons and tons of emails in that corpus that are talking about physics. Because most all of Sebastian's are, don't talk about physics at all. It's only going to be mine. And then likewise, maybe there's a bunch of emails that talk about Stanley the robot. Which of course is one of his projects, but not something that I'm a real expert in. So, the fact that words like physics and Stanley would be rare in this corpus, compared to words like Udacity or machine learning. Means that these might be the words that tell you the most important information about what's going on. Who might be the author of a given message? And so, another way to think about that is that this is why it's called the inverse document frequency. That you want to weight the words by inverse of how often they appear in the corpus as a whole. I'm not going to have you code an example of Tf Idf right now, as a quiz. But this is something that we're going to cover in the mini project that's coming up very shortly, at the end of this lesson. So, you will get your hands dirty with this representation a little bit. Welcome to the mini project on text learning. I want you to think back to some of the earlier lessons in this class, where you were identifying the author of an email based on the words that they were using in the emails that they sent. Now you have much more of appreciation for all the work that goes into preprocessing text before you can feed it into a naive Bayes classifier, an SVM, or anything like that. We took care of all that preprocessing for you at the beginning, but now you're going to sort of open that black box. You're going to put all those things together yourself so that you can start with what's effectively a raw email, and then at the end of this mini project, you'll have output that's ready to go straight into a supervised classification algorithm. Welcome to the lesson on Feature Selection. I think Albert Einstein probably said it best when he said you should make everything as simple as possible, but no simpler. This is the idea behind feature selection, that you want to have the minimal number of features that it takes to really capture the trends and the patterns in your data. Your machine learning algorithm is only going to be as good as the features that you put into it. So this is a place where you should really put some time and attention as a machine learner. There are two major things that are involved here. The first is that you should have a way to select the best features that are available to you. In other words getting rid of the stuff that doesn't help you very much. On the other hand there will be hopefully patterns in your data that you need to draw out, and so potentially adding new features can be ways that you can use your human intuition to access those patterns. We'll go through both of these examples in this lesson. Let's start with the example of adding a new feature. Specifically, I'll be adding some features for the Enron data set and walking you through how you can figure out whether you're moving in the right direction. Here's a process I usually follow when coding up a new feature. The first thing is to start with your human intuition. What feature do you think might contain some pattern that could exploit using machine learning? An example of a human intuition I might have on the Enron data set is that persons of interest might have particularly strong email connections between each other. In other words, they send each other emails at a higher rate than the, than people in the population at large send emails to persons of interest. The next thing to do is actually code up the feature. This is what you'll do next, in a quiz. After that, visualization will tell you if you're on the right track. Does your new feature give you discriminating power in the classification problem we were trying to solve? And then presumably through this process you've learned something new. If for no other reason than because you have this new feature available to you. And very often you'll repeat this process a few times as you zero in on what you think will be the most helpful new feature for you The new feature you're going to code up will be called something like from_poi_to_this_person, which should be an integer. It's the count of messages that are in this person's inbox but are sent to them from persons of interest. So here's what you'll do in the programming quiz. >From each message we have to extract the author of that message. I've already done this for you. So, that will be in the starter code. Then what you have to do is you have to compare this author email address to the list of known person of interest email addresses. So basically, is the author a person of interest. You return a boolean, which is the answer to that question and then you repeat this over all the emails for a given person. So that the outcome is this integer. Give that a try in the programming quiz and when I show you the solution, I'll show you what this feature actually looks like in the Enron data set. Here's a visualization of the new feature. Along the x axis here, I have the number of emails from a person of interest to a given person in the data set. Along the y axis, I have something else that I think might give me some discrimination as well. Which is the number of emails that this person sends to persons of interest. What I've also done is colored my persons of interest red in the scatter plot, so I can easily identify if there's some sort of pattern in this feature that I start to see clumps of red dots all together, for example. That would be an indication of something that a supervised learning algorithm could exploit in trying to predict persons of interest. And what I see is that there doesn't seem to be a very strong trend here. The red points seem to be mixed in rather equally with the blue points. Another thing that I notice is that there are a few outliers. Most people, we only have maybe less than 100 emails to or from them, but some people we have many, many more that that. So this visualization leads me into the next step of repeating this process. Using my human intuition to think about what features might be valuable here. The thing that I'm thinking of at this point is maybe the feature that I need is not the absolute number of emails from a person of interest to a, a given person. But the fraction of emails that a person receives that come from a person of interest. In other words, if you get 80% of your emails from persons of interest, my intuition might be that you yourself are one. But of course, I have to actually code up the feature to test this hypothesis. This visualization has made it clear that maybe the number of emails from a person of interest to a given person is not that helpful, or the number of emails that a person sends to persons of interest. But now I'm using my human intuition to say maybe what I want is to effectively scale this feature by the total number of messages to or from this person. And this is something that I'm going to ask you to do in a coding quiz. So I'll give you the Enron data set and I'll point you towards the features that are relevant. I'll give you these two features as well as the total number of messages to and from a person, and I want you to return two new features. So what I'll give you is these two features that you just made and also the total number of messages from and to each person in the data set. And what I want you to do is to do the division so that you get a scaled variable of sorts in return. The fraction of messages to this person that come from persons of interest and from this person to persons of interest. When I visualize this new feature I get something that actually looks quite good. On the x axis I have the fraction of emails that this person gets that come from persons of interest. On the y axis is the fraction of emails that this person sends that they to persons of interest. And I can see that while my red points aren't particularly tightly clustered. There do seem to be big chunks of feature space where I don't see any persons of interest at all. So for example, if less than 20% of the messages that you send are sent to persons of interest that puts you in this band of points down here. And it means you're almost certainly not a person of interest yourself. And patterns like this can be taken advantage of in your machine learning algorithms to help give you better performance. Now that you've some practice making new features, and before we move on to feature removal, I want to give you one quick warning. Beware of bugs when making new features. This was a mistake that I made once or twice when working with the Enron data. And in the next video I'll tell you one example of a mistake that I made in a new feature that I added. A programming bug had crept into my new feature and what resulted was something that was a little bit too good. So far we've talked about engineering new features that you might want to add to a data set. But it's just as important to know how to get rid of features that you might not want. Now it might not be clear right now why you would ever want to get rid of a feature. You might be like well, but Katie that's throwing out information, that's throwing out data. Why would I ever want to do that? And I would propose to you that there are many very good reasons why you might want to get rid of features. We'll be talking about some of those in the rest of this lesson. But I want to make you think about it on your own first. So here are a few reasons that you might want to ignore a feature. And I want to you to place a check next to all the ones that you think actually sound like good reasons to ignore a feature to you. So perhaps one problem with a feature is that it's noisy, that it's hard to distinguish whether it's, it's reliably measuring what you want it to be measuring. Another thing that could potentially be happening is that a feature maybe it is causing your model to over-fit for some reason. Maybe you think that the feature is strongly related or what we call highly correlated with a feature that's already present. So it's giving you information but it's just repeating information that's already present in the form of another feature. And then the last possibility is that additional features maybe they slow down the training or the testing process. And so in order to keep things moving along quickly, you want to be working with the bare minimum number of features that are required to get good performance. So you tell me what you think. Which of these are good reasons to ignore a feature? And I would actually say that from time to time you'll see examples of all of these going on in the features that you have. They can be noisy, they can be causing your model to over fit and run slowly. And also depending on the model especially if you put in features that are redundant with each other, that have very high correlations, it can actually cause the model to break. The mathematics stops working. And, and your code will have a, a problem in it. You won't know why. So any of these would be a really good reason to be skeptical of your features. To get rid of them if you don't think you absolutely need them. The general message that I want you to take away from this discussion is that features and information are two different things. What you really want out of your data is information. You want to be able to draw conclusions and have insights. And that's not, strictly speaking, the same thing as having lots of features in the data. So the feature is the actual number or characteristic of a particular data point that's attempting to access information. But it's not exactly the same as information itself. So for example, it's a little bit like the difference between the quantity of something and the quality. If you have lots of feature you might have a lot of quantity of data. The quality of those features is the information content. So, in general, what you want is the bare minimum number of features that it takes to give you as much information as possible. If there's a feature present that isn't giving you information, you want to get rid of it, because it can cause all of those problems that we were talking about in the last video. So in order to make this a little more concrete, which might help you understand what I mean. I'm going to show you some actual example from some code that you've actually been running already in this lesson, maybe without even knowing it. So think back to the first couple of lessons where we were learning about supervised classification algorithms and we were trying to identify emails by their authors. Now, what I wanted you to do was to focus on the naive base classifier or the SVM or whatever and just leave the pre-processing to me. I would take care of, of reading in the emails and getting them into a format that was good for you. Now what I'm doing is I'm taking you into the code where I actually did that pre-processing step. So that you can see that there's, there's really no black magic there. And that you would be able to do this completely on your own in the future. Let me take you into that code and show you what I'm doing. So I load in the data. I put it in my TfidfVectorizer, which you're very familiar with by now. And then there's another step right here. Select percentile. I make a selector and I select a percentile. And what this step actually does, is it gets rid of a lot of the features. I know that this is text data. And that it's going to have thousands or perhaps tens of thousands of features because of just the large vocabularies that we have. And I also know that a lot of those words are going to be irrelevant. So stop words is one example of irrelevant words. But there's probably a lot of words in there that aren't stop words, but still just don't have a lot of information in them. They're not going to help me figure out who the author is very well. And so what I've done here in this step, where I select a percentile, is I actually say, get rid of a bunch of the features. You go in and figure out for each feature individually how good of a job that feature does at telling you the two people's emails apart from each other. And only accept the best 10% of the features for me to then use in my classifier. So what I've done here is I've kind of sliced off that very top layer. The features that seem to have the most information. And I'm focusing on those when I make my classifier. Now there's one other place where I did some feature reduction as well. And you might not always want to mix these two together. But I want to send you on a little bit of a treasure hunt to find it. So I've told you about select percentile. This is something that's available to me in the sklearn.feature_selection module. But there's also some feature selection that I can get when I'm performing the TfidfVectorization of my data. When I'm actually taking the words in my corpus and putting them into my Tfidf matrix. So what I want you to do is look up the documentation for this function, the TfidfVectorizer. And take a look at the arguments that I'm passing to it. One of these arguments does something kind of interesting. It says that if there's a word that's quite frequent, that's something that we want to ignore as well. By quite frequent, what I mean is that it occurs in many of the documents. So for example, if you have a word that occurs in every single document, there's an argument in here that will actually remove it while you're making your Tfidf. So this is a little of a tricky quiz, because I haven't told you exactly which argument is doing this. Although a quick Google and the sklearn documentation should make it pretty clear. But the question that I'll ask you in the quiz is what this threshold is, for this particular Tfidf to ignore the word. So, does a word get tossed out, if it occurs in 10% of the documents, 50% or 90%? This max df argument will actually shrink down the size of my vocabulary. And, it will use it based on the number of documents that a particular word occurs in. So, if there's a word that occurs in more than 50% of the documents, this argument says, don't use it in the tfidf, because it probably doesn't contain a lot of information in it. because it's so common. So, this is an example of another place where you could do some feature reduction, some dimensionality reduction, as, as we also call it. But of course, you also always have your old standby of doing something like, SelectPercentile. So, I hope what you found in that coding exercise underscores this point that we're talking about right now, that features are not the same as information. You just got rid of 90% of your text features, but your classifier accuracy basically didn't suffer at all. And in fact, in some ways the performance improved because it's able to run so much more quickly on the smaller number of features. So, this, obviously, is going to be something that you want to be keeping in mind. Especially, when you're working with very high dimensionality data. Data that has lots and lots of features. You want to be skeptical of all of those features and think, which are the ones that are really going to get me the most bang for my buck? Let's dig in a little more to see how the number of features that you use in your algorithm is connected to the bias-variance dilemma. Remember what we said before is that a high bias algorithm is one that pays little attention to the training data and is kind of oversimplified. It just does the same thing over and over again regardless of what the data might be trying to tell it to do. On the other hand an algorithm that's too high variance, pays too much attention to the data. It doesn't generalize well to new situations that it hasn't quite seen before. It's basically just memorizing the training examples. And as soon as it gets a new example or a new data point that's not exactly like one of the training examples, it doesn't really know what to do. Another way to think about this is that it's overfitting to the data. Another thing that would be fair to say is that high biased algorithms tend to have high error on the training set. So in the case of a regression, for example, would be mean a low r-squared value or a large sum of the squared residual errors. High variance on the other hand might have a very, very good fit to the training data but a bad fit to the test data because it's not generalizing very well. So as soon as you give it something new it starts to run into problems right away. You usually expect to do a little bit better on the training set than you do on the test set. But high variance means that your doing much better on the training set. But high variance is when your over fitting to the training set, you get much worse performance on the test set. So, here's a quiz question for you. Let's suppose you have an algorithm that's only using a few features in it. Out of, say, very many that you have available to you. So maybe one or two out of dozens, potentially. Without knowing anything more about the exact problem or the exact features that you're using, do you think that this would be more inclined to be a high bias situation, or a high variance situation? Click which one sounds right to you. Using very few features, puts you into a classic kind of a classic high bias type regime. So pretty typical thing is that you might have several features you need to fully describe all the patterns that are going on in your data, but maybe you only use one or two of those. That's going to put you into a situation where you're not really paying as much attention to the data as you should be. It's an oversimplified situation. In other words, high bias. Here's another question for you. Suppose you have a model where you very carefully tuned it to minimize, say the sum of squared errors on a regression. And, let's say that in order to minimize the sum of squared errors, you found yourself very carefully tuning the parameters, and also using lots and lots of features to try to get every little bit of information out of the data that you could. Maybe you're using all the features that you could even think of, which could potentially be dozens. In this situation, do you think you should be more worried about the variance of your model, or the bias? Click which one you think is right. In the case where you're using lots of features to get the most tightly fitting regression or, or a classifier that you can, that's a classic high variance situation. You want to be careful that you're not overfitting to the data when you're doing this. And so, another way to frame this whole discussion about the number of features that you should be using can be phrased in terms of the bias variance dilemma, or how many features should you be using so that you're balancing these two concerns. You want to have the accurate description that comes with having enough variance to your model, you want it to be able to fit your data in a, in an accurate and true way. But you want to do it with the minimum number of features that's needed to do that. So there's this tradeoff between sort of the goodness of the fit and the simplicity of the fit, the number of features that you have to use to achieve that goodness of fit. And so what that means is you want to fit an algorithm with few features. But using the case of a regression as a large r squared or conversely a low sum of the squared residual errors. This is the sweet spot that you want to find. So let me just show you concretely what a regression looks like when it's overfitting so you know exactly why it's so bad. Suppose that this is my input data. My best fit line might look something like this. But let's suppose that because of the, the model that I'm using and the features that I'm using, I have the possibility also of doing a regression that's a little bit nonlinear. So for example, I could fit it with something maybe that looks like that where there's a little bit of curvature to the line. And having a curved line is probably not necessary in this case. You saw that the straight line fit it perfectly well, but now it's something that the computer sort of has available to it as a possible solution. Once the curvature is allowed, and if I let it get too complex by using too many features, I can actually get something that looks like this. So you could imagine how a computer that's trying to find a function that, that minimizes the sum over the squared errors on the training set might actually really prefer a function that looks like this, because it gets very close to all of the training points. However, you could imagine that if I took another sample from sort of the same population. So I just collect some more data points that are consistent with the ones that I've already collected. Then you can imagine that this wiggly line is actually going to do a worse job of predicting the trend in these red data points than my original straight line does. So this wiggly line is going to be a classic case of where I've used too many features. It's a very high variance solution, and it's not going to generalize well. Here's another way of thinking about this trade-off. We want to balance the errors that our regression gives us, with the number of features that are required to get those errors. We're going to try to maximize the quality of the model that we're making. I haven't totally defined this yet, but let me fill out this graph, and you'll get a sense for what I mean by this. Let's suppose I have only one feature in my model. Then, I would say that the likelihood is pretty high that I'm under-fitting my data that I have something that's very high-bias. So, I'd say the quality of the model might be all right. But, not as good as it could be. Now, let's suppose that I allow a little bit more complexity into my model. I allow to use three features. And, I happen to pick three features that are good at describing the patterns in the data. Then, my error is going to go down, because it's my model is now fitting my data in a, in a more precise way. But it's not too complex yet. I'm only using three features, and I've checked each one of them. So, the quality of my model has now gone up. So, then I say, well, that's pretty sweet. I can improve my model, just by adding more features into it. So, let me try to add in a whole bunch of features. I'm going to jump up to 15. And, I'm hoping that that'll put the quality of my model, say somewhere up here, at the maximal value that it can have. What I'll actually find though, is that I'm starting to become susceptible to this over-fitting, high variance type regime. So, maybe the quality of my model has actually become a little bit lower than the quality that it was at, at three features. Because, now I'm starting to over fit, where before I was under-fitting with one, I'm now over fitting with 15. And, if I were to go all the way out to 20, then I might even be doing worse than if I had only one input feature. And, so you could imagine that if I sort of filled out this, this graph for all of the possible number of features that I could get, there would be some best point where the quality of my model is going to be maximized. So, a reasonable thing to think, and what we'll talk about for the next few videos, is how we can mathematically define what this arc might be so that we can find this maximum point of it. And, this is a process that's called regularization. It's an automatic form of feature selection that some algorithms can do completely on their own, that they can trade off between the precision, the goodness of fit, the very low errors, and, the complexity of, of fitting on lots and lots of different features. So, let me show you an example of how regularization works in regressions. One very powerful place that you can use regularization, is in regression. Regularization is a method for automatically penalizing the extra features that you use in your model. So, let me make this a little bit more concrete. There's a type of regularized regression called Lasso Regression. And, here's the rough formula for the Lasso Regression. A regular linear regression would say, I just want to minimize the sum of the squared errors in my fit. I want to minimize the distance between my fit, and any given data point, or the square of that distance. What Lasso Regression says is yeah, we want a small sum of squared error. But, in addition to minimizing the sum of the squared errors, I also want to minimize the number of features that I'm using. And, so I'm going to add in a second term here, in which I have a penalty parameter, and I have the coefficients of my regression. So, this is basically the term that describes how many features I'm using. So, here's the result of this formulation. When I'm performing my fit, I'm considering both the errors that come from that fit, and also the number of features that are being used. And, so let's say I'm comparing two different fits, that have different number of features in them. The one that has more features included, will almost certainly have a smaller sum of the squared error. because, it can fit more precisely to the points. But, I pay a penalty for using that extra feature. And, that comes in the second term with the, with the penalty term, and the coefficients of regression that I'm going to get for that additional feature that I'm using. And, so what this is saying is that the gain that I get, in terms of the, the precision, the goodness of fit of my regression, has to be a bigger gain than the, the loss that I take as a result of having that additional feature in my regression. So, this precisely formulates, in a mathematical way, the trade off between having small errors and having a simpler fit that's using fewer features. And, so what Lasso Regression does, is it automatically takes into account this penalty parameter. And, in so doing, it helps you actually figure out which features that are the ones that have the most important effect on your regression. And, once it's found those features, it can actually eliminate or set to zero, the coefficients for the features that basically don't help So here's the way that a lasso regression can actually make a regression simpler in terms of the number of features that it's using. So what it does is for features that don't help the regression results enough, it can set the coefficient of those features to a very small value, potentially to zero. So imagine that I have up to four features that are available to me, x1 through x4, and I don't know which ones are the most powerful. What are the ones that I actually want to be using? I don't want to use all of these if I don't have to. And that m1 through m4 are the coefficients of regression that I get for these features when I actually perform the fit. So in an ordinary multivariate regression, I would get a result that looks something like this. It will use all of the features that I make available to it and it will assign to each one a coefficient of regression. What lasso regression will do, is it will try adding them in one at a time. And if the new feature doesn't improve the fit enough to outweigh the penalty term of including that feature, then it won't be added. And by not added, it just means that it sets the coefficient to zero. So let's suppose that x1 and x2 are two features that are really critical to include in our regression. They really describe the pattern with a low of power. But that x3 and x4 don't improve the fit by very much. They might improve it by a little bit, but we could just be fitting to noise. Then what will happen is that the coefficients for those two features will be set to zero. So m3 will be set to 0 and m4 will be set to 0. Which effectively means that these two features become irrelevant in the fit, and the only thing that's left is x1 and x2, the original two features that we wanted to have anyway. The power of regularization, the power of having this penalty term for the extra features, is that it can automatically do this selection for you. Now it might not surprise you to learn that the lasso regression is supported in sklearn. So I'm going to give you some quizzes about it now, but I'm not going to do them on the actual Python interpreter. I'm going to pretend that I'm one of your co-workers, and I'm kind of struggling to get Lasso regression working. And there might be a number of things that are going wrong with my approach. So let me show you my code and you can start to help me debug it a little bit. So the first thing I'm going to do is import the Lasso model from sklearn.linear_model, the next thing that I do is I get the features for my data and the labels out of a function called Get My Data. So this is going to return two different lists. One of them contains all the features. The other one contains all the sort of target values. The, the things that I'm trying to find using my regression. The next thing I'm going to do is I'm going to create my regression, so I just call Lasso open closed parenthesis. And the next thing that I do is I try to fit my regression using the features that are available to me. But then when I run this code it throws an error. And I'm trying to figure out why. So I have this all set up. And I'm actually going to give you code just like this in the programming quiz that's, that's coming up. And so your job is to figure out where the problem is in this version of the code. And to fix it so that all of these lines run properly, they don't crash. And the answer that you should have gotten has to do with this last line of code here where you're trying to make the fit. So remember a regression is going to be a supervised learning algorithm. So you need to have both the features and the targets. So when you're creating the fit, you need to pass both of those to the fit command. Otherwise, it doesn't know how to figure out what answer it's supposed to be getting. Now you've helped me fit my regression, I want to be able to predict a label for a new point. So the question is, what's the next line of code that I should write? So just to be concrete about it, let's say the new point has the features 2 and 4. So type the line of code that you would put after performing the fit that would predict a label for a new point. To predict the label for a new point you want to use the predict function. You pass to the function the features of the new point that you want to make a prediction for and then you have to call that predict function on the regression that you've created and that you've fit. So the line should look something like this. Now this is a tough quiz because we haven't done this before. So you'll probably want to do to the documentation page to figure out how to do this. Here's the question. Suppose now that I've done my fitting with my Lasso regression, maybe I've predicted a point or two with it. Let's say that I want to understand the effect of the fact that this is a regularized regression, which means that I want to look at the coefficients of the regression. I want to see, for example, if it's setting any of them to, to zero or to very small values, which is effectively the same as discarding those features. In other words, I want to start accessing the information of which features are the most important in my regression results. So the question for you is what would you put in this box here to print the coefficients of the regression? So I've already given you the print command, and the question is what line would you put in this box to access the coefficients? I told you this was a tough quiz because we hadn't actually done this before. So hopefully you were able to find it on the documentation page or on the internet somewhere. And so the way that you get the coefficients of a Lasso regression or of any regression really is by calling .coef_ on your regression, which in our case just has the name regression. So if you call this print statement, it's going to give you a list that contains all of the coefficients that have been found by your regression. So you can look at this list now, and you can figure out which features have the large coefficients, which means they're important features, and which ones have the coefficients that are set to very small values. So here's your last quiz. And that is how to interpret the coefficients. So suppose I call this print regression coefficients command. That's going to return a list of the coefficients on the different features. And let's suppose the list that it returns has two elements in it. One has a value of 0.7. The other has a value of 0. In this situation how many features would you say. Are really mattering in the regression. How many features are really important? In this case, there would only be one feature that really actually matters. So we have two that are available to us potentially, because we have two features for each training point. But you can see that the coefficient for the second one is set to zero, which means that it's effectively not being used in the regression. So using this result, I can tell that the second feature is one that I can basically disregard, at least in this particular regression and that all of the discriminating power is coming from the first feature. Welcome to the mini project for feature selection. In one of the earlier videos in this lesson I told you about when I was working with the e-mail data, that there was a word that was effectively serving as a signature on the e-mails and I didn't initially realize it. Now, the mark of a good machine learner doesn't mean that they never make any mistakes or that their features are always perfect. It means that they're on the lookout for ways to check this and to figure out if there is a bug in there that they need to go in and fix. So in this case it would mean that there's a type of signature word, that we would need to go in and remove in order for us to, to feel like we were being fair in our supervised classification. So this was a really big learning experience for me. So I want to share it with you in this mini project. I'm going to sort of take you into my head as I was trying to figure out what was going on that I couldn't over fit this decision tree. And how I figured out that there was one feature or a couple features that were responsible for that. And then, specifically, how I figured out what words they were and how I removed them. So that's what you'll be doing in this mini project. Okay, Katie asked me to introduce to you Principle Component Analysis, often called PCA. PCA is one of these things that's been used thoroughly in all kinds of data analysis including feature set compression. And it's really good to get a basic understanding of what it is so you can use it every time you want to visualize data. Let me start with an example quiz. You have a two dimensional data space. And here's your data. And for the time being, I'm just going to use the single class, we can change this later. Question asks you, sounds trivial, but I want to know what is the dimensionality of the data. And just give you two choices, either one dimensional or two dimensional. If you look at that data what would you think? And, it argues really filling the two dimensional spaces. It's very hard to see how this is a one dimensional data set. Let me modify the data. Suppose this is would you say it's one dimensional or two dimensional? And I would argue the data is really one dimensional, this line over here. In fact you abbreviate it up x and y. But it's sufficient to just give you the x value because the y value seems to constant, so that to me is one dimensional. And here's another data set that offers you x and y value. Would, just intuitively do you think it's two dimensional or one dimensional? Just give me your answer. And I would argue it's clearly one dimensional, except the dimension right now doesn't really align very well with the axes. But if I remap the data into a new space, say we call this axis x prime and this axis y prime, by simple rotation of the original coordinate system and possibly a shift you can see that now in this new coordinate system. The data is easily mapped onto one of the two variables over here. I call this one-dimensional. If you got this wrong, no big deal because you didn't quite know what I what I meant by one-dimensional, but now we know. So let's take another example. This example doesn't have a clear answer, but in spirit if we look at this data over here, what do you think that is? Is it 1D or 2D? So my answer in this case is one dimensional, and you might disagree, but, in principle, the data really falls onto this line over here which is one dimensional line. We can make a new coordinate system, x prime and y prime. And the truth is, it's not exactly one dimension, there's sort of little deviations here and there. But for the sake of understanding the data, I'm happy to treat those deviations as noise and think of this as a one-dimensional data set. Let me ask you a more tricky question. How about the data I'm going to draw now? What would really because one or two dimensional? And again, it's a trick question. And you might disagree. For the sake of principle component analysis actually it turns out to be two dimensional. Even though there's clearly a one dimensional structure underneath this data that you could draw. It seems pretty clear that whatever you have to do to the coordinate system twists it in some sort of a non-linear way. And PCA in particular specializes on just shifts and rotation for the coordinate system. And you can't possibly come up with a coordinate system that is just shifted and only rotated from the original one to obtain one-dimensionality. So, as a result, two is the answer I wanted and it was a trick question. It was certainly a little unfair. Because the best you can possibly do is put a coordinate system like this and like this, where there's still significant variation in the y-dimension, in the data points. Way too much for my taste to call it one-dimensional. So now through all these misleading quizzes that I gave you, you now are in the perfect position to understand principal component analysis. And I will explain to you in 20 seconds, and it's going to stick for the rest of your life. If your given data of any shape whatsoever, like this data point cloud over here, PCA finds a new coordinate system that's obtained from the old one by translation and rotation only. And it moves the center of the coordinate system with the center of the data, right over here. It moves the x-axis into the principal axis of variation, where you see the most variation relative to all the data points, and it moves further axis down the road into a orthogonal less important directions of variation. So look at the original data here. I would argue that the data really lies mostly on this line over here. And then secondly on your orthogonal line over here, and principal component analysis finds for you these axes and also tells you how important these axes are. So let's exercise this. So now that you have a glimpse about what PCA does, let's do PCA by hand, and I'm going to kind of make up an example here that hopefully makes sense to you once you do it. So we've kind of made data that lies between the coordinates 1,2, somewhere uniform and 3,4 along this line of elongation. So we know that PCA finds a coordinate system that centers itself in the data and aligns the x axis with the principal axis of variation. Where do you think, relative to the old coordinate system x and y, is the center of origin for this new coordinate system? Please truncate to the closest integer. I would argue it's about (2, 3), this center over here. In fact, I would argue this is the new coordinate system right here. So the answer would have been then 2 and 3. The next question is even more interesting. The x prime axis is a vector. In the old coordinate system, obviously starts at (2, 3). And, this one, it has the vector component of this axis over here, in the old coordinate system. So, we call this delta x and delta y for the x prime axis. And, I wanna know, if I set delta x to one, so we go one step to the right. What do you think the delta y is to define this axis over here? So, give me that number, please And the answer is, as you go one to the right, we go one up, so therefore it's one to the right and one up I can do the same for the other orthogonal vector over here. Delta x and delta y in red, it defines the y prime axis. So just like before, give me your best guesses for delta x and delta y, it's not an ambiguous question. But I want to be as simple as over here, describing the vector to the left. And let's say, I mean, there's many different solutions, but I'd say the best answer is, minus 1 for the x because we go left and still plus 1 for y. These are two what's called orthogonal vectors. Those define the two principle directions here. In reality, when we write vectors in PCA, what you find is lowest output vectors is normalized to one. This one here has a length of square root of 2. So when you, when you rescale those vectors, you find that square root of 2 over here minus square root of 2. And square of 2 vectors of that length have a total length of 1, because the total length is given by the square root of the squares of the sum. Being as it is; the principal vectors found by PCA are the one over here, and the one over here. So let's do this again. In all these data cases, I kind of make them up to make my point here, but they could look like this. For the new coordinate system, there's an x and y, and again, maybe we truncate to the nearest 0.5. And then the x prime axis has delta x and delta y, and so does the y prime axis of the new coordinate system. I'm filling in some values to resolve ambiguities. Let's give it a try first with x and y. What's the new center here? Give me those two numbers. I would argue it's about 3, 3, like this. And, then filling it to delta x and delta y for the new axis, over here And, I would argue delta x is 2, so if we go down with, here in this axis, one down in the y direction, then we go two to the right. And, kind of orthogonally, as we go one to the right here, we go two up. So, the delta y over here is also 2. It turns out these two vectors again are orthogonal. If you understand what a dot product is and you did the dot product of these two vectors you will get zero which is a measure of orthogonality. But that's not really that important. What's important is that we found out where the center of the data is and we found the principle axis of variation. The only last thing I didn't tell you about, the only last thing is that PCA also returns an importance value, a spread value for this axis. An that spread value tends to be very large for the very first axis of variation and much smaller for the second axis of variation if the spread is small. So this number is, happens to be an, an eigenvalue. It comes out of an eigenvalue decomposition that's really implemented by PCA. But what it really gives you is an importance vector, how important to take each axis when you look at the data. So when you run the code you'll find the new origin, you find these different vectors and you'll find an importance value is given to these vectors that really measures the amount of spread. So let me do a little quiz here to see if we are at odds with each other. So here are three scenarios, one where the data lies on a line like this, one where the data literally comes from a circle like that in a kind of very equal distribution in the X and Y axis, and one where the data falls vertical. And they're going to ask a couple of true or false questions here. First one, does the principal component analysis return a result? Yes or no for either of these cases. And the answer is yes in all cases. Now the clear cut case is the left one, which is the one we already discussed. But we have data on a circle that could still be a main axis and a secondary axis. And PCA will actually give you a deterministic result, typically the first X in this direction, second this direction. The third one is surprising. When we, remember regression, it's impossible to build a regression that goes vertically because you can't really divide this data set here as a function y equals f of x. But regression treats the variables very asymmetrically. One is the input, one is the output. In PCA, all we get is vectors. So I can easily imagine a coordinate system where the x axis falls vertically, and the y axis goes to the left, and that's the answer for PCA in this case. Let me ask you a different question. Does the major axis dominate? And by dominate, I mean that the kind of importance value, or the eigenvalue of the major axis is vastly larger than the one of the minor axis. After running PCA, can you answer this for the three different cases? And there was a challenging question. In this case, I would say yes. Once you have the spread captured in this direction, there isn't much left in the orthonormal direction. The dominance is also here clear where the major axis really captures all of it and there's really nothing left in the minor axis. But over here is not so clear so even if you draw the major axis to be over here, the minor axis is still as spread as the major axis. In fact, in this case, I would assume that both Eiger values are of the same magnitude and we really haven't gained much by running PCA. So it's a clear no over here. Here's a simple example that will help you develop some intuition for what you're doing when you use PCA. Suppose I have the following question, given the features of a house, what is the price that I would expect for it to sell for? Which of the following algorithms would you use to answer this question? Would you use a decision tree classifier, an SVC or a support vector classifier, or does this sound like a linear regression question to you? And I hope you said that this sounds like a regression question. This is a classic type of regression exercise because the output that we expect to get is going to be continuous. So it's not appropriate to use a classifier. We want to use something like a regression that can give us a number as an output, where this number's the price of the house. When you're making your regression to determine the price of a house, there's going to be a number of input features that you might potentially be interested in. Things like the square footage, the number of rooms in the house, and also things about maybe the neighborhood or the location, things like how well are the schools ranked, and how safe is the neighborhood. And these are all things that you can measure fairly directly. You can go look up the school ranking on the Internet. You can measure the square footage with a, with a measuring tape. But I would argue that when you're making these measurements, you're really just probing two things even though I have four features here. You're probing the size of the home and you're asking questions about the neighborhood. And these are two examples of latent variables. These are variables that you can't measure directly, but that are sort of driving the phenomenon that you're measuring behind the scenes. So, here's a question. Suppose that the latent variable that I'm trying to probe is the size of the home that I'm interested in. Place a check next to any of these features that you think are basically measurements that are probing the size of the home. And I would say that the square footage of a home and the number of rooms are both ways of measuring how big the house is. Of course the square footage is probably going to be a fairly direct measurement of this, but things like how many bedrooms there are, what the floor plan is, potentially also other things like the number of windows can affect how big the house feels from the inside. So that's why size can be a little bit subjective and not something that you can probe directly with a single number. School ranking and neighborhood safety, on the other hand, I would say, are measurable variables that are associated with the quality of the neighborhood overall. So now that we know that we have potentially a large number of measurable features, but maybe a small number of underlying latent features that contain most of the information. My question is what we'll be exploring in this lesson. What's the best way to condense our four features to two? So that we really get to the heart of the information. So we're really probing the size and the neighborhood. One way to get at this is just with simple feature selection, like with the last lesson. And if you put these four features into a feature selection tool, maybe it would keep the square footage, but throw out the number of rooms. Maybe it would keep the safety and ignore the school ranking. So here's a little practice quiz. Which of the feature selection tools do you think would be most suitable for this? These are two examples that are both available in sklearn. Select percentile you're already familiar with, it just selects the top X percent where you're allowed to specify the percentage of features that you want to keep. Another option is SelectKBest and this one's fairly straightforward. What you do as one of the arguments you specify what k is. That's the number of features that you want to keep. So if you put in 10 for k it's going to take the ten best features. And just to make you think about this a little bit more let me get rid of the knowledge that we already have for features. Let's suppose that the number of features you have available to you isn't something that you know very well. Say it's one of the first times you're looking at this data set. But you do know that the size and the neighborhood are going to be underlying latent variables that are sort of driving the trends in, in all of the features that you have available to you. So the question for you is, if you want to have two output features, what's the most suitable feature selection tool to use in this scenario? Do you want to use SelectKBest or SelectPercentile? And SelectkBest is probably going to be the right answer here, because you know exactly how many you're expecting to get out. You're expecting to get exactly two, so it will throw away all the features except the two that are the most powerful. Select Percentile isn't a great choice for this scenario, because you don't already know exactly how many features you have. Although in the case that we gave in the warm up example, where you have exactly four features and you know you want two, you could also just run Select Percentile with 50%, and that will give you two features. So here, are the two crucial ingredients. I have many features available to me, but I hypothesize that there's a smaller number of features that are actually driving the patterns in the data. And then, what I do is I take that knowledge and I try making a composite feature that more directly probes the underlying phenomenon. These composite features are what we'll be talking about in this lesson. They're called principle components. And it's an extremely powerful algorithm. In this lesson, we'll mostly talk about it in the context of dimensionality reduction, how you can use principle component analysis. Or PCA, to bring down the dimensionality of your features to turn a whole bunch of features into just a few. It's also a very powerful standalone method in its own right for unsupervised learning, and I'll give you some of my favorite examples of that in the latter half of this lesson. It's going to be really cool. So, here's an example of what PCA is, transforming from square footage plus number of rooms into a single variable that roughly tracks the size of the house. So, let's suppose this is my training data, I have the square footage on the x axis and the number of rooms on the y axis. Now, what I'm going to do is I'm going to draw in the principle component. It will look something like this. I know this looks like a regression at this point, but it's not, and here's why. With the regression, what you're trying to do is you're trying to predict an output variable with respect to the value of an input variable. Here, we're not really trying to predict anything. We're trying to come up with a direction in the data that we can project our data onto while losing a minimal amount of information. So, here's what that means. Once I've found my principal component, once I have the direction of this, of this vector, of this line, and just go with me for now, that it exists. We'll come back to how we find it in a little bit. Then, what I'm going to do is I'm going to take all of my data points. And, I'm going to go through a process that's called projection. And, what a projection means, it's a little bit of a physical term almost. Imagine that I have a light and the light is sort of shining down perpendicular to the surface of this principle component. Then, you can imagine that all of the data points are going to be sort of casting shadows down onto like a piece of paper if I put it there. Now, my principle component looks like this. So I start out with two dimensional data. But, once I've projected it down onto the principle component, it's in one dimension and it's going to look something like this. Now, instead of being the blue circles, my data is going to be the black xs. I've turned it into a one dimensional distribution. I've taken the feature that I've made in this diagram and sort of turned it so that it's lying flat. What's it going to look like? The correct answer is going to be this last one. This is what the new one-dimensional feature is going to look like after I've done the projection. In other words, if I took this box and I removed all the blue circles and all the green lines, and I just had sort of the red line here with the, with the black x's on it, the new feature, it would look like this last option. So that example that we just did was an example of combining the number of rooms and the square footage into a size feature. And I asked you to just take it on faith for a moment that the principle component existed and it was where I said it was. Now I'm going to use the neighborhood example to show you how to determine the principle component. First I want to start with a little bit of vocab. And the word is variance, which is a little bit confusing because you actually already know a definition of variance. It's a very important definition. It's the willingness or flexibility of an algorithm to learn. But we're going to mean it in a different sense. Variance is also a technical term in statistics, which means roughly the spread of a data distribution. It's something that's very similar to the standard deviation if that's something that you're familiar with. So a feature that has a large variance has instances that fall over a very large numerical range of values that it can take. Whereas something with a small variance means that the features tend to be more clustered together tightly. So here's an example using a as yet unlabeled data set. So suppose this is my data. Then what I can do is draw an oval around it that roughly contains most of the data. Maybe 95% of the data are within this oval. Now this oval could be parametrized using two numbers. One of them is the distance across the narrowest point. The other one is the distance across the longest point. Now here's the critical question. Which of these two lines points along the direction of the maximum variance of the data? In other words, in which direction is the data more spread out? Check the box that you think has the right answer. And this longer line is going to be the direction of maximum variance. You can tell just by looking at the length of the line here. The long line is going to be longer than the short line. So that's going to be the direction of maximum variance. So now you know what determines the principle component of a dataset. It's the direction that has the largest variance in the data. So, in our example, that would be this heavy red line here. You can see how this is very different from a regression, although I know it's sometimes looks like the same thing. But here what you're trying to do is you're trying to find the direction of maximal variance, not to make a prediction. So, here's a question for you. This is a tough one, so give it some thought and then tell me what you think. Why do you think we define the principal component this way? What's the advantage of looking for the direction that has the largest variance? In other words, when we're doing our projection of these, this two dimensional feature space down on to one dimension, why do we project all the data points down onto this heavy red line instead of projecting them onto this shorter line, just as an example of a different way that it could be? Do we want to use this direction because it's the computationally easiest direction to find? Do you think that it's the best because it will retain the maximum amount of information from the original data, that when we do this two-dimensional compression down to one dimension that we keep as much information as possible with this formulation? Or do you think that it's just a convention and maybe there's no real reason. The correct answer is that we do it this way because when we project along the dimension of the largest variance this retains the maximum amount of information in the original data. So, if we have to do a compression we want to choose a compression algorithm so to speak that retains the maximum amount of information from the original data. And, as it turns out, that means projecting it onto the direction of the largest variance Now, it's a little bit beyond the scope of the class for me to actually show you how to find the direction of maximal variance. It involves some linear algebra which isn't too difficult, but we're not going to get into here. But if you're willing to believe me that the principle component, the direction of maximal variance can be found. I can justify to you that, that's the direction that's going to minimize the information loss. Now let's look at the neighborhood information for our housing data. This is my data that I have available to me. On one axis I have a number of safety problems in a given neighborhood. On the other axis I have the school ranking of that neighborhood. And I'm going to use PCA to combine these two features into a single feature that I'll say is roughly gauging the quality of the neighborhood. So just by visually inspecting the data we can see something like this red line is going to be the principal component. Then when I project down the two-dimensional points onto this one-dimensional line I'm going to be losing information. And the amount of information that I lose is equal to the distance between a given point, as it existed in this sort of 2D space, and its new spot on the line. So, for this blue point here, an amount of information that’s roughly equal to the, the length of this yellow line, is the amount of information that I lose. So, here's a little quiz to make sure you understand. I'm circling a point in green here. And this point is going to be projected down onto the principal component as well. Like all the other points in this dataset, eventually. So my question for you is, how much information are we going to be losing when we perform that projection at this point? Is it going to be more, less, or about equal to the amount of information that's lost on our original point, this example that I just showed you? The green data point is going to be the one that's going to have a greater information loss, because the distance from the green point to the red line, is greater than the distance from, the yellow point to the red line. And, the amount of information that's lost is proportional to that distance So if this red line is our principle component, then the information loss is going to be something like the sum of all these distances that I'm drawing in here. The distance between the points and their new projected spots on the new feature on the line. And we can sum this up over all the points, we'll get some number. And here's the key insight. Let me draw in another principle component that we could have hypothesised as, as the first principle component, as the one we wanted to use. Let's suppose that instead of the red line, we were looking at this purple line instead. Then we can ask the same question of the purple line, what's the information loss when we project all of the points down onto it. And we'll start to get something that looks like this. I know it's a little bit cluttered, but I hope what you can see is that on average these purple lines are all going to be significantly longer than the red lines. For any given point that might not be true, but for the points in aggregate, it will be true. Then when we maximize the variance, we're actually minimizing the distance between the points, and their new spot on the line. In other words, it's a mathematical fact that when we do this projection onto the direction of maximal variance, and only onto that direction, we'll be minimizing the distance from the old point to the new transformed point. And what this is necessarily doing is minimizing the information loss. So here's a quick quiz just to make sure that you got it. Suppose this red line is the principal component. It's our new neighborhood quality feature. Of course, that's kind of a meaning that we ascribe to it. mathematically, it's just defined as the definition of maximum variance. And the question is, once we project down all our data points onto this principal component, what is the data distribution that we get for our new feature? So, click the box that looks right to you. And I hope you said this first one. That's roughly what I would expect if we were to project all of these points down and start making xs all along the line like this. There doesn't seem to be any particularly tight or distinct clusters in this data, that PCA would accentuate. And the data is also not particularly spaced out evenly. So that gives you this first answer as the correct one. Now I think you have a good idea of what PCA generally does. And I want to take a step back and talk about it as a general algorithm for future transformation. So remember what we started with was four features that we manually split up into two categories. We said square footage and number of rooms are related to size, and the safety and schools are related to the neighborhood. Then we applied PCA to each of those independently and we came up with that single feature. Then we can put those features into say, a regression and come up with a house price. You could also imagine that these would be good inputs for a classifier if you're trying to answer some slightly different question. But there's a little bit of a problem here and maybe it's been bothering you. And that's this step right here that we manually took our input features and we used our human intuition to figure out how we should group them. And the problem with this is that this manual process is not scalable. Let's suppose we're trying to use PCA for something like facial recognition where the inputs might be thousands or tens of thousands or even millions of pixels in a picture. There's no way that we could inspect all of those pixels by hand and try to split them up according to what we think might be the combinations that make the most sense, and then apply PCA to each of those individually. But here's the magic of PCA. I can take a second strategy with this. What I can do is I can put all four of these features into PCA together, and it can automatically combine them into new features and rank the relative powers of those new features. So if we have the case where we have two latent features that are driving most of the variation in the data, PCA will pick those out and it will call them the first and the second principal components. Where the first principle component, the one that has the most effect, might be something like neighborhood, and the second principle component might have something like size. It's a little bit harder to make these interpretations now because what can and does happen is that the first principal component will be an admixture that has little bits and pieces from potentially all of the features. But this is a very powerful unsupervised learning technique that you can use to fundamentally understand the latent features in your data. If you knew nothing about housing prices at all, PCA would still be able to give you an insight like there are two things that seem to drive the house of prices in general. It would still be up to you to figure out that they're the neighborhood and the size, but in addition to doing dimensionality reduction now, you're also learning something fundamentally important about the patterns of the variation in your data. Here's a concrete follow-up example. Suppose you have a data set that has a 100 training points, and 4 features for each point. What's the maximum number of principal components that are going to be allowed by sklearn, and that you could find using any other implementation of PCA? And, the answer is going to be 4, because it's, remember, the minimum of the number of training points, and the number of features. So, it's the minimum of 100, or 4, which, of course, is 4. If you're still with me at this point, great job. I think PCA is one of the trickiest topics in machine learning, so if you're still sort of struggling to wrap your head around it, don't worry. That's normal. So let me take this opportunity though to just review PCA briefly at a high level and give you kind of a working definition that you can use in the future. PCA is a systematized way to transform input features into their principal components. Then those principal components are available to you to use instead of your original input features. So you use them as new features in your regression or classification task. The principal components are defined as the directions in the data that maximize the variance, which has the effect of minimizing the information loss when you perform a projection or a compression down onto those principal components. You can also rank order the principal components. The more variance you have of the data along a given principal component, the higher that principal component is ranked. So the one that has the most variance will be the first principal component, second will be the second principal component, and so on. Another thing that we should point out is that the principal components are all perpendicular to each other in a sense, so the second principal component is mathematically guaranteed to not overlap at all with the first principal component. And the third will not overlap with the first through the second, and so on. So you can treat them as independent features in a sense. And last, there's a maximum number of principal components you can find. It's equal to the number of input features that you had in your dataset. Usually, you'll only use the first handful of principal components, but you could go all the way out and use the maximum number. In that case though, you're not really gaining anything. You're just representing your features in a different way. So the PCA won't give you the wrong answer, but it doesn't give you any advantages over just using the original input features if you're using all of the principal components together in a regression or classification task. Katie has pulled together, from our favorite data set, information about people's compensation. So each dot in here is a person. And horizontally graphed is an annual bonus, so this person on the right, in this year made $8 million, that's 0.8 times 10 to the 7. And over here is a long term bonus that goes all the way to $4 million. And you can see that there's roughly a bit of a correlation between how much money people made in the annual performance bonus in their long term bonuses. We are less concerned about whether this was ethical and much more concerned about running principal component analysis on this data. so, to do this let me test your intuition first. I'm going to give you for the primary principle component three guesses, a black guess, a red guess, and a green guess. And, from what you've learned so far, which one do you think is going to be the main principal component along which the, the vast amount of [INAUDIBLE] occurs. Pick one. And, the correct answer is something like this line over here. I'm not entirely sure it's the correct line, but there's a correlation between x and y. And, you can see, the more you go to the right side, the more these, these numbers increase. And, that gives me, as the main axis of variation, kind of the spectrum over here The principal components analysis that I'm doing happens to live in this function called doPCA. And it looks very familiar to a lot of the stuff we've done before in scikit-learn. You have an import statement where you actually get the module out that has the code that you want. You create, in this case, the principal components analysis. You fit it. And then you can return that as an object. And so what I do is I, I get my principal components that, analysis that way. And I can ask some very interesting, some very important questions of it by accessing the attributes. So let's explain these three lines. This is how I actually get the information out of my PCA object. The first, the explained variance ratio is actually where the eigenvalues live. So by printing out this, this line here. This is the way that I know that the first principle component has about 90, 91 percent of the variation in the data, and the second one has about 9, or 10 percent. Those numbers come from this statement. And then the second thing that I do is I look at the first and second principle components. I get these out of the components attribute of my PCA object. So the components is going to be a list, a Python list that has as many principle components in it. As I ask for as a parameter. So in that case, I have two principal components that I'm getting. So I name them the first and second pc. So in previous quizzes where we were talking about, what's the direction in of, say, x prime in the xy original feature space, we came up with two numbers that were sort of packaged together into a vector. You can access that directional information through these components. Once I've fit my principle components analysis I have to, in order to do anything, perform something like a transformation of the data. And this code I will just give you in the starter code for the quiz. What I'm doing here is, I'm visualizing it. The first line is in red. I'll be plotting the first principle component, the locations of all the points along that principle component. As well as the direction of the principle components. I'm accessing that information by using the elements of the first PC vector. Then in Cyan or kind of a teal color, I'll be accessing the second principle component, and in blue I have the original data. So let me show you what this looks like, and then you give it a try yourself in the quiz. The first thing that you get is that you print out the eigenvalues. Remember that's this explained variance ratio information. And then the second thing is that you'll get a scatter plot. And it should look something like this. So you remember the red was the direction of our first principle component. And that's hopefully exactly where you guessed it was. Certainly intuitively seems like it's in the right place. The cyan is perpendicular to that. And then the blue is the original data points. One thing that I'll add is that it looks to the eye like the red and the cyan are not perfectly orthogonal. This doesn't quite look like a 90 degree angle. But remember that our axis have different scales. That this one goes all the way out to ten million, the y axis only goes out to less than half of that, about four million. So, in reality, if we were to plot everything proportionally, this graph should be twice as long as it is tall. And if we were to visualize it in exactly that way, they would be orthogonal So now you should have a good intuition for what PCA is, and how to use it. The last example I'm going to give you, starting very soon, is one of the coolest examples of where PCA is actually used in the wild. But before we get into that, let me just pause and, and talk about when you want to use PCA. When is it an appropriate approach? The first one is if you want access to latent features that you think might be showing up in the patterns in your data. Maybe the entire point of what you're trying to do is figure out if there's a latent feature. In other words you just want to know the size of the first principal component. An example of this might be something like, can you measure who the big shots are at Enron. The second one of course is dimensionality reduction. There's a number of things that PCA can do to help you out on this front. The first is that it can help you visualize high-dimensional data. So, of course when you're drawing a scatterplot you only have two dimensions that are available to you, but many times you'll have more than two features. So there's kind a struggle of how to represent three or four or many numbers about a data point if you only have two dimensions in which to draw it. And so what you can do, is you can project it down to the first two principal components and just plot that, and just draw that scatter point. And then things like, k-means clustering might be a lot easier for you to visualize. You're still capturing most of the information in the data but now you can draw it with those, with those two dimensions. Another thing the PCA can help with is if you suspect that there's noise in your data. And, in almost all data there will be noise. The hope is that the first or the second, your strongest principal components, are capturing the actual patterns in the data. And the smaller principle components are just representing noisy variations about those patterns. So by throwing away the less important principle components, you're getting rid of that noise. The last one, and what we'll use as our example for the rest of this lesson, is using PCA as pre-processing before you use another algorithm, so a regression or a classification task. As you know if you have very high dimensionality, and if you have a complex, say, classification algorithm. The algorithm can be very high variance, it can end up fitting to noise in the data. It can end up running really slow. There are lots of things that can happen when you have very high input dimensionality with some of these algorithms. But, of course, the algorithm might work really well for the problem at hand. So one of the things you can do is use PCA to reduce the dimensionality of your input features. So that then your, say, classification algorithm works better. And this is the example that we'll do next. It's something called eigenfaces, and it's a method of applying PCA to pictures of people. So this is very high dimensionality space, you have many, many pixels in the picture. But say, you want to identify who is pictured in the image. You're running some kind of facial identification, or what have you. So with PCA you can reduce the very high input dimensionality, into something that's maybe a factor of ten lower. And feed this into an SVM, which can then do the actual classification of trying to figure out who's pictured. So now the inputs, instead of being the original pixels or the images, are the principal components. So let me show you this example and you'll see what I mean. This whole discussion has been a little abstract. So now I want to show you one of my favorite uses of PCA in the wild. And that's facial recognition. So first let's think about what makes facial recognition in pictures a, a good application of PCA. Why is this problem and the solution well suited for each other. Here are some options and the answer could be one or more of the following. The first option is that pictures of faces generally have a high input dimensionality. In other words if there's many pixels in a picture of a face. And that potentially any of those pixels could be important for figuring out who that person is. The second possibility is that faces have general patterns that could be captured in a smaller number of dimensions. So there's always the general pattern that someone has two eyes that are sort of near the top of their face. There's a, a mouth and then a chin that's on the bottom, and etcetera. And the third possibility is that facial recognition is simple, once you use machine learning. That since humans can do it so easily, a computer can do it easily too. I would argue that the first two answers are the ones that are more accurate. Picture of faces certainly do have many pixels. A one megapixel camera will have a million input features if you're using it in the raw input space. So this is really prime to get some reduction somehow. because a million features would be really difficult for, say, an SVM to handle. The second thing that's true is that there are some underlying patterns in faces that is really well suited for PCA. So since we think that there are just a few underlying latent variables, so in the example of a one megapixel image, I wouldn't say that there's a million points of difference between two faces. That there will just be a few general points of difference and that maybe PCA can, can tease those out and use them to their maximum ability. And the third thing that I wouldn't say is true is that facial recognition is simple using machine learning. For example, a decision tree will have a lot of difficulty trying to do facial recognition. As we'll see in the following code example using PCA and, say, an SVM together, you can get a lot of power. But in general, even though it's something that humans do quite easily. We have brains that are particularly well optimized for recognizing each other. It's an evolutionary advantage. And computers a priori don't necessarily have that ability. So doing facial recognition with machine learning, while it's a well understood problem at this point, is something that is maybe a little bit trickier than you would naively first guess. And now for the next few videos I'm going to show you an actually coded up example of PCA as applied to facial recognition. This is taken and adapted from an example on the sklearn documentation, so I'll include a link to the original code so you can go in and take a look at it, but I want to walk you through some of the most important parts. So the first thing that the code does is, import a set of pictures of famous world leaders from about 10 to 15 years ago. The next thing that it does is it splits it into a training and testing set. Very good, but then this block right here is where the PCA actually happens. You'll see in this example, and sometimes in the literature that PCA is also called eigenfaces when it's applied to facial recognition. So in this line here we actually see the PCA being created, it's called RandomizedPCA in sklearn. And then it's also being fit to the training data. Then what this line does here is it asks for the eigenfaces. The eigenfaces are basically the principle components of the face data. So it takes the pca.components, and then it reshapes them. because right now they're just strings of numbers, and it wants to turn those back into squares so that they look like pictures. You can also see that the number of principal components that are being used in this example is 150. And if you look at the example code on the sklearn documentation page, you'll see that the original dimensionality of these pictures is over 1,800. So we've gone from 1,800 features down to 150, a compression factor of more than 10. Then the last thing that I do with PCA is I transform my data. When I perform the fit command I'm just figuring out what the principle components are. It's these transform commands where I actually take my data and transform them into the principle components representation. And the rest of this code is creating an SVM. Remember the SVC is what its called in sklearn, a support vector classifier. They do a little bit of fancy footwork here to figure out exactly what parameters of the support vector machine they want to use. And then using the principle components as the features, they try to identify in the test set who appears in a given picture. Now I'll show you what this looks like when I run this code. First thing that it does is it prints out a doc string, just tells me what's going on. And then some information about the data set. So I have 1,288 samples, with 1,850 features available to me in the input feature space. Seven different people are appearing and then we use 150 eigenfaces from the 966 faces in our training set. The next thing that appears is a list of the people in our data set, and how often we get them correct. Precision, recall, f1-score and support are things that are sort of related to the accuracy, their evaluation matrix. We'll talk a lot about those in a coming lesson. You can see that in general though, it's getting things correct. Roughly 60% to almost 90% of the time. So even though we've reduced our dimensionality by a factor of ten, we're still having really good performance. Another thing that's really cool that I love about this example is they actually show you the eigenfaces. So this is the first principle component of our data, and this is the second principle component, the third one. So this image here represents the maximal variation that it sees across all the data. It's a little bit hard to understand exactly what that means in this example. It would be better if it said something like, how far apart the eyes are, or whether the person wears glasses or not. Instead, what we get are these kind of ghost images. But then using these composite images together as features in an SVM, it can be very powerful in predicting who's face we see. And then last but not least, there's a little printout that gives you 12 representative faces and the algorithms best guess about who it is, and the actual answer. So you can see it doesn't always get them correct. This one right here I think this is Tony Blair, but it's actually George W Bush, but on most of them it's getting it correct, pretty cool. I don't know about you but I think the eigenfaces example for PCA is really, really cool, and I like it enough that I'm going to have it be the basis of the coding exercises you'll be doing in this mini-project for the PCA lesson. So what I'll do is give you the eigenfaces code, and point you towards some of the places that I want you to play around with, say the number of principle components that are being used. Or to figure out what the eigean values are for the various principle components. So, at the end of this mini lesson, you're really familiar with the PCA code. And you can go in really quickly and identify where all the important moving pieces are. I'm going to let you explore that code we just looked at on your own during the project portion of this lesson. There's just one more thing I want to cover though before I set you loose on that. And that is how to think about selecting how many principal components you should look at. In that last example, they went from 1850 features on the input to 150 principal components. And the question you might be wondering is, how did they come up with that number? It's a little bit tough to answer that, because there's no cut and dry answer for how many principal components you should use. You kind of have to figure it out. Which of these options do you think sounds like a good way to figure out how many principal components you should be using? Here are your options. The first one is, just always take the top 10%, as a rule of thumb. A second option is that you can train on a different number of principal components, and then see how the accuracy response for each of those possible numbers of principal components. And then, once you do this several times, it will become clear that there is a point of diminishing returns where as you add more principal components, you don't get much more in terms of discrimination. And that you should cut off once you start to plateau. And the third option is that you could perform feature selection on the input features before you put them into PCA, and then you just use as many principal components as you had from input features after you've performed that selection. This is sort of a tricky question, so if you, if you guessed, I don't blame you. The correct answer is number two, that what you want to do is be open to different number of principal components being relevant to you. And the way that you figure out, sort of, what that trend is, is just by trying different numbers. You say, just give me the first principal component, the first plus the second, the first plus the second plus the third, and you watch how the accuracy responds to these increasing numbers of principal components. You can also do this in just plain old feature selection as well. You say, give me all the features in the order of their importance, and I'm just going to try adding them one at a time and see how the accuracy responds, and cut off when it seems like it's plateauing. One thing you don't want to do though is performing feature selection before you go into PCA. So remember PCA is going to find a way to combine information from potentially many different input features together. So if you're throwing out input features before you do PCA, you're throwing our that information that, that PCA might be able to kind of rescue in a sense. It's fine to do feature selection on the principal components after you've made them, but you want to be very careful about throwing out information before you perform PCA. That having been said, PCA can be fairly computationally expensive. So if you have a very large input feature space and you know that a lot of them are potentially completely irrelevant features, go ahead and try tossing them out, but proceed with caution. Welcome to the lesson on cross-validation. You've been doing validation all along in this class, so this won't be brand new for you. But in this lesson, we'll actually go into some more detail, some more comprehensive algorithms that can help you validate your machine learning in a better way. Remember back to earlier in this class, I gave you an example of a machine learning algorithm that actually sells me clothes. And that was not a made up example. There's a startup in San Fransisco that does that. But you can imagine that if you're the person who's in charge of writing that machine learning algorithm, you don't want to find out if I'm going to like a shirt by just mailing it to me. You want to have some way of assessing whether your algorithm is, is actually doing what you want it to do, of testing it, of validating it. So that's what we're going to be talking about in this lesson. I'll see you in the classroom. Here's a quick quiz reviewing why we use training and testing data. Check all the answers that apply. The first option is that using separate training and testing data allows you to make an estimate of the performance of your classifier or your regression on an independent dataset. The second option is that having separate training and testing data serves as a check on overfitting. The third possibility is that the training testing split is kind of deep and mysterious, but the experts tell us we should do it, so, that's why we do it. And the last option is that since it's so important to have as much training data as you possibly can, separating your dataset into training and testing data helps you maximize the amount of data that's available for training. And I would say that the first two answers are probably the ones that are the most accurate. So testing data of course gives you an assessment of the performance of your classifier or your regression on an independent data set and also helps you prevent overfitting. The experts do say that you should do this training testing split but that's not really the reason why we do it. We do it because it helps us understand our results better. And one thing that it definitely doesn't do is maximize the amount of training data available. So if you have to hold out 10 or 20% of your data for testing, then of course that's not going to be available when training. And this is one drawback of the training testing split. We've talked a lot about splitting your data between training and testing sets in this class so far. What I want to do now is show you how you can actually do this very simply using some tools in sklearn. Of course, you don't need sklearn to do this. You just need to be able to split up your data, but sklearn has some functions that turn this really into a, a one or two line of code type problem. If we want to do a training testing split in sklearn, we want to look for a group of functions that fall under the general title of cross-validation. In this documentation page they have a little bit of a warm up, the idea of cross validation, why you might do it. But these are things that you're already familiar with because we've talked about them a lot in this class. What I want to do is skip ahead to this code example right here and show you how this can actually be deployed in code. The output of this is going to be four sets of data. The first two are going to be the features for the training and for the testing data. The second two are going to be the labels for the training and the testing data. So in this example there's a couple lines here probing how many events fall into each of these two data sets, the training and the testing. And then down here you can see that there's actually an SVM that's now being trained on the training data, the features in the labels, or as they call it X_train and Y_train. And then we can do an evaluation on the test set. So this is looking at something called the score of the classifier. This is sort of related to the accuracy. So this is something that you want to be evaluating on your test data. Now one of the things that maybe you noticed on some of the mini-projects, if you were paying very close attention, is that we've actually been deploying this code all along. So in all of the, for example, classifier training exercises that you did there was a line of code that was very similar to this one, where I was splitting the data into a training and testing split for you. So now what I'm going to ask you to do is, in the form of a programming quiz, I'm going to ask you to actually deploy a line of code like this on your own so you have some practice doing it. I'll give you a dataset that's being read in with all of the features and all of the labels. And what I want you to do is to insert a line of code here that splits it into the training and the testing sets. Then I'll take those training and testing sets and train and make predictions using a classifier. So your job is just to practice deploying a line of code like this one. The train test split in sklearn. So that code will execute correctly. So give it a shot. At this point in the class, you've learned a lot about many of the different moving pieces that might go into a machine learning analysis. And I think it's worth taking a step back and explicitly talking about where you want to use your training data and where you want to be using your testing data, because this can be a little bit confusing sometimes. Suppose that your overall analysis flow looked something like this. You start off by splitting your overall data into training and testing sets. Then the next thing that you want to do is apply PCA, so a feature transform. And then you want to take a few of your leading principle components and put them into an SVM, a classification algorithm. Remember that PCA itself can have a few different commands that you might call on it. So you can call fit on your PCA, which actually finds the principal components of your data. But then you also have to call pca.transform. Which uses this fit that it's found to actually transform the data into the new principal components representation. Likewise, a support vector machine or a support vector classifier can be fit on some of the data and then be used to make predictions on a different set of data. So let's suppose that coming out of the training testing split, we have two different sets of features. We're only going to look at the features in this quiz. We have what's called training features and test features. The first thing you want to do is apply principal components. Specifically, you want to find the principal components using the fit command. Do you want to apply the pca.fit to the training features or to the testing features? You want to call fitting on the training features because you only want to be looking for patterns in the training data right now. Remember the testing data is only something we're going to be using as a tool for validating the steps that we're taking. So, if we fit our PCA using our testing data, we're sort of giving ourselves a little bit of an unfair advantage. We only want to be looking for patterns in the training data right now. Here's the next quiz. What I want to do now is take my principal components and I want to feed them into my support vector classifier. I do that using this train command on my svc. But first, I need to do a transformation. When I do this pca.fit, I'm only finding the principal components. I'm not actually transforming my data into the principal components representation. That transformation happens via the transform function. So my question for you is, to which data set should I apply this transform function, the training features or the testing features? The correct answer here is going to be the training features. In order to use a principle components when I'm training my support vector classifier, I need to transform my training data set into the correct representation. So you want to be feeding your training data set to this transform command. So at this point the training of my support vector classifier is done, including the principle components transformation. Now I want to move on to testing. In order to do that, I need to repeat what I did with my training features, this time with my testing features. And that means starting with a PCA transform. Something very important that I'm not doing here, is refitting my PCA on my test features. I want to be very consistent between what I've done with the training features, and what I'll be doing with the testing features here. So that means using the same principle components that I found in the training features and representing my testing features using those principle components. So that having been said, when I call PCA to a transform, do I want to be doing it on the training features or on the testing features? I want to transform my test features now, just like I did with my training features above. And remember, since I haven't called pca.fit again, what this means is I'm going to represent my test data now with the principal components that I found on my training data. This is very important. If I refit my PCA using my test features at this point, it's not going to work correctly. Last but not least, I want to make some predictions using my Support Vector Classifiers, this time on my test dataset. So, will you use the test features or the training features to do that? The correct answer, of course, is that you want to make your predictions using your test dataset, that's the whole point. After this PCA transform, your test dataset is now in the same form as your training dataset. It's been, it's had the principle components analysis applied to it. So now you want to feed it into your Support Vector Classifier and see what predictions you get. So Katie, you told everybody about training and test sets, and I hope people exercise it quite a bit. Is that correct? Yes, that's right. So now I'm going to talk about something that slightly generalizes this called cross validation. And to get into cross validation, let's first talk about problems with splitting a data set into training and testing data. Suppose this is your data. By doing what Katie told you, you now have to say what fraction of data is testing and what is training. And the dilemma you're running into is you like to maximize both of the sets. You want to have as many data points in the training sets to get the best learning results, and you want the maximum number of data items in your test set to get the best validation. But obviously, there's an inherent trade-off here, which is every data point you take out of the training set into the test is lost for the training set. So we had to reset this trade-off. And this is where cross validation comes into the picture. The basic idea is that you partition the data set into k bins of equal size. So example, if you have 200 data points. And you have ten bins. Very quickly. What's the number of data points per bin? Quite obviously, it's 20. So you will have 20 data points in each of the 10 bins. So here's the picture. Whereas in the work that Katie showed you, you just pick one of those bins as a testing bin and the other then as a training bin. In k-fold cross validation, you run k separate learning experiments. In each of those, you pick one of those k subsets as your testing set. The remaining k minus one bins are put together into the training set, then you train your machine learning algorithm and just like before, you'll test the performance on the testing set. The key thing in cross validation is you run this multiple times. In this case ten times, and then you average the ten different testing set performances for the ten different hold out sets, so you average the test results from those k experiments. So obviously, this takes more compute time because you now have to run k separate learning experiments, but the assessment of the learning algorithm will be more accurate. And in a way, you've kind of used all your data for training and all your data for testing, which is kind of cool. Say we just ask one question. Suppose you have a choice to do the static train test methodology that Katie told you about, or you do say 10-fold cross validation, C.V., and you really care about minimizing training time. Minimize run time after training using your machine learning algorithm to output past the training time and maximize accuracy. In each of these three situations, you might pick either train/test or 10-fold cross validation. Give me your best guess. Which one would you pick? So for each minimum training time, pick one of the two over here on the right side. Obviously if you want to minimize training time, training once is faster than training ten times. So it's obviously TRAIN/TEST is, is, is probably the better one to choose. If you just want to minimize run time, well in each case you end up picking one machine running algorithm. I would then say if you just care about minimize run time, you might as well get the benefit of 10-fold Cross Validation, to have a better assessment of what's going on. So I would put my cross-mark over here, but it's not clear this, either one could be correct. But if you want to maximize accuracy of your assessment, how good your algorithm is, you definitely go for the penalty of going into 10-fold Cross Validation. Take the extra time and you will get a better result. Now that you've had a little introduction to the theory behind k-fold cross validation, it might not surprise you to know that we're going to take a look at the code. K-fold cross validation is supported in the cross validation set of functions that are available in sklearn. So I want to show you what it looks like when it's being deployed. The code that I have right here is code that you're actually already familiar with. This is the code that we used in the very first mini project, to identify the author of the emails. Was it Sara, or is it Chris? I'm just using a Gaussian Naive Bayes. I'm keeping track of the training time, the predicting time and the accuracy that I get for this classifier. Remember, the features that I am using are the words in the emails and I'm trying to predict who wrote the email based on the vocabulary that's being used. Up above that I see some feature transforms that I'm doing, I am reading in the text, and I'm doing some feature selection. These are all things that we've talked about, so I won't belabor them too much right now. More interesting bit of code is up here. The first thing that I do is from sklearn.cross_validation I import KFold, which is the name of their k-fold cross-validation function. Then I have to declare a k-fold object, which accepts two arguments. One is the number of items in the total data set. So in this case, I say that's the same as the number of authors in the data set. And the second argument is going to be how many folds do I want to look at. In this case, the answer is two. This is something that you might want to change though. So then what k-fold is going to do is it's going to give me two lists. The first one is going to be a set of indices of all the data points that I'm going to use in my training set. The second list is going to be a list of all of the indices that I'll use in my test data set. then what I can do is I can use these indices to go into my features and into my labels and to assign each set of features or each label, to either my training or my testing data set. So this is the actual partitioning of the data into the training and testing data sets. Then, below that it's just like before, we do a little bit of transformation on our training and our testing data. Then we train our classifier and we assess the accuracy. Now if I just do this out of the box, something kind of wrong is going to happen. So let me show you what happens when I run this code. So I have two folds, so I should get two sets of training times, predicting times, and accuracies. See, it only takes a few seconds to do the training and predicting, but the accuracy is really terrible, less than 1% on my first fold and only 15% on my second fold. So what on Earth might be going on here? To help unpack this, I'm going to add a couple of print statements to this. The first thing that I'm going to do is I'm going to actually look at the indices of all of the events that are going into my training data set. Are there any patterns where all of a particular type of event end up in the training data set, and all of a particular other type of event end up in the testing data set? because if that's the case, then I shouldn't expect that training on one type of event allows me to classify very well another type of event. The other thing I'll also do is print out the labels of all of the events that are in my training data set. So this is repeating some similar information, but now I'm looking at the labels directly. And last, I'll print out the authors of the testing data set to see if there's some important difference between the events that are going into the training data set and the testing data set. Ha, so we get a lot of output because I have thousands of events. Let me go back up to the first line. This line here is showing me the indices of all of the data that's going into my training data set, and I can see that what it's doing here is it's just counting sequentially. So all of the events from 8,789 until the end of the data set, 17,577, are going into the training data set. The other half that go from 1 to 8,789, those are all going to the testing data set. So in other words, I'm not shuffling my events around at all. I'm not doing any randomization. I'm just splitting my data set into two pieces, right down the middle. And as it happens, there's a pattern in my data set that this will mess up. The pattern happens to be that all of my emails from Sarah are in the first half of the data set. And all of my e-mails from Chris are in the second half of the data set. And this becomes immediately clear when I start looking at the labels that are showing up in the testing and the training data set. So, all of these ones right here are the labels of the items in the training data set. You can see everything that's in the training data set is of class 1. Then I start a second list. These are the labels of all of the items that are in the testing data set. I can see that everything that's in the testing data set is of label 0. And then I have maybe a few 1s at the very end. So what's happening is I'm doing all my training on the emails from one person and then using them to try to classify emails from the second person. This is like training a machine learning algorithm to drive a car and then asking it to cook you breakfast. It's not going to work. So, and this is a feature, I would say, of how the sklearn KFold algorithm works. Is it's just going to split the data sort of into equal sized partitions. It's not going to perform any type of shuffling of the events. So if there's patterns in the way that the classes, especially, are represented in your data, then these patterns are going to be reflected in sort of big lumps of certain labels, ending up in particular folds of your validation. This is probably not what you want. Now that you know all about K-fold cross validation for validating the performance of your algorithm, I want to show you a different use of it that I really love. Remember back to very early in the class we were talking about machine learning algorithms like naive bayse support vector machines and decision trees and especially the parameters that you have tune for these to get the best performance out. The way we were tuning these parameters was kind of with a guess and check method. But as it turns out this is really clunky. It takes a long time. Nobody likes to do it. It's just the worst. Cross validation can actually help you out here. It can automate a lot of that testing. And pick our the parameter tune that's going to be best. So let me show you how to do that. So we did it. We came to the end. Great job. So now you know how to use cross validation to validate your machine learning algorithm and to also pick the parameter tune that's going to work the best for you. So what is next? Next thing is the project. We're going to give it a little bit of practice. Hands on the keyboard. It's really exciting. Have fun with the project now. The final project for this class is building a supervised classification algorithm that can tell, based on our Enron data set, who at Enron was a suspected person of interest in the fraud case that happened when the company went bankrupt. In this mini-project, you're actually going to build your first iteration of this person of interest identifier. So, the first thing will be building a, a sort of minimum viable product. We're not expecting it to be really great, we're not expecting it to be really optimized, but you'll have something up and running in the first part of this mini-project. In the second part of the mini-project, we're going to wrap this in a training testing framework. So that from this point forward, when you go in and you compute the accuracy of your person of interest identifier, you have it all set up properly so that it's, it's computing that accuracy on the test set like it should be. And then from this point forward, once you have this code set up, your job is just going to be then to fill in all the features that you want to use or do any transforms, things like that. So, let's get started on building that code framework. Welcome to the lesson on evaluation metrics. Evaluation metrics are how you can tell if your machine learning algorithm is getting better and how well you're doing overall. And we are at my favorite place in the entire company, which is the foosball table where I get to evaluate it myself, pretty much every day by applying or playing my colleagues and see how I score. And people like me who aren't quite as good at foosball might not use wins and losses as our evaluation metrics, but maybe, I would say something like how many goals do I score overall or how long does the game last? In machine learning, it's really important that you know your evaluation and understand how to apply your evaluation to measure the right result. And that's what we'll be talking about in this class, so shall we? Let's play a game. All right. So far in this course, we've been using a very simple, very straightforward metric to say whether our classification algorithms are performing well or not. We've been using the accuracy. Remember, accuracy is the number of items or data points in a given class that are labeled correctly, that we identify correctly as being a member of that class, divided by all the number of items or data points in that class. So here's a quiz for you. Although accuracy is a perfectly fine metric and a good place to start, it does have some shortcomings, so let's think about what those might be in the case where we want to identify persons of interest in the Enron data set. So here are a few possible shortcomings. And you check all of the ones that sound like they could be problems in our case. The first possible problem is that maybe accuracy is not ideal for skewed classes. So skewed class would be an example of where you have many data points and a few of them fall into one class and many of them fall into the other class. This is what's going on with the Enron data set because, although there were thousands of people who worked at Enron, there were only a few dozen who ended up really being pursued by the law for their involvement in, in the fraud. So accuracy might not be an ideal metric in that particular case where you have very few examples of, of one of the classes. Another possibility is that maybe, depending on exactly what you mean by identifying a person of interest, you might want to err on the side of guessing that someone is innocent when you're in doubt. That if, for example, as a result of this, that maybe you're you're machine learning algorithm is identifying people who should be thrown in jail. In that case, I would say you want to be really sure that someone is involved in the fraud before you risk putting them in jail for a long time. And that if you have any doubt at all, you should err on the side of assuming that they're innocent. That accuracy perhaps might not be particularly tailored to that outcome that you want. And a third possibility might be that accuracy isn't well tailored to the outcome that you want but in the other direction. Let's suppose that you want kind of a broad net being cast. That all the people who are identified as persons of interest just means that they're going to get a little bit of a higher level of scrutiny. And that there might be still lots of investigation going on. And so in that case, what you would want to do is make sure that you get as many people who were actually involved in the fraud as possible identified. Even if it comes to the cost of identifying some people who were probably innocent. Because those innocent people will then be investigated and cleared in later rounds of the investigation, and that accuracy might not be the right metric if that's the goal that you're going for in trying to identify persons of interest. So check any of these boxes that you think could be a little bit of a problem if all you're doing is trying to maximize the accuracy of your classifier. And I would say that all of these are concerns that you should have when you're thinking about the metric that you should pick. And they do all happen to be problems with accuracy in the, in the case of identifying persons of interest. So, accuracy can give you a problem when you have skewed classes because the denominator of this formula, all the items in this class, ends up being a very small number, which means the accuracy that you compute as a result is something that you might not be sort of as, as trustworthy of as if you have lots and lots of examples of, of instances of that class. Also, the second two options are going to be things you want to think about, that you want to pick an, a metric that's really tailored to the operation that you're trying to do, and that, as a result, you might want to maximize the chance that you err on the side of guessing that someone's innocent. Or that conversely, you err on the side of flagging them as a person of interest even if it means that you catch more innocent people sort of in your net as a result. Accuracy is not particularly well-suited to either of these outcomes, but there are some evaluation metrics that are, and that will be what we talk about in this lesson. Now that you've had your refresher on accuracy, which of course you're well familiar with from earlier in this class. We're going to talk about some other evaluation metrics that might be a little bit more specific depending on what you want to be doing with your machine learning algorithm. In many cases in real life you care about certain outcomes more than others. We have an Asymmetry. For example, if you write a cancer detection test. I would argue care more about that the cancer gets detected. Even if you tolerate, occasionally, false detection, than overlooking cancer which could be disastrous for you. That's a case where your performance metric favors one type of error differently from another type of error. So depending on the metric you use it can help you really zero in on what types of mistakes you might be making. And you can tune your algorithm for exactly what you want it to be optimized for. And it's really important you know about this and apply it because otherwise you might not like your results. So I'm going to talk about confusion matrices. It's a word you've probably never heard before, but it's good vocabulary if you want to do machine learning and talk to somebody else about it. Imagine we have a classifier above this line, the class tables are red Xs, and below the line we have green circles. As before, we call the red X as positive examples and the green circle's negative. The confusion matrix is a two by two matrix. It compares the actual class, which can be positive or negative. With it, we called a predicted class, it's our output which also can be positive or negative. And each on the right increments the count on the left. So let's see. Let's take a data point over here that is negative, and that is classified to lie on the green side of the classifier. If I wanted to count this type of data, in which of the four buckets on the left would it fall? Just click on the corresponding pocket. And the answer is it would fall into the bucket over here. The actual class is negative, but our classifier also puts it in the negative side so it's this bucket over here. Let`s take this point over here, and click on the corresponding bucket on the left. This one obviously is a positive data point and it falls on the positive side of the classifier. So this prediction is also positive, so this is the right packet. And obviously things become more challenging for misclassified points. So let's take this one over here. We'll just add it. Where on the left would that fall? And this data point is truly positive. It's a red cross, but it's predicted to be negative. So truly actual positive was predicted negative falls over here and not over there. I hope you got that right. So let's now define the confusion matrix. I added a few data points on the right side here. Now I want you to count the number of points that fall into each of the four quadrants of the confusion matrix. So count on the right and put the numbers on the left My answers would be, there are 2, 4, 6, 8, 9 positive positives. 8 over here. Number of positives, actual positives that I misclassified as negative are 3, so the guys over here. And only one truly negative is misclassified as positive. So that's the confusion matrix. So I have one final question here. Suppose this is an alarm system and true means or positive means there's a burglar in your house. What box, what type of events and these would you think best characterizes a false alarm of the alarm system? So click on those four boxes and tell me which ones are false alarms I say the false alarm is one where your alarm goes off, so positive for burglary, but there isn't a burglary. So in the true world the event is negative, but your system says positive. So this is where the check mark should go and you can see a bit of an asymmetry. You can see that you might care about negative events that give you a positive alarm differently from positive events that are being missed. And just to give you kind of a little bit of look ahead, often by changing parameters you can move this curve in either direction. And if you believe that a false alarm, which is the police come to your house checking for burglary, is a different price point from a missed event like someone robbing you of all your belongings without police even noticing, then you might want to reshift this line up and down. So here I'm drawing what could be the result of decision tree learning. Where again red is my positive class and green's my negative class, and then giving you a whole bunch of data points. And in the mere counting exercise I want you just fill in the confusion matrix. Once again we have a actual class and a predicted class. The actual is on top. The predicted on the left. And we have once again the four different cases. So take a look at the right side and do your counts. The main diagonal is easy. It's green counted correctly. One, two, three, four, five, six, seven, eight, nine. For white, I get a total of eight, and then the number of, and then the misclassified ones are these three data points over here that I'm circling in blue. This is a true negative classified as positive, and it goes over here. It's truly negative but predicted as positive, and these guys over here are just the opposite. They go there. So next, you're going to do a seven by seven confusion matrix from what? From the PCA lesson. The item faces. Okay, I'm handing it over to Katie. So talk about confusion matrices in different context, you might remember the work that Katie showed you on principle component analysis, on PCA. Were we looked at seven different, I guess white male politicians from George Bush to Gerhard Schroeder, and we ran an eigenface analysis. Extracting the principle components of this data set, and then re-use the eigenfaces to re-map new faces to names in order to identify people. So what I'm going to do now, I won't drag you through the same PCA example again, so let's do away with those faces. But instead what I'll do is, I give you a typical output and we're going to study the output using confusion matrices. So Katie was so nice to run a PCA on the faces of those politicians and take the resulting features, put it into a support vector machine and then go through the data and count how often any of those people were predicted correctly or misclassified. And just to confuse everybody, in this example, we follow the convention of putting the true names, the true class labels, on the left, and the predicted ones on top. So, for example, this number one over here was truly Donald Rumsfeld, but was mistaken to be Colin Powell. And the way I know it's Colin Powell was the same names over here, from Ariel Sharon to Tony Blair, apply to the columns over here. Ariel Sharon's on the left and Tony Blair's on the right. So we ask you a few questions now. First, a simple one. Which of those seven politicians was most frequent in our dataset? And the answer is simple it was George W Bush. It's a big number over here. Second question, how many images of Gerhard Schroeder are in our data set? And the answer is 26. It's the sum of these numbers over here, 1, 7, 14, and 4. And here's a related question. On the prediction side, how many predictions of Gerhard Schroeder are there? Should be relatively simple to compute. And the answer is 15. It's this column over here, 14 plus 1. This will orient you a little bit to understand this data. Obviously, in a confusion matrix, you're happy if the main diagonal has the largest number of values, because all the off-diagonal elements correspond to this classification. So on to trickier questions. Suppose we look at an image of Hugo Chavez. What's the probability that Hugo will be classified correctly by our learning algorithm? That's going to be a number between zero and one where one means with absolute certainty and zero means never ever. You can calculate the probability based on this table over here. And the answer is 0.625. And the way I got this,I look at the cases where we have Hugo Chavez. There's a total of 16. Three plus two plus one plus ten. And of those 16 cases, the number of ones that are classified correctly are only 10. So 10 over 16 is 0.625 Now I'm asking a different question. Suppose your predictor says it's Hugo Chavez. You run your learning algorithm and it classifies it as Hugo Chavez. What's the probability that the person actually truly is Hugo Chavez? And here the answer happens to be one. Seem to have a perfect of Hugo Chavez is actually correct. That is, if you look at all the cases where Hugo Chavez was recognized in the image by your learning algorithm, in 10 out of 10 cases you got it right. So 10 divided by 10 is one So, let me tell you about two technical terms. One called recall, one is called precision. These are the terms for what you just computed. For example, the recall rate on Hugo Chavez is the quotient of how many, is the probability of our algorithm to correctly identify Hugo Chavez, provided that the person actually is Hugo Chavez. So in our case, it was 10 over 16. Precision of the same concept would be the rate of which the rate of which if our algorithm predicted would be the second rate that we studied, which happened to be one. It's the quotient of, suppose our algorithm observes that there's Hugo Chavez, what are the chances that it actually is Hugo Chavez and it happened to be 10 over 10? So let's apply the recall and precision concept to a different name. Let's take Colin Powell. Can you, from this matrix, compute the recall rate and the precision rate for Colin Powell? The answer for recall is 0.87. We know that there is 63 instances of Colin Powell at the outset of which we only get 55 right. That's our recall. Our precision is even lower in this case. It's only 0.82. We look at the number of cases where we identified Colin Powell. That's a total of 67 over here. We find that only 55 out of 67 are correctly classified. That gives a precision of 0.82. So just one more training exercise. And let's do the same for George W Bush, give me the two numbers on the right. The recall, the cases when it's truly Bush, and we recognize it's Bush, is 123 over 127, which resolves to about .97, 97%. Where as the precision for George W. Bush, that's this column over here. In my calculations 123 over 156, which is the sum of all the predictions of George W. Bush. And that resolves to about 0.79. Much lower. Whether this warrants a recall for George W. Bush, is an open question, but you can see the asymmetry. This specific learning algorithm is really good in recalling or finding George W. Bush, provided that it's actually Bush in the image. But it mistakes many, many other people. Most notably Colin Powell and Donald Rumsfeld for George W. Bush. So just to make life a little more complicated, let's recall that we talked about things like true positives. False positives and false negatives. Notice I left out true negatives because Kate and I couldn't quite agree what it would mean in this example. But we can generalize that these, these three words, these three expressions to a confusion matrix with more than two concepts positive and negative. We have seven different concepts, as in this example over here. Tell me for the concept, Tony Blair. What you think is the number of true positives And I would argue it's 26. It's the number of cases were Tony Blair was positively identified and it was truly Tony Blair. Let's do the same for false positive now. What is that number? The number here is 8, which is the sum of all the observations of Tony Blair that were actually false. That's what's called a false positive. We think it's positive, but it actually is not. The last one is now easy, the false negatives for Tony Blair, please. And finally it's 8 again, so it wasn't the best example to quiz you. Because the number of cases where I say negative in my machine learning algorithm, but it was nevertheless Tony Blair. So it's a false classification, all the ones in this horizontal row all the way up to 26. Let's figure with it for Donald Rumsfeld again. What are the correct numbers over here? And here is the correct answer. Obviously the number of true positives is 25. There's only two false positives. But there are a total of 11 false negatives for Donald Rumsfeld. So here's my final question which will be challenging. I'm going to redefine precision as a quotient where up here we have the number of true positives. And down here we have the addition of Which ones on the left side go into the denominator on the right side? Think carefully, and click on the corresponding items on the left side that go right over here And this one's tricky. Let's take Hugo Chavez again, where we knew that the precision was 10, number of true positives, divided by all the numbers in this vertical column over here, all zeros. We knew the precision was one. And the numbers in the vertical column, except for 10, recall those false positives. So the correct answer here, is we take the true positives and the false positives, add them up, and those become the, and, and those become the denominator of this equation over here. So, it's going to be the true positives, plus the false positives that fall into the denominator over here And we can do the same for recall, again, it's a quotient where the numerator is true positives, and give me the two things on the left that fit over here into the denominator. And the answer is the true positives plus the false negatives. Just consider Hugo Chavez, again. We said that the recall was ten divided by 16, because there's six false negatives on the left. So it's the number of true positives, plus the false negatives that defines recall. Try to memorize those formulas, they're very useful in talking about machine-learning algorithm. Congratulations. You've made it to the end of the, the content of what we have to teach you. Now you know about everything from the beginning, the data sets,to evaluation at the end. And I have to say I'm a big fan of you that you made it this far. It's actually quite a bit of work. It was work for us too I should say, but it's great to still have you in class. And I hope you're going to enjoy the final little bit, which is a little more hands on work. A little more hands on work in the projects, and then a summary. And a summary of the big picture ideas, and then we can let you go hopefully with a certificate that you can use for wonderful things like getting a job In the last lesson, you created your first person of interest identifier algorithm, and you set up a training and testing framework so that you could assess the accuracy of that model. Now that you know much more about evaluation metrics, we're going to have you deploy those evaluation metrics on the framework that you've set up in the last lesson. And so, by the time you get to the final project, the main thing that you'll have to think about will be the features that you want to use, the algorithm that you want to use, and any parameter tunes. You'll already have the training and testing, and the evaluation matrix all set up. So Katie, you look a little bit fatigued at this point. We've done an awful lot. Yeah. How many lessons? About to start the 15th, so. Oh, my god. But I wouldn't say that there's 15 totally separate topics. I'd say that there's maybe more like four main topics. Okay, what are they? The first one is the data set and question. If you want to use machine learning, can you define a question and find the data could help you answer it? I guess the second one is feature selection, finding the right feature spaces for your machine learning algorithm to apply? Once you have the features, do you have an algorithm that you can use, and the parameter chains you want for that algorithm? And finally the fourth step is validation. Make sure what you've got is actually trustworthy. And of course, these steps, you won't just hit each of them once. But you'll cycle through them repeatedly as you start to figure out a little bit more about each individual question, you learn more, and you do a little bit better. So in this last and final unit, in this last and final unit, you put the big picture together. We're going to go up to a higher altitude and learn big picture composition of machine learning, so you actually apply these techniques in practice. So let's head up to that higher level. In summary, we had four different major blocks of techniques. You start with the data set or the question that inspires you. You try to extract the information from it, in the form of, of features. You feed those features into a machine learning algorithm, this is the heart of machine learning. And then you evaluate that entire process to see how good you've done. One of the hardest things is actually coming with the data set or the question that interests you. So one example that I loved was the Enron data set. For you, this will be something different. We spent a lot of time in class on features, how to present features, how to get rid of features, how to find new feature spaces that actually work better, and you can apply these techniques to pretty much any data set to get really reasonable feature sets. The heart and soul of this class were the algorithms which is where the most fun is in machine learning. You know that they are supervised if you have labels, unsupervised if you don't have labels. There are actually more classes in machine learning that you can talk about, but for the sake of this class, this is what we taught you. And really important is each algorithm needs to be tuned. We talked about quite a bit how to tune your algorithms. Last but not least will be the evaluation. How well are you doing? There's different types of evaluation metrics that you can use. Depends on, depending on what's most important to you. And at this point, once you've worked your way through from beginning to end, the evaluation metric will tell you if you want to go back and revisit any of these steps. Or if you're happy with your results and you're ready to move on. And the cool thing is this is the entire class on one slide. Isn't that amazing? Congratulations, you've made it to the end of the content this is the finish line. almost. There's one thing missing, the project, right? The project, where you're going to be building the person of interest identifier on the Enron data set, you still have that to look forward to. It's real world data, you can apply a lot of the stuff you've learned, and if you get stuck, we have our coaches don't hesitate to contact our people. They're going to help you and give you feedback as you do your project. So good luck. And we hope that this has been as good as a class for you to learn as it has been for us to teach. Guys, that's it. It's the sad moment that where we're reaching the end of the class. It's been a pleasure for us to teach this class for you. I hope you had as good of a time learning as we've had teaching. And we're really excited to see you again either working here in Silicon Valley as a machine learner or the awesome projects that you're going to go and do. And hopefully, you post them online and we can see them there. And it's, every time we reach this point in class, I'm really sad because it's been such a great experience to have you in class. Fun too, I have, see, Katie do most of the work. But, but, but you brought something, right? I did. I did. I was inspired by our conversation during the intro video about winemaking. So after that, here we go, I actually made my own wine. Your own wine? My own wine. This is the first bottle of it that we've opened. We'll see how good it is. I'm sure it's not as good as the stuff I'll eventually be making but, you know, there's a first time for everything, so shall we? Cheers. It's pretty good. Surprisingly drinkable actually. Wow. So it's been a pleasure teaching you and we hope to see you again soon. And don't forget, there's many more courses on Udacity of course, but there's also really great jobs here in Silicon Valley. Machine learning people are in very short demand, and Udacity's standing by to help you develop your career. So we'll see you again soon. So this is the unit on support vector machines, or SVMs. Support vector machines are a really popular algorithm. They have lots and lots of uses. And it's actually so young that I was personally around when Vladimir Vapnik invented the, the support vector machine. It was a big surprise to the community that someone could find an algorithm that is so phenomenally good. It's really a great algorithm. So let's get started. So, let's talk about an algorithm called SVM, which is short for Support Vector Machines. Why someone would call an algorithm a machine, I have no clue, it was invented by a Russian, but it's a very popular algorithm. At first approximation, what Support Vector Machines do is find a separating line, or more generally called a hyperplane, between data of two classes. So, suppose we have some data of two different classes. Support Vector Machine is an algorithm that takes this data as an input, and outputs a line that separates those classes, if possible. Let me you give you three lines. In each line is a little check mark. Which line do you think separates the data? And obviously the line over here is the one that separates on one side the x's on the other side the o's So, that was easy. Let me do this again. Here's a new data set. And, I give you three lines again. This line over here, this one over here, and this one over here. Check the line that you think best separates the data. Obviously, all three of them seem to separate the data, but one is the best And I would say this line is the best separator. Why is this so? What this line does that the other ones don't do, it maximizes the distance to the nearest point, and it does this relative to both classes. It's a line that maximizes the distance to the nearest points in either class, that distance is often called margin. If you go to a machine learning party, and everybody talks about support vector machines. It's really good to know the word margin because it is the thing that is being maximized. The margin is the distance between the line and the nearest point of either of the two classes. Is this because it's more simple? It's in line with an axis. Did I just pick the line at random or is something else going on over here? Is there some criterion that would make you pick this line, over the other two lines that I gave you And I would say something else is going on here. What this line does that the other ones don't do, it maximizes the distance to the nearest point. This is relative to both classes, this line maximizes the distance to the nearest point in either class. That distance is often called margin. If you go to a machine learning party and everybody talks about support vector engines, it's really good to know the word margin because it is the thing that is being maximized. The margin is the distance between a line and the nearest point of either of the two classes. And just to make sure you got this, let me give you three linear separators for that same dataset. Which one do you think maximizes the margin? And just before it's the middle one. There's a really large margin to the blue data set, but a small margin to the x over here. So it doesn't really maximize the margin to both data sets, just that's the one. And the same is true for the line on the right side. So the reason why we pick the one in between is it seems to be most robust to classification errors. If you pick a line that's really close to existing data, then a small amount of noise would make the label over here flip and it feels not very robust. So the inside of support vector machine is to maximize robustness of your result. [SOUND] And that makes them really great, so let's practice this Suppose this over here is your data. Which of the two lines do you think maximize the margin and would be the correct result of a support vector machine? Warning, this question is tricky. It's either this line over here or this line over here. Now I tricked you, it's actually this line over here and you might have gotten this right. Now this does maximize the margin, in some sense to all the data points. But it makes a classification error. The red x is on the wrong side of the green line. Whereas in this case over here, all the points are classified correctly. Support vector machine puts first and foremost the correct classification of the labels, and then maximize the margin. If we didn't care about correct classification, this line back here has an even larger margin to all the training examples. That's obviously the wrong line. So for support vector machines, you are trying to classify correctly, and subject to that constraint, you maximize the margin. So sometimes for support vector machines, it seems impossible to do the right job. For example, you might have a data set just like this, in which clearly no decision surface exists that would separate the two classes. You can think of the point down here as an outlier. So my question for you is, what do you want support vector machines to do? Should they give up if there's no decision surface to be found, should they give you a random result, or should they do the best they can? Well, it's not clear what best means, but try to find the decision surface that best describes the data despite the fact you can't linearly separate them. And, you know, the truth telling, we don't want it to give up because then most of the time it would just give up, we didn't know what to do. Something random is even worse. The best you can might be to make a decision boundary just like this and just denote the effected one point on the other side. And, in fact, support vector machines are good enough to really do that. They often find the decision boundary that maximizes the clearance to both data sets. And tolerate at the same time individual outliers, as shown over here So, as you dive into the code of SVMs, it is a piece of software that truly ignores outliers. So, just to practice this, in this example over here of red data and blue data, where there seems to be exactly an outlier, say you're willing to ignore an outlier. Which separated line would you pick? Just choose one of the three And I would argue it's this one over here. because this one truly maximizes the margin to those points, ignoring the outlier. Whereas the one over here has a smaller margin to those points. So this would've been the correct answer over here. But the thing to learn is that SVM is actually somewhat robust to outliers. And it somehow mediates the attempt to find the maximum margin separator and the ability to ignore outliers. Obviously as a tradeoff, and the parameters of the SVM as we'll see, determine how willing it is to achieve new outliers. So congratulations that's a good start and for the magical world of support vector machines and Katie will now take over in making your code here own, very own SVM. So, as a reminder, this is what our training data looked like. It's the same terrain data that we had in the last one. And we're trying to figure out whether you can go fast or you have to go slow on this terrain. So with that, I'm going to head back over to Google. I know that I want to use scikit-learn and support vector machine. All right, so we've lots of options here. Let's just jump right in. We'll click on the first one. Okay, so support vector machines, scrolling through a little bit, the first thing that I see is there's some advantages and disadvantages to support vector machines. So, that's pretty interesting. Those might be points that I'll, we'll revisit a little bit later in the class, but I am impatient and I want to start coding. So, the first thing it gives me, classification. Fantastic, and I can see that there is already example code that's ready to go. So, let's find this example code. okay, this is starting to look a little bit familiar now. So, I have my import statement, I have my training features, and I have my training labels, like I did in a last example with naive phase. I create my classifier. I fit it using my, my training features and my training labels, and then down here, they have us they show us how to do a prediction. So as you can see, it's actually exactly the same few lines of code. Only instead of now saying Gaussian Naive Bayes or whatever, I'm calling it an SVC. And this is one of the things that's really nice about scikit-learn, is that basically all the classifiers that we'll be looking at and all the classifiers that you might use sort of on your own in the future will follow this same pattern, where you import it, you create it, you fit it, and you make predictions. And it's always going to be the same syntax. Now I can draw the decision boundary, this time in scikit-learn, and voila. This is what it looks like. So, you can see immediately, where before in Naive Bayes, we had something that was a little bit, little bit curvy maybe. Our SVM decision boundary is now perfectly straight. And of course, this is the line that's going to give us the maximum margin between the decision boundary and any misclassified points, so the points on either side. So, just to make that a little bit more illustrated on this particular example, that's, that means that it's going to have, it's going to maximize all of these distances here. The points that are nearby the decision boundary, it's going to try and find a decision boundary where all of these little orange lines are as big as we can make them. That means that there's as much separation between the two classes as possible. Okay, so I bet you know what I'm going to ask you now. I'm going to ask you to code it up yourself. If you guessed that, you'd be correct. so, let me show you what I did, and then you're going to be able to make a SVM, just like I did. So, what I would suggest, is you head back to Google, just like I did, find this page on the Support Vector Machines' documentation from scikit-learn. Remember, that if you scroll down just a little bit, you'll find this Classification section. And, down here, is the example code that I used to get to get my code running. Now, there's one little hiccup, one little catch. In exactly how we created the classifier, right here. So, what I'm going to do is actually get you started with this line and this line. And, you'll see that there's a little difference in the way that I wrote my code. We're going to be talking about this a lot in the next few videos. But now it's your job to fit, and make predictions using your classifier. And then, the last thing that I want you to do is to print out the accuracy of your classifier, the way that we did in Naive Bayes. remember, Naive Bayes the number to beat was about 88.4%. So, there's, there's an interesting question. Is an SVM, or Naive Bayes doing a better job of classifying your terrain in this example? Welcome back. I hope you were able to get your support vector classifier working, but in case you need a little nudge, let me show you what I did. So, starting from the starter code, there's only actually two lines of code that I need to add. The first one is that I have to fit my classifier, and I have to fit it on the training table. The first one is that I have to fit my classifier, and I have to fit it on the training data, so that means passing features_train, and labels_train to my fit function. Then, the second thing is making a list of predictions. So, I store to a list named pred, and that's just the familiar predict function that we've seen with the Naive Bayes classifier. Remember, when you're making predictions, you're only going to be passing one set of information, now, the features. Because the labels are what you're trying to predict. And also, of course, you want to do it on the test dataset, not on the training dataset. So, with two lines like this you should be able to get your SVM up and running. Okay, now I want to show you something really crazy, that's going to make it immediately clear how cool SVMs can be. So, what I'm showing you here are two different SVMs, this one you just made, and this is one that I just made. You don't know how yet, but we're going to spend a lot of time talking about this. Look at how intricate this decision boundary is, you have little pockets and you have little, little peninsulas that reach out. You can see an SVM can do some really complicated shapes to the decision boundary, sometimes even more complicated then you want. So, let's go talk to Sebastian again. And he has some pretty interesting insights as to how this is coming out of a classifier. Remember, the SVM is a classifier that's built on giving us linear separation, so how on earth can it give us this incredibly non-linear decision boundary? Well, that's something Sebastian's going to explain to you now. So hey, that was great. I hope you guys had fun programming all this. Let's get back to some more theory. I'm going to give you a very tricky data set. There's a couple of pluses and I've clustered around the origin of this coordinate system, x and y. And there's a whole bunch of o's that lie more in the periphery. And give what you've learned so far. Do you think support vector machines give you a good answer here? Will support vector machines work? YES or NO? And based on what you've learned so far I would say no is the correct answer. There is just no good linear hyperplane or linear separator between these two classes. Or put differently, it's really hard to draw a line between these two classes. But in reality, the answer is yes. But now we're going to learn a trick that would change your answer from no to yes. And that trick relates to features. So far we assumed that we input our data features, X and Y, into this magic beck post call support vector machine and out comes a label. Where label's either a blue circle or a red x. So we're going to pick a feature that is helpful for my example here. It's going to be x square plus y square. Why that? You'll see in a minute. But I can convince you that it's easy to compute x square plus y square from x and y. So, now I have a three-dimensional input space. It's hard to draw over here. And I can run the exact same algorithm on these three features that I previously ran on the two features. And here's my $100,000 question. Do support vector machines work by now? Or put differently, is this new situation linearly separable? By which I mean, can we find the line or a hyperplane that separates the axis from the circles if this is now a official space? It's going to be a trick question. It's a hundred thousand dollar questions which means if you get it right I won't give you a hundred thousand dollars. But give it a try. And magically, the answer is yes. And it takes a little bit of time to understand why. If our support vector machine learns the class x square plus y square. Let's call this new feature over here z. So, we know that z equals x square plus y square. And we're trying to learn a hyperplane between x, y and z. z is always nonnegative because of the distance to the origin. And magically, for all the red points, the distance to the origin is small. And for all the blue circles, distance to the origin is large. So we show the exact same data in this new projection of x and z, omitting y. I find that all the x's have a small value of z. Whereas all the circles have a larger value of z. And just to understand this. To really get this. z measures the distance to the origin. So here's a small z, here's a small z, here's a small z, here's a small z. And for the blue circles, the distance is large, as you can see over here. So if I remap the data into this new coordinate system with the z, you find all the blue circles have a large value for z and all the red x's have a small value for z. So, you look at the data in this new coordinate system, with x and z. Is this data over here linearly separable? Yes, or no? And, I would say the answer is yes. Here is the line. And, what's really interesting, this line over here, in the original data, corresponds to a circle. The circle of equal distance to the origin in the coordinates system. So, as we added this new feature, and I picked one that I really wanted to pick here. As we added this new feature, we're able to use support vector machine to learn a nonlinear decision service in the form of a circle. That is quite amazing. So let's practice this a little bit. So here's a data set that is clearly not linear asymptote. So you won't be able to find a line that separates the x's from the circles, within the xy coordinate system. But now I'm going to ask you to add one more feature. And that feature should make it linear asymptote. And I'll give you some. Possibilities x square plus y square, as before, the absolute value of x or the absolute value of y. Obviously, the absolute value just takes out the minus sign. Take one of those. I can tell you one of those actually makes linear separable. And it's a trick question, it's hard. And the answer is the absolute value of x, and I can prove it to you. If you were to show the absolute value of x, then it can't be anything negative. And all the points here get moved from the negative side to the positive side. So, this point in particular over here becomes this point here. Gone are these. This point over here becomes this point over here. So this is how it looks like if you look near all the points to the absolute value of X corner system. You just flip them over. And if you look at this diagram, what you find is that there is now a beautiful, perfect linear separation. And if you ask yourself, how did the linear separation look in the original space of x and y is effectively something like that. It is non-linear because it has this inflection point over here. That's non-trivial to see, but as you look at this, I hope you get that adding this one new non-linear feature makes support vector machine linear separate the Xs from the circles. So Sebastian, this sounds really useful but really difficult. Are you telling me I have to write a bunch of new features? The cool thing about support vector machines is that you don't. There's something called the kernel trick. So are you going to explain the kernel trick to us? I bet you are. No won't really, I'll just give you the gist of it. There are functions that take a low dimensional input space or feature space, and map it to a very high dimensional space. So that what's used to be not linear separable and turn this into a separable problem. And without going into any detail, these functions are called kernels. And I encourage you to check out in this, these aren't just functions with a feature space, these are functions over two inputs. And when you apply the kernel trick to change your input space from x,y to a much larger input space, separate the data point using support vector machines, and then take the solution and go back to the original space. You now have a non linear separation. And this is the really cool thing about support vector machines. You can take a very simple idea of finding the best linear separator, or linear line between different classes, apply what's called the kernel trick in a higher dimensional space, and what you effectively get is a very, very powerful system to set datasets apart where the division line might be nonlinear. That is really cool, and one of the most central tricks in all of machine learning Okay, so now that we know all about kernels, let's go back to the SK Learn documentation and see how kernels are supported in the actual code. See if there's different kernels that we could give a try and see how those affect the decision boundary that our SVM gives us. So okay I'm back at the page where we were a few videos ago. Now I'm looking at these, at these, some of these points a little bit more carefully and one of the things that I, I notice is that there are different kernel functions that can be specified for the decision function. So there's common kernels that I can use straight out of the box. great! And there's also the, the option for custom kernels if I want to get really fancy. We'll be just using the common kernels that you can just use out of the box. But that might be really interesting things to play around with is to write your own kernel. So there's several different SVMs within scikit-learn that you could use. The one that we're using is SVC, the support vector classifier. Okay, and that takes me to another documentation page of the support vector classifier and I have this box at the top. This might be a little bit reminding you of what we were seeing in the naive bayes documentation earlier. So we have this whole complicated thing here that tells me how I can create my SVC. And I notice that there's all sorts of arguments, all sorts of parameters that I can pass when I'm creating it. One of these happens to be the kernel. Now let me take you back to the code that we wrote. You might recognize this from our last quiz. And one of the lines of code that I gave you, that I said we would explain later, was this line right here where I'm creating the classifier. So this line of code would work if these parentheses were just empty inside, but they're not. What I've done is I've specified a kernel. So the first example where we had that straight line decision boundary, this is using a linear kernel. Makes sense. So now I have a quiz for you. This is a little bit of a tough quiz because I haven't shown you exactly how to answer this question yet, but I've given you a big hint. The answer is on this page right here of the documentation. And the question for you is which of the following are SVC kernels that we can use? Your options are linear, poly, rbf, sigmoid, or all of these options plus more. So which of these kernels are available to you when you're using the SK Learn SVC? And the correct answer is all the above and even more. And the place that I found that was right here. So I read down the documentation page a little bit and i see that the kernel is one of the parameters. And then I read this little statement that says it can be linear, poly, rbf, sigmoid, precomputed, or a callable. So, of course, all of these are correct answers. The one that gave us the straight line in the first example was the linear kernel. And in general, that's what linear kernels are going to do. They're going to draw a straight line. The one that gave us this crazy squiggly decision boundary, that was using an rbf kernel. All right, so let's take a moment and just explicitly talk about parameters in machine learning. And parameters are arguments that you pass when you create your classifier. So this before fitting, and they can make a huge difference in the decision boundary that your algorithm arrives at. So what are some of the parameters for an SVM? Well, we've talked about one already, which is the kernel, and there are two more that we're going to talk about right now, which are C and gamma. And that was the difference between these two SVMs that I showed you just a minute ago, is they had different kernels, so the one on the left was a linear kernel and the one on the right was an rbf kernel. And as it happened, I was also monkeying around with gamma here. If you don't monkey around with gamma, they'll actually look much more similar than this. So the gamma in this case happened to be a 1,000. So here's a quiz for you, a reason for you to try some stuff on your own. Which of the following SVMs do you get when you use the parameters, kernel is a linear kernel and the gamma is set to 1.0? So remember before, it was 1000, so now we're bringing down our gamma by three orders of magnitude. Let me give you a few options. Which one do you think is the linear kernel with a gamma of 1.0? So, I hope you said this one. So, there's a, there's a few ways that you could've answered this. You could've just coded it up with a linear kernel and a gamma of 1.0. You would've found something that looks like this. Another thing you could've done is you could've looked at these two decision boundaries and said, well these are both wiggly. And, I know that a linear kernel is only going to give me a linear decision boundary. It's always going to look like that The other important parameter for an SVM is the C parameter. So what does C do? It controls the tradeoff between a smooth decision boundary and one that classifies all the training points correctly. So let's suppose our data looks like this. And just like we saw with gamma, there's a number of different, completely valid decision boundaries that you can draw in here. You could draw something that might be very straight, but it comes at the cost of a few points being misclassified. You could also draw in something that's considerably more wiggly, but where you get potentially all of the training points correct. Of course, the tradeoff of having something that's very intricate, very complicated like this. This is that chances are it's not going to generalize quite as well to your test set. So something that's a little straighter, a little bit more straightforward may be actually the better choice once you start looking at the accuracy on your test set. So here's a quiz for you. It's a little bit tricky because you'll have to go to the SK learn documentation or potentially to play around with some code on your own to come up with the answer. And the question is let's say you have a large value of C. Does that mean you're going to get a smooth boundary or that you'll get more training points correct? There's going to be a tradeoff. Again to answer this quiz you're going to want to go to Google or potentially to the SK learn documentation. For those of you who are more adventurous or who like to go directly to the code to figure these things out, you can certainly play around with some code that will help you visualize this. One way you can do that is you can go back to the previous lesson, the naive Bayes lesson where you were drawing some decision boundaries that you could actually visualize in the Udacity interpreter. And instead of running those with naive Bayes, of course you can just run them with an SVM and play around with different values of C and use that to answer the question. If you do that, just a pro tip, use the rbf kernel, because this is where you'll actually see a big difference based on the value of C that you use. What you'll find when you start to poke around, either at the code or at the documentation, is that a large value of C means that you're going to get more training points correct. So what that means in practice is that you get the more intricate decision boundaries with the larger values of C where it can wiggle around individual data points to try to get everything correct. But at the cost of something that, that could be a little bit more complex than you liked it. Figuring out exactly how much you want a straight decision. Figuring out how much you want to have a smooth decision boundary versus one that gets things correct is of course part of the artistry of machine learning. So, we just saw an example where the decision boundary was really, really complicated. So, we have a situation like this. We call this overfitting and it's a common phenomena in machine learning that you have to be always aware of every time we do machine learning. To give an example, if you look at the data over here, it seems to be linearly separable. A decision surface between red and blue, it looks like this. Correctly classifies the red data, but looks erratic at many other places. That happens when you take your data too literal, and when your machine learning algorithm produces something as complex this, as opposed to something as simple as this you are over-fitting. So, in machine learning we really want to avoid over-fitting. One of the ways that you can control over-fitting is through the parameter of your algorithm. So, in SVM we have several parameters we've talked about. C, gamma and the kernel come to mind. So, which of these do you think affect the over-fitting potential of an SVM? Click as many as you think might be correct. And, the answer is? The answer is, all of the above. Any of these in different types of combinations, can make you more or less prone to over fitting. And, this is a lot of the artistry of machine learning is tuning these parameters. So, you're not over fitting your data. But, we will learn techniques for which we can automatically detect overfitting, where you have a test set. Oh, absolutely. Absolutely. Okay, at least for now, play with those parameters and see what happens, and see how the output varies depending on how we set those parameters So, we've reached the end of support vector machines, and I think we learned quite a bit here. That's right. At the end of Naive Bayes we started a list of all the different things that we should be thinking about when picking an algorithm. We've learned a lot about support vector machines that now we can add on to that list. Yes. So they work really well in complicated domains where there is a clear margin of separation. And they don't perform so well in very large data sets, because the training time happens to be cubic in the size of the data set. They also don't work well with lots and lots of noise. So when the class are very overlapping you have to count independent evidence. That's where then a Naive Bayes classifier would be better. So again, it comes back to the data set that you have and the features that you have available. If you have a really big data set, if you have lots and lots of features, SVMs right of the box might be very slow. And they might be prone to overfitting to some of the noise in your data. Okay. Yes, so, just test it out on the testing set. See how it how it performs. And then you're ready to go. Exactly. So, I think we're ready to go on to the next lesson? Awesome. Great. Welcome to the mini project for support and vector machines. We'll be tackling the same topic that we did for naive Bayes which is, can you identify the author of an email based on the words that are used in the text of that email. In the second part of the project we'll also dig into the parameters of the SVM a little bit more. You had some practice gamma and C. And now we're going to go in and play around with those parameters and see how it speeds up the, the training time or the prediction time of the SVM and how it affects the accuracy of the SVM as well. So let's get started. So, welcome to our third, and final, supervised classification algorithm. On? Decision trees. Decision trees are one of the oldest and most used elements in machinery in general. They go back for decades, and they're extremely robust. They have really interesting looking decision boundaries too, that you'll see as soon we start coding things up. And, they're very intuitive, so you can look at the result and understand it. It's a great algorithm and we're really excited to get started. So one method in machine learning that's very, very popular is called decision trees. And just as in support with the machines, you were able to use a kernel trick, to change from linear to non-linear decision surfaces. Decision trees use a trick to let you do non-linear decision making with simple, linear decision surfaces. So let's start with an example. I have a friend named Tom, and Tom loves to windsurf. But to enjoy wind surfing, he needs two things. He needs wind, and he also needs the sun because he doesn't really like surfing with overcast or in rain. So, we take all the days of last year and make them data points. He doesn't surf when the weather isn't very sunny. And he doesn't surf when there's not enough wind. But when these conditions are met as windy, and it's sunny, he enjoys windsurfing on the lake. My very first question, is this data linearly separable? Yes or no? It's obviously not linearly separate, you won't find the line in this space to tell these datas apart. Decision trees allow you to ask multiple linear questions, one after another. So, in essence what you could do is you could first ask the question is it windy? Yes or no. And by windy we define some sort of a threshold in this access over here. Let's take this one over here because it's kind of looks the right one. You can answer with yes or no. And depending on what the answer is, you will either go on this branch in the tree or this branch in the tree. And what this effectively does is it puts a linear decision surface right here to the diagram. Now how can you construct this? In one of the two outcomes, you already know the final class, which one would you know be done with, and you have the right answer. And the answer is no. If it's not windy, then all the remaining points, they go on this branch over here, our negative. So you could say for no the answer is a big red x. In the case of yes however, we have to ask another question, and this question is whether it's sunny. There's two outcomes as before yes and no. Course I'm cleverly select the threshold that I'd say is right over here. And if you answer this with yes we asked if you answer it with no. You get the red x. So this thing over here is called the decision tree. And what decision tree learning does in machine learning is it takes data like that, and finds something like this, that can help you classify future data using this very simple data structure called the tree. So, let's practice this using a slightly more complicated example. Suppose, we have two coordinates, x and y, and here is our data. So, at the root of a tree, I am going to first ask a question about x, and I want x to be either smaller or larger than some threshold. >From these numbers over here, one. two. three. four. five, which do you think is the best threshold to use in this data set? Take a look, and give me the the best threshold for an x related question between one and five. . So but, definitely, we are going to build a whole tree, what is the most useful x language you'd like to split the data? And I would say it's three, and three is kind of tricky because as we split the data over here we see a distinct different phenomena on the left set of the data from the right side of the data. In fact, once we split the data into the left half and the right half they become fairly inseparable, using axis parallel decision lines. These are lines that run parallel to the axis only look at one variable at a time. So, let's say there's two outcomes. Yes and no. For the left side, I'm now going to ask a Y question. Is Y smallest in value, and I give you again five choices. One, two, three, four, five. Which one would you pick over here as the decision boundary? Well, this one's tricky. X smaller than 3, yes means you're on the left side. And if you're on the left side, I would say 2 is a great decision boundary over here. So, I'm going to go put in 2 over here as a test. And again, we have two outcomes yes or no. And for the yes outcome, which class would you assign, the blue circle or the red cross? Just for the yes outcome. And, I would say it's the red cross, because for y value smaller than two on the left side, we have red crosses. When larger than two, we have blue circles Let's look at the right side. Again I'm going to ask a why question, and what's the best here? One, two, three, four, of five? I would argue it's roughly 4 in my very deficient drawing here, because it separates the blue circles and the red x's. And again, when I ask the question, is y smaller than 4, there are 2 outcomes, yes and no. For the yes branch, which one do you pick, the red x or the blue circle? And you picked wisely, it's actually the red x. And that means for the no outcome, where the value is larger or equal to 4, we get blue circles. So this thing here is the decision tree or one of the main decision trees that characterize this data. The algorithm decision tree learning, is to use a computer algorithm to find the decision boundaries automatically based on data, and Katie's going to tell you all about that That's right, I am. Thanks for that great introduction. In this video, I'm going to show you how we can use SK Learn to do the heavy lifting in terms of creating and training our decision tree. We'll start at the documentation page from SK Learn for decision trees. You can find it using Google. When I arrive at the landing page for decision trees, I see that I can use them as classifiers. They also can be used for regression, which something we'll cover later. Scrolling down to find a little bit more information about their use as classifiers, I find that there's something called DecisionTreeClassifier, which sound like what I want. Even better, I find a few lines of starter code that I can use to get going. Looking at this starter code a little bit more closely, I see that it's very similar to what we had with naive Bayes and the support vector machines. The first thing that I have is an import statement. The second important line of code is going to be creating the classifier. The third important line is going to be fitting the classifier using in this case some training data. And then, the fourth important line will be using that fitted classifier to make a prediction about a new point, in general a point from the test set. So, this should all start to look very familiar to you now. So, with that little crash course I'm going to throw you into the deep end of the pool now and have you actually code up a decision tree yourself, so that you can see what it looks like. Then, in the following videos, we'll talk about it a little bit more. Here's some starter code that I'll provide for you in a quiz. We have our features and our labels for both the training and the testing set. And, what I want you to do is to create a decision tree classifier that you train using the training features and the training labels. It should be three lines of code to get you to the point where you're fitting the decision tree, and four lines of code if you decide to make some predictions using the test set. I've given you a couple lines of code here that will visualize the decision boundary for you so you can see right away what it looks like. So, go ahead and give that a try and I'll see you in the next video to show you what I got So I hope you had some success with your decision tree in Now let me show you the decision tree decision boundary that I got when I tried this problem. So here's what I got. And you can immediately see that the decision tree decision boundary is very distinctive. All these jagged little lines. It almost looks like, you know, a piece of, of modern art or something. It's out all these horizontal and vertical cuts that it makes. Because of the structure of the decision tree where it's running around trying to slice the boundary in such a way that it can get as many points as it can in the training set. You can see that it can also have little, little islands, just like the FBN did. And in general, this is the, the shape that's going to characterize a decision tree or a decision tree boundary. One of the things you might be noticing is that there's some signs of overfitting, perhaps in this decision tree that we have these long, these long slices. Like this one here and this one here. Where the decision tree seems to be kind of going out of its way in an unnatural way. And I bet that if you plotted the training points instead of the test points on top of this decision boundary you'd find that there's a few blue training points that are maybe out along here, that are causing the decision boundary to come way out and try to pick those up. And likewise, for maybe some red points over here. And now, our next quiz, something we always want to ask of our classifiers. What is the accuracy? So, in the prompt, write a couple lines of code that will return the accuracy so can compare it to the other classifiers that we've looked at Okay. So the accuracy that you should have gotten was 90.8%, when you run the decision tree out of the box. But you can also see that there might be some problems with overfitting in this particular tree. Maybe there are some parameters in here that we could be tuning that we could try to bump this number up a little bit more. So let's jump back into the documentation and try to figure out what parameters we can tune so that we can get this number a little bit better. To understand what parameters I have available to me, going back to the SK learn documentation. So, this should look kind of familiar. We found this through Google, remember? I am going to scroll down a little bit until I find the decision tree classifier, this being documented here and I am just going to click on this link. And, this takes me to the specific page that's about the DecisionTreeClassifier. And this, this box right here is what I'm looking for. This is going to tell me all of the parameters that I have available to me when I'm creating my classifier. So, I can see that there's a whole lot that's going on here that I can tune. There's something called the criterion, there's the splitter, there's max depth. There's minimum number of samples in the split, minimum samples in the leaf, and so on and so on. So, these are all things that I can try tuning. So, let me pick one of these. We're going to look at the men's sample split. We can see if by training the min_sample_split. Maybe, we can alleviate some of that over-fitting that we were seeing in the example that I just showed you. So, here's what min_sample_split does. Say, I have my decision tree. Start out with a bunch of training examples, then I start to split them into smaller sub-samples. At some point, I have to figure out if I'm going to keep splitting any further. So, for example, if I start out with 100 examples here, then maybe I get 60 and 40. And maybe, I get 40 and 20. And then, maybe, this 20 turns into 15. And, five and so on and so on. So, the question is for each one of these bottom most sort of layers in the tree whether I want to keep splitting it. If it seems like that might be a good idea. And, that's what min-samples-split governs is basically whether I can keep splitting or not. Whether there's enough samples that are available to me to continue to split further. So, the the default number for min-sample-split is two. So, all of these nodes I can further split except for one of them. So my question for you is which of these nodes here are a few of your choices would I not be allowed to split any further? And the answer of course, is this one because I only have one sample in this node. Any of these other guys I can continue to split because they all have more than two samples in them. So I can keep drawing my tree if I liked. I can continue to keep going. Maybe to quite some depth. Until I get a very complicated decision boundary. So you can see how once it starts to get really deep, really complex. That's where you start to get some of these decision boundaries that do all these complicated things. And probably end up overfitting your data. Now let's see what this does in code. So here, I have two decision trees. One of them is quite complex with lots of intricate little pieces of the decision boundary. The other is much simpler. It only has maybe two or three major cuts. And one of these I made with a min_samples_split equals 2, the default value, and one of them I set it to 50. And the question for you is, which one is which? So, write in the text box either your intuition, or you can code it up and actually see for yourself, which one you think has which parameter setting. And, the answer that I got is that this more complicated one has two samples required for splitting. So basically, it can split until you have a leaf size that's, that's only one because if you have two samples, the branch and then it allows it to split, then you can have one and one. So, we can get very, very finely tuned. You might only have one point out here. And, it's drawing this big peninsula, a big, narrow peninsula all the way out just to grab that one point. And then, this one, of course, is going to be a lot simpler. You have to have at least 50 samples in a branch. Otherwise, you won't be allowed to fit it. So, you come up with something that's much simpler And the next question is, what is the accuracy for each one? So, in order to answer this, you definitely have to code it up, so give that a try in the quiz. As it turns out, when you compute the accuracy you can actually see that, that the simpler decision boundary gives you a better result. You have a 91.2% accuracy with the min sample split set to 50. And then when you allow it to go all the way down to two, you only get 90.8% accuracy. So, you can see that it does make a difference, not just in terms of how the decision boundary looks, but actually in terms of the performance of your classifier. And, this might not look like a huge difference. But, that's something that we got for free just by tuning a parameter. And, if you recall, there were a lot of parameters that we had to play around with in decision trees. So, I would encourage you right now or at some point in the future to sort of go in and play with, around with a lot of those. Get some intuition for what they're controlling and how they actually change the shape of the decision boundary. Because in the future, those are going to be the parameters that you're going to be tuning to get the best performance out of your algorithm Now were going to talk about something that's very important for decision trees, and that is entropy. Entropy is basically the thing that controls how a decision tree decides where to split the data. So, entropy is this very powerful thing. What's the definition? It's a measure of impurity in a bunch of examples. Let me show you what I mean. Sebastian self-driving car data, we have the grade and the bumpiness of the train, like we did before. But, let's suppose that there's a new variable, a new feature now that we're going to take into account, and that is a speed limit. Remember, the question was whether the car can drive fast or slow. So, if there's a speed limit, it shouldn't be driving very fast. So remember, before we had data that looked like this, where he had the bumpiness and the grade. Now, let's suppose we had the speed limit and the grade. And, let's suppose the speed limit worked like this. If the speed limit was in force, you're in this half of the data. And the, if the speed limit is not in force, you can go at whatever the speed you like, you're in this half of the data, the lower half of the scatter plot here. So then, when the speed limit is in effect, doesn't matter what grade it is, everything's going to be slow. Whereas, when the speed limit is not in effect, you still have to go slow when it's too steep, but now you can still go fast if it's flat enough, if the train is good. All right, so let's recast this conversation in terms of purity. Say I have two splits that are available to me. I can split based on bumpiness or split based on the speed limit. And then let's say I'm only going to look at one of the halves. I'm going to look, compare this half to this half. Which one of these two sets of examples has more purity in terms of being all of one class label or the other? Check the button that has your answer. And, as you might've guessed, this is going to be the one that has more purity. You can see that everything in this set of examples is all going to be something that has to go slow. Whereas, over here, you get some contamination from a few fast points. And, this is an introduction to the idea of entropy, and the idea of impurity. What you're trying to do when you build a decision tree is, is you're trying to find variables and split points along those variables that's going to make subsets that are as pure as possible. And, by repeating that process recursively, that's how the decision tree actually makes its decisions Entropy has a mathematical formula. Let me give it to you now. The formula is negative Pi log2(Pi). Let's unpack this. Pi is the fraction of examples that are in a given class, class i. And then, you sum over all the classes that are available. In the next few videos, I'll give you lots of hands on experience with performing entropy calculations. But first, let me give you just a little bit of intuition. As you can see from this definition, entropy is basically the opposite of the purity. So, in one extreme you can have all the examples be of the same class. In that case, the entropy will be 0. In the other extreme, you can have the examples be evenly split between all the available classes. In that case, entropy will take on it's mathematically maximal value which is 1.0. Now let's do an example. So here's your reminder of the formula and suppose this is my data. I have four data points and for each data point I have three features, the grade of the train, the bumpiness, whether there's a speed limit, and then the speed that the car goes. And i'm just making up some data here. So let's start out at the top of the decision tree here. So I have two slow examples and two fast examples. Slow, slow, fast, fast. So the first question is, what's the entropy of this node? So let's do this piece by piece. How many of the examples in this node are slow? Write your answer here. The answer is two. Next question is, how many examples do I have total in this node? Write your answer in the box. And, the answer is four. I have one, two, three, four. Now here's the $100,000 question. Pi, in this case, is going to be the fraction of slow examples. And what is Pi equal to? Write your answer as a decimal in the box here. To get the answer you just divide the number of slow examples by the total number of examples, and you get 0.5. And now the next question is, what is Pfast? So now you do the calculation on your own this time. Calculate the fraction of examples in this node that are fast. You should get 0.5 again because we still have four examples. And now, two of them are fast. So 2 divided by 4 gives you 0.5 again And now, here is the big question. What is the entropy of this node? Let me walk you through this calculation because if you're like me log base two numbers aren't something that you can do in your head and it can be kind of hard to find a calculator that can handle them. I'm little let Python do the heavy lifting for me. So I go into Python and I import the math module. So I want negative PI times log base two of PI. So I do negative pslow is 0.5 times log 0.5. If I put this second are you in here the two that tells you the base of the log to use, so since I need log base two, I put a two there. If I wanted log base 10, I would put a 10 there, and so on. And then I need to add in a second term, so this takes care of pslow, I also have a contribution from pfast. Remember, I need to sum over all the classes that are available. So, pslow and pfast happen to be the same. So this is pretty easy, just type it again. And the entropy, as it turns out, is 1.0. So this is the maximally impure state, right. We have two class labels that are available to us. And I get an entropy of 1.0. So remember, the largest that we could get, the largest value for the entropy that we could get was 1.0. This means that, this is the maximally impure sample that we could have. If we have two class labels, the most impure situation we could have is where the examples are evenly split between the two class labels, so the entropy of the starting node is 1.0 While I'm sure you're really excited about entropy right now, let's take this back to decision trees and see how entropy actually affects how a decision tree draws its boundaries. That involves a new piece of vocabulary, information gain. Information gain is defined as the entropy of the parent minus the weighted average of the entropy of the children that would result if you split that parent. The decision tree algorithm will maximize information gain. So this is how it will choose which feature to make a split on. And in cases where the feature has many different values that it can take, this will help it figure out where to make that split. It is going to try to maximize the information gain. Let's do an example. And remember that we, when we started with all the data, the entropy of the parent was 1.0. Now let's use information gain to decide which variable to use when splitting. We'll start by doing an information gain calculation for the grade. So here's my parent node, with all four examples in it. Now I split based on the grade. So the first question for you is how many examples are going to go in the steep branch? Write your answer in the box. I hope you got three. One, two, three. And of those three, I see that this one and this one are slow. So two of them are slow. And the last one in the list there is fast. So this is the steep path of my tree. Then on the other side is my flat terrain. I only have one example of a flat terrain, and that's fast. Here's your next question, what is the entropy of this node? Write your answer in the box. You shouldn't need to do any calculation. The entropy of this node is 0 because all of the examples in this node belong to the same class. So, everything in this node is fast. This node is a little bit more complicated. What is p slow for this node? Write your answer in the box and use a fraction So P slow in this example is going to be two-thirds. Because I have three examples in this node and two of them are slow Next question is what's pfast in this node on the left? Write your answer in the box and use a fraction. Pfast equals one-third. Because again we have three examples and one of them is fast. Remember, the entropy is the sum over all the class labels of negative P, log base 2, P. So what's the entropy of the children once we make this split? So I have two class labels that are available to me, the slow and the fast. We'll start with slow, so Pslow is two-thirds. So that's what I'm going to put in here. Negative two-thirds, log base two of two-thirds. And now we do the calculation for the fast component. So Pfast is one third, so this negative one third, log base 2 one third. Now you do the calculation and write the decimal value of this expression into the box there. Remember if you need to do the log base 2 calculation, one of the things that I found really helpful is the math module in Python. The entropy that I get is 0.9184. So the entropy of the children is going to be a weighted average of the two branches. So I have three quarters of my children going into the steep branch with an entropy of 0.9184. And I have one of my children going into the flat branch with an entropy of 0. That's this number up here, remember. Remember, this was the entropy that we started out with, 1.0, it was the most impure system that you can have with two labels. So the question is, what's the information gain if I were to split based on grade? Write your answer in the box here. So the information gain is going to be 1, that's the entropy that we started with, minus three-quarters times 0.9184 minus one-fourth times 0. So this is just going to be, the answer is 0.3112. This is the information gain we get if we split based on the grade. The next variable that we'll look at is bumpiness. So let's compute the information gain when we make a split based on bumpiness. So what's the entropy of the bump terrain? Write your answer in the box as a decimal. The entropy of the bumpy terrain is 1.0. The next question is, what's the entropy of the smooth terrain? And the numbers for the smooth terrain work out to be the same as the numbers for the bumpy terrain. That means the entropy for the smooth terrain is also 1.0. So, what's the information gain if we make a split based on bumpiness? Write your answer in the box. On this split, the information gain is actually zero. We don't learn anything by splitting the train, these particular training examples based on the bumpiness of the train. So this clearly is probably not where we want to start splitting our sample if we want to build a decision tree. Last, but certainly not least is going to be the speed limit. So this time I'll only ask you for the information gain. You'll have to do the entropy calculations on your own. What is the information gain when we make a split based on speed limit? Write your answer in the box. As it happens when we split based on speed limit we get perfect purity of the branches that we make as a result. So our information gain is going to be equal to 1. We started out with an entropy of 1. At the end we had an entropy of 0. So the information gain is going to be 1. So this is the best information gain that we can have, definitely this is where we want to make a split. And just to sketch out the decision tree it would look something like this. Where when we look at samples where the speed limit is in effect, so these first two rows where the, the answer for speed limit is yes, then we get all of our slow examples over there. On the other side when there's no speed limit, everything is going to be fast. So this has just been a very simple calculation. It still took us a while to get through, but I hope you have a little bit of a better sense for what information gain is in decision trees and why it's so important. So, it's calculations like this that the decision tree is figuring out when it does the training. It looks at all the training examples, all of the different features that are available to it, and it uses this information gain criterion in deciding which variables to split on, and how to make the splits. Now that we've gone through that calculation, let me just point out one little caveat to you. I'm going back to the scikit-learn documentation, just really quickly. So, I've gone over to the DecisionTreeClassifier documentation here. And I see that the splitting criteria that I used, is actually a tunable parameter for scikit-learn. And, in fact, as it happens the default is a metric that's called the gini index which is another similar metric of impurity. You can see that they also support entropy or the information game, but this is something that you have to put in by hand. Either one of these will work probably fine out of the box. But I did want to mention it to you that in scikit-learn the default is going to be something slightly different from the entropy. But both of them should give you reasonably good results. It's also something that would be interesting to play around with. So let's head over to Sebastian now. He's going to talk some about bias and variance. And what that has to do with decision trees and all types of supervised classifiers. So welcome back to the self driving car. Katie just asked me to explain to you two terms of machinery called bias and variance. They are often used as bias, variance dilemma, and, and here's the gist of it. A high bias machine learning algorithm is one that practically ignores the data. It has almost no capacitor in anything, and it is called a bias. So a, a bias car would be one that I can train, and no matter which way I train it, it doesn't do anything differently. Now, that's generally a bad idea in machine learning, but you can go to the other extreme and make a car that's extremely perceptive to data, and it can only replicate stuff it's seen before. That is an extremely high variance algorithm. And the problem with that is, it'll react very poorly in situations it hasn't seen before because it doesn't have the right bias to generalize to new stuff. So, in reality, what you want is something in the middle. You have what's called a bias-variance trade-off. You want an algorithm that has some authority to generalize. But is still very open to listen to the data. And that's in your choice, very often. Many algorithms exist, you can actually turn a knob. And make it more biased, or it can be high variance, and the art of, of tilting that knob is the art of making machinery amazing. You did it again, congratulations, your third machine algorithm decision trees, you got all the way to the end. We've learned a lot about decision trees. Let's add some of this new found knowledge to our list of things to consider when you're picking a classifier. So they're really easy to use and they're beautiful to grow on. They're, they're graphically, in some sense, allow you to interpret the data really well, and you can really understand them much better then say the result of a support vector machine. But they also have limitations. That's right. One of the things that's true about decision trees is they're prone to over fitting. Especially if you have data that has lots and lots of features and a complicated decision tree it can over fit the data. So you have to be careful with the parameter tunes that you're picking when you use the decision tree to prevent this from happening. Yeah. What comes out could look crazy if your node has, only did single data point. You almost always over-fit. So it is really important for you to measure how well you're doing, then stop the growth of the tree at the appropriate time. One of the things that's also really cool about decision trees, though, is that you can build bigger classifiers out of decision trees in something called ensemble methods. In the next lesson, we'll give you a chance to actually explore an algorithm completely on your own. And a couple of the ones that will give you as choices are examples of ensemble methods. So if this sounds interesting to you. Building a classifier out of classifier, then stay tuned in the next lesson. Yeah, we are super, duper So stay tuned. Let's get started. Welcome to the mini project on decision trees. In this project we'll be tackling the same project that we've done with our last two supervised classification algorithms. We're trying to understand who wrote an email based on the word content of that email. This time we'll be using a decision tree. We'll also dig into the features that we use a little bit more. This'll be a dedicated topic in the latter part of the class. What features give you the most effective, the most accurate supervised classification algorithm? But you'll get a little sneak piece of this in the second part of the lesson. So get ready to code. So you successfully learned about three different supervised learning algorithms, now it's time for the fourth. But this time, we won't prescribe one for you. That's right. We're going to give you a choice of three, and it's up to you to figure out which one you want to pursue on your own. And what are the choices? The choices are, k nearest neighbors. Very simple, very straightforward algorithm, a great one to know. Ataboost. Mm-hm. Very powerful and random forest. Those, those latter two are usually used with decision trees, which you know all about by now. Do you have a favorite? I like eta boost. Eta boost girl. I love eta boost, yes. So you're going to go out and check on the documentation, pick an algorithm and run it, and report results, and see if it can beat any of the algorithms here in about so far. That's right, lets get started When I was a kid, I used to read these books called Choose Your Own Adventure. And the idea of Choose Your Own Adventure is that you are the protagonist and you get to make choices about what to do at various points along the storyline. And based on what you do, you get sent to another page. You get to see what happens as the results of your choices. And in that spirit, for this final lesson, I'm going to have you choose your own algorithm to investigate. We've given you your first three and hopefully a little bit of a structure for how you can approach them. Now we're going to guide you through doing it on your own. And in the future, as you become a more advanced machine learner, you'll be doing this on your own all the time, so this is your first chance to leave the nest and to find about a new algorithm on your own. I'm going to suggest three different algorithms to you. These are all examples of supervised classification algorithms and you should pick one. Your choices are k nearest neighbors, adaboost and random forest. You don't know what any of these are right now, of course, but that's sort of the point. Here's a tiny bit of information on each one to maybe help you start making your decision. If you're brand new to machine learning, you've never done it before, or it's still kind of a challenge for you to understand the algorithms that we've talked about so far, I would suggest trying k nearest neighbors. This is a classic algorithm. It's very simple and easy to understand, which is a big advantage in a machine learning algorithm, that you know what's going on. If you're down with everything we've done so far and you're ready for a really interesting challenge, I would suggest adaboost or random forest, which are both examples of what we call ensemble methods. The name ensemble methods gives you a little bit of a hint of what these two might be doing. They're meta classifiers that are built from many, usually, decision trees, so you have many classifiers and you sort of have them all working together to come up with a single decision. It's a little bit like how we choose the president by voting. There's a single decision of who's the president going to be, and there are many different people who have different opinions on what that answer should be. And so what you have to do is you ask the question of many different people and all together you come up with a single answer. That's a little bit like what these two algorithms do, but they do it in slightly different ways. However, any of the algorithms that you pick should all be supported in SK-learn so you can rely on SK-learn having something that you can use straight out of the box. Here's the process I want you to follow. It has several steps. The first is to take the name of your algorithm and head on to Google to do some research. Get a general understanding of what the algorithm does. If you can explain the algorithm to a friend, for example, then you probably understand it well enough that you're ready to move on the next step. Then what I suggest you do is find the sklearn documentation for this algorithm, exactly the way we've been doing all throughout this course. Remember that the sklearn documentation also usually has examples. This would be a huge help for you in the next step, step number three, which is to deploy the algorithm. Get your hands dirty. Try to get it running at least. That's, of course, the first thing that you need before you can go on to step number four, which is using it to make some predictions about the test set. And then, of course, you use those predictions to evaluate it. What's the accuracy on the test set? I've given you set of starter code that has all the training and testing data ready to go. There's also a function in there that allows you to plot it, so you can visualize the decision boundary, which is always something that's really nice to do. By the time you've completed this process, you will be a real machine learning expert in the sense that maybe you don't know everything yet. The very few machine learning experts would say they know everything. The thing that separates the good ones from the great ones is very often that they're able to go out and learn something new, and this is your chance to do that. Congratulations you made it this far. You actually did a lot of great work in get all the way to the end implement the algorithm and running it and testing it. I wonder if you are beating the performance of our self driving car showing classification algorithms or not. If you do then we should let you drive our car. Fantastic job guys. Welcome to the end of the lesson on Choose Your Own Algorithm. There's no dedicated mini project for this lesson because the entire lesson is a mini project. But hopefully, you now have a fourth supervised classification algorithm in your toolbox and you know how and where to deploy it. So great job So here we are back in the self driving car. Katy, what have we learned this unit? We've learned so much, we've come so far. We've learned three supervised classification algorithms. And we've also learned a lot of things that are more higher level than just the algorithms themselves. We've learned about bias and variance. These are two extremely important ideas in machine learning. We've learned about over-fitting. We've learned about the parameters. How you can take an algorithm and make it look completely different just using the parameters. We've covered a lot of material. That sounds so amazing. So when you get to it like in [INAUDIBLE]. What really makes a great machine learning researcher? A great machine learning researcher is someone who's really excited and curious and adventurous and they have a lot of questions and they really want to answer them. And what it does when you have this attitude is, it makes you go out and find the way that you can answer the question that you have. And you have a really great start with the stuff that we've taught you. But there's a lot of stuff that we haven't taught you. Machine learning is a gigantic topic. And a great machine learning researcher is somebody who goes out after this and tries stuff on their own. That's how you learn. So my last question. I have a hunch. Is this what the next unit is all about? Your hunches are usually correct, Sebastian, and in, in this case it is again. In the next unit, we're going to be talking about data sets, we're going to be talking about questions. We've talked a lot about algorithms that are very important, but at the heart of it machine learning is about the data that you have and the questions that you're trying to answer with it. And we're going to really dig into this topic. And I'm very excited. We have a new dataset we're going to be looking at. It's the Enron dataset. It's a really interesting case of emails and fraud, and we're going to be taking a really deep look at this dataset, trying to figure out what we can get out of it. That's totally amazing, I guess we'll see you in class. I hope so, we'll see you in class. So Katie this is about Enron. Tell me, what is the Enron story? Oh, the Enron story is so interesting. The Enron story is the largest case of corporate fraud in American history. What happened? That's what it is. So Enron was an energy trading company, that, in around the year 2000 or so, was one of the top ten largest companies in America, huge company. They had their fingers in all kinds of different projects, and by the middle of 2002, it was gone. It was bankrupt, there were dozens of people in jail, there was billions of dollars, tens of thousands of people's jobs that were, poof. And as you might suspect when something like this happens, a multibillion dollar company just collapses, it didn't happen totally organically. There was a lot of fraud that was going on underneath the surface. In some ways, it was amazing that the company didn't go bankrupt even earlier than that, considering how much fraud was going on. Well, that's, that's super interesting. So why is it relevant for this class? It is relevant for this class because, as it turns out, Enron has had, sort of, a second life in the form of one of the most famous data sets of all time. And the data set that we'll be using, sort of, to motivate a lot of our projects in the rest of this class. Its called the Enron Corpus, and its tens of thousands of emails. Hundreds of thousands of emails. So real emails by real people? Real emails by real people. It's one of the, as far as I'm aware, the biggest corpus of emails that sort of exist in the wild. That exist in the open, I guess. In the open, that's right. Because emails are usually so confidential, right? And so, it's been used for many, many different purposes. And we'll be using it to answer a very interesting question in this class. I want to use this corpus to try to figure out if there are patterns within the emails of people who were persons of interest in the fraud case to see if you can identify those patterns. So these are likes real emails for real people- Yeah. And if they have an affair, you'll find it in the email and their misappropriate money, you'll find it in the emails. Yes, yes. So you're going to see all this. Oh, you can, you see everything. In fact, one of the things that was very interesting to me as I was learning about sort of the Enron fraud case. And I was reading newspaper articles and watching, watching documentaries. You get a sense for some of these individuals and exactly the roles that they were playing in this fraud. And as you go in and actually read their emails is a very weird experience. So, yes, it's, it's real. So that's the case of the past, right? But this machine learning is about the future. Why is it interesting to go this email corpus and find panels of fraud? Like who would care? The thing that I love about this project is while this data set has been out in the world and very famous for the last decade or so, no one's ever actually tried to answer this particular question. Can we identify patterns in the emails of people who were- Who would care about this? The NSA, are you working for the NSA? I care about it, I think it's a really interesting question. That's the thing is it, is it, this is at least, for my case, it's a, it's a case of curiosity. Of course, if you were a some kind of forensic, machine learning expert, you might, this might actually not be a, a case of the past for you. You might be really interested in trying to answer this question and might have real world implications. For me, I'm just, I'm just perversely curious. Interesting. So Katie, you've done experiments on the data already. I've done a lot, yes. On your own so- So what do you do, what worked, what didn't work? Well, that's basically going to be what I'll be teaching for the rest of the class. I'll be, a lot of the projects all of the mini projects, in fact, are going to be coming back to this Enron corpus. As I'm trying to understand things about it. And using different machine learning techniques to attack it from different angles. I've done a lot of smart things, and I've done a lot of very stupid things, and I've learned in the process of, of, this this exercise. So one thing that I will be doing also from here on out is pointing out some of the places where I've done something that, that I didn't realize at first but was kind of stupid. And how I figured out what my mistake was and how I caught it. So my hope is that this is a little bit of watching a machine learning exercise in practice. That in some ways, I am two steps ahead of you, kind of macheteing my way through the jungle, but that the students are there, one step behind me. And they're sort of watching my thought process as its unfolding. As I'm trying to answer this question. What do you think the students will learn from this data, and where do you think they can apply it? It's an incredibly rich data set, and there's a lot of places where we'll be talking about things that have broad application. So the things we'll be addressing in this coming few lessons, for example, we'll be talking about regressions. So using regressions, I was able to understand the relationship between the salaries of the people in Enron and their bonuses. And see if there are any interesting relationships there. As it turns out, I'm not sure that it has a lot of implication for the fraud, but it was something that I found very interesting. We'll be looking at clustering. You can use clustering on the data which is a, a type of unsupervised learning. It's a very interesting algorithm to figure out who within this within this organization was a member of the board of directors and who was just a regular employee. We'll be using outlier detection and removal to find certain lines in the data set that were bugs basically. That were outliers, that I had to go in and clean out manually. To make sure that we were getting sensible results from our data. Stuff like that. Great, great. And students will they basically be watching you do this? Or will they do it themselves? A little bit of both. There are places where there's something that I would wrestle with for many days at a time before I finally figured out what was going wrong. So my hope is that they will not be wrestling quite as hard that, as I've learned some of these things, I can point them in the right direction. But for the most part, it will be replicating the steps that, that I've been following and coming up with the answers to the questions that I've been curious about. And in terms of the skills of student learn by doing this, the skills that they can take on in their professional lives. Like, what are other examples? Because the Enron data set is like, one in a decade. And it's a specific one that was made public for some specific legal action. Right. Like, what other things, today, can the students use it for, once they've taken this specific example? Well, it's, a lot of the topics that we'll be covering will be using Enron as a use case. But of course, that's not the only use case for any of these things. So let's use the example of clustering. So I said that clustering will be using to identify the difference between people on the board of directors and ordinary employees at Enron. But clustering is something we have an interview with a vice president in Netflix, who is very involved in their machine learning. And in Netflix, what they do with clustering is they actually identify particular types of people by their movie choices. And so they can have clusters of users that they say, here's, here's the, whatever 25 to 35 year old men who like to watch action movies and comedies. Here's the children who like to watch Disney movies. Here's the women who like to watch romances and these, based on these clusters, they can figure out who their, yeah, who their customers are. So you're telling us by analyzing the email of a fraudulent organization and then many years ago, you learn about movie preferences of people in Netflix? By using the same algorithms, yeah, it's not too, it's not too much of a stretch. because there are many different questions that you can be asking, and machine learning is a tool that you're going to be using to answer them. So whichever, whichever direction you point that fire hose in is- That, that, that's really exciting. I'm, I'm very excited about this. And there's one thing that I, I do want to emphasize. Which is that what we're trying to do with this data set identifying persons of interest in the fraud, it, it hasn't ever been done before, as far as I'm aware. And we picked it because I think it's a really cool problem. Not because I know that there's an answer that's easy to find. And so I, I think it's very important that that point be made because it's there's lots of things out there that would be easy to solve. And this isn't one of those cases, and in fact, at the end if there's a student who gets very into this and really starts to dig into the data on their own, it can very easily become more of an expert than, than I am right now. So I really hope that there are students out there who get as excited about this as I am and, and make it their own project, start to engineer their own features, come up with their own ideas, and figure out how to implement them because while I'm setting a good example, I'm getting you off to a good start, there is no way that I'm covering all the smart ideas that could solve this, right? This is great. Thank you, Katie. Thank you. If I want to be able to identify persons of interest in the Enron dataset, of course, a really important thing to do is to define what I mean by a person of interest. So I thought about this for awhile and I came up with a few different things that might define a person of interest. Maybe they were someone who was indicted, which means they were charged with a crime. Another possibility, and there were several people like this, is that a person of interest could be someone who settled without admitting guilt. So that usually means that they paid a fine, but there were no criminal charges. And they didn't admit that they were involved in the fraud in any specific way. It's a little bit of a judgment call whether you should include these people, so that I would have as many examples as possible of persons of interest. And then the third possibility is a person of interest could be someone who testified for the government in exchange for immunity from prosecution. Now I want to show you a list I made of all the persons of interest that I was able to identify. So in this poi.txt file, I just have a list of names. This first line here is actually an article that I found very helpful in trying to identify all the people who were persons of interest. And it was interesting because this was really a manual problem, to come up with this list. In all the examples that we've done so far, you just kind of have the data handed to you and it already has class labels assigned, so it's really simple. But here, what I had to do was actually go out and figure out who should be assigned the class label of person of interest. So all those people, I found their names using newspaper articles basically and I wrote them all down. You can see that there were quite a few of them. This is consistent with it being one of the largest corporate fraud cases in American history. All told, I counted about 35 people, 30 of whom or so worked for Enron. The rest were working at other firms, but they were doing business with Enron. And one thing that I want to point out is that this list might not be perfect. I made it by hand and there's completely the possibility that I missed someone or I missed several people. And so, in the videos to come for the rest of the class, I'll show you what I did, but I'll also try to point out places where it's possible that mistakes could sneak in. So you should always be skeptical of the things that I find and you should try to check them out yourself. So, if you're really interested, go through this list and see if you can find anyone else who should be on it, and if so, post it to the forums. There's a very important thing that I was keeping an eye on as I was drawing up my training set of all the persons of interest, and that is basically the size of that training set. The reason that the size of your training set is so important is that it usually has a big effect on the accuracy that you are able to achieve with your classifier. Let me show you an example of what I mean. I once had a problem that I was trying to solve in physics. I was using a naive Bayes classifier to try to identify certain types of particles, and I had 1,000 events of each class in my training set. And my question was basically, is this enough events to capture all the trends in the data? So here's what I do to answer that question. I took my 1,000 events of data and I put about 800 into a training set and about 200 into a test set. Then I took these 800 and I split them again into 4 times 200. So then by recombining different numbers of these slices, I could come up with a training set that had 200 events, 400, 600, and 800. And then I would always be testing it on the same 200 event testing set. And what I found was something that looked like this. The maximum accuracy that you can ever get will be 100%. And usually in practice, the maximum accuracy that's even hypothetically possible will be some number less than 100%. And if you don't have enough statistics in your training set, if your training set isn't big enough, you'll get an accuracy that's even lower than that. So for 200 events, maybe I would get an accuracy that was around 55%. However, when I go up to 400 events, my accuracy jumps to let's say 70%. So I got another 15 percentage points just by adding more training data. I didn't do anything else. Then I go up to 600 events. Again, my accuracy improves, but probably not as much as before. So let's say this is 80% now. And then when I add in my last slice of 200 events, I might be at about 82%. And what this told me was that there was a trend in the data, maybe something like this, that when I looked at it, describe to me whether I could expect the accuracy could, to get very much better as I increase the size of my training set. Obviously, 200 wasn't ideal. If I only had a training set that had 200 events in it, I couldn't expect to get a very good accuracy. By the time I got up to 800, I'm all the way up to 82%. And moreover, by looking at this trend, I can see that it's starting to plateau, and even if I went out to, let's say, 1,000 events, I might only get up to 83%. So there wasn't as much of an advantage to add in another 200 events at the end than this first 200 events at the beginning. And so, one of my concerns, when trying to identify persons of interest, is whether we'd be all the way down here, that we would have so few examples of persons of interest, especially relative to how many completely innocent people are in our data set. But it's very hard to distinguish the patterns that set apart the persons of interest. Now, in a perfect world, I could maybe try to make a graph like this. And if I found out I was down in this corner, I could go out and try to find more data. In this particular example, that's not really possible, right? We only have as many persons of interest as we have. It's not like more of them are just going to appear because I want to make a better classifier. But this is something that's very foremost in my head when I first tackle this problem, is how many examples can we get of persons of interest? So when I look at this list that I've compiled of the names of the persons of interest and I see that we have about 30 people, do I think that's enough to proceed with the project? And the honest answer is, I don't really know, especially when I first started. And there's no good way to know except to try it out. But ideally, if you're working in a situation, say, the self-driving car, where you have the option of asking a question like this, how does the accuracy change with the number of training events, and especially if you have the power to go out and collect more data if you need it, this can be an extremely useful series of questions to ask. And in general, more data is going to give you a better result than a super fine-tuned algorithm. This is something that surprised a lot of machine learning researchers when they first started to discover it many years ago, but it's become a truism of machine learning right now, that being able to use more data will almost always help out the performance of your algorithm. Now that I've defined a person of interest it's time to get our hands dirty with the dataset. So here's the path that I followed to find the dataset. I started on Google as I always do. My Google for Enron emails. The first thing that pops up is the Enron Email Dataset. You can see that it's a dataset that's, that's very famous. Many have gone before us in studying this for many different purposes. It has it's own Wikipedia page. It has a really interesting article that I recommend you read from the MIT Technology Review about the many uses of this dataset throughout the years. But the first link is the dataset itself. Let's follow that link. This takes us to a page from the Carnegie Mellon CS department. It gives a little bit of background on the dataset and if we scroll down a little ways. We see this link right here. This is the actual link to the dataset. Below that is a little bit more information. If you click on this it's going to download a TGZ file. Which you can see I've already downloaded down here. If you do this on your own. It took me nearly half an hour to download the whole dataset. So I recommend that you start the download and then walk away to do something else. Once you have the data set you'll need to unzip it. So move it into the directory where you want to work and then you can run a command like this. There's no real magic here I just googled how to unzip .tgz and found a command like this. Again this will take a few minutes. When you're done with that you'll get a directory called enron mail. And then CD into maildir. Here's the data set. It's organized into a number of directories, each of which belongs to a person. You see that there's so many here I can't even fit them all on one page. In fact, you'll find that there's over 150 people in this dataset. Each on is identified by their last name and the first letter of their first name. So, looking through on a very superficial level, I see Jeff Skilling. Let's see if I can find Ken Lay. Looks like he might be up here. Yep, there's Ken Lay. Of course, a whole bunch of people I've never heard of. And remember, my question is, how many of the persons of interest do I have emails from? Do I have enough persons of interest, do I have their emails, that I could start describing the patterns in those emails, using supervised classification algorithms? And so the way that I answered this question was, again, using some work by hand basically. I took my list of persons of interest and for each person on that list I just looked for their name in this directory. Let's go back to that, remind ourselves what it looked like. You can see the annotated list here. You might have been wondering what these letters were before each of the names. These are my notes that I wrote to myself. As to whether I actually have the inbox of each of each of these people. So Ken Lay and Jeff Skilling we already found. But then it started to become a little more difficult. So you can see there are many, many people than I have n's next to their name. And that means no I don't have, for example, Scott Yeager. If I go over to the dataset, I don't see a Yeager down here. So Scott Yeager is a person who I'd love to have his inbox. I'd love to have some emails to in and from him, but I don't. As it turns out, I don't have the email inboxes of a lot of people. So I'll be honest, at this point I was actually really just discouraged about the possibility of using this as a project at all. I think I counted something like four or five people that I had their inboxes. And while that might be a few hundred emails or something like that. There's really no chance that with four examples of persons of interest I could start to describe the patterns of persons of interest as a whole. In the next video, though, I want to give you a key insight that I had that gave this project a second chance. A different way of trying to access the email inboxes of the persons of interest. Let's say a few words about types of data. It's something that's come up in this class already, but let's make it a little bit more explicit so that you know that it's something to look out for. Here are a few important types of data that you should be aware of. One is numerical. So something like salary information would be an example of a numerical variable, where basically the feature is a number. Another thing that we've talked a lot about in the context of supervised classification is categorical features. These can take on a limited number of discrete values. So, a cat, an example of a categorical feature would be the class label. This would also be something like if a person is a man or a woman, or maybe how many stars you give when you're rating a movie. Although stars, you might argue, could also be called a numerical value. Another one that's very important, especially in particular fields like finance, is time series data. This is data that has a temporal value attached to it, so this would be something like a date or a time stamp that you can look for trends in time. One that we're already very familiar with, text data, which is basically just words. A lot of the time the first thing that you do with text is you turn it into numbers using some interesting functions that we'll, that we'll learn about, things like the bag of words formulation. So these are all types of data that you might encounter, and depending on exactly the type of data, this might have some repercussions for the type of algorithm that you can use or the type of questions that you can ask of it. So let me get you a quiz just to practice this. I'm going to give you a bunch of types of features you might see and ask you what type of data that feature is. First example is salary information. Would you call this numerical, categorical, time series, text, or let's put other on the list as well because this isn't a complete list. Salary information is almost always going to be numerical, especially if you have someone's raw salary, so I hope you said numerical. The next one is job title. Let's suppose for someone in Enron I have their job title. What type of data would you say that is? The job title's going to be a categorical variable. There's a limited number of types of jobs. A person can be a, a director, a vice president, a trader, a secretary, and so on and so on. And each one of these is going to be a category. Third feature in timestamps on emails. What type of feature is that? Timestamps on emails is a classic example of time series data. Here's one that should be pretty easy. What type of data is the contents of the emails? So, the actual words that I write to someone when I write them an email. The contents of the emails are going to be a pretty classic example of a text data Another feature that might be really interesting to look at is the number of emails sent by a given person. As it turns out, this is a feature that we'll have to write ourselves. It doesn't come to us straight out of the data. We have to look at the data and actually build this particular feature ourselves, but that is one we will take a look at. What type of data would that be? Since the answer will be a number, that's a numerical feature. And the last one is the to and from fields of the emails. So if I send a list to five different people, it will be those five email addresses. That could be a very interesting feature for persons of interest. Maybe they send lots of emails to other persons of interest, and that's one way we could figure out who they are. What type of data would that be? And this one is maybe a little bit tricky. You might have said other, but I would actually argue that this is a type of text. It's not words maybe in the most classic sense, but it's words of a very specialized vocabulary, where the vocabulary is email addresses. And so, text wrangling tools might be one of the more better ways to deal with email addresses. Great job. Now that we've gone through all these examples of types of data, hopefully you have a good sense for what might be some interesting features to look for and what type of data those features are. Now obviously, not everything we talked about here is something that's going to be available to us in the email corpus. For example, salary information is very, very interesting, but it's not going to show up in your email inbox. So now, let me tell you a little bit more about the data set that we've made because I haven't been totally honest so far. We do have a lot of emails, but there's some other very interesting stuff that we have available as well. Welcome to the mini-project on data sets and questions. This is where you're going to get your hands on the Enron data for the first time, and this is going to serve as sort of the backbone for the rest of the class. We want you to get good and familiar with it. I think this is a really fun lesson because this is where you get to go in and actually figure out who were all the players, how much money were they making, who was sending emails to who, stuff like this. So in this mini-project, I'll provide you with the data set and then ask you a series of questions that help guide you through how to navigate through the data set and extract some of the information. So this next section is about continuous supervised learning. And as always we start with a little quiz. In learning so far we had an input dimension and an output dimension. And one of them was constraint, not to take arbitrary values but what we call, discrete or often binary values like zero, one or fast or slow. Which of the two axis was it? I give you two choices, input or output. So check the appropriate radio box. And the quick answer is output, in that we are allowed to have arbitrary input values, but the outputs tend to be binary. For example, slow and fast. Now this kind of output is called discrete, but in many learning problems, your output could be continues as well. So for example, if the input is the height of a person, and you put it output is the weight, then what you find is probably a function like this. It says that the taller a person, the more a person weights. And in this case the output is not a binary concept like, light or heavy. It's a continuous concept, and the output itself is also continuous. So this is what we call continuous supervised learning So, that's a quick quiz. The word continuous, do you think it refers to the input or the output? [SOUND] Of the problem set up. One of the two is correct. And, yes, it's the output. The input was allowed to be continuous all along, but the output before was binary So in practice when you apply a machine learning algorithm, you have this distinction between continuous output with discrete output, but it's a bit more subtle, and I'm going to give you some examples that are a bit surprising. Suppose you want to predict the age of somebody, would you use a continuous classifier or a discrete classifier? We'll say age is encoded a couple of number of seconds after they're born I'd say it's continuous. How about the weather, and see really care about, sunny versus rainy. Set this discrete or continuous. I'd say the way I articulated sunny vs. rainy, it's actually discrete, but in reality, weather is a continuous phenomenon, it turns out. So, if you, articulated weather as the amount of light that hits a given area on the ground in a given period of time, that's a continuous measurement. So, you can actually really measure weather continuous effect, I would argue that most things we consider discrete are actually somewhat continuous. So what you want to classify which person wrote an email. On the other side are now 100 people in your organization. And they even have a, I don't know, an id equals 1 to 100 depending on when they're drawn. So the first person has number 1. And the last person is number 100. Is this is continuous, or discrete? And here would I use discrete, even though it might look continuous, they have 100 people with 100 order numbers, because there isn't really an order here. The person who joined as number 56 is in nowhere related, usually, to the person who joined as number 57. And if this order doesn't exist, a discrete classifier tends to be better even though you might be dealing with numbers. What if you had this magic classifier where you toss in the name and out comes the phone number of the person. Would you if you were to train something like that say in fact I would say the NSA has training algorithms to train something like that. Because people have multiple phone numbers and there's often typos in phone numbers. Suppose you, you, your job was to, to find the phone number of a person. Is this continuous or discrete? And again I would say it's discrete. In a sense that, phone numbers, they have a bit of an order. The, in the United States, the area codes are similar for people in the same region. And there's a chance that people with the same name live in the same region, because people marry and exchange names. But by and large, the phone number of this type, 024-3111 and 024-3112. There isn't really any relationship. They're not nearby. There's no real ordering in phone numbers. So, as a result, you're better off with a discrete classifier than continuous Final question. Income level how much money you make. Continuous or discrete? And here I'd say it's continuous. Because making $10,000 is just about the same as 9,999 or making 20 bucks is about the same as 19. So there's an order here. So what I'm trying to drive home is that when we make it continuous really implies that there's some sort of an ordering to it. So we find that there's many different outcomes and they fall into linear order where you can make greater or equal decisions, go continuous. If there's many outcomes and they even occur by numbers without an order, then you go discrete. So let's talk a little example here. You might remember the Terrain classification problem. Classification's often meant to be a discrete output. In our situation we had two output variables, fast or slow. And as it generalizes to a continuous output. Tell me which one you think is the best generalization here. Is it fast, medium, slow? Is there something like the speed in miles per hour, or perhaps the angle of the steering wheel? Just pick one of the three. In every alley the speed is miles per hour is the best. Obviously fast medium slow is still discrete. It's not binary. It's not three choices but it's still discrete. Angle of the steering wheel is continuous but it's not really a good generalization to fast or slow. But of course speed in miles per hour or kilometer per hours is a good generalization of fast slow into the continuous space. In fact, I would argue speed is always continuous and our choice to make it binary was an artificial choice to make it fit for specific machine learning algorithms. So let's do a little example here just to get a feel for what you can do in continuous output spaces with machine learning. It's a contrived example, but suppose we graph all the people in the world and record their age and their net worth. If you don't know what net worth is, this is kind of the total money that you kind of own, both in cash, and houses, and cars, and so on. And just for the sake of the argument, let me assume there aren't quite seven billion people, and the relationship between net worth and age looks about like this. In reality, that's often not the case. But let's assume that's the case here. I now want you to fit an imaginary line through the data and pick two of the points I'm kind of marking here, as two representative points to which the line would go. Just click on two of those points that best represent the line, that best describes the data. I know the question was a bit ambiguous, but if you draw a best line through the data points, I would say these things over here are the two boxes that best represent the line. Obviously, not those two over here, because that is not a good approximation of the data. Neither are those over here. Neither are those over here. So, I wanted you to click on those two points over here. So let me add numbers to this diagram over here. Say the age ranges from 0 to 80, and your net worth might be 500 at its best whatever that means. So if you were 80 years old your net worth would be exactly 500. And I want you to give me an equation that describes this line that passes to the point 80, 500 and 0, 0 over here. So net worth should be some number times the age, plus some other number. Can you fill in these two numbers over here that describe this graph? The right answer over here is 6.25 times age plus 0. It's zero because we are. When age is zero, the net worth is zero that means that we are crossing through the origin. And when you divide 500 by 80 you actually get 6.25. Now this is the way to write, in general, a result of a supervised learner. On the left side is our target variable, the one you are trying to predict, often called the output. And on the right side, age, is our input variable. Two other words here, this fact over here, 6.25 is also called the slope. And this number over here, which happens to be zero, is called the intercept. Since they define how steep this curve goes up, 6.25. And where this, of course, intercepts the vertical axis over here. Let's see if it can help us understand slope and intercept in the next quiz. Say we have a situation with an input space x and an output space y. And our linear function is m times x plus b, where m is the slope and b is the intercept. I will give you three graphs of intercept zero. And I wonder for which one is m the largest? Which one has the largest slope. Pick one of the three. And, the answer's the top one. A large slope makes it go up faster. In fact, you can have a negative slope, in which case the graph would actually go down. Asking the same question about the intercept, I'm going to give you three lines with equivalent slopes, say this one, this one, and this one, and I want you to tell me which one has the largest b, or the largest intercept. Pick one of the three please And again we're picking the top line. The value of b is the y-value when x is zero. So when x is zero, this thing over here is the value of b. And is the largest for the top graph. And is actually negative for the bottom curve. So this is slope and intercept. So, coming back to our example of age versus net worth, and say my machine learning algorithm determined that my target function is net worth, written as y, equals 6.25 times age plus 0. Suppose you meet a person whose age is 36. What do you think our machine learning algorithm would say his net worth y is going to be? And the correct answer might have required a calculator. It's 225. That is the same as 36 times 6.25 plus 0. And suppose we have a positive y intercept on the right side, it's kind of a curve like this where there's a positive y intercept of 30. For the same 36 year old person, what would the net worth be now? And I'd say it's now 255. Before, 6.25 times 36 equals 225. And we're now adding 30 and we get 255. And as you can see, with this continuous machine learning example, called a Regression. You can make continuous predictions for any age in between. And you get a meaningful answer that differs continuously with the variation of the input parameter. That's not possible in discrete classifications. In fact if you were to draw a discrete classification into here, it might look very much like this where for an entire range of the input space the output stays the same So, congratulations. I gave you a nice introduction, I hope, into regression. And, now it's up to Katie to give you some hands-on experience of what that actually means. Katie? Welcome now to the coding portion of the regression lesson. We start, as always, on Google and we look for what sklearn has to offer us. As you can see, there's plenty going on here. This linear regression is what we'll end up using eventually. But I actually think this link is a better one to start at because it gives us a little bit more of an introduction to generalized linear models. So we have here a formula that, it's a little bit hard to tell, but this is just rewriting y equals mx plus b, but now we can have more than one x available to us. We'll get into this a little bit later in the lesson right now, but at least this tells us that we're in the right place for something that has a linear form to it. Scrolling down a little bit, I see something called ordinary least squares. The exact name for the code is linear regression, and this looks like exactly what I want. I have a variety of points. I'm fitting them with a line. And maybe even better, I have some example code that can get me started. I hope you recognize that the code here really isn't all that different from the types of codes that we were seeing for the supervised classifiers. So that means that we start with an import statement, from sklearn import linear_model. Within linear_model, there's an object called linearRegression and we use that to create our classifier. The next thing that we do is we fit our classifier. And then here, they're not making predictions with it, although we'll be doing that as well. What they're doing here is they're reading out the coefficients, or what we called the slope. Scrolling down a little further, I see that there's a more detailed example here that I could use to help me if I get stuck anywhere. But as it happens, this is going to be enough to get us off the ground. Now that you've had a, a quick crash course in the regressions documentation for sklearn, let's deploy it on our ages net worths problem so you can really see how it works. Here's how I'm going to have you do this. I've created a data set of net worths and ages, and I've split it into training and testing sets, just like we do in supervised classification. I've plotted the training data here. So you can see that there's definitely a general trend in the data, where as you get older, your net worth goes up. But I've also added a little bit of noise to the data, so that it's not a perfect relationship. It's a little bit more true to life in that respect. However, the way that I generated the data was using the equation that Sebastian was using in the previous few videos. So I start with an age and then I calculate a net worth using the coefficient of 6.25. So the answer that we get out when we perform our regression when we fit it should give us a coefficient that's pretty close to 6.25, although not exactly because of this noise that I've mixed in here. So this is the training data and this is what you should get once you've performed your regression. In this scatter plot, I now have the training data, but I also have the testing data, which I've colored in red so that you can distinguish the points. Using the blue points, I fit a line, which I've drawn in here. This is the result of the regression. Then when I overlay on these points, you can immediately see that it seems to be capturing the trend in the data pretty well. Also using this line, I can use it to predict the net worth of anyone with an age between 20 and 65 or so. So this is what we're going for in the end. Let me show you a little bit of the setup code that I've given you and in the quiz, I'll have you fill in the blanks. So when you get into the quiz, here's what it's going to look like. I create training and testing data for you with the ages and the net worths that you're going to be using. And then the meat of the regression is actually going to be in this studentReg function. I'll take you over there in just a second so you'll know what it will look like. But what you need to do with this is return a fitted regression, which then I'm going to be using in this plotting code down here. So I put in my testing ages and then the predictions that I make for my testing ages using my fitted regression, and that's going to give me the line that you see. So okay, what is studentReg doing? Well, the first thing is I know that I'm sending it the training ages data and net worths data. And what I want you to do here are a few things. The first is importing the sklearn regression module, and then you want to create and train your regression. You want to name your regression reg because you're going to be returning an object named reg. So, it will be looking for something with that name. So that's the assignment and the quiz. Give it a shot. So here's the solution to this quiz in case you had any problems. The first thing you have to do is import the SK learn regression module. Of course it isn't quite this simple because it's called sklearn.linearmodel, but you can figure this out from the documentation pretty easily. There's a familiar linear regression function, and I have my regression which I named reg. Reg gets, linear regression, and then I don't have to worry about parameters in this particular case, and then the next thing to do is just to fit it the same as we did a classifier. We do this of course on our training data, and of course now we have this regression its been, trained on our training data and we're just returning it. Before we move on from this coding example I want to show you just a couple more things that help make your regression as useful as it can be. The first thing is you might want to be able to make a prediction with it. And of course this is something that shouldn't be that difficult. For that you can just call the prediction function on your regression. But here's one little catch, it's going to be expecting a list of values. So even if you only have one value that you want to be predicting. You still need to put it in a list. You might also be wondering what the coefficients and the intercept of the regression are. And you can access these using reg.coef and reg.intercept_ like you can see here. Don't forget this little underscore here. Otherwise it won't know what you're talking about. Just remember the slope we expected to be pretty close to 6.25. But maybe not exact. And the intercept should be close to zero, but again, we wouldn't expect it to be exactly right, because there's a little bit of noise in our, in our training data. There are a few more lines here about the r squared score. Let me come back to that in just a second. But I want to show you what the prediction, the coefficients, and the intercept are right away. So these three lines are the ones we were just talking about. I can predict my net worth, based on the regression. It's about 160, based on my age. We can also print out the slope and the intercepts. Remember we thought the slope would be about 6.25. It's close, but not exact. Similarly for the intercept, it's not quite zero. In the next few videos we're going to talk a lot about the types of errors that you can get on regressions. Because they're fundamentally different from the types of errors that you get in classification. And where we're eventually going, is we're going to be computing something called the r squared. Now, the next few lines give you some output about the performance of your regression. So one way you can evaluate a regression, that we'll be talking about much more in the videos to come is a metric called the R squared. There's also the sum of the errors, we'll be talking about all of these. But let me show you just what it looks like now. So you have some reason to understand why it's important. The way that you access these performance metrics is using something called the score function performed on your regression. And you always want to be looking at the score on your testing data. Because of course you're fitting your regression using your training data. So if there's any over fitting going on that'll show up in having a lower score when you look at your testing data. Now that we've talked about the slope and intercept, I'm going to go back to these few lines. What I'm doing here is printing out some performance metrics on both my test data set and also my training data set just for a point of comparison. Remember the test data set actually provides the numbers that we're going to trust. Let me go back over here. In the next few videos we'll be talking pretty exhaustively about the performance metrics that you can use to evaluate your regressions. But let me give you a sneak preview here. One performance metric that we'll talk about is something called r-squared. And to make a long story short, the higher your r-squared is, the better. R-squared has a maximum value of one. So what we can do is we can ask for this regression that we've made, how good is our r-squared. And we want to be asking that question of our test data set. If we ask it of our training dating set we'll also get some interesting information, but only by using the test data set can we be sensitive to things like over-fitting. So when I go over here and I look at the values of my r-squared score for my test data set, and then also my training data set for comparison, I get a little bit of information about how well my regression is performing. I said the maximum value is 1.0. The maximum value for r-squared is 1.0. So since I'm getting 0.81 on the test data set and 0.87 on the training data set, I'm actually not doing too badly. So rather than talking about r-squared anymore, let's go back to the tablet and we'll talk some more about the types of errors that you can have on regressions. And how you can quantify them. So you really understand what these numbers mean. A very good question you might be asking at this point is how to evaluate your linear regression. One of the obvious things that you can do is visualize the regression on top of your scatter points. Another thing you can do is you can look at the errors that your linear regression makes. By errors, I don't mean mistakes, not exactly. What I mean by errors is something like this. Suppose I have my age and net worth data. And it will be following along in some pattern, although not perfectly linear. I fit my line to it. And this fit is going to have associated errors. In this context, errors is a technical term and that's the difference in this example between the actual net worth of a particular person and the net worth that's predicted by our regression line. Let me give you an example. Suppose we want to look at this point right here. We'll say the age of this person is 35 years old. So using our formula, their predicted net worth is 6.25 times 35, or about 219. But let's suppose their actual net worth is actually 200. What's the error for this point? Write your answer in the box and be sure to be careful about minus signs. According to our formula, the error for this person is negative 18.75, that's the difference between the actual net worth and the predicted net worth. Another way to think about that visually, is it's this distance right here, that this point on the line gives you the predicted value, this point right here is the actual value and the distance between the two is the error. So, here's a quiz. It's a little bit tricky, so think carefully. The question is, what's a good criterion for a good fit? What's something that a good fit will minimize about the errors? Is it the error on the first and the last data points? Is it the sum of the errors on all data points? Is it the sum of the absolute value of the errors on all data points? And, the last choice is the sum of the error squared on all data points. This is tricky, because you can make a good argument for each one of these. So, I want you to select the ones that you think would have the very best chance of giving you a good fit to the data. I told you that this was a really tricky quiz. So if you got it right, good job. Let's walk through the answers one at a time. The first possibility that you minimize the error on the first and the last data points isn't a bad thing to be keeping in mind. You certainly want to have a good fit to the first and last data points. But you could easily end up with something like this. Where the trend of the data as a whole, may look like this. But if you're only basing it on the first and last points, you would ignore all the data in between and come up with something that isn't fitting the pattern as a whole. The next choice, the sum of the error on all the data points might have sounded like a good idea. But here's one place that it can go wrong. Suppose you have really lovely linear data like this. But you fit it with a regression that looks like this. Clearly not a very good fit. However, if this error is, let's say negative 100. And this error is positive 100. Then these two will cancel each other out. And likewise for this point. This point, this point, and this point. So, the sum of the error in this case, will actually be zero even though it's a terrible fit. That's why we usually don't use the sum of the errors. Two things that will work a little bit better, will be the sum of the absolute value of the error. So, in this case what would happen is we will turn all of these into positive errors. These three points right here and then these were already positive. So all together, the sums of the absolute values of the errors will be quite large. So that would probably work. And the last thing that will work, and that we'll explore in more detail, this is actually how linear regressions are calculated, is by minimizing the sum of the squared error on all the data points. This has the advantages that you get with the absolute value of the error because even if you have an error that's negative. When you square it, it becomes positive, and of course, if it's positive to begin with, it'll still be positive after you square it. So, this will work as well. When you perform a linear regression, what you are doing is minimizing the sum of the squared errors. What this means is that the best regression is the one that minimizes the sum of the squared errors, and in this expression, I put the equation for the errors. Where the actual values come from the training points, and the predictions come from the equation that you get from your regression. So, the name of game in linear regression is finding the m and the b that minimize this quantity. Once you find that m and b, that slope and that intercept, you're done. You found the regression result that you want. There's several algorithms that help you find this. We won't go into great detail of the implementation of any of them. But just so you know what they're called. The two most popular are Ordinary Least Squares, or OLS. And this is what the linear regression algorithm in sklearn uses. And another popular one is Gradient Descent. We won't go into any more detail on that one in this class, but I want you to at least know that it's out there in case you see that term. That it's a way of minimizing the sum of the squared errors in a regression. Here's a quiz to help you understand why minimizing the sum of squared errors is the best way to think about a regression. So, here I've given you some data and my warm up question for you is what regression line looks best for this data? Just using sort of your intuitive heuristic for what best might mean. I'll give you three options. They're all pretty close together, so just give me your best guess. Would you say it's the top one, the middle one, or the bottom one? I would say the middle line is probably giving you the best regression result, because it's kind of equally balancing out the points on either side. However, in all of these cases you have roughly as many points above the line as below the line. It's just that the one in the middle happens to be the one that's the most in the middle so to speak. If we were talking about SVMs, we might say it has a large margin. Now you've thought about that let me show you why this matters. Let me take the example of the middle line first. My errors are going to look something like this. I'm sketching them in right now. It's just the distance from each point to sort of the predicted value for that point on the regression line. And if you were just summing the absolute value of the errors you would just sum up all of these orange distances and you would have your answer. But now let's look at the top line. We can do the same thing here too. So now we start drawing in the distances. So while we'll be closer to all the points above it. It would be further away from all of the points below it. So taking these two points as concrete examples. It'll be a little bit closer to the top point and a little bit further away from the bottom point. But overall the total error for these two example points would be exactly the same. As a total error for these two points to the middle line. And, in fact, the same thing would be true for the bottom regression line as well. And if you have equal number of points above and below each of these lines then, in general, that's always going to be true. There's a fundamental ambiguity when you use the absolute value of the errors in terms of exactly where the regression can fall. It could be anywhere in this range. In other words, there can be multiple lines that minimize the absolute errors. There's only going to be one line that minimizes the error squared. In other words, this ambiguity does not exist when the metric that we use is not the absolute value of the distance, the absolute value of the distance squared. There's one more thing that I should add as well. This is a little bit more of a practical concern. And that is this using the sum of the squared error as a way of finding the regression also makes the implementation underneath the hood of the regression much easier. It's much easier to find this line when what you're trying to do is minimize the sum of the squared errors instead of the sum of the absolute errors. And this is something we have the luxury of not worrying about too much when we're using SKlearn to do most of the computational heavy lifting for us. But of course, if you're actually writing the code that goes in and does the linear algebra to find the regression, or maybe the calculus to find the results of what your regression answer should be, then this is a big concern to you. This is another reason why traditionally regressions are going to be minimizing the sum of the squared error. Is because that computationally it's just much easier to do that. There's one important problem with using the sum of the squared error to evaluate your regression. I'll show it to you now. Here are two different data sets, and the regression line for each one of them. And here's a quiz question. Which one of these regressions is doing a better job of fitting the patterns in the data? They're two slightly different data sets, but you can still make a comparison, but they're not so different you couldn't compare them, at least by eye. So do you think the one on the left is doing a better job of fitting the data, the one on the right is doing a better job of fitting the data, or would you say they're probably about the same? I haven't defined what better means very precisely here and I've kind of done that on purpose and this is going to get to the heart of why sum of squared errors isn't perfect. So this is a little bit of a tricky quiz, but just give it your best shot. There are a couple of answers that I think you could make arguments for, for what might be the correct answer for this quiz. One of the things you might say is that the right one is doing a better job because it's fitting so many more data points, that it's easier to describe the pattern in the data, because you just have more data and so you're a little bit more certain that, for example, if you kept collecting data, something weird wouldn't happen that would mess up your fit. So that's one argument you could make. Another argument you could make, which I think is perfectly valid, is that they're about the same. That, considering how much data you have for each one, each one's doing a pretty reasonable job of fitting. And look, the data's pretty comparable between the two of them. It's just that the one on the right happens to have more data. But then if the trend continued on the left, then you would probably have the same fit. The point is though that there isn't a huge difference between these two fits. That they're both generally describing the patterns and the data appropriately. Now here's another quiz. Which one of these two fits has a larger sum of squared errors? Remember, a larger sum of squared errors usually gets interpreted as being a worse fit. So, give it some thought and tell me what you think. In this case, the distribution on the right is going to have the larger sum of squared errors, and it should be fairly straightforward to see why. You can compare point by point, these would be all the errors on the left. There's very similar sum of squared errors on the right for these data points. But, then on the right, you have all these additional data points. And each one of those is going to contribute a little bit of error that we'll add to the overall sum of squared errors of the fit here. So what that means is that the distribution on the right has a larger sum of squared errors even though we agreed that it's probably not doing a much worse job of fitting the data than the distribution on the left. And this is one of the shortcomings of the sum of squared error in general as an evaluation metric. Is that as you add more data the sum of the squared error will almost certainly go up, but it doesn't necessarily mean that your fit is doing a worse job. However, if your comparing two sets of data that have different number of points in them then this can be a big problem, because if your using the sum of square errors to figure out which one is being fit better. Then the sum of squared errors can be jerked around by the number of data points that you're using, even though the fit might be perfectly fine. So this motivates me to tell you about one other evaluation metric that's very popular when evaluating regressions. An evaluation metric that doesn't have this shortcoming is called r squared, and is a very popular evaluation metric for describing the goodness of fit of a linear regression. And, what r squared is, is it's a number that effectively answers the question, how much of my change in the output is explained by the change in my input? And, the values that r squared can take on, will be between 0 and 1. If the number is very small, that generally means that your regression line isn't doing a good job of capturing the trend in the data. On the other hand, if r squared is large, close to 1, what that means is your regression line is doing a good job of describing the relationship between your input, or your x variable, and your output, or your y variable. And, remember, this is the whole point of performing a regression, is to come up with a mathematical formula that describes this relationship. So, if your r square is close to 1, it basically means your fit is doing a good job. The good thing about r squared, is that it's independent of the number of training points. So, it'll always be between 0 and 1. In general, it's not affected by the number of points in your data set. So, this is a little bit more reliable than a sum of squared errors. Especially, if the number of points in the data set could potentially be changing. Now that I've explained r-squared to you, question you might be asking is this is all well and good Katie but how do I get this information? You haven't given me an equation for it or anything like that. And what I want to do instead of giving you a big mathematical equation, which I don't find that interesting and you could look up on your own. I want to show you how to get this information out of scikit-learn. This is the code we were looking at a few videos ago when we were building our net worth predictor. Now, I filled in these lines that are importing the linear progression and making some predictions. Another thing that happened was I printed some information to the screen, you may remember. Two of these things I explained to you already. The slope and the intercept. I access that information by looking for the coefficients and the intercept of the regression. These are just lines of code that I found in an example online. But one thing I did promise you we would come back to, and now we are, is this r-squared score that I was printing out. And the way access that, is through the reg.score quantity. This is kind of similar to how we computed the accuracy in our supervised classifier. So what we do is we pass the ages, which are the features in this case, the input, and the net_worths, which are the outputs, the things we're trying to predict. And then since the regression has already been fit, up here, it knows what it thinks the relationship between these two quantities are. So this is all the information that it needs to compute an r-squared score. And then, I can just print it out. So let me take you over here and show you again what that looks like. I have the same output as I had before, this might look a little bit familiar so I'm predicting my own net worth. I have my slope, my intercept. But now you understand the importance of the r-squared score. So my r-squared score is about point eight six which is actually really good. I'm predicting, I'm doing about 85% of what the best I could doing is. I would say 86% is close to one. It can be a little bit of an art to translate between an r-squared numerically, and saying whether it's a good fit or not. And this is something you'll get some intuition for overtime, as you play with things. I would certainly say that .857 is a good r-squared. We're doing a good job of capturing the relationship between the age and the net worth of people here. I've also seen higher r-squareds in my life. So it's possible that there still could be variables out there. For example, features that if we were able to incorporate the information from additional features we would be better able to predict a person's net worth. So in other words, if we were able to use more than one feature, sometimes we can push up this r squared even further. On the other hand, there are sometimes really complicated problems where it's almost impossible to get an r squared that would be anywhere near this high. So sometimes, in Political Science for example they're trying to run a regression that will predict whether a country will go to war. Of course there's one more thing that I should always be doing when I'm practicing machine learning. And that we've neglected so far which is visualizing our results so let me do that right now. So what I do here, I make a scatter plot of all my data points and then I also write a line like this. plt.plot is going to draw a line. The x values of some of the points along the line will be the ages. And the y values will be the predictions for the ages that are given to me by the regression. So these are kind of like the features and the predictions and then this is just some formatting information. Then I apply some labels and I show it. And I use the show command to see it and it gives me a picture that looks like this. Where these are all the points that we had just like before, and now I've overlayed them with this blue line. And just by eye you can tell the regression isn't doing anything crazy. Always a good sanity check is just to look at the results of your regression. Especially if it's a one dimensional regression like this where you only have one input variable, like the ages. Then it's very straightforward to just look at it by eye and see how things are going. Another reason that visualizing can be very helpful is there's some data that just isn't going to be very good. There's just going to be some data that's going to be difficult to fit with a regression. Let me show you what I mean with a quiz. Here are a few different data sets I might have. And I'm making a scatter plot of some features of each one. And my question for you is, which of these data sets would you look at it and say, that looks like a good candidate for linear regression? We've done a lot of examples of linear regression, and by a good candidate, I mean you can write an equation y equals mx plus b that does a good job of describing the patterns in the data. Place a check box by all the ones that would be good candidates. This is a tricky question, and maybe one that you had to think about pretty hard. I would say there's only one answer. This one. One data set. It would be a good candidate for linear regression. Hopefully, you got that one. So, what are the problems with the other ones? Well, let's start with this one on the lower right. It would be pretty easy to think of a function that might fit it nicely, but this function would be something like Y equals X squared, which is not of the form y equals mx plus b. In other words, this isn't a line. This is more of a curve. There are ways that you can fit these functions, but it wouldn't be an example of linear regression. So that's why this would not be a good candidate for linear regression. One possibility that's almost good for linear regression is this one. Because you could fit it with lines but you need two different lines to fit the pattern adequately. In fact, if you try to fit this whole data set with one line, it'll probably give you something like this. A clever thing that you could do would be to split it in half along this dotted line and fit one half of the data to one line and one half to the other one. So if you checked that box because you're thinking something like that, then I would say that's probably an acceptable answer. But just as it is, there's not a single line that fits all of these data points. So I would say that without doing something like, fitting it piece wise, this would not be a good candidate either. Next, let's see the data set in the top middle here. The problem here is just that the data is all over the place. There doesn't seem to be a very strong pattern. So there's lots of different lines that might all have very similar r squares or sum of square errors that could all look very different. And in this case, it's hard to say that any of these lines is really describing a pattern in the data very well. So I would say this is not a particularly good candidate. And this last one, on the upper left, is very tricky. And if you got this one wrong, you shouldn't feel too bad because it's tough. And the problem with this one is, let's look back at this equation y equals mx plus b. And what this means is that by varying along the x axis, the input variable, I want to be able to predict something about what the y value will be along the vertical scale. So I only have one x value, and that's right here. And I have many different y values all along this range. And so there's no way just using this data. That I can vary x and use that to predict a change in y. Another way to say this is that when the equation tries to fit this data, it's going to come up with infinity for m. And usually when you get infinities, it means that something's going wrong. This is very tricky because you definitely need to have variation in x. Now in this case, of course, on the lower left, you only have variation in x. You don't have any variation in y. So in this case, the slope m would be equal to zero. So fitting it with a line might be a little bit of overkill. You could just say that, suppose that this is eight. You could just say that y is always equal to eight. And that would do a perfectly good job of describing the pattern. But it's very important that you have a range of x values that you can examine. So that's why this is not a good candidate for linear regression, but this is. Here's a quick comparison between classification and regression. We'll go into more detail in everything that I write here. But I want to give you a quick overview just so you know what to look out for in this lesson. So the first thing I'll mention is the output type of the algorithm. As you know, for supervised classification, this is going to be discrete in the form of class labels. For regression this is continuous. Which basically means that you'll be predicting a number using a regression. The next question, a very important question is what are you actually trying to find when you perform a classification or a regression? In the case of classification this is usually a decision boundary. And then depending on where a point follows relative to that decision boundary you can assign it a class label. With a regression what we're trying to find is something we usually call a best fit line. Which is a line that fits the data rather than a boundary that describes it. This will make a lot more sense in the next couple of videos. The last thing I'll mention is how to evaluate. When we were doing supervised classification we usually use the accuracy, which is whether it got the class labels correct or not on your test set. And for regression we'll talk about a couple different evaluation metrics. One of which is called the sum of a squared error. Another one is called r squared. I'll tell you a lot more about both of these in the middle part of the lesson. The main point I want you to take away from this is that, while regression isn't exactly the same as supervised classification. A lot of the things you already know about supervised classification have direct analogs in regression. So you should think of regression as a different type of supervised learning. Not as a completely new topic that you now have to learn from scratch. So we learned a lot about regression. In all the examples we saw so far, there was exactly one input variable. For example, we learned a function that predicted the net worth based on age. One input, one output. Now, there's a form of regression where it can have many different input variables, IQ, your age and maybe your education. And those jointly can predict the net worth of a person. There's a name for this. This is called multi-variate regression. It's not an intuitive name, but that's what it's called. So let me give you an example in which the net worth is the function of two variables, x1 equals age, and x2 is the IQ. And we have certain age levels, 20, 40, and 60 here, and certain IQ levels, say, 80, 100, and 120. And in this example, I'm going to fill out all the nine combinations of these input variables. In this example, I'm going to give you nine data points corresponding to the nine combinations of these numbers over here. And your task is it, by hand, by sheer thinking, to figure out the formula that explains this data. So we have y, and y is the net worth, is some number that you will tell me times x1, which is the age, plus some other number, times x2, which is the IQ, plus potentially a third number. It's a challenging question, but can you figure out what numbers to put in here, here, and here? And as a hint, work horizontally first, then work vertically, and the other numbers should just fall into place. So this is actually a surprisingly challenging question if you've never done this before. So let me dissect it for you first. I'm going to only look at the very first feed over here and ask how much does Y grow as you vary X. And you can see as we add 20 to X, from 20 to 40, or another 20 to X. Every time, x1 goes by 20, y goes by 100. So the factor that goes in over here is y divided by x. It is a five. I hope you got that right. As we do the same thing for the x2 variable, we find when x2 goes by 20. Y only goes up by 50. So we divide 50 by 20 and divide, at 2.5. Now if we left it at this and believe this is the formula and this number of here doesn't exist then what we will predict for this value over here is 20 times 5 plus 80 times 2.5 and that is 300. So we get 200 too large. So we have to subtract minus 200. And it turns out, the way I made this table, this is actually the correct equation. So you can check any value in this table over here and you'll find that this specific linear equation exactly explains the value over here. Now that's an artifact. In reality we don't have such nice situation, where a, what we call a linear hyperplane explains data so well. But it gives you a sense that the predictor variable, in this case, net worth, could be a factor of more than one input variable and that is called multi-variate regression. And all you have to know about this, there's a piece of software that just does it for you. So you can actually plug data like this into a table and out comes the correct result. So just to practice a little bit more, I'm going to give you a second example. You're trying to predict the price of a house. Again, assume two input features, the size of the house and the age of the house. The house could be brand new, or older, perhaps all the way up to 40 years. And the size in square feet might be 500, 1,000, or 1,500. And obviously any value in between those numbers makes sense in regression. But for the sake of this exercise I want to assume these exact numbers that I graphed over here. Further, I want to call size X1 and age X2, and the house price is called Y. So, my result would be of the type Y equals some number times x1 plus some other number times x2 plus some constant. So let me fill in the numbers, and this example is also a little bit contrived in that you can get an exact answer for those coefficients. But, try to do it in your head. And see how far you get. So to solve this, we do the same trick as before. And again I lined it up at the number so correctly, it's not as easy to do real regression. But, you're going to find that as we increase our size by 500 feet from 500 to 1000, or 1000 to 1500, the price also increases by 500. So that means the ratio of price increased to size increases one. And that goes in over here. So go vertical, we find that if we increase the age by 20 years, our price drops by minus 200. So that means the ratio of those minus 200 that's divided by 20 is minus 10., It's really important to get the minus sign right, because older houses are cheaper in this spreadsheet. And eventually as we try and find the y intercept so to speak. Or put differently the constant over here. We plug in those two constants over here on the left data point. 500 times one would be 500. Zero times minus ten is zero. So we would with these two coefficients only we get a 500 over here. If we need to get all the way to 1,000, so we have to add 500 to it. And that's the correct result. Okay once again this is a multivariate regression. It happens to be an exact fit. If you get into situations where these numbers don't exactly fit, you have to apply a much more complicated formula. You can't just look at the data over here and data over here. You have to look at all the data, but explaining the detailed formula to you is a bit outside the scope of this class. But what you will do is, we'll use the existing software to calculate results for situations just like this Welcome to the mini-project on regressions. If you worked at Enron in 2000 or 2001, chances are you're getting a pretty hefty bonus. How hefty? Well that's what we're going to to figure out in this lesson using the power of regressions. So, if you know someone's salary, if you know someone's stock options, et cetera, et cetera, are you able to predict how much money they're making in their bonus every year. And the answer is, well, maybe, but you have to use a regression to find out. So that's what we'll be doing. In the second part of the mini project, we're going to get a sneak peek at next lesson's material, which is about outliers. Outliers are data points that fall far outside the pattern as a large, and you'll get an idea of, of how those actually affect the result that you get on something like your regression. It can be bigger than you might think. We're going to talk now about something called outliers. I want to just draw a little example over here, to illustrate what an outlier might be. Suppose you're measuring some phenomena. And you're getting this amazing data. It looks almost like a line, and then there's this one data point. Now, in some cases that's really important. In other cases, it might just be the result of a little measurement error. So whoever typed in the data into the spreadsheet, mistyped and instead of saying 20, types 200. It's a very common thing that you get data where some phenomena that's very rare, corrupts individual data points. Either way, this thing is called an outlier, because it lays way outside the other data. And I'm going to give you a little quiz here. If you did linear regression, and I give you three hypotheses for what the best linear regression is. Which one do you think is the best of the three? This one? This one? Or, this one? Please check the appropriate line, if you think under consideration of the outlier. And assuming that linear regression minimized the least squared error. Tell me, which you think are the best to describe the data? None of them are perfect The line over here would describe the data, ignoring the outlier. But it will give us a huge least squares error over here, which has an enormous effect on the line. This one over here makes kind of no sense, because even though it pulls a little bit on the right side, it pulled the left side down, and gives a huge least squares error over here. So that really leaves this one as the most plausible line. And in reality, you might get something just like this. With a totally three over here is balanced with many of these squares arrows on this side over here So one of the questions we always have is what causes outliers? And I give you a couple of hypotheses, one is sensor malfunction, one is data entry errors. If someone types in data by hand, it's really easy to just get a digit wrong. One is aliens, and one is freak events. Things that happen very, very rarely, but they do happen, and they might even be important. Sort of like plausible ways to explain outliers. And in this question, there's no best answer. I would say a very common one is sensor malfunction. So you often get sensors when a malfunction, they give it a zero. So, these zeros are often outliers. We have them a lot in robotics. Data entry errors are common when someone enters your data. I'd say aliens are not common causes of outliers. You might be able not to believe, but I won't check them, sorry. About freak events, sometimes you get a situation that is very unlikely. But that combination caused an extreme data point. And these are outliers you want to pay attention to. Those over here, you'd like to ignore, and those over here, you'd like to pay attention to. So in, in much of machine learning, we try to ignore the outliers because they often really caused by sensor malfunction. And when you over-fit them, you get really bad results. But there are fields, where people pay attention to the outliers. A common one would be fraud detection. Which is a special kind of anomaly detection. Where people really care about, is that something in the data that makes the report look so different, they should pay special attention to it. So, depending on your application, you might hate your outliers. Or you might love them. So let's see if we can get an intuition. And the following examples are clearly contrived. And there's usually no simple answer to it. But bear with me and try to take the most obvious guess. I'm going to ask you the question. Which of the six datasets I'm drawing over here likely has some sort of an outlier? So I'm giving you six different diagrams. And again, the answer is not obvious, but if you feel you'd call this an outlier, check the box and see if you can guess the way we will assess it. And I would say the first one is probably an outlier, this data point over here seems to be really far away from the other Xs. This one isn't, it's non-linear, but it seems the data is very, very consistent. Whereas, here I'd say we do have an outlier because this data point over here is far away. This one is interesting, and Katie and I debated it. You might call it an outlier or not. I did. And since I do this video, I get to choose. She didn't. I would say it's not outliers in space but also in class labels, but it could also be just a single example of a different class that's not an outlier. So that's unclear. This one to me looks more clear, the data point further away. And a single data point called an outlier is kind of, I don't know, crazy. So I would never call this thing here an outlier. So, I'm going to give you a practical way to detect outliers that work with almost every machine algorithm. It's actually really straightforward, and it's very very beautiful. Suppose you have this wonderful data set over here, with one outlier. Obviously, you don't know what the outlier is, because you haven't you haven't even discovered the structure of the linear set. The algorithm is very simple. Step one, Train with all the data. In our case, it would be linear regression. Two is Remove. Find after training, the points in your training set with the highest visitor error, and remove those, perhaps usually remove 10% of your data points. And, step three is Train Again, using now the reduced data set. And, you can actually repeat this if you want, and do it multiple times. But, now our example over here, what it means is, the first time we run the regression, we get something that looks approximately like this. And, while this is not a good regression, it is good enough to recognize that if you look at all the visible errors of data points, that this one over here has the largest. This happens to be ten points, so 10% of removal would remove exactly one point. So, we take this point out over here. Our new regression line would look pretty much like this. Which is what you want. So, here's another data set, which it does have outliers. And, I want to, you imagine to fit a linear regression to this data, all the data points, and look at the resulting error of each data point, often called residual error, an error that the data point has after you fit the best possible line. And, I'm going to focus on three data points here. This one, called A, B, and C, and I want you to check which of the three points you believe with the highest error, the highest residual error after training, after you fit a linear regression to this. I'm now going to practice my hand-drawn data fitting skills. But I think the line would look a bit like this. It'd be closer to C than to B, because there's other points pulling on it over here. It'd be really close to A. So B would have the highest Let's now do a residual outlier rejection. Let's assume we remove point B, we now refit the line with linear regression. Would the new regression line after outlier rejections, be steeper, shallower or would the steepness of the line be unchanged? Check one of the three And this one has a clear answer after we move in point b, our best fit will move down on the right side, but not on the left side. So, as a result it will be a little bit steeper than before and it would have been the right answer So we learned a lot about outliers and outlier rejection. Rejection is what you're going to do to clean up your fit, but if you're into anomaly detection or under fraud detection, you reject the good data points, and you look at the outliers. Either way, a really great algorithm that can be looped on any machine algorithm is first train, then remove the points with the largest error, often called the residual error. And for the remaining points, you re-train. And if you so wish, you could repeat these removal and re-train procedure. But you normally converge to a much, much better solution than if you kept all the original data. And as I said before, the fraction of data you often remove is something in the order of 10%, but it might vary from application to application. So correlations, you know about outlier rejection, a really important tool in the arsenal of machine learning algorithms. Welcome to the mini project on outliers. As you saw in the last lesson, having large outliers can have a big effect on your regression result. So in the first part of this mini project, you're going to implement the algorithm that Sebastian has suggested to us. So what that means is you take the 10% or so of data points that have the largest residuals, relative to your regression. You remove them, and then you refit the regression, and you see how the result changes. You'll be implementing that algorithm in this mini project. The second thing we'll do is take a closer at the Enron data. This time with a particular eye towards outliers. You'll find very quickly that there are some data points that fall far outside of the general pattern. So we'll talk about these explicitly, and whether this means they should be removed or they should be given extra special or extra heavy consideration. It's really cool and I think you will really enjoy it. So Katie, this is going to be a unit on unsupervised learning. Unsupervised learning is something that's very important, because most of the time, the data that you get in the real world doesn't have little flags attached that tell you the correct answer. So what are you to do as a machine learner in that case? You turn to unsupervised techniques to still figure something out about that data. Okay, let's talk about them. Given a dataset without labels over all the data points are of the same class. There are sometimes still things you can do to extract useful information. Like this dataset over here, where I would say this dataset is structured in a way that is useful to recognize for a machine learning algorithm. When we look at this by eye, it looks like there's clumps or clusters in the data. And if we could identify those clumps or clusters, we could maybe say something about a new, unknown data point and what its neighbors might be like. Or here's a second example of data. Maybe the data looks just like this. There's something we can say here as well. Right. So all the data in this example looks like it lives on some kind of line or some complicated shape that you seem to be drawing in there right now. Yeah. And it's, it's, it's used to be a two-dimensional space, with x and y over here. But some of it we can reduce it to a one-dimensional line. So that's called what? That's called dimensionality reduction, usually. Dimensionality reduction. So we learned about, a little bit about clustering. Clustering is what we'll learn in this lesson. And you can see here an example of something also called unsupervised learning of dimensionality reduction. Which we will get in a future lesson. So these kind of things where you find structure in the data without labels, they're called unsupervised learning. And we're now gong to dive into the wonderful, wonderful magical land of unsupervised learning. Sounds great, let's dive in. So here's an example that should make it intuitively clear the clustering sum doesn't make sense. So take Katie and me, we both have a movie collection at home. And just imagine that both of us look at each other's movies, and all movies, and Katie gets to rank them from really, really bad to great. And I to get to rank the same movies from bad to great. Now it so turns out that Katie and I have very different tastes. Maybe some movies that I love, like all my James Bond movies, but Katie doesn't like as much. And there's others or these chick flicks, that Katie loves, and I don't. So somewhat exaggerated. It mind end up, that our movies fall into different classes, depending on who likes which movies. So say, say you're Netflix, and and you look at both my queue and Katie's queue, and you graph it like that. Then you can conclude, wow, there's two different classes of movies. Without knowing anything else about movies, you would say, here's class A and class B. And they're very different in characteristics. And the reason why Netflix might want to know this is next time Katie comes in, you want to kind of propose a movie that fits into class B and on to class A. Otherwise, she's very unlikely to watch this movie. And conversely, for me, you want a reach into class A versus class B. In fact, if you were to look at those movies, you might find that old style westerns are right over here. And modern chick flicks might be sitting over here. Who knows? But that's an example of clustering. Because here, there's no target labels given. Then they move down for you if class A and B existed. But after looking at the data, you could, through clustering, deduce as two different classes, and you could even look a the movie titles to understand what these classes are all about. The perhaps the most basic algorithm for clustering, and by far the most used is called K-MEANS. And I'm going to work with you through the algorithm with many, many quizzes for you. Here is our data space. And suppose we are given this type of data. The first question is intuitively, how many clusters do you see? Truth telling, there is not a unique answer, it could be seven it could be one, but give me the answer that seems to make the most sense Okay, and I would argue it's 2. There's a cluster over here. And a cluster over here. And the cluster centers respectively lie right over here and somewhere over here. So that's the place we would like to find to characterize the data. In k-means, you randomly draw cluster centers and say our first initial guess is, say, over here and over here. These are obviously not the correct cluster centers. You're not done yet. But k-means now operates in two steps. Step number is assign and step number two is optimize. So let's talk about the assignment. For classes in number one, I want you to click on exactly those of the red points that you believe are closer to center one than center two. And the answer is this guy is closer these guys over her are closer. And the way to see this is you can make a line between the cluster centers and then draw an equidistant and orthogonal line and that line separate the space into a half space that's closer to center number one, which is the one over here, and a half space that's closer to center number two, which is the space over here. So now we know that these four points correspond to the present classes in the one that was randomly chosen. And these three points over here correspond to classes in the middle. That's the assignment step. obviously that's not good enough. Now we have to optimize. And what we are optimizing is, you are minimizing the total quadratic distance. Of our cluster center to the points. We're now free to move our cluster center. I think of these little blue lines over here as rubber bands, and that we're trying to find the state of minimum energy for the rubber bands, where the total kinetic error is minimized. And for the top one, I'm going to give you three positions. They're all approximate, but pick the one that looks best in terms of minimization. And that's a not a trivial question at all. So one could be right over here, one could be at and one could be right over here. Which one do you think best minimize, or is minimize of the three positions, the total quadratic length of these rubber bands? So after the assign step, you can now see these little blue lines over here that marry data points to cluster centers. And now we're going to think of what this rubber bands. They're rubber bands that like to be as short as possible. In the optimize step, we're not allowed, now allowed to move the green cluster center to a point where the total of our band is minimized. Same exercise for the optimization step for the center below. I give you a couple of hypotheses, four in total. Pick the one that minimizes the total bubble length should be easy now And then argue this is the one that minimizes it. In fact, a set of summary over here minimizes the total of events in the bottom case. And we can argue where exactly it falls, but you get the principle So, let's do it again now. With these new cluster centers, pick the one up here and click on all of the seven data points that you now believe will be assigned for the cluster center on the left And this example is now easy. You can see that all the four over here fit with the green one. In fact, the separating line will be somewhere here and, in fact, after the next iteration of optimize, with the assignment of those point, four points of this cluster center and those three points of this cluster center. You can see that this cluster center will move straight into the center of those four points. And this cluster center will move to the center of those three points. Have we truly achieved our result? We now have assumed it's two clusters. But our algorithm of iteratively assigning and optimizing has moved the cluster center straight into what we would argue is actually the correct centroid for those two clusters over here. That is called the k-Means algorithm. So you learn about k-means. Katie is going to give you one more example of how to apply k-means in practice. Now I want to show you a visualization tool that I found online that I think does a really great job of helping you see what k-means clustering does. And that should give you a good intuition for how it works. So I'd like to give a special shout out to Naftali Harris, who wrote this visualization and very kindly agreed to let us use it. I'll put a link to this website in the instructor notes that you can go and play around with it on your own. So it starts out by asking me how to pick the initial centroids of my clusters. I'll start out with Randomly right now. What kind of data would I like to use? There are a number of different things here, and I encourage you to play around with them. A Gaussian Mixture has been really similar to one of the simple examples we've done so far. So Gaussian mixture data looks like this. These are all the points that we have to classify. The first question for you is, how many centroids do you think is the correct number of centroids on this data? And I hope you said three, it's pretty obvious that there should be three centroids here. So let's add three, one, two, three. So they're all starting out right next to each other, but we'll see how as the algorithm progresses, they end up in the right place. One of the things that's immediately apparent once I start assigning my centroids, with these colored regions, is how all the points are going to be associated with one of the centroids, with one of the clusters. So you can see that the blue is probably already in reasonably good shape. I would say that we got a little bit lucky in where the, the initial centroid was placed. It looks like it's pretty close to the, the center of this blob of data. With the red and the green it looks like they're sitting kind of right on top of each other in the same cluster. So, let's watch as K-means starts to sort out this situation and get all the clusters properly allocated. So, I hit Go. The first thing that it does is it tells me explicitly which cluster each one of these points will fall into. So you see, we have a few blue that fall into the wrong cluster over here. And then, of course, the red and the green. So this is the association step is all the points are being associated with the nearest centroid. And then the next thing that I'll do is I'm going to update the centroid. So now, this is going to move the centroids to the, the mean of all of the associated points. So in particular, I, I expect this green point to be pulled over to the right by the fact that we have so many points over here. So let's update. Now this is starting to look much better. If we were to just leave everything as is, you can see how the clustering was before. So now all these points that use to be green are now about to become red. And likewise with a few blue points over here. You can see how even just in one step from this bad initial condition, we've already started to capture the structure in the data pretty well. So I'm going to reassign the points. Iterate through this again to reassign each point to the nearest centroid. And now things are starting to look very, very consistent. There's probably just one, one or two more iterations before we have the centroid's right at the middle of the clusters so I update and reassign points. No points have changed so this is the final clustering that would be assigned by k-means clustering. So in three or four steps, using this algorithm, I assigned every point to a cluster and it worked in a really beautiful way for this example. Now I'm going to show you another set of data that won't work out quite so perfectly, but you can see how k-means clustering is still. And the type of data that I'll use in this example is uniform points. This is what uniform points look like. It's just scattered everywhere. So I wouldn't look at this and say there's clear clusters in here that I want to pick out, but I might still want to be able to describe that, say, these points over here are all more similar to each other than these points over there. And k-means clustering could be one way of mathematically describing that, that fact about the data. So I don't a priori have a number of centroids that I know I want to use here, so I'll use two. Seems like a reasonable number. One, two. And then let's see what happens in this case. Few points are going to be reassigned. Move the centroids. If you can see that there's a few more little adjustments here. But in the end, it basically just ends up splitting the data along this axis. If I try this again, depending on the exact initial conditions that I have and the exact details of how these points are allocated, I can come up with something that looks a little bit different. So you can see here that I ended up splitting the data vertically rather than horizontally. And the way you should think about this is the initial placement of the centroids is usually pretty random and very important. And so depending on what exactly the initial conditions are, you can get clustering in the end that looks totally different. Now, this might seem like a big problem, but there is one pretty powerful way to solve it. So let's talk about that. Now that I've explained the theory of k-means clustering to you, I'm going to show you how to use the scikit-learn implementation to deploy it in your own studies. So I start over here at Google, and I find that there's a whole page on clustering in scikit-learn. The first thing that I notice when I get to this page is that there are many types of clustering, besides just k-means clustering. So all of these different columns right here are different types of clustering. We won't go into all of these, instead I want to use this page to navigate to the k-means documentation that you can get a little bit of a better idea of how this is handled in scikit-learn. So here's a list of all of the different clustering methods that I have. And here the first item on the list we see is k-means, and some summary information about the algorithm. And so one of the parameters that you have to define for k-means is the number of clusters. Remember, we had to say at the outset how many clusters we want to look for and this is one of the things that can be most challenging actually about using k-means is deciding how many clusters you want to try. Then he gives us some information about the scalability, which basically tells us how the algorithm performs as you start to have lots and lots of data, or lots of clusters. A use case, which gives us a little bit of information that this is good for general purpose when you have clusters that have even number of points in them and so on. And, last, that the way that k-means clustering works is based on the distances between the points. So, very consistent with what we've seen so far. Let's dig in a little bit deeper. Now we're at the k-means documentation page. And there are three parameters in particular that I want to call your attention do. First and most important one is n_clusters. The default value for n_clusters is eight. But of course we know that the number of clusters in the algorithm is something that you need to set on your own based on what you think makes sense. This might even be a parameter that you play around with. So you should always be thinking about whether you actually want to use this default value, or if you want to change it to something else. I can guarantee you that you're mostly going to want to change it to something else. The second parameter that I want to call your attention to is max_iter=300. Remember that when we're running k-means clustering we have an iteration that we go through as we're finding the clusters, where we assign each point to a centroid and then we move the centroid. Then we assign the points again. We move the centroids again. And each one of those assign and move, assign and move steps is an iteration of the algorithm. And so max_iter actually says how many iterations of the algorithm do you want it to go through. 300 will usually be a very reasonable value for you. In fact most of the time I would guess that it's going to terminate before it gets to this maximum number. But if you want to have a finer level of control over the algorithm and how many times it goes through that iteration process this is the parameter that you want. And then the last parameter that I'll mention, another one that's very important. Is the number of different initializations that you give it. Remember we said that k-means clustering has this challenge, that depending on exactly what the initial conditions are, you can sometimes end up with different clusterings. And so then you want to repeat the algorithm several times so that any one of those clusterings might be wrong, but in general, the ensemble of all the clusterings will give you something that makes sense. That's what this parameter controls. It's basically how many times does it initialize the algorithm, how many times does it come up with clusters. You can see that by default it goes through at ten times. If you think for some reason that your clustering might be particularly prone to bad initializations or challenging initializations, then this is the parameter that you want to change. Maybe bump the number of initializations up to a higher number. But again, just to reiterate, of all those parameters, number of clusters is definitely the one that's most important. And that you should be playing around with and thinking really hard about the most. Now this wraps up what we're going to talk about in terms of the k-means algorithm. What I'll have you do is practice much more in the coding aspects of this in the mini project. But before we do that, here are few thoughts on things that k-means is very valuable for and a few places where you need to be careful if you're going to try to use it. So now we look at the limits of what k-means can or cannot do, and you're going to try to break it. And specifically, talk about local minima and to do this, I want to ask you a question that you can think about and see if you get the answer right. Suppose you use a fixed number of cluster centers, two or three or four. Will the output for any fixed training set, always be the same? So given a fixed data set, given a fixed number of cluster centers, when you run k-means will you always arrive at the same result? Take your best guess. And the answer is no, as I will illustrate to you. K-means is what's called a hill climbing algorithm, and as a result it's very dependent on where you put your initial cluster centers. So let's make another data set. In this case, you're going to pick three cluster centers and, then, conveniently, we'll draw three clusters onto my diagram. Obviously, for three cluster centers, you want a cluster to be here, right here, and right over here. So my question is, is it possible that all these data points over here are represented by one cluster, and these guys over here by two separate clusters. Given what you know about k-means, do you think it can happen that all these points here fall into one cluster and those two fall into two clusters as one what's called a local minimum for clustering. And the answer is positive, and I prove it to you. Suppose you put one cluster center right between those two points over here and the other two somewhere in here. It doesn't even have an error. In your assignment step, you will find that pretty much everything left of this line would be allocated to the left cluster center. And as a result, this is the point where the total rubber band distance is minimized. So this cluster is very stable. These two guys over here, however, separate between themselves the data on the right. And they will fight for the same data points and end up somewhere partitioning the cloud on the right side. And that is a stable solution because in the assignment step, nothing changes. This guy will still correspond to all the guys over here, and these guys will correspond to the guys over here. That's called a local minimum. And it really depends on the initialization of the cluster centers. If you had chosen these three cluster centers as your initial guesses, you would never move away from it. Thus, make sure it's really important in clustering to be aware of the fact it's a local here climbing algorithm. And it can give you suboptimal solutions that, if you divide them again, it gives you a better solution. Obviously, in this case with three cluster centers, you want them over here, over here and just one on the right side over here. Let me give another example and ask you a quiz. Suppose we have data just like this over here. Do you think there could be a local minimum if you initialize this data set with two cluster centers? Is there a stable solution where which the two cluster would not end up one over here and one over here? Or put differently, is there, does there exist a bad local minimum? Yes or no? And I would say the answer's yes. You could make it so that the cluster centers sit right on top of each other, and the separation line looks like this. And all the top points are associated to the top cluster center, and all the bottom points are associated to the bottom cluster center. Granted, it's unlikely, to have init-, ,initialization like this, but if it happens, then the algorithm would believe this is one cluster. And this is another cluster. If we re-run it and you initialize differently. Say one of the cluster centers sits over here. Then a separation line will fall like that. And the classes would automatically resolve themselves. It's unlikely, but there exists a bad local minimum, even in the example I showed you over here. Now as a rule of thumb, the more cluster centers you have, the more local minima you find. But they exist, as a result, you are forced to run the algorithm multiple times. Welcome to the mini project on k-means clustering. You know a lot about k-means clustering at this point, but it's been all on sort of made up datasets for pedagogical purposes. Now you're going to be doing k-means clustering on real data, in this case, the Enron finance data. And as we sometimes do in the second part of the mini project, we're going to give you a sneak peak at the next lesson, which is on feature scaling. Feature scaling is some pre-processing that you can do on your features, and it will actually change the output of your clustering algorithm after you've performed feature scaling. So you'll get an idea of what's at stake when you do something like that Welcome to the lesson on feature scaling. Feature scaling is an important step in pre-processing your features for some types of machine learning algorithms. We'll talk a little bit later in this lesson exactly where you need to use it. Let me start out by giving you some intuition for what it accomplishes and what can go wrong when you don't do feature scaling. So let's suppose I have a buddy Chris and Chris wants to know what size T-shirt he should wear. For some reason he doesn't know his t-shirt size but he does know two good friends Cameron and Sarah and they each know their t-shirt sizes. Chris can than get some data from Cameron and Sarah about say how tall they are and how much they weigh. And based on this information Chris can tell what size t-shirt he should be wearing based on whether he is closer in size to Cameron or to Sarah. So here are some numbers to make this more concrete. Chris is tall and very skinny. He weighs 140 pounds and he's 6.1 feet tall. Cameron is also fairly tall, but not quite so skinny. He clocks in at a 175 pounds and 5.9 feet. And Sarah's pretty petite. She's 115 pounds and 5.2 feet tall. Cameron of course wears a size large T-shirt, Sarah wears a size small, and we'll assume for the sake of argument that there's no such thing as a medium in this example. So my first question for you is just your intuition. Should Chris wear a large like Cameron or a small like Sarah, based on his height and his weight? And I would say that Chris would be much happier in a large. He does weigh less than Cameron does, but he's quite tall, a bit taller than Cameron, and certainly much taller than Sarah. So I think putting him in a small would be pretty uncomfortable for Chris Now let's suppose you're a computer who's trying to answer this question. And you don't have any particular knowledge about what height and weight are. But you can compute numbers pretty easily. And the metric that you'll use is the height plus the weight. Whoever Chris is closer to in the height plus weight metric, that's the size that he'll wear. So the first question is, what is the height plus weight for Chris. Height should be measured in feet and weight in pounds. Write your answer in the box. And for Chris the height plus weight is 140 pounds plus 6.1 feet, 146.1. Next, let's do Cameron. What's his height plus weight? Write your answer in the box. The height plus weight for Cameron is going to be 175 plus 5.9, or 180.9. And last let's do Sarah. What's the height plus weight for Sarah? Sarah will be 115 plus 5.2 or 120.2. And now here's the most important question of all. Who is Chris closer to in height plus weight? Remember this will be basically the answer of what size shirt he should wear, if you're a computer doing this calculation. Should he be wearing a large shirt like Cameron, or a small like Sarah? And the answer, very counterintuitively, is that by this metric of height plus weight, he should be wearing a small like Sarah. Now remember, using our intuition, we said he should be wearing a large like Cameron, so what went wrong here? So the way that I got the result that Chris, should be wearing the same size tee shirt as Sarah rather than Cameron, is that I compared these numbers that I computed for each of these people. And I said is Chris closer to Cameron's number or to Sarah's number, and as it turns out, he's about 26 away from Sarah and he's about 35 away from Cameron. Closer to Sarah. Now what went wrong here is that this metric of height plus weight has two very imbalanced features in it, height and weight. So here's what I mean by that, the height is going to be a number that generally goes between let's say, the numbers of five and seven, the weight, on the other hand, takes on much larger values. Between 115 and 175 pounds in this example. So what ends up happening when you compute the sum of the two of them, is that the weight almost always will completely dominate the answer that you get. And height ends up being effectively a rounding error. Whereas what you probably want is something where the two features are equally weighted in, in the sum when you add them together. And this is what feature scaling does. It's a method for re-scaling features like these ones, so that they always span comparable ranges, usually between zero and one. So then, the numbers that you get from height will be between zero and one, they'll still contain the same information. But just expressed in different units. And the weight will also be expressed between zero and one. Again, you'll still have the information there, that Cameron raised the most and Sarah raised the least, but it'll be expressed over this much smaller range. Then when you add them together, weight won't completely dominate the equation anymore. And when that happens, you should get a much more sensible result for Chris's t-shirt size. Because even though he's a little bit closer to Sarah in weight, he's a lot closer to Cameron in height and so he'll probably end up getting grouped with Cameron. In the next video I'll show you the equation for feature scaling. One of the nice things about feature scaling is that it's fairly straightforward. There's a formula, and you just have to apply it to your data. Here's the formula. x prime is the new feature that you're making. And you use information about the old features to figure out what value it should take. x min and x max are the minimum and maximum values that are taken by the old feature before it was rescaled. And x is the old value of the individual feature that you're rescaling. So just to make this more concrete, let me give you a quiz. Suppose the old weights, the feature that we want to rescale, are 115, 140 and 175 pounds. If this is the old feature, what is x min? Write your answer in the box. In this example, x min is going to be 115, because it's the minimum value that's taken on by the old weights. If this is our input data what is x max? X max is going to be 175 pounds because it's the greatest value of this feature that we see it take on before we do any rescaling. And now I want you to use this information to rescale a feature. So the feature I want you to rescale is to find x prime of 140, so what is the 140 pound feature after you've done the rescaling? Remember, the x min we just found is 115, and x max is 175. So you figure out what to plug into this formula to find x prime 140, the rescaled value of the 140 pound person's weight. The answer that you get when you us this formula is 0.417. So 0.417 is the new value of the feature that belongs to the 140 pound person. So the way we did this is we take our x min and x max. We plug them into the formula. And we plug in the 140 as well in the position for the sort of old value of x. And from that we can compute x prime. And one of the features of this formula for rescaling features is that your transform features will always be between 0 and 1. Now this has some benefits and some disadvantages. One of the benefits is you have a very reliable number as to what you can expect in the output. One of the disadvantages is that if you have outliers in your input features then they can kind of mess up your rescaling because your x min and your x max might have really extreme values. So now what I want to do is I want to give you a programming quiz. Where I have you actually program this up yourself. So now, I'm going to give you a programming quiz to have you actually implement this yourself using Python. What I want you to do in this quiz, is to write a function that implements a min/max rescaler, which is what we just did by hand. It transforms the features so they have a range of outputs between 0 and 1. And the way you do that is using the formula that I just introduced to you. So I will give you a list of input features, and your function should return a list of output features that have been rescaled according to this formula. So I hope you were able to get your min/max rescaler working. Hopefully it wasn't too difficult, but as you'll see in the next video, there's actually some tools that sklearn has built in so that you can do that very elegantly in just a single line of code. In this video I'm going to show you how to do the MinMaxScaler example that we just did by hand but now in sklearn. So the MinMaxScaler, as always, is something that I first find out about by visiting the sklearn documentation using Google. So when I google for an example I get something like this. And I click on the first link. There's a number of different functions that Python has available to you for preprocessing data. We'll be looking at the MinMaxScaler today, and that falls under this paragraph of scaling features to a range. This is the documentation page that I had open when I came up with the example that I'm going to show you right now, so in order to make it maximally easy to follow, I would suggest that you sort of have this open in a parallel tab in your browser. But I'll also be explaining each step as I go through it, to show you how to use this in a practical example. I come over in my Terminal window and I fire up the Python interpreter. And the first thing that I am going to need to do is import the module. The next thing that I actually need to do, this is one of the things that was tricky for me the first time around. Is import numpy, because as you might have sort of noticed by now, a lot of the sklearn functions require numpy inputs. So let me show you what I mean. I'm going to be using weights data in this example, so I'm going to create a feature called weights, and as it happens I'm going to make this into a numpy array. I just got this from the example on the sklearn documentation page that I showed you. This is kind of me following the steps that they laid out. What I do now is I make my input data. So in this case, I'll be using the weights example. And something that's a little bit tricky that I had to play around with to figure out, is that what you actually want is a numpy array. Each element of the numpy array is going be a different training point, and then each element within that training point is going to be a feature. So in this case I'm really only interested in one feature, the weights feature, and then I have three different training points for my three different people. So when I put in my data, it looks like this. Now that I have my weights data input I'm ready to use the scaler on that. Here's the scaler that I imported from sklearn.preprocessing. And I want to have a new feature that's named rescaled, let's call it rescaled_weight. And to make rescaled_weight, what I do is call fit_transform on the weights. So this actually does two things. The first thing that it does is it fits. So it does things like find the x_min and the x_max, and then the transform actually applies the formula to all the elements in the, in the set of data. So this is doing two things at the same time. Sometimes you'll only want to do one or the other. Now, I've actually set myself up to make a mistake here. Let me show you what happens. All right. So we got an error here that says its going, it's assuming floating point values as, as input, but instead it got integers. And the reason why has to do with this line right here. All these inputs are integers, but it's expecting floating point numbers, and if you were looking at the documentation, the example, very closely as I was doing this, you might have figured out that this was going to be a problem down the road for us. So, I am going to turn these into floating point numbers by putting a decimal point after each one of them and we'll try this again. Great. That looks much better. Let me print it out now. And there is our new transformed feature. So, there is the 0.417 that we calculated in the last example. You can also see that the smallest value in a list has a rescale value of zero, and the largest one has a rescale value of one. Which, if you look at the equation carefully, will immediately become apparent that that's always going to be true. So this is an example of how you can use the MinMaxScaler in sklearn to do in just a couple lines what we did by hand in the last example. Okay, Katie, you did a fantastic job telling people about feature rescaling. And now when I put your knowledge to a test, to see which algorithm you believe is susceptible and impacted by rescaling features, and which ones are somewhat not impacted. So here is a set of algorithms. So which algorithm would be affected by feature rescaling? Here's a set of algorithms. That you know very well. Decision trees, support vector machines, say with a RBF kernel. You might remember linear regression. And I'm going to toss in k-means clustering. It's a pretty non-trivial question. Some of those will give you the same result even if you rescale the features, and some of them will have substantially different results. So check the ones that would be effected. Where the results really depend on the feature rescaling. So Katie, what do you think? Which ones are the right answers here? The right answers, so the, the ones that do need rescaled features will be the SVM and the k-means clustering. So both and, support vector machines in, in, in k-means clustering, you're really trading off one dimension to the other when you calculate the distance. So take, for example, support vector machines. And you look at the separation line that maximizes distance. In there, you calculate a distance. And that distance calculation, trade-offs one dimension against the other. So we make one twice as big as the other, it counts for twice as much. The same is true, coincidentally, for k-means clustering, where you have a cluster center. And you compute the distance of the cluster center, to all the data points. And that distance itself has exactly the same characterization. If you make one variable twice as big, it's going to count for twice as much. So, as a result, support vector machines and k-means both are affected by feature rescaling. So, Katie, tell me about the decision trees and linear regression. Why aren't they included? Decision trees aren't going to give you a diagonal line like that, right? They're going to give you a series of vertical and horizontal lines. So there's no trade off. You just, make a cut in one direction, and then a cut in another. So, you don't have to worry about what's going on in one dimension, when you're doing something with the other one. So if you squeeze this area little area over here to half the size, because you rescale the feature where the image line lies. Well, it'll lie in a different place but the separation is chronologically the same as before. It scales with it, so there's no trade-off between these two different variables. And how about, linear regression? Something similar happens in linear regression. Remember that in linear regression, each of our features is going to have a coefficient that's associated with it. And that coefficient and that feature always go together. What's going on with feature A doesn't effect anything with the coefficient of feature B. So they're separated in the same way. In fact, if you were to double the variable scale of one specific variable, that feature will just become half as big. And the output would be exactly the same as before. So it's really interesting to see, and for some algorithms, rescaling is really a potential if we can use it, for others, don't even bother. Welcome to the mini-project on feature scaling. In the mini-project on K-means clustering, you clustered the data points. And then at the end, we sort of gestured towards feature scaling as something that could change the output of that clustering algorithm. In this mini-project, you'll actually deploy the feature scaling yourself. So you'll take the code from the K-means clustering algorithm and add in the feature scaling. And then in doing so, you'll be recreating the steps that we took to make those new clusters.