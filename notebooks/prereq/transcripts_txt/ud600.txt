Hi, Michael. Hey, Charles, how's it going? It's going fantastically well. How are things going with you? I've been told I can't complain. You could always complain Michael. [LAUGH] I'd rather not. Okay, well fair enough. Okay, well guess what we're going to talk about today? I'm reading Decision-Making and Reinforcement Learning. That's correct. Is that the name of our class? Sure. [LAUGH] I feel like the class has an actual name. Yeah, decision making and reinforcement learning, how do you feel about that? Awesome, I like it a lot except there is kind of an extra hyphen in there. Where? Between decision and making you only put that in if it's modifying another noun. Wait, haven't we had this conversation before? It does seem vaguely familiar. Hm. Oh, you know what it was? When we did our introduction in the machine learning class for decision making and reinforcement learning as a sub-piece, we had a very similar conversation. Oh, so why are we doing it again? Well, I mean do we need to cover the same material? Yeah, absolutely important that we cover some of the same material in the beginning. So maybe we shouldn't do it again, maybe we should kind of cut and paste from the other class. Oh, I like that, we can fix it in post. That's fantastic, let's do that. So why don't we just pretend that somebody's put all the right video stuff in the right place and go do that. How's that sound to you? That sounds like it saves everybody a lot of time, except for the people actually watching the videos. That works for me. Okay, so let's do a flashback. You want to do the flashback sounds? Sure, I know how to do that. Okay, let's go. [NOISE] Hi Michael. Hey Charles. How's it going? It is going quite well. Thank you very much for asking. How are things going with you? I can't complain because I've been barred by a, a judge. [LAUGH] I'm pretty sure you can still complain. Complaining is a fine art that requires lots of practice. Guess what we're going to do today? I'm reading decision making and reinforcement learning which is very exciting. It is. Except the hyphen. Except for the hyphen? Yeah the hyphens wrong. Why? because it's not decision dash making. You're right. I normally don't do that but people always want to put a dash so what I'm going to do is for your Michael. Just for you. I'm going to remove the dash. Thank you. And I'm going to put it over here where it belongs. [LAUGH] No. There. Well, Michael, this is the first of our discussions on the last mini course of machine learning, Reinforcement Learning, which I believe you know a little bit about. I am very interested in Reinforcement Learning. Excellent. So I'm just going to do a little bit of background. It's going to be relatively short and straight forward. We're going to do a couple of quizzes, because I know how much you like quizzes. And then we're just going to go back and forth and see what we get to learn about Reinforcement Learning. Sound good? Excellent. Excellent, okay so, let's get started. Okay Michael so before we get started diving into a particular formalism or framework that I want to talk about, that we are going to use for this mini course or at least the first half or so of it. I want to remind everybody what the differences are between the three types of learning that we said we are going to look at over this entire course. Th, you recall what they are? Well reading off the slides supervised, unsupervised and reinforcement. That's right. And you know, again these things are all strongly related to one other but it's very useful to think about them as being very separate. So just as a reminder, supervised learning sort of takes the form of function approximation where you're given a bunch of x, y pairs And your goal is to find a function f that will map some new x to a proper y, you recall that? Yep. Unsupervised learning is very similar to supervised learning except that it turns out that you're given a bunch of x's and your goal is to find some f. That gives you a compact description of the set of x's that you've seen. So we call this clustering, or description as opposed to function approximation, and finally we're getting to reinforcement learning. Now reinforcement learning is actually a, a name that means many things in different fields, and here we tend to talk about it in a relatively specific way, and superficially it looks a lot like Supervised learning, in that we're going to be given a string of pairs of data, and we're going to try to learn some functions. But in the function approximation case, a supervized learning case, we were given a bunch of X and Y pairs. We were asked to learn F, but in reinforcement learning, we were given something totally different. Were instead going to be given x's and z's, and I'll tell you what the x's and the z's stand for in a minute, and then were going to have to learn some f that is going to generate y's, and so even though its going to look a lot like function approximation, its going to turn out that it's going to have a very different character. And that's really what the, the next few slides in a little bit of discussion is really about, is understanding what that character is. You'll also notice from the title here that I have decision making reinforcement learning, and that's because reinforcement learning is one mechanism for doing decision making. And again, I'll define that in just a second. Okay, so you with me, Michael? I think so. So should y be circled? because in some sense, you, you underlined the things we were given and you circled the things we needed to find, so y is something that we're going to find, right? Yeah, I suppose that I like that. All right. Interesting. Now it's a theta. [LAUGH] It's a Y trapped inside of a theta and it's yelling, Y? [LAUGH] I like that. I'm a Y trapped in a theta. Hm, I need to write a book about that. Okay, good. That's a good point Michael. So before we were learning, effectively trying to learn one thing and here we're still learning one thing. Because it's going to produce another thing for the deterministically, usually. But it's worth pointing out that we are going to be figuring how to produce both of these things as opposed to be given those things. Good job. OK. Are you ready to move forward,l Michael, with an example and a quiz? Awesome. Excellent. Michael, here is a world. In fact, for the purposes of this discussion it is the world. Okay. So imagine the entire universe is well described by this picture over here. Okay? Now this is called a grid world, which is something that people in reinforcement learning love to think about, because it's a nice approximation for all the complexity of the entire universe. Now this particularly world is a three by four grid. So you have one comma one, two comma one, three comma one, four comman one. You have one comma two, one comma three and so on and so forth. For the purpose of this discusision we can think of the world as being a kind of game where you start out in state. Which we're going to call the start state. And your able to execute actions, one of these four, up, down, left to right and the purpose is to wonder around this world in such a way that eventually you make it to the goal over here, this little green spot. You see that Michael? Yep. And under all circumstances, you must avoid the red spot. No. Exactly. Now for this particular example, up does what you think it does, down does what you think it does, as do left and right. But if you find yourself at a boundary, such as right, up here in thi upper-left-hand corner, and you try to go up, you just stay where you are. If you try to go left, you just stay where you are. But if you go right, you do actually end up in the next square. Got it? Think so. Okay. Three last things. One, this little black, space here, is a place you can't enter into, so it acts just like a wall. This green space is the goal, and once you're there it's over. The world is over and you get to start over again. Hm. And once you enter into the red spot, the world is also over and you have to start over again. So you can't go through the red square to get to the green square. Okay, you got it? Yeah. Excellent. So, here's the quiz. Given this particular world, with the physics I just described to you, and given these actions that you can take, up, down, left and right, what is the shortest sequence of actions that would get us from the start state to the goal state? And you can just type in the words up, down, left and right, separated by spaces, or commas or anything like that. Okay. [LAUGH] Say semicolons are allowed, colons are not. Yeah, good. I am glad you made that distinction. Well, it's an important one to make. Okay, you got it? Yeah, I think so. Alright, cool. So lets see what you get then. Go. Okay Michael, you go an answer for me? This isn't meant to be a hard quiz. Oh good. Cause I actually have two answers for you. Oh, I like that. So I would say right, right, up, up, right. Okay, so, right, right, up, up, right. But I feel like I could have also said up, up, right, right, right. And you could have. Both of those are acceptable. right, right does what you think it does. You move right and then right and then up and then up and then right. And that takes five steps. Or you could have gone up, up, right, right, right, which would also have taken you five steps. So that's pretty easy, right? Yeah, I thought so. Rag. So yes, either of these would be correct. We're going to stick with this one in particular, since they're equal, because we gotta pick one of them. So this does point out something that often in a decision problem like this, there are in fact multiple answers or multiple optima, if I can tie it back to our randomized optimization discussions. Okay, so you got the quiz, you got the world, you understand what you can do here. Yes? Yep. Okay, cool. Now I'm going to throw in a little wrinkle. I'm going to change the world just a little bit Michael. Okay, that last one was really easy, and it was easy for a bunch of reasons, but one of the reasons it was easy is that every time it took an action it did exactly what you expected it to do. Now I want to introduce a little bit of uncertainty, or stochasticity into the world. So, let me tell you exactly what that means. When you execute an action, it executes correctly, with probability of 0.8. So 80% of the time, if you go up, it goes up. Assuming you can go up. If it goes, if you say down, it goes down, assuming you can. Left, it goes left; right, it goes right. Got it? Yeah! Now, 20% of the time, the action you take actually causes you to move at a right angle. Now, of course, there are two different right angles you could go to, if you go up, you could go either left or right at a right angle. And so that 20% gets distributed uniformally. Okay? Does that make sense? Yeah, I think so. And so if you, if you're in the start state and you try to go up, then there a 10% chance that you tend to bump into the wall. Yes. And then you just stay where you are I guess. Right. So if you, if you decide that you'd have x here, you have a 80% chance of moving up, you have a 10% chance of moving to the right. And you have a 10% chance of moving to the left but, of course, you'd bump a wall and you would end up right back where you started. Got it? Yeah. Okay, good. So, here is my quiz question for you Michael. You recall, we came up with two sequences and the one that I decided to keep was up, up, right, right, right. My question to you, is, what is the reliability of the sequence, up up, right right, actually getting you from the start to the goal, given these probabilities, this uncertainty? And so, we're just going to try that sequence, and ask whether or not it actually got us to the goal. Exactly. And when I say, reliability, I really mean, what's the probability of it actually succeeding? Interesting. Okay. Alright. So let's see if we can figure out the answer. You're ready? I'm ready. Alright. Go! Okay Michael do you have an answer? I have an answer indeed. Okay, what's the answer? Okay, maybe I don't have an answer indeed. But I have I have the ability to compute one. Okay. I'm, I'm doing some math. So why don't you walk me through the math you're doing. I got .32776. That is correct Michael. Woo-hoo. That was trickier than I thought. Okay well, show me what you did. Alright, so the first thing I did is I said, okay, well this is not so hard, because, from the start-state, if I execute up, up, right, right, right. Each of those things works the way it's supposed to do independently with probability .8. Yep. And so, .8 raised to the 5th power, gives me the probability that that entire sequence will work as intended. Exactly. And what is .8 to the 5th? Do you know? 32,768. with a decimal point in front of it, because you know, powers of 2. Wow. That is correct. But I notice that 32768 is not 32776. No they differ by a little smidge. Mm-hm. So this is what occured to me next is, is there anyway that you could have ended up falling into the goal from that sequence of, of commands not following the intended path. So since actions can have unintended consequences, as they often do. Mm-hm. I was going to ask okay, so if I go up in the first step, there's a probably .1 that I'll actually go to the right. Yep. From there, if I go up, there's a 10% probability that I'll actually go to the right. Mm-hm. From there, the next thing I do is take the right action, but that can actually go up with some probability, .1. Mm-hm, yep. And then another .1 to get to for the next right action to actually cause an up to happen. Mm-hm. And then finally, that last right might actually execute correctly and bring me into the goal. So I could go underneath the barrier, instead of around the barrier with that same sequence. It just isn't very likely. Okay. Well how unlikely is it? Alright. So I did .1 times .1 times .1 times .1 times .8. huh, so the .1 to the 4 times .8 and that's right so this is the probability that the, 4 of the sequences, 4 of these go wrong. In fact exactly the first 4 go wrong. And the last one goes right. Right. And that's equal to, some very, very small number. And when you add it up. You end up with .32776. In fact that's equal to 0.00008. And that's how you get that number and that's correct. And you'll notice that this this sequence happens to work out to be the second sequence that, we had options for. Yeah, the other thing that took us to the goal, right. Yeah. Was exactly the probability of executing that action executing that sequence of transitions given the first command. Yeah, and I think it would work the other way, too. No, it wouldn't quite work the other way. If you had done the sequence right, right, what is it? Right, right. Up, up right? Yeah. In order for that one to work out, you'd add .8 to the 5 and working. And, for it to work out wrong but work out right, the right would have to send you up and then the ups would have to send you right, and then, yeah, so it would actually work out to be the same. Yeah. So no matter which of the two sequences you came up with, they would have the same probability of succeeding. Neat. Nice. That's actually kind of cool. So, good job, Michael. Very good job on the quiz. A lot of people forget this part. And, in fact, if you forgot that part, and got, just this part right, we actually let you pass. But it was wrong. I was kind of expecting you to get it wrong, but I'm glad you got it right too. Thank you Michael, I appreciate your faith in me. [LAUGH] I wrote the quiz. Actually, I stole this from a book, Oh. which, whose little words are now showing up in front of you, exactly where you can go through the details of this quiz. Okay, alright, Michael. So you might ask yourself why I brought this up, and the reason I brought this up is because, what we did in the first case where we just came up with the sequence up up right right right, is we sort of planned out what we would do in a world where nothing could go wrong. But once we introduced this notion of uncertainty, this, this randomness, this stochasticity, we have to do something other than work out in advance what the right answer is, and then just go. We have to do something a little bit more complicated. Either we have to try to execute these, and then every once in a while, see if we've drifted away. And then re-x, re-plan, come up with a new sequence wherever it is we happen to end up, or we have to come up with some way to incorporate all of these uncertainties and probabilities. So that, we never really have to think, rethink what to do in case something goes wrong. So, what I'm going to do next is I'm going to introduce the framework, that's very common, that people use as a way of capturing this stuff, capturing these uncertainties directly. Okay? Hm. You ready? Yeah. Excellent. So this is the framework that we're going to be using through most of the discussions that we'll be having at least on reinforcement learning. The single agent reinforcement learning, and it's called the Markov Decision Process. This should sound familiar to you, Michael. Well, you did say we're going to talk about decisions. That's true, and we need a process for making decisions. And we're going to introduce something called the Markovian property as a part of this discussion and I'll tell you exactly what that means in a moment. So, I'm just going to write out this frame work and just, and tell you what it is and what the problem it produces for us. And then we're going to start talking about solutions through the rest of the discussion. So a Markov Decision Process tries to capture worlds like this one by dividing up in the following way. We say that there are states. And states are a set of tokens that somehow represent every state, for lack of a better word, that one could be in. So, does that make sense to you, Michael? Yeah, I think so. So what would the states be in the world that we've been playing around in so far? So, the only thing that differs from moment to moment is where, I guess, I am. Mm-hm. Like, which grid, grid position I'm in. Right. So, I feel like each different grid position I could be in is a state, maybe there's a state for being successfully done or unsuccessfully done? It's possible. But let's stick with the simple one. I like that one because that's really, I think, easy to grasp. So, there are at least, of all the states one could reach, there's, well let's see there's four times three minus one, since you can never reach this state. Although we could say it is a state we just happen to never reach it. So, at most if we just think of this grid literally as a grid there are something like twelve different states. And we can represent these states as their X,Y coordinates, say. We could call this, the start state as say 1,1, which is sort of how I described it earlier. We could describe the goal state as 4,4. And say this is how we describe our states. Or frankly, it doesn't matter. We could call these states 1,2,3 up to 12. Or we could name them Fred and Marcus. It doesn't really matter. The point is that they're states, they represent something, and we have some way of knowing which state we happen to be in. Okay? Sure. Okay. Alright so we've got states. So next is whats called the model or the transition model. Sometimes people refer to it as the transition function, and basically it is a function of three variables. It's a state, an action, which I haven't defined for you yet and another state. In fact since I haven't defined what an action is for you yet, let's skip that and actually do define actions for you. So the third part of our model are actions. Actions are the things that you can do in a particular state. So what would the actions be in this world? Well the four different decisions I could make were the up, down, left and right. Though it's maybe a little confusing because I think those were also the four possible outcomes. That's true. Well no, there are other outcomes. You could stay where you were. Right. Right. So these are actually the actions. These are the things that when I'm in a given state I'm allowed to execute. I can either go up, go down, go left or go right. You'll notice that as I described before there was no option not to move. Mm. But there could have been, and there could have been other actions, like teleport, or there's anything that you can imagine. But the point is that your action set represents all of the things that the agent, the robot, the person, or whatever it is you're trying to model, is allowed to do. Now, in its full, generalized form, we can think of the set of actions that one can take as being a function of state Because there's some stage you can do some things and some stage you can't do those things. But most of the time people just treat it as a set of actions. And the actions that aren't allowable in them particular states have no effect. Alright, so we understand states. They're the things that describe the world. We understand actions. Those are the things that you can do, the commands you can execute when your in particular states. And what a model describes, what the transition model describes is in some sense the rules of the game that you're playing. It's the physics of the world. So it's a function of three variables: a state, an action and another state, which, by the way, could actually be the same state. And what this function produces is the probability That you will end up transitioning the state s prime, given that you were in state s, and you took action a. Got it? I think so. so the s prime is where you end up, and the s, a is what you're given. So it's, so you plug these three in. Oh I see, and you get a probability Mm-hm. But the probabilities have to add up to one if you if you sum it up over all the s primes. Right. That's exactly right. So for example if you think about the deterministic case where there was no noise then this is a very simple model. If I'm in the state the start state. And I take the action up, then what's the probability, I end up in the state immediately above it? Was that a 0.08? No, in the, in, when we first started in the deterministic world. Oh, that was probability one. Right, and what would be the probability, you ended up in the state to the right? Probability zero. Right. In fact, the probability that you end up at any of the other states is zero in the deterministic world. Now what about the case when we were in the non-deterministic world where an action would actually execute faithfully only 80% of the time. If I'm in the start state and I go up, what's the probability that I end up. In the state above. That was 0.8 Was the probability, I end up in the state to the right. That was 0.1. And, what was the probability I end up where I started. That was also 0.1. Right, and 0 everywhere else. And, that's just sort of the way it works. So, the model really is an important thing. And the reason, it's important. Is it really does describe the rules of the game. It tells you what you, what will happen if you do something in a particular place. It captures Everything that you know about the transition u, of the world is what you know about the rules, got it? You called it physics before? I called it the physics of the world. Huh. These are the rules that don't change. But they're very different from real world physics. Well, yeah, although they don't have to be. I mean in some sense, you could argue that a mark off decision process, what we described so far, these three things In fact, do describe the universe, the states are, you know, in the positions of all the atoms. The positions and velocities of all the atoms in the universe. The transition miles as you do, take certain positions in the world whatever they are how the state of the universe changes in response to that. And the actions or whatever those set of actions could be. And it can be probabilistic or it's not probabilistic. It's definitely probabilistic. The transition models are by their very definition probabilistic. Gotcha. Now, I actually snuck something important in here, I actually snuck two things that are important in here. The first is, the, what's called the Markovian property. Do you remember what the Markovian property is Michael? From what? From, I dunno. Actually, where does the Markovian property come from? I'm going to say Russia. Okay, yeah, from the, from the Russian. Yeah, so I have Russian ancestors, and they passed onto me this idea that. Markov means that you don't have to condition on anything past the most recent state. That's exactly right. The Markovian property is that only the present matters. And they had to pass that down to me you know, one generation at a time, because you know, Markov property. Exactly right, that was very good Michael. So what does this mean? What this means is our transition function, which shows you the probability you end up in some state as prime, given that you're in state S and took action A, only depends upon the current state S. If it also depended upon where you were 20 minutes before then, you would have to have more S's here. And then you would be violating the Markovian property. So is this like, do historians hate this? Well, you know, one never learns anything from history. No, you're supposed to learn from history, or you're doomed to, I don't know, let me make something up, repeat it. [LAUGH] Fair enough. Historians probably don't like this, but there is a way for mathematicians to convince them that they're okay with it. And the way that, you, mathematicians convince you that you're okay with this is to point out that you can turn almost anything, into a Markovian Process by simply making certain that your current state remembers everything you need to remember from the past. I see. So, in general, even if something isn't really Markovian, you need to know what you were, not only what you're doing now, but what you were doing five minutes ago. You could just turn your current state into what you're doing now and what you were doing five minutes ago. The obvious problem with that, of course, is that if you have to remember everything from the beginning of time. You're only going to see every state once and it's going to be very difficult to learn anything. But, that Markovian property turns out to be, turns out to be important and it actually allows us to solve these problems in a tractable way. But I snuck in something else, Michael. What I snuck in is this idea about the transition model, is that nothing ever changes. So this second property that matters for Markov decision processes, at least for the sets of things that we're going to be talking about in the beginning, is that things are stationary. That is these for the purposes of this discussion, it means that these rules don't change. Over time. That's one notion of stationary, okay? Does that mean that the agent can never leave the start state? No, the agent can leave the start state any time it takes the action, then it gets another start state. Then how is it, how is it stationary, then? It's not stationary, the world is stationary. The, the transition model is stationary, the physics are stationary, the rules don't change any. The rules don't change. Right. I, I, okay. I see. Then there's another notion of stationary that we'll see a little bit later. Okay, last thing to point out about the definition of a mark on decision process, is the notion of reward. So, reward is simply a scalr value, that you get for being in a state. So for example we might say, you know, this green goal is a really good goal. And so, if you get there, we're going to give you a dollar. This red dual, on the other hand, is a very, very bad state. We don't want you to end there. And so, if you end up there we're going to take away a dollar from you. What if I don't have a dollar? Someone will give you the dollar. The universe will give you the dollar. [LAUGH] And then take it away? Or, the universe will take it away. Even if you don't have it. You'll have negative dollars. Oh, man. Okay, so Reward is very important here in a of couple ways. Reward is one of the things that, as I'm always harping on encompasses our domain knowledge. So, the Rewards you get from the state tells you the usefulness of entering into that state. Now. I wrote out three different definitions of r here, because sometimes it's very useful to think about them differently. I've been talking about the reward you get for entering into the state, but there's also a notion of reward that you get for entering into a state and taking an action. There's a rewar, or, being in a state and taking an action, there's a reward that you could get for being in a state, taking an action, and then ending up in another state as prime. It turns out these are all mathematically equivalent. But often it's easier to think about one form or the other. But for the purposes of the, you know, for the rest of this discussion, really you can just focus on that one, the reward of the value entering into a state. And those four things, by themselves, along with this Markov property and non-stationarity, defines what's called the Markov Decision Process. Or an MDP. Got it? I'm a little stuck on the, how those could be mathematically equivalent. Well we'll get to that later. Would you like a little bit of intuition? Sure. Well, you can imagine that just as before we were dealing with the the notion of making a non-Markovian thing Markovian by putting a little bit of history into your state. You can always fold in the action that you took to be in a state or the action that you took to get to a state, as a part of your state. But that would be a different Markov Decision Process. It would, but they would work out to have the same solution. Oh, I see. Speaking of solutions, this is the last little bit of thing that you need to know. And that is. This defines a problem. But, what we also want to have, whenever we have a problem. Is a solution. So, the solution to the Markov Decision Process, is something called a policy. And, what a policy does. Is, it's a function, that takes in a state. And returns an action, in other words, for any given state that you're in, it tells you the action that you should take. Like as a hint? No, it just tells you, this is the at. Well, I mean, I suppose you don't have to do it, but the way we think about Markov Decision Processes, is that this is the action that will be taken. I see, so it's more of an order. Yes, it's a command. Okay. So that's all a policy is. A policy is solution to a Markov Decision Process. And there is a special policy, which I'm writing here as policy star, or the optimal policy, and that is the policy that maximizes your long-term expected reward. So if all the policies you could take, of all the decisions you might take, this is the policy that optimizes the amount of reward that you're going to receive or expect to receive over your lifetime. So, like, at the end? Well, at yeah, at the end, or at any given point in time, how much reward you're receiving. >From the Markov Decision Process point of view, there doesn't have to be an end. Okay. Though in this example, you don't get anything, and then at the end, you get paid off. Right, or unpaid off. Right. If you fall into the red square. So actually, your question points out something very important here. I mentioned earlier when I talked about the three kinds of learning that there, supervised learning and reinforced learning were sort of. Similar, except that instead of getting Ys and Xs we were given Ys and, Xs and Zs. And this is exactly what's happening here. Here what we would like to have if we wanted to learn a policy is a bunch of sa pairs as training examples. Well here's the state and the action you should've took, taken, here's another state and the action you should've taken, so on and so forth. And then we would learn a function, the policy, that maps states to actions. But what we actually see in the reinforcement learning world, in the Markov Decision Process world, is we see states, actions, and then the rewards that we received. And so in fact, this problem of seeing a sequence of states, actions, and rewards. It's very different from the problem of being told. This is the correct action to take to maximize a function. Or find a function that maps from state to action. Instead, we say well, if you're in this state, and you take this action, this is the reward that you would see. And then from that, we need to find the optimal action. So Pi star is being the F from that previous slide? Right. And R is being Z? Yes. And y is being a. And s is being x or x is being s Got you. Right. So but, I'm, okay I'm a little confused about this notion of a policy. So we have the, the, the thing we tried to do to get the goals was up, up, right, right, right. Yes. I don't see how to capture that as a policy. It's actually fairly straightforward. What a policy would say is: What state are you in? Tell me what action you should take. So, the policy, basically is this: When you're in the state, start, the start state, the action you should take is up. And it would have a mapping. For every state that you might see, whether it's this state, this state, this state, this state, this state, this state, this state, or even these two states, and it will tell you what action you should take. And that's what a policy is. A policy, very simply, is nothing more than a function that tells you what action to take at every, in any state you happen to come across. Okay, but the, but the. The question that you asked before was about up, up, right, right, right. Mm hm. And, it seems like, because of the stochastic transitions. You might not be in the same state. Like, you don't know what state you're in, when you take those actions. No, so, one of the things for what we're talking about here, for the Markov Decision Process. Is, there're states, there're actions, there're rewards. You always know what state you're in, and you know what reward you receive. So does that mean you can't do up, up right right right? Well, the way it would work in a Markov Decision Process, so what you're describing is is what's often called a plan. You know, it's, tell me what sequence of actions I should take from here. What Markov Decision Process does and what a pr, a policy does is it doesn't tell you what sequence of actions to take from a particular state. It tells you what action to take in a particular state. You will then end up in another state because of the transition model, the transition function. And then when you're in that state you ask the policy what actions should I take now? Okay. Right, so this is actually a key point. Although we talked about it in the language of planning, which is very common for the people who di, for example take any ag course, the thing about this in terms of planning, what are the things that I can do to accomplish my goals? The Markov Decision Process way of thinking about it, the reinforcement way of thinking about it, or the typical reinforcement learning way of thinking about it, really doesn't talk about plans directly. But instead, talks about policies. Which from which you can infer a plan, but this has the advantage that it tells you what to do everywhere. And it's robust to the underlying stochastic of the word. World. So, is it clear that's all you need to be able to behave well. Well, it's certainly the case, that if you have a policy and that policy is optimal, it does tell you what to do, no matter what situation you're in. 'Kay. And so, if you have that, then that's definitely all you need to behave well. But I mean could it be that you wanted to do something like up, up, right, right, right which you cant write down as a policy? And why cant you write that down as a policy? Because the policies are only telling you what action to do as a function of the state not sort of like how far along you are in the sequence. Right unless, of course, you fold that into your state some how. But thats exactly right, the way to think about this is. The idea of coming up with a concrete plan of what to do for the next 20 time steps is different from the problem of whatever step I happen to be in, whatever state I happen to be in, what's the next best thing I can do? And just always asking that question. Hm. If you always ask that question, that will induce a sequence, but that sequence is actually dependent upon the set of states that you see. Whereas in the other case where we wrote down a particular policy, you'll notice that was only dependent upon the state you started in and it had to ignore the states that you saw along the way. And the only way to fix that would be to say, well, after I've taken an action, let me look at the state I'm in and see if I should do something different with it. But if you're going to do that, then why are you trying to compute the complete set of states? Or I'm sorry, the complete set of actions that you might take. Okay. Okay, so there you go. Now, a lot of what we're going to be talking about next Michael, is, given that we have MDP, we have this Markov Decision Process defined like this. How do we go from this problem definition to finding a good policy, and in particular, finding the optimal policy? That makes sense. Good. And there you go. Okay, so, I want to talk about two things on this little slide, here. The first one is kind of a general observation about rewards and what makes the reinforcement learning MDP problem different from the supervised learning problems that we did before. And that's the notion of not just rewards, but this notion of delayed rewards. So, what do I mean by that. Well, so, if you think about the way we've set up kind of problem like, like we have here where we're trying to. Start in this little bottom left-hand square, and wind our way up into this plus one. There's really this notion of sequences of action. So in the reinforcement learning context, and all the things that we're talking about, at least in the foreseeable future, there's this sort of problem where you take some action and that puts you in some place and then you take another action and that puts you in some place. And then you take another action and that puts you in some place and maybe it puts you at a place where you get plus one. Or maybe it puts you at a place where you get minus one. And what really is going on here is this idea that you take actions that will set you up for other actions that will set you up for other actions. And only then do you know how good those particular actions you took were. So this reward is not just an idea of getting a reward at every state, it's an idea of getting delayed reward. So you don't know how your immediate action is going to lead to things down the road. So, let me give you a concrete example about that. So, have you ever played chess? Sure. So let's say we played a long game of chess and maybe took 60, 61, 62 moves. And then at the end, I win the game. So, what do you know about the way you played that game? That I probably made a bad decision around the time when I decided to play chess against you. That's possible, but maybe you made a good decision and you kept making good decisions, and you only messed up at the very last move, when you could have mated me but you didn't. I see. Well you, what, my experience in playing chess is that usually, that's not what happens. Usually it's not that I make a bad move and then there's a problem. It's usually I make a move that I think is reasonable that only later I discover put me in a position where I can't possibly do anything good. Right. Well that's actually the game that I played in New York many, many years ago that I lost was exactly like that. The game went on for like literally a hundred moves. I really lost the game on the third move. Oh. I made a mistake, i transposed two moves because it was a new opening that I was just learning and I knew at the time that i screwed up, i played beautiful chess from that point on, but the truth is the other player had a positional advantage from that point on that I could never overcome, so I lost the game, but not because I played poorly for, you know, 80 moves. It's because I paid, played poorly for one move, and that move happened to be fairly early. So this is this notion of delayed reward, that I played this long game of chess, and maybe it's I played well, and I screwed up in the end. Maybe I played medio, you know, mediocre game, but I had a couple of brilliant moves, and that's why I won, or maybe I played very well in the beginning, poorly at the end or the other way around. And the truth is you don't really know. All you know is that you're taking a bunch of actions. You get rewards signals back from the environment. Like I won the game or I lost the game. And then you have this problem of figuring out of all the actions that I took, what was the action that led to me ultimately winning or losing, or getting whatever reward that I, I got at the end of a sequence. Does that make sense? Yeah it seems like that could be really challenging. You probably need your own like sportscaster listening and, and commenting. Yeah and in fact the sportscaster who is listening and commenting at least in the NDP world is in fact the sequence of rewards that you get in the sequence of states that you see right? It really is sort of a play by play. In this state this action got this reward and you want to take all of that and figure out whether this was a good action you're first action or a poor action or your second action was good or poor and so on and so forth. Now contrast that with supervised learning, so in supervised learning what you would be getting is while I was in this state This first date and then this was the proper action I was supposed to take lets say action seventeen. And your goal then is just to simply learn a function from states to specific actions, that's how the supervised learning problem is setup, right? Yeah exactly. In this particular case you're in some state you take some action and you get some reward for the action you took. Or may be the state that you ended up in or something. And you get a sequence of these state action award triples, and ultimately you have to figure out for the given states you're in, what was the action you took that helped to deter, or actions you took that helped to determine the ultimate sequence of rewards that you saw. Perhaps this one plus one or this minus one that you got at the end. This is really difficult problem, its got its own name, its called the credit assignment problem. In fact, we're talking about a sequence of events over time, we typically refer to this type of problem as the temporal credit assignment problem. Does that make sense Michael. Yeah that's really cool. Although I guess its credit but also blame right? Well credit and blame you know, blame is just the minus of credit. So without loss of generality Assume that credit can assume a negative or positive vibe. And yet when I go to the supermarket they never say blame or credit. Well that's just because they don't understand the technical terms. Got it. We're really interested in solving this temporal credit assignment problem as we will see as we talk later on in the course. This sort of issue comes up again and again and again. But, I didn't really want to talk about that in any great detail, I just wanted to make certain that, that people saw the difference between supervised learning and then sort of problems we're trying to solve now. That really has to do with sequences and time. What I want to just is focus a little bit more on rewards. And in particular, I want to look at the little grid world that we've been playing with so far, and think about how we would learn how to get from our start state. We always start it over here, over to one of the plus one or the minus 1, depending upon the kind of rewards that we see. So, let's push on this rewards notion for a minute, if we think about how MDP's are made up. And I'm just going to write down for you what the reward is, for the states in this particular grid world, okay? Yeah. So, let's say I'm going to set the rewards to be equal to, minus 0.04. So, the first thing I want to do, is I want to make certain you understand what I mean by this. So, what I mean when I write this down, for this particular example. Is that the reward that I'm going to receive in every single state is minus 0.04, except of course for these two states which I've already labelled as plus 1 and minus 1. Okay? Alright, so, it seems like you should just never ever move then, right? no, I don't think that's true. Let's think about that for a little while. So, if I set this reward to be minus 0.04 and you're starting out here in this state, and you have to keep living, you always have to take an action, right? That's the way I've set this up. And you don't get to stop until you reach either plus 1 or negative 1, we'll call these terminating or absorbing states. Once you reach into one of these things that ends the game. What would having a small negative reward like, this encourage you to do? It's, it's making me think about kind of walking across hot beach, because like each time you take a step you're going to get a little bit of punishment and the only thing that ends it is if you can get into that nice cool ocean. And then you get a plus 1 for that and the minus 0.04s, minus 0.04s, stop. Right, so, that means, so wait how would you put that another way. Sounds to me like what you're saying is, by having a small negative reward everywhere, it encourages you to end the game. Yeah, that's [UNKNOWN]. Yeah, I agree with that. Okay, yeah and, and I think that's exactly what I like that analogy a lot. Your in a, well, it's not a very hot beach it's just a you know, a slightly unpleasantly warm beach and you really want to hurry up and get into the ocean. In fact, you want to hurry up and get into the ocean even though on your way to getting to the ocean. You might step on some glass, that the minus 1 is. So, let's go back to this notion of policy of mapping states to action and let's think about as being in this world where you have a reward of minus 0.04 every where except in the absorbing states. What do you think the best set of actions are to take in these different states? What do you think the best policy is? Actually before you tell me, let me write down what the actual policy is. I'm not going to tell you, how we get here but I'm about to write down the policy that's sort of the best one to take in a world where you have these sets of rewards. Okay. Sure. Okay, you see this Michael. Yeah, and it makes a lot of sense. Like if you're basically heading towards the, the rewarding state, the green state. Right, so if I start out here in the leftmost bottom state, basically, it says, go up and then go to the right which, coincidentally is the policy that we had found before. Cool. There's a couple spots that are a little bit strange, though. Like what? Well, I'm thinking about the one, not directly under the minus 1, that makes sense to me, but the one, it takes you now to a position. Where it seems like you want to go straight up to the goal. But yet, it's going the long way around. It just didn't see the goal. No that's not it at all because of the way we are going to learn this, you're going to. So this is a global policy and I'm just telling you it's the optimal policy given the rewards. So let's kind of work out why going to the left makes sense here. Well, so here's my argument. What this basically says is take the long way around. Yes? Yeah? But, by taking the long way around, what's happening? Well, on the downside I'm going to pick up a little bit of negative reward, for a while, you know, one, two, three, four, five, six or so which is something like negative 0.2, negative 0.3, something like that, right? But, by doing this, I avoid every getting into a state where it might fall into the negative 1. Because it's, it's stochastic. Right, it's exactly, so stochastic means if I'm in this state here no matter what I do I have som, if I go up I have some chance of moving to the right and following into the negative 1. It's a relatively low chance, it's only 10%. But, it works out that moving from here to here, the probability of me falling into here. Is to high compared to the sort of near impossibility of me ending up falling into the minus 1, if I can follow this path. Interesting, okay, I guess that makes sense, it's cool. Which actually suggests sort of the point that I want to make here which is, that minor changes to your reward function actually matter. So, if we had a slightly different reward here, say. Not minus 0.04 but something else, you might find that some of these decisions would be different. I think you can see that? Hm. I mean at the level that I'm understanding it, it seems like if it's zero, that then it's in no particular hurry. If it's minus something big, then maybe it's in a bigger hurry. But, I don't, yeah I don't see exactly what the difference is going to be. Okay, so let's see if we can help you see the difference by making you take a quiz. Aha! Okay, Michael, so here's the quiz. Are you ready? Sure. Okay, so here's what's going on. I have copied over our little grid world over here twice. So it's the same grid world except I have changed the rewards that you will receive in all of the states other than the two absorbing states. So for the top one you get a reward of plus two. In every single state, except for the absorbing states, and in the bottom one you receive a reward of minus two in every single state but the absorbing states, got it? Interesting. So, what I'm going to want you to do is, I'm going to want you to fill in for these boxes that I've indicated, these same four boxes in the top and the bottom, whether you should go up, down, left or right. So enter into each one of them either Up U, D down, R right, L left, okay? Yeah, I appreciate you making the numbers nice and big, because I was worried you're going to say something like minus 0.13. But this, this I think. I can do that. No, no, no, that wasn't a suggestion. Yeah, let's go with the plus two and minus two. I feel like there's a better chance I might be able to get it. [LAUGH] Okay, so you think you got the what we're trying to do here? I do. Now just to, just to keep, just to make sure that I'm there, the plus one and the minus one end the game, and there's no more reward after that, it's like zero forever. Yep. And the other things, the blueish rewards, I would get them. Mm-hm. Continually until a state is reached. Right, exactly. Where the game ends, okay Yeah. No, that's, that's, that's cool. Okay, cool. Alright so go. Okay, Mikey, you got answers for me? sure. Alright. Wait, should I, do need to make any assumption, or can I make any assumptions I want about the non-blue states? No. It doesn't matter. [LAUGH] Actually. [LAUGH] So the answer is no. Maybe it does. The answer is no. But it doesn't actually matter because Remember, it's all very stationary and Marcovian. So the only thing that matters is where you are. You're stationary Marcovian. Alright, so I think, okay. So let's, let's do the first one, then. Okay. So, so this is, this is pretty cool. So by changing the reward the way that you did, it is now, it's not a hot beach anymore. It's like. You know like, super awesome money beach. Yes, it's like a beach made of bacon. Sure. Okay. Let's think of it that way. Although neither of us is eating bacon at the moment. At the moment. But. [CROSSTALK] Great, because, because then our mouths would be full and it would be very difficult to give this lecture. Alright so, no, no, no,no, no I mean like, even. Alright, never mind. The point is that it's awesome, and there's like diamonds. I don't know, I just saw The Hobbit movie, and there is a room filled entirely with treasures. And so, this is kind of like that. And so now the question is, what should I be doing, like I should never go to the plus 1 or the minus 1, because that ends my treasure gathering. I should just continue to stay in states that aren't those states. Okay. So I would say left for the, for the top state. Mm-hm. Let's say left for the bottom state. This one? The, yes, uh-huh. Okay. And the other two, I need to think about for a moment. So I feel like I'd like to get away from the minus 1. Mm-hm. So, like, you know, to go up. But then that would give me some chance of slipping, wouldn't it? Mm-hm. Lets see, I don't want to go to the right. I don't want to go down. I don't want to go. I'm going to go to the left. Mm-hm. I'm going to bash my head against the wall, because then I get money. Yeah. That's weird. There's really no pain for running into the wall? No. You just stay where you are. Alright and for the bottom one, I feel like it's the same kind of thing. I don't want to go to the right or to the left, cause then there's a probability that I'll slip and end up ending my game. So I'm going to go down. yeah. I like that. So most of these are right, and one of them is wrong. Or, it's not that it's wrong, it's just that's not as right as it could be. Which one do you think that is? I think they're right. But if, if there's one that I could imagine you not being so happy with, which is the bottom one, There are two bottom ones. You mean this one? Yeah. Yeah. What would I like? I think you might prefer if I bash my head against the wall. No, actually, it turns out that this is a correct answer so all four of these are correct. But in this particular state, it actually doesn't matter which way you go. Oh, I see. A more complete answer. Yes. Would be, doesn't matter. Yeah. And is that true of any of the other ones? No. I guess, I guess you're right. The other ones, you have to go in the direction indicated. Right. So, by the way, just like here, it doesn't matter which direction you go in. It's actually true for all of the other states as well. It doesn't matter where you go because In these three states, these are the only states where you can accidentally, you could end the game. And in each of them, you can take an action that guarantees that you don't end the game. So it really doesn't matter about where you do it in the other states. Cool. Right? So that's a good argument. This makes a lot of sense. So at the end of the day, basically, because the reward is positive. And you're just accumulating reward or diamonds and treasure, as you put it. You just never want to leave, so you always want to avoid either of these states. It doesn't matter that there's a +1 here and a -1 here. If I can stay here forever, then I just accumulate positive reward forever. Okay, so what's the next one? So, interesting. So now, now the beach is really, really hot. It's minus 2, so. Mm-hm. I want to get off here as quickly as I can. So definitely the top one, I would go to the right. Just try to get, get that +1 and get out of, get outta here. Yeah. Alright, so then, let's think about the bottom right. Okay. So, in the best of all possible worlds I go around and dump into the +1. And that would take me again if, if I don't slip at all one, two, three, four steps to do that Mm-hm. Actually wait one, two, three, three steps one the hot sand. Yup And I'm getting and -2 for each of those. So that's like minus 6. Mm-hm. So I could get that plus 1 at the end, and that makes it only minus 5. But I think I'd rather just get off the beach. So I'm going to say in the bottom right to actually dive into the pit of pain, because it can't be as bad as where I am. That's exactly right. So, okay, so then the, the one next to it on the left is a little trickier. If I go up to the plus 1, I get a minus 2 Followed by a +1 which is also -1. Mm-hm. So okay. I'm not so sure about that one. Well remember you'll always have some probablitly of ending up back where you started Is that true? Oh because, if I. Well no, no, that's not true if I dive straight into the -1. Then I have 0.8 chance of going to the -1. 0.2 of going up. To the arrow that's labeled, 0.2 going down to the blue box that's not labeled Mm-hm. Wait, 0.2? No, 0.1 Mm-hm. Whereas if I go up, right, I might stay where I am. And accrue -2, like, yeah I feel like I need a calculator for this. Yeah, but you're, I think your intuition is right, though. Just think about it as How much, I mean what do you think has the best chance of most quickly ending things? Well certainly jumping into the -1. Right. I think that's going to be marginally better because there's only one chance of slippage and delay. Where as if I take additional step, there's two chances of slippage and delay. Exactly, that's exactly the right kind of argument to make and, and because the -2 is so big. You're you're going to, you're just better off ending it even if you ended in pain, in the same way that you were in the bottom right hand corner. And so bottom left I would say the cleanest thing would be to jump into the -1. So to go to the right so that I can go up and get into the -1. And what's nice about that move is that is that you either end up in the bottom right hand corner. Or you end up staying where you are, or you end up moving up. But in either case, you are always sort of, you know, you never get farther away, right? Whereas if you try to go up, there's a chance that you will get farther away. Interesting. Is there a way to be sure about these? Yeah, you just have to do the math. [LAUGH] Do we know how to do the math? Yeah, you just start, you just do expectations. You can basically just say, well what's the expected time. Since you know what you're trying to do here is to get to the end. You can just, you can calculate the expected length of time it'll take you to get there. And then just, you know, multiply by all the rewards you're going to get in the meantime. If it helps, though, I will tell you that for any value where the reward is less than minus 1.6284. [LAUGH] This is the right thing to do. [LAUGH] Okay. Just so you know. Alright, so you're saying you actually do know that this is the optimum. The thing that we have here. Yes, I know that this is the optimum. Alright. Let me just draw out the rest of it for you. So it's, it's I think interesting to compare this one. The bottom right one, where you have given me negative rewards to encourage me to end the game. With our main one up here in the upper left. Where you have also given me some negative reward to encourage me to end the game. If you look carefully you'll notice that in this one, you're encouraged to end the game. But you're really encouraged to end the game at a plus 1. Here the reward is so strongly negative, you just need to end the game. And so you end up with a different response, here. Because this is, is just as quick to get to the minus 1 and into pain as to, take this extra step and try to get to the plus 1. And you end up with a different response here. Here, here, and here. That's really interesting. Yeah, it is. And so, these changes matter. They matter a great deal. So, how am I suppose to choose my rewards? Carefully and with some forethought. Nice. But, you know, again There's lots of ways to think about this. So the way we've been sort of talking about MDPs is kind of the way we've been talking about we talked in the first part of the course. When we talked about having teachers and learners. You can think of your reward as sort of the teaching signal. Sort of the teacher telling you what you ought to do, and what you ought not to do. But another way of thinking about this is that because the rewards define the MDP The rewards are our domain knowledge. I see. So if you're going to design an MDP to capture some world. Then you want to think carefully about how you set the rewards in order to get the behavior that you wish. That's fair. I mean, it seems a little bit like a cop out, but I think it seems like a necessary one. Yeah. I mean, there's really, I mean, again. No matter what you do, you've gotta be able to inject domain knowledge somehow. Otherwise, there's no learning to do. And in this case, the reward basaiclly is telling you how important it is to get to the end. Cool. Okay. Alright. Let's move on. Alright. So having gone through that exercise, Michael, I think it's, it's worthwhile to step back a little bit and think about the assumptions that we've been making that have been mostly unspoken. And I'm going to say that the main assumption that we've been making in some sense boils down to a single word. And that word is stationary. So let me tell you what I mean by that and why by kind of illustrating what it is we've been sort of doing for a little while. Okay? Sure. Okay. So the first thing I'm going to say is that we've actually been. Kind of assuming infinite horizons. So what do I mean by that. When, when we think about the last grid world that we were playing with, we basically said well, you know, I want to avoid going to the end as quickly as possible if I have rewards of a certain value or whatever. Because, you know, the game doesn't end until I get to an absorbing state. Well, that sort of implies. That you basically can live forever. That you have an infinite time horizon to work with. Now can you, can you imagine why if you didn't have an infinite time horizon to work with you might end up doing something very different? Different then what, what we're doing in the grid world? Right, so here let me let me show you the game that we were, the grid world that we were doing before. Might help you think about it. So here's the grid world we had before. And as you recall. We had a particular policy that sort of made sense. Here, I'll, I'll write it out for you again. And this was with a case where we had a reward of minus 0.04. Remember? We just did this. Remember? Yep. Okay, and this was the policy that turned out to be optimal, and in the future I want you to pay attention to here is that when you're over right here near possible end state, rather than going up, it made sense to take the long way around. Because you're going to get some negative reward but it's a small enough negative reward compared to where you might end up. Okay with a positive one. Yeah, I see. And that makes some sense. Well, that only makes sense if you're going to be living long enough that you can take the long route around. What if I told you you only had say three times steps left, and then the game is going to end no matter where you end up? Well, it might be, it might make more sense to take some risk than just try to take the short way because there's really no chance you're going to get to the plus1. I'm entirely convinced of that though because there's still a chance you'll fall into the minus 1 along the way. Right, so the exact val, whether it makes sense to take the risk or not is going to depend upon two things, we've already talked about one of them which is the actual word that you get. If this reward were, you know, negative enough, then clearly it makes sense to just try to end things quickly, right? We just showed that in the last quiz. But another thing that it's going to depend upon is how much time you have in order to get to where you're going. If you've only got one or two time steps before everything's going to end. You can imagine that there are cases where, without changing the reward too much it makes a lot of sense to try to go ahead and quickly get to this plus 1, even though you have some chance of falling into the minus 1. As opposed to, trying to move away, where you're then kind of, all but guaranteed that you're never going to reach the plus 1. So. Whether it makes sense to take the risk or not will depend upon the reward but it's also going to depend upon whether you have an infinite amount of time to get to where you want to get to or whether you have a finite amount of time. And the real major thing I want you to get out of that is that if you don't have an infinite horizon but you have a finite horizon then two things happen. One is the policy might change because things might end. But secondly, and more importantly, the policy can change, or will changee., even though you're in the same state. So, if I told you, if you're in this state right here, and I told you you didn't have an infinite amount of time, but you still had 100 million time steps then, it, I think it's clear that it still makes sense to go the long way around, right? Yeah, I mean the, the probability that this policy is going to last for a million timesteps has got to be tiny. Right. So, I might as well. It's 100 million timesteps might as well be infinity. But if I make that number not 100 million but I make it 2, or 3, or 4. Then suddenly your calculus might change. In particular, your caluculus will change even though I'm in the same state. Right? So maybe this state right here, if I've got a million, 100 million timesteps I still want got to go the long way around, but if I've only got a few time steps, the only way I'm ever going to get a positive reward is to go this way. Does that make sense? I guess so. So, you're saying, for example, even within the single run, it could be that I'm in a state and I try an action and maybe it doesn't work and I stay where I am. And I try it again, and maybe it doesn't work and I stay where I am. It might then switch to a different action, not because the other one wasn't working, but because now it's running out of time. Right, exactly. So we talked about this notion of a policy which maps states to actions, we talked about this notion about stationarity. So you believe that this sort of Markovian thing said, it doesn't matter where I've been it only matters where I am. And so if i'm in this state, since it only matters where I am, I'm always going to want to take the same action. Well that's only true in this kind of infinite horizon case. If you're in a finite horizon case. And that finite horizon, of course, is going to keep counting down, every time you take a step. Well then suddenly, depending upon the time step that's left, you might take a different action. So we could write that I think, just for the sake of kind of seeing it as some thing like, your policy is a function both the stature and, and the time step you're in. Hm. And that might lead you to a different set of actions. So this is important, this is important, I mean were not, we are not, for, for this course going to talk about this case at all, where you're in a finite horizon, but I think it's important for you to understand that the, without this infinite horizon assumption here. You loose this function of stationarity in your policies. Okay? Yeah. Interesting. Okay. So, that all, I think is, you know, making our something that's obvious, but becomes obvious after someone points it out to you. So, the second thing that I want to talk about, I think, is a little bit more subtle. And, and this notion of utility of sequences. So, as we've been talking, Michael, we have been sort of. Implicitly discussing not just the rewards we get in a single state, but the rewards that we get through a sequences of states that we take. And so I just want to point out a little fact that that comes from that, and where that ends up leading us. And then we'll get to some nice little cute series of math. So. Here's what I want to point what utilities, what we mean by utilities sequences. It means we have some function I'm going to call U for utility over the state, the sequence, sorry. Of states that we're going to see lets call them, S0, S1, S2 and so on and so forth. Well, I think an assumption that we've been making even if we haven't been very explicit about it is that if we had two sequences of states. S0, S1, S2, dot dot dot. And a different sequence S0, then S1 prime and S2 prime, that is two sequences that might differ from S1 on, but all start in the same start state. Okay? If we have a utility for the first, and that utility happens to be greater than the utility for the second, then it also turns out that we believe. That the utility for S1, S2, dot dot dot, is greater than the utility for S1 prime, S2 prime dot dot dot. Alright so these are two different sequences, S one, the S's and the S prime's are two different sequences. Yes. And in the beginning we're comparing them with S0 stuck in front of both of them. And we're saying if I prefer the S0 followed by all the S's, to S0 followed by the S primes, then I have that same preference even with those S0s missing. Right, and so this is called stationarity of preferences. And, another way of saying it is. That if I prefer one sequence of states today over another sequence of states, then I prefer that sequence of states over the same sequence of states tomorrow. So isn't, isn't this just obvious? Because the whatever the rewards for those two cases, we're just adding the reward we get for S0. So. It's going to be the same. But listen to what you just said. You just said, well, it'll be the same, because all we're doing is adding the reward for S0. But what did we ever say about adding up rewards? I thought, I thought that's what we were doing. That's right, that is what we were doing. But we never actually sat down and wrote that down and said, this is what it means. To talk about the utility of a sequence of states as opposed to the reward that you get in one state. Okay, so you're saying that if we, if we are adding rewards, then this follows. Right. Okay. And then I've actually been saying something even stronger, which is, I will show you on the next slide, which is if you believe that this is true, that the utility of one sequence of states is greater than the utility of another sequence of states. Both today and tomorrow. Then it actually forces you to do some variation of what you said which is just adding sequences of states. Or adding the rewards of the sequence of states that we see. That's really interesting. So then, so the adding isn't really an arbitrary thing it follows from this, this deeper assumption. Right, and the reason I bring this up is because. It would make sense if you were to just to grab someone off the street and start talking about Marco Decision Processes. One of two things will happened. Either they'd run screaming from you like you're a crazy person or they would sit and they would listen and if they listen they would just completely buy into the idea that you just add up sequences of rewards. You know, sequences of rewards that you see as a way of talking about how good the states are because that's a very natural thing to do. But it turns out that mathematicall if you have this notion. A sort of stationary of preferences and this sort of infinite arise in world. You really are in a case where this has to be true. And it has to be the case if you have to do some form of addition. Because nothing else sort of can be guaranteed to maintain this property over stationary preferences. I mean, as you said, if I got one sequences of states and another sequnce of states and by just prepending or appending another set of states to it, I'm still going to always guarantee that one's greater than the other. You kind of have to do some form of adding the reward that you see in the states in both cases. because if you don't do that, then eventually this inequality will not hold. So, let me write that down in math terms. And see where that gets us, okay? Cool. Alright, Michael. So, let's write that down in math, okay? You like math, right? I do. Okay. So here's the math version of it. I'm going to just say that the utility that we receive for visiting a sequence of states, S0, S1, S2 ellipsis, is simply the sum of all the rewards that we will receive for visiting those states. Sure. Does that make sense? Yeah. Is that consistent with what you were doing when you were thinking about the grid world? Yeah, exactly. But I thought we were going to make that not definitional, we were going to derive that it had to be that way. No, we're not. They can read about it. It'd take me, like, an hour and a half to do it. Alright. And I'm already losing my voice. But. What I do want people, I want you to believe is that the utility of the sequence of states, thinking of it as the sum of all the rewards, makes sense, right? And then if nothing else, at least it makes, it's consistent with what you've been doing as we've been talking about this grid work. Yeah, absolutely. I mean it, it also kind of makes me think about money. Go on. Well, so, so, I mean, that's sort of how money works. If we get some kind of payoff each day, those payoffs get added to each other. They don't get, you know, subtracted or multiplied or square rooted or whatever. They just, you know, they go into your bank account, and things add. So this feels like, kind of like that. It's like money in my pocket. Sure, if you have a big enough pocket. Okay, I'm with you on that. Alright, so I'm going to say that this all makes sense. It, it's really awesome and it sort of doesn't work. And to illustrate why this doesn't work, I'm going to give you a quiz. Yay. because you like quizzes. So I've been told. So, here's the quiz. You see on the screen this sort of unexplained two little squiggles with a bunch of numbers in front, on top of them or under them or near them? Yeah I, I assume that's a river and on one bank it's the land of the plus ones and the other bank has been infilitrated by plus twos. That's not what I intended at all but I actually like that enough that I'm going to pretend, that that's what I intended. So these work out to be sequences of states. Oh, I see now. Okay, and rewards that you receive. No, no, no, it's a riverbank and on one side of the bank, as you walk along it, you get a bunch of plus ones. On the other side you get a bunch of plus ones and some plus twos. And this just goes on forever, okay? And, I'm not going to tell you what all the rewards are after that except that they, they look similar to the rewards that you see here. Okay. Plus ones and plus twos, okay? And let's say the top, the top one, in fact, nothing but plus ones. And the bottom ones are some plus ones with maybe some plus twos sprinkled in and out, but those are really the only options. Okay? Yeah. Here's my question to you. So, would you rather be on the top side of the river bank, or the bottom side of the river bank? Okay? You think you know the answer already, don't you. Yeah. Okay, cool. Well then, let's, let's give our listeners a, a chance to come up with the right answer. Okay Michael, I didn't give you as much time as I normally do because you think you already know the answer. So what's the answer? So, you know, I'd rather get the plus 2's occasionally, so I'll say the bottom one. No. Wait, what? You're not going to tell me the top one's better? no, it's not. Okay then that feels like a trick question. It's not a trick question. The answer is, neither one is better than the other. Oh, so I had to give neither? Or both. Which is better, and I can click on neither. Or you could click on both. Okay. So you thought that the bottom one was better, what was your reasoning for that? Because, sort of moment to moment, sometimes I'm getting plus one's, and that matches what I would've gotten if I had been on the other bank. But then sometimes I get plus two's, which is actually better than what I would have gotten on the other bank. And so, I'm, I'm, I never feel any regret being on the bottom bank. I only feel regret being on the top bank. That's fine. So, what would you say the utility of the sequence along the bottom actually is? 1 plus 1 plus 2 plus 1 plus 1 plus 2 plus dot, dot, dot. Which is equal to? infinity, I guess. That's right. What about the utility of the top one? 1 plus 1 plus 1 plus, that's also infinity. Yep. So the utility of both of those sequences is equal to infinity. Do you think one of them is bigger than the other? You drew the bottom one a larg, a little bit larger. I did. Or longer, anyway. Yeah. But, in the end, the sum of the rewards that you get are both going to be infinity. So the truth is, neither one of them is better than the other. I still don't think you can say that both are better. You can't get around that. But yeah, I see, I see, neither, I can see neither is better. Or that both are better. Neither is better, both is better, whatever. The point is that, they're both equal to infinity. Hm. And the reason they're equal to infinity is because all we're doing is accumulating rewards. And, if we're always going to be able to get positive rewards no matter what we do, then it doesn't matter what we do. This is the existential dilemma of being immortal. Oh, living forever. Right. So if you live forever then, like why should you care about anything ever? Right I mean, everyone, every all the mortal people are going to die and one day they'll all be, you know, an infinite amount of time in your past. I could do this thing here, which is pleasurable, or I could do this thing right now, that, you know, will, is less pleasurable, but will eventually get me to a better place. But if I'm going to live forever, and I can always get to a better place, than it really doesn't matter what I do. It really doesn't matter. Mm. Because I'm just accumulating rewards, I'm living forever, and I'm going to, infinity is infinity and there's no really no way to compare them. Having said that, your original notion that, look, it feels like I should never regret having taken the second path compared to the first because I will occasionally do better. Seems like the right intuition to have. I see but it's just not built into this particular utility scheme. Right, but it turns out there's a very easy way we can build it into this utility scheme by just making one tiny little change. Would you like to see it? Yes. Beautiful, let's see it then Okay Michael so here's the little trick. So all I've done is I have replaces the equation at the top with an alternate version of the equation. 'Kay. So it looks there's, you're now exponentially blowing up the reward. I am not exponentially blowing up the reward. Instead of what I've done. Is I've added of the see, the rewards that I'm going to see. For the states that I'm going to see. And I multiplied by gamma to the T. The gamma is between 0 and 1. I see. So... Do you? Well, kind of. So, so it doesn't exponentially blow up. It exponencially implodes. Right. So, so things that are like a thousand steps in the future If we multiply, if we take something that's less than 1 and we raise it to that power, it goes essentially to 0. So it's like it starts off the rewards are kind of substantial and then they get, they trail off quickly. Right. And in fact we can write down mathmatically what this is If you actually, if you stare at it long enough and you remembered your intermediate algebra or your calculus or wherever it is you learned this stuff. You would probably recognize this as a special kind of sequence or series. Do you remember what it is? Television series? [LAUGH]. No, but that's remarkably close. So let's see if we can bound this particular equation. So we know that this version of the equation eventually ends up being infinity if all the rewards are positive. Right? Mm, hm-mm. So what does this end up being even in the case where all the rewards are positive? Well, we can bound this from above by the largest reward that we will ever see, in the following way. So all I've then is said well I don't know what the rewards are but there is some maximum reward I'm going to call it Rmax. And if I pretended that I always got the Rmax reward. So long as that, as long as that's an actual number. A finite number. I know that this is bounded from above by this expression. Well what does this look like. Maybe this does look like a series you remember. Geometric series? Yes. This is the geometric series. And this is exactly equal to this. . And I assume that the summation causes the max to become lower case. Yes, yes it does. That's a, that's a trick that they don't really go over deeply in calculus, but it's true. Oh, that's cool. Alright so, the reward, the maximum reward divided by 1 minus gamma. So when gamma is really close to 0, you're just getting, oh you're getting just that one reward in the beginning and then everything falls off to nothing after that. Yep. And if gamma is really close to 1. You're dividing by something that's teeny tiny, which actually is like multiplying it by something that's really big. So it kind of magnifies the reward out, but it's not infinity until gamma get's all the way up to 1 and that's the same case we were in before. Right, so in fact you'll notice the way I wrote this, gamma has to be between 0 and 1, 0 inclusive but strictly less than 1. If I made it so that it could include 1, well that's actually the first case. Got it. Because 1 to a power is always 1, and so, this is actually a generalization of the sort of, infinite sum of rewards. This is called discounted rewards, or discounted series, or discounted sums, and it allows us, it turns out, to go an infinite distance in finite time. Wait. That makes sense. No. No. [LAUGH] An infinite distance? Yeah. That's at least the way I like to think about it. So, if we're discounted in this particular way, then that gives us a geometric series. And what it does is it allows us to add an infinite number of numbers, but it actually gives us a finite number. Cool. So this means we can still have our, and it turns out, by the way, just to be clear, that this world that we're in. Where we are in between 0 and 1, 0 inclusive is still consistent with our infinite horizons and our stationarity over preferences. So we can add things together and be consistent with those assumptions we were making before Or we can do discounted rewards and we're still going to be consistent with what we're doing before. But the difference between this case and this case is that by doing the discount, we get to add infinite sequences and still get something finite. The intuition here is that Since, if gamma's less than 1, then eventually as you raise it to a power, it will basically become 0. I mean, that's why it's a geometric series. Which is like having a finite horizon. But that horizon is always the same finite distance away, no matter what, where you are in time. Which means it's effectively infinite. Or unbounded, or however you want to think about it. So it's like you take a step, but you're no closer than where, when you started. Right, so, what does that mean in, a world where you meant to say you're no closer than where you were trying to end up? So that means that even though, you've taken a step forward, the horizon sort of remains a fixed distance away from where you are. Mmm, mmhmm. That's what it means to To, to have this kind of this gamma value here and so you can still treat it as if it's always going to be infinite, even though it gives you a finite value, and that gives you your stationary. So does that make sense? I mean that's sort of the intuition. Yeah. You really are have an in, you're always in infinite, It's always in, infinity. But the gamma value means that at any given point in time, you only really have to think about what amounts to a finite distance. You never get closer to the horizon, even though you're always taking steps towards it. But there still is a horizon that you can see, and I can kill that analogy. [LAUGH] So can we, can you explain to me why it has this form, this R max over 1 minus gamma? I could. There's a You can kind of prove that, in a little cute math way, and I'm happy to take that diversion if you want. Yeah, I think so. I mean, unless, unless you were going to tell me something else. Well how about I tell you something else and then I take that diversion? Okay. Okay, so here's the something else. In the beginning I said, well, this is like going in infinite amount of distance in finite time, which you rebelled against. And my explanation is more about. How infinity looks finite which is not the same thing as going an infinite amount of distance in finite time. The reason I said that is beacuse of the sningularity. What? The singularity so, some of our listeners no doubt have heard of the singularity. You never heard of the singularity? The singularity is like when computers get so fast that you do infinite computation and, oh. Right. So the singularity, for those of you who don't know is is this sort of observation that has been made by many folks. That the sort of limit to computer power growing faster is the fact that it takes us amount, some amount of time to design the next generation of computers. Right, So Moore's law says everything doubles you know, everything 6, 18 months or whatever. But if we could design things faster, then we could actually make it double more quickly. So, one day we'll get to the point where the computer, which I will draw here, like that. The computer can actually design the next generation of computer. Well, when it designs the next generation of computer, that computer will also have the ability to design the next generation of computer. But it will be able to design it twice as fast. And then the next one will be able to design its next successor twice as fast, and so on and so forth. So this little time between generations, will keep Having every single time, which looks remarkably like a geometric sequence. And so once we reach this point, where the computer can actually design its successor, we will then be able to do an infinite number of successors. In finite time. [LAUGH] And that's when the world comes to and end. And that's what's called the singularity. Because we can't understand what happens after that point. So that's what I mean by going an infinite amount of distance in finite time. It just doesn't seem like distance. But, yeah, that's a weird example, for sure. I think it makes sense. [INAUDIBLE] infinite number of doublings in it. Yeah, [INAUDIBLE], no. [LAUGH] Well, we'll, we'll, we'll do we'll do some assignment where we allow people to decide whether this makes sense or one of your analogies make sense. And I'll just remind the listener that I'm the one who'll be assigning final grades. Okay. So with that let's go and do your little diversion. And and then we'll come back to. Doing a whole lot of math. So I actually think this little nice diversion will be warmup to all the math we're going to be doing. Okay? Oooh. Hm. More math. Hm. So, if we think about the equation that we were doing before, which was you know, the sum of all these gammas times some R max. We can basically take out the R max and what we end up with is something that looks like that, right? You're summing together a bunch of gammas and then multiplying the result by R max. So what does that acutally look like? Well that looks like this. Gamma to the zero, plus gamma to the one, plus gamma squared, plus dot dot dot dot. Eventually multiplied by R max. Right? So let's call this x, okay? Does x include the R max or not? No. So we'll just call the little sequence of, it's going to turn out not to matter. Let's just call the sequence of gammas we're adding together, let's just call that x. Good. Well, if you look at it, this is actually recursive. Right? Because this is an infinite sequence, if you sort of shifted it one over in time, you would end up with just the repeated sequence again. All right? Which means that we can write x in terms of itself. x equals gamma zero plus gamma times X. I see, so it's shifted over, but then you have to multiply it again by gamma, to get up to gamma one, gamma two. Right, exactly. So, so, this is just, you know, it's just math, that's just all it is. so, we can try to solve for x and figure out what x is, right? Cool. So, what is x? Well, we can subtract from both sides and so we end up with like, something like x minus gamma x equals gamma 0. Right? Seems like we can stop writing gamma 0. Isn't that just one? Yes, but I've already, I've gone too far, Michael. I've alrea, I've already written gamma 0. Alright. [LAUGH] so, this becomes x times 1 minus gamma equals gamma 0, or as Michael so astutely points out, 1. [LAUGH]. Alright. Which means what? It means that. So then we divide by 1 minus gamma. a is 1 over 1 minus gamma. Neat. And that, of course, we are going to multiply by R max. To get the formula that we had. That's, yeah, that's nifty. Yeah, there we go. So, why do we do this? Well, because you like stuff like this and it points out something very simple, which is that geometry is easy. [LAUGH] I don't think that's the kind of geometry that people usually think of. Well, they should be thinking of this kind of geometry. So geometry is easy. And by doing a little cute algebra, we can derive ridiculous equations that turn out to help us deal with go an infinite distance in finite time. [LAUGH] I don't think that's what they're doing, but okay. [LAUGH] So, let's think of this as warmup for what I actually wanted to show you, which is going to turn out to be a whole lot of math. Okay? Alright, let's, let's go for it. Alright, let's do that. Okay, so Michael, in the spirit of what we just went through in deriving the geometric series, I'm now going to write down a bunch of math. And what I'm going to do is I'm just going to say it at you, and you're going to interrupt me if it doesn't make sense. Okay? That makes sense. It does. Okay, so here's the first thing I'm going to write down. I've decided that by going through this excercise of Of utilities in this kind of reward, we can now write down what the optimal policy is. The optimal policy, which as you recall is simply pi star, is simply the one that maximizes our long-term expected reward. Which looks like what? Well, it looks like this. There, does that make sense? Let me think, so. We have an expected value of the sum, of the discounted rewards, at time t. And, given, pi. Meaning that we're going to be following pi? Mm-hm. So these are the, the sequence of states we're going to see in a world where we follow pi. And it's an expectation because, things are non-deterministic. Or may be non-deterministic. And do we know which state we started? It doesn't matter, it's whatever s zero is. I see. Whatever s zero is, but isn't that random? I mean s one and s two, s three; those are all random. Well, we start at some state, it doesn't matter, so t is starting out a zero. And going to infinity. Okay? So does this make sense? Yes, so then, so we're saying, I would like to know the policy that maximizes the value of that expression. So it gives us the highest expected reward. Yeah, that's the kind of policy I would want. Exactly. So, good, we're done, we know what the optimum policy is. Except that it's not really clear what to do with this. All we've really done is written down what we knew it was we were trying to solve. But it turns out that we've defined utility in such a way that it's going to help us to solve this. So let me write that down as well. I'm going to say that the utility of a particular seque, of a particular state okay. Well it's going to depend upon the policy that were following. So I'm going to rewrite the utility that takes the superscript pie. And thats simply going to be the expected set of states that I'm going to see from that point on given that I've followed the policy. There, does that make sense? It feels like the same thing. I guess the difference now is that you're saying the utility of the policy out of state is what happens if we start running from that state. Yep. And we follow that policy. Got it. Right. So, this answers the question you asked me before about, well, what's S0? Well, we talk about that in terms of the utility of the state. So how good is it to be in some state? Well, it's exactly as good to be in that state as what we will expect to see from that point on. Given that we're a following a specific policy where we started in that state. Hm,. Does that make sense? Kay. Yeah. Very important point here, Michael, is that the reward for entering a state is not the same thing as the utility for that state. Right? And in particular. What reward gives us is immediate gratification or immediate feedback. Okay? But utility gives us long term feedback. Does that make sense? So when reward [UNKNOWN] is the actual value that we get for being in that state. Utility [UNKNOWN] state is both the reward we get for that state. But also, all the reward that we're going to get from that point on. I see. So yeah. That seems like a really important difference. Like, if I say, here's a dollar. You know? Would you poke the president of your university in the eye? You'd be, like, okay. The immediate reward for that is one. But the long term utility of that could be actually quite low. Right. On the other hand, I say, well, why don't you go to college? And you say, but that's going to cost me $40.000. Or better yet, why don't you get a masters degree in computer science from Georgia tech, bu you can say that's going to cost me $6600. Yes, but at the end of it you will have a degree. And by the way it turns out the average starting salary for people who are getting a masters degree or undergraduate degree is about $45000. So is it considered product placement if you. Plug your own product within the product itself? No, I'm just simply stating fact Michael. This is all I'm doing. Just facts. Alright. This is called fact placement. Alright. The point is, there's a, an immediate negative reward, of say, $6,600 for, I'm going through a degree. Or maybe it's $10,000 by the time, the 15th person sees this. But anyway, it's some cost. But, presumably it's okay to go to college, or go to grad school, or whatever. Because at the end of it you are going to get something positive out of it. So it is not just that it prevents you from taking short term positive things if that is going to lead to long term negative things. I also always you to take short term negatives if it will lead to long term positives. That makes sense. What this does is this gets us back to what I mentioned earlier. Which is this notion of delayed reward. So we have this notion of reward, but utilities are really about accounting for all delayed rewards. And if you think about that, I think you can begin to see how, given you have a mathematical expression delayed rewards, you will be able to start dealing with the credit assignment problem. Cool. Okay, so let's keep going and write more equations. So, now that we've got utility fine, and we've got this pi star to fine, we can actually do an even better job of writing out pi star. And let me do that. All right, so does this equation make sense, Michael? Let's see, so the policy, is that a star again, or is that a K. That's a star. So it's the optimal policy. All right. The optimal action to take at a state is, well, look over all the actions, and sum up overall the next states, the transition probability, so that's the probability we end up in state s prime. And now we have the utility of s prime, the problem being that that's not defined. Well, it sort of is, we defined it immediately above, at least with respect to some policy. But that's concerning because we don't know. The policy that you want to put in there is gotta be the policy that you're trying to find. Right, so in fact implicitly what I mean here is pi star. So, in fact, let me write that down that whenever you see me write from now on, the utility of a state, I'm almost always going to actually mean the utility of the state if I follow the optimal policy. We might call this the true utility of the state. I see. So I'm just going to write this off to the side here as something for you to remember. So this this says then that the optimal policy is the one that, for every state, returns the action that maximizes my expected utility. With regard to the optimal policy, it feels rather circular. It is rather circular, but you're a computationalist. You're a big fan of recursion. We just went through a whole exercise where we figured out the geometric series by effectively doing recursion. It's a similar kind of situation, for this? It kind of is. So, let me write one more equation down and then you'll be one step closer to actually seeing it. Of course, if we're in an infinite horizon with a discounted state, even though you're one step closer you won't actually be any closer. Well let's worry about that when we get there. So let me write one more equation down. We're never going to get there. It's infinitely long. [LAUGH] Yeah. Wait are you demonstrating something with this lesson by making it infinitely long? [LAUGH] I'm certainly demonstrating something with this lesson. I don't know what it is. So let me write this next equation down. So then the true utility of a state s is then, I'm just basically going to unroll the equation for utility. It's the reward that I get for being in that state, plus I'm now going to discount all of the reward that I'm going to get from that point on. Got it? All right, so once we go to our new state s prime, we're going to look at the utility of that state. Okay, that's sort of fine, modular recursion. We're going to look at overall actions, which action gives us the highest value of that. Oh I see, that's kind of like the pi star expression just above. Yup. All right, so once we figure that out, we know what action we're going to take in state s prime. We're going to discount that, because why? Because I guess that just kind of ups the gamma factor on all the rewards in the future. Right. And then we're going to add to that our immediate reward. Yes, okay I think I follow that. In some sense all I've done is I kept substituting pieces back into one another. So the true utility being in a state is the reward you get in that state, plus the discount of all the reward you're going to get at that point, which, of course, is defined as the utility you're going to get for the states that you see, but each one of those is defined similarly. And so the utility you will get for s double prime say will also be further discounted but since it's multiplied by gamma that will be gamma squared and then s triple prime will be gamma cubed, and so that's basically just unrolling this notion of utility up here. Okay so now it seems like all the pieces are in one place. Right. And so it would be nice if we were done. And I'm going to say that we're not just one step closer, but you can see an oncoming light and it is not an oncoming train, okay. So Yeah, this seems like a really important equation. It is, in fact, it's so important, it's got a name. You want to guess what the name is? Bill That's actually very close. It's Bellman Equation. Bellman equation Esquire. This equation was invented by a guy named Bellman, and it turns out to be in some sense the key equation for solving MDPs and reinforcement learning. Wow. And it's actually even more [INAUDIBLE] than it looks. But basically, this is the fundamental recursive equation that defines the true value of being in some particular state. And it accounts for everything that we care about in the MDP. The utilities themselves deal with the policy that we want to have, the gammas are discount, and all the rewards are here. The transition matrix is here, and the actions or all the actions we're going to take. So basically the whole MDP is referenced inside of here and allows us by determining utilities, to always know what's the best action to take. What's the one that's going to maximize the utility? So if we can figure out the answer to this equation, the utilities of all the states, we per force know what the optimal policy is. It becomes very easy. So we've sort of taken all that neat stuff about NDPs and stuck it in a single equation. Bellman was a very smart guy. So was he the same Bellman from the curse of dimensionality? Yes. Cool. There can be only one Bellman. [LAUGH] Actually, are there any more Bellmans? I don't think so, I think that they retired, like retiring a jersey. They retired his name. I could've sworn that I saw one at the last hotel that I went to. It was probably the same one. Oh, I get it. Hotel, Bellman, that's really good. Very good. Okay good. Well, so now that we've killed that as much as we could, let's see if we can actually solve this equation, which since this is clearly the key equation since it has a name, okay? Yeah, that would be cool. Especially because it looks like, if you could solve this, you could solve it, right? because then you have u. You could just plug the u in and get the u out. Right. And once you have the u in, and you get the u out, then you got the policy. Right. For u. It's always been for u. [LAUGH] It's for us, Michael, it's for us. Okay Micheal, so I've erased the screen and kept Bellman's equation, the most important equation ever and we are going to solve it. Oh. So, how are we going to make that work? Okay, so how many, so we wrote this down as a utility of s, how many S's are there? s, n, m, I don't know? Pick a letter. N. N, so we N states which means this isn't really one equasion. This is how many equations? N. Yes it's N equations. How many unknowns are there? Well we have U, the Rs are known, the Ts are known, the only things that's missing is the Us. And there's, oh and there's N of those as well. Right there's N equation in N [UNKNOWN]. So we're done. Yes because we know how to solve n equations in n unknowns, right? If the equations were linear. If the equations were linear. Are these equations linear? I'm going to go with no. Why not? because the max is problematic. That's right. This max operation makes it nonlinear. So, it looks really good for a moment there. We've got n equations, n unknowns. We know how to solve that. But max makes for a very very very weird non linearity. And there's kind of no nice way to deal with this. Actually, one day, Michael, if you ask me, there is a cute little aside you can do where you can turn max into something that's differentiable. Oh. But. That doesn't actually help us here so I'm not going to go off on that aside yet. But even differentiable wouldn't quite be linear. That's right. And it wouldn't help us in this case. Yeah that's exactly right. But the fact that you can have differentiable maxes I think actually [UNKNOWN]. But also unimportant for what we're talking about now. So, we've got n equations, n unknowns, they're nonlinear, which means we can't solve it the way we want to by like inverting matrices or like. You people are in the regression would normally do but it turns out that we can in fact, do something fairly similar, something that actually allows us to solve this even though it is nonlinear. And here's the equation, or the equation, here's, here's the algorithm you use that sort of works, okay? So it's really simple. Just simply start with some arbitrary utilities. Declare those the answer and you're done. That would be one way of doing it but it turns out we can do even better. Wait. Start off with the correct utilities. That would work except we don't know what they are. Oh right. So we're going to start with some arbitrary utilities, but then we're going to update them based on their neighbors. And then we'll simply lather, rinse, repeat. Alright, so what does that mean based on neighbors? So, it means, based on the states, you're going to update the utility for a state based on all of the states that it can reach. So, let me write down an equation that tells you how to update them and then maybe it'll be clear, okay? Yep. So we're going to have a bunch of utilities since we're going to, we're going to be looping through this. And let's just say every time we loop through, that's time t. Okay? So we know what the equation for utility is where the utilities are. It's just the equation that's written up here. So it's r of s plus gamma times the mass over a of. The expected utility. Right? Except we have some estimate of the utility at time t. Okay? and, probably the right thing for me to do would be to write this like as you had or something like that. This is my estimate of the utility. And so now using this, I'm going to want to sort of update the, you know all of my utilities to make them better. And it turns out I can do that by simply doing this. So I'm going to update at every iteration update utilities based on neighbors I'm going to update at every iteration, at every iteration my estimate of the utility of some state S by simply recalculating it to be the actual reward that I get for entering state S, plus the discounted utility that I expect given the original estimates of my utility. Does that make any sense at all? Yeah, I guess so, so. So the s, okay, so, so we need the whole u hat t. Mm-hm. Like at all states, because we're not just using the values that state s to update the values that state s. We're using all the values, at the, at the previous time step to update all the values to the current time step. Yep. So, so all these n equations, they're all tangled together. Right, because of this This, expectation. So really, just to make it clear, this should be a summation over s prime. So, I'm going to update the utility of s by looking at the utilities of all the other states, including itself, as prime. And weight those based on my probability of getting there given that I took an action. And what action am I going to take? Well, I'm going to take the one that maximizes my, expected utility. So it's sort of like figuring out what to do in a state assuming that this uhat really is the right answer for the future. Right. Now why is that going to work? So I just made up the uhats, right? They started out as arbitary utilities. The next little step about updating utilities based on neighbors makes some sense because effectively is any state that you can reach. Which is determined by the transition function. But, all I'm doing is reaching states that are also made up of arbitrary utilities so, arbitrary values. Why should that help me? Well, it's because of this value right here. This is actual truth. The actual word I get for entering into a stage, is a true value. So, effectively what's going to happen is I'm going to be propagating out the true value, or true reward, the true immediate reward that I get for entering into a state, say. Through all of the states that I'm going to see and propagating that information back and forth. Until ultimately I converge. Still not obvious why we should converge, because we start off with an arbitrary function and it seems like that could be really wrong. So we're like adding truth to wrong. Yes but then, the next time around I'm going to, I've been adding truth twice to wrong, and then more truth to wrong, and more truth to wrong. And eventually, I'm going to be adding, more and more and more and more and more and more and more and more and more and more truth, then it will overwhelm the original wrong. And is that, does it help that the wrong is being discounted? Yes, it helps that the wrong is being discounted. Does it help that the wrong is being discounted? I actually don't know that matters. I'll have to think about that for a moment. It certainly doesn't hurt. But I, I guess the way that I think about this, I mean there's an actual proof you can look it up but in the end I think, I, I tend to think of this as a kind of simple contraction proof that effectively at every step you have to be moving towards the answer. Because you've added in more of the reality, more of the truth. And if you remember the utility of a state is just all of the rewards that you're going to see. So, basically at every time step you have added the reward that you're going to see, and then all of the rewards you're going to see after that. And so you've gotten a better estimate of actually the sequence of rewards you're likely to see from the state. And if that gets better for any of the states, then eventually that betterness will propagate out to all the other states that they can reach or can reach them. That will keep happening and you'll keep getting closer and closer and closer for the true utility of the states until you eventually run out of closeness. Cool. Does that make sense at all? Well does it also help that the gamma is less than one. Yeah it does. the, the way I like to think of this as, is as a sort of contraction proof, that makes, if you've heard of those. So, the basic idea here is that you start out with some noise, but at every step, you're getting some truth, and that truth gets added, and then, the next dideration, more truth gets added, and more truth get, gets added. So, as you have some estimate of some particular state S, you get to update it based on truth. It's actual reward. And you bring in more truth from the other utilities, as well. As this particular utility gets better. That closeness to the true utility then gets spread to all, from, to all the states that can reach it. And because the gamma is less than one. You basically get to overwhelm the past in this case which is the original arbitrary utilities. And so as you keep iterating through this, the latest truth becomes more important than the past less truth. And so you are always getting closer and closer and closer to the truth until you eventually you do. At least that's the intuition that I like to think of. Yeah I, I kind of get that as an intuition, though I'd probably be happier going through the math, but. Well, we could do that, and by we, I mean the students can do that by actually reading the proof. All right. Okay, so cool. So, this right here is a an easy way to find the true value of states. And you do it by iterating and it kind of has a name. It kind of has a name. Yeah, what do you think the name could be? Bellman. Bellman's algorithm. No. No though that's probably reasonable. Utility iteration. Yes, except utility sounds better if you say value, so it's value iteration. And it works. Remarkably well. So, and it doesn't, doesn't give you the answer but it gets you something that is closer and closer to the answer. Right. And, eventually it will converge. You'll get so close to converging it doesn't matter. And, once you have the two utilities, if you recall. We know how to define the optimal policy in terms of utilities. So, if I give you the utilities, then the optimum policy is just, well I'm in a state. Look at all the states I might get to. Figure out the expectation that I'm going to get for a given action. Pick whichever one is maximum and I'm done. So solving for the utilities are the true value of a state. Is effectively the same thing as solving for the optimal policy. Hm. Excellent. That's cool. I think so. Okay, Michael, so you seemed like you knew what you were doing so I thought I would verify that by giving you a very easy quiz. This doesn't look so easy. It is easy, so here's the quiz. To help you I've written up both the Bellmans equation and the update that we would give to utilities. I neglected to write the little hats and everything because I just don't think you need that but these are the two equations that you need to be able to think about. This is just what we've written up before. And I've written down the grid world we've been playing with the entire time. And I want you to figure out how value iteration would work from the first iteration and the second iteration for this particular state here that I marked with an X, okay? Okay. Alright, and there are a little more information you need to go. Gamma in this case is going to be equal to one half. Mainly because it's easy to do the math if you do it that way. The rewards, just to remind you, for all of the states except for the two goal or absorbing states is going to be minus 0.04. And my initial arbitrary utilities for all of the states, is going to be 0, except in the two absorbing states where I already know the utilities are 1 and minus 1 respectively, Okay? Okay. You got all that? I think so. You sure? [LAUGH] I feel confident that I'm going to slip up, but Okay, yeah, I think I can take a stab at this. Alright, so gamma's one half, rewards are minus 0.04, arbitrary starting utilities at times 0 is 0, except here at the absorbing states. Tell me how the utility here will evolve after one step, or one iteration, and two steps, or two iterations. Okay then, go. Alright, Michael. What's the answer? I think I've already made a mistake. Okay. What's the mistake? Suggesting that we do a quiz. [LAUGH] I'm pretty sure that's always true. Unless Michael, by doing a quiz now and suffering pain, you, in the future, are a better person. In which case, you have made the right long term decision. I see. You're, this is a MDP metapoint you're making. Mm-hm. Alright. So let, but let's, let's just buckle down and do this thing. So. Okay. Alright, so at that state x, we have to consider, according to the equation, we're going to do U sub one at x. And that's going to be the reward at x, which is minus .04. Mm-hm. Plus gamma, which is a half. Yes. Feel free to write these things down. Okay, so, let's see. It's going to be minus .04 plus one half times. Alright. So now we need to do four different actions. Right. So, I don, I would make like a brace-y thing at this point. Not a bracket, but a brace. Alright? Or I could do a bracket, because you're going to notice immediately that it's obvious what the right action is. Okay, alright. Well, we know that the, the right action's going to be to go to the right. Yeah, but even then, you know you don't have to do the rest of the computation because my first guess at all the utilities is that they're 0. Which means you're always going to want to take the action that gets you to plus one with the highest chance. Right. So there's just no point in. I see. Okay. Fair enough. Thank you for. Okay. The shortcut. So, so we only have to do the one action which is to the right. Mm-hm. And so if we go to the right, there's three possible next states we could go in. Yeah. One is back to x, which has a value of zero. Mm-hm. One is to the thing underneath of x, which has a value of zero. And then the last one with probability 0.8 needs to go to the plus one. Which is. 0.8 times plus one. Which is 0.8. 0.8. Okay. And that is? Okay. So 0.8 times a half is 0.4 minus 0.04 is 0.36. Yep. And that is correct. Okay, so to do the same thing for you too, we're going to need the U1 value for a bunch of other states, seems to me. Maybe, so let's right that down. So we know for now the utility here is .36 right? Yeah. And you're saying that in order to do U two, I'm now going to have to, you know, I was able to avoid doing some of the math before because, these are all zeroes. So it was just easy to do. But when I went right, I either stayed where I was, went to the plus one, or I ended up going down. Well I think by the same argument that allowed us to cheat our way out of to cheat our way out before. Okay, It's still going to be best to go to the right, so. Yeah but I know that but the value itself is going to depend on the value in several other states. Yeah well how many other states? Oh, just that one. Just this one, so what was U one of this state? I see. So, presumably, we want to avoid falling into the pit, so the, the best thing we can do is bash our head against the wall, Mm-hm Which will get us a -.04 for that statement. Right. Okay, alright, so maybe this isn't so, so bad. Mm-hm. So now for our u2 values we need -.04 plus a half times point, point one times point 36. Plus. Mm-hm. 0.1 times negative 0.04, so that's minus 0.004, plus 0.8 times one. Oh, which is 0.8 again, just like it was last time. Yep. And I get 0.376. Which is what I get by also getting out your calculator. Okay, so 0.376, and you can imagine how we would do that on and on. I want to point something out, Michael, which is that. You decided to figure what the true utility was for the state under X by bashing your head into the wall. But you know that based on the discussion we had earlier that actually the optimal policy would involve going up instead of bashing your head into the wall. What you did at that point was in fact right because everything else in this utilities were all zero. The best thing you could do is avoid getting, avoid ever falling into minus one, so the policy of the very first of bashing your head into the wall, is infact the right thing to do at that point. But what youll notice is that next time around the utility for the X state Is bigger than zero. In fact, will keep getting bigger and bigger than zero. As you can see, it went from .36 to 0.376. Which means that at some point it's going to be worthwhile to try to go up instead of bashing your head into a wall. I see. So this, so this is kind of cool that it works. But, it does seem like a really roundabout way of getting there. I mean, is there some way that we could some, I don't know, maybe take advantage of the fact that there's not that many policies? Yeah, so you actually said something, fairly subtle there, so let's see if we can unpack it. So, lemme point out two things, which I think will get us to what you're answering. The first is. Do you realize that the reason that the value iteration works is because eventually value propagates out from its neighbors, right? The first time we can calculate the .36 without really worrying about anything around it because the utilities are all zero, and in fact, based on the initial utilities. Even for this state here, we do the wrong thing. But after some time, this state becomes a truer representation of its utility and, in fact, gets higher and higher. The right thing to do here will go up. You'll also notice that after another time step I'm going to need to start looking at the value of this state as well. Alright. So, eventually I'm going to have to figure out the value, the utilities or the values of all of these states and this plus one is going to propagate out towards the other states. Where this minus one will propagate out less because you're going to try to avoid falling in there. So that makes sense, right? But what's propagating out, Michael? What's propagating out is the true utilities, the true values of these states. But what's a policy? A policy is a function from what to what? States to actions. Right, a policy is the mapping from state to action. It is not a mapping from states to utilities. No, that's what U is. That's what U is. Or that's what you are. So, [LAUGH], so if we have U, we can figure out pi, but U is actually much more information than we need to figure out pi. If we have a U that is not the correct utility, but say, has the ordering of the actions correct, then we're actually doing pretty well, right? It doesn't matter whether we have the wrong utilities. I see. Uh-huh. You'll remember we did this in the, the first third of the class as well when we noticed that we were computing in the Bayesian learning case actual probabilities. But we don't really care about actual probabilities. We just care that the labels are right. There's a very similar argument here. We don't care about having the correct utilities, even though by having the correct utilities, we have the right policy. All we actually care about is getting the right policy. And order matters there rather than absolute value. Does that make sense? Yeah, that's interesting. It's almost kind of like pi is more of like a classifier, right? It's mapping. Inputs to discreet classes and the user kind of like, more like regression where its mapping these states to continuous values. Right and given one, given the utilities, we can find pi, given pi there's an infinite number of utilities that are. Consistent with it. So, what you end up wanting to do is get a utility that's good enough to get you to your pie. Which is one reason why you don't have to worry about getting the absolute convergence in [UNKNOWN]. But it gives us a hint of something we might do that's a little bit better with the policies that might go faster in practise. So, I'm just going to take three seconds to give you an example of that. Okay? Awesome. Okay so Michael let's see if we can do something that might be faster and just as good as what we've been doing with value durations. And what we're going to do is we're going to emphasize the fact that we care about policies, not values. Now it's true given the true utilities we can find a policy, but maybe we don't have to find the true utilities. In order to find the optimal policy. So, here's a little sketch of an algorithm okay, so it's going to look a lot like value to ratio. We are going to start with some initial policy, let's call it pi sub zero and that's just going to be a guess, so it's just going to be an arbitrary setting of actions that you should take in different states. Then we're going to evaluate how good that policy is, and the way we're going to it at time T. Is to calculate its utility, which I'm going to call U sub t. Which is equal to the utility you get by following that policy. Okay? And I, I'll show you in a moment exactly how you do that, alright? But I just want to make certain that you, that you, you kind of buy that maybe we can do that. So, We have, given a policy, we ought to be able to evaluate it by figuring out what the utility of that policy is. And, again, we'll talk about that in a second. And then, after we know what the utility of that policy is, we're actually going to improve that policy in a way similar to what we did with value duration, we're going to update our policy, Time T plus one, to be the policy. That takes the actions that maximizes the expected utility based of what we just calculated for [INAUDIBLE] of T. Now notice it will allow us to change Pi over time because image that we discover that in some state that actually there is an action that is very good in that state. That gets you some place really nice gives you a really big reward and that went on and you do fairly well. Well then all other states that can reach that state. Might end up taking a different action than they did before, because now the best action would be to move towards that state. So these two steps will actually, or can actually lead to some improvement of the policy over time. But the key thing here is we have to figure out exactly how to compute u sub t. Well, the good news is we know how to do that, and it's actually pretty easy. And it boils down to our favorite equation, Doman's equation. [LAUGH] So our utility at time t, that is the utility that we get by following a policy at time t, is just well, the true reward that we're going to get by entering that state plus gamma times the expected utility, which now looks like this. There, does that make sense? Do you see that? Okay, hang on, it looks a little different from the other equation. So did you mean for it to have T UT is defined in terms of UT and not UT minus one? Yes. Okay, that's interesting. And the max is gone but instead of max, there's a policy. Stuck into the transition function. Yep. A choice of action is determined by the policy. Right. And that's actually the only difference between what we were doing before is that rather than having this max over actions, we already know what action we're going to take. It's determined by the policy we're currently following. Okay, but isn't this just as hard as solving? The thing with the max, you said? Well, what was the problem that we were solving before with the max? That was the Bellman equation. Yes, but we were solving a bunch of equations. How many of them? N. So we were solving n equations, and how many unknowns? N. What's the difference between this, n equations and n unknowns, and the other n equations and n unknowns? Well, n is the same. Mm hm. There's no max, though. There's no max. And what was it that made solving that hard before? It made it, the max made it non linear. The max is gone now. You're saying this is, this is a set of linear equations? Yeah. Because, well, there's, there's just a bunch of sums. And the pi is not like some weird function. This is just effectively a constant. I see. So now, I have n equations, and n unknowns. But it's in linear equations. And now that I have in linear equations and unknowns, I can actually compute this is a reasonable amount of time, by doing matrix inversions, and regression, and other magic hand wavey things. That's very slick. Yeah. It's seems, it's still more expensive than doing the valued [UNKNOWN], I guess. Yeah, but you don't have to, perhaps, do as many iterations as you were doing before. So once you've evaluated it, which we now know how to do, and you've improved it, you just keep doing that until your policy doesn't change. Very cool. Mmhm. And this does look a lot like value iteration to you, doesn't it? Yeah, though is seems like it's making bigger jumps somehow. It is, and that's because instead of making jumps. In value space, it's making jumps in policy space. Which is why we call this class of algorithms Policy Iteration. Cool. Right. Now, this inversion can still be fairly painful, it's, you know, if we don't worry about being highly efficient, you know, it's roughly n cubed, and if there are a lot of states, this can be kind of painful. But it turns out there's little tricks you can do, like do a little step evaluate iteration here for a while to get an estimate and then, you know, kind of cycle through. So there's all kinds of clever things you might want to do, but at a high level without worrying about, you know, the, the details of constants, this general process of moving through policy space. And taking advantage of the fact that by picking a specific policy you're able to turn your nonlinear equations into linear equations turns out to often to be very helpful. So, is it guaranteed to converge? Yes. Nice. Well there, that was easy. I'm, I'm not going to go into it but, you know, there's a finite number of policies. You're always getting better so eventually you have to converge. It's very similar to the, or at least intuitively it's very similar to the argument you might make for [UNKNOWN]. Cool. [SOUND] Whoa, that was really weird. Yeah, that was awesome, except that I noticed that I've come back and it looks like your handwriting not mine. Yes indeed. And in fact, since control has come back to me anyway, maybe I can add a little bit to that lesson that we just watched. What could you possibly add? That's a good question. I was thinking about saying a little bit more about the Bellman Equation. And how it relates to reinforcement learning. Okay, that seems reasonable. Because, ultimately, we're going to have to connect these things up. Seems fair, looking forward to it. So this is the Bellman equation that we were just talking about, right? So the value of a state is the max over all the actions, the reward that you get for taking that action on this state plus the discounted value of the state you end up in weighted by the probability that you end up there. So this should look familiar, right? No, it doesn't look familiar at all. I mean, there's a V, there's a max on the outside, there are more parenthesis, it's a reward of SA instead of, what? You're making this up. So, first of all, parenthesis shouldn't be an issue, right? Parenthesis just let you group things, but yeah, I think you did actually explain this a different way in the other lecture. So, I use v for value instead of u for utility. Okay. And, I think of rewards as being issued as a function of taking a state in an action so taking an action in a state. Which is to say that you're in some state of the MVP. So in particular if we're in some state S and we take some action A, we get some reward for having done that R(SA) and then we land in some new state S prime I guess that is different then how you were talking about it before. Yeah when we went over the definition of an MVP I did point out that you could think of reward as a function of state, or as a function of state in action, or as a function of state action and next state. And, they were kind of all mathematically equivalent, so this is what we're going to be doing for the rest of this class. We're going to be talking about it this way? Yeah, I'm used to thinking about v as being the value function, so I think I'm going to slip into it no matter what, we might as well just switch to it. Okay, so we're going to do V's, we're gong to do R S of A's, but everything that we learned before in our Scooby Doo flashback still holds now. Yeah, yeah. It's notational difference and otherwise it really leads you to the same stuff. Okay we shall see. So what the Bellman Equation is supposed to represent is the sequence of rewards received by an agent that's hopping around in the world. It starts off in state S1 it takes action A1, it gets reward R(s1,a1) for that. Then it lands in state s2 and from there it takes action a2 and receives reward R(s2,a2) and ends up in s3 and this whole process just continues ad infinitum. Okay, so you're seeing a bunch of s a r, s a r, s a r, s a r, s a r's. Exactly, right. You can think of history as just being SAR, SAR, SAR, SAR, SAR, SAR. Oh, so it's like a pirate with a reverse lisp. [LAUGH] Yeah, I guess that's one way to think about it. So when we talk about the value of this sequence, it's almost as if what we're talking about is the value given that we start off in some state S. And we notice that eventually, that we're going to get to some new state S, and, that value can also be represented by this value function, so that's what gives us this recursive form, that the value of the next state can be plugged in to represent the infinite rest of the sequence. Sure, that's sort of the whole point. Okay, I'll buy that. Cool, all right, so we're going to now, try to mess this up a little bit, by thinking about infinity a slightly different way. As bigger or smaller? Just starting it in a different place. So we're going to generalize this notion of a Bellman equation to Bellman Equations. And the way we're going to do that is by noting that in the previous derivation. The way that we're thinking about things is we capture this V here, as the repeating structure that's starts with the max a, right? So it's actually this piece. This sort of infinite piece here. So the whole V consists of this sub V in it and this is why this piece gets substituted for the V in the equation. Yeah, and that makes sense. And you have an infinite number of these things going on forever and ever. And I noticed an infinite number of left parentheses and not enough right parentheses. Oh, no. This stripe bracket has traditionally represented the balancing of all left parentheses. Oh, right. That's from LISP. Yeah, it's just a shorthand. Nice. Nice lisp. LISP. See? Lisp. Lisp. [LAUGH] Nice LISP reference. Thanks! But the thing is because this is an infinite sequence, there's actually a number of other ways that we can group this infinite sequence. So let me draw another one. Okay. So we can also group the infinite sequence here. And notice that it repeats like this. So, this expression after the max, the starting from the R and then moving forward repeats again here. And so we can actually make a new equation that depends on that substructure repeated. So let me right that out. So, the idea is, we can actually just give a name to this big outer brace. So we'll call it, Q (s,a) and we'll say it's equal to R (s,a) plus the discount factor times the sum over the next possible state, so the probably of getting there. Times max a2, our next max, of and then there's that repeating structure again so we can just plug in Q of S prime a prime for that. This gives us another Bellman equation that instead of starting at the S point. Kind of starts after the action has started. Well, wait if it starts after the action has started why isn't it Q of a? Okay that's a fair question. So could we have written the Q value of an action, the Q value of this point in the sequence, this way? Right, the reward for leaving that state and taking that action, plus the discounted value of the future. What's the problem with this? Well, I see two problems. One is kind of mathematical and one is sort of semantic. So, mathematically, it doesn't make sense, because where did the S come from, in R(S,a)? Yeah, this is an unbound variable, right? So that's an error. We can't have that. And the second is, when you actually write it that way, so here you're arguing that Q is like V? Yeah. Right? So you're asking me what the qual view. [LAUGH] Of a is. But the value of an action is meaningless, except in the presence of the state where you're taking an action. That's exactly right. So, in particular, we're conditioning our reward on both the state and the action. So it doesn't really make sense to be able to talk about what our future value's going to be if we don't really know where we started. So the idea here is that we started at some state S, we take action a and then we proceed every after. Okay, so that makes sense. All right, so here's a summary of what we're just talking about. We have, actually have these two different forms of the bellman equation. One V which we can think of as standing for value. And one Q which we can think of as standing for quality. But mostly they're just letters from the end of the alphabet that we made up names for. So you had U for utility. This is V for value and Q for quality. But anyway, the point is that these are just expressions, that represent different pieces of this overall sequence of states and rewards and actions. Okay, so I got two questions. One is, why? I mean, why are we bothering to create this quality function, when the value function worked just fine, thank you very much? Well, I'll ask the second question after you give me a satisfactory answer to the first one. So why? So, in some sense because they're both equally good. But in fact what's going to turn out to be the case, is that this Q form of the Bellman equation. Is going to be much more useful in the context of reinforcement learning. And the reason for that is we're going to be able to take expectations of this quantity using just experienced data. And you don't need to actually to have direct access to the reward function or the transition function. To do that. Whereas if we're going to try to learn V values. The only way to connect one V value to the next V value is by knowing the transitions and rewards. So this is going to be really helpful in the context of reinforcement learning. Where we don't know the transitions and rewards in advance. Okay, I'll buy that. So here's my second question then. You've got V function for what happens when you're at a state. You've got a Q function would happen when you're at an action. There's at least one more thing when you're at that infinite jar jar jar jar jar, and that's R. Yes. You're saying we can group things this way, or we can group things this way. So why can't we group things this way? Sure, that's what I'm asking. because if we go one step more than that then we're back to the max again and that was our V. Right. So yeah, that seems to be the third member of our little trio that's been left out. So I think maybe we should have a quiz about that. Of course we should. So for this quiz we're going to actually derive a third Bellman equation in addition to the V equation that we talked about and the Q equation which we just talked about. We're going to derive an equation, we'll call it C, because we want to make sure the front of the alphabet gets a little bit of love. So, just as Q here stands for this part of the equation, starting from the R, and V captures this part of the equation starting from the max. We want to express an equation that starts essentially from where the gamma is. And so I've written four choices that you can select among. Okay, I think I got it. So we're just doing the same thing we did when you did that little clever trick to derive Q. We're going to do the same thing by moving one step down this lot. Exactly, yes. Okay. So we have b of s, and q of sa, and we c of, I don't know, something. So wait, wait here. I don't think I can answer this question until I get something resolved. So the v stands for value. The Q stands for quality so the C stands for. Quantity! No, that's not right. It stands for continuation. The idea was introduced in a paper by Tom Dietrich where he showed that sometimes if the reward function is really complicated it can help to to derive, or re express the Bellman equation in this form where the reward gets left off and we only observe the reward later in the flow. So it has a little bit of usage, but at the moment it's just to try and get you to play around a little bit with this idea of capturing different quantities as it goes by. Okay, I think that's a hint, and I think I'm ready. Go. So which one's right, Charles? I think I know which one's right, but I'm going to try to figure this out by talking through. Is that alright? Yeah, of course. Okay, so I'm going to look at the first one and I'm going to say that can't be right, for sort of a similar argument that we had when we were going from V(s) to Q(s,a). And I said, well why can't you just have Q(a). And it's because the action doesn't make any sense without some kind of state. And so here, you're talking about a continuation you said, from the state, but actually you wanted it to happen after the R. Right. So, it just sort of doesn't work. Also I'll just point out syntactically if you look inside the equation and asking for A, but where is A coming from. All right so that's good enough to kind of rule this one out. Yeah, so let's rule that one out, let's just mark it no. Good I feel pretty good about that. So, I actually think the second one is right, but let's see if I can convince myself that's true. You've gotten your state, you took an action and then you sort of got your reward. And now I want to know what's going to happen next. So, let's see. So the next thing that happens is I'm going to get the discount sorted towards the future. What's the future? Well the future is, I have to get to the next state. So that summation over S prime is my expectation on where I'm going to end up next. It refers to s and a, which are passed in, on the left side of the equation. And then what I'm going to do next, is I'm going to actually take the best action, and then I will see the word that I got for taking the action in this data ended up in. And that puts me in the same place I was in before and so that I can continue so to speak with the rest of the sequence. That seems convincing. That seems to me like a good argument. But before I believe that, let me just sort of convince myself the other two aren't right. So if I look at the next one, c, s, a and r. I actually think that could be right, except if you look inside the equation there, it's the same reasoning holds, I was in some state. Is that prime underneath the summation? Yeah, sorry. Okay, so I'm going to do the same thing to my expectation. Or the state end up in the state. S prime then I take the best action that I can. And then I receive the reward that I received already? That doesn't make any sense. The reward I'm going to receive next has to depend on where I'm going to be afterwards. So the r is in the wrong place so that can't be right. Yeah and also the next call requires the reward for the following state which of course we don't know yet. So there's nothing to pass in here. So this just doesn't work. Right. Yeah, that's right. Actually, you can use the same argument to eliminate the next one. So this is an interesting function. I want to know, I was in a state, took an action, ended up in the next state. It seems like something you kind of might want to keep track of. So, I do the gamma, I'm going to go to the next state, but I'm summing over the next state I'm going to get to, but you passed in already the next state that I'm going to go to. So just on syntactic grounds this doesn't make any sense. And I think the rest of it has the same problem you described before with our prime, which is I now have to know the next state I'm going to end up in. Right, and that just hasn't been computed yet. So this would work if we could go back in time maybe? We've done that before today. [LAUGH] [LAUGH] We have done that before today. But you can only use the Scooby-Doo flashback effect once or twice in a day. So I think I've convinced myself that the other ones don't work and, in fact, the second one is the right answer. Excellent. All right, that's what I was going for. So we now have three double equations, V, Q and C. We're really going to focus mostly on C. All right, sorry, Q, from now on. But I thought we really just kind of see the relationship between these. Okay, well that makes sense. I mean what you're saying, is that we have an infinite sequence of things. And where we choose to start the sequence, really sort of, mathematically doesn't matter, right? But you might choose to do one versus the other because it's easier to think about or easier to kind of learn or to do what it is you're trying to do. And you've argued so far, although you haven't told me why yet, that the Q is kind of going to be easier if you're doing reinforcement learning. Exactly. And actually that makes me think that maybe it would be helpful to have a quiz. We just had a quiz. Maybe it would be useful to have another quiz. Okay, sure. So, all these different Bellman Equations, since they all come from that same infinite sequence, they can all be related to each other, like by blood. Whose blood? Well, [LAUGH] I'm hoping not yours. So, and this is why. So what I want you to do is actually fill in, how do you express the V function In terms of here, say the Q function. How do you express the V function in terms of the C function? In each row here, it's how you express that function in terms of the other three. The Q in terms of the three, and the C in terms of three. And I'll do some to get you started, ta-da! Oh, that's really cool. So why can't I just do that in every single box? No, because you have to express the Q function in terms of the V function for this box. Okay, that's fair. Should I write the equations up there again or do you know them well enough by now? I think I'm okay without the equations being up there. I can always just go back to to the previous video. Good point. Anyway, I've been taking notes all along as have all of our students who are watching this now. I don't doubt that for a minute. Good. I doubt it forever. [LAUGH] All right, okay. So I think I understand. I'm going to now I have to write V in terms of Q using magic. And V in terms of C and Q in terms of V, assuming that's possible. And Q in terms of C, and so on and so forth. All right. So, let's go. You ready to fill some of these in? You might not know all of them right off the bat, but I think we can piece them together. Okay, well if we're going to work together, I think we can do a pretty good job here. Well can I ask you a question first? Sure. Do you know the answers? No, [LAUGH] but I'm confident they exist. Okay, good, so let's work this out. So I do think I know one of them and it's really easy. So the one that I'm going to do that's really easy, is writing V in terms of Q. Excellent choice, sir. Right, so if you think about the equation, which we have conveniently not written on the screen, the value is immediately a max over actions. So I think, it's just the max of over actions of Q(S,a). Yeah, exactly so, very good. So one way of saying that is that the true value of a state is just, you're going to take the best action. Yeah, and so that also gives a nice interpretation of what Q values are. Right, it's the value of each of the actions you could take in the state, the best of which is the one we're actually going to take. Right, exactly, exactly. So I think there's a similar argument to make for C. So I guess, I don't know where you want to put the max, but I guess what I'm thinking of is, you had a state followed by an action, and that gives you Q. But in C, you have sort of the state followed by an action, and a reward. So C is happening after the reward, right. So it should be the reward that you get plus C. Right, so is that right? That makes sense, right? Yeah, that feels right to me. So one of the things that's interesting, already, is that means if we have C and we want V, we still need to know the reward function. But if we want V and we know Q, we don't need any other special knowledge about the way the model works. The Q values kind of encapsulate all that we need in terms of, at least for computing V. Okay, but that's because, let me just see I understand that. I think you're right, that's a cool insight, but basically, we need to know what the reward was after we did the state. So we yeah okay that makes sense, we have to kind of keep that with us. All right I'll buy that. Okay, so let's see what happens in some of the other boxes. Right, so this is harder for me, because I think doing V in terms of Q and C is easier because it's the future. V is the like the first thing before the future comes along. And Q and C captures the next set of things in the sequence. But here, I guess I want to know how that's going to work. Well, I like the way that you started when you thought about this box here. You said that the V equation starts with max A. Well what does the Q equation start with? Good. [LAUGH] Right. And then after that, oh, well once you have the rewards, it's just, discounted value where you're going to end up next. So what's the end of this thing? V of S prime. Yes. Right, so the trick there, is to realize that you don't have to go backwards in time. You just have to unroll the infinite sequence, until you get to the point where you can now express it in terms of V. Exactly, and that's the trick to all these, in fact. Right, so, in fact, I don't know how you're going to fit this, but I could just substitute what I learned about how V can be expressed in terms of C, by just substituting that bit, for the V that you did in Q. Express in terms of V. So it's as if we fill in the same answer that we did, and then we say, okay, well what's V? We know V in terms of C looks like this, and we just can keep, [LAUGH] I guess we need to keep doing that. That can't be right. Why not? I mean, I don't think you have to do that. No, because in fact. This is already C. Exactly. So that's not so bad actually. But again, to getting Q from C requires us to know R. Getting V from C, requires us to know R. Getting Q from Q, doesn't require us to know anything other than Q. And same from V from Q, Q from V, we need both R and T. Great. All right, so let's do this last row. Wait, I have a question. Sure. Do I need a gamma in front of C for Q? I'm going to say no, because the equation for gamma starts, sorry, the equation for C starts with gamma. Oh, you're right. Okay, yeah, yeah that's right. Okay, cool. All right, so how do we do this one? We'll start the same way. We'll start off, just writing the definition of C as it is. Which is usually then what? Plus? The next reward I'm going to get followed by C. No, I'm missing something. Wait what am I missing? This is where me not having written down the equations, now comes back to bite me. There's a max of A in there somewhere. I ended up in S prime, so now I need to take an action. So it's max over A prime, then I'll get the reward, and then I get C. Yeah, so we could do [LAUGH], we could write C in terms of C, but that's not what we're trying to do. We were trying to write C in terms of V. The thing with the max in front of it is V. Boom. Right. Because it already contains everything we need to know. Right, yeah. So in particular, the continuation for being in some state taking some action is the discounted next state value, which we already have in this case, we have V. Yeah, that's exactly right. All right, last box! All right, so there's a similar argument there. I think you just write the same thing, and then just replace V with, max A prime over Q(s,a). There you go. Yeah, that seems plausible. So is everything that you said before still true about how I don't need to know R? I guess so. Read down the columns. If we have the C function, we can get the V and the Q from it if we just have R. If we have the V function, then we can get the Q and the C, but we actually need the T, the transition function in the case of getting C, and we need the reward function and the transition function to get Q. And if we have the Q function already, then we don't need anything special, except for if we want the continuation, we need the transition function but not the reward function. So this is why that the C function turns out to be a really useful form, if the reward function is hard to represent. We're going to be able to kind of work around it. And Q function is actually really powerful, because it gives us answers to lots of other questions without having the know the model. That makes a lot of sense to me. Okay, that's good. That's really cool. So, what we should do next I think, is talk through how we can actually make a reinforcement learning algorithm out of this idea. I like that. All right, so this seems like a good time to stop and ask ourselves what have we learned? Oh, you're asking me? I am indeed, I'm sorry. Okay, that's not a problem, I think I know the answer. So the first thing is we relearned everything we learned from the MDP lessons from before. So, we learned all about Scooby Doo MDPs. [LAUGH] I don't think that's a kind of MDP. It is now. We should write a paper, quick. And so that included things like what are states, and transitions, and rewards, and so forth, right? Right, I think. Anything else? Well, I think we learned a bunch of new things. So for me, I learned all about Q functions Mm-hm I learned about continuations And, in general, when I think about that, I really learned about what the Bellman Equations, as opposed to equation, are really trying to capture for you. Even more than we did when we talked about that with MDPs. Alright, so deeper understanding of the Bellman Equation and how it relates the rewards in this infinite sequence of rewards to each other. Yeah, that's sort of what I was going for. Yeah, and I think that's a lot. And I think I learned that notation matters. [LAUGH] Okay. [LAUGH] No, remember we talked about the reward in the Scooby Doo MDP. We talked about R is a function of S. And when we talked about Q functions and C functions in the Bellman equations, we went to R being S of a. And even though we had mentioned before that they were mathematically equivalent, it turns out, just like sometimes thinking about Q, sometimes thinking about C might be better than thinking about V or U. It turns out that sometimes thinking about R of S of a, Is better than thinking about R of S, even if they really are mathematically equivalent. And in some sense it doesn't matter though, right, because we actually were able to do similar things with the R, S as we are with the R, S, a. True, although, you know what, that's not really notation, right? Notation is used doing V instead of U. This is really a representation. Ooh. Representation matters. Yeah, and I think that's actually a pretty important lesson. And when I think about some of the other things that we know about reinforcement learning, the representation is going to make a huge difference. I think that's right. I think that's a good lesson for I want to say AI in general, but certainly machine learning. Okay, well that seemed like a profound point to end on. [LAUGH] Okay, cool. So we'll pick it up with reinforcement learning next time. Done. Let's do that. Have a good time, Michael. You too. Bye. Hi Michael. Hi Charles, How's it going? It's going quite well, how's it going with you? Not too bad. What are we going to talk about today? Well what does the screen say? It looks like a wordle. Yes it's a wordle. It's generalizing generalizing and the key words are. Abstraction, scale, search. What we're going to do is I'm going to try to jump off from some of the discussions that we had earlier that you lead, about generalization. And try to sort of hop up a level above that and talk about abstraction and scale and search and sort of the things that make machine learning and reinforcement learning, in particular, hard. Okay, so, but it seems to me that I talked about function approximation, and everything is a function approximation. So, why is there more to say? Well, we'll find out. That's the whole point of today's lesson. [LAUGH] All right. Okay, so, let's get going. So in order to elaborate on all of that, Michael, I want to ask you a simple question. And here it is. What makes reinforcement learning hard? I'm going to say living on the streets. Makes reinforcement learning hard? Yeah, it sort of hardens it. Let me let me ask that question slightly differently. Okay. From a computational point of view, from an algorithmic point of view, what makes reinforcement learning hard? So well there's a lot of things we've been talking about. There's the notion of delayed reward is a very tricky thing. I like that one, I like that one enough for me to write it down. You're not going to wait? No, I'm not going to wait, I'm just going to write it down right now because it's ironic that way. And a related issue well there's two pieces in that. There's a delay and there's a reward. The reward part is like unlike say supervised learning reinforcement learning is trying to do its job based on very weak feedback. That it's only told how it's doing and not necessarily what it should be doing. Right which makes it very different from normal function approximation, in the way that we think about in supervised learning. Okay, what else? If we do try to use something like supervised learning, one thing that makes the function approximation hard is the idea that it's kind of a moving target. We tend to base our estimates on our current estimates, as opposed to anything kind of legitimately valid in the world. Right, so doing estimates based on estimates is, well, like averaging averages. Okay, I want to say bootstrapping is the issue. Yeah I agree with that. Bootstrapping is sort of a problem. I think I would probably summarize that as the problem with the need to do good exploration. And that's a little different from the normal supervised learning set up where you've got a bunch of training examples. Here because you only have estimates in the reinforcement learning context, you have to do a bunch of extra things to actually figure out how to get good estimates so that you can actually do good learning. So one more thing I think sort of worth mentioning that makes this sort of hard and it's kind of very basic kind of computational problems, right. You know when we describe computation complexity we talk about it in terms of big o. And the question is big o of what? So what's the what here? What's the in, in our big o that makes ROR? Well there's a lot of them. Like what? Well I mean if we're thinking about sort of M.V.P. style reinforcement learning then the size of the state space is a big o. Mm hm. The size of the temporal window in some sense, like how how delayed is the reward. Mm hm. There's issues on what could be the actions as well. There's lots of things that could scale. But I guess what makes state super hard is that you can very easily specify a problem that has a state space with an exponential number of states that is still relatively simple, but algorithms that don't generalize between states are not going to be able to learning them very effectively. I think all of what you said is true. Some of it, I think, is kind of captured not necessarily as explicitly in the first two bullet points. But I'd like to focus on states and actions. The problem is that the learning that we need to do grows super linearly in general, we aren't very, very careful and smart in the number of states and the number of actions. And so we really need to have kind of clever ways of thinking about learning across multiple states and perhaps multiple actions. And in fact when you talked about function approximation and generalization, I would claim that at least in sort of this discussion, we could talk about that as a way of dealing with generalizing overstates, that you did function approximation over value functions. Right, so here you tried one way of dealing with the problem of scale when it comes to states. Is well, if I learn something about one state, I want to somehow have that teach me something about a bunch of other states. And then I won't have to visit every single state an infinite number of times, and take every action an infinite number of times. And that's really good, that's really important, right, and the trick here is I can see many, many states by just seeing a single state and that's what function approximation over the value functions actually gives me. Agreed? Yeah I don't know that I'd say it quite that way, but that we learn about many states when we learn about other states or something like that. But yeah, that's absolutely the way I was thinking about function approximation is helping us to kind of fill in the gaps for states that we haven't visited nearly as often. So in fact, I could be aggressive and ask you the question, why did you do all those function approximation over states but you didn't do any function approximation over actions, Michael? That would be aggressive And the [LAUGH] I don't have a wonderful answer to that. I mean that it's tradition, in the sense that a lot of the MDPs that people have been working with really do try to keep the action space relatively small and expand in the state space but that's not really that good of an answer. I think action, large actions bases are really important tom we just don't have as good ways to deal with them. For one thing the computing the max ends up being hard in q learning. Right so in fact what I'm going to try to talk about in the rest of the lesson is trying to do To generalizations of the sort of generalization that you are talking about and think about how we can learn about actions and how we can abstract over actions in the same way that you are abstracting over states. That seems great and a little bit meta. A little bit meta. And so that's why the lessons generalizing over generalizing and the whole point in the end I'm going to claim other than just sort of it's interesting in and of itself is that it's going to allow us to scale. So all of these tricks ultimately have a kind of practical use including function approximation of allowing us to scale. Okay Michael so to demonstrate this idea or to demonstrate some of these ideas I'm going to look at grid worlds. One a very simple grid world to try to get across a pretty straightforward idea, at least what I hope's a pretty straightforward idea. And then a slightly more complicated and prettier grid world in about a moment. But first let's look at kind of a basic grid world. So this is like our famous four by three grid world except I added a couple of details. I haven't drawn all the little squares and all the little tiny grids. Because really, I just sort of want to pop up a level here. But you start somewhere in this world, it almost doesn't matter where, and you have to try to get here. To the little plus one that's where all the reward in the world is. And you have the normal actions. You can move up, down, left, right. And you know it's slippery. So every once in a while when you try to move up you end up moving in a perpendicular direction. And the usual sort of thing that we normally do. Now other than that, what's different about this world is that I've got walls, like we had before. Except these walls actually happened to coincidentally create little rooms. So let's imagine that you're starting over here somewhere and your goal is to eventually learn in this world how to get to, well let's go. So I think we know how to do this. I think we've done this before many many times we certainly did it in the four by three grid world but I'm going to ask you a slightly different question. I want you to think about this in a slightly different way. If you were trying to tell a person how you should get from this point to this point what would you tell them? I'm going to say Google Maps. This is not Google Maps. This is in a GPS dead zone. What? I know, I know it's amazing. Wow I guess that's one of the real benefits of reinforcement learning is it can work even even a GPS dead zone. So the question is how is this? How would you tell them to go? Well, so, you know, we talk to agents via policies. So I could try to give a policy. But you didn't really get. So that's a mapping from states to actions. But you don't really give me a lot of details about what the states where. So probably if I was talking to a person I would say something kind of high level like well pass through the doorway to the east. Then you enter a room and there's a doorway to the south. Go through that and then a little bit inside the doorway you'll find a great reward. Or at least a reward. Plus one is the greatest reward that anyone can receive Michael. You know that. [LAUGH] So right and I like this because what you did is I gave you the actions up down left right. And you could have said okay so move right, right, right, right, right down right, right and then go I don't know right, right down, down, down, down, down, down, down, down, down, down write or something but instead you said well go to the doorway and then go to another doorway and then once you're past the doorway, you know you'll just take a couple steps and you'll end up where you want to be where the great reward is. And that's perfectly reasonable in fact it's more than perfectly reasonable it's a kind of abstraction in the same way that function approximation is a sort of attraction of the states which you just described is what we call temporal obstruction. I see. And you know that has a specific meaning but I think to really kind of grasp what is the right way to think about it is you came up with new actions That's making the action space bigger. Making the action space bigger which is a problem you would think but each of these actions actually covers an enormous amount of ground. So in fact, if you had said right, right, right, right, right, right, right, right, right, down, right, right, right, right, right, right, right, right, right, down, down, down, down, down, down, down, down, down, down, down, down, down, down, down, down, down, right or something like that, that's a lot of actions to have to reason over over a long period of time. But what you actually said was, take one action, take another action, and then, I don't know, take one or two actions. So we managed to do with your new actions in, I don't know, about five steps what would have otherwise taken, I don't know, 100 steps or maybe That's cool idea. Maybe 60 steps that's right that is where the abstraction comes from and where the temple nature comes from. Because again, one of the things that makes reinforcement learning hard is this delayed reward, and what's implicit in that is that you have to reason over time that you have to take many, many steps and because all the options there are several options at every state you're actually getting an exponential blowup over all the paths you might take. And here what you've done is you've traded off the price in adding new actions with being able to skip over, having to make a bunch of decisions along the way. And so now I can do in five steps would have taken 100 steps and that's going to be a gigantic exponential savings. Does it make sense? Yeah, and I appreciate you giving me credit for thinking of this idea, but the reality of it is you kind of forced my hand by not. I couldn't said right, right, right, right, right, down because I don't see the low level grid positions. So I don't know if it's 100 or 1,000 or 60 or anything, but cool, I mean that gives me credit for something that I don't really deserve. Well, but I did tell you that you could go up, down, left, right if you'd said well, I'd tell you to go right, I just would move right a little bit. And you would have seen where I was, and you would have, I gotta keeping going right, keep going right, keep going right. And yeah, I could have made that as granular as I wanted to, but it has to be fairly granular. There's no way to get from here through one of the doorways, which you absolutely have to do. So let's be a little bit more concrete. So let's write down exactly what that action was. I don't know what it is, but you created a bunch of actions that I'm going to say are things like goto doorway. And we can even name those doorways. Let's call therm x. And so that would be one, two, three, four more actions, which are going to allow me to save possibly many, many steps. So let's think about that. What if I had started here? What would you have told me? Well, in principle, the plan that I gave you could still work. Just go to the doorway to the east and then go to the doorway to the south. Right, and by the way, well, go ahead. Well, it seems like the room layout that you have makes it possible to go kind of the southerly route as well. Right, there's some point where going east and then south is not as good as going south and then east. And I don't know exactly where that point is, but I'll make it easy for you by drawing a dot there. [LAUGH] What would you tell me to do there? I would totally tell you to go east and then south. Right. So east to the doorway, south to the doorway, then down. Right, so for a whole bunch of states inside of this room, no matter what I do, the action is the same, even though I'm in different states. I should still just go here. I take one action for one particular state. So the plan itself and sort of what I'm searching over is actually much simpler, and that would be true for, I think, probably any space inside of this room. Your answer would be? To go to the south door. Right, and not to drive the point home too much, but what about from here? Well, I think it's obvious, you tell me to go here, from here, you tell me to go here, from here, you tell me to go here. At some point, there's a switch off between whether going through this door or going through this door is the right thing to do. But there you go. And then once you're in the room, well then, you're going to have to take, I guess sort of baby steps towards the goal. But regardless, I'm going to claim that this is an easier kind of space to reason over than having to live in the original action space. Notice I've changed nothing about states. I've said nothing about function approximation over states. I'm talking about abstraction over this set of actions that you have. Well, you did implicitly talk a little bit about states when you said that all the states in say the northeast room end up being sort of equivalent. Right, equivalent with respect to the action that you want to take, yes. Yeah. Right, which is its own kind of approximation, its own kind of a track. All of these are different kinds of abstraction. Okay. And will allow us to do scale. So let me just add one more thing to this before we move on to the next line. And what I want to point out is that this isn't just a way for you to think about this problem or to sort of in a compact and efficient way explain how you would accomplish a particular goal. It actually has other computational benefits. So if we think in this sort of southeast room here about how value propagation works, right. You took a bunch of actions in here. You got to this good space. Then you start backing up values, right. So I know this is worth a lot, because one is the biggest number in the universe without loss of generality. Assume one is the largest number. And then I'm going to learn that being in this state and taking this particular action is also good. I'll just draw a little dot here. The big size thing, that's really good, and a smaller dot, a slightly smaller dog. And so the value of the states around here are nice and big because I can easily get from here to the goal state. And as I move farther away the value gets to be a little bit less and then a little bit less and a little bit less, and it sort of propagates out into the other room. And the way that you propagate this information is actually kind of slow, right. You have to get to the state and sort of back it up a little bit. Your horizon is such that the value seems relatively small and you kind of keep going. And you have to visit this often, very often actually, to back up the information to tell you sort of what the value of the states are, what the Q values of state-action pairs are. And that's fine, that's how we learn in MDPs. But, because I have this abstract action, I have this go to doorway, I can actually from here, from this particular state at the doorway, back up information all the way to here. Wherever it is I executed that particular action in sort of one step. So I learned a lot about the value of being in this state immediately from having taken that and got here. So soon as information from my little plus one box here gets me to here, I can back that information all the way up to where initially executed the go to doorway x. And similarly, now that I've got information here, I can back it all the way up to the value of being in this state. And so I can back up information in value much more quickly without having to back it up sort of one little state at a time. So there's a lot of advantages here sort of computational and otherwise that will hopefully allow us to learn faster. Cool, and I think that's helping me understand why it's temporal abstraction, right. So state obstruction is treating a whole bunch of states sort of equivalently. This is actually treating a whole bunch of time steps equivalently, right, that can just kind of align over them. All right, and now you'll notice that it will have the side effect of allowing us to treat a bunch of states similarly, because we know that one way of thinking about similarity between states is how their features are similar to one another. That's what you talked about. Another way to think about similarity among states is to think about whether you should take the same action when you're in them. And so every single state in this room is the same in the sense that the optimal action is the same, go to this doorway. We're going to make all this little hand wavy stuff that I did, which I like, a bit more concrete, which you should appreciate because I'm making abstraction more concrete. Thank you, thank you very much, I'm here all week. In order to do that, we're going to actually introduce a real formalism called options. And options are just a way of writing down explicitly what we mean by this temporal abstraction new actions idea. Okay, and is this related to concepts like the financial version of options? Like stock options? I'm going to say no because I've never really understood those things. So maybe they are related. Probably not though, I'm going to say that options are just a fancy way of saying choices is a fancy way of saying actions. Here's what an option actually is. So an option is a tuple because everything in life is a tuple and it has three parts to it. So in fact it's a triple, it has I which is the initiation set, it has pi which is almost always the case a policy, and then it has beta which is a termination set. So let me explain what I mean by those things. So the initiation set I is just the set of states where the particular option or super action, like go to doorway in the south, is actually legal to be executed. So this is where it may start or initiate. So there's a set of states where an action actually makes sense. So let's take your action, which was go to the the door to the south, right? So that's clearly legal inside the northeast corner, but I think the way you meant it anyway, it's not really something that you would execute from, say, the southwest corner. That seem reasonable? Yeah, I feel like it sort of violates the spirit a little bit if it's the case that you can navigate to that south doorway from anywhere. Then why not just navigate to the goal from anywhere, right. We're not really breaking it down into simpler subproblems. Right exactly, so in fact we should just have one action which is solve the problem. Right. And you should just execute that. That's going to be at least as hard to learn as the original problem and probably as hard to execute. If you already had that then there was nothing to do in the first place. Is the aesthetic here sort of like, I don't know, algorithm design in computer science? The sort of idea that you want to break a problem up into subproblems but somehow have solved a little bit of the main problem along the way in thinking of things like Quicksort and Mergesort and so forth where you recursively solve the same kind of problem, but it's not cyclical. It's not like you make no progress because at each time you break down the problem it actually makes it a little bit easier. Right, so in fact I think the word abstraction here is quite important, we do abstraction when we code, right, we create functions. And then we use those functions to think about the problem at a more abstract level and it allows us to solve the problem more easily. So it's yes, I think that's a pretty good analogy, it's not a perfect analogy, there's not really an input and an output here. But it's trying to solve the same idea of modularity and re-usability for that matter. And it allows us to make the problem easier to think about and easier to solve. Cool. I like how computer science comes up even inside of computer science. Yes, computing is everywhere it is computing all the way down. So we have this initiation set. So that's places where it's, let's say, legally reasonable to actually execute this option or this abstract action. Similarly, we have termination sets. And a termination set is not the set of states where an option may be ended, it's actually a generalization of that. It's a set of probabilities. So, it maps states to a number, which is the probability that the option will end in that state, or can end that state. I think that makes it sense. So they're both they're both kind of sets but this is kind of a more soft general kind of set. And is it saying that no matter what, when the option gets to one of those termination states a coin is flipped, and if it comes up heads then the option's done? I think from the point of view of the person sitting outside the option, yes. Inside the option it's not quite done that way. It's not like the option is running and it's always flipping a coin. This is more sort of a a way of describing the fact that something can end in a particular state. Now, of course, you could turn this into a set by putting a probability of 0 over all the states, where you don't want it end and having a probability of 1 exactly where you want it to end. I see, so that's a sense in which it's a generalization of that idea? Right, exactly. But basically, I'm going to execute this action starting from one of the states in set I. And it's going to run and it will in end up in some state when the option terminates. And this gives me a probability of it being in one particular state or the other. So all an action is, is it's, well you're in a state, you take an action and you end up in another state. But since we're talking about these temporally abstract options, things that happen over time, we need to know what's happening inside the option while it's running. And we already have a term for that. It's called a policy. So, here we have a generalization of the way we've been talking about policies in a way. Within an option of policy is more than just a mapping from states to action. Here, it's a probability of being in a state and taking a particular action. But again, we could turn that into a normal policy, as we've mostly been talking about it, by just having a probability of 1 over the action that we're going to take. And 0 zero over everything else. And then it is just a direct simple mapping from state to action. Like a deterministic Markov memoryless policy? Thingamajiggy, yes, exactly right. But, anyway, this is a formalism, but I think it sort of is just trying to capture this idea of, as you say, kind of a procedure or a function that runs. There are places where you can execute the function. There are states where you'll end up after you've executed the function. And there are things that are happening inside the function. The code itself is the policy. It's the actions that you're going to take in the states that you're going to see. Hm, okay. Okay, so let's see what we can do with that. Okay, Michael so enough of all this hand-waving you like to do. Let's actually write out some equations. [LAUGH] Okay. There's no need to fear the equations Michael. So I'm going to leave our little picture up so you can remember what it was we're talking about, and I'm going to leave out the definition or the tuple part of what an option is so that you remember that and then I'm just going to write an equation up here and here it is. So V sub t + 1 of s = max over O of the quantity R of (s,o) plus sum over s prime of s, o, s prime times V sub t of s prime. What does that equation look like to you? It has a lot in common with the Bellman equation. Right and in fact that's exactly what it is. It is a rewriting of the Bellman Operator. So what do you think the various constituent parts are? Maybe an easier thing to do would be to explain what looks like it might be different. Okay. So for one thing, a has become o, because I guess actions have become possibly temporally extended actions. Or options. Or options, good. Do we need to include a in there as well? Why, how can we ignore all the a's? Well because the a's are inside the o's. The a's are inside the o's. Yes. There's actually an o inside an a. Well I think that depends upon whether it's a lowercase or capital a. That's fair. Thank you. But i still don't quite understand. So, you're saying options are made out of actions. Yes because there may have little pulses. But can you have an option that is just an action? Of course, right. So options are a journalization of actions. If I wanted to take an action and I wanted to turn it into an option, then the initiation set would just be everywhere where that action is valid. The policy inside that option would just say for whatever state I'm in, take that action with probability one and all other actions with probability zero and then the termination probability would be one for all of the states that I can reach from that particular state to particular action. Actually would just be one everywhere. One everywhere. I see. I say, that's okay. That's actually really cute. That kind of helps me understand why these three quantities are the way that they are. So it allows it to specifically say, yeah, well we can always include an action which is a primitive action, that just, you can use it whereever you can use the action. It just does the action and then it stops right away. Right exactly. Okay, all right, good. So then the o, okay, I get the o's generalizing the a. Then another difference is there's no discount factor that I can see. And then the last difference is that we seem to have spelled T as an F. Right well, F and T are the same for an appropriate kind of thinking about. Only in logic. Only in logic? Yeah T and F are the same. Actually they're the opposite in logic. Whoops. Yeah, T is true and F is false. I hadn't thought about that. That's kind of an interesting thing that we could talk about for an hour and a half or I could just explain to you what F is. So you did the right thing, you noticed that we still have a reward function as we did before in the Bellman operator. We still have value functions, is what we're trying to do, trying to update value over states. We have little s's and the s primes, the o's are basically a, so that doesn't seem different, but the two big differences are a lack of a discount factor which isn't quite true. We actually have a discount factor but it's hidden and instead of a transition model we have F. So F is, this really is a Bellman operator. F is sort of, I think, clearly playing the role of a transition function. It's somehow telling you something about states, options and next states. And what I really need to do to make this clear, I think, is to just really write out what F is. That would be super helpful. That would be really nice, but it turns out that I have to write out something else as well and that is R. So R look sort of superficially like the reward function that we had before, although, in this case the reward is from you know taking a particular option a particular state. But actually this reward function is hiding a lot of complexity as well and to see what the complexity is I think it helps to draw a picture. So what have I drawn down here? So what I've drawn is an MDP and a particular trajectory through that MDP. So here are the dots or states and the little lines are sort of the actions that you took to move between those states, and whatever awards you happen to pick up along the way. So I'm in some MDP, a Markov decision process and I am moving through states taking actions and receiving rewards. And I'm sort of moving through that, and that's kind of what we've been thinking about before, right? This is what the Bellman equation sort of captures for us. We're going to be in a state, take an action, end up in the next state, maybe getting rewards. I should accumulate all that and use that to update my estimate of the value of being in a particular state. Right, that all sound familiar? That is a diagram of SAR, SAR, SAR, SAR, SAR. Right this is SAR, SAR, SAR, SAR, SAR, SAR, SAR, SAR, SAR, SAR, forever and ever. Now we know how this works. One of us, probably both of us at some point or another, actually started explaining the math behind here and how you would back up values and why. This is a way to do learning. But in this space where we have these temporal abstractions, we have these options. We're actually violating everything that we know about MDPs. And I think it helps there to draw a slightly different picture. So because we have these temporally abstract options, we are not having very simple time steps, right? So here, kind of implicit in this, is we're in state, we take an action, and then we're in another state. We're in a state, we take an action, we're in another state, so on and so forth. And how much time passes between visiting each of these states, Michael? I'm going to call it one time step. One time step, which I'm going to claim, you may not agree with this, might as well be the same thing as 0 time stamps. In some sense, we treat the actions in an MDP as if they are instantaneous. And perhaps, to drag on more of the computer science analogy, we treat them as if they are atomic. Okay, I'm along with you for the atomic idea. I definitely am not along with you in the it takes 0 time idea, but I guess, your time in the sense that there's no, I don't know, there's no crack in it. There's no window that anything else can jump in, and that's really the idea of atomic, so I think we agree. Yeah, I think we do agree. You're basically, we're picking constants, you want to call it 1, I want to call it 0. We can both go with that. But that's like me saying T and F are the same thing. 0 and 1 are not the same thing. They're the same thing within a constant. [LAUGH] Not a multiplicative constant. [LAUGH] Fair enough. But the point here is that, in some sense, atomic, instantaneous, what really is happening here, I was in a state, I took an action, I end up in another state. Nothing else can happen. We treat that time stamp. It may be a year, it may be a day, but whatever it is, we treat them all as if they're the same and we're just moving along. In fact, we talk about time steps. And implicit, I think, in the way we've been talking about time steps so far is that they're all the same length, whatever they are, which is why you can call them 1 as opposed to 7. It doesn't really matter. The fact is that I take this, there's an atomic action taken, I end up somewhere else. Nothing else happens in the middle. I can reason about everything. By contrast, once we have these options, I mean, think about this picture up here. Think about the upper northeast room. If I'm in this little pink state here, I just entered into the room, I'm going to get from here, the east door, to the south door in some amount of time, right. It's going to be made up of atomic actions, because all I really have is up, down, left and right. And I don't know how long that's going to take. Let's say it takes 50 time steps, it doesn't really matter. It's going to take the same number of time steps. But if I'm in this little space down here near the bottom, already near the door, that's going to take fewer time steps. So what I'm actually seeing, with respect to the underlying MDP, is I start in some state. I take one of these options, some amount of time passes, often more than the single time step, and then I end up in another state. And then I take another option, end up in another state, take another option, I end up in another state, and so on and so forth. And the amount of time that I spend in each option is variable, potentially. And is not necessarily the same as the atomic actions that we had before. Interesting, okay, so variable time steps. Right. So does that mess up everything? It does, because if you think about the way Bellman works is it assumes a discount that happens at every time step and it accumulates information along the way. So we need to have a way to capture all of that, that allows us to get all of the neat little features of an MDP, while still allowing us to use these variable time actions. It turns out that's pretty straightforward to do. I'll just write out the equations for you and hopefully, it'll be easy to see. Okay Michael. So here's the definition of the reward function (s, o). It's an expectation over the set of rewards that I'm going to see discounted at each step by gamma. So the first reward that I see, r1 step one, is going to be R. In Step two, it's going to be R 2 discounted by gamma. In step three, it's going to be R 3 discounted by gamma squared. And so on and so forth, until the option ends after K steps. And that final reward will be discounted by gamma to the K minus 1. So this is in some sense the average, in fact in a very particular sense this is the average discounted reward that I will see by executing this option in this particular state. Does that make sense? Yes I mean it's I guess it's weird because it's kind of immediate reward in the sense that it's the reward that comes from executing that one option. But it's also clearly not immediate reward it's a sum of rewards. Right, so one way of thinking about this is remember all an option is a policy that I'm going to execute. That policy will be realized in specific actions which means it will be realized in a particular states and rewards that I will see a bunch of source, source, source, source. So even though from the outside, I start here in this particular state. Some time passes I end up in this state. What's actually happening along the way is I'm visiting every single one of these states and picking up reward along the way. But we need to be able to discount that reward. And so the discount factor is hidden inside of the reward function. So this is as if rather than executing the option. I had actually just executed all the atomic actions at the option itself. Will in fact execute. And then you'll call that sort of one bolus of reward? Right because all that's really happening right is that Whenever you have this kind of summation over you taking you seeing a bunch of states and taking a bunch of actions, all you're doing is adding up all the rewards you're going to see. Remember we got the bellman equation by just writing out that we're going to visit a whole bunch of rewards. That the definition of the value of being in the state is the expected discount of reward. So we're keeping that consistent here in this in this abstracted temporally abstract actions setting by virtue of the fact that still the immediate rewards are discounted. According to the true time steps of the underlying process even though we, I guess as the agent get to think at a higher level we get to think about an action taking multiple steps. Right, exactly and so we do something very similar for F and we end up with this nice equation. Here's F Michael, so just like I wrote R is an expectation over the set of rewards that you're going to see after k steps. I'm going to write F in the same way. And I've sort of got some really funky notation here but let me try to explain it. In English, F is simply the [LAUGH] expectation of me in ending up in s prime after k steps. So this Delta is because I can't draw lowercase deltas well, this is a delta function. It is one in the case where the kth state that I see is actually equal to s prime, and zero otherwise. And Delta to the k is just, well the discount factor raised to the kth power. So, F is an expectation over after I've executed the option that I will end up in state s prime. Having started in state s and having taken k steps. Now notice, why do I even have to go through all this? Why couldn't I just put T of s,o, s prime here? Well first off, T is defined in terms of A is not defined in terms of o, but that's okay. The reason I have to do this little funky thing, and why we call it F instead of T, is because. As you pointed out in the beginning, there needs to be a discount factor attached to the value of the next state, which, in this case, is s prime. So what we've done is, we've rolled up that discount factor inside of F. So F is the expectation of me starting in s, taking option zero, ending up in s prime after k steps. And the Delta sub k gets sort of brought along with that expectation so that I can appropriately discount the s prime state that I will end up in. Hm, okay. Right, so in the end, both R and F are just expectations, they're exactly doing what they were doing in the original Bellman equation that we were using before. In the original Bellman equation the reward was just the immediate reward that we got for taking an action here. The reward is just an expectation over all the rewards that we're going to see, given that we've taken this particular option. If the option is in fact a single action, then it would be exactly the same thing as a reward we would get for being in that state and taking that action. Similarly, F is just the transition function, except now it's sort of the accumulation of all the transitions of starting in s and ending up in s prime. After k steps given that I took the particular actions that are inside of the option, and dividing by pi. Now there's a couple of neat facts about this and I will write them down on the next slide. But I just want to point out what's particularly cool about this, is that it really is a Bellman equation. It really does allow us to do all of the things that the Bellman equation, Bellman operator allowed us to do in MDPs. But it allows us to analyze or to reason about or to plan over this multiple variable time MDP. So, in fact, this kind of MDP has a name, which I probably should have written down earlier. And it's called SMDP, where the S stands for, what do you think, Michael? Something? Yes, and that something is Semi, so this is called a Semi-Markov Decision Process. And the difference between the Markov Decision process and the Semi-Markov Decision Process is, you get to make these jumps in time. And because we have this Bellman operator that takes into account all the possible jumps in time that you might take. In the end, it grounds out to an underlying MDP and allows us to inherit all the things that Bellman allowed us to in here. SMDP, by the way, theres an entire literature about semi-Markov decision processes well beyond the sort of scope of this lesson. But from our point of view what's really neat about this, and this comes up again and again when you use SMDPs, is that once we define an SMDP and the appropriate Bellman operator for it, we can basically just pretend that it's an MDP. Because at bottom it is an MDP and these expectations allow to ground out in that in MDP. And so we can just pretend we're always dealing with an MDP. And we only have to deal with the fact that it's a SMDP. It just works, I take an option is just an action. It just takes time and isn't atomic but whatever. I might as well treat them as if they were atomic. I just let it run and it comes back. And when it comes back it will have accumulated for me all of the reward that I would accumulate if I were living in the underlying MDP. And it will discount the future in exactly the right way after I've taken k steps. Whoo-hoo! [LAUGH] Yes, exactly. So this is a lot of sort of equation writing and deltas and delta raised to powers and expectations that allows us to say, you know what? We're still dealing with MDPs with options. Everything we knew before works. Okay, that's a relief, I guess, that we didn't have to throw all that out and start afresh. Yeah, and again I mean it's actually important. For me this seemed like a subtle point for a long time but it's actually not that subtle. It's just an SMDP is just an MDP and where we have to keep track of the fact that our actions are not atomic and accumulate reward and accumulate all the transitions. But really, that's all captured very nicely by this expectation. It just means we have to hide things like the discount factors inside of our functions. Whereas before with the MDP we could stick them outside of the transition functions. Okay, well then I'm not quite getting that name, I think. Semi-Markov decision process sounds like it's kind of Markov but also kind of not Markov. Yeah, that's right, it's semi-Markov. But this seems Markov, right? I mean, underneath it's just a whole bunch of standard MDP-type transitions. Yeah, that's exactly right. But what's allowing it, but at the level of the options right, it's not Markov because you have to know how much time has passed. I see, I see, so where, right, the state that we end up in could actually depend on how much time we've spent, how much time has elapsed. In fact it will, in general, right? Because if I do it in one step I'm going to end up in some state. I can't reach the states that are more than one step away. If I take ten steps then I can reach more states. So the state I end up in does depend upon k. It depends upon how long the option was executed. And in general, since beta is a probability over states, it could happen at any time. I see, so it's got elements of Markov and elements of not Markov. So elements of Markov include things like well, we can think about the probability distribution over where we're going to end up. It really is Markov. But we have to be much more careful when we actually sum up rewards because the time that has lapsed is variable. And so we have to actually take into consideration how many steps have passed to do that. Right, and in fact is that's a very important point. And I'll sort of point out two things here. One is when we have a process that is not Markovian, often we can turn into, in fact we can turn basically any non-Markovian process into a Markovian process by just keeping track of enough history, right? And history is, often time is enough. Sometimes you actually want to know what states you visited but often time is enough. And so we can turn a non-Markovian thing into a Markovian thing. And in some ways that's what we're doing here, right? We are keeping track of the places that we saw in between. But at the level of the SMDP, at the level of the options, I can ignore the MDP and pretend that the SMDP is an MDP and that these are really atomic actions. But in reality they aren't, which is why R and F actually depend upon k. It actually matters how many steps you took. And that affects the expectation. And in fact, if you look at R I am actually accumulating all the reward. So I'm taking something that's non-Markovian because it depends upon the actual rewards that I see, which depends upon the actual states that I visit. And I'm gathering them with me as I go along. So that when I pop out a level it goes back to looking Markovian again. Cool. Right, so an SMDP, it's an MDP and I don't have to think about it anymore. I feel like I'm getting the kind of math of this. But it's still, it's still a bit of an abstraction to me. Can we get into some kind of an example to to practice with these ideas? So yes, first I want to point out that I appreciate the pun, using the word abstraction here. Thanks. Very good, I like that. But I think the answer is both yes and no. I'm going to go through an example that I think might help you think a little bit about this. But I also think that actually working out all these expectations would take us a long time. And it's a really good and useful exercise, which is why it should probably be an exercise. We'll let the students do it. I mean, I don't know, it seems like a cool exercise in dynamic programming and algorithm design. But Michael, you think everything is a cool exercise in dynamic programming. Well that may be, but that's because everything is a cool exercise in dynamic programming. Yes, that's an excellent point, and why we're going to move on to the next slide. All right, Michael. So I'm going to deal with your request by trying to go through an exercise with you, where we try to figure out what these options are, and there's going to be a secret point here that has to do as always with abstraction. So here's what to do. So we've got the same little grid world that we had before up here but I've introduced another grid world. Do you recognize this grid world? It looks like a very tiny, little game of Pac Man. It is exactly a tiny, little game of Pac Man. So what I want to do with this tiny, little game of Pac Man, which again is just a grid world, is I want to explore this notion of options and see what we can figure out about them. So everybody remembers Pac Man. So Michael, for the benefit of the audience that has never played Pac Man, all zero of them, why don't you explain to us how Pac Man works. Sure, so in the game where a little Jewish guy named Pac Man and we're running around in the grid, we're the yellow guy and we chomp, chomp, chomp, chomp, chomp. We can't go through the blue walls. We're being pursued by [LAUGH] it looks like a sort of- Careful. Yeah. [LAUGH] I'm going to go with the french fry goblins from McDonald's. So as we move around in the grid, we consume those little dots, little pellets, and that gives us points. If we can actually clear all the little dots and the big dots from the screen, then we have successfully completed the screen and we get an even more exciting challenge next. Yes. But I want to say that there's one other important thing is that if we eat one of those power pellets, the big fat dots, then the tables are turned on our ghost friends. And we can actually, if we touch them, consume them. We get points. And then they have to, I believe their eyeballs run away and regenerate. Yes, that's exactly what happens, just like in real life. So, yeah. [LAUGH] So the ghosts touch us, we die, unless we've had the power pellets, and then they allow us to eat the ghost but only for limited amount of time, that doesn't last forever. Once I get a big fat pellet, I don't get to do it for the rest of the game it last for some amount of time which- For limited time only. Limited time only. And I don't remember if that time is fixed or that time is random, I think it's fixed but maybe not. And there are different versions of Pac Man that have slightly different rules, but those are the basic rules. So, here's the exercise I want to go through. Now obviously, and when I say obvious, I'm saying that to beat down anyone who doesn't think it's obvious, there are four atomic actions, left, right, up and down, north, south, east and west just like in any other grid world, and that's pretty much what we're given. So here's the exercise I want you to go through. It's got sort of two parts to it and this is actually an exercise that we have done with other people as a part of studies we've done. I want you to think about options that you might create here. If you were going to describe to a human being, here are the things you need to accomplish, here are the things you need to do in this game in order to play it and play it well presumably, what kind of action would you come up with? So, I'm asking you do the same thing that I asked you to do before. Where you said, well go to the doorway, and you know that kind of thing. So, I'm asking you do the same thing here but in Pac Man land. Sure, I can give that a try. I mean, I feel like in some ways this is is harder than the other example because it was nice to be able to just navigate to that plus one and know that our task was complete. Here, there's a lot of things going on. Yeah, but you have to simplify it for me and so abstract away. Simplify it for me. What are the actions you would tell me to take? What are the things you would tell me to do? There's like eating the dots. Yeah like that. So that seems like a reasonable thing to do eat dot. I guess the little dots versus the big dots they might be two different things because I want to eat a big dot if I want to turn the tables on the ghosts and pursue them. Right, okay, so, I'll just call the little dot dot. Okay. And then I feel like it's really important to have a strategy for escaping from the ghosts or pursuing the ghost depending on the mode. Okay, so, yeah so, you might want to eat a ghost and you might want to avoid a ghost. Anything else? I mean I guess we could come up with more detailed strategy ideas but those feel like the basic things. Yeah I think that pretty much covers it. And if you were, when we go back and we listen to the recording later Michael I think you'll find that this is almost exactly what you said when you explained how the game worked. You said you need to eat the little dots. And you also have to eat the big dots eventually because you need to clear. If the ghost touch you then you die, so you need to avoid the ghosts. But if you eat the big dots then you can actually eat the ghosts. And that's pretty much how you describe the game. I see, so, the way that I kind of thought about the game to convey it to people, is at this sort of high level that kind of makes sense with respect to options as well. Right, which is what I also think was true in the in the other grid world. Now, we could look at this and we say well these are options and maybe we can learn these options, or they're given to us and somebody could code them up for us, I don't know. I want to point out a couple of things about these options though and this is going to bring us back to your excellent analogy. About programming and about computing. Which is if we think of these things as functions or procedures to execute. Which I think is a perfectly reasonable thing to do. I want to point out that what we get from functions, procedures is not just the ability to reason at a high level. But we also get something called separation of concerns. Which allows us to only think about parts of the world. So for example if you want me to multiply two numbers. I only have to worry about those two numbers. I don't have to worry about the state of the stack or whatever else is going on in this program. I just need to worry about the two numbers you're asking me to multiply. So there's a similar kind of gain in abstraction that you get here by picking options carefully. So, to illustrate this I'm going to ask you to do one more thing. I want you to tell me, what are the states of this world? What are the features that make up the states in Pac-Man? Okay so there's, well it's definitely different if there's any dots left versus there's no dots left. Right, so- That's an important distinction. So the dots and where they are on the screen matter. So there's- You want, okay so not an abstraction of the state. You actually want literally the things that make up the state. Well which I claim as an abstraction of the state, almost by definition. So, I think what you said before works in either case, right? So there're dots, there're dots on the screen. I need to know where all the dots are, and so there's some set of features, the make of the state that tell me where dots are. Right, okay, so then there's where the Pac-Man is. The location of the little dots, location of the big dots, location of the Pac-Man, location of the ghosts. And I guess the walls aren't really part of the state. In the sense that they don't really change. Though I guess if we were talking about multiple levels of the game we might have to also say where the walls are. True, I'm going to claim that you don't have to do that because it's all going to be taken care of by the, you could and there's nothing wrong with it. But it's actually all going to be taken care of by the transition function. Right there's, you can't enter wall. So you can't from a state go into where a wall happens to be. Okay, and whether the ghosts are in, I don't know what to call it, but power pellet mode. Whether the ghosts are fleeing from us, or whether they're attacking us. Right, so we talk about scared ghosts. Scared, whether the ghosts are scared, I see. They always look a little scared with their bulging eyes. Right. Not when they're coming after you they don't. But when they start blinking purple and they're running away from you and screaming, aah! Then they seem a little scared to me. [LAUGH] Maybe that's just my version of Pac-Man. Anyway. So here's a bunch of features. And the details here matter in implementation, but I think for what we're talking about don't, right? And we talk about where the dots are. It doesn't have to be where the dots are, it could be where the closest dot is to you. Or where the farthest dot is to you, or it could be every single dot and whether it's on or off. Same true for the big dots. Pac-Man, you have to know where Pac-Man is. You need to know where ghosts are maybe just need to care about where the closes ghost is whatever. It does matter whether they're scared or not. In the same way that we don't have to keep track of the walls or other things here we might not have to keep track of. But, whatever it is we're going to do in the detail I think to solve the game, I think your reasoning is right. You need to know where the dots are of various flavors we need to know where the ghost and where we are and we need to know whether we can eat the ghost or not. And these features however they're exactly expressed, are important for solving and planning in the game. Now, here's the observation I'm going to make, just like when I ask you to multiply two numbers I don't need to know everything else that's going on in your program. I suspect that is the case that, even though I need all of these features to effectively and efficiently solve a Pac-Man board. My options do not necessarily need to know all of these things. So, for example if you told me to avoid the ghost ask yourself whether it matters whether the ghosts are scared or not. [LAUGH] Interesting, okay. Yeah, I guess if I'm if I am avoiding the ghosts. Well, yeah, it changes their behavior, so it may depend on that, sure. Right, so I think we should delve into this a little more deeply. And the way I want to do that is with a quiz. Okay Michael, so here's the quiz. I have here a grid because we're doing grid worlds. And on each row I have the four little options you came up with, eat the big dot, eat the dot, eat the ghost, avoid the ghost. And for the columns, I have the features that we came up with. I didn't put the Pac-Man feature up here because I didn't feel like it. I ran out of room and I think you always need to know where you are. So I'm just going to assume that that's important and I want you to tell me for each of these pairings, for example the dot in the ghost pairing, whether I need that feature, whether it'll be useful in order to execute a particular option. Does that makes sense? In principle. Sure. Okay, so for this very first one, if I want to eat the big dot, do I need to know where the little dots are and so on and so forth. Okay. All right, cool. Got it? I think so, I mean so am I doing things at the same time in a sense? Like when I'm eating the big dot, am I also responsible for avoiding the ghosts en route to that? I want you to write code that will go and eat a big dot. But it will fail if a ghost gets in the way, so I maybe do need to avoid the ghost as part of that. Maybe, I will point out that you do have this little beta thing which allows an option to be interrupted. Okay, all right so I'm not sure I perfectly get this but I can I can take a stab at it. I love it when you take a stab at things. Let's do that, you ready? Sure. Go. Okay Mikey, are you ready? Sure. Give me an answer. All the answers. So [LAUGH], if I'm trying to eat a big dot, and that's really literally the only thing I care about. Then where the big dots are on the screen matters to me a lot. Yeah. And I'm going to say the other stuff doesn't. Right. I think that's right. Now you made a point before we went through, that, well, it matters where the ghosts are if I want to eat the big dot. But you can actually say you need to eat the big dot while avoiding eating ghosts, who by the way, unless the ghosts are scared, you can pick them up along the way. You could do that, and maybe that's okay, but you don't really have to in order to execute the eat the big dot option, and in fact, we'll see when we go through all of these that there's some sort of neat features that fall out of it, so let's just kind of take that idea that the only thing I strictly need to know besides where I am in order to be dot is where the big dots are. Don't care where the ghosts, so I don't care where the ghosts are, I don't care whether they're scared or not, and if I happen to have eaten little dots along the way that's wonderful, but if all I want to do is accomplish eating the big dot, then I don't really need to know where they are, okay. So what about eating dots? It still feels debatable to me because I feel like sometimes I'm driving and my spouse is navigating and she tells me, get off at this exit but she means get off this exit while not hitting any other cars. So maybe two of these options can be active simultaneously or do we have conjunctive options that somehow handle multiple things at the same time. Let's keep that question in our heads for after we finish the quiz because it's the very next thing I want to talk about. Okay. All right. So then, I'll just use the strict interpretation and so for eating dots I need to know where I am, the Pac-Man and I need to know where the dots are but the other stuff doesn't matter to me. Right. To eat a ghost, I need to know where the ghosts are. Do I need to know whether or not they're scared? I mean what does it mean to eat a ghost if they're not scared? Like maybe the initiation set of that option would prevent us from even having to care about whether or not they're scared. And so maybe we'll just leave it at that and then avoiding the ghost so definitely depends on where the ghosts are. Yeah. And then I would argue that it also depends on whether they're scared because if you're trying to avoid scared ghosts, they behave differently than non scared ghosts. So knowing that they're scared makes it easier to predict where they're going to be. Well that would also be true for eating ghosts as well. But again I don't see how you can eat a ghost that is, you can avoid a ghost that you can eat. This is true. But you can't eat a ghost that can eat you. That's a good point. So I think that this gets to a neat little point here, which is sort of what do these things really mean? I asked you and you gave me kind of English descriptions. Eating a ghost may simply mean you can't eat a ghost if the ghost isn't scared. Right. Or it could also mean just collide with ghosts. In which case I agree that it's just like avoid ghosts. Right. It's like the opposite of avoid ghosts. Yeah and of course, by the way, even though eat ghosts made itself not care whether you're scared or not Even if it doesn't make sense to eat a non-scared ghost, one could argue that that's all hidden inside the beta function because I just have a probability of one of exiting if I ever enter state where the ghosts are not scared. Okay. All right, so, I don't know, plus minus on those other boxes, I guess I would check scared for avoid ghost but not for eat ghost. By my interpretation of those. Which I think is fine. I probably wouldn't, and I'll tell you why I wouldn't. I wouldn't because avoiding ghosts sort of in the kind of Markov decision sense of the world. Here's the world right now, basically involves you moving away from the ghosts. The fact that the ghosts are also moving away from you, you know is it strictly necessary for accomplishing the goal of avoiding the ghost. So, you don't really need to know where they're scared but a lot of it depends upon the detail so I would accept, I think scared in either of these cases and I feel pretty good about that. If we pop up a level you can make a similar argument here which you sort of made when you asked me that question, all right. You asked me the question before we start the quiz which is, well if I want to eat a big dot, I need to avoid the ghosts along the way. So maybe eating a big dot eating a dot is really requires that I know almost everything. Except, in the worst case, eating a big dot doesn't require that over the little dots are, even if I need to worry about where the ghosts are and whether they're scared or not. Although, if I know where the dots are then I might choose to eat the big dots in such a way that eat a bunch of little dots as well. But this is really a kind of, this points out two things. One, it sort of points out that if my options are not written in exactly the right way, then I may not be able to find an optimum policy, we'll talk about that in just a moment. But, if I worry about that, then all of these options have to solve every single problem. And I kind of get nothing out of it. I don't get a big win by even thinking about it in terms of options because effectively, I'm just trying to solve the entire problem. The whole point of abstraction, it's a kind of the Software Engineering and Computing sense and here we are using it, is that in part it allows you to do an abstraction not just over time, it allows you to do state abstraction. And other kinds of abstraction. So, what do I mean by state abstraction here? Well, if you look at the big dot case, if I don't have to worry about where the dots are, never mind where the ghosts and whether they're scared or not. That means that if I needed to learn this policy, if I needed to figure out how to most optimally solve the problem of eating dots. I can actually ignore large parts of the state space. What happens if I can ignore large parts of the state space, Michael? Then you can solve the problems more efficiently. Right in the very beginning we said the thing that made reinforcement learning hard is that as the number of states grew, the problem became more and more difficult, it was sort of super linear in the number of states. So if I can avoid whole features of the states, that is I can collapse states that are otherwise different as if they were single state where the dots are don't matter, then the problem of learning this policy, learning this option actually becomes easier. Both from a planning point of view and from a learning point of view. So it's if the state space gets smaller? Right. So temporal abstraction doesn't just buys temple abstraction. It buys a state abstraction. At least if we can divide the world up appropriate. Neat. Okay. I mean is there a guarantee that we can always do that? No. There's not a guarantee that we can always do that. In the kinds of problems that people can solve maybe it's pretty common because people don't have infinite computational resources either. Right. And in practice, I mean, if you just even think about the way you were describing things, you sort of ignore large parts of what's going on around you in order to solve the problem, right? We do these kinds of things all the time. It had better be the case that we can ignore large parts of the state space with otherwise, we're in trouble it's going to very difficult to move around in the world. I mean first off, the world upon DP so [LAUGH] we're ignoring large parts of this database anyway. But beyond that, I'm in this room talking to you there are things going on outside I can't see. By and large I can mostly ignore them at least for short periods of time. And we just have to hope in the real world that the parts of the state space that might matter such as I don't know a polar bear, the location of the closest polar bear. It will be announced to me when that becomes relevant. [LAUGH] And I can start paying attention to it, in which case I'm going to stop worrying about eating my big dots which is what I call teaching this class and instead start avoiding ghosts which is what I call running away from polar bears. So just to drive the point home a little bit I just want to write down, sort of summarize what it is that we've talked about so far and how it all kind of comes together. And I think it's actually going to push it back to the very first slide. So, we've done MDPs we talk about SMDP's. We talked a little about what options actually do for you, And we talked a little bit about state abstraction before. This all brings us to a bunch of neat little facts. The first one is that because we are doing this SNDP type thing, because we are doing options. We get to inherit all of the optimality guarantees up to a point of what we had before in regular MDPs with regular actions. So in particular we get all the convergence proofs, we get all the stability proofs. In other words, everything is all nice, and well, and good in SMDP land as it was in MDP land. There's a slight caveat to that which is worth mentioning, and it's really what you brought up over the quiz. Which is well if we don't pay attention all these things, and we live around in the options space are we actually going to be able to really be optimal? We really going to build to solve the problem? Do I need to know all of these things? And the answer is, well if you've got the right sort of options in the right side of world, right kind of world, then in fact, everything's good. But you cannot guarantee that if all you do is reason over options, that you will in fact end up with an optimal policy. We have another notion called hierarchical optimality. And that's the notion that given the set of options that you have, you can in fact converge to an optimal policy with respect to those options. And I think that makes sense. And of course whether you are allow those options to pay attention to all of the state space, also affects whether you're going to get the true optimality that you would have. Or only a sort of optimality with respect to the particular set of options, and the states, the state features that you happen to be paying attention But I actually don't think this is a big deal right because I think this is true for the problem in general. So I've got an MDP and I have a bunch of states. I made up the states anyway as we've talked about before we talk about the palm and MDPs and PSRs. It's not clear these dates actually exist in some very real sense. I came up with the state features I described the way the world was supposed to be. There're clearly some ways of describing the state of Pac-Man that make learning easier and some that make it harder, some that allow us to be truly optimal, and some that don't. Like if I had came up with state features that only track the closest dots to me I may not be optimal. But on the other hand it might be easier to learn and these are tradeoffs that we make all the time. So, in some sense we're always dealing with a limited notion of optimality. Which we define with respect to the set of actions we decide that we're going to consider in the set of state features that we're going to pay any attention to at any given time. So in particular, once we've supplied a set of actions in a state representation, we would like agents to do as well as they can, without necessarily questioning what we gave them. So in this case of temporal abstractions, again, hierarchical optimality is the notion that the agent should do the best that it can, respecting the high level, their temporal extract actions that they're given without having to question whether it could do better by not listening to that whole concept. Right and so maybe, it's the case that we should include in our options, every single primitive action, every single atomic action. Maybe we don't have to do that. It really just sort of depends upon what you're trying to accomplish and what the trade-offs are. But we're making those trade-offs all the time, even when we define the problem in the first place. I guess that's fair. Right, but in the same way that once we have all the machinery of SMDPs in place, we might as well just pretend that we're dealing with MDPs for this kind of purposes of planning and learning. We sort of should do that with the set of options that we have in the set of state features that we have. So that's a big win that we're going to still get optimality. It'll be limited by the set of options that we've constructed, but it was limited by the set of actions that we had in the first place. There's other things that sort of fall out from that. One, is state abstraction, which we talked about before, that because we have these different options, if we are lucky or clever we might be able to define options that don't require that we pay attention to the entire state and pay attention to everything that's going on. And if so, that can be a big win. But there's also something else and I said this on the first slide, because we're able to jump ahead from one doorway to another doorway, it allows us to avoid wandering around in say, this northeast room, right? Which means, we get to avoid what I'll call the boring parts of the mark off decision process. The parts of the state space that don't matter. Because remember, fundamentally, in order to do reinforcement learning, we have to worry about exploration versus exploitation. But the options, because they have these policies built in, and because they're trying to accomplish very specific things, allow us to avoid having to pay attention to large parts of the state space. So, they actually help us with exploration. So, with the idea that we can focus our decision making energy on the places where those decisions actually have an impact, as opposed to stuff that really there's only one right answer, we shouldn't be thinking about it. Right, so in fact, the word that you that you mentioned as a phrase used there that I think is just perfect, which is, we need to focus on the places where we can actually make decisions. And because options allow us to kind of jump forward in time, it means, in some very important sense from planning at that level, there are no decisions to happen here. We made the decision when we decided to execute the option, go to the southern door. How it gets there, is it's business. We're just assuming that it works. In the same way, that when we decided to execute the up action or the left action, we didn't worry about what was happening while that action was being executed. We didn't think about it at all. Those are the decision points. That's what makes the decision process and that's where we should be building our computational energy. So, insofar as we can jump ahead in the future, we can avoid making decisions, which means we avoid doing exploration and that is a huge win. Let's delve in a little bit to the some of the things that we've said in the last couple of slides. So I brought back the options that we had before and the little pictures and I erased a bunch of stuff and then I made a change. Do you see the change? You got rid of the word, yes, I was going to say you got rid of the word options but you actually changed the word state into goal. Well that wasn't what it used to say in the title, it didn't say state abstraction, although I did write state abstraction at some point. It had temporal extraction. That's right. So what I want to point out, you asked the question when before we did the quiz, you said, well, wait, are these things all happening at the same time or are they happening like, how does this work? Well, you touched on something kind of neat there, right, which is We have talked about so far this kind of options things if well, as if they're just primitive actions, right? You take one and you take the other and take the other. In other words we've talked about sequencing. And that's perfectly reasonable and that's really what we've been talking about and everything that we've talked about in this class. But when you ask whether the things could be running to the same time or whether we're trying to accomplish all these at once, we're really talking about something different, which is goals. So I'm going to claim that although we've been focusing on sequencing, which makes sense, right? We're talking about sequential decision processes, right? That's the whole point of what we're doing. You could think about these things as running in parallel. Although we talk about, well first we're going to take the option of go to the east door, and then I'm going to execute the option go to the south door, and then I'm going to go down, down left or whatever I'm going to do. You could conceptualize this as actually all the options are running at the same time. So, I am always trying to go to the east door, the south door, the other south door, the other east door, and any given time, one of them is in control. I mean conceptually that's no different, really, right? So, maybe, I mean I guess it sounds like what you're describing is a little bit like a, like a call stack in computer languages. The idea that sure while we're going after that first goal of getting to the east door in the north east room. You know in the back of our mind just the idea that we're; the reason we're doing that is so that we can do these other goals next. No I actually mean something a little bit different. So, imagine I had a bunch of things running in parallel, imagine we had a thing called a computer right, and the computer did many things at once. So it was recording, it was printing, it was letting some program compile in the background while the user was editing. >From the computer's point of view, right, the way we think about these things is these things are all sort of running at the same time. But at least from with respect to any individual processor, only one thing can be happening at the same time. So we can write down everything as if it's happening in sequence, but in some sense it's happening in parallel or they're all fighting for resources at the same time. But only one of them can be in control at any given point in time. So, even though the world is sequential, we think about executing an option then executing another option. A kind of, it's a reason way of thinking about the world but there's another kind of conceit one could take which is all of these things want to run they all want to happen at the same time. They're constantly saying take me, take me, take me. I want to have control of the mouse, or I want to control the screen or whatever, and we're just choosing which one to let go. So they're still running at the same time they're just not in control at the same time in aren't able to do anything. Once you do that, you end up thinking, or you can end up thinking about of this problem of temporal abstraction is not one of sequencing things directly but as one of managing competing goals. So maybe this becomes clear with an example so we go back to the Pac-Man example, at all times you want to eat ghosts, eat dots, eat big dots, and avoid ghosts. It's sort of all happening at the same time. It's just at any given point in time one of them happens to be ascended. Okay, yeah that example makes more sense to me than in the navigation example. Sure, but I think conceptually they're the same it's just it's easier to think about it. In fact we actually have a name for these kind of things, we call them predator-prey scenarios. Who's we? You and me. So here- No, no, no I don't call them that. Yes you do this is a pretty, yeah well okay the literature is replete with examples of things called predator-prey. Where you basically want to go around eating as much as you can, but there's some predator who's trying to you. So you're always trying to eat, because you have to eat or otherwise you die of starvation. But you have to avoid a predator otherwise you die while the other one is not dying of starvation. And Pac-Man is just an example of predator-prey. Right? Pac-Man needs to eat all of these things. Meanwhile there are predators out there trying to eat it and so they're kind of competing goals happening at the same time. I need to eat, I need to avoid being eaten. [BLANK_AUDIO] Right? And all of these are really examples of that, they even use the word eat and avoid them. [LAUGH] So I need to eat my food and at the same time I need to avoid being eaten. And occasionally because I live in a magical world where suddenly rabbits become as big as wolves, I can occasionally the wolf. I see. Right, but once you think about it that way you can stop thinking about, I mean, so I'm not trying to make a gigantic point here. Other than we can think about actions as actions we're executing or we can think about these options as things that are accomplishing goals. And this is where goal abstraction comes from in my title here. That really all of these options are about accomplishing different goals, you're trying to always accomplish those goals. Some goals are more important others at any given point in time. And options in fact really make this sort of nice to think about because of beta. Because beta captures sort of the probability that we might terminate in a state. >From this sort of goal obstruction point of view you could think about this as either the probability that I've succeeded in executing that option, that is I have accomplished my goal. Or that another goal has become important and I need to interrupt whatever goal I was paying attention to at the time. That makes sense. Right. So I like this notion of thinking about options, or thinking about actions as always accomplishing goals and always sort of happening in parallel. Options are easier to think about than primitive actions that way because in fact when we define the options I think we sort of naturally think of them as accomplishing goals. That's sort of the whole point. And primitive actions are sort of so low level they don't really have, other than primitive goals like moving left or moving right, they don't really have big high level things that they're trying to accomplish. But options almost always do, that's why they're useful and that's why people tend to come up with the ones they tend to come up with, so it seems to me. And so we think about managing goals then we actually end up with a different way of thinking about this problem. And we move from a world where we're worrying about sequencing and hierarchical abstractions to a world where we think about managing and arbitrating between goals. Yeah, so let me write down different ways you might do that because it turns out there's a whole subfield that worries about thinking about the problem this way. So as I know from the last slide, right? Once we start thinking about goal abstraction and sort of all of these functions or modules or procedures running at the same time, what you really have done is turn this into a problem of arbitration. By the way this does have a name. I really like the name, it's called modular reinforcement learning. And that just sort of captures the idea that we divided the world into a bunch of modules or what we might call options or what we might call temporally extended actions. And these things always have associated with them goals. They're always trying to do something or to accomplish something. They're different models and with operating system, and we need to decide which program we're going to let run next. And there are lots of ways you might imagine doing this. Here are three of the more popular ones each of which has strings and drawbacks and they're kind of the things that you would come up with if you were given five minutes on a test to come up with something. So here are the various arbitration techniques. So the simplest one is greatest mass Q learning which well, Michael what do you think that means? It means Greatest Mass Q learning. So we have Q functions maybe for each of the individual subtasks and we say well whoever's got the highest Q value gets to take the next action. That's close. So the first thing you said is right. Since each of these things are their own little goals running around they're like their own little RL bots or agents, then each of them has a Q function. And what greatest mass Q function does is it adds up all of the Q functions. So for the state that we happen to be in, add up the Q value for every state action pair. And whichever one is the largest that's the one that we execute. So I was saying whichever Q function has the largest value you're actually saying which action after combining the Q functions has the largest rate. Right. So it's sort of like they're voting. That's exactly right it's exactly like they're voting so I say that the Q value of taking a particular action in any state is actually the sum of all the Q values for each of the agents indexed here by i. And then we take, we go greedy. I picked the action that has the most mass across all the agents. Okay, so then why is it Q then? Why isn't it just greatest action mass or something?. Well the reason you have it as Q-learning is because you assume there was Q-learning going on and you're using the Q-function to determine how to vote. So that's exactly why it's called Q-learning because there's a Q-function involved. So this is adding everything up and it does sort of exactly what you would expect. Basically everyone votes and whichever wins has the biggest vote wins. So I want to say that the the modules are voting but they're voting on different actions. And whichever action has the largest number of votes wins. Right, I'm sorry, that's what I meant to say. What did I say? Okay. Well it sounded like, it was interpretable as the whichever Q-module gets the most votes gets to decide. I'm sorry. Yeah, so that's actually closer to what you were describing. Yeah I know. But yeah so here all you're doing is you're voting for each action. That's the greatest mass Q-learning and you just go from there. And that's neat and kind of makes sense if you assume sort of they're all care about some ultimate goal together. But people have observed that that could be bad because it might not be good for any particular agent, right? So imagine that I have a couple of, I have ten agents, they're ten actions, and each one strongly believes in a subset of those actions. But they're different ones. So each one being the different action. And so, or better yet, there are five, there are ten actions and there's five agents. And there's an action that is the fifth choice of every one of those agents. But because the fifth choice of every one of those agents it's the one that ends up getting the most mass and so you haven't really satisfied any of the subgoals. Yeah I mean I could see that as being a bad thing or I could see that as being good thing right because it is choosing that action not just because it's bad it's just that actually because it makes everybody a little happy. Instead of making some people really unhappy. Right but you can construct examples where sort of everyone's least favorite choice will still end up being the one with the most votes if nobody agrees on anything else. But that's okay I mean it's still as you say it's a reasonable thing to do under lots of circumstances or just happen to be cases where doesn't always work which would there's no free lunch so it's not surprising that that's the case. The other alternative, or another big alternative, is actually top Q-learning which is what you described. You basically take whichever thing had the most associated with it. The most value. Okay, yeah. Good. The most value. And that works pretty well, again those are the kind of things that you would probably come up with, if you just spend a little bit of time. Just say well what are the obvious things that you could do, both of those make sense. It of course also has a problem and that problem is that the agent, well, there are several problems but one that isn't necessarily obvious, is the agent that happens to have given the most Q-value to a particular action may also give very nearly the same Q-value to all the other actions as well. So it doesn't actually have a strong preference, it just happens to put high value on everything. And so you're not really doing much for it either. Meanwhile, make every other agent might, for example, like the second action. For that particular agent. And so it makes a lot more sense to take the second action but you can't because by a quirk of fate the other action got slightly more value to it for just that one agent.pericardial sac So it starts to feel a little bit like a negotiation. Right, which is why there's something called negotiated W-learning. [LAUGH] W-learning. So, this is a lot more complicated and I can't write it down in just a fancy little equation like that. But what negotiated W-learning basically boils down to is, the agent with the most to loose gets to figure out what the action is. So you sort of minimize loss rather than maximize an average or pick the best thing. So you look at the the subagent that, basically what you do instead of looking at the things that everyone thinks might be good an average of them are doing a max over them, if you look at the things where the difference between the maximum choice and the minimum choice are really large and let that agent make a decision. And just sort of weighted across all of the actions. It can be arbitrarily complicated but basically you don't want any agent to lose as opposed to having agents win. And all of these have their strings and all these have their weaknesses And they're all very kind of cool and under different circumstances. Greatest mass makes sense, in other circumstances top makes sense, and negotiated is good for other situations. And you can come up with lots of other things, the key thing is that you need to do arbitration. The problem with all of these though, all of these methods and any other methods that you come up with is that they're terrible, and in fact, They're impossible. What does that mean, you say. What does that mean? I'm going to summarize it simply this. The very first example you gave, you said, or the very first response that you gave to me when I described greatest mass Q-learning is you said, what you're really doing is you're voting. And voting is definitely not impossible. But as you know, there is an impossibility result for voting and that is arrows impossibility result. And if you don't know what arrows impossibility result is for voting, then you need to stop what you're doing right now and go look it up. What it basically says is that it is not possible to construct a fair voting system. The way that you can think about this in very simple mathematical terms. That I know that I think you'll like, Michael, is that all of these things assume that everyone is using exactly the same units. Right, but if one of the agents is using the metric system and someone else is using the imperial system it turns out you can't even compare the two. There's absolutely no reason to believe, that the values that each of these individual agents are developing, follow the same linear notion of ordering of Q values. And if that's not the case then all of these things are basically going to break. Because all of them boil down to doing what looks like some kind of addition or averaging.. Or maxing. Or maxing but maxing is just kind of a weighted average. Kind of an exponential averaging. And so, you end up with a max that way. Same thing with negotiated W learning you're subtracting things and taking mins. In the end you're basically assuming that I can compare your seven with my seven and it means the same thing. Why would your seven of my seven mean the same thing? So all of these have that kind of problem. And in fact there's a paper that written by a student of mine, that is a part of the readings, that delve into this in more detail. And we could spend a lot of time on it, but actually the idea is very simple. Which is really that once you start thinking about goals, and you think about these individual modules or individual options as trying to accomplish different things. You have to deal with the fact that their goals may not be completely compatible with one another. And you have to come up with some way of making them compatible. And you can either assume that compatibility by just pretending your seven and my seven are the same. Or you can say well there's some larger goal you're trying to accomplish that the arbitrator knows about. The arbitrator is going to make decisions based upon whether it accomplishes that big goal. In the predator prey case, for example, the big goal might be living long enough to reproduce. And so I'm going to pick the subgoals that allow me to live long enough to reproduce. I see, I think. So there's a high level goal, and then these subgoals that are chosen to try to bring about the high level goal. Right, and so it's the arbitrator that has to decide what's going on. So this is, this seems like a kind of straightforward thing. But I actually think it's kind of cool, right. That basically we went from actions to abstract actions these options. Those abstract actions actually turn out to be. Well at least one could look at it this way. Ways of accomplishing goals. Once you have these ways of accomplishing goals then suddenly you realize well these goals don't necessarily have to be compatible with one another. And so I have to deal with that compatibility. And so you move from an MDP with primitive actions to this larger notion of balancing tradeoffs. As you're trying to accomplish some large goal for the entire system. And in the end, they're still all MDPs because they're SNDPs and SNDPS are MDPs and in the end, all of that reinforcement learning stuff that we've been talking about throughout the entire term still applies And you've kind of brought together. Generalization with multi agent. You know type of thinking. Yeah, so we got multi-agents, we got single agents, we got multiple goals, we got conflicting goals. We've got abstraction and this gives us scale and everything's wonderful. Hallelujah. Okay Michael, so if you go all the way back to the very first slide, when we started this conversation. The slide said something about generalizing generalizing. But one of the words that was written on there was scalability. So one of the things that we've been talking about, sometimes explicitly, sometimes implicitly, is this notion of getting scale to actually happen. So just because I think it's important to think about scale more than in terms of abstraction. I just want to take a moment to talk about a algorithmic approach to getting scaling to work. That's different from just doing abstraction of the sort that we've done before. Although it's going to turn out that all the things that we've done with our abstraction will actually work in this new algorithmic view of scalability. That seem interesting? Yeah that would be really helpful. Cool let's do that, all right, so here's a particular algorithm that I want to introduce. It's called Monte Carlo Tree Search, and there are four words there, and there's two different parts. So for the beginning I just want to concentrate on the tree search part, and then tell you what I mean by the Monte Carlo part. So, when I use this picture on the right, the little tree, all I'm trying to get you to think about with this tree, is what we've done in the past. Say in an AI course, where you've got nodes representing states. There are actions that you might take that would get you to other states. And then more actions to get other states and so on and so forth. But this particular tree has a nice little form that's kind of hidden by the edges and the nodes, and I want to harp on that for a little bit. And I think it would help if you understand the algorithm that I'm trying to get through on this data structure. So the algorithm is written on the left, and it's a big loop loop loop loop loop loop loop. And it works like this, there are four steps, the first step is selection. And I'll define what all these things mean in but a moment, but I just want to go through at a high level what each one is. So the first one is selection. And selection is basically the way in which you're going to decide what actions to take from a particular state. What's going to happen is you're going to take a bunch of actions and eventually going to get to some point in this tree here. Of possible states and actions you might see, where you don't know enough to know how to make a selection. Once you're at a place where you don't exactly know how you should make a selection, the idea is that you're going to expand the tree there. And then do simulation to figure out what it is you ought to be doing to make selections. And the way that's going to happen, is you're going to estimate from that expanded set of nodes the true value of taking actions in those states. And then in a very kind of, in a very Bellman equation way, you're going to back up what you learned through your simulation. And then that will allow you to select what to do next. And you'll just keep doing that over and over and over again making selections where you know what to do. Expanding, simulating the world for a little while, so that you learn what to do, and then make those decisions, and so on and so forth. So does that makes sense a very high level? See what I'm trying to accomplish here? Yeah I think so. Okay. It reminds me of a lot of other kinds of tree search that I've seen in AI classes. Like what? A star is a kind of tree search, a game tree search is a kind of tree search. They have similar pictures where you repeatedly expand nodes and get estimates of values. Right, and in fact I like the game trees or the games search one. This is not game search because we're living in an MVP, and it's just us, and so we're not playing against an opponent. But something very nice about that particular one, is the way it works in that world often is. You have a particular way of figuring out what action to take or you sort of expand and do a search among a bunch of possibilities. And then eventually you run out of time, and so you have to decide what the value is or whatever leaf nodes you've gotten to. And the way you do that is use something like an evaluation function, which tells you how good we think this node is. And that's just what you what you've got to work on. And you use that and you back up the values, that helps you to make a decision about what to do. And then you make a decision, then your opponent makes a decision, you end up in a state, and you do it all over again. Basically you search out as far as you have time to search out for. When you run out of time, you use some estimate that you got from somewhere of how good the node is, and then you back that up. And that's basically what we're going to be doing here, except we have another wrinkle. And the wrinkle here is where the Monte Carlo part comes in. So the tree search is very standard. The Monte Carlo part is, well, we've got randomness, we've got stochasticity in our transition model. And so we're going to have to come up with some way to do this estimation that takes that into account. And that's where the simulation is going to come from. All right, so let's break that up in the little parts. Actually that's a lot of words, but I think walking through this picture might help a little bit. So let's do the first stage. We're going to select. Well, how do we do selection in an MDP? Well we follow a policy, right? So there's going to be some policy that we have. Let's not worry about where it comes from right now. But some policy based on experience. That tells us what we ought to be doing in some state. So let's say for the sake of argument. I'm in this particular state, and my policy tells me that I should take a particular action. And I take that action. Let's say it's this one. And, it ends me up in this state. Well, once we end up in this particular state, let's say the policy still tells us what to do, and it says take an action. And I take that action. I end up in yet another state. And the policy says well from here, you should take a particular action. So I take that action, and then I end up in this state. Okay. So, I have a policy. The policy tells me how to select what to do. What actions I should take. And it gets me through the tree, until I get to a place where I don't know what to do. So at this point in the tree. And in fact, just so you know. The way I've drawn this tree, all the leaves of the tree are places where I don't know where to go next. I don't have a policy for it. I've gotta figure out what to do. So here's what I'm going to do. I'm at this state, I don't know what to do next, I have to do something, so I'm going to do expansion, and then I'm going to do simulation. So, here's how the expansion state expansion step works. >From this state. I can take a bunch of actions right. And from those actions I can get to a bunch of states. In fact, I can just look at my transition model. And I can see all the possible next states I might end up at. Based upon the actions that I take. And often that's what you would do. That's what you would do in a normal kind of game tree search. The problem here of course, is that we might have many, many, many, many, many, many, many, many, many, many, many, many, many, many, many, many states. And so, we don't want to expand out the tree that much. So instead what we're going to do is we're going to kind of do a sort of sampling step. We're going to say, well, for each of the actions that I might take. Why don't I take that action and then simulate, for one step what state I might end up in. And then do that for another action. And then do that for another action. And so on and so forth. Until I have a few possible next state action pairs that I might see. And maybe that number's really small. Maybe it's six, maybe it's 100. It sort of depends upon your state space. So I've done the expansion step. I've figured out where I might end up next given all the actions that I might take. So I'm not drawing this because there's not a lot of room. But, each one of these edges represents some particular action I could have taken, or I did take in my imagination. And the state that I ended up in, or what the nodes represent. And so each one of these edges has some action that's associated with it. I'm just not writing it down because of space. Okay? Mm-hm. All right, so now I've got this. So I now expanded sort of the fringe. Here's all the things that I might do, and where I might end up next. And now I have to use that to kind of decide what I actually ought to do. Now if we were doing the normal kind of tree search, like we talked about with game search. I would just use my evaluation function, but I don't have an evaluation function. So instead, I'm going to do simulation. And what that means is I'm going to follow some other policy. We typically have a name for that. We call it the roll out policy. And just for the sake of discussion here. Let's just say it's a random policy. So I'm going to say I took this particular action. I ended up in this particular state. And then I'm just going to behave randomly for awhile. And then I'm going to do the same thing here. [LAUGH] Yeah, that looks random. And I'm going to do that here. And then I'm going to do that here. And then I'm going to do that here. And I'm going to do that here. And I'm going to do that here. And I'm going to get a whole bunch of spaghetti. Now, the spaghetti actually has a nice little bits. I know spaghetti is delicious. The spaghetti actually has all kinds of nice information associated with it. As I move through this path, and through this trajectory by behaving randomly. Say from this particular state. I actually see a bunch of rewards along the way. And I can just take that out as far as I need to. Given my horizon say, my discount factor. So I do this for a long time. And I collect rewards along the way. That gives me an estimate of being in this state, and taking this particular action. And since I might take the same action multiple times, and get to these states. I now can do a sort of average over all of these possibilities. Get lots and lots of, sort of estimates of this. And then that gives me an estimate of a Q value for each of the actions that I might take. So all I'm really doing is building an evaluation function by doing well, Monte Carlo simulation from there. So I'm in this state. I took a bunch of actions. That gives me a concrete set of next states. >From there, I just behave randomly for a while. I use that to gather a bunch of estimates of rewards. I add them all up. I average them appropriately. And now that gives me an estimate of the Q function. But by the way, it does more than that. Now that I have an estimate of the Q function for this state. Backed up from here, I can actually back up information all the way to the top. And that updates the estimate of this particular node. This particular node, and this particular node. So now that I have that, I backed everything up and I can just do selection again. Now this time around because I've changed sort of my estimates of the Q functions along the way. I might want to make a different selection. And so that actually tells us where this original policy comes from. This original policy is just an estimate of two values that I have where I happen to feel very comfortable about those Q values. I've visited these states many times, I've backed up a lot of information from them so I have a decent estimate and I'm willing to do the kind of greedy policy that is implied by these Q functions. I see, so we're taking greedy steps down the black part of the tree when we hit a leaf them we do the roll out policy pi r to get estimates at those different values. Do we now know anything new about the tree? Yes, and in fact, if we did this 100 bazillion times, or however many number of times we sort of wanted to, we just can say well, I know the actions that I can take from this node. And so I feel comfortable in expanding my policy all the way down to here for this particular node as well. And when if I did this again, and let's say the selection algorithm had me go here, here, here and I got back to here again. I would actually know which action I wanted to take, say it ended up with me being here. But then once again I've gotten to a place where I don't feel comfortable about what to do next, and I would do the expansion and the rollout again, over and over and over again. So I just keep doing this again and again and again and again and again and again and again and again. And what this gives me is an ever expanding tree where I feel more and more comfortable about my cue values and, therefore more and more comfortable about a policy that I should take. And it seems like it's getting more accurate over time. Yeah, and in fact if you're at each of these nodes, right? and you took those actions and you did the expansion and you did that well let's say an infinite number of times, and then you do an infinite number of roll outs over and over again. Eventually you would converge to a property two value. Cool that's right so this is a planning algorithm we need to know the transition model to be able to do those roll outs. Right, or you have to have some way of doing the simulation. So when you say you need to know the transition model, you could mean a couple of things. You could mean I actually have the transition model and I can do calculations with it. Or you could just mean I have some way of simulating the world and I can just sample from it. So this is pretty neat, right? I think it has some neat little features to it. It allows me to kind of build up a model the world. Do a bunch of simulation and figure out what to do. But there are a couple of questions that should pop out to you even though I kind of ran over them fairly quickly. I think the first one is, well, when do you stop? What does this actually, I mean, this is kind of an inner loop of a process, right? I'm at this state I have an estimate of Q values that I'm comfortable enough in that I'm going to take actual actions following the policy that's implied by them. And then I'm going to keep expanding and learning more and more about it. Well in principle I could just do that for eternity. But at some point you have to stop. And when do you stop? Well you stop when you get bored. And what do you do when you stop? Well, once you stop so next question well what I do after I stop? Well, you execute. You do a sort of one step policy based upon what you've learned and then you throw it all away and you do it all over again. Wait, so you take that step in the world using policy the whole chain that you figured out? Well, you could but and in fact sometimes I mean, well you can do actually any number of things there. But sort of the simplest version of this is I started out with an empty tree. I'm in some particular state, I'm going to hallucinate all the things that I might do from there. As I do that I'll be learning more and more about the world. This will allow me to do better and better exploration because I do this kind of Monte Carlo search and simulation and hallucination. And then eventually I learn enough that I know what the right thing to do is from this state and then I do it. And then I end up wherever it is I end up in the real world and when I'm asked to do something again I just do it all over again. Wow, so it does a lot of thinking each time it takes a step. Yeah but it's very fast thinking because you know I'm just flipping coins. If I have a fast simulator and I'm able to do addition pretty quickly and maybe some averaging I can actually do this estimate fairly well. What's going to impact this a lot is sort of how far down I can go before I run out of computational power? Typically that's what board really means board means okay I've taken 15 seconds that's too much time or maybe I've taken three seconds. That's too much time it's time to do the best thing that I can based upon what I know. But if I can do that every single time, if I can expand this tree pretty deeply at every time step, then I lose 50 milliseconds. But 50 milliseconds is a long time for a computer and not any time at all for human beings. And the alternative would be to actually explore the entire space and try to solve the underlying MDP. And if you have a whole lot of states and a whole lot of actions then that can take a very, very long time. So do you take 50 or 200 milliseconds every time you take an action and do it online? Or do you take three and a half months of super computing power to figure out the optimal policy and then use that? And the answer is, it depends. All right one other question. I'm actually kind of surprised you didn't ask me, is what's this rollout nonsense? I recognize that from reading about T.D. Gammon. How is it the same? Because it's a word actually comes from an understanding of how people figure out which backgammon boards are better or worse. They actually, the rolling here is the rolling of the backgammon dice. That's not at all what I was thinking. What are you thinking? Well, I was just thinking that we have some roll out policy which tells us what to do when we're here. And I said well let's just act randomly, but I don't know why that's the right thing to do. I mean, if I end up here and I've got to do something we're trying to get an estimate of this state, and taking this particular action and ending up in this state. Why is that good or bad? And the way you're going to do it is, I'm going to say, well I know how good it is to be here by behaving randomly from that point on. I mean, that'll give you probably an underestimate of the true value, but it would be nice if you could do something that perhaps made a little more sense. So people who have done work on MCTS have often put some effort into, well lots of things. Being clever about this expansion, keeping the right set of statistics, kind of shoving their expiration in very smart ways. But other people have spent a little bit of effort trying to figure out can I come up with a better policy besides behaving randomly? That will help me to have a better estimate of this particular q value. And we can talk about I'm going to just quickly talk about one by mentioning something that we have already talked about so far in this lesson. Do you remember the command you came up with Pac-Man? It was like eat, eat, eat, avoid. Right. So like eat little dots, eat big dots, eat ghosts, avoid. Right, so I'm going to point out that of those four. Which I'll just call eat, eat, eat, and avoid. Which one of those is different? So, I don't know. The first one. Eat? Where you're eating big dots. Yeah, or small dots. Now I guess you want to say, avoid. Because it's the word that's different in the list. Avoid is different, but it's actually different in a different way. Here we said, eat a little dot, eat a big dot, eat a ghost, and avoid ghosts. Remember when I said before that temporal extraction becomes goal extraction? I'm going to claim that each of these does have a goal. One is to eat little dots. One is the big dots. And one is the little goblins. But that when you execute the eat small dot, it actually terminates. At some point you eat a small dot. And now you can move on to the next thing. At some point you eat large dot and now you can move on to the next thing. At some point you eat a goblin and then you can move on to the next thing. But avoiding goblins, avoiding ghosts, that isn't really something that ends. You're supposed to be avoiding ghosts all the time. I see. Well, I guess it's sort of the negation. I do understand how that's different now. So hitting the ghost ends when you succeed. Avoiding I want to say ends when you fail. [LAUGH] Right. Right. Like I'm avoiding the ghost then said ooh. I wasn't able to avoid the goals. Right, so in fact these things have sort of goals, they have success conditions. They end when you succeed. Avoid is a little different. It sort of ends only when you fail. You can never succeeded avoiding goals you can only fail at it in some sense. Whereas here there a sort of finite time in which you achieve your goal and that success. So it turns out that when you ask people to come up with these kinds of options, they actually come up with things that look like this. Lots of goals with lots of success metrics. But they also come up with things where you can sort of only fail. We actually have a word for that and they're called constraints. So here you have goals you're trying to accomplish, sort of positive things you're trying to do and then you have constraints. Basically I want you to eat little dots, eat big dots and eat big ghosts while honoring the constraint that I don't want you to be eaten by a non-scared ghost. Okay, yeah, I mean so now from a logic standpoint, we've covered everything. Success and things that are defined by success and things that are defined by failure. Right, and but the failure here, I mean well, right, in exactly that way. So why did I bring all this up? I brought it up becausem one I just think it's kind of neat that people actually come up with these things. But the other thing that I think is kind of neat is that it gives us a hint, it gives us two hints actually. It gives us a hint about how we might do a better job than just simply behaving randomly in our rollout policy. So here's something you might do. You might say when I get down to here, rather than behaving randomly. And where by the way, I sometimes will get eaten by a ghost even though I could've easily avoided it. I'm going to try to behave randomly while still honoring whatever constraints I happen to have defined. So here, rather than trying to just walk around until something bad happens. I'm going to try to stay alive as long as possible which is what this constraint tells me. And that's a different kind of roll-out policy, but one that I claim that gives you a sort of better view of the goodness of this state. This state action pair is better because you can avoid violating your constraints for longer than if you are say in this state and take this action. And there are lots of other things that you might do. I just sort of wanted to give you a flavor of it which is that I can use these sort of options if they are of a certain type in order to be smarter about the way I do my roll-out policy. There's also another way that we can do this. I can still use options in general, in this entire search up here before. Rather than taking primitive action and doing my search through the tree in my expansion, I can replace this with my actions with options as well. Ooh. And what's the big win there? Well the big win there is, by following the options, I'm getting deeper into the tree. Yeah, it's the same kind of idea as in the option case that we talked about already. Except now we're in trees instead of, I guess, grids. Yes, but trees and grids, it turns out they're all the same thing. So there's lots of things you can talk about. There's been a lot of work in this space. Some of them are available as readings for the students. I recommend thinking about them but what's I think kind of nice about it is that this general notion of treatingm solving an MDP not as just I've got to solve the MDP, but as a kind of tree search problem where we take advantage of and use the randomness is something that unifies a lot of approaches that you might have. Okay, cool. Okay. And I think that's pretty much. I'll say one more thing about this and then I think we're done and that's just that there are lots of ways of looking at this. You might ask yourself is this even reinforcement learning, where we've just gone back to searching and I'm going to claim it is. I'm going to claim it's a policy search algorithm. What's your justification for that? Well, my justification for that is, I'm basically by building this up, I am doing, I am searching over possible policies. And what I'm doing in the inner loop of searching for policies is I'm doing a kind of reinforcement learning step here when I do my backups and in fact, the way you do the backups is you use the Bellman equation as a way of re=estimating Q values. And once I have those I now have another policy that I'm looking at and I just keep dong that over and over again. So I'm really searching through policy space which is sort of what the tree expansion is. And I have an inner loop every time I run out of my confidence about the policy. I'm doing a little bit of reinforcement learning to figure out what the policy ought to be. So it's policy search with an inner loop of evaluation, sort of. And Monte Carlo policy evaluation somehow. Right, exactly. Well that's really what it is and the way I'm doing the policy evaluation is by doing this well, doing the Monte Carlo. So it really combines a lot of ideas that we've talked about into kind of one algorithm. Has this been successful? It has been successful. I've actually used it in my own work. It has this really neat feature that allows you to avoid searching over lots and lots of the state space. It's actually really wonderful. It actually has a couple of properties that are worth mentioning. So let me mention those properties and then I think we're done. So I promised you some properties. There's a bunch of them. One of them is that it's useful, and that in particular, it's useful for large state spaces. And the reason that that works, right, is I don't have to actually look at all the states, do all the value iteration for everything. I basically just kind of do it as I go along. And I only visit the states that I need to visit, given where I happen to be in the world. There are some downsides of this, and one downside is, depending upon just how much randomness there is in your transition model, you need lots of samples in order to get a good q hat. But you know what, that's true anyway. If you have many, many, many, many many, many states, then you're going to have to do a lot of visiting to figure out what's going on usually. So, this is the sort of, I think it's kind of intrinsic to the problem. Now of course, if you don't have a lot of states, you still end up generating lots and lots of samples because of the way Monte Carlo works. Luckily there, it's relatively inexpensive to do, in most cases, at least, most of the cases that we worry about. And in the end, despite all that, it has another good feature. I really like this feature. It's my favorite of its features, is that the planning time is actually independent of the number of states that you have. That's why it's useful for large state spaces. I see, I mean, I guess if there's a lot of states, do we have to expand like a fixed percentage of them to know what to do? Well sure, you have to. That's how you're going to generate your samples. You're going to have to generate lots of samples that way. So yeah. But then it's not independent of s? Well, it is independent of s. That's one of the cool things about Monte Carlo Tree Search. No, I know, but what I asked was, so that means that as the state space gets bigger, do we have to expand a fixed percentage of the state space? You said percentage, I'm sorry. I didn't even turn that into percentage, I'm sorry. So the answer is no, because if you did, it would depend upon the number of states. [LAUGH] Okay, but how could that be? How could it be that the state space gets bigger, and yet, we don't have to look at very much of it to know what to do? Well, it's because you're really only going through, you're executing through the world. And the trade off happens in the other bullet, which is the bad thing about MCTS. It kind of reminds me of, like, I don't know, election surveys. It's probably not the right word. But this sort of idea that you can actually get an idea of how many people are voting, no matter how big the country is by just surveying a fixed number of people. Right, so long as they have properties that are representative, and so long as you can get a representative sample of the states and the actions, then you're good. That's really interesting. Right, now, of course, that's going to depend, I mean, so to me, what's going to drive that is going to be how much randomness and entropy there is in the transition function. I see. Because if it's completely deterministic, you don't have to do a lot of samples at all. Yeah, that would make sense. But there's something else that drives how expensive Monte Carlo Tree Search is, which is the fourth bullet that I was about to write down before you asked me this complicated question, which is that the running time. Is independent of the size of the state space. It's independent in the sense that it's exponential. Wait, in the size of the state space? It's exponential in the affective horizon. So you're saying it doesn't matter how many states are in the state space, but it does matter how far into the future you need to look to make a decision? Yes, and of course, the tree itself is growing exponentially. [LAUGH] So you're going to need to have more and more and more of those samples to make it work. Actually, due to Kearns and his colleagues, we actually know exactly what the running time is. It is big O of the size of your action space, which makes sense because that's your branching factor. And I need another variable here. Let's call it x, which is how many steps you're going to actually run the thing for. And that loop that I did before raised to the horizon. Now, one thing that's important about the horizon is that the horizon isn't just something you make up, right? The horizon actually depends upon things like your discount factor. And if you have a very small discount factor then so far out in the future it doesn't matter what's happening anyway. And so you can do better. But if you have a reasonable discount factor then you're going to have to look pretty far in the future to get a good estimate of taking a particular action in a particular state. Okay, makes sense. Right. So this is both good and bad, right? Monte Carlo is kind of neat because it's sort of neat on its own. It's very useful, it's one way of dealing with scale. It's completely compatible with options and other things that we know. It has a bunch of dials in it, that we can play with that sort of help us. And that we might be able to take cool advantage of, but you still need lots and lots and lots and lots of samples in order to make it work, which is true for any Monte Carlo method. So is there a sense in which we traded, in big states basis, we traded being exponential in the number of state features say? For instead exponential in the size of their horizon? Yeah and the trade off is, I mean it's right here, I suppose this is our trade off and you're right and that makes sense, right? Before with the kind of algorithms, that we used before, you visit all the states, you back up information about them and so on and so forth. So that clearly has to depend upon the number of states. Here, we're not worrying about the states, what we're doing is we're just looking forward into the future by doing simulation. And so every single time we have to look forward into the future, we have to look forward into the future. And perhaps we have to look forward very far into the future. And somehow, because we're doing this kind of Monte Carlo thing, we're doing kind of, I don't know, like a almost a non-parametric estimate of whatever the true transition model is, that's going to require lots and lots of states. And lots and lots of samples, and more and more samples the further out into the future that we go. So we are going to be trading that off. We're always trading off kind of space versus time in a particular way, and here the space is represented by the states. As opposed to the time, which is represented by how far out you're going to search. That's nice. Thank you. Okay Michael, so I think we're done with this section. So it's time for you to tell me what have we learned? So, this was the section on generalizing generalization and scaling. Right. So, we connected that idea with function approximation, which was the section that I did. Right. So, we talked about generalization beyond function approximation. To function approximation. And beyond! [LAUGH] To function approximazation. And beyond! Okay. And what kind of generalizations did we do? So, we did temporal abstraction. Mm-hm. And in particular, we talked about temporally extended actions. Mm-hm, or? SMDPs? Well, options. Right, right, right. And SMDPs. And, what does SMDP stand for? Semi markup decision processes. That's right, just like the truck. Right, a semi markup decision process. Exactly. And so, SMDP with 18 wheels. Yes. So, we learned about options in temporal abstraction, which is really cool. And what else? I want to say that led us to goal abstraction. It did, as a matter of fact. And in particular, do you remember? Mediating between the different goals, competing goals. We did, and that had a larger name. We talked about modular reinforcement learning. Right. I should remember that because I was alive when phones were modular. That was like a big deal. Nobody was alive when phones were modular, Michael. In any case, one of the things that we got out- I was alive before phones were modular and I was alive when they switched to being modular. [LAUGH] You're such a hipster. Okay, so we talked about modular RL or MRL as the kids call it, and the word you were searching for before, I think, was arbitration. Arbitration! It just seems so arbitrary. That's right. We talked about one other kind of abstraction, you might recall. State abstraction? Yeah, state abstraction. We got to state abstraction from the temporal abstraction. That's right. But not only do we get some state abstraction from the temporal abstraction, that is, not having to pay attention to all possible, all states for all particular options, it actually turns out that state abstraction is sort of what function approximation is doing. Hm, okay. All right, Yeah. All right? Yeah. So, I think that's pretty cool. Was anything else that we learned? I don't think so. Well, we had a name for at least temporal abstraction, we should at least mention that, nothing else, that this is a form of hierarchical. And we talked about quintessential search, but I think we actually meant quiescence. We did, [LAUGH], and what does quiescence mean? Quietness. Right, and in particular it means that you are forced- In the search it means that there's not all this crazy search backtracking stuff happening anymore. Things have settled down and you know exactly what the next thing is going to be. Right. So everything is settled. Right, and once things are settled, then you might as well not count it against your search. It has no more sort of exponential hit on your search. Right. You're forced. We can treat it as almost as if it's a single move, say, or a single action because once you do this the next thing's happening, the next thing's happening, the next thing's happening and [INAUDIBLE] Okay so I think that was almost it, was there anything else? Yes, we then made a leap to Monte Carlo tree search. I wouldn't say it was a leap. We climbed a Monte Carlo tree. Yes, and did some search, and what did we learn about Monte Carlo tree search? So it's really important to help with scaling because it lets you have computation that's independent of the size of the state space Even though it depends very possibly badly on the length of the horizon. Right so it's got a low o on the state space practice independent. But it has a big o on the horizon which I'll just call h here. And it's exponential with m. Yeah it's exponential with m. [LAUGH] It looks like hydrogen now. So does MCTS and abstraction fit together in any way? We could make it if we were abstract enough. If you pop back up far enough you can sort of think of it as a kind of abstraction over actions and policies. But no I was thinking in particular that the idea that in MCTS the way we talked about it, it was talking about specific states ground level states. Like if we have a good way of abstracting states can we use those abstract states in an MTCS context. Sure. In fact, what I was about to say is that all of the stuff that we learned about generalization up here can apply to MCTS more easily than others. Doing the function approximation is sometimes quite hard, but not really because of the way that we're sort of thinking about the q functions. But the temple abstraction works very well because I can use options instead of actions to do my MCTS. Right, you said that. Yeah I said that. And as a result the state abstraction and the the goal abstraction can also follow through there if you think about each of the individual modules as doing their own version of MCTS. But MCTS is a very general notion. So if I were trying to decide between different modules that I might take, it will look exactly like trying to do the sequential decision making and MCTS will work either way. But the big temporal abstraction is a huge win for MCTS because again all of its difficulty lies in the link to the horizon, right. So if I'm able to search far ahead in the future, that lowers my effective horizon. Nice. All right. So if these ideas are so important for scaling up do you do you feel like they've contributed to, I don't know, the ability of reinforcement, learning to solve some real problems. Yeah. Absolutely. No, these things actually come up quite a bit. They've been used in a lot of games. MCTS has been used, well it's all in the papers that the students have written. But MCTS has been used to solve a bunch of game problems in just my own work as well as in a variety of other things. You probably know a few more off the top of your head. But really any time you have a huge state space, or potentially a huge state space, these kinds of interactions turn out to be extremely useful. Some of them are easier to think about than others. Function approximation is very nice, but does have its own problem with picking the right features. Particularly, if you are going to be non-linear about it. Temple abstraction is often quite useful because people can help you come up with things like options or constraints. Goal abstraction has kind of a similar flavor to it. And because of those things, the state abstraction helps. Basically, anything that lets you lower the number of states or the effective horizon has to make the reinforcement learning problem easier. Nice. Alright. So there, scalability. These things actually work they actually are helpful and this is a huge wide open field in reinforcement learning. In fact, to your point I'll just sort of ended with this that I think that reinforcement learning has gotten so good at solving. Kind of well thought out grid like problems that. The field is really interested in, and increasingly more interested in figuring out how to make it work in the real world. And these kind of techniques, these abstraction techniques, these scaling techniques, become increasingly important as you try to move it out into the real world. So this is a good thing. Wow, I'm glad you told me about it. I'm about helping others. So I think we're good. So I guess, goodbye, Michael. Until next time. All right. Yeah, see you then. Hi, Micheal. Hi, Charles. How's it going? It is going quite fine. How's it going with you? Well this seems really exciting it looks like maybe you're going to drive the next lesson. I am going to drive the next lesson, and the next lesson is called Game Theory Three. Game Theory Three. I feel like that presupposes the existence of Game Theories One and Two. We have in fact done Game Theories One and Two, we did it in the last class. And so there's no point in doing it all over again. [LAUGH] Okay but maybe people don't remember all of that. Well I'm sure they don't because I don't so maybe what we should do is we should do another Scooby Doo flashback. Ahh, and we can do this in a super special way. Because? We're actually in the same place at the same time. Yeah. It's crazy. When's the last time that happened? Yesterday right. That's true, but this is the first time we're actually recording something like this that we are able to actually do that in a way that it makes sense that the people. Good. So let's do this go to flash we're going to do it together. Sure. Let's do it together, okay. You ready? Uh-huh. All right.Okay. You ready? Let's do it. [NOISE] Hi Michael. Hey Charles. How you been? I've been doing just fine. How've you been? Good. It's been a while. I understand you went to Africa. I went to Africa. The entire continent, or at least parts of it. How's Africa? It's fine. It's, warm. The entire continent? [LAUGH] The entire continent. Or at least Kenya and Nigeria. Okay. Particularly in Nigeria. Which was 80 something degrees and 4,000% humidity. [LAUGH] So, do you want to do a shout out to any of, any of your friends from there? Yeah, I'd like to say hi to Chairman and I'd like to say hi to Prof. Oh, and Good Times. I would like to say hi to Good Times. He was awesome. Okay, so Michael. I am back now so that we can talk about the last little section. That we're going to do in this class, and it's game theory. That sounds cool. What does that have to do with machine learning? So that's an interesting question because, in fact, game theory comes from a tradition way outside of machine learning in A.I. but you'll see in a moment why it is I care about game theory, and really this is a very natural extension to all the stuff we've been talking about with reinforcement learning. Interesting. Are we talking about games like Monopoly? In some sense, we are. Because all of life is like Monopoly. So in fact, what is game theory? Maybe that's what we can do. We can just try to define game theory, and maybe it'll be clear why it is we're worried about this at all. Seem fair? Yeah. Okay. Alright Michael, so there's lots of definitions of game theory that we could use. One that I like in particular is that game theory is the mathematics of conflict. Hm, [CROSSTALK] that's interesting. I think it's kind of interesting. Or generally it's the mathematics of conflicts of interest when trying to make optimal choices. because I feel like a lot of people have their own conflicts with mathematics. I think everyone but mathematicians have their conflicts with mathematics. I think that's fair. I see. But do you see if you, can you see how worrying about the mathematical conflict might be a sort of natural next thing to think about after you've learned a lot about reinforcement learning? I guess then well the next bullet kind of, kind of suggests a trend. So, so we've been talking about decision making and it's almost always in the context of a single agent that lives in a world and it's trying to maximize reward. But that's kind of a lonely way to think about things, so what if there's other agents in the world with you? Right and of course evidence suggests that there are in fact other agents in the world with you. And what we've been doing with reinforcement learning which, you know, has worked out very well for us, is we've been mostly pretending that those other agents are just a part of the environment. Somehow all the stuff that the other agents do is hidden inside of the transition model. But truthfully it probably makes sense if you want to make optimal decisions to try to take into account explicitly the desires and the goals of all the other agents in the world with you. Does that seem fair? Yeah. Right. So that's what game theory helps us to do and then at the very end I think we'll, we'll be able to tie what we're going to learn Directly back into the reinforcement learning that we've done and even into the Bellman equation. Oh, okay, nice. Yeah, so that is going to work out pretty well but, but we have to get there first and there's a lot of stuff that we have to do to get there. But right now what I want you to think about is this notion that, we're going to move from reinforcement learning world of single agents to a game theory world of multiple agents and tie it all back back together. It's a sort of general note that I think that, that's worthwhile is that, game theory sort of comes out of economics. And then in fact, if you think about multiple agents there being millions and millions of multiple agents, in some sense that's economics. Right? Economics is kind of the math, and the science, and the art of thinking about what happens when there are, lots, and lots, and lots, and lots of people with their own goals possibility conflicting, trying to work together to accomplish something, right. And so what game theory does, is it gives us mathematical tools to think about that. I feel like I feel like other fields would care about some of these things too, like sociology. Right. And what about, I could kind of imagine biology caring about these things, too. Even biology, I like the idea of biology. Biology. Why would biology care about this? Well, I guess the way you described it in terms of lots of individual agents that are interacting. Like, you know, creatures that live and reproduce. I feel like they, they have some of those same issues. Sure. So certainly biology at at the level of entities, at the level of mammals or level of insects, you might be able to think about it that way. But perhaps even at the level of genes and at the level of cells. Little virii and, and bacteria. You could possibly think about it that way. because they're in conflict too, I guess. Yeah. Now there's probably this notion of intention. It's not entirely clear what that means here and I think implicit in the notion of what we're doing here is this notion of intention and explicit goals as opposed to ones that are kind of built into your genes, but I think that's a perfectly reasonable way of thinking about it. I think the, the lesson from this discussion though is that. What game theory sort of captures for us or what would like for it to capture for us, is ways of thinking about what happens when you're not the only thing with intention in the world and how do you incorporate other goals from other people who might not have your best interest at heart or might have your best interest at heart. How do you make that work? And so if you think about that problem then I think it makes sense that it's been increasingly a part of AI over the years, and in some ways machine learning has started to think of it as being a mainstream part of what we do. Cool. Hence, why it's worth talking about today. Okay. Sound good. Gotcha. Okay. Let's, let's try to make this concrete with a very simple sort of example. Okay, Michael, so here's a, a nice little concrete example to, to think about this. Let's pretend that we're no longer in a world of a single agent like we've been thinking about with reinforcement learning, but we've gone full-blown generality to two agents, [LAUGH] okay? And let's call those agents a and b, and they're going to be in a very simple game where a gets to make a choice. And then b gets to make a choice. And then a might be able to make a choice. So this tree that I've drawn over on the right is going to capture the dynamics of this specific game that I'm imagining. So, these little nodes here, these circles represent states. And we can think about those in states in the same way that we. Talked about reinforcement learning in the past. The edges between these nodes represent actions that one could take. So, this should look familiar, this is just basically a game tree like anyone who's taken a, an AI course might have seen. Okay? I guess so. It doesn't look like a very interesting game. No. But I guess it's a, sort of abstract example. Yes. It's a very simple game just so that we can get a handle on some basic concepts. So, in particular, if you look at the details of this game, you start out in state one. Ok? And A gets to make a choice between two actions, going left or going right. If A goes right, goes right, she ends up in state three. If she goes left, she ends up in state two. Regardless B gets to make a choice. From state three we can choose to go right, and really that's all that can happen. And this, what happens if B goes right from state three is that, a value of plus two is assigned to A, okay? All of these numbers at the bottom, a the leaves here, are going to be values or rewards if you want to think about 'em that way that are assigned to player A. And, in fact, for the purposes of this game, it's going to be the case that B always get's the opposite of what A get's. So, if A get's plus 2 then B get's minus 2, if A get's plus 4 then B get's minus 4, if A get's minus 1, B get's plus 1, does that make sense? Yeah, though could you write it down so that I won't forget? Okay, that's fine. So, by the way, this is a very specific type of game. here, and it has a name, which I want to get right. This is a two-player zero-sum finite deterministic game of perfect information. So as a, as a title or description of, of this kind of game, does this make sense to you? Do you think you know what they all mean, what all those words mean? So, two players because it's a and b, zero-sum. Because you said the leaves are a's rewards and b's reward is the negation so if you add the two rewards together you're always going to get zero. That's almost right. [LAUGH] Ok. It's not exactly right. Actually, so zero sum really just means that the sum of the rewards is always a constant. And that constant needs to be zero. It doesn't need to be zero. So if it added up to eleven, that would still be zero sum? If it added up to eleven everywhere. Yes. Huh, okay, interesting choice of terminology. finite, I don't know, everything seems to be finite here. There's no infinite number of choices or states or depth. Mhm. deterministic, well, again, thinking about it in an MVPish kind of way, there's no sort of casting transitions in this particular. Picture. Right. So if I'm in, state two and I go right, I always end up in state four, period. Right. Mm-hm. Game. I guess, a game is because it's more than one player? Sure. Of perfect information. That doesn't quite sound like the same terminology that we used in the empty MDP setting. But, I'm wondering if that's like, I know what state I'm in, when I'm making a decision. So, it's like a, like an MDP as apposed to a POMDP. Well it, that's exactly right. It's, it's that you know what state you're in and. Yeah. That's exactly what it means. It's like being the MDP versus the Palm DP. That's a great analogy. Cool. And does it matter that it's a tree like this? because when we were looking at MDPs, we had more complex structures of graphs and things. Well, you can think of this as unrolling the MDP if you want to. So then those states are sort of time stamped and history stamped. For, yeah, for the purposes of this discussion, yes. And that's a perfectly reasonable way of thinking about it. But, okay. But in general, we're going to be thinking about game trees, but actually, we're going through all of this for nothing, because we're going to discover pretty soon that none of this matters. [LAUGH] but, give me a couple of slides to get there, okay? [LAUGH] Sure. Okay. So this is about the simplest or at least the, the least complicated game that you can think about. Two players, zero sum. Finite deterministic game of perfect information. You know, basically, I can look at this tree, I know everything I need to know, and I can make decisions about what action I might want to take in order to maximize my reward. Okay? Good. All right. Now, in NBPs, of course we had this notion of. policies, right. You remember what a policy was Michael? Mapping from states to actions. So in the game theory world, we have something very similar to, policies. We call them strategies. So, all a strategy is, is a mapping of, [LAUGH] all, of all possible states to actions. So for example, here's a strategy, that A might have. When in state 1, go left. And when in state 4, also go left. Seems like a terrible strategy. Does it? Well, just, if nothing else, just in state 4. Sure, but it's a strategy, right? Okay, but it's just, it's a strategy for one of the players. Right, exactly, each player has a strategy. And that makes sense, right? Before when we talked about a policy, and mapped [UNKNOWN] to action, there was only ever one player, only ever one agent. And so we didn't have to worry about what other strategies there were. Here, when we talk about a strategy, it's always with respect to one of the players of the game. Okay, so, question. I've just given you one strategy, which is what A does in all the states A could potentially end up in. How many other strategies are there for A? For A? Okay, that sounds like a quiz. That does sound like a quiz. Let's make it a quiz. Okay Michael, so here's the quiz, because you made me make it a quiz, I decided to make it even harder, so I want you to tell me how many different strategies are there for A and how many different strategies there are for B. And you are talking about just deterministic mappings only, right? Well, we are in a two player zero sum, finite, deterministic game of perfect information. So, [LAUGH]. So yes, I mean deterministic. Okay, alright. because, you know, it could be that it might be helpful to be stochastic. Oh, that's true, that's true. So in fact, that's a good point. I was going to mention that later, but l'll mention it now. What I just wrote down here, is actually not just a strategy, it's something called a pure strategy. Hm, it's always about purity with you. It is, purity of chocolate and bacon mainly. But, yes. So, these are simple pure strategies. Okay, so how many pure strategies are there for A and B. Okay. Okay, then let's go. Okay, Michael, you ready? Yeah. Alright, what's the answer? So I was thinking about A before you threw in the B. So let me, let me not think about B yet. I'll just think about A. So you had said in state one, it can go either left or right. And in state four, it can go either left or right. So boy, that sounds a lot like two times two equals four. Yes that's exactly right. But generally speaking, so actually walk me through that again Michael. How did you get two times two? So, well I, I had a little choice about how to think about it. One is that in some sense, if you go right from one, then you don't really have to make another choice. Right. But if you go left, then you have this other choice to make of either left or right. So it's, you if, if you're just writing it down as a mapping from state to action, you've got two choices at state one, and two choices at state four. Mmhmm. And so that is two times two, right? You can make, independently choose each of those. Right, that's exactly right. So in fact it's important there that even though if I can gone right on one, I would never, have to make another choice because I can't reach state four. In order to be a strategy, you have to basically say what you would do, in all states where you might end up. Okay, that's fair. Okay, alright so what about B, using that incredible reasoning? [LAUGH] So yeah, so B seems a little trickier because here it can only ever matter whether you're in like if player A sends us down to the left then we have a choice of three. If player one sends us down the right, we have a choice of one, which is really no choice at all. Mmhmm. You can have any color car you want as long as it's black. Yeah, like my Tesla. I was thinking the Model T, but maybe T stands for Tesla. T does stand for Tesla. So, by the definition of how many different you know, sort of reachable strategies it would be one answer but, if you're defining it the way you're defining it, it's going to be three times one or, three. Yeah. And that's exactly right. Good Michael. So, let's actually write down what those strategies are. Okay, Michael so I have written out biased on upon your rather impressive way of thinking about it. All the possible strategies for a. And all the possible strategies for b. A, can go left, left, left right, right left, right, right. For states one and four respectively. And b can go Left right, middle right or right right. Right. Right. In particular, three in stage three we can only go R. Which makes me a? Conservative? A pirate. Oh. That makes more sense. It does make more sense. Okay. So, now why did I write it this way Michael? Well, I wrote it this way because if I write it this way with all the choices, all the strategies that b has up here and all the strategies that a have here. I actually form a matrix. And I can put in each of cells of the matrix the value of taking a particular from a and a particular strategy from b. Does that make sense? Yeah, that's very clever. Yes, it is very clever. I'm very happy that I came up with it entirely on my own. Okay, so let's start filling in these numbers. Or, instead of filling in the numbers ourselves, we could ask the students to do it by making it a quiz. Nice, I see. Shall we do that? Sure doesn't seem very hard. Okay, so let's make certain everyone here understands and what exactly we're asking you to do. We're saying that if for example A takes this first strategy go left and stay one and left and stayed four. And B takes it's first strategy which is go left and stay two and right and stayed three. What is the value for A that will result? So, let's actually do the first one as an example and ask everyone to do the rest of them. That seem fair? Yeah, that's what I was going to suggest too. Okay, good. So let's see. If A chooses to go left in state one, since A goes first, we'll end up in state 2, right? And then in this first strategy. B goes left in state two, which means we will end up going down this path and the value there is plus seven. So seven is in fact the value of this game with respect to A. Now we know that because this is a two player zero sum finite deterministic game of perfect information. That if a gets a value of seven, b gets a value of minus seven. So we could write minus seven here, seven comma minus seven here. But since we know that it's equal and opposite let's just write down the value for a. Okay? Just for compactness-sake. That seem fair to you? Yeah. Okay, cool. So with that in mind, let's see if we can fill out the rest of this table. Think you can do it? I think so. Okay. Then. Can you give me a place to write everything? It will magically appear. Go. Alright Michael. Tell me the answer. Lets go across. Alright. Across. So. The first row it's all left, left, but the second left doesn't matter because that's in state four. Oh it might matter. I take it back. So left, left you did was seven. Left and then, player two goes to medium, middle. Middle. That would be three, so then the pay off is three. Mm hm. If the first player does left, left and the second player does right, right; then we're going to go left, right, left again, or for a minus one. Mm hm. Oh wait, hang on. Left, right, left. Yes, huh, yeah. The next row should be exactly the same as this one, except for in that third case. So it should be seven, three. And the r, the difference there. The r in four only matters in this case where we've gone to the right state in [INAUDIBLE], sorry, right action from state two. And so that should give us a plus four, so four. Correct so far. All right half way there, so, all right now, now, we've got something different so now player one is going to the right, so now actually player one's choice in state four never matters, so the next two rows are going to be identical to each other. And so in the first case, oh, actually [LAUGH] all of these numbers are going to be identical to each other, so player one goes right, player. Or, sorry, player A goes right, player B has to go right. There's no choice. So we're going to get 2s, no matter what now, so it's going to be two, two, two, and then the second row will repeat that two, two, two. Okay. So there you go. Michael, you are correct. You can feel very good about yourself. Very good, very good. So you're right, Michael, we do have we do have this nice little matrix and we have these numbers, and you know what's really interesting about this? That we can figure out what the payoffs are for B without having to do any additional work. That's true. But it's even more interesting than that. Which is, now that I've written down this matrix, nothing else matters. This whole tree, all these rules. None of it matters. It's irrelevant. Wait, wait, but. Okay. Because everything about the game is captured here in this matrix. This is actually called the matrix form of the game, and it comprises everything you need to know about it. It doesn't matter ha, what it means to go left or right or what it means to go middle or whatever. The point is by following this strategy and this strategy, you end up with seven for a. This strategy and this strategy, you end up with two for a, period. And how you got there does not matter. But it does creep me out a little bit that what your saying is that all that matters is the matrix. You know we should write a movie about that. I think it's been done and it's scary. Well, not the first one. It was kind of scary. Like people are trapped in this big box and their slurpyness. But their happy. But they don't know their happy. What do you mean they don't know their happy? [LAUGH] What is means to be, Michael, Michael, Michael. Speaking of which now how do we, how do we figure out. What do we do with this now? So that's the big question right? So, what was the whole point of doing reinforcement learning? The whole point of doing reinforcement learning was to optimize your long term expected reward right? Yeah. And so you pick the policy that would get you to the best place possible, so here's a question I have for you. Give that this is everything we need to know, this little matrix of values. These are all the policies A could choose from. We're calling them strategies here. But, you know, strategies, policies. There are four A can choose from. And there are three B can choose from. What will you think A and B actually do? Okay, so, you know? A wants the rewards to be high. So, seven is the highest, so A should choose the upper left corner. But A can't choose the upper left corner. Why? A can only choose a strategy. B gets to choose some strategy. I see. So A is choosing the row and then B gets to choose the column. Yeah. So then B should choose that also because that's what A wants. No, B wants to maximize what B gets. And remember, B always gets the opposite of A because. Because it's a two-player, zero-sum, finite game of perfect information, deterministic something. So, so you're saying that if we, if A chooses the first row. Say. Mm-hm. Which is the left, left strategy, and B now has a choice between three values and will choose the one that is worst for A, which would be the minus 1, so that would be a terrible thing to do. Yep. Okay then, so then what if A chooses the second row? Okay if A chooses the second row? So now again B is not going to let them have that seven which is kind of sad, but still cant make it to bad for A so B would choose the middle right. Strategy. Mm-hm. And then get, and then that would be a 3 for A, a minus 3 for B. But wait, hang on. So that was done thinking of it as A move, A kind of making the choice first. Mm-hm. That doesn't seem fair. Okay, well then do it the other way. Alright, so if B chooses first, then B could choose the far right column because that's where the minus 1 is. Mm-hm. That's where it's going to be happiest. See, B seems kind of, kind of mean. It's like it's happiest when others are suffering. Well, but A is also happiest when others are suffering. I see. So, if B chooses that column then A wants to choose the second row and get the four, so B could choose the middle column, which then A would choose the two. A would choose what? One of the twos. One of the bottom two rows. No he wouldn't. Oh, right. A is trying to maximize, so A would choose one of the top two rows. Oh, yeah, that makes more sense. Right. Then choose a three which is better for B than having chosen the far right, and B could choose the far left column, but then A would choose one of the sevens, and B would be unhappy with that. So we'd end up with B choosing the middle column and A either choosing the top or the second row. Mhm. So that's kind of the same answer we got the other way. Yep. Huh. That's exactly right. And is that, was that just luck, or did you make this example to make that happen? No, I didn't make this example to make that happen. In fact, the process you went through is exactly the process you would expect to go through, and you will always end up with the, with the same answer. For a two player zero sum game. You know, deterministic, finite, all those other words. [LAUGH] The process you just went through, is exactly the process you would expect to go through. So let's see if we can, if we can be a little bit more, you know, explicit about the process you went through. So, in particular, the way I heard you write it down was that A must consider the sort of worst case counter strategy by B right? I see, because when A chooses the row, then B was going to make things bad for A along that row, so that's the counter strategy you mean. Right, and in fact, when you try to do it the other way with B. Well, B has to do the same thing. B has to consider the worst case counter, as well. And in this particular case, the way we set it up. Where the values for A. A is always therefore trying to maximize. And B is always trying to minimize A. Which works out to be the same thing as maximizing itself. Does that make sense? Yeah! I mean, other than the fact that, you know, I name my first child Max. I really wanted to name my second child Min. [LAUGH] That actually would have been pretty cool. Why didnt you do that? Because I'm married to someone with more sense then i have. Yeah i understand, i completely understand. Okay so, A is trying to maximize B is trying to minimize they both have to consider the worst case that the other will do. And so whats thats going to is going to force them through exactly the same process you went through. We just figure, I'm going to make the choice so that my opponent makes the counter choice, the worst case choice. I will end up as best as I can. So A is going to basically try to find the maximum minimum, and B is trying to find the minimum maximum. Hmm In fact that strategy has a name, or that way of thinking about it has a name. What do you think it's called? The minimum, maximum. Yes, or Mini max. Which is movie production company, I think. No, that was Miramax. Do you recall that where you have seen Mini max before Michael? In some other class that you once taught or once took years and years ago? Mmm. No. Mini max was exactly the algorythm that we use for game search. And intro to AI. Oh, which was a game tree, which is just what we started with in this case. Even though we turned it into a matrix. Exactly. So in the end, we, you know, this matrix induces a game tree, if you want to think about it that way. Or, game tree induces a matrix, if you want to think about it that way. And the strategy then, in sort of basic AI search, and the strategy in game theory is Minimax when you're in a two player zero sum game of perfect information. So is there a way to do alpha beta pruning on this? All alpha beta pruning does is gives you a more efficient way of finding the answer. I see but it's the same answer no matter how you set it up. Right. Cool. That's right. Okay, so this is pretty cool. So, we have set up a kind of game where we have multiple agents, you know, or at least two agents in this case, who have different strategies. And we actually sort of know, [SOUND], you know, sort of, in a world where it's a zero sum game, and you know the other person is trying to minimize what you get, maximize what they get. That the mini-max strategy would actually give you sort of an answer, in this case, by the way. We say that the value of this game for a is three. If a does the rational thing, and b does the rational thing, that is trying to maximize their own value, you will end up in this situation. That's kind of cool, don't you think? Very cool. I feel like there should be a theorem. There is in fact a theorem. I'm going to write it down. Okay Michael, so here's this theorem that you, you so desperately wanted. You ready? Yep. I'm going to read it to you, because you can't read. In a two player zero-sum deterministic game of perfect information, minimax equals maximin. Alright you told us what minimax was, but you didn't tell us what maximin was. Well maximin is like minimax, except the other way around. So a is trying to. [LAUGH] You know, one side is trying to minimize the maximum, the other side is trying to maximize the minimum. Okay. It's exactly what we described before, just depends upon whether you're looking at it from a's point of view or b's point of view. Oh, I see, like, which, do you choose a column first or do you choose a row first? Exactly, so whether you go a first followed by b, or b first followed by a. You're end, going to end up in the same, with the same result. And that, more importantly, or at least as important, there always exists an optimal pure strategy for each player. In other words, you can solve those games and you know what the answer is. Once you write down the matrix. You just do Minimax, or you do Maximin and you end up with the proper answer. And now you know what the optimal players would do. Now there is a subtlety here which I got it a little bit in the previous slide, when I talked about rational agents. And what we're sort of assuming in everything that we discuss here is that people are always trying to maximize their rewards, okay? So we define the, the reinforcement learning problem that way. That my goal is to find a policy that maximizes my long term expected reward. You know so I'm trying to find, to get the best reward that I can. And what you're assuming here is that everyone else is doing the same thing and they're assuming that everyone else is doing the same thing. Okay. DOes that make sense? It does thought I'm a little bit stuck on this word optimal at the moment. Right. Well, that's what I'm trying to get at actually. That optimal here really means I'm maximizing the reward that I can get, and I'm assuming everyone else is doing the same thing. And I'm, furthermore, I'm assuming that they're assuming that everyone else is doing the same thing. So, so I guess I'm wondering whether. Whether this theroem is vaccus in a sense that are we defining optimal to be mini max. What we're defining optimal to be, I think. So that's a good questino. I think I would unroll the vaccus one level by saying this. Optimal here, basically has to be optimal in respect to what? And the respect of what here is the underlying assumption that everyone is trying to maximize their rewards. And that everyone knows this. So, in a world where you have perfect information. It's zero-sum. Then, the strategy of Minimax and Maximin give you the same answer. And that furthermore there is always some place where the column and the row cross, the best column and the best row cross. And that is always going to be the solution to that particular game. Now, if we weren't in a two-player zero-sum deterministic game of perfect information, that might not be the case. But in a case where we're in this sort of simplest version, where everyone's being. Rational, that is, optimal, that is, trying to maximize their, their own reward. And assuming everyone else is maximizing their own reward, this is the right thing to do. Now, I've got a little weasel word here as well which we're going to get to in a moment which is this notion not just of a strategy but of a pure strategy. [INAUDIBLE] There's a reason why we have notions of pure strategies because in the end as we get more complicated we're going to have to do it with impure strategies. Mmm. Okay, but are you with me so far? I think so, yeah. So basically all that stuff we did in AI with game trees and search is kind of what you would expect people to do if they knew everything. [LAUGH] So, So, I feel like I could prove this theorum in the case of trees because you just prop, you just kind of commute values from leaves up to the root. Yeah. And, it, it doesn't matter. There is no notion of who goes first or who goes second. So there's just going to be one answer, to that process. It's not obvious to me, how to show it, if you, once you've. Turn the tree into a matrix, that that matrix, I guess because it captures the same information, it ought to be the case that this is still true, but like, I'd have to kind of sit down and think it through. No, and it's, so, so, to help you think it through, I guess what I would suggest is if you have the matrix, you can create a tree that's consistent with it. Because every row and every column represents a strategy. You don't know what that strategy is, but you can, since it's a finite matrix. You can construct a tree that is consistant with that major. In fact, possibly an infinate number of them, I'm not sure, but you can certainly construct at least one that is consistent with it. And then once you have the tree you just do what you said. Good. Alright, good. So we've got this fundamental result and now what we're going to do is we're going to try to be a bit more interesting. But it is important to go through this because now we've got some basic vocabulary and some basic building blocks okay? Yep. Alright. So here's another game tree. We have, again, two players, a and b, and a gets to make a choice, first, to go left or right. And b will find herself in a situation, perhaps, where she gets to choose to go left or right as well. I've drawn these little square boxes, though, to represent chance. So what this means is that if a goes left, you end up in a chance box where you flip a coin and 50% of the time you end up over here, and 50% of the time you end up over here. Along a similar vein, if a goes right and then b goes left, then you end up in another chance node, and 80% of the time you end up here, and 20% of the time you end up here. Alternatively, if b goes right, you always end up here. Does that make sense? I think so. Uh-huh. So what we've done is we've gone from a two player zero-sum deterministic game of perfect information to a two player zero-sum non-deterministic game of perfect information. Okay, so we've relaxed, we gone at least one level up in complexity. Okay, so do you understand this tree? Do you understand this setup? Yeah, I think so. I mean here the stochasticness is happening essentially at the leaves. What does that mean? That there's no choices to make for either player after this randomness happens. But is that. That's not what you mean in general, right? No, that's right. There could be, after here, you end up in a different state where you can then make more choices. But because I don't have enough room, maybe because I want to make it simple, it just sort of ends after that. Okay. But yes, this tree could keep going on and there could be choice, random, choices chance nodes happening everywhere. There could have been a chance node that happened at the very beginning, even. I feel like I could work out the value of this game. Oh, well, how would you go about working out the value of the game? I would probably have it as a quiz. Okay, but what would the quiz look like? It would say, what's the value of this game? Okay. Do you think that just having that be the quiz is the best way for someone to learn or do you think maybe it can be done in stages? Oh, I don't know. Do you have other questions that you want to ask? Well, you know, how would you go about determining the value of the game? I think the first thing that you would do, at least if you were patterning after what we just did, is you would try to write down a matrix. Sure. So, I'm going to write down a matrix. So that our students will get a chance to, to work out what that matrix looks like and then from there figure out the value of the game. Are you okay with that? Sure. Okay. So if we were going to do that of course. The first thing we'd have to do is we'd have to figure out, figure out what the strategies are. So. What are a's strategies. I would have called them left and right but it, but they're not labeled. Yeah well let's do that we can call them left and right. A can go left or A can go right. What about B's strategies? B can go left or right. And B can go left or right. Here's your first quiz question. In a world where A can go left or right and B can go left or right. What are the values that you would put in the cells of this matrix? Again, this is zero sum. So, these values are from A's point of view and implicitly, there's we know what the values are for B. So, we're just looking for the values from A. Okay? Do you understand the quiz, Michael? Yep. Okay, so go. All right Michael, you know the answer? Yep. Okay, what's the answer? Do you want to know the value of the whole game? No. I want to know the matrix. [LAUGH] Okay. So right. So, if A goes left, it doesn't matter what B does. And at that point there's a chance node, and it's 50/50 for negative 20, which I feel like ought to be, negative eight. That's right. How'd you get negative eight? because half of 2 is sorry, half of 4 is 2 and half of -20 is -10. So -8. Right, so you just took the expectation of where you would end up. Exactly. Okay. What next? And that, it doesn't matter what b does. So then negative eight is also in the upper right corner of the matrix. Fair enough. The next easy on to do is if both go right. A goes right and b goes right then we get the three. It's just sitting there. Mm-hm. And the last thing requires me to do some multiplication. So Rrr. -5 times 0.8 Is like -4. Mm-hm. 10 times 0.2 is like 2 so -2. That is correct Michael. Shoo. So now we have the matrix. Now, what did we just notice here? Remember what I said about matrices before? It has all the information that you need. Right, so in fact none of this matters. Who cares how we got here? wait, but, but, oh. [LAUGH] I love erasing stuff! So here's the thing, we cannot reconstruct that tree from this matrix. We can't reconstruct, well, actually we could reconstruct that tree from the matrix. No, we can't. Yeah, we, of course we could. No, we can't. Yes, we can, because there's an infinite number of trees we could do, and one of them is that one. I see. We just don't happen to know that its the right one. I don't think that's really what people mean when they say I could reconstruct this. Like I could reconstruct the crime at this crime scene, it could be any of a million things. Like no, we can't construct that specific tree. But you know what it doesn't matter, because the only thing that matters is the matrix. Ahh. And you will notice, Michael, that you did all that multiplication and you multiplied 0.8 times 5 and after a couple of seconds of thinking about which was probably edited out but I know how long it took you do. You came up with the right answer. That's great. But notice that the number you came up with doesn't say anything about expected values and what the probablities are. It doesn't even matter that it's nondeterministic, because once you have these numbers, these expected values, that's what you're going to end up with. So who cares what the original tree was? Who cares if you can reconstruct it? Who cares what the rules of the games are? All you know is I have a choice of two strategies. We call them left and right here, but we could have called them one and two, or Q and Z. It doesn't matter. For the purpose of solving the game. So what is the solution for the game by the way? Oh, A is trying to maximize right? Yes. So A would never, ever, ever, ever want to go left. Mm-hm. So A's going to go right and then B is trying to minimize. So B is going to go left and we get the -2. I believe that is correct. And it makes sense the other way as well, right? That B would not ever choose to go right because then A would choose to go right as well. Gotcha. So you'll still end up here. So, here's in fact another theorem for you Michael. So, it turns out the theorem that we wrote down before is still true in the case of non-deterministic games. Oh, cool. Of perfect information, right? By the way, do you know whose theorem this is? Charles' theorem? No, although that would be cool. Von Neumann's theorem? Yes, this is von Neumann's theorem. Oh really? Yeah. Did you just guess? Von Neumann, he's responsible for everything. Do you know, well, you going to remind everyone who von Neumann is? He's my uncle? No. No, you're right. You're right. Von Neumann, so we talk about von Neumann architectures in computer science, that the basic design of a microprocessor is still following the ideas that that he worked out. Right, the von Neumann Machine. So there you go. So, this is pretty good, right? So, what we did, so what did we learn so far? What we learned so far is, the only thing that matters is the matrix. And once you have the matrix, you used mini max, at least if you're in a two player zero sum game of perfect information. At least if you're in a world of two player zero sum games of perfect information. We can just write down this matrix, throw everything else away and use mini max or maxi min and figure out what the value of the game is, which is the same thing as saying as, we know what the policy is. The kind of joint policy, in a world where everyone is being rational and trying to maximize their rewards and assumes everyone else is doing the same. That's pretty cool, I think. Awesome, so though, you know, I feel like I could make a matrix that wouldn't have this property. This maxi min equals mini max. And, and that we couldn't make a tree out of it. You probably could, but I'm going to guess that it's going to require that it's no longer zero sum. Nooo, I'm going to say no to that. Well, you know what, I, actually now that, I think I understand what you mean and the answer is yes you could. And in fact, that's what we're going to do next when we relax the next little bit. Ooh. So, would you like to do that? Sure, though I'm afraid if we get too relaxed, we might just fall asleep. In fact, why don't we do that, why don't we take a nap and then come back? [LAUGH] the, the joy of being on video, asynchronous napping. Done. Okay, I'll see you in a second. Okay Michael, so here is a, another little game for us to play. And, what I want you to notice about this game before I describe it to you in detail is that we have relaxed yet another one of the constraints. So, we started out playing two player, zero sum, deterministic games of perfect information. There's also a finite in there somewhere. From now on we're just going to assume everything's finite, because why not? And then, what we did last time. Just a few seconds ago. Is we relax the deterministic part, so we have two player, zero sum, non-deterministic games of perfect information, and now we are going to relax the requirement for perfect information. So now we're going to look at two player, zero-sum, possibly non-deterministic games of hidden information. And this is really important Michael because, this last little bit of relaxation, going from perfect information to hidden information is going to be sort of a quantum leap into the difficult problems of game theory. So this is where it actually starts to get interesting so we have been building a foundation so far and now we are going to get to the interesting and complicated stuff, okay? Wow, one of the things that I learned just now is that the opposite of perfect is hidden. Yes. I always thought the opposite of perfect is imperfect but okay but hidden. I guess if I do not have a perfect face I should hide my face. Thats in fact the atomology of the phrase. Alright, I understand now. Yeah, cool, It's in wikipedia. Here we go, let me describe the game to you, are you ready? This is a version of mini poker where there are a set of cards, but they have no numbers of faces on them they are just red or black, okay. And red is bad, for our hero, Player A, and black is good for our hero, Player A. Okay? I see. So-- Wait. Why's it have to be red? I don't know, man. You know, red. You know how it is. Okay. So, here are the rules. They're all written down on the screen, but let me walk through them with you. So A is delta card. Magically. It will be red or black. and, the probability of it being red or black is 50% each. Okay? Yes. Right. So we have a uniform prior over, over the color. Now, remember red is bad for A and black is good for A. So, it's going to turn out without loss of generality, that if A gets a black card, A's definitely going to hold onto the card. Okay? Now A gets this card. B, player B, does not get to see the card. A can choose to either resign or to hold. If A resigns given a red card, then he looses 20 cents. Okay? Okay. Wait. So A's dealt red. A may resign if but only if red. Right. And then A loses 20 cents. Right. Okay. Alright, okay. Okay, so this is a betting game. It's not strange it makes perfect sense its sort of a metaphor for life. Now A can choose to hold instead, hold the card. Thus requiring B to do something. So if A holds the card B can either resign or can demand to see the card. Now if B resigns then A gets 10 cents. Regardless of the color of the card. Okay? Yep. That make sense? Yep. Okay. Now if B chooses to see the card, in fact demands to see the card. Then if the card is red, then A loses 40 cents. But if the card is black, then A gets 30 cents. And since we're betting, this means that whatever A wins, B loses and vice versa. That makes it zero sum. Got it. Okay, is this all make sense? Yeah, I don't know if I can hold all those different numbers in my head. But I, but the basic pattern of it is, that as you say Red is, red is bad, black is good. If A gets a bad card, A can essentially either fold Mm-hm. Right, resign? Or kind of bluff, like hey I've got this great card and then A and if B believes that the card is bad then and calls A or, or, or, and, and just folds then, then A gets, A wins that. But if B says no I think maybe you're bluffing and calls him, then everybody's rewards are more extreme, I guess. >Exactly. So it's just a version of poker. A weird version of poker. Simple version of poker. But a version of poker, none the less. There's a minor little detail here which isn't that important, but you know, notice it's written that A will A may resign if its red. Basically, A will never resign on a black card. Because it just doesn't make any sense. And so there's really, it just, there's not point in riding it out. Okay. Because black is always good, sort of, nothing bad can ever happen to A, if A gets a black card. So there's really sort of no point in riding anything out. But that's just a minor detail. Regardless these are the rules. Okay? Okay. You got it? Sure. I'm going to re-draw this as a game tree which might make it a little easier to keep all the rules in your head. Okay so Michaell here's the tree version, of what I just said. So remember I draw squares as chance nodes. And so, chance takes a, chance. And half the time we end up in a state, where I have, where A has a red card. And half the time I end up in a state where A has, or we end up in a state, where A has a black card. Okay? If A has the black card. Then B gets to choose whether to hold or not, so let me add a little bit of color for you since you wanted some color. This is the place where A gets to make a decision. Okay. Yep. And then this is the place where B gets to make a decision. Got it. Okay? So, A is going to either be in a red state or a black state. Only A knows this. B does not know this, does not know what state He is in. And so let's say A is in a black state. Well, A can only hold in that case. In part because it makes no sense to resign. And then B gets to decide whether to resign, and therefore A gets ten cents or to see, in which case A gets thirty cents. By contrast, when we're in the red state A can either hold or resign. If A resigns he loses 20 cents. If A holds then B gets to decide whether to resign and which case A gets 10 cents, or to see the card in which case A loses 40 cents and B of course gains 40 cents. So this is just a tree version of what I just wrote. Does that make sense? Okay, yeah I can see how this kind of captures the flow of information but I feel like there might be a missing constraint to it. So [INAUDIBLE] I don't know if there's actually a constraint in this thing but let me just point out something. And that is that B has no idea which of these two states his in, and its hidden information. And because B doesn't know what states he's in. He doesn't know whether resign over see. In particular B knew that he was in this left mode state then he would always say see. If B knew he was always in the rightmost state, then he would always resign. But he doesn't know which one, so it's not entirely clear what to do. Neat! Where did you get this game? This game is awesome! . I got this game like I got all of the examples that we're using today, from Andrew Moore. He's clever. He is clever. And I have shamelessly stolen... All of his examples and materials for this. But he said it was okay, by putting up slides in the world and saying, feel free to steal all of the stuff. You want to write his name so that people can credit him? Yeah, I'm going to write it at the very end. When we talk about. Okay. What we've learned. Awesome. Okay. So here's a question for you Michael. We know that I wrote down a bunch of words which describe the game, and then I made it a tree, because I could do that. And it makes it nice and easy to see what's going on. But now we know, that at least everything else we've done. We want to make a little matrix. And so we want to figure out what the value is for different strategies for A and B. So I'm going to assert, and I think it's pretty easy to see, I hope. That A basically has only two strategies. Either A is the type of person that resigns when a card is read. Or A is the type of person who holds, when a card is read. Agreed? Interesting, okay. Yeah. Right. So it really is a conditional policy, right? It's basically If red hold to resign, if black hold to resign, but your point is that black isn't really a choice, red is a choice, and there's only two choices. Okay I can see that as the two strategies sure. Right and of course this does say, this is, this does kind of say when you're in the black case you know you're going to hold. So you know what's going to happen. Ultimately B can either be the kind of person who resigns, whenever A holds or chooses to see, whenever A holds. Right? I see so, like in the previous trees. B would have four different strategies Resigner see in the left state, resigner see in the right state. Here there's this kind of extra connection, this sort of a quantum entanglement between these two states that it, that makes them have to be the same. So there really is just those two choices. That's exactly right, although, I probably wouldn't have used the phrase quantum entanglement. But yes, there's an entanglement, certainly. because we can't tell which state we're in. You're either going to resign or you're going to see. And you just don't know what else to do. Okay so A's a resigner or holder. B's a resigner or a seer. So the question is what numbers go in this matrix? And we're going to figure that out by having a quiz. I kind of saw that coming. Mm-hm, do you think you can figure this out? yeah, yeah. Sure. Sure. Yeah, I think so. Yeah, see, see, see. Yeah, I can do that. Okay, cool. So, let's go then. Go. Okay Michael whats the answer? Let's start with resigner resigner. Alright, resigner resigner. Resign resigner diner. Alright, E, so resigner vs resigner. So resinger, if A is a resigner that means whenever A gets a red card A resigns. Yes. And that would be a negative 20. Yep. But that's not going to be the answer is it. Because A doesn't always, A doesn't always get a red card A sometimes gets a black card if A gets a black card [CROSSTALK] than a resigner, resigner that means B is going to resign and, and a plus 10 will happen. Mm-hm. So those are the two possibilities and they're equally likely, so it's a minus 10 divided by 2 which is minus 5. Right. You were correct, sir. Whew. Alright. Which one do you want to do next? Resigner seer. Okay. So again, oh, so the, yeah, good. This is a good choice, because now it's the same as argument as before, except for when we end up in that far right node, and that means minus 20 in half the cases, plus 30 in half the cases Which is a plus 10 divided by 2, or plus 5. That's exactly right. Well done Michael. Thanks. Okay which one next? So holder resigner. So holder reigner. That means when A, gets a card. A is going to hold the card. Mm-hm. And that's true, red or black. Yep. And then, then, it's going to be B's turn, and B is going to, oh, we're doing holder resigner. So, it's going to resign. Mm-hm. So, oh, well, interestingly, I think that takes us to those two leaves, both of which are plus ten. Yep. Because, why does that make sense? Because B, oh, 'cause B doesn't get any, no. Right, right, right, because it's independent of the card. You actually said that when you explained the rules. Mm-hm. So its, its average of plus 10 and plus 10, which ought to be plus 10. It is in fact plus 10. Well done. Okay what about holder's here? Whew, alright, so that's a case where A holds so we go down those branches and we end up, we always end up in one of the blue circled states. Yep. And B sees half the time that leads to minus 40, half the time that leads to plus 30. So that's minus 10 divided by 2, which is minus 5 again? Yeah, that's exactly right. So that's correct, Michael. And that's pretty cool, isn't it? Yeah, I really like this game. I didn't think you could have anything that had sort of poker essence and be this tiny. So yeah, so we live in this really nice little structure here. So I have a question for you. You ready for the question? I'm trying to guess what it is, but sure. Okay, here's my question. What is the value of this game? I was thinking that you might ask that. So, can I, can I step through it, is that okay? Yeah, sure, go ahead. So a's choosing the first row or the second row. So if a chooses the first row, and then b chooses the column, then if it's the first row, then b is going to choose the first column. So a's going to get minus 5. Mm-hm. The same story's going to go through on the bottom row. If a chooses the bottom row, then b's going to choose the seer position which gets the minus 5. Right. So from this so, it seems that the value of the games minus 5. But now let's do the same thing on the b column. So if V, resigns, now a gets to choose resign or holder. And it gets a plus 10. And if B is a sire. Then A chooses between re signer and holder and gets plus 5. Yes. So then from this perspective, the value of the game is plus 5. So, so here's a case where it better not be that we could take a perfect information game and put it into a matrix and get this out, because this is something that can't, like, it doesn't, it doesn't fit your theorem, right? We can't get the value of it by doing minimax or maximin. Exactly, the problem here is that once you move the hidden information. Minimax is not necessarily, and in this case, definitely is not equal to maximin. So von Neumann. Fails. As we all see. Idiot. Yeah, what has, what has he ever done for us, anyway? His theorems and his computer architecture that rules the world. Anyway, so we seem to have a problem here. And the problem is that once we go to this hidden information, as I promise complexity enters in Michael, and now we can't do something very simple with the matrix, find a pure strategy that's going to work. It really is the case that A strategy depends upon what B will do and B strategy depends upon what A will do. And if you don't already know what that's going to be, you don't actually have a value for the game. And in some sense you can get every single one of these values. So, there's got to be some [INAUDIBLE]. I feel like, that's sort of what you'd expect in a game like this. Right. So, if, because of the way that it is. If you know that I'm always a resigner. That I'm always going to, what? [LAUGH] Oh, that when ever I have a red card, I'm going to resign, then. You know that if I don't resign, I have a black card, so you know that you should resign. Mm-hm. Yeah, so it's, it's, it's one of these things where if I am really consistent and never bluff, say, then you can take advantage of me, and vice versa. Like, if you always respond the same way to when I act a certain way, then I can manipulate that. So it's kind of like this game of this sort of mind game like I want you to not know that I know the thing that you don't know that I don't know. Right but the problem is that I know what you know and I know that you know what I know. And that I know that you know that I know that you know that I know that you know that you know what I know. Did you really think that I didn't know that? [LAUGH] And so you end up in this terrible situation. But. There was a key word that you used there, Michael, and it was consistent. And everything we've talked about so far, pure strategies, is exactly the same thing as talking about consistency. So, the way we're going to get around this is, we're going to stop being consistent, or at least, consistent in the same way. Okay? Yeah. And to cheat here, is that we are now going to introduce, instead of pure strategies. Let's see the opposite of pure. Is? Impure. Mixed, that's exactly right. Contaminated. No, so, rather than sticking with pure strategies, we're going to start using mixed strategies. So what's the difference between a pure strategy and a mixed strategy, Michael? Well, it's, it's simply this. A mixed strategy, simply implies, or means, some distribution, over strategies. So in the case of two strategies, like we have here, where you can either, A can be either a resigner or a holder, we're going to simply say that the mixed strategy for A is some value for P. Which is the probability of choosing to be a holder. So do you, you see what's going on here? So the only difference between a mixed strategy and a pure strategy. Is that for a mixed strategy, you choose some probability over all the different strategies that you might choose. So you decide that, you know, going into this I'm going to flip a coin. And half the time I'm going to be a resigner, and half the time I'm going to be a holder. Say. Or 30% of the time I'll be a resigner. And 70% of the time I'll be a holder. Okay? Yep. Where as with pure strategies, you always chose on or the other. So technically, it's the case that a pure strategy's also a mixed strategy where all the probability mass is on a single strategy. Makes sense. So in this case we're going to, in fact, choose P to represent the probability for A of choosing to be a holder rather than a resigner. And so P can be 0%, probability zero, or can be probability one, or any value in between. You with me on that? Yeah, that's neat. Okay, good. To make certain you understand this I'm going to give you a little quiz. Which I have up here on the screen. You ready? Oh, I see it. [LAUGH] It's like those, those very square boxes. Yes. I didn't even realize what, what, what this could be about. It certainly couldn't be a quiz because Charles has never drawn a straight box in his life. I had drew those Michael. It took me 17 hours. Oh man, you are, you're committed to this and I appreciate that. I am committed to this. Okay, so, given that we have a mixed strategy, and we have a probability P of A being a holder, here's my question for you. In a world where B is a resigner, okay? B is always going to choose to resign. What is A's expected profit? To make it easy for you, I copied the matrix over here in the upper right hand corner. Wait, wait, wait. If B is always a resigner. B is always a resigner. Then, what's, and what is A? A is going to choose to be a holder with probability P. Oh, so you want this to be a function of P. Maybe. If it's, if it's, okay, sure, maybe [LAUGH]. It could, well, I mean, yes. It's a function of P, it just might be a constant function that ignores P. It could be, in principle. Okay, now, after you figure that out, I want you to decide, in a world where B is the seer, B always chooses to see the card. What would A's expected profit be, in a world where A will choose to be a holder with probability p. Okay? Hm. Got it? And that's going to be, oh yeah, that could be different because it's a it's a different strategy, though you know seering. Yes Like like if you're a resigner you're a resigner, but if you're a seer then what you are doing is seering Mm-hm. That would be an anagram of resigner. How do you see these things? I don't know. They're just there. I just, they words just kind of mix themselves up. Alright anyway, I'm, I think I am ready to do the function. I'm, I'm ready to stop looking at the letters. Okay? And go! What's the answer? If B is the resigner, we don't really care about the other column anymore. Mhm. Then what's going to happen is A is mixing between resigning and holding. Yap. And probability P is a probability of being a holder, so whatever P, whenever that event happens it gets ten, and whenever one the opposite of it happens it gets minus five, so I want to say ten p plus one minus p five. Okay, is that your. Let me try that again. Is that your final answer? Ten p. Well I could simplify it. Okay. Well then say it to me again. I'll write it out here so it's easy for you to see. Okay and that is. My answer? That's fair. Do you want to simplify it? You don't have to. Sure, let me simplify it, so it's a minus minus p five and a ten p so that's fifteen P minus one. Minus what? One. Minus what? Five. Uhun, there you go. Thank you. That's correct. We would obviously accept either answer or any combination of those letters. That, [LAUGH] No, I think I might have to do this quiz over actually. No, not the one. I'm talking about either 15 p minus five or ten p minus one minus p times five. Or ten p plus one minus p times minus five. Or any other combination that we can get push car to actually Bothered to, you know. Check. Im, Implement or check. Okay. Well, that was pretty good. And this, of course, is exactly the expected profit. As you put it, P times A as a holder and P times or P percentage 1 minus P percentage A chooses to be a resigner. And so it's just the weighted average between those two values. So, Kent, let me just double check that. So, if P is 0, that means it never holds, it means it always resigns and it gets -5, so that's right and if P is 1, means it always holds, so it should get a +10. And 15 -5 is 10. So, boom! Yeah, it works. You used math there, very good. Okay, what about B? B. So the same story, except on the seer side. Mm-hm. So yeah, I might need that space again. So 5, oh I see, right, minus 5 times p. Mm-hm. Plus 5 times 1 minus p. Mm-hm. If we simplify that we get. There's a minus 5 and another minus 5. So we get minus 10 p. Mm-hm. Plus 1. Plus what? Plus 5. There you go. See you learned. Okay you want to check it? yeah. Oh, that's a good idea. So again if P is 1, then that means you're always which should be the minus 5 and if we put in a 1 there, we get 5 minus 10 is minus 5. And if P is 0, that means that we're always a resigner, and we should get a 5 for that. And yeah, so we zero out the negative 10 and we get the 5. Exactly. Now, it's not clear to me why we're playing this game. Oh, it is clear to me why we're playing this game, because we want to figure out something about. Strategy that is mixed. Right. So this is how well a mixed strategy does, but not against another mixed strategy. This is against two deterministic strategies. But is it, Michael? But is it? So I'm going to, oh, okay. I'm going to notice something here, which is that you, as you astutely pointed out earlier, we have two functions. Of P or to equations of P, and by the way, do you know what they are? They're lines. Sure, because it's just a, it's linear in P, so that's what linear means. Right, so what would happen you, do you think if we were to actually draw these lines? I think we'd have two lines. Yes, and what would those two lines look like? Let's take a look, shall we? Sure. Okay, so what I've done here Michael, is I have drawn both of these lines. So here's the line 15 p minus 5. This is a case when b is a resigner. This is my best attempt at drawing it to scale. You start out minus 5, as you point out, and you end up with plus 10. And this is the line where b is a seer. That's minus ten b plus 5, so you start out with plus 5. And you end at minus 5. Okay? Cool. So, what do you notice about these two lines? They make an x? So they intersect. They do intersect. Can you tell where they intersect? How would I solve that? You'd have a quiz. Okay Michael, so here's a quiz. Where do the two lines intersect? Think you know how to figure it out? Yep. Okay. Then go. Okay Michael, what's the answer? I don't know, but I would, here's how I'd get it. I'd set the equations of the two lines equal and find the p where the, where those values are equal. Oh, okay. So here let's do that. And I feel like that's like 25 p on one side and ten on the other Okay. So like 10 over 25, which is like 2 over 5, which is, like, 2 over 5. Yes, which is also 0.4. So 0.4, that's exactly right, and it's, you should do it exactly the way you said. Set the two equations equal to one another, do simple algebra, and. So what would have happened if p ended up being, like, not a probability? Then they wouldn't have crossed inside. Good point. So by the way, here's a question for you. We can make it a quiz but I'm not going to make it a quiz. What is the value of the game at p equals 0.4? So I just plug it into those equations, and so negative 10 times 0.4 is like, negative 4 plus 5, is 1. So I'm going to say 1. Mm-hm. So the value here is in fact $0.01. And if you put it in the other equation you get the same thing. Yes, right, because they're equal there. That's actually the answer to the entire kit and kaboodle. If. Sorry, where were the kittens? If A chooses a mixed strategy, where with probability 0.4 he chooses to be a holder. Notice that it doesn't matter whether B is a resigner or B is a seer. You will end up here and there will be a value of plus 1 penny to A. On average. The expected value is. Yeah. plus 1. Neat! Now you might ask yourself, well what if B decides to be a smarty pants and also do a mixed strategy? What do you think would happen in that case? So of course it changes something, but it doesn't change the value of the game, because B, if B is a resigner, A is getting one on average. If B is a seer then A is getting one on average, and an, any average, any convex average of one and one is going to give us one. That's exactly right. So in fact if you think about it, if B tries to do a mixed strategy between these two lines. It's going to have to be for every single point between the two lines, somewhere the average is going to have to be somewhere in between there. No matter how you weight that average. So it's like a bow tie is this space of possible payoffs. Right and since for any let's if I just, If B decided to pick some, you know, some values such that it's in this region of the space. And it's going to end up somewhere, say between here that's fine, or here that's fine, or here that's fine, or here that's fine, or here that's fine. More importantly, right here it's going to have to be between the two lines. Where those two lines cross. So, no matter what mixed strategy B chooses. On average we will end up here, so the value of this game is the expected value of this game and it is, plus 1 for A. Okay so hang on why is that not there are other values that can be obtained. There is a minus 5 a plus 5, true. Why is it plus 1? Well its plus 1 here on average, the expected value of this game. Is plus 1. So you say that the strategy for A is to choose .4. Mm-hm. But why? Like, why is that, of all the different values. What [INAUDIBLE], just because the lines intersect. I don't understand what makes that a good idea for A. Like, it's not like it gets any additional payoff for having things intersect. Well, if a is going to choose a mixed strategy, and b is going to choose a mixed strategy. This is the mixed strategy for a, that guarantees an expected value of plus 1. Meanwhile, let's imagine this is the way you're going to set it up. So we've been kind of talking in general that a's going to choose the strategy; b's going to choose a strategy, and they're going to go. And that, because we know everyone's rational, you already know what strategy b's going to pick, and b already knows what strategy a's going to pick Right, because we made this assumption about everyone is trying to maximize their own utility, there own reward. So in a mixed strategy, if you know you're going to go for a mixed strategy, you have exactly the same situation. B can figure out well what is it that A should choose and A can figure out what is it that B should choose. So notice that in this particular version of the game, the way it's setup. Even if A announced beforehand to B. Listen I am going to choose to be a holder with probability 0.4. It doesn't matter what B chooses the expected value is this +1. Right? Yeah. But imagine if A said I am going to pick, I'm going to choose to be a holder with probability 1. Well then, what should b do? B should choose to seer. Right. It's exactly the situation we were in before. Okay, but here's the thing I'm having trouble wrapping my head around. So, it's not special that it's an intersection, what's special maybe, is it that, you know, if b is always. Okay; taking what you said before, that a is going to announce a strategy. So for anything that A can announce B is going to presumably do what's best for B which is to just minimize right? Mm-hm. So if you look at that triangle at the bottom. Mm-hm. If you think about that as those lines that V, upside down V shape. Yeah exactly that thing. If you think about that as being the payoffs, the payoff function for A, as a function of the probability P that it chooses to hold. Mm-hm. Then we've chosen the maximum. Right. Right, we're trying to find the, the probability for which A gets the highest expected value. And it's at that, it's at that peak there. Hm-hm. But notice something, Michael. For any case where the two lines cross, you're going to have a function of this form. Well, let's be sure of that, because that's, I wasn't seeing, I was thinking that that was not true. because basically in this case, we have a, a line of positive slope and a line of negative slope. Mm-hm. Right? So, what if we, we can have an intersection between two lines of negative slope. huh. So again, by doing the same exercise you were doing before, where you draw the, you sort of take the minimum of the two lines at all points. Yeah. You'll end up with this. Where do you pick it then? The far left, p equals 0. Yeah. But not the intersection in particular. No, that's true. Okay, alright, I just, wasn't quite getting that. But so, the intersection is special, Because in some cases that is where the max is. It can only I guess the max can only be in those three places, right, it could be far left far right or the intersection. There' s no other way to have a maximum of lines. Right. And b by the way there's another case like this where they never actually cross. Mm. Or like this where they never actually cross. Got it. Right, but in either case, right, it's always going to be those three points. It's going to be the extreme or where they cross. If they happen to never cross, then that point goes away and you just pick the maximum. So what you could, so, in fact, if you wanted to think of an algorithm to choose the right thing to do for A, you basically plot the lines. You take the min at every point and you find the maximum point. Oh I see, so you could just kind of discretized it almost. Yeah, and you're done. Seem reasonable? Well discretized seems problematic but like I get that we're trying to find the probability so that we maximize. Oh wait we maximize the minimum of the two other things. So it's like maximin again. Yeah. Except here, so, in fact, it is exactly min and max or max and min, except, oh, I guess, if you're thinking from A's point of view. It's min and max or max and min but in this case, there's this other parameter, which is the probability. Which is how you determine how you're doing the min and max. Got it, right. In this bigger space, it's min and max. I see, yeah, yeah, yeah, that makes sense. What about so, why is A the one who needs to be random? Why isn't B the one who needs to be random? It's, you end up with the same, you end up in the same place. Because again, remember, you're, you're doing this kind of optimal notion, where underneath all of that is this belief that both people are going to be rational. So the only reason to do this, to take the, you know, maximum minimum, is if you believe that B is always going to try to minimize what you do. But B's in exactly the same situation. Right, from B's point of view, That's what's happening. Huh. It's the same equations? Yeah, it just it gets turned around. That's actually an interesting exercise. Maybe that's the next homework assignment. Okay. So, did that all make sense to you? [LAUGH] [CROSSTALK] It looks a little green and scribbley, but yeah. Sure. That's cool. That's how you know when you're done, Michael. When it's all green and scribbley. [LAUGH] Does this generalize to more than two options? It seems like it's going to get messy fast. Yes, it does. It generalizes to more than two options. Effectively now, you're just doing a max over n things, instead of two things. And you have to search for, there's possibly way more intersections to worry about. Yeah, but all you care about is the minimum. Think of it as you're always drawing the minimum function. Okay Michael, so, feels like we've relaxed almost everything we could relax except for one thing. We're now going to look at two player non zero sum. None possibly none deterministic games of hidden information. Cool. And that's going to turn out to be messy, but get us to the place where I've secretly been trying to get us all along. So, let me describe a game for you. Very carefully, and let's see where it leads us. Okay, here's the game, you've got two people. These two people are criminals. Oh no! Are they smooth criminals? One of them is. [LAUGH] [MUSIC] [LAUGH] Oh, you're terrible. Okay, so we have two people. They're criminals. Okay, and unfortunately they've both been captured by the cops. I have actually no idea how to draw that, but lets just try to draw that like this. The cops come along, and capture them both because they are suspected in a particular robbery. Okay? Hm. And they take them and put them both in jail. So those are jail bars. [LAUGH] Okay? But they don't just put them in jails. They put them in two separate jails. Oh no. Okay. And one cop goes to one criminal and says listen, here's the deal. We know you did it. Okay? We know you did. And your friend over there, he's currently singing, singing like a bird. And he, is going to pin it all on you. So this is your last chance to give us a little help and admit that you two did it. Or, that the other guy did it. You admit that the other guy does it, you say it was his fault, then we'll cut you a deal. Now to make it worse, the cop tells him that there's another cop over there. [LAUGH] Who's talking to the smooth criminal, and is offering him the same deal. Hm. And whoever, goes first, whoever pins it on the other guy first, gets to walk. Okay? Hm. So, if I can get the curly-head guy to defect, okay? Hm. Then, I am going to let him walk and he will spend zero months in jail. Okay. Okay. On the other hand, if the other guy defects, then he's going to get to spend zero time in jail. And since we've got all that we need. We got a confession. To get this guy to go to jail. He's going to go to jail for nine months. Okay? Negative nine months? Yeah. Oh, that's a cost in months. I see. Uh-huh. [CROSSTALK] Yeah that's a cost in months. Okay? So if, curly-head guy defects before the other guy does, then zero. He walks. He pays no cost, other than the cost he's already paid for a life of crime. [LAUGH] Now if he refuses to drop a dime on the other guy, but the smooth criminal decides to defect he's going to lose nine months. That make sense? And the other guy's going to walk. So he's getting the same deal, they're both getting the same deal. You got it? Yeah. All right, now. It's a little more complicated than that, all right? But I, I hope that you're getting all, keeping all of this in your head. It's a little more complicated than that. There's actually four choices here, not just two. It's not just a case of. I defect or the other guy defects. I defect, the other guy doesn't. I don't defect, the other guy does. Okay? You could both refuse to drop a dime on the other. You could cooperate, or you could both rat out the other. At the same exact moment? At the same exact moment. So, there's a big thick wall here. Curly guy doesn't know what smooth guy is doing, and smooth guy doesn't know what curly guy is doing. Right. So the question is, if you have a choice between either defecting or. Which means blaming the other guy? Which means blaming the other guy you defecting from your friendship. Oh. Or you can cooperate, that is be true to your friendship. I can, we both people can defect, both people can cooperate, one person can defect and the other person cooperate, so there's actually four different options there. I fee, I feel a matrix coming on. Yeah, I think there's a matrix coming on. So let's draw the matrix, because I think that makes it easier to see. But you have the background here, right? I think so, yeah. The key piece here is that each person has a choice of either defecting from their friendship or trying to cooperate. Keeping their mouth shut, and, they don't know what the other person is doing. So, what are all the costs here? What are the worst case things? Let's just draw it all out as a matrix. So let's just call the smooth guy A, because that's what we've been doing all along. And he can either choose to cooperate, or he can choose to defect. Now by the way, I'm saying cooperate and defect here because they're terms of art, not because these are the words I would have chosen. Okay? B can do the same thing. B can choose to cooperate or B can choose to defect. Now I've set up the game in a particular way here. The, the, the cops have set up the game in a particular way here. If B defects but A cooperates, then B gets to walk and A has to pay the price, nine months in jail. So, I'm going to put into this cell minus 9, 0. So, this means this is the value of this set of strategies for A, and this is the value for B. So the first number. In the pair is what A gets and the second number's what B gets. Okay? Got it. You with me? Yep. Okay, now notice we have to do this now because it's no longer zero sum. It's not going to always add up to a constant. Oh. Well, it is, so far it's a constant negative 9. That's right, oh and in fact it looks this way as well because I have a symmetric deal here. So if A defects and B cooperates, A gets to walk and B goes to jail for nine months. Okay? Yep. So right now, you're right, it's looking like a zero sum game, but it isn't. Because what happens if both A and B drop a dime on each other? Well, they both confessed. So it's a little good, it's a little bit better than having one of them, only one of them, confess. The deal that the DA is willing to give them is that both of them will spend six months in jail. Now these are just, these are just, the, the numbers that are a part of the game. This is, I'm not computing this from anywhere. This is just the way the, the cops have set this up. And I'm going to, now what's an interesting question here is what happens if both A and B keep their mouth shut? They both choose to cooperate? It turns out that it's not a perfect world because they were caught with a bunch of guns they didn't have permits for. So if neither one of them admits to robbing the bank, they're still going to do a little bit of time. But in this case, it's a small weapon's charge and so each only spends a month in jail. I see, so now it's definitely not zero sum. Mm-hm. Because we have a negative 2 there, a negative 12 there, a negative 9 and a negative 9. Right. Okay. Okay. So, you got the game, you understand it? Yeah, I think so. It's very simple. Now, just looking at this, what's the best possible outcome for the duo? So if they cooperate with each other, then there's you know, one month later, they're back on the streets, back to their criminal activities. Or they're reformed. You never know. Sure. Back to their choice of whether to have criminal activities. The, the mutual defection one, that, where they both defect, it's either, well, 12 months or six months, depending on how you think about it, but they don't do nearly as well. And then the defect and cooperate, it seems like there's a lot of incarceration that will happen. Mm-hm. So, it feels like the best for the, for the two of them is to mutual cooperate. Cooperate, cooperate. Yeah. And that makes sense. This is sort of what you want to happen. Both of them keep their mouth shut. They do a little bit of time, but on average, they do pretty well. Right? Yep. So my question to you is, is that going to happen? Sure, why wouldn't it happen? Well, you tell me. If I know that you're going to cooperate. Let's say that I'm A, okay? And you're B, and I know that you're going to cooperate. What should I do? You should cooperate. Should I? You've chosen, you've chosen this column. All right? I see. Well, if you think of it that way then it's not a joint choice, but it's actually an individual choosing. Then you're better off defecting because then you get off scot-free. Yep. Then, but I go to jail for nine months. I could have a whole baby in that time. Usually takes closer to ten but sure. But you go to to jail for nine months, I don't. So if you are just that cold. I guess because you're a criminal. It doesn't matter. Remember, the matrix is everything. The value of doing this to me is 0 versus minus 1. Alright, that's, that's cold, man. I agree, but it's what the numbers tell you. What if it wasn't criminal? What if it was just, you know, the amount of money that I was going to win in a, in a mini poker game? Yeah, but it's a different kind of mini poker game, because we're both, because. [LAUGH] It's like you beat me. But, by beating me that way, you like, kill, nearly kill me. So? Are you saying that when? What do you mean so? Are you saying you always let me win whenever we play poker? No. Okay then. Because you're cold, is that what you're saying? No. Wait, what? Exactly, right. So the point, Michael, is that if I know you're going to cooperate, you're going to choose this column, then I should defect. Okay, alright, so, fine. So now I'm going to jail for nine months. Mm-hm, if you choose to cooperate. Aha. If, if is good. So what you're saying is, I could drop a dime on you. [LAUGH] You could. So, you could choose to defect, and if I knew you were going to choose to defect, what would I do? Well, you already, you already showed your colors, man. You're, you're defecting, so I'm just switching, I'm just saving myself three months by, by ratting you out. Yeah, so we would end up here. By the way, you know that this game is symmetric, right? no. You defected first. [LAUGH] No, here's the thing. Since the only thing I care about is maximizing my own reward, my own value. I'm going to do the thing that makes sense for me to do in this case which is if you cooperate, defect. If you defect, defect. But you would do the same thing because your whole goal here is to maximize your value. Yeah, well you don't know me like that. Yes I do because it's everything that's here in the matrix. This is the wonderful thing about game theory. All the stuff that you're concerned about is all inside the matrix. Remember the rules of the game don't matter. There's no prisoners here. There are no criminals. There's just, I get a dollar, I lose a dollar, or I lose zero dollars. I lose $9 or I lose $6. Or, for that matter, cents. I lose one penny or I lose no pennies. I lose nine pennies or I lose six pennies. It doesn't matter. This is the value of these particular strategies. And I'm going to want to defect if you cooperate. You'll notice if you defect, I'm also going to want to defect. In fact, if you look closely at this matrix, you should notice something, Michael. From A's point of view, when does it make sense for me to cooperate versus defect? You mean if you have a, some kind of probabilistic policy over cooperate and defect? No, no, not even that. Just in general, is there ever a time as A that I would rather cooperate than defect or vice versa? Oh, I see. So if I know you're going to cooperate, I should defect to, to save myself a month. But if I know you're going to defect, I should defect to save myself three months. Either way, I'm coming out ahead. Right. So in fact, let's take a look at this. If I look at the value for me as A, between cooperating and defecting in this first column. It's minus one versus zero, right? Which number is bigger? [LAUGH], zero. Right. If I look at it in this column, it's minus 9 or minus 6. Which number's bigger? Negative six. Right. In both cases, defecting is better than cooperating. So in fact this choice, this strategy, dominates the other. In other words it is always better for me to defect than cooperate. So I will never cooperate, ever. Because it's always better for me to defect. So fine. So I'll never cooperate ever. That'll show you. By exactly the same argument, minus one versus zero, minus nine versus minus six. Defecting is always better. B will never cooperate. What's the only thing that's left? Pain. Pain. This is called the Prisoner's Dilemma. Huh. You said it was simple, but it seems kind of evil. I didn't say it wasn't evil. Those things aren't opposite of one another. It's not like perfect versus hidden. Or pure versus mixed. It's not easy versus evil [LAUGH]. Evil's often actually easier. It's easy and evil. So we're in a little depressing place here, Michael. You claimed in the beginning, and I agreed with you, that this is sort of the best option, you want everyone to cooperate, because that's sort of what's best for the group. But because defecting dominates cooperating both for A and B you're going to end up here. At least if you reason about it that way. Yeah, I see that. Hence prisoner's dilemma. The only way to break this is if somehow we could communicate and collude with one another. So that we could guarantee that we would both choose to cooperate at the same time. So that wall? What about that wall? What if you put a like a Skype connection or a Google hangout? Well, if you were forced to say what you were going to say at the same time, or somehow be able to punish someone maybe for, you know not doing the right thing, then you might be able to make a different decision. But for this very simple version where I'm going to do it once, even if I could hear what you had to say, if one of us, whichever one of us went first, the second one would always be able to take advantage of that. I do find this kind of depressing. Yes, it's a dilemma. It's a true dilemma. So, this brings us to a more general sort of strategy. This whole notion of strict dominance, works in this case. But, you could imagine very complicated large matrces where it may not work. But it turns out there's a generalization of this notion of dominance. That works remarkably well. And that's what people tend to use to try to solve these kinds of games, to find out what the true value of a game is. So letme describe that for you, ok? Okay. Alright Michael. I'm going to define a new concept for you. It is called the NASH EQUILIBRIUM. Nice. Okay, here's the set up. You have n players. So we move beyond simply two players. You have n players. And each player has strategies that it can chose from. So here I'm referring to them as, S1, these are all the strategies for player one. As two are all the strategies for player two up to S N all the players for strategy N. Got it? Yep. Okay so each of these are set, so im going to say that the particular strategies, S1 star S2 star S3 star, so on and so forth as N star that is a strategy that each of the N players has chosen. Is in a Nash equilibrium if and only if for each one of those strategies chosen by the n players it is the strategy that maximizes the utility for that particular player, it is the strategy that maximize the utility for that particular player Given all the other strategies that were chosen. Now I actually find that difficult to kind of work through, so let me try to say it a different way. Given that you have a set of strategies as one star, as two star, as three star, as n star. We know that they are in Nash equilibrium, if and only if. If you randomly chose one of the players, and gave them the chance to switch their strategy, they would have no reason to do it. Interesting, okay. So does that make sense? Yeah. Right. So an equilibrium right, just in general, the word, the word sort of makes sense, right, an equilibrium is at a place where everything is balanced. And in some sense, there's no reason for anything to move because they're in balance. So we set that a set of strategies are a Nash Equilibrium if no one person has any reason to change their strategy in a world where everyone else's strategy remains the same. Then you're, then you're in equilibrium, and in particular you're in a Nash equilibrium. Nash equilibrium. Okay good. Do you know who Nash is? Ogden Nash was a poet. Not that Nash. I think there was a TV series with a Nash in it. Yes there was. But you're thinking of John Nash. That's right The Nobel Prize winning person who was the, was featured in the movie A Beautiful Mind. Yep, that's exactly right. And it was, in fact, a Beautiful Equilibrium. Okay, so, there's the notion of a Nash Equilibrium. It was admitted by John Nash, if you believe the movie, in order to pick up women. Which I, you know, you know I never, I've never seen this movie. Oh really? Oh, I watched it and it was, it was surprising how unhelpful it was in explaining what a Nash Equilibrium was. [LAUGH] I'm not surprised. I hope this is helpful, though. So, it really is a kind of difficult concept to completely wrap your, your mind around, but what it really boils down to is, listen, if we all picked a bunch of strategies and we knew that one other person, one of us, we don't know who before hand, but we know that one of us Would have the opportunity to change their strategy after they see what everyone else's strategy is. We'd be in a nash equilibrium only if that person, whoever that person is, has no reason to change their strategy. Gotcha. So, so specifically you've made a distinction between strategies that were pure and strategies that were mixed. Right. And I guess I don't see in this case which kind we're talking about. Right. So, in this particular case, we're talking about. Pure strategies; however, exactly this wording works with mixed strategies. Oh, I see. So you could have a pure Nash equilibrium or a mixed Nash equilibrium. Right. And so, you could, each one of these, instead of choosing a particular strategy and saying they're a Nash equilibrium, you could talk about a probability dristribution over each of these sets of strategies and say those are Nash equilibrium if. No one would want to change their probability distribution. Got it. So this works for both pure and mixed. Alright, so you think you understand this? [LAUGH] sure. Good, we will test that. Okay, Michael. So you think you know everything, so here's a quiz. So, here are two matrices, matrix prisoner's dilemma and matrix bunch of numbers that look vaguely symmetric but aren't quite. So here's what I'm going to ask you do do. In each of these cases find the Nash equilibrium. Rut-roh. So do, you told me what it was but you didn't tell me how to find them so that seems kind of rude. It was implicit. It's intuitively obvious even to the most casual observer. Oh, well then, okay. So, and just to remind me now, each row and column is a choice for one of the players or the other player. Yeah. And the first number in the pair is the row player, or A's pay off and the second number is B's pay off. Yes. And we might need probability distributions, or we might not, depending on what it takes to be a Nash equilibrium. Right. okay, I don't [LAUGH] I'm not, it's not like I see the answer yet but it, but I either will be able to find it, or I won't. At least I understand the question. Okay. And by the way, just to make it easier for you there are pure Nash equilibria here, okay? Hm. Alright, so you ready? Yep. Just circle the one or underline it or whatever it is Pushcar does to make it so that you can answer this quiz. You ready? Totally. Go. Alright Michael, you got the answer? Yea, I'm ready to try and figure it out. Alright let's go. Let's try the first one. I feel like the first one's going to go rather well. So we need a pair of strategies so that no player is. Motivated to switch. Mm-hm. And you told us they were pure strategies, so we actually have a nice algorithm for doing this, which is we could just check one of them with the definition. Mm-hm. But, but in the case of prisoner's dilemma, I think a natural place to start would be that minus 6, minus 6. Mm-hm. So let's say that A chooses the second row and B chooses the second column, let's see if that's a Nash Equilibrium. So both players need to be happy. So if, would A be happier switching? If A switched, it would be getting minus 9, which is worse. Mm-hm. So A is happy, where A is. And if B switches, B would be getting minus 9, so boom, Nash Equilibrium. Done. Now I didn't verify that other ones weren't a Nash Equilibrium, but you didn't say. Find all of the Nash Equilibrium. You just said find, well you did say find the. So you kind of implied that there's just one. Right. But actually that's true, but you don't have to check this because we already went through an exercise where we, where we knew the answer was minus 6, minus 6. And the way we did that was we noticed that for A defecting the second row is always better than this. Right? And then in particular this row strictly dominates this row. Right. Which implies that if I picked anything on this row I would rather move to the other row. And you can see it. Minus 1 0 is better. Minus 9, minus 6 is better. That's what it means to be strictly dominated. So, I'd never pick this row anyway. And this same argument for B for this column. So, you'll notice that by getting rid of the things that are strictly dominated, the only thing we're left with is this. And, it turns out, in fact, to be a Nash Equilibrium. So, this is correct. So, you just told me how to do my job, which makes me little sad. Well, in this instance. Because I already had the answer to this. But maybe, maybe you were signalling to me how I might attack this next problem. Maybe. Maybe. So, A is choosing a row and gets the first number. So, is there a row, that, were they dominates all the other rows? It doesn't seem that way. Mh-hm But maybe, maybe, oh I could start with B, maybe B, there's a column that dominates all the other columns. But no, it looks like it's totally symmetrical. Yep, so strictly dominated doesn't necessarily help here, and by necessarily I mean doesn't. So that was kind of mean. Thank you. All right, so oh, but we have something else we could do. Yes. So, there is a, the largest number that anybody can get is 6. Mm-hm. And there's a play where both of them can get the 6. Yeah. So there's no way they're going to want to switch away from that, because everyone's getting there kind of maximum out of reward. So A bottom row, B right column, gets us Nash Equilibrium. And that is in fact correct, and you can see it because from here I would always, it would always be worse for me, and it would always worse for me. So, these are in fact the Nash Equilibrium, for these two problems. Cool. They've seemed easier than I was expecting. Mm-hm. So here are three, fundamental theorems that come out of all of this work on Nash equilibrium. They're all in the screen in front of you. I'll read them for you in case you can't read my handwriting or if there's some typo you discover that forces me to erase some words and then write some new ones. Okay, are we ready? Alright, the first one is, in the n-player pure strategy game, if elimination of all strictly nominated strategies, eliminates all but one combination of strategies, then that combination is in fact the unique Nash equilibrium. So that's, that's what happened in prisoner's dilemma, we got rid of all but one option and that option had to be the unique Nash equilibrium. You say, eliminate all of them, do we, is it possible that things that we couldn't eliminate in one round, we could eliminate in the next round? Yeah it's possible, so in fact, this is done in an iterated fashion. You get rid of whatever you can eliminate and pretend they were never there. Because the truth is, no one would ever choose them. And then with what you have left, you just do it again. So it is possible. Although not in any examples that we did. Okay. The second one is and this, both of these I think are kind of Obvious. They sort of make sense anyway. At least to me. That any nash equilibrium will survive the iterated elimination of strictly dominated strategies. In other words if you get rid of things that are strictly dominated you will not accidentally get rid of nash equilibria in the process. And that makes sense because, if they're strictly dominated then if you ever end up there, you would want to leave. And therefore can't be a Nash. And therefore can't be a Nash equilibrium, that's right. So, those two things sort of make sense I think. The last one is true but isn't at least painfully obvious, at least not to me, and that is. If n is finite, that is you have a finite number of players, and for each of the set of strategies, that set of strategies is also finite, in other words you're still in a finite game, then there exists at least one Nash equilibrium which might involve mixed strategies. So you're saying there's always an Nash equilibrium? Yes, possibly mixed, for any finite gain. So, when you said I relaxed everything, I didn't actually relax the requirement that it be a finite gain. Fair enough. But you stopped writing finite, so it's sort of the same thing. Yeah, I agree. Because why would you play an infinite game? I don't know. Maybe you got a lot of time on you hands. Mm. That would be like an infinite jest. [LAUGH] Okay. So those are the, the main, results here. So, what did we learn? What we've learned is that with interesting games. We end up in these weird situations where we have to figure out how to solve the game and one really nice concept, is this notion of an equilibrium and in particular the notion of a Nash equilibrium. Cool. And there you go. Let me just wrap up by doing a couple quick things. Earlier on when we were talking, you made a kind of offhand comment about, or maybe I made an offhand comment, about the fact that because we're doing this little prisoners dilemma thing, we have this sort of wall here, we're going to end up in this little bad place. Right? With the -6, -6. And there was this kind of notion that what if we could hear each other, would that make a difference? And the answer was, no not really, because who ever goes first is going to lose. And if you try to get people to go at the same time, well in some sense, that's the same thing as having a wall and you can't hear each other if you have to go at exactly the same time. Right, does that make sense? I didn't quite understand. Why does the first, person who goes first lose? Because if I find out whether you're going to cooperate or defect, then I will just then do the thing that makes sense. So if you do anything but defect, I will defect, and then I will win and you will lose. So whoever goes first runs the risk of the other person screwing them over. So. I see. Unless, unless that person defects. Right, but if that person defects, the other person's going to defect. Right, so, but you don't lose any more than you would have. That's true, and, but you still end up in this kind of unfortunate place. So, one question you might have there is, yeah, but that's just because you can only do this once. But what if you could do this twice? So, lets imagine were, going to be in this situation today and then were going to be in this situation tomorrow because, you know, you keep leading me astray. So, whatever happens today I can use as information, for what I might do tomorrow. So, if I decide to go ahead and cooperate this time. And you defect, and I know you're the kind of person who defects, and so when we're in this situation again, I am definitely going to defect. So maybe in that situation, it's in your best interest to go ahead and cooperate because, then we can keep coopoerating together, and in the end it sort of works out. So maybe this notion of not just playing prisoners' dilemma once, But playing prisoner's dilemma twice or three times, or four times, or five times, might come to a different result. Yeah, that seems plausible. I mean, I could imagine saying to you though this channel in the wall, something that says, you know, cooperate with me or, yeah, right, or I will, I will stop cooperating with you. And so, you get more reward if you, if you cooperate with me. Right. So, my question to you is, what does happen exactly in this set? Where I have two versions, two consecutive games of prisoner's dilemma to play. What am I going to do? Ok, well now I mean, we could turn this into a game, right? I mean, that's what you were teaching us how to do. Yeah. So you, so now the game has four, well, I don't know. So one way to do it is to say A has a choice to make on the first step, and [COUGH] that could be cooperate or defect, and then the second step could be cooperate or defect, so there's four possibilities. Right. B has the same four possibilities. But maybe we can actually have A be responsive to B. So A has two things to do on the first step and then two things to do on the second step for each thing that B did on the first step, for a total of eight combinations. Right. You could do that. [CROSSTALK] So if we made an eight by eight matrix, we should be able to solve it. Right. So, that's pretty easy to do. You just kind of draw an eight by eight matrix in, you know, that many dimensions. And it, you know, you end up right here. I wasn't thinking about that many dimensions. I just thought, like the normal kind of matrix. But, okay. Oh, I see, I see, I see. I mean its eight by eight, so that's kind of a pain. Right. So, its 64 cells and, I guess, I guess we don't want to fill that out. Yeah, but you know. We could fill that out. But, I'm going to help you out here by pointing out You know what? It's not going to make a difference. Oh. So, here. Let's see if I can walk you through why it's not going to make a difference. Let's imagine I have 20 of these games. Okay? Sure. So I'm going to play these games 20 times in a row. Now what you've been doing is you've been going forward. You've said, well, if I can get us the right thing or if I can basically threaten you and say. If you screw me over this time, I'll screw you over from now on. Then maybe I can get you to do the right thing and vice versa, since it's a symmetric game. And then we'll end up doing the right thing all along. And that makes a lot of sense, right? Yeah. If you're going forward. But what if you're going backward, Michael? Let's imagine we are doing [CROSSTALK]. What is amazing to me is that is exactly backwards what you just said. So, let's imagine we are doing these 20 dot dot dot dot, and now I'm on the 20th game, okay? What's going to happen on the 20th game? Well, I mean, I guess I could have built up trust in you from the previous games and think that you're going to cooperate with me, because we've been cooperating together. And then that would be the perfect time to drop a dime on you. Exactly. So, if this is the 20th game and we're in this situation, remember whatever value we've been adding up along the way, that's sunk cost. Right. The only thing that's left is the final game. And the final game looks like this, which means we're going to end up here. But guess what, Michael? The final one is determined. So since we already know the outcome of the final game, the only one that we, the next one that we can look at is the 19th game. Well, we already know what the outcome of the final game is. So this is effectively the last game. Oh. And what is the outcome of that going to be? It's going to be this and backwards, backwards, backwards, backwards proof by induction because the only that proves computer scientists know how to do is proof by induction. It turns out that we will always defect For what it's worth, those are the only proofs worth doing. But, okay. That's true. It's a fine point. Truth by induction and perhaps by conduction [LAUGH]. So, there you go Michael, even if I can play multiple, multiple games to try to build up trust, the truth is the Nash equilibrium, if I filled out that eight by eight matrix you wanted me to fill out. I would still end up in the same place where I would defect both times. Certainly if, yeah okay, so if you're going to be a jerk then, then I have to be a jerk, but yeah, I guess that's right. That's, that seems pretty horrible. It is. And by the way this is not just something I'm making up. This is another theorem that comes out of Nash equilibrium. That if you have an n repeated game then the solution is n repeated, Nash equilibrium. So whatever the Nash equilibrium is for the first game, the one version of the game, is the repeated Nash equilibrium for all versions of that game. Okay, wait a, hang on, hang on. So I. Okay, I buy that. Mm-hm. But couldn't you, well so, in a game that has more than one Nash equilibrium, couldn't we like, al, alternate? We could, and in fact you'll notice that in so far, I have not talked about the case where you have multiple Nash equilibrium. Because in fact, that's it's own problem. If I have one Nash equilibrium, then we know what's going to happen. If I have two Nash equilibria, which one do we choose? Now, what's important to know here, is that if we have two Nash Equilibria then that means that if you're in any one of them, you won't leave it. So it's not like you'll move from one Nash Equilibrium to another, sort of in the general case. Because the fact that it's an equilibrium means that if everyone else is fixed except one person, that person won't choose to move. But. I haven't said anything about how you would choose among multiple Nash equilibria. And that's beyond the scope of this class. Okay. But it is an active researcher and in fact some of my own students have done some work in thinking about. What it would mean to choose and actually the answer always boils down to, let's not worry about that. [LAUGH] All right. But I'm still kind of disturbed by this, right? So it seems like it's sort of saying, is if we knew the world was going to end tomorrow then we might as well be greedy bastards. And, in fact, if we know that the world is going to end at any particular time, which of course it will. We might as well be greedy bastards. That's right. So, be greedy bastards. Oh, no! Or at least, or at least be you know, Nash bastards. [LAUGH] We'll, the Nash is kind of greedy right, it's like I'm always taking an action that's always best for me. Yes, you are. No. Every single thing we've talked about today. Has always assumed that you're always doing what's best for you. The fact that it might also be best for someone else is neither here nor there, you're always doing what's best for you. Right, I get, I get that, but it's, it sounds like this argument is saying that we might as well be like that, like all the time and never really form, like never, never self-sacrifice even a little bit for greater advantage even to yourself. Well, that's not true. This, it's just all hidden in the utilities, once you've gone through all the what I'm going to do today, what I'm going to do tomorrow, you add it all up, whatever the right strategy is to take. That's the strategy that you take because it's already accounted for all the self-sacrfice you might do that then leads to later good deeds or later good outcome. So the wonderful thing about all of this, is that this matrix is everything you need to know. It captures the future, past and everything else, so everything you need to know is right here. And it's all been reduced to a bunch of pairs of numbers. But you did say something kind of interesting though, Michael, which I think is worth pointing out, which is that everything you say requires knowing when, at least in this sort of iterated version, requires knowing when the world is going to end. So an interesting question to ask would be, what would happen if I knew the world was going to end but I didn't know when. Would that change my behavior. It, it doesn't seem like that should make any difference because it's still going to end. If not today then tomorrow. Or if not tomorrow then the next day. True. But I bet yeah it does make a difference. I'm willing to go and think about that. Okay, why don't you go think about it and let me know. Alright. I'll, I'll tell you about it in the next lecture then. Okay, I like that. Excellent. Okay, cool. So, let's move on. [LAUGH] Okay, fair enough. All right, Michael. So, I think that brings us to the end of what I wanted to talk about anyway. So, can you help me remember what it is that we've learned today? In particular, what you've learned today? Sure. So, I guess the first think I learned is that game theory can make you depressed. And, in fact, in particular that my friend Charles, given the opportunity. Would totally drop a dime on me just to save a month of incarceration. Yeah. Wait, no, no, no, I don't think you summarized that correctly. Game theory is not depressed. It's depressing. Oh, yeah. That's a good point, that's a good point. And Michael is not cruel. He is the victim of cruelty. I don't think so. Because you want to know what the secret here is, Michael? You've got a little matrix of numbers. Those number capture what's going on. The truth, Michael, is that, if we were in Prisoner's Dilemma. I would cooperate with you because my utility is not simply the number of months that I would spend in jail. But it's the number of months you also would spend in jail. Oo. Interesting. So if I, the best way to beat Prisoner's Dilemma, is to change the numbers. [LAUGH] I see. It's like the Kobayashi Maru of game theory. Exactly. So there's an interesting question for you right there, Michael. If I had prisoner's dilemma here, here, let's write it out so you can remember. If I had prisoner's dilemma here, we already know that we're going to end up here, because that's what the numbers tell us to do. But what we'd have to do is change the game. So how would you change the game in prisoner's dilemma? I see. So, if, if we're thinking about it in particular in terms of I care about how long you spend in jail. Maybe not as much as I care about how much I spend in jail. Like maybe half as much. Mm-hm. Then, the payments shift, right? Right. So, now we have like minus1 and a half, minus 1 and a half in the upper left hand corner. Mm-hm. Minus 9 comma minus 4.5, minus 4.5, minus 9 and minus 9, minus 9. So, yeah. So now that, that bottom right becomes a lot less attractive if we actually care about the other person. Right. Well that's, that's, that's, okay, I'm less depressed now. Except of course, that requires that you feel that way internally and that I feel that way internally. There's another way that you could change the game here. Which is, what happens to sniches in jail? They are rewarded. No. No they're not. They're punished. Yes. Oh. So, if your a part of the criminal fraternity, and you don't like prisoners dilemma, then what you have to do is to create a system Where the people who snitch get punished. So it's not just the months that they spend in jail, it's everything else that's going to happen to them if they drop a dime. So you're saying that minus 6, minus 6, ends up being worse? No, what I'm. No, the minus, wait, no, wait, what? Yeah. Oh, the zero ends up getting, oh I see the zero ends up being worse. Because even though your not in jail your going to get I don't know somehow thwarted or, or punished for your past behaviors. Accosted. Interesting. That's right. So that's what you have to do and that works not just with criminals but with the real world. Whenever your in this sort of situation like a prisoners dilemma, you can change the game by changing everyone's utilities. Like for example hiring police officer, police officers or hiring members of the mob to take care of everything. I see. So it almost seems like what you're talking about is a kind of inverse game theory, where if there's a particular behavior that I want to see. How do I set up the payments and rewards so that that behavior is encouraged. Right, and by the way that has a name, and it's called mechanism design. Mechanism design? Yes. I'm not sure I understand either of those words. [LAUGH] Well, that's where you're trying to set up the set of incentives, the mechanisms that you're using to pay people. You're trying to design them in such a way to get particular behavior. This is what a lot of economics is all about. This is what a lot of government is all about. Tax breaks for example, for mortgage interest, encourages you to buy a home, rather than rent a home. I see, by changing the payoff structure. Right. Oh that's neat. And so that's what we learned today. At least right now. [LAUGH] Okay. Alright. So, let's see. So, just to try to rattle off some of the other things. The whole notion of Game Theory. We talked about, especially the idea that. You can think about a game as a tree or you could represent it as a matrix. And, I believe you said, repeatedly, the matrix has everything. Is that how you said it? Or the matrix is all you need. Yip! Let's see. We talked about minimax and maximin. Mm-hm. We, we relaxed a bunch of constraint on games. Mm-hm. So we, we looked at both perfect and hidden information. Mm-hm. We looked at both zero sum and non zero sum. We learned a lot today. We looked at deterministic and [UNKNOWN] I would want to say, but you called it non-deterministic. And assuming that we can get rid of the first two bullet items that look like maybe they were jokes. I would suggest saying things like, we talked about what strategies are and that they come in different flavors. We talked about the evil prisoners dilemma game. Mm-hm. What else? You gotta give me more, otherwise it's [INAUDIBLE]. Oh, more, good point. Andrew Moore gave lots of really good examples that yes. Michael may be cruel, but Andrew Moore is awesome. He's more cool than me. Andrew Moor is very cool. All of the examples that we've used today, or almost all of the examples we used today actually come from Andrew Moore's slides. Andrew Moore is a professor at Carnegie Mellon or at least he was before he went off to Google. And is a really smart guy who cares very much about machine learning. And game theory and produced a bunch of slides that it turns out lots and lots of people use in their own courses. And his examples were so good for game theory. That I decided to coop them with his permission of course. He tells everyone that they may use them. And, in fact, we have pointers to the slides in the resources links and folders for all of you to look at. And I recommend that you do. Did we learn anything else, Michael? The only other thing that I would want to mention is NASH which is a concept that is nashtastic. It is nashtastic. There are by the way, I should mention briefly. Other kinds of equilibria concepts they're beyond the scope of this class. But there's a whole lot more to game theory as you might imagine. And sometimes when they ask you to in these situations where you can't do what you want to do, you end up in these prisoner's lemonade situations. Other kinds of equilibria can get you out of it. And I'm going to argue without explaining why that the way that they get around this is by introducing Other ways of doing various kinds of communication. and, in fact, I claim they're a particular part of mechanism design. But that's a topic for another day. Okay. Fair enough. Okay. Did we learn anything else, Michael? I don't know. That's what I was thinking about. I mean, that seems like a lot to absorb. And, the other thing is that repeated games, even the prisoner's dilemma kind of unravel if you know when the end. And I was going to look into what happens if you don't know when they're going to end. Okay. So, I guess, that will be something that we will learn. Next time. Right, what we, what will we have will learned? Yes. Future past tense. [LAUGH] Alright, Michael, well, I think that's about it. At least my brain is full. So I will talk to you next time, and you get to lead what I believe is the last full lesson of the course. Oh, exciting. It is excuiting. Alright, well bye Michael. You have fun. I'll see you next time. Alright. Bye Charles. Bye. Hello Charles. Hi Michael. How are you today? I'm doing just fine. How are you doing? Alright. I'm a little out of practice with this lecturing thing. So, I hope this goes well. I'm absolutely sure it will. So, today's lesson is continuing what you were talking about in terms of Game Theory. But, I'm going to be focusing in on what happens when you are worried about making decisions. With more than one player in a sequence. Which we started to get into at the end of your discussion but I'm going to go, more into it. Okay. I really like the logo by the way. Thanks very much. Yeah, it's a, it's a specially game theory logo. I like it very much, I like it very much. I will point out, however, that all sequels should be called the quickening. Yeah, I was going to go with the quickening. Or judgement day, but I didn't, didn't think that made any sense. Hm, that's a fair point. So let me take you back into what we were talking about last time. We were talking about the iterated prisoners dilemma, and here's the prisoners dilemma payoffs that you wrote down for us. You remember this? I vaguely remember this. And remember it was about these two criminals, Smooth and Curly. And they were, deciding whether to cooperate or defect against each other when, after they've been arrested. Right, and they defect, they always defect. They always defect, right. So in particular, we say well what happens if it's, if they have multiple rounds in which to interact, and so here they are, here are the two of them, and if they've got one round to live, we did an analysis and we indicated that there's really nothing they can do other than defect against each other. Mm-hm. It's irrational to do anything else. Yep. So we said, all right, well, what happens if we allow there to be more than one round? So now we've got two rounds. And what we realized was that if you got two rounds to go, then these players essentially, face a one-round game because, after this round, what they're going to do in the last round has already been determined. So it's almost as if that round doesn't really matter. There's nothing we can do now that's going to change what they're going to do in that last round, so it's sort of like there's just one round and we're going to defect again. Right, so life is terrible and everyone is out to get everyone else. Exactly, and not only is it for two rounds, but this same argument continues as you go three rounds or more. Oh, it's like a proof by induction. It's kind of like a proof by induction, yeah, well it's proof by ellipsis. Mm, that's my favorite kind of induction. Proof by ellipsis. [LAUGH] So the question then becomes, what happens if the, number of rounds left is unknown, right? So what we've realized is that if you know how many rounds are left, the whole thing comes unraveled and they're just going to defect forever. But we raised the issue of, what happens if the number of rounds left is unknown? Hm. And it seems like it shouldn't really make any difference because if it's say some finite number we just don't know what it is, then it seems like it should still reduce to this same setup that we have. So I was looking into this and it turns out that is it's not the case it actually does make a difference and it's, it's really interesting how it goes and how it connects back with other things we've talked about. Woo, tell me more. So here's how I started to think about it. So let's say how can we represent the idea that we have an uncertain ending. We'll one way would be if we had some kind of generic probability distributions over the number of rounds that the games going to be played. But this seems like an, the simplest idea that I could think of. So here, here we have our, two criminals, and what their going to do is their going to play a round of prisoner's dilemma. But at the end of that round, they're going to flip a coin. And with probability one minus gamma, that will be the last round, it's all over. But with probability gamma, they're actually going to play again. And, and they do this after each round. And so each round is, is basically statistically independent of the other rounds. I see. So the set up here is, with probability gamma, the game continues. Now, notice that I chose gamma here. This was a, it's representing a probability here, but in the past, we've used this to represent a discount factor. Mm-hm. But that actually is the same thing, right? In, in, in the normal discounting, we say that the value that you get one step from now, is discounted downward by gamma. And that's exactly what you'd take if you worked out the expected value of a game where you continued making steps with probability gamma. And with probability one minus gamma, it ends if you get zero from then until the rest of time. So every round here could be your last, or not, right? It could be that you actually get to continue playing. Does that make some sense? That makes perfect sense. Awesome. All right, so, so yeah, this is, this is exactly that kind of situation where, well, here, let me, let me ask you a question. What's the expected number of rounds of this game? Well, I'll bet it's finite if gamma's less than one Yes, I even wrote that down. Yeah, I'm smart. Or at least I can read. Sure, but what's the, but, specifically we could actually write it as a, as a, function of gamma. Let's see. If gamma were something like, 99% then I would expect it to be about a 100, right? I think that's right. Yeah. So is that, is that your answer? [LAUGH] My function of gamma is if gamma is .99 the answer is 100. Yeah, something like that. It's not a total function but it's a function. Well, it's a sample. I mean, you do machine learning. Why don't you tell me what the function would be given that sample. Well, we can make it a quiz or I could just tell you. Why don't you just tell me. Alright. So one over one minus gamma is the answer. It works for your example. Um-hm. 1 minus .99 is 100th and we're talking 1 over that so, you get a 100. And, yea we could go through the argument as to why that's that's what it is. But this one over one minus gamma is what shows up all the time. If gamma is zero, then we're talking about one over one. The game lasts one round. That's exactly what we'd expect. Mm-hm As gamma gets closer and closer to one this pro, this quantities getting closer and closer to infinity. So, Right. in fact if you know, it becomes unbounded as gamma hits one. So yeah. So this is the expected of rounds, and so that means like yeah. So as you said if gamma is 0.99, it's a 100 rounds. And we already, reasoned that at a 100 rounds the whole thing falls apart. Right, huh, and I noticed the one over one minus gamma, of course, is just like the way we did discount factors, when we started doing MVP's in the first place. Exactly, yeah, that, that kind of links them together. Hm, that's actually kind of neat. So if we're going to be talking about strategies in this game that has an uncertain ending, we can't just write down sequences of, of actions anymore. We can't just say cooperate, defect, defect, defect, defect. Or even some kind of tree of possibilities. Because those are going to be finite representations. We need some other representation that allows us to play for an unbounded number of rounds. Mm-hm. And I'm going to start off by presenting an example of such a strategy, one that's, that's very famous for the iterated prisoner's dilemma, and it's called tit for tat. And the structure of tit for tat goes like this, on the first round of the game, A player playing this strategy will cooperate, and then in all future rounds, the player is going to copy the opponents previous move. Does that make sense? It does. So basically, we start, I start out, acting as if, you're going to cooperate with me. And the moment you don't cooperate with me, I will start to defect, and we'll be in the, the old style prisoners' dilemma. Right? Well no, not. What this says is that it actually copies the opponent's previous move. So if, if an opponent goes cooperate, defect, cooperate, defect, cooperate, defect, defect, defect, defect, cooperate, cooperate, cooperate. You're going to see something very similar coming out of the tit for tat agent. I see, I see, I see. In fact, we can represent the strategy as a little finite state machine, like this. Yeah, I like that, okay. And you, so you can see exactly how it kind of proceeds. It starts off cooperating. And then in each, each round it waits to see what the opponent does. That's the green letters here. And then it follows the corresponding arrow, to determine whether it is going to cooperate or defect in the, in the current round. Sure, that makes sense. So in this picture, the black letters here represent my move. And the green letters represent my observation of the opponent's move. Or at least if I'm being tit for tat. What happens if we follow Tit for Tat? That's a great question. So, let's make it a little more concrete. So, here's a set of strategies that an opponent might adopt. And now the question is, what does it look like Tit for Tat is doing in response to each of these? And I was hoping that you'd, you know, check the corresponding boxes. So basically, for each row, say okay, if you're playing against, if Tit for Tat is playing against always cooperate, what does Tit for Tat do? Does it always defect, does it always cooperate, does it cooperate and then defect, defect, defect, defect, does it alternate between cooperate and defect? And this should, just to give you some practice in, in interpreting the behavior of Tit for Tat. Okay, that works for me. I think I could do this. Go. Alright, so let's start off, maybe you can tell me what happens when Tit for Tat plays against always cooperate. So what happens if you always cooperate? Well, let's see, I start out cooperating. Mm-hm. And since the other person is cooperating I will continue to cooperate because that's what they did the last time. Mm-hm. So, I will always cooperate. That's right. Good. Alright, so what about if we play against always defect. Well if we always defect, the first time I'm going to cooperate because that's what you said Tit for Tat is. Hm. But from that point on, I will do what my opponent does, which is defect. So I will cooperate and then defect, defect, defect, defect, defect, defect, ellipses. Yeah, so I put this always defect in there, but actually it can't ever be the right answer right [LAUGH] because Tit for Tat always starts off cooperating. So the, the other three seem like they might be possible, always defect is not possible. Good. Alright. So, what if Tit for Tat plays against another Tit for Tat? Well, I started out cooperating my opponent cooperated. And since I'm going to do what that person does I will cooperate, but since that person's doing what I did, they will also cooperate. And so we will both cooperate forever, so we will always cooperate. Nice. So isn't that kind of interesting? So Tit for Tat even though, if, when it's playing against itself, is a very cooperative fellow. Hm, I like that. But if Tit for Tat is playing against something that defects, it becomes a little bit more vengeful. Yes. Alright, so what if it plays against something that is a little unsure of itself? So it starts off defecting, then cooperating, then defecting, it's sort of almost an anti kind of thing, right? So it's, this is one that starts off with a defect. Right, so it always, so I, first thing I do is cooperate. And then after that, effectively I do what the opponent does one step before. So I basically take what you, I'm pointing to the screen, you can't see me. I take the the D-C-D-C-D, and I just put a C in front of it, because that's what I'm going to do. So, I will do, C-D-C-D-C-D-C-D, ellipsis. So that's your last choice. Isn't the C-D-C-D-C-D, ellipsis in Atlanta? It is, actually. It is the home of all such analysis of diseases in the United States. The Center for Disease Control? Yes. Is that what it's called? Yes, that's exactly what it is. Come to Atlanta and work for us. [LAUGH] Alright, so good. So, that, that's the pattern of, of responses that Tit for Tat makes against this set of strategies. Oh, so we answered my question. Alright so now that we have a sense of what tit for tat does against various strategies, lets try to think about what we should do against tit for tat, so what do we do if we're facing tit for tat. So I'm just going to break it down to two possibilities it turns out there's actually more, but these these two are pretty instructive. So lets pretend that we have to choose between always defect as a way of playing against tit for tat, or we have to be always cooperate playing against tit for tat. So what I've written down here is what the total discounted reward is going to be or the total reward in this case. As a function of gamma. So, lets start with what happens if you play always cooperating against tit for tat. Well, you already told me that it. Such a thing will result in tit for tat always cooperating. Mm Hm. And that means we're going to play in this box. The cooperate-cooperate box. And that means on every single round, we're going to to get a minus one. Which means over an infinite run we're going to get an average of one, sorry minus one over one minus Gamma. Mm Hm. That makes sense. Okay? You agree with that? I do. Just minus one repeated over and over again. Now always defect as you recall you told me that, that will result, well for the always defect agent against tit for tat, the first thing that is going to happen is it's going to defect while the tit for tat cooperates right? So we're going to get zero for playing that strategy on the first round. Zero doesn't sound very good but look at the alternatives. They're all negative so zero's pretty good. Mm-hm. So it does this sort of you know good thing in the first step then after that tit for tat responds by always defecting in response. Right? And that means we're going to be stuck in this box the defect defect box where you get minus sixes for the rest of ever. Yes. So that means minus six over one minus gamma. But that starts one step from now so we multiply it by another gamma. Okay. Alright. So these are two different expression that represents what our pay off would be for adopting two different strategies. And in fact, if gamma is really high, very close to 1, then this is a really good answer, right? Because it sort of grows, it's like, minus over one minus gamma. So, for high gamma, we're talking about something that's minus one times a really big number. Mm-hm. Whereas this first one is not so good for high gamma because what's going to happen is it's going to get, end up getting minus six on every step. So it's going to do worse overall. But if, if we're talking about the low gamma. Then, let's say, you know, zero for example. A gamma of zero will, will always defect. We'll get zero plus zero. But always cooperate, we'll get negative one over one. And zero is better than minus one. So for really small gamma, like if the games unlikely to last many rounds, you should defect. But is the game is going to last a long time, then you should cooperate. I believe that. Cool! Alright, so then my question to you is: What's the value of gamma for which these two different strategies to play against tit for tat are equally good. I think I know the answer. Woah! That was fast. Let's give everybody else a chance to think about it. Okay. Go. Alright Charles, what, you said you had the answer how do we figure it out? It's 1 6th. I guess that's what, which, we don't, I don't know if it will accept that, whatever 1 6th is expressed as a decimal, but yes 1 6th. How did you get that? I saw the number six and figured it had to be 1 6th because you said it was low, but here is what actually I did, well, I was thinking about it is, you said, well when are they equally good? Well, if you always defect, you get minus 6 gamma over 1 minus gamma. And if you always cooperate, you get minus 1 over minus 1 minus gamma, so they're equally good when those two values are the same. Good, alright, and the denominators are the same, as long as gamma's not one, that's fine. Right. If we divide by the negative 6, we get gamma equals 1 6th. Exactly. Excellent. So, so that's interesting, right? So, that's, it's saying that for gamma values that are less than 1 6th, we should be doi, we should just defect because there's no. Well the games not going to last long enough for us to form any kind of coalition. But for things higher than 1 6th. A half. 3 quarters. 0.999. It's going to be better to cooperate than to defect against tit for tat. Or 6 plus epsilon. Indeed. Yeah. I like that. That's actually very cool. Now, we kind of cheated here. Because I told you there's just these, those two strategies. But there's actually a bunch of other strategies you can play against tit for tat. And it's worth thinking through, how do you compute a best response to some finite-state strategy? So tit for tat is a finite-state strategy in that it has these two [LAUGH], these two states. And the strategies expressed in terms of transitions between those two states. But in general, if we have some kind of finite-state strategy, like tit for tat, how do we figure out how to maximize our own reward in the face of playing against that strategy? So in this picture here that I drew, the states are labeled with the opponent's choice, the finite state strategies choice, okay? Mm-hm. The edges, that's in black, the edges and labeled in green here, are labeled with our choice. So, for example, if we're in this state of the, sorry, if our opponent is in this state. Mm-hm. We have a choice. We can either cooperate or defect. On this round. Mm-hm. So, the green arrows tell us how that will impact the state of the opponent. And then these red numbers, I just added the information about well I know that if the opponant is about to cooperate and I choose to cooperate. I can just look up in the pay off matrix that that's a -1 for me. Right? Agreed? Agreed. So I just annotated all these edges, all these choices with these extra numbers. So, one of the things that's cool about this is unlike just the payoff matrix representation that we had before, our choice, it impacts the payoff, which is the same as that, but it also impacts the future decisions of the opponent. And that gives us this structure here and also says that maybe this is a slightly harder thing to figure out because of the fact that we can't just maximize our, the number. We actually have to think about where that's going to lead us in the future as well. So two things then. One, I was always fond of saying that the matrix was all that you needed. But that really only made sense when you were just playing once. Yes. That's right. Right? And, two, I look at this and it's a finite state machine but you know what else it looks like to me? It looks like an MDP. Excellent. It is indeed an MDP. Now, it's a, in this case, my opponent's finite state strategy is deterministic, so it's a deterministic MDP, but it is. It's a discounted MDP. Gamma's playing the role of the discount factor. The entries from the payoff matrix are playing the roles of rewards Our action is playing the choice of our action, and the opponent's internal state structure is playing the role our states. So it is, it's an MDP, and so how do we figure out what an optimal strategy is against a finite state strategy? We solve the MDP. Yeah, exactly. So any, any method for solving an MDP can then be used to actually compute the strategy, so what is the strategy going to look like? It is going to be a mapping from states of the opponent to action choices for us. Right, but that's fine because a state does not have to be your state, it's just what matters. What matters in this case is what they opponent is going to do. Right. So now what are the strategies that can be meaningful against tit for tat? So if we cooperate then we're going to stay in this state and it's always going to be the right thing to do to cooperate. So always cooperate is one. If we always, if we defect, now we have a choice again so we could defect from this state which would cause us to defect forever, so always defect is another one. But what's the other thing that could happen? Well, we could tit for tat ourselves. Well, sort of. I mean, so, we could defect, so we could defect against cooperate, but cooperate against defect. Which would actually cause us to do D-C, D-C, D-C. So those are the only, oh I see. No, you're right. I'm not sure how to say it. But the policy is, defect when you're in this state, and cooperate when you're in this state. But the effect of that is to go back and forth against tit for tat. Right. Basically, take this loop here. And those are the only policies that matter. And in this case, we worked out that. Always cooperate is good against tit for tat if it has a high discount factor and always defect is better if you have a low discount factor. But, we can get that for real by solving the MDP. Right and that makes sense and the reason that those are only 3, let me see if I get this right, the reason those are the only 3 that makes sense because if you think of this as an MDP then it has no history, so when you are in C there are only 2 choices and when you are in D there are really only two choices so if you look at the way you've drawn it. You either stay were you are in c or d, or you take the loop. And those are really the only three options because the rest of them would require you remember what you did, you know, a time step or two ago, and there's no way to do that in an MDP, at least not as you've written it. Well, there's a way to do it, it just would never be better than doing it this way. So an MDP always has a mark off deterministic optimal policy. Right. So we only need to consider those. I think that was the same thing that I was trying to say, but with different words. [LAUGH] Ok. Alright so, now that we have a handle on what it means to compute a best response against some finite state strategy. Let's actually take a look at these. So, so this is a quiz. Imagine that we have a gamma that is large, so greater than a six. Mm-hm. What is the best response to each of these strategies? So this will be a quiz, but let me just make it clear what the, the goal of the quiz is. So, if you're, if you are playing against, always cooperate. Which of these is the best response to it, which of these is going to actually have maximum reward? If we're playing against always defect which is going to have maximum reward? And if we're playing against tit for tat which are going to have maximum reward? Any questions about that, does that make sense? I'm just making certain I have the rows and the columns right. So my opponent is playing on the rows. Yep. And so I'm choosing among the columns for each one of those rows. Yes? Yeah. So I labelled it best possible response for the columns. And these are the opponent's strategies on the rows. Okay. Okay, go. Alright. So is this, is it clear what these answers are? Let's find out. So, let's see. we, we already worked out the math on this. So we know that, for gamma, greater than 1 6th, cooperating is better than defecting, in general. So if I'm going against someone who's always going to cooperate, then I should always cooperate. Incorrect. What? Yes, so, so, we didn't actually work that out. What we worked out was what to do if you were playing against tit for tat. If you were playing against someone who was always cooperating, and is completely oblivious to us. No, no, no, no, no, you're right. You should always defect because you're always going to win. Yes, yes, yes. You're always going to win. You're going to get zero on every time step. Right, right. Yeah. No, that makes sense. That's beautiful. Alright, what about always defect? Well, if you're always going to defect, you might as well defect. Indeed. because we're just in the regular old prisoner's dilemma world. [LAUGH] good, alright. So now we, now we have this, this other strange beast here. So our opponent is playing tit for tat. So we could always defect. Right. But we would do, we'd get a higher score if we can convince tit for tat to cooperate with us. For a gamma greater than a 6th. That's right. So that you should always cooperate. That is true, however. Mm-hm. What if we played tit for tat against tit for tat. You'll end up in the same place. Yeah, so that's just as good. Mm-hm. And that's, that's kind of interesting. If you think about mutual best responses. Yes. So that's a strategy that, a pair of strategies where each is a best response to the other, there's a, we have another name for that. Do you remember? No. [LAUGH] You taught, you told us what it was. Yeah, but I probably used different words. It's, it's a Nash equilibrium. Oh. This, that's what a Nash equilibrium is. A pair of strategies where each is a best response to the other. Each, ea there's no way that either would prefer to switch to something else to get higher reward. And that makes perfect sense. You're in an equilibrium. And that is a Nash equilibrium. Okay. So we can use this little table here to actually identify Nash equilibrium. So, what would a strategy be? So, so if one player plays always cooperate, then the best response to that is always defect. Mm-hm. But the best response to always defect, is always defect. So always cooperate is not part of a Nash equilibrium. Right. But what about always defect versus always defect? No, it is. So that's a Nash. Right? Yep. Because they're both doing the thing that is the best response to the other. Right. Alright, this box here, always cooperate against tit for tat. That's not okay, because if a player does always cooperate, it's always better to switch to always defect. Yep. But, check this out. If you are playing tit for tat, and the other player's playing tit for tat, there's no reason to switch because it's actually a best response, it's the optimal thing to do. And that works from both players' perspectives. And that makes sense. So like you said check this out and you've been using check marks that's very good. [LAUGH]. So we're in this situation where we have two Nash equilibria. Indeed, and one of these Nash equilibria, [NOISE] is cooperative. Which is the thing that we were sad about, or at least that I was feeling really sad about, in the, in the last lesson. The idea that, man, there's just, it's clear that they should just try to get along. You explained that you can modify the reward structure, and then they would get along better. But here it turns out, well, no, another thing you can do is just open it up to the possibility of playing multiple rounds, as long as you don't know how many rounds, it becomes possible to to have a strategy that is best off cooperating, and is in fact a Nash Equilibrium. Isn't that equivalent to changing the reward structure? It's definitely related to changing the reward structure. It's a particular way of changing it. A very particular way. But it's. Yeah, because it's not true any more that we can do this in the one shot case. You have to be in the, in the repeated game, setting. Right, so you change your rewards structure to be a sum of rewards. But it's actually an expected sum of rewards, and you don't know where it is you're going to stop. So I guess you're changed, you're changed the game, you changed the, the rewards. But in a, sort of very subtle way. Yeah, the whole game is different, really. Yeah man, you changed the game. [LAUGH]. Sometimes you gotta change the game. Don't hate the game. No, you are supposed to hate the game. Don't hate the player, hate the game. I'm really proud of us for figuring out a way to make the prisoner's dilemma a little more friendly. It turns out, though, that this, this idea, this sort of core idea, is very general and kind of cool. So, it leads us to a topic that we could call, repeated games, and the folk theorem. Mm-hm. So the general idea here is that when you're in the repeated game setting, this possibility of retaliation, the possible of, you know, not being cooperative actually opens the door for cooperation. Because now it becomes, it can become better to just get along and cooperate than to get retaliated against. Right. And that makes sense as long as the retaliation is plausible. Well, we're not talking about plausibility of the retaliation. We're just talking about, right, because the tit for tat, it's, it's not analyzed in that way. It, it doesn't say whether or not it's actually plausible. It just says, well, if you have this strategy and you play against this strategy, there's no incentive to switch. Right. But we'll, yeah, we'll get into this plausibility thing in a little bit. Oh, I, I stumbled across a word. Okay, go. [LAUGH] But I want to point out one thing first, which actually kind of irritates me. Kind of along the same lines as like, regression is the wrong word and reinforcement is the wrong word. Folk theorem is the wrong word. So, what is a folk theorem. It's two words. In general mathematics, sorry say again? A folk theorem is two words. That's fair, yes. So, I think it's actually two different terms of art. So in mathematics folk theorems are results that are known, at least to experts in the field, and they're considered to have established status but they're not really published in their complete form. There isn't some kind of original publication saying, oh look, I, I found this thing. It's more, something that like, kind of everybody knows, and so you don't really give anybody credit for it. So it's like a theorem that is in the general mm, cloud of understanding. It's sort of among the population, among the group. Wait! Does, does anybody every prove these folk theorems? You can, yeah! All folk theorems are provable. But it's not like you say you know this is Charles Isabel's theorem. Mm. It's like. No, it's just a folk theorem. It's like. Charles, Charles can prove it. We can all prove it. But we, we're not really sure whoever proved it first. It was just one of those things that everybody knows. Oh, so it's like an oral tradition. Yeah, yeah. I think that's a good way to think about it. Okay. I'm just imagining mathematicians sitting around a fire in the winter, cuddled up against one another sharing theorems that have been proved since the beginning of time to one another. Exactly. Right that's the image that I have as well. This, just it's folk theorems. It's this sort of, you know, I learned it from my grandmother and now I'm telling you. Hm. I like that. I like to think of mathematicians that way. [LAUGH] As grandmothers. Yes. So so that's what a folk theorem is in mathematics. However, in game theory it means something different. It is referring to a folk theorem, but it's also referring to a particular folk theorem. So the folk theorem in game theory refers to a particular result that describes the set of payoffs that can result from Nash strategies in repeated games. So it's a funny thing, it's a funny way that they use the word. It's, it's a, it's a folk theorem but it's also the folk theorem. Hm, well, well, maybe it was actually, proven the first time by some guy named Folk. That's a good idea, we should probably, we should push that forward like call him Folke or something like that. Folke's theorem Yeah. But but no. [LAUGH] Oh well, I tried. Yeah that's that's really great idea that we're going to just move on from. [LAUGH] But what I, what I'd like to do next is kind of build up to this notion of the Folk Theorem and it's going to require a couple basic concepts that are not too different from stuff we've already talked about, but we're going to kind of make them concrete and then we're going to show how they all come together to provide us with this idea of a Folk Theorem. Okay. The thing that I find most useful in trying to understand the folk theorem and, and what it says and how it works is a thing that I call a two-player plot. I don't know if other people have other names for it, but this, this concept is out and, and often discussed. I just don't know what it's called [LAUGH]. But here's the idea of it, it's a really simple idea. It's like a folk plot. It is a, it is a folk, a folk plot, right, I don't know who invented this plot. So here's what we're going to do, remember this prisoner's dilemma game. We've got two players, there's Smoove and Curly. Mm-hm. And, what we're going to do, is we're, there's a bunch of joint, actions that they can take. So Smoove cooperates and Curly defects, or they both defect, or they both cooperate, or one defects defect cooperate in the other direction. So what we're going to do is for each of those joint outcomes, each of those joint action choices. I'm going to plot a dot, put a dot on a two-dimensional plot, where this is the Smoove axis, and this is the Curly axis, okay. Okay. So cooperate-cooperate, remember from the prisoner, prisoner dilemma payoffs, is minus one, minus one. So I put a dot at minus one, minus one. Defect, defect. Is it minus six, minus six? Cooperate defect is it minus nine zero? And defect cooperate, is it zero minus nine? . So do those four points make sense to you? Do you understand like the idea? They do. It may not be so obvious why we do, do it this way, because as you have told us, the matrix is all you need. Mm-hm. But this is really just representing the matrix in another form. But it actually is sort of losing some information. Because these dots don't tell us what the relationship is in terms of if, if one player keeps the same action and the other player changes to a different action. The matrix captures that but this plot doesn't anymore, it's kind of washed out. I see. All right. Now so just to kind of reinforce this idea and also to let us think about it in a slightly different direction, I want you to consider solving the following quiz. Here's four payoffs that I've put little boxes around. This one at minus 1, minus 1, this one at minus 3, 0, and this one at minus 4, minus 4. And the question is, which of these payoffs can be the average payoff for some joint strategy? And this is in the repeated game setting, [CROSSTALK] okay. So what we're imagining is these two players can coordinate in any way they want. We're not talking about trying to maximize reward or, or, being Nash or anything like that. They're just going to execute some, some pattern of, of strategies over infinite run. We're going to average the payoffs that the two players get and we'll say, you know is it possible for them to adopt a strategy so that the average per time step gets minus 3 for Smooth and 0 for Curly? Yes or no? So check that box if that's possible. Can it be that they both get minus one on average? If so, check this box. And can it be that they both get minus 4 on average? Check this box. Does that, does that make sense? It does make sense. All right, so I'll give you a chance to think it through and answer it. Okay. Alright, so are any of these easy to answer right off the bat? Well, one of them is really easy to answer right off the bat, and that's minus 1 minus 1. Good. Because that's one of the points, so. [LAUGH] Yeah. So, the joint strategy to get the minus one minus one would be for both players to cooperate, right? Again, they're just, you know, they're just willing to do that just for the sake of showing they can make that value. Right, and I think I know the answers for the other ones as well Alright, hit me. So here's how i'm going to, here's how I'm going to, going to talk you through my reasoning so that if any point I'm wrong, you can gently steer me away from embarrassing myself. Basically I was looking at these points and I thought hm, they form a kind of a convex hull. Aha! And I thought well, surely that's just an accident of the numbers and then I thought, oh wait, of course we're talking about averages here. So that means that all sort of possibilities have to be inside the convex hull of the outer points. So if I drew a line between those four points, I would end up with all possible, achievable averages. What, achievable averages, how? How would you achieve things inside the convex hull? I would appropriately average them. So in particular, the first thing I'd notice that minus three, zero, is outside that. Mm. So, it can't be, it can't be something you can achieve. Right, there's no way to make this minus three, zero, certainly not by choosing any of these points, but also no combi, no convex combination, no probabilistic combination of them, is going to do that either. Right. So, this one is just right out. Right, so that leaves minus four minus four, and it seems pretty. That's inside the convex hull, so there is some combination of them that would work. That's right. Any, any sense of what it would be? 2 3rds, I think 2 3rds D,D 1 3rd C,C. And I get that by noticing that D,D is minus six, minus six, and C,C is minus one minus one, and four minus four is two thirds of the way between there. Boom. Cool. Alright, so you're good with that? I'm good with that if you're good with that. It's your quiz. Yeah. I'm excited. Oh good. So those, it's this, this, the minus one minus one and the minus four minus four are, as as you pointed out there is a more general result here. Having to do with the convex hull. Right. Alright, so through the magic of computer graphics, I have a slightly better depiction of this particular region now. As you pointed out this, this convex hull of the points is really important and what it represents is the, we can call it the feasible region, these are average payoffs of some joint strategy they, they may have to collude to do this. And they may not be particularly happy to do that. [LAUGH]. But the fact of the matter is, they can achieve, by working together, payoffs anywhere inside this region. Huh, so that's what my student Liam always meant when he talked about feasible regions. Maybe so, I mean it could be that he meant any number of other things like places he's willing to live when he goes to get a job. No, it was all game theory stuff, I just never knew what he was talking about, but now I do Michael, thanks to you. Sure, hey I'm, I'm happy to help. So this is a really useful kind of geometric way of picturing something that would otherwise be a little bit harder to see, I think in the matrix form. Sure. All right, the next concept that we're going to need to understand the folk theorem is the notion of a minmax profile. So a minmax profile is going to be a pair of payoffs, one for each player. And the value for player represents the payoffs that that player can achieve by defending itself from a malicious adversary. So what do you suppose a malicious adversary would mean in a game theory context? Someone who's desperately trying to hurt you. And what does hurt mean? Gives you the lowest score. Yeah, and what does that remind you of from your lesson? My grad students. You think they're malicious? It would explain a few things. Yeah, I don't think they're malicious. They're sweet. [LAUGH] Yeah, I know a lot of them and they're, they're wonderful people. Well, what it reminds me of is, they are wonderful people. It reminds me of zero-sum games. Exactly. So you can imagine thinking about the game that we're playing, now, no longer as being I get my payoff and you get yours, but I get my payoff and you get the negative of my payoff. So you don't, you don't really care about yourself anymore. All you care about is hurting me. And that's, that's the idea of a malicious adversary. I have some ex-girlfriends like that. I'm so. Oh. [LAUGH]. It is. People do get into this mode sometimes. And that, that's actually going to be important in understanding the folk there. Hmm. So what I'd like to do is figure out what the min-max profile is for this game. So this is a very famous Game theory, game example. Sometimes goes by the name battle of the sexes. That what the b and the s stand for? Sometimes it stands for Bach and Stravinsky. Blech. Those are like composers, I think. You mean like the Backstreet Boys and Sting. Ahh. Alright, that works for me. So, so, let me explain this story. It turns out that Smooth and Curly actually got away, they didn't make any kind of deal, they actually just figured out a way of escaping from the jail. So they're, they're back out on the streets again, and they decided that they'd like to celebrate their freedom by going out to see a concert. And they both decided that in advance, but what they didn't know was which concerts were available. Once they escaped out into the world, they couldn't communicate with each other, they discovered that there's in fact two concerts in the city that night. The Backstreet Boys are playing, and Sting is playing. Okay. Now as it turns out, each of them is now going to have to choose, whether to go to the concert with Backstreet Boys or Sting, and they're choosing independently. Now, if they end up going to different concert's they're both going to be unhappy and get zero's. I see. If they end up at the same concert, then they're going to be happier, but in fact, as it turns out, Smooth really likes the Backstreet Boys and would prefer that they both end up at the Backstreet Boy concert. But Curly really likes Sting and would prefer that they end up at the Sting Concert. That's not realistic. Which part? The fact that I prefer the Backstreet Boys to Sting. What, what do you mean you? I mean this is Smooth. He's a criminal. Mm, that's a fine point that you make there. There is no connection between these characters and ourselves. Real life characters, living, dead or fictional, or mathematical, or instructional. If so, otherwise purely coincidental. Yeah. I could switch these around if you'd prefer. No, no. I'll go with your fantasy. [LAUGH] All right. I, yeah, I think that payoff matrix may look something like we both have twos in, in the s, the same place. Mhm. But anyway, but let's say for the purposes of this example, there's a little bit of a disagreement. Okay, so now what we need to figure out is what the minmax profile is for this game. Okay. Alright, so that's going to be a pair of numbers. Mm-hm. One number corresponds to the payoff for Curly and one number corresponds to the payoff for Smooth. And it should be, the payoff for Curly should be the payoff that Curly can guarantee himself even if Smooth is trying to get him to have a low score. Okay. And vice versa. Smooth's score is going to be the score that it can guarantee, he can guarantee himself even if Curly is trying to minimize Smooth's score. All right. So let's do this as a quiz. So,I want you to find the min-max profile for this game, this Bach, Stravinsky game or Backstreet, Sting game, and put the, the number for Curly in the first box and Smooth in the second box. Okay. Go. Alright, what'd you got? I'm going to say it's one, one. Alright, how would you figure that out? Well first, I would ask you if that's correct. [LAUGH] [LAUGH] We'll see when you try to. [LAUGH] Figure it out. Okay, so, the idea here is I'm trying to figure out what Curly would get, if Smoove was out to get, make certain that Curly got the worst possible value. So, Curly gets to ch Curly gets to choose among rows. And Smoove gets to choose among columns. So, If I chose the B column, then Curly could get a one. You're given choice Curly would go for the first row instead of the second row, because Curly would get a one. If I took the S column, as Smoove, Smoove took the S column, then Curly would choose the second row, and would get two. And one is lower than two. So, Smoove would choose the first column and Curly would have to get the first row and would end up with a one. That make sense? Yes, but here's what you haven't figured out. Yes? What happens if Smoove, no because, because Smoove says, I'm always going to choose the Backstreet Boys. Mm-hm. Then you're right, Curly should also choose that and at least coordinate. Mm-hm. But what if Smoove is random say half and half random between the two options. And I don't know what's going to happen before then? And you don't know what's going to happen, sorry? I know that, I know it's half Curly knows it's half and half but doesn't know which one he's choosing any given time? Exactly. I see. Then, I would end up with, one half and one. Then for choosing B. Uh-huh. Expect the score would be a half, and for choosing S, they'd expect the wor, expect the score would be one. Yep. Oh, I see. And, and then Curly would choose S and get the one. Right. And that's still consistent with, with that? Mm-hm. Can we work out what the what the worst case is? What do you mean? Well I feel like, we should solve it like a [INAUDIBLE] game, like we did in your lesson. Oh. Say that Smoove is actually choosing a pro, probability either X or 1 minus X. So I thought you were asking for like a pure decision, I wasn't even thinking about mixed decisions. Uh-huh, yes, it could be a malicious and randomized adversary. [SOUND] So we are. [LAUGH] You said yuck [LAUGH]. We, we are really talking about my ex-girlfriends. Okay so you just do the math. Do the math. Alright, so if we, if Smoove chooses Backstreet Boys with probability X and Sting with probability of one minus X. Mm-hm. Then, Curly for choosing Backstreet Boys will get X on average and for choosing Sting, we'll get two times one minus six on average. And the useful point is going to be to discover when these are equal to each other. Yep. So in fact, Smoove, by being malicious and stochastic, can actually force things down to 2 3rds. Hm. And things being symmetric as they are, Curly can do the same. Okay. So, basically, Curly can behave in such a way, that even against a malicious adversary, it could, he could guarantee himself to a score of 2 3rds. Yeah, a malicious possibly mixed adversary. That's right. Okay. But one, one would be right if we were sticking with pure strategies, but why would we do that? That's right, but for the purpose, if that's right, and you can do a version of the folk theorem. In fact, there's lots of different flavors of the folk theorem. The one that we're going to focus on is going to allow for these mixed strategies. Mm-hm. But. In fact in general you could say you know, no I kind of like the mixed strategies, let's just stick with that. No, I like the mixed strategies too, I just wasn't thinking about them. So, I want to point out that in fact the solution that you gave, I think that actually does correspond to what is usually called Minmax, which is the pure strategy. Yeah! So the, [LAUGH] the minmax is, is in fact one, one. The other concept is really important too though. And I think it's sometimes called the security level profile. So instead of the min max level profile the security level profile. And that allows for the possibility of, of mixed strategies. So that gets you down to the 2 3rds, 2 3rds. I think you know, it turns out that there's folk theorems that can be defined with with either of those concepts. I prefer this one. But I do like this name better [LAUGH]. So I, I apologize if that made things confusing. I'm not confused now. I think in the end Michael, the important thing is we were both right. Well, the example that I'm going to next, these two concepts line up. So let's let's do that, and then we don't have to care. [LAUGH] I, I'm all for that. Let's do that. So, here we are, back at the prisoner's dilemma, again. You may recall this picture. Vaguely. Let's add to this, the minmax, or security level profile. So, for prisoner's dilemma, what is the, the minmax? Isn't it d comma d? It is indeed. D comma d. Right so this is value that you can guarantee yourself against a malicious adversary. Malicious adversary is just going to defect on you, and the best thing you can do in that case is defect yourself. Yep. Agreed, agreed? Agreed. Alright, so now let's take a look at the intersection of the following two regions. There's this nice yellow region that we've already got, and then we've got a new region that's defined by this minmax point. This minmax profile. So the region that is above and to the right of this, of this minmax point. Mm-hm. So, the, that's the region. This, this region, alright we already said that this yellow region is called the feasible region. Mm-hm, or orange or whatever color it is. So, I'm thinking we can call this other region the acceptable region. And it, and, the, what I mean by that is if you think about it, payoffs in this region are, smooth, getting more than what smooth can guarantee itself in an adversarial situation. And Curly getting more than Curly could guarantee himself in an adversarial situation. So, these are all, like, you know, better than it could be. So, why not call it the preferable region? The preferable regions, preferable to not being in this region. Mm-hm The intersection of these two Is the feasible preferable acceptable reason? [LAUGH] Exactly. It's kind of, you know, special, from the perspective that it is both feasible and preferable. And now we are ready to state the Folk Theorem. So here's the Folk Theorem. Any feasible payoff profile that strictly dominates the minmax or security level profile can be realized as a Nash equilibrium payoff profile, with a sufficiently large discount factor. Prove it. What we're going to do is we're going to construct a way of behaving where both players are going to play so that they achieve this feasible profile. And the reason to make that a Nash equilibrium, what we need to do is make it so that it's a best response. And the way that we are going to make it a best response is we're going to say do what you're told. Follow, follow your instructions to achieve that feasible payoff. And if you don't, then the other player is going to attack you, is going to adopt a strategy that forces you down to your minmax or security level, and that's your threat. So the best response to that threat is to just go along and, and and do what you're told to achieve that feasible payoff. The only way that that's going to be stable though is if the thing that you're asked to do, the feasible payoff, is better than the minmax, right? Because that has to be a threat. You can't threaten somebody and say, you know, do this or I'm going to give you candy. It's gotta be do this or I'm going to do, give you something that's less pleasant than what I've asked you to do. Okay, that actually makes sense. Yeah, so this is, this is a really cool idea. I like it. Hey, could you try saying that again, but with a Southern accent, just the Folk Theorem part? Any feasible payoff profile that strictly dominates the minmax/security level profile can be realized as a Nash equilibrium payoff profile with sufficiently large discount factor. I like that because now it's a folksey theorem. [LAUGH] So another way to think about the proof of the folk theorem, is you could prove it with a little strategy that is referred to as grim trigger. Mm. I like that. I like the way it sounds. So here's, here's the basic structure of grim trigger. It says that what we're going to do, is we're going to start off, taking some kind of action or pattern of actions, it's a mutual benefit. And as long as it's cooperation continues this mutual beneficial, behavior will continue. But however, if you ever cross the line, and fail to cooperate with me. Then I will deal out vengeance against you forever. Hm, so once again, we're talking about my ex girlfriends. I don't know why you're obsessed with this. [LAUGH] Maybe it's just, maybe it's just trying to help you understand. So you know kind of what this situation is. Anyways here's, here's what that looks like in the context of prisoners dilemma. Alright so cooperation is the, is the mutually beneficial action. Yes. And as long as you continue to cooperate with me, that's this C arrow here, then I will continue to cooperate with you, but if you ever defect on me, I swear I will spend the rest of my life making you pay. So no matter what you do at this point, defect of cooperate, I will just continue to defect on you. Pain will rain from the sky. Okay, well this makes sense. So, the idea here is that if you know that I"m going to do this, then hopefully it makes sense for us to continue to mutually benefit. Right. So the whole purpose of this is to create, a, a Nash Equilibrium System kind of situation. Right? Where if I'm playing this strategy. And you're playing this strategy, then neither of us has any incentive to cross the line, and so we're just going to continue to cooperate. Crossing the line is going to decrease your reward, so there's no benefit to doing it, so you won't do it. So it, it's nice because it gets us a Nash Equilibrium. Hm. But there's a problem with it. Of course. And you pointed it out before, so let's, let's dive in and make sure that we understand it and see if we can fix it. Okay. The problem is that in some sense, the threat is implausible. And it, and it, in a very kind of real sense. So what's happened is that if you do fake out on me, if you do cross that line, the idea that I will then spend the rest of my days punishing you, forgoing reward myself, right? Not taking the best response against you, seems kind of crazy. Do you agree? Yeah, again, my ex-girlfriend. Yeah, I totally get this. No, but no, I'm saying that nobody would do that. Right, so it, it would be like being in a elevator with a stick of dynamite and seeing that someone has a hundred dollars and saying, give me your hundred dollars or I'll blow us both up. That's not really a reasonable threat because the alternative to me not giving you a hundred dollars is, you die, which seems probably not worth it. That's right, so you could think about the possibility of, okay, I'm going to not give you the $100, you say that you're going to blow me up, but you will hurt yourself more than you hurt me, so it won't be a best response. Not blowing me up and just not getting the $100 and leaving the elevator is better for you than blowing me up. Right. So that is an implausible threat. So the way we formalize this idea, in the game theoretic context, is to say that we're interested. A plausible threat corresponds to something that's called a subgame perfect equilibrium. Okay. So subgame perfect means that each player is always taking a best response, independent of the history. All right, so let's actually look at a concrete example here, let's imagine playing Grim Trigger, against Tit for Tat. So my first question to you is, are these two strategies in nash equilibrio with each other. Yeah I guess so. And why is that? Because, the fact, if I'm playing Tit, if one player is playing Tit for Tat then the Grim Trigger thing doesn't matter anyway because both of you are going to cooperate forever and it doesn't make any sense to deviate. Right, so any strategy that I could choose that's different than Grim Trigger is going to on average do no better, possibly worse. Right. So I might as well stick with Grim Trigger and Tit for Tat has the same kind of feeling about it, that, it's cooperating with Grim. And any, it can't really do anything better so it might as well do that. Right. So the next question to ask is. Are these two strategies in a subgame perfect equilibrium with each other? And the way that you actually test that, is you say, well, they are not subgame perfect. If there's some history of actions that we could feed to these machines, so that, so that, you know, here's, here's what Grim is doing. It's, it's some, some sequence of cooperates and defects, and here's what Tit for Tat is doing. It's some sequence of cooperates and defects. And once we've reached some particular point. Is it the case that one or the other of these machines is not taking a best response that it could actually change it's behavior away from, what the machine says and do better then what the machine says. If that is the case then it's not subgame perfect but if it's the case of all histories. They're always taking a best response. Then, it is subgame perfect. So, so do you see a, a sequence of, of moves that these two players can take where one or the other of them is not going to be doing a best response? It's, can take, right? As opposed to, will take. I don't understand. Yeah, I'm not sure I do either. That's why I asked the question. It's not a, you know made up history, it's like an actual set of moves that are consistent with Grim and Tit for Tat. No no, no no, so it is, it is not necessarily. So we know that if we actually play these against each other the only history that we're going to see, is. Cooperate forever. Right, Grim is going to do cooperate cooperate cooperate cooperate, Tit for Tat is going to do cooperate cooperate cooperate cooperate. And so they are, and everything's fine. The question is, can we actually go in and alter, the history, so that one or the other in the machines could take a better action than the one that the machine tells it to take. Yeah if Tit for Tat, ever does defect. Alright, so let's take a look at that. So, let's say, on the first move Grim cooperates and Tit for Tat defects. Okay, so let's say that, that's the moment in time. What will the machines do at this point? Well, at this point the and next time step, Tit for Tat will cooperate and Grim will defect. Good and then thereafter. Grim will always defect. And then Tit for Tat will always defect. Right. So the pay of that Grim gets at this point is going to be, well initially high but then very very low. Mm-hm. On the other hand could Grim have changed it's behavior to do better than this? Yeah. Just by doing just, by choosing to cooperate. By choosing to cooperate, so it sort of ignore the fact that, that Tit for Tat did the defect, and instead do a cooperate here, then Grim would do better. So the idea is that Grim is making a threat, but when it comes time to actually follow through on that threat, it's actually doing something that is worse for itself. Than what it would do otherwise. Do, do you see that? I do. So is it subgame perfect? No. And the proof of that is exactly, exactly what you said, Take, take a look at this history. Here's a history where Grim would not be willing to actually follow through on its threat. Right. So it's an implausible threat, and that's bad. So maybe we've now just undone all the awesomeness that we had done. No. Well maybe. I mean the awesomeness was hey look we can actually get machines that are in nash equilibrium and they're cooperative in, in prisoner's dilemma so they're actually kind of doing the right thing. And, turns out well they are but they're, depending on this notion of implausible threats to do it. Mm, how should I feel about that? Well, let's see if we can fix it. Okay. All right. So let's make sure that we get this concept. So let's evaluate, tit-for-tat versus tit-for-tat, spy versus spy, and ask whether or not they are subgame perfect, or in a subgame perfect equilibrium with each other. And your choices are yes and no. Mm hm. But if you say no, I'd like you to give me a sequence to show that it is not subgame perfect. In other words, that if they were to take this sequence of actions and this one was to take this sequence of actions, it would leave the machines in a position where they would not be willing to follow through on their threat. That it would be better for them to do something else in the long run, assuming that they're still playing against the other tit-for-tat machine. From that point on. Yup. Just imagine that someone could go in and change one thing. Would you still want to follow tit-for-tat the next time step or not? Yeah, I don't know if that has to be just one thing, but yeah. That's right. Change, change the sequence leading up to this point and then say, okay, do you still want to do what tit-for-tat is telling you to do, or would you rather do something else? Right. Okay. I think I got it. Cool. All right. Go. Okay. What's your answer? My answer is no. No, I'm sorry, that's wrong. No, no, I think that, well, if it's, if it's right, then we need to provide a sequence that proves it. Okay. So what are you thinking? Well, I was thinking actually something very similar to what we just saw. Okay. Where so tit for tat, what they're going to do is they're going to do cooperate, cooperate, cooperate, cooperate, right? That's what they normally want to do. Exactly. So what would happen if at one point, one of them defected? Okay, just for simplicity let's make it the very first point. Mm-hm. So, tit for tat number one defects, and tit for tat number two also defects. At the very first time? No, it cooperates, because it's done at the same time. Well, I mean so we're, we can feed it anything we want. So we could tell tit for tat two to cooperate. So it's sort of like we've taken over its brain for a brief amount of time. Right. So I'm not yet convinced it's going to matter for this, but the thing is that from that point on, tit for tat two is going to want to, for the next step tit for tat two is going to want to defect. That's right. And, tit for tat one would want to cooperate. Uh-huh. And that's sort of. Sucks. [LAUGH] For tit-for-tat two, right? So, I don't know, so maybe we should try to think through. What is the expected reward, for tit-for-tat two, to actually do this defect at this time? Or wait, no. So, so, sorry to, to stick with the tit-for-tat machine at this time. Well, what's going to happen at that point is, it's going to keep alternating. That's exactly right. So it's going to get the, the rewards corresponding to D versus C, C versus D, D versus C, C versus D. Over and over again. Yeah. So let's thi, let's think about it in the average reward case. So, in the D versus C, if it does D when the other machine does C, then it gets zero. Mm-hm. If it does C when the other one does D it gets minus nine. Mm-hm. And then this alternates. So if we look at the average award, which is basically what you get when the discount factor's very, very high. Mm-hm. It's scoring negative 4.5. Right. Is there any way, that it could behave against tit for tat, starting from this point that would do better than negative 4.5. Just go ahead and cooperate. Just cooperate forever? Well cooperate the next time and then keep doing tit for tat from that point on. It'll work out to be cooperate forever. On average. That's right. What will get is a minus, minus one. So not being tit for tat at that point but instead, instead turning always to cooperate would actually get it better. So the idea that is should defect at this point, is an implausible threat. Exactly. So this is not sub-game perfect. So yes, you nailed it. Yeah! Does that make sense? It did make sense. Good, alright. So that leaves open the question of, is there a way, to be sub game perfect in Prisoner's dilemma? Can I ask you a question? Sure. Before you answer that. So, I had sort of convinced myself that it didn't matter whether tit for tat number two started out with C or started out with D. I'm trying to decide whether that's actually true. 'Kay, that's a good question. So what will happen, at this from this point on if we now continue. We, you know, we took over the brains for tit for tat, and we forced them to play defect against defect. Mm-hm. And now we release that, and we let them do what, whatever it is that they're going to do. And what is th, what are they going to do? Is [CROSSTALK] They would defect forever [CROSSTALK] Defect forever. Yeah. And is there anything that tit-for-tat two machine, could do to get a better score than that? Cooperate. Yeah, so it could. Cooperating with tit for tat will bring it back into mutual cooperation. Hm. It will actually get a better score. Yeah. So, in one case, it would average to minus one and the other one, it would, it would average to minus three and minus one is better, so, you're right. Good point. Okay. So what matters is that we get we get them defecting. Right. Okay, so that makes sense. So, I, I was right that it didn't matter, although you do get slightly different answers, or slightly different averages. That's right. But in both cases, there's a way of getting a higher average. Right. Okay, cool, that's what I thought. I thought it was something like that. So now, let's go back to what you wanted to do. So, are we going to be able to figure out how to do Prisoner's dilemma in a way that is sub game perfect? Well, how about I propose a machine, and we'll see what it does? Okay. So here's a machine that is sometimes referred to as Pavlov named after the Russian psychologists who was studying gastric juices and then figured out how animals learn. So I don't know why it's called that in this particular case but, here's what the machine looks like. It says start off cooperating and as long as the opponent keeps cooperating, then cooperate. So, so far it sounds a lot like tit for tat. Yeah. If you defect the then move to the defect state. So, okay still looks like tit for tat. And again, this defect state has two arrows coming out of it. But their reversed from what they were in tit for tat. Here it says, if you cooperate with me, I will continue to defect [LAUGH] against you. But if you defect against me, then I'm willing to cooperate with you. So, that's a little strange, right? That is a little strange, yes. So, you know, the way to get me to stop defecting against you is to defect against me. And then, I I become cooperative again. So, in other words, take advantage of you until you sort of, pull a trigger on me. Seems like it. Yeah. It's a funny thing. So so again, so tit for tat is like repeat what the opponent did. Pavlov is basically cooperate as long as we're agreeing. If we both defect, I'll cooperate. If we both cooperate, I'll cooperate. But if we disagree, if I dis, if I defect and you cooperate or you cooperate and I defect, then I will defect against you on the next round. That sort of makes sense. Really? Yeah, right? I mean, it says: if you're cooperating, I'm going to cooperate. If, we're both going to start out cooperating. And then if you ever defect on me, then you have basically attacked me, and so I'm going to defect on you. Unless you start cooperating again, in which case I think that you're, you're being reasonable, because of what I did, and so now we're going to start cooperating again. Except that's tit for tat [LAUGH]. No, you're right. You're right, I, I take it back. It doesn't make any sense, too many. Yeah, it's weird. It's a little bit weird. too many V's. Too many V's. Too many V's? Mm-hm. In Pavlov. Yes. Two V's. Yeah, but you can think of them being like arrowheads. Oh well then that makes much more sense. Yeah, exactly. Okay. Yeah, so it's this weird thing where I'm going to continue to defect against you until you realize I'm hurting you, you punch me once. [SOUND] And now okay, good, we're even again. We good? Yeah we're good. Oh this is like men on a basketball court. Yeah, sure but, now here's the question. Is this Nash? So, we'll make that a quiz. So, here's a quiz. And this is, I chose it this way so that I can maximize the number of v's in one sentence. Is Pavlov v. Pavlov, Nash? [LAUGH] I think I know the answer. Alright. Let's, let's give everybody else a chance to think about it. Okay. Go. Alright, what'd you get? I say yes. And the answer, that comes from what? Well, we both start off and cooperate. And so, you are always going to cooperate and it doesn't make any sense for anyone to ever move away from cooperation. Indeed. Hence, you are at equilibrium, in particular, you are at Nash equilibrium. That is correct. So, that's very good. So, you, were able to realize that Pavlov is in Nash equilibrium. But we can go further than that. Or farther, than that. Further. We can go further than that. And show that in fact Pavlov is sub-game perfect. So, unlike tit for tat. It's actually subgame perfect. So let's let's take a look at how we could convince ourselves of that. So I think the important thing to see is that by feeding these two Pavlov machines different sequences we can get into any of these four different combinations of states. They're both in the cooperation state. They're both in the. Both in the defect state, ones cooperate, ones defect, ones defect and ones cooperate. Mh-hm. so is it the case, that no matter which state that those are in, that the average reward is going to be mutual cooperation. So let's check, if they are both cooperating, and we continue with these Pavlov machines then they will mutually cooperate, so yes. Mh-hm. Alright if they're in defect defect then what's going to happen? Well then they both agree, so then they cooperate. So they're going to move to cooperate and then they'll stay there forever, so then that'll be mutual cooperation. Awesome. What if ones in cooperation and ones in defect? Then they disagree. Right. And they move to the other state. More specifically what? Oh, I don't know [LAUGH], I can't remember. I'm trying to keep track of who, who's who. So if I cooperate. and you defect, then let's see the guy who cooperates moves to defect. And the guy who defects moves to defect. Because you, and now you agree, and so you're going to cooperate. Boom. So, when we're in the cooperate defect state, then on the first move. Let's see, you just, yeah, the right-hand Pavlov just defected, so that causes this transition. And this guy just cooperated, which causes this transition, so that we've gone to defect defect. Right. Which means that we're going to average cooperation, because [CROSSTALK] Mm-hm. That's where we're going to get stuck in the long run. Right. And the same thing works through here. Boom. That's actually very cool, and kind of counter-intuitive. Yeah. And truly neat. So sort of no matter what weird sequence we've been fed we manage to resynchronize and then return to a mutually cooperative state. So I have a question for you. Go for it. So presumably this is really cool like mathmatically because now we shold all do, we should all be Pavlovians. Like we're all Kinseans. ANd then we just kind of move forward from there. D people do this? I don't know the answer to that question. I mean other than men on a basketball course. Sure, you can always return to that. Though I'm not sure I'm aware of any analyses of men on a basketball court and whether or not You know, people have analyzed that. Hmm. But how about this. If I find out, I will post something on the instructor's comments. Okay, that sounds reasonable. So Pavlov is subgame perfect. That's awesome. So remind me again why I care that something is subgame perfect? Because it means that, so let's say that you actually, so I'm being this left Pavlov and you defect and me and you're like yeah I'm going to defect on you because I just want to take advantage of you, and you're, you're going to forgive me and I will have gotten this extra bonus for that. And what it turns out is that No, if we do Pavlov versus Pavlov, we're going to fall into mutual cooperation no matter what. So, so, so this defection that I do, this, this threat, this punishment that I deal out to you, you can earn it back, and we can go back into a cooperative state. Right. So, it's worth it to me to punish you, because I know that it's not going to cost me anything in the long run, and it stabilizes your behavior in the short run. Sure. It makes perfect sense. So it becomes a plausible threat. I like it. So this Pavlov idea actually is more general than just the prisoner's dilemma or iterated prisoner's dilemma. And in fact, led to a result that I like to call the computational folk theorem. The idea of the computational folk theorem says that you give me any two player, bimatrix game. What's a bimatrix game? Just that there's two players. [LAUGH] Okay. [LAUGH] So it seems kind of redundant, doesn't it? It does. What makes it bimatrix as opposed to two player zero sum game which you can write down with a single matrix, this is like, each, each player has its own reward matrix. I see. But you're right, I should have, I could've just said bimatrix game and left out the two player. And it's an average reward repeated game. So we're going to play. Rip, Round after round after round. And we're going to look at the average reward. Or, you can also think of it as discounted with an extremely high discount factor. Okay. So you give me one of those games. And what I can do is, I can build a Pavlov-like machine for, the, for any of these games. And use that to construct a subgame-perfect Nash equilibrium, for any of these games, in polynomial time. Wow. And, so the way that this works is if it is possible for us to have some kind of mutually beneficial relationship, then I can build a Pavlov-like machine. Quickly. If not, the game is actually zero sum like, right? because in a zero sum game we can't mutually benefit, so we cant do anything like Pavlov we're just going to beat each other up. So we can actually solve, linear program in polynomial time, and work out what the strategies would be if we we're playing a zero sum like game. And so either that works, and produces a Nash equilibrium, and we can test that. Or it doesn't work, but at most one player can improve its behavior. And by taking that best response against what the other player does in a zero-sum like sense, then that will be a Nash equilibrium. So there's three possible forms of the Nash equilibrium. But, we can tick through these, figure out which one is right and drive the actual strategies in polynomial time. Wow, that's pretty impressive, who came up with this idea? So this is a result due to Peter Stone and somebody, Oh yeah me. Oh, well that's very impressive, so you managed to find a way to sneak in some of your own work into this class? Here, let's do some more of that. Okay, I'm a big fan. And I think that's fair because I did that way back when on mimic. Mimic. So, yeah, so what the last topic that, this is, that's all I really wanted to say about the Folk theorem and repeated games. What I'd like to do now is move to stochastic games, which is a generalization of repeating games. And, talk a little bit about how this relates Back to things like queue learning and MDPs. Oh, okay. That sounds cool, almost sounds like you're wrapping up. It is, that. And that will be the end of, end of the new material. Wow. Well that means we're coming towards the end of the entire course. I know. We're going to all cry with, disappointment. And I think. And, and I just say this, you know, as a, as an idle suggestion, that the students should demand that we teach more classes. I concur. So let's get there so that they can demand. [LAUGH] So what I would like to tell you about is a generalization of both MDPs and repeated games, that is, that goes by the name of Stochastic games, also sometimes Markov games. Mm. I like the name Marcov game better, but I used Stochastic game because that's what people call it and sometimes it's good to use words that other people use. And what what Stochastic games give us is a formal model. For multiagent reinforcement learning. In fact, I like to think of this in terms of an analogy. Which is something like MDP is to RL as stochastic game is to multiagent RL. It's a formal model. That lets us express the sorts of problems that take place in this formalized problem setting. Hm. That sounds very promising. Cool. Alright so let me let me give you a, I'll start off by explaining it in terms of an example and then I'll give a more formal definition because you know, I can't not. So so this is a little game played between A and B. Oh, I should have it between smooth and curly, but At the traditionally it's played between A and B. Mm, and sometimes it's good to use the words that other people use. [LAUGH] I've heard that. I wouldn't say it quite that way. So this is a three by three grid each of the players can go north, south, east and west. And can stay put if that's helpful. And the, the transitions are deterministic, except for through these, these walls here which are called semi-walls. Mm-hm. So these thick lines represent walls that you can't go through, the thin, wall, lines just represent cell boundaries, but this kind of dashed line here is a semi-wall, and that means If you try to go through that, say by going north from, if A goes north from this position, then 50% probability A will actually go to the next state, and 50% probability A will stay where A is. So, the goal is to get to the dollar sign. And if you get to the dollar sign you get a hundred dollars. So if we ignore A for a second, what should B do to minimize the number of steps necessary to get the reward. Go left, and then go up and go up. Oh, I'm sorry. Go west, and then go north and then go north. Yeah, and what should A do ignoring B? Go east and then go north and then north. Yeah. Unfortunately these guys live in the world together, and what happens is, they can't occupy the same square. And as soon as somebody reaches the dollar sign the game ends and the other player, if the other player hasn't reached the dollar sign, gets nothing. I see. So now there's a little bit of contention. So what happens if A and B both try to go, to the same square at the same time? Let's say that we flip a coin and one of them gets to go, first and then the other one will bounce off of the first one. But that's not a problem when it comes to reaching the money. But it's not a problem, yes, right, so the money is kind of like a money pit. [LAUGH] I don't think that's what a money pit is, but okay. And so they can dive in and they both get the money, because they're in the money pit. I like it. So what do you do if you're A? How do you play this game? Oh! Let's think of another thing. Is, can you think of what, what it might mean to, to have a Nash Equilibrium in a game like this? Oh, that's an interesting question. It would mean, well, it would mean, well, what do you mean, what would it mean? It would mean that, neither one of them would want to deviate. It would mean a pair of strategies for the two players. Now the strategies are now multi-step things that say, they're like policies, right? So... Yeah. Like it's a pair of policies, such that neither would prefer to switch. So can you think of a pair of policies that would have that property. Well, no I'm not sure. I was trying to think about that. I was thinking that kind of, if I were a nice guy what I would want to do is I would want us both to try to go through the, the semi walls, and if we both go through the semi-walls we just go up again and then we, we hit the dollar sign at the same time. And that's very nice. So okay, good. So that, that seems like a cooperative kind of strategy, right? Where they're both you know, 50% oh I'm sorry, 25% of the time both will get through, both will go to the goal together. Hooray. But... 25% of the time neither one will get through and then we're in the same place we were before, so that's okay. That's right. The problem is the other 50% where one of them gets through and the other one doesn't. Right, so what do, what you do if you make it through and the other one doesn't? What do I do, if I get through, and the other one doesn't? Well if I am only going to do this the one time then I just keep going and get the dollar, and the other person loses. Yeah, alright, so what this works out to be, is that A is going to get to the goal. 2 3rd's of the time, and B is going to get to the goal 2 3rd's of the time. Mm-hm. So, alright, so if that's the case, if I say, okay, A, that's what you should do, B, that's what you should do. Then is there a way that either A or B can switch strategies and do better? Well, if B, for example, decides to go west and then go up, what happens? Yes, that's a good question. B will now make it to the goal a 100% of the time, and A will only make it to the goal 50% of the time. So B has an incentive to switch to that, to this strategy if we tell them to both go through the semi-wall. Right. So that wasn't a Nash Equilibrium. B would wanted switch this new policy. Mm-hm. Is this a Nash Equilibrium? No. Wait, is it? No. Because, why doesn't A just choose to go west east? Well, would, would A do better on average by switching to this strategy? Well let's see. no, actually. Oh, no, no, you said half the time they go through. Yeah. So half the time you flip a coin. So half the time I don't make it. Right. But half the time I do. Right. So, actually, it looks the same. It looks the same. That's right. And B would go from 1 to one half. Yeah, that's true. [LAUGH] So, it, A doesn't have an incentive to do it, but B is hoping very much that A doesn't do that. Right. So so, yeah. So that, so there's one Nash Equilibrium where B takes the center. Another one where A takes the center. I guess if, if they do, if we do this coin flip thing, it, it works out this way. If it's the case that if they both if we change the rules here. So that if they collide, neither of them gets to go. Then go, both trying to go to the center is not a Nash equilibrium anymore, because you can do better by actually going up the semi-wall. Right. And so if we, if, if collision means nobody goes through, then, suddenly, you'd want to do the other thing. Exactly. Or one of you goes through the semi-wall and one goes the direct way. Right. So we can see that there's a bunch of different Nash equilibrium here, sorry, Nash equilibria here. And that it's not so obvious how you'd find them, but it is at least clear that they exist and they have a different form than what we had before, because they're not policies instead of these otherwise simplified just you know, choose this row of the matrix. Mm-hm. Cool. Alright. So let's think about how we might learn in these kinds of environments. Oh, okay, I like that already. So, stochastic games, originally due to Shappley, have a bunch of different quantities. State, actions, transitions, rewards, and discount factors. And here's how we're going to do it. We're going to, we're going to say that s, little s, is, is one of the states. And actions could be like little a, but actually, since we're going to focus mostly on two player games for the moment, I'm going to write actions as a and b. Where a is an action for player one and b is an action for player two. Sound okay? Sure. Alright. So next we have the transition function. So, the transition function says: If you're in some state, s, and there's some joint action that's taken, like all the players choose their action simultaneously, a comma b, then what's the probability of reaching some next state s prime? And we can write rewards the same way. So there's reward for player one, given that we're in state s and there's a joint action ab. And there's the reward for the other player, the second player. And a discount factor is you know, like a discount factor. Totally makes sense. So oh its the same discount factor for everyone. Yes! Good a good point. One need not define things that way, but in fact that is the way it's always defined. Hey, not to go of on a tangent here, but sometimes I see NDP's and things like stochastic games defined with a discount Factor being a part of the definition, and sometimes not. Like it's just a part of the prob, definition of the problem or sometimes its a parameter of an algorithm. Which do you prefer? Why do you, why haven't, why have the discount factor actually listed as part of the definition of the game? I have no justification other than it's nice to have listed the things that might be important as oppose to you know, working through algorithms for while and then saying, oh yeah there's this other number that kind of matters too. Okay that's fair. I was just curious. So one of the things that I actually find really interesting is that this model was laid out by Shapley. Mm-hm. One of the, like a former Nobel prize winner. I guess once you're a Nobel prize winner, you're a Nobel prize winner forever. Yeah, [INAUDIBLE]. So, Shapley, the Nobel prize winner, and as we're going to see in a moment, this model is actually a generalization of MDPs, but Shapley published it before Bellman published about MDPs. Oh. This is pre-Bellman. So MDPs, to some extent, can be thought of as a narrowing of the definition of a stochastic game. Huh. So, all right. Let's do a little quiz. And see that we really understand the relationship between this model and other things that we've talked about. Okay. Alright so, Stochastic Games are more general than other models that we've talked about. And so, just to make that case here's a way of making the Stochastic Game settings more constrained. And, by making them more constrained, actually turning them into other models that we've talked about or could talk about. So, I wrote down three different ways of constraining the Stochastic Game model. One says that we're going to make the reward function for one player the opposite of the reward function for the other. The next one says that the transition function has the property that for any state and joint action and next state. If that's going to be equal to the transition probability for state joint action, next state, where we've changed potentially the choice of action for player two. Mm-hm. So basically, player two doesn't matter to the transitions or the rewards for player one, and the rewards for player tow are always zero. So that's, that's again, you can specify this as a Stochastic Game and then in the third case we are saying that the number of states in the environment is exactly one. So I claim that by doing these restrictions. We get out the mark-off decision process model, a zero sum statistic game model, and the repeated game model that we've been talking about in the context of, like, the folk theorem. So, what I'd like you to do is write the letters, A, B, and C in the correct boxes. Okay. Go. Alright, talk me through it. Okay, so I'm going to say that I think I know the answers for this one. And let's start with the first one. So R1 equals minus R2, which you'll notice they're equal and opposite. And in fact if you add them up, that is you sum them you end up with zero. So I'm going to say that's a zero sum stochastic game. Nice. For two, basically you're saying that for all intends and purposes, there's only one agent. Which just makes it a regular Markov decision process. Yeah. So isn't that interesting? That just by the other player irrelevant, then that's what an MDP is. It's like a game where the other players are irrelevant. Yeah, which, both of my children are like that. But okay, I think that's pretty cool. And in fact, I'd be right in saying that R2 doesn't have to equal to zero. As long as it just equals to some constant. Yeah, that's, I mean, constant. Actually, depending on how you think about it, it could be, we could just ignore the whole R2 thing and just say that. As far as the first player is concerned, since the second player really has no impact on anything. It doesn't matter. But the reason I put that in is I got kind of scared that like. I feel like if I lived my life and knew that my actions effected the state and my rewards, but they were also effecting the rewards of somebody who didn't matter. Like I feel like that would actually still have an influence on me. Sure, but then the way you get around that is you would say, well, your R1 is actually equal to your R2. Oh. [CROSSTALK] It would somehow [UNKNOWN]. So, so if I had gone like that, wouldn't that be the case then that we're saying? Oh yeah, I see. That the second player is irrelevant, but the reward, but the first player may be relevant to both. Right. Yeah, okay, yeah I like that a little bit better. Yeah, I mean, once again, it all boils down to changing the rewards. Okay, and so given A and B, I know the answer to three must be C, unless you're tricking me, and it could be A or B again. And which I suppose you could have done, you didn't say they were mutually exclusive. So let me actually argue why it would be C? Well there is only one state and since you're in a stochastic game and you're going to be continually doing this. It means that you're basically doing the same game over and over and over again, so it's a repeated game. Yeah, yeah, yeah so in particular the actions impact the rewards, but they're not going to impact the transitions because you're always just going to back to where you were. The discount factor plays the role of, of decided when the game's going to end, stochastically. And, so yeah, it's exactly a repeated game, this is the one I feel most comfortable about because this really does recover that same model we've been talking about. I like it. Cool! Now, given that we actually are now in a model that generalizes MDPs, it would be nice if we can generalize some of the things that we did with MDPs, like Q learning and value iteration to this, this more general setting. So, that's what we're going to try next. Cool. Now what makes stochastic games more interesting, perhaps, than repeated games, is the idea that the actions that the players take impact not just the rewards, but also future states. Right? And, so this is the same issue that comes up when we're talking about mark off decision processes and the way we dealt with it in that setting was by defining a value function. So, it seems pretty reasonable to try to go after that same idea again. So what I've got here is actually the Belmont Equation And let's look at this together and let's see if we can fix it because it's not quite right. Okay. For dealing with the idea of zero sum in a stochastic game. Okay so you remember the Belmen equation? We've got Q I star. Mm-hmm. So there was no I before, but Q star is the state. Is to find over state actions, so here we're going to define it over action, joint actions Mm-hm. For the two players, action pairs. The immediate reward to player i for take, for that joint action in that state plus the discounted expected value of the next state. So we need to factor in the transition probabilities So the transition of actually going to some next state s prime is s, sorry t of s, AB as prime. Right? So now, we're imagining what happens when we land in S prime. So what I've written here says, well, we're going to basically look a the Q values in the state that we landed in, and kind of summarize them, summarize that value back up so that we can use it to define the, the value of the state that we left. You with me? I am with you. Alright so if we put this in if, if we say the way we're going to summarize the value for the new state that we land in. Is we think of it as actually a matrix game. That there's payoffs for each action choices of A prime and B prime. And over all of those, we need to kind of summarize well which of those actions in this table of values that we get for s prime. Which of those values are we going to propagate forward and call the value of that state? So what we did in regular MDPs is that we said we'll take a max over all the actions or in this case all the joint actions. Uhun. So what do you think that translates to? Well you wrote down max but that doesn't make sense, that doesn't, that can't be right. Well it translates, it means something. It just doesn't mean what we mean it to mean. That's true, that's fair. So what does it mean and then, and then, how can we fix it? So let's start off with what does it mean. It means that you kind of always assume that the joint actions that are going to be taken Will benefit you the most. So, everyone, everyone is trying to make you happy so, this makes you optimistic? yeah, sort of optimistic to the point of, of Delusion? Yes, very good. Right, it just basically says that whenever we're in a state, the whole world is going to choose their actions to benefit me, and this is not what we get a say a zero sum stacastic game, but a zero sum stacastic game, we should be you know. Like fighting it out at this point. So that would work out if everybody's rewards were the same, or everybody's rewards were the sum of everyone's rewards, or something like that. That's right. If it was some kind of team based game. Mm. Or if everybody was, you know, going to sacrifice their own happiness for the benefit of Qi, or i I mean. Hm. So it's not reasonable to assume that. In fact, what was it that we were assuming when we had a zero sum game that was just a single stage? Right? Just a single game and then we were done. Oh, that people were doing minimax. Right. And maximin. So what if we changed the equation to look like that? So, what I mean by this, is when, when we we evaluate the value of a state. We actually solve the zero sum game in the Q values and take that value and use it in the, in the context of this equation. That seems closer to right. Yeah. I mean, it's not an unreasonable thing to do. It's just to say, I'm going to summarize the future by imagining that we're going to play that game that represents, you know, all the future. Sure. And I'm going to act in such a way to, to try to Maximize that assuming you're trying to minimize it, which makes perfect sense if it's a zero-sum game. Right. I was, yeah, and we're still, we're still acting as if there are only two people here. Yeah, yeah, that's right. It turns out that when you're talking about zero-sum it really implies that there's only two players. Because if you have a zero-sum three player game, it really is just a general sum game. You can imagine that the third player is just an extra factor that's just messing with the numbers to make things sum up to zero. So, yeah, so zero sum really does kind of focus on this two player setting. That makes sense. So we got this modified kelvin equation and we can even translate it into a form that's like Q learning. So the analog of the kelvin equation and the Q learning update in this setting would be that we. If we're in some state, there's some joint action that's taken, there's some pair of rewards that comes and some next state that's visited that the Q value for that state, joint action pair, is going to be updated to be closer to, the reward for player i plus the discounted expected value, or sorry, the discounted Summarized value or v value of the new states as prime, and we'll again, we'll use mini-max to summarize what the values are in that new state. I like it. And that equation is sometimes referred to as mini-max Q, because it's like the Q learning update but just with the mini-max operator instead of a max. That makes sense. So, if we set things up this way, we actually get some wonderful properties coming out. So here are some things we know about this set up for zero-sum stochastic games. Value iteration works, so we can actually solve this system of equations by using the value iteration trick, which is to say, we initialize these Q values to whatever and then we just iterate this as an assignment, right, we just say, you know, equals. So value iteration works. This minimax Q algorithm converges under the same kinds of conditions that Q learning converges, so we get this nice, you know, Q learning analogue in this multi-agent setting. The Q star that's defined by these equations is unique, so we iterate it and we find it and it's just, there's just that one answer. The policies for the two players can be computed independently, that is to say, if two different players are running minimax Q on their own and not really coordinating with each other except for by playing the game, that the policies that they get out will actually converge to minimax optimal policies. So it really does solve the zero sum game, which is maybe not so surprising because, you know, they are trying to kill each other after all. [LAUGH] Yeah. So, the idea that they'd have to collaborate to do that efficiently would be weird. [LAUGH] I never thought about it like that, but, yeah, that would be weird. The, this update that I've written here can be computed efficiently, which is to say in polynomial time. Because this minimax can be computed using linear programming. Yes of course. And, finally, if we actually iterate this Q equation and, and it's converging to Q star, knowing Q star is enough to figure out how to behave optimally. So we can convert these Q values into an actual behavior, again, by using the, the solution in the linear program. So it's just like MDPs of value iteration with Q-learning? Exactly. It's like we've gone to, to a second agent and it really hasn't impacted things negatively at all. This is, this is all the, pretty much all the things that we want, come out. There, there are some things that don't come out. For example, in the case of an MDP, we can solve these, this system of linear equations in polynomial time. Not just by value iteration, but we can actually set up it as a single linear program and solve it and be done in linear time or, sorry, not linear time, polynomial time. This is not known to be true in the zero-sum stochastic game case, it's not known whether it can be solved in polynomial time. Hm. So there, it is a little harder as a problem, but it's, you know, not harder, not deeply harder and not harder in a way that matters in a machine learning setting. Cool. So this is really great. So let's, let's try to take this same approach and see if we can deal with general sum games. Okay. So okay, so let's think about General sum games, so not zero sum any more. But we're not, you know, restricted, it could be any kind of relationship between the two players. And so the first thing we need to do is realize well, well we can't really do minimax here any more. Right, because that doesn't make sense. Right. That only works with zero-sum games. Well it's only, yeah. That's, well, it sort of assumes that the other player's trying to minimize my reward and that's not the, that's not the concept of the Nash equilibrium. We'd like to do something analogous and find a Nash equilibrium in this general sum setting. So what, what operator do you think we would need in this context here? Nash equilibrium? Yeah, so that would be a very reasonable thing to do, is instead of computing mini max, we actually compute of the two matrix game, right, using Q1 and Q2, compute the Nash equilibrium of that and propagate that value back. It's a well defined notion, right, that we can summarize the value of these two pay of matrices with with a pair of numbers which are the values of the Nash equilibrium. Mm-hm. Alright, so so good. So we can do the same thing in the Q learning setting. Substitute in a Nash equilibrium. And we can call that algorithm Nash-Q, which is, appears in the literature. Nice. Oh minimax Q by the way is something that I wrote about. Nash-Q is a different algorithm. So it's not as cool, is what you're saying. Well, let's let's see how it goes. So this is now an algorithm, you can actually, well, this set of equations it's not exactly clear what it means, but we can think about turning that into value iteration, right? By turning this into an assignment statement. Mm-hm. So, what happens? Well, value iteration doesn't work. No. So, yeah, so if you, you repeat this over and over again, things, weird things can happen, it doesn't, it doesn't really converge, it doesn't really solve this system of equations necessarily. Hm. And unfortunately the, the reasoning here is even harder in the case of Nash-Q because in the case of Nash-Q, it's really trying to solve this system of equations using something like value iteration, but with extra stocasticity. And so it also suffers the same problem. It doesn't necessarily converge. There's not really a unique solution to Q star because you can have different Nash equilibria that have different values. Right. So there isn't really much hope of converting to the answer because there isn't the answer. The the policies can not be computed independently, right, so Nash equilibrium is really defined as a joint behavior, and so we can't just have two different players computing Q values. Even if we could compute the Q values. It wouldn't necessarily tell us what to do with the policies, because if you take two different policies that are both half of a Nash equilibrium, two halves of a Nash equilibrium do not necessarily make a whole Nash equilibrium. Right. because they could be incompatible. So, you know, so far so good, right? Yeah, I can't wait to see what happens next. The update is not efficient unless P equals PPAD, which is to say, computing a Nash equilibrium is not a polynomial time operation as far as we know. It is as hard as any problem in a class that's known as PPAD. And this is actually a relatively recent result, in the, in the last five, ten years. And this class is believed to be as hard as, as NP. So, possibly harder. So it doesn't really, doesn't really give us any leverage to, computational leverage to kind of break it down in this way. So that's unfortunate. And finally, the last little hope of, well, maybe we can define this kind of learning scenario using Q functions the same way we've been doing, Q functions are not sufficient to specify the policy. That is to say, even if I could do all these other things, efficiently compute a solution of, you know, build the Q values, make them so that they're compatible with each other. And now I just tell you, here's your Q function. Now decide how to behave, you can't. It's, there's not enough information. You're depressing me, Michael. Yes, so this is kind of sad. We go to the general sum case, which in some sense is the only case that matters' because zero sum never really happens. And what we discover is that we lose all, seemingly lose all of the leverage that we have in the context of Q type algorithms. Mm, mm, mm. And that's where we'll stop. Oh. So we're going to end on a high note. No, maybe we should say something before we depart. Let's do that. Come up with something positive to say. Okay. So even though things are kind of grim, with regard to solving the general sum games. There are lots of ideas that, that have proven themselves to be pretty useful for addressing this, this class of games. It is not the case that any one of them has, has emerged as the dominant view, but, but these are all really cool ideas. So here's one. You can think about stochastic games as themselves being repeated. So, repeated stochastic games. We're going to play a stochastic game and when it's over, we're going to play it again. And that allows us to build folk theorem-like ideas at the level of stochastic games. Oh, that's cool. And so there are some efficient algorithms for dealing with that. So that's one idea. Another one is to make use of a little bit of communication side-channel to be able to say, hey, other player. Here's this thing that I'm thinking about. And it's cheap talk in a sense that, it's nothing that's being said is binding in any way but it gives the two players the ability to co, to coordinate a little bit. And you can actually ultimately compute a correlated equilibrium, which is a, a version of a Nash equilibrium that you know, requires just a, a little bit of coordination, but can be much more efficient to compute. And you can actually get a near optimal approximations of the solution to stochastic games using that idea. Yeah, that's cool. Didn't, didn't I do some work in this space? You did. That's where I got the idea from. Oh okay. There's some, some work by Amy Greenwald looking at how correlated equilibria play into stochastic games and then your, your student Liam and you developed a, a really cool algorithm that actually probably approximates, the solutions. Nice. Another idea that I've heard a lot about lately, that I really like, is the notion of a cognitive hierarchy. The idea that what you're going to do is instead of trying to solve for an equilibrium, you think about each player as assuming that the other players have somewhat more limited computational resources than they do. And then taking a best response to what they believe the other players are going to do. This turns out to be a really good model of how people actually play when, when you ask them to do games like this in the laboratory. Huh. Yeah, the good news about this idea is that, because they're best responses, they can be more easily computed. That, that it's more like, cue learning in MDPs again because you're assuming that the other player is, is fixed. Okay. I'll buy that. And, the last idea I want to throw out is the notion of actually using side payments so that the players, as they're playing together, cannot only take joint actions, but they can say, hey, I'll give, I'm going to get a lot, but if we take this action, I'm going to get a lot of reward. I'm going to give some of that reward back to you, and that will maybe encourage you to take the action that I need you to take so that we'll both do better. And so there's this lovely theory by a father and son duo that they call coco values. Coco sounds awesome but it stands for Cooperative competitive values [CROSSTALK] and so it actually balances the zero sum aspect of games with the mutual benefit aspect of games. So it's, it's, it's a really elegant idea. So basically, the problem isn't solved but there are a lot of cool ideas that are getting us close to solving it. That's right. Yeah. So even though the one player and the zero sum cases are pretty well understood at this point, the general sum case is not as well understood. But there's a lot of really creative ways that people are trying to address it. So, that is good news. Okay Charles, what have we learned? And I mean specifically in the context of this game theory two lesson. That's a that's a good question. We learned about Iterated Prisoners Dilemma. Which turns out to be cool, and it solves the problem, and we learned about how we can connect iterated prison's dilemma to reinforcement learning. What do you mean? Through the discount. Yeah, so I think of that as being the idea of repeated games. Right. Let's see, what else have we learned? So we learned about iterated prison's dilemma, which allowed us to get past this really scary thing with repeated games, connected it with reinforcement learning. The discounting. And then we learned other things like for example, I don't remember. What, what did we learn? Well so the, the connection between iterated prisoner's dilemma and repeated games was the idea that we can actually encourage cooperation. And in fact, there's a whole bunch of new Nash equilibria that appear when you work in repeated games. That was the concept of the Folk Theorem. Right. The Folk Theorem. So, the Folk Theorem is really cool. And this whole notion of repeated games really seems like a clever way of getting out of what appear to be limitations in game theory. Right. Yeah. And in particular by using things like threats. Right. But only plausible threats. Right, so that was the next thing we talked about. The idea that an equilibrium could be subgame perfect or not, and if it wasn't then the threats could be implausible. But in the subgame perfect setting, they're more plausible. Right let's see and then we learned about Min-max Q. Well there was one last thing we did on the repeated games, which was the Computational Folk theorem. Yes you're right. So basically what we learned is that Michael Littman does cool stuff in game theory. Or at least he does stuff that he's willing to talk about in a MOOC. Yes, so that's, that's there's actually a technical term for that right? MOOC acceptable research? Oh, I didn't know that. Mm-hm. So all these things are by virtue of the fact that they showed up in this class look acceptable. Exactly. Alright, you're right, but then we switch to stochastic games. Mm-hm. And they generalize MDP's and repeated games. Mm-hm. Anything else? Well, that particularly got us to min-max Q and then eventually to Nash Q. But despite the fact that Nash Q doesn't work, we ended up in a place of hope. [LAUGH] We end with some hopefulness. Yeah, and you know, I think that that's actually a lesson for the entire course. That at the end of the day, sometimes it doesn't always work, but there is always hope. [LAUGH] We don't give up and that's, that's, that's how research works. Even when we have impossibility results for things like clustering, or multi-agent multi-agent learning and decision making, we still keep struggling forward. And keep learning, and isn't that what's really important. I think so. Its important for us, and its important for machines. Yes, that is beautiful. I feel like we've made it to a good place, Michael. Perhaps we should stop. [LAUGH] Well it has been, it has been delightful getting to talk to everyone, and it has been very fun getting to talk with you, Charles. And thanks to everybody for making this happen. I agree. And we have one more chance to talk with one another as we wrap up the class. And I look forward to that. So, I will see you then, Michael. Awesome. Do we get to see each other in person for that? We get to see each other in person for that. That will be fun. Yay! Okay, well, bye, Michael. I'll see you next time. Bye. See yeah. Bye, bye. [SOUND] That was beautiful. That was tiring. It was tiring. That was a very long, long, long flashback. It was like two whole lessons of flashback. Two whole lessons of flashback, hence Game Theory III. All right, well now that we're back. [LAUGH] Is there anything left to say or do we basically cover everything? Well, I think we covered almost everything, but if you recall from the flashback there were two or three things we said were probably best left for another day. Yeah. And it turns out that today is another day. Not according to that video, right. If they watch that video now, then today is that day. Every single person who watch those videos had to take several days to do it. All right, that's fair. At least emotionally, so I'm going to claim it's another day. And so we're going to talk about three things that we said that we wanted to talk about before. In particular, we're going to talk about solution concepts. And we're going to spend a little bit of time talking about mechanism design. Neat. So, let's start with some solution concepts. All right, Michael. So the first thing that I want to delve into is the notion of solution concepts. Mm-hm. Now, we've actually talked about various solution concepts and game theory one and game theory two but we never actually, I don't think, sort of explained what they were or even pointed out that we were talking about different solution concept. So do we need to talk about the concept of solution concepts? That's right, we need to talk about the concept of concepts because it's concepts all the way down. All right, so here's the basic idea. A solution concept or really just a concept is basically just a rule for predicting how a game is going to be played. So the one that we spent the most time on was Nash as a solution concept and particularly Nash equilibrium. And so in the notion in the language of solution concepts you're basically saying that there's a rule there's a kind of formal process by which some particular game's going to be played and that is you're going to play with the idea that you're getting into a Nash equilibrium. An equilibrium in general and a Nash equilibrium in particular because it doesn't makes sense. So we talked about a few others mention a few others so we talked about Nash at length. And what made Nash a Nash was that that all the players were playing in a way that they couldn't unilaterally improve. Right? So they were doing best responses to whatever it was the other player was doing. Right but actually that wasn't really what made it Nash so much. Well it was but the big emphasis there is actually on equilibrium. The idea is that you were in an equilibrium because it was in no individual's best interest to move or to change in any way. In the particular kind of equilibrium we're talking about was Nash. Okay. So in fact I want to talk about correlated equilibrium because that's a different kind of equilibrium, that's still in equilibrium, but is different from Nash and in some ways turns out to be better under certain circumstances and I'm going to even argue that in fact we use correlated equilibrium all the time in real life. I'll give you an example of that at the end. But it's correlated, so related to Nash? Yes it is and in a very particular way and I will tell you how in a moment, awesome. Okay so we're going to talk about correlated equilibrium because that's what I'm interested in but I shall point out that there are several others. There's trembling hand, there's Stackelberg competitions. There's subgame perfect. We actually talked about subgame perfect and when I say we I mean you. RIght, in the context of multi-stage game sequential games. Right, game theory too. But it's still a solution concept so this notion of concepts and solution concepts is not limited to a simple Matrix Games but actually all of these work whether the games are repeated or not. Okay. There's also Bayes Nash we did not talk about that. Coco you mentioned Coco. But I'm not going to talk about that and there are a whole bunch of others and they just kind of go on and on and on and I think when I talk about correlated equilibrium and explain how this solution concept entire back in you might see various other ways you might come up with solution concepts. I mean Trembling Hand sort of scares me a little bit but are we going to go into any details about that in Stackelberg? No I am not going to talk about this, I'm not going to talk about this, I'm not going to talk about this, and I'm not going to talk about this. And you know what I'm not going to talk about this, I'm going to talk about that, sound fair? Sounds like a segue. Sounds like a segue. In order to illustrate correlated equilibrium and why you would care about it, I want to first introduce a game. This is game theory after all and this is the game of chicken. Are you familiar with the game of chicken? I have seen the game of chicken on movies from the 40s and stuff or 50s or 60s. The General Tso part doesn't make sense to me, but the chicken part. It's like cars rushing at each other and teenage, I'm going to say boys, deciding whether or not they're going to be chickens and swerve off the road or whether they're going to be, tough guys and risk dying. Right, that's exactly right. So the two versions are two cars going towards one another, vroom vroom vroom. Voom voom voom. And who's going to swerve first? What happens if they swerve in the same direction? I've never even thought of. The other one is where they're both driving towards a cliff as fast as they can and who's going to either dive out of the car at the last minute were turn their car away. And the chicken is the one who loses the girl because this is in the 1950s. The girl's in the car going over the cliff? No, the girl's usually the one that starts the race. It's the 50's, it was a different time Michael. Understand. And what does that have to do with General Tso's, well I'm- Or game theory, for that matter. Well, it's a game right? In fact, we can turn it into a game. And how do we know it's a game? Because I can draw a matrix. So I'm going to say that there are two actions here. You can either chicken out. Okay. Or you can dare. Okay. So let's just draw a matrix that captures that idea. So I'll just put it right here. Okay. So there's a matrix you can either dare or you can be a chicken. Okay, uh-huh. Okay now, if both of you dare then you both get zero because that's what death is equal to in this world zero. I see nothing good ever happens again. Right nothing good ever happens again you just you dare. So you both got the girl, but you both died so it didn't really help. You don't get the girl if you're dead. That's probably true. Okay, so, on the other hand you could there and the other guy could chicken out in which case you get seven and the chicken gets to why two? Because you get to live another day he. Live but you get to live in constant shame. That's exactly right which is worth two. Cool. Apparently. All right. Or of course the this is a symmetric game, so if I chicken out and you dare it works the other way. And the other option is both chicken out and that's a whole lot better than being dead. And it's actually better than living in shame alone because you're sharing the shame. But it's not as good as having dared while the other chickened. Chickened out. Okay. Okay, so that sounds good? Yeah, 6 seems kind of high but I can live with that. Yeah it's math, Michael. It doesn't really matter. [LAUGH] So it's a general sum game and it's a matrix and all this other stuff about cars and everything don't matter anymore because everything is the matrix. I'm going to give you a quiz. You love quizzes. You make me do them, you have to do them. That's fair. So what are the, in fact this is not a part of the quiz but I'm still going to make you answer a question. What are the possible combinations of strategies here. Pure strategies? Yeah, pure strategies. I want to say D D, C D, D C, C C. I'm going to assume you said what I thought you said and I'm going to say yes, both can chicken out. One can chicken and the other dare, or both can dare, Okay? Yeah. Is that right? Yeah. C C, C D, D C, and D D. Right. I'm going to draw little boxes here. Those look like quiz boxes. They are quiz boxes, and what i want you to do is, I want you to check off each one that happens to be a Nash equilibrium. So, each one, independently, is either a Nash, or it's not a Nash. And if it's a Nash, I should check it. Right, and if it's not a Nash, then you don't. And check it, I mean, put a check mark. Right. Okay. We'll call this quiz the Monster Nash. [SOUND] I'm here all week. [LAUGH] Okay, so. You got it? You understand the question? I'm leaving tomorrow [LAUGH] This is the matrix. Yeah. I think I'm good. These are the things, check the Nash quiz. So, basically, check if it's a Nash, and if it's a Nash, check it. That's right. Check if it's a Nash, and if it's not a Nash, don't. Got it? Got it. All right. Go. All right, Michael, so do you know the answer? Yeah, I've been checking chickens and chicken checking and now I'm ready to check some boxes. Okay, so you want to walk me through this? Yeah absolutely, so all right. So let's look at the first one first. Okay. Chicken, chicken. Chicken, chicken. So it would be a Nash equilibrium if neither player would have any ability to benefit from switching strategies. Okay. But that's not true. Because the first player, if the first player switched to dare knowing the second player was playing chicken, the first player's score increases up to a whopping 7. From 6 to 7, that's right. So CC is not a Nash. CC is not a Nash. CD on the other hand. It's a CD. Good, that's the 2, 7 thing. So the chickener can switch to dare and that will bring that player's score from 2 to 0, which is worse. And the darer can choose to chicken, which would take that player's score from 7 to 6. Which is also not good, so everybody's better off sticking with what they got. C, D, and by symmetry, D, C, are a Nash. Excellent, excellent use of symmetry there, Michael. Thank you, so now D, D, our good friend D, D, so that's the upper left box. Mm-hm. Well, we don't have to think too deeply in this case because anything's better than that box. [LAUGH] There's no way that could be a Nash equilibrium because if either player switches they get more than 0, which is better. Okay, and that is absolutely correct. So then, we actually have two pure Nash strategies here, Nash equilibrium. Or two strategies that are in pure Nash equilibrium, or some sort of words like that. [LAUGH] I would say there are two Nash equilibrium pairs. I think that's too much. Okay, so there's two of them and they're both Nash equilibria and so we're done. Well, there could be others, right? We just were checking the pure ones. That's true, but actually I think it's worse than that. It does turn out by the way there's a mixed strategy, and I'll tell you what it is in a moment. But I think what's more interesting here is that in this case of the pure strategy which, I mean, they kind of suck, right, basically what it says is. I see. If both you chicken out it's bad, both of you daring is bad or rather they're not equilibrium. So if I know that you're chickening out and I was going to chicken out I should dare and some other way around. But the two things that are in equilibrium, neither one of them is kind of what I would call sort of stable in the medicines in that, okay, so here's what we'll do. I'll dare and you chicken out. How does that sound? That sounds like our relationship. [LAUGH] Okay, well, then let's try it the other way. You ask me. Okay, I'll chicken out and you dare. How does that sound? No, no, the other way around. You dare and I chicken out. How does that sound? No, the other way around. I dare? Yes, you dare. That doesn't sound right at all. [LAUGH] I out dare you. You chicken out. Right, well, okay. That's a Nash equilibrium, and it's true that if we're both there that's where we will stay. But I don't want to do that. This actually brings up a couple of points. One is something that we kind of brushed over in Game 3, 1, and 2, which is this idea that yeah, there's a Nash equilibrium as if that solves the problem. But we never, if there are many Nash equilibria, we never kind of said how you would pick one over the other. And I think in this particular case, I would never. The person who is being asked to chicken out would not want to do that one, they would want to do the other one. So what if we play a game to decide who gets to be the C and who gets to be the D. Right, well, I think that's a good idea. That's in fact really what, sort of, a mixed strategy is, right? You could, well, maybe it's not. I was going to say in particular, we could play a game of chicken, and whoever wins that gets to be the the D, the dominant player in this game of chicken. But then of course how you decide who's going to get to play which role in that game of chicken, and then it's chicken all the way down. It's chickens all the way down being eaten by turtles. Every level the hierarchy tastes just like chicken. That's true. Okay, see, so it's just sort of not a big win. But there is a way, usually we get out of these kinds of things by coming up with mixed strategies. And it turns out there is a mixed Nash equilibrium here. I will not quiz you on it, however. I will just simply tell you what it is. All right so here's the mixed strategy. The mixed strategy is that everyone just dares, chooses to dare with probability one-third. Really? Yeah, that works out to be a Nash equilibrium. We'll leave it as an exercise to the reader to convince themselves of that, but it does actually work out to be the case. And so that means we both, okay, we both dare with probably a third, which means two-thirds of the time we chicken. So there's going to be some probability that we both chicken, some probability that I chicken and you don't, some probably that you chicken and I don't, and then the lowest probability is we both dare, boom! Right, so in fact, I'm glad you said that, because that's exactly how we would figure out how good a strategy this mixed strategy is. Right. So, let's do this. So, let me ask you a question. What's the probability that we both chicken? Four-ninths. And how do you get four-ninths? I looked at your notes. That's the problem with being in the same place when we're doing these recordings. Well, I'm pretty impressed that you can actually read my handwriting, I'm pretty sure. No, I cannot only not read it, but my eyes are old enough that I can't read it anyway even if you write it nicely. So, no, what I did is, I took if the probability of dare is a third, then the probability of chicken is two-thirds. And we're independently choosing that, so the probability that both of us choose it is two-thirds times two-thirds, or four-ninths. So what's the probability of one person chickening and the other person daring, in particular C, D? C, D, so that's one-third times two-thirds, which is two-ninths. Mm-hm. And by symmetry? Nine-seconds. Sorry, two-ninths. Mm-hm. And then dare, dare would be one-ninth because it's dare times dare. Right. And let's just double check that four, five, six, seven, eight, nine. Whoo-hoo! Wait, no not one-third, the bottom thing. Thank you so much. Okay. And so, now we can figure out from this what our expected, let's say, my expected reward would be for playing this Nash equilibrium. Right, so let's say you're player one. So this is you. So then if we do C, C, what is your, actually you're the row. What is your a payoff? 6. So, we can just say 6 times four-ninths. What's the payoff if you chicken out and I dare. 2. 2. And what's the payoff if you dare and I chicken out? 7. And what's the payoff if we both dare? 0. 0. So we add all that up, multiply these numbers, add them, and what do we end up with? You're asking me? [LAUGH] Yeah. You're the one who can do arithmetic. You're right, it's 4 and two-thirds. Really? Yeah, we just did that by keeping the denominator the same. So, that's 24 plus 4, which is 28, plus 14, right? Mm-hm. Which is 42, plus 0, 42 over 9 is 4 and two-thirds. This is actually pretty cool, and the payoff is 4 and two-thirds on average for you. And by symmetry it's actually 4 and two-thirds. Which I like better than both of us chickening out. Right, both of you chickening out is- No, no, sorry, sorry. That's better. I like better than me chickening out. [LAUGH] Right. And then I'll only get two. Right, and so actually doing this, even though occasionally we will, well, in the game we'll both die, but of course we aren't going to die, we're just going to get 0 utility because all that other stuff doesn't really matter. Okay. We're going to do this. This is actually pretty good, right, this is a pretty good pay-off. Okay, I agree with that. In fact, for either one of us, if we get in the short end of the stick for the C,D,D,C, this is actually better for us. That's true, the worst of us, or sorry, not the worst of us. That doesn't sound right at all. But the least fortunate of us is much happier with this outcome than the C, D. Right, and by the way, we're actually happier with his outcome than if we had uniformly randomly chosen between them. And we're better off with this outcome than if we had done C, D, and then like split the winnings, which would be 4 and-a-half. Yeah, so it's the same thing as doing half, or going by half, right? It's 4 and-a-half, 4 and-a-half is less than 4 and two-thirds. Those are, in fact, the same thing in this case. Got it. This is why it's something like a third and not something like a half. So, that's pretty cool. So, this seems like a pretty good win, but I'm going to claim that we can do better. Hm, okay. Okay. So, let's remember this 4 and two-thirds and keep it around for a little while, and let me try to convince you that we're going to do better. Okay? Okay. Here's what we're going to do that's actually going to be even better than dying one ninth at a time. What we're going to do is we're going to introduce a third party, so here's Here's Smooth Mm. And here's Curly. Yep. And then, here's my best sort of drawing of Chris who's helping us with this class, our course creator. Course creator [CROSSTALK] I like course creator better because then it's CCC. CCC. [CROSSTALK] The course developer CD which is on the screen. That's true, course developer Chris, OK we'll do that. Does sound like a good hip hop name. You know what we should do? We should do some kind of wrap of some sort. You're going to have to teach me because I don't know how to do that. Okay. Well, we'll talk about it later. Okay. So, we're going to put our little CD here and he is going to stand between us so he's going to act as a kind of go between, and what he's going to do is he's going to make a decision for us of a certain sort. S, here's the decision that we're going to have Chris do. Chris is going to Write down three cards. One's going to say (C, C), one's going to say (D, C), and one's going to say (C,D). Three cards. Three cards. Okay? Yeah. And he's going to shuffle those cards and then pick one of them. So in other words he is going to uniformly, randomly choose among (C, C), (D, C), and (C, D). Comedy. Okay. Yeah, yeah. Okay, I get it. So, you'll notice that's three of the four strategies. It's three of the four strategy. Yeah, you're missing Deedee. Deedee, which is the worst one. Yeah, so you should put that one in. No. Okay. Well, I could put it in and give it and assign it a probability of zero. But I like that you're their cards and they have equal probability. Right, they'll have equal probability, so uniform choice. All right, so are you with me? I guess so. Okay, so Chris randomly picks one of these three things. Neither of us knows what it is and we don't know anything about what the other has but Chris gets to pick up this card. And then what Chris is going to do is something very simple. Chris is going to inform one of us, perhaps both of us, one after the other, and Chris is going to, let's say for the purposes of this discussion, Chris is going to inform one of us of what that person's action is. Let's say Chris picks c,c. Chris will turn to me and will say, I want you to choose c. Now, Chris would also turn to you and say, I want you to choose c. Okay? And there's no conflict there, because we can both chicken. Do whatever you want to do. I'm just telling you. Here's what the card says you should do. All right card says you should chicken out. And he's going to force me to do that now? No, let me do one more example though before I really answer that question. All right. Let's say Chris had accidentally and that is uniformly probabilistically chosen the comma c. Yes. Then Chris would turn to me and say the card says You should do D. Chris would turn to you and say the card says you should do C. But I would know that he was actually just making me chicken out because that's what people do. No, that's because that's what the card says. Now let me be a little bit clearer here because I wasn't. These three cards are known to both of us. And we know that there's a uniform probability of getting any of those three cards. And we trust Chris enough that he's not going to always give you the good thing. Yes, he's going to with probability one-third. Okay. Do that, okay? I trust Chris. I trust Chris. Look at that hair. How could you not trust Chris? Okay, so Chris is going to pick one of these cards, and then tell each of us what it is we're going to do or what we're being asked to do. Okay. But not tell us what the other person is being asked to do. Okay. So that's important, got it? Yeah, that's interesting. Okay. because I can use inference. You can use inference. So in fact, let me ask you what inference you should use. So- Bayesian inference, or you didn't ask it, I'm sorry. I didn't ask it. So, in fact, I'm going to, because you said Bayesian. I didn't say Bayesian. You just said Bayesian inference, I'm going to make this a quiz. As punishment for saying Bayesian, okay. Yes, how dare you. So here's the quiz. Let's imagine that Chris shows you C. What should you do? Should you do C or should you do D? And if Chris shows you D what should you do? Should you do C or D? So I have to figure out everything because those are the only two choices right? Yeah. Okay. This is how life works Michael. Luckily there aren't a lot of things. All right, so let me just make sure and so I know that it's a third a third a third I know that Chris is actually going to show me my piece of whichever card he picks. And so, that gives me the ability to probabilistically tell what you're going to A pick, which means I can actually evaluate both my choices of action and figure out which is better. So are you solving the quiz now? No I'm just making sure I have enough information to sell the quiz. Do you have enough information to sell the quiz? I think I do. I think you do or as much information they give anyway. So are you ready? Ready. Go. Okay, Michael. So what's the answer? I don't know. Yes you do. I mean probably it's going to be C and D. Because why would Chris tell me to do something he thinks I shouldn't do? Yeah. He's not that kind of guy. That pretty much But Actually, Chris But to figure that out, I have to figure out a lot of different values. So it's, I need scratch area, like maybe on Do you? the right. I bet you don't even need Really, alright so [CROSSTALK} alright, alright, alright, alright. Well, if I'm shown D, then I know that Chris has C. No. Not Chris, Smoove. If I'm shown D, I know Smoove has C, and I can't do any better. Right. So I'm going to take that. So show me D, I'm going to do D. Done, that is correct. Well, that's assuming that you're actually going to do what you're told. [LAUGH] But this is interesting right. You'll notice that this whole game is symmetric Sure we will have to do the other box. Yeah, so what's good for the goose is good for the gander. What's good for Smooth is good for Curly. So certainly if I saw our smooth because this is me of course, so D it would still be D, right. It looks a bit like you. Yeah, right. So, again, I need to figure out the top box. Let's figure out the top box. Do you need scratch area? Maybe. Alright, so let's think of it this way. So if I'm shown a C, actually I guess it's 50/50. What the other players going to be shown what Smooth is going to be shown. That's right. If you're shown a C, then it's got to be one of these two cards and so Smooth will see is see half the time. And see a D half the time in that situation. So and that means that my payoff for doing C. Okay, I'll help you out here. Your payoff for doing C. Is going to be the average of two of the boxes. Which boxes? The C, C box, the bottom right. Mm-hm. And the D, C box. Which is the one above it. Well, let's think of it this way. So I'm chickening and you're choosing whether or not to dare or not. If we both chicken I get a 6. If you dare and chicken I get a 2. So that means 6 and 2 is 8 divided by 2 is 4. So I average 4 for chickening. So with probability one-half you'll get a six with probably one-half you'll get a two and that happens to be four. Good, all right now. If I dare you are randomly either daring or checking in. If you dare, I get zero and if you chicken I get seven so on average that's three and half. What would you do? I'm better off chickening. Look at that. And by symmetry. When I am shown D. You're also going to be better off chickening so I should actually do D. So I can actually kind of predict your behavior as well. We should all just do whatever Chris says. Exactly which I'm sure Chris loves to hear. It's not going to happen, but I think Chris sort of would like to hear that. So what have we just done? We have just created a situation where if you're told to do C, you will not change your behavior. If you're told to do D, you will not change your behavior and the same is true for me. What do we call that? It's a kind of equilibrium. It's a kind of equilibrium and in fact it's what we call a correlated equilibrium. And why is it even equilibrium? Well, it's an equilibrium because there's no need to change given a set up, and it's correlated because we have a sort of information item, a bit, a thing, an object, in this case cards or Chris representing the cards in this case where we can. Associate what we've got with what you likely have. A decision that we want to make with the decisions that you want to make and that correlates. Interesting. And this creates a wonderful little incentive to do the right thing in this case and or at least to create an equilibrium. Now we were very particular about this. If I had thrown in the fourth card and I had made them all say one-quarter, one-quarter, one-quarter, we basically would be back where we were before. So constructing these actual distributions and coming up with the right ones is something that requires a little bit of work, but I just want to show you here that it can be done. So we have created a correlated equilibrium by creating this Chris thing that tells each of us what we ought to do. We know a little bit about how that process is done, and so we're going to do what Chris asked us to do. That's really cool. I think I have some questions though. Ask me some questions. You said things sort of fast. But just to be clear, you're saying that there could be other distributions on the cards other than uniform that also have this property, this correlated equilibrium property? I suppose there could be. I actually haven't worked that out. So you didn't pick a half by solving it out and there was some unique answer. You just kind of it you never heard right sorry a third, a third, a third for the three cards. You didn't do you can solve for that you just picked it and it worked. I happened to do that. And then there was a question of am I better off with this then doing the mixed equilibrium. Well that was a question that we started with yeah. So I guess we should answer that question would you like to answer that question? Yes. Make it a quiz? [LAUGHS] Okay. All right. Okay, Michael so here's the quiz. What's expected value of the correlate equilibrium? Expected value where the expectation's taken over the card choices. Yeah, that thing. And what I get in those different cases. I think I can do that. I think you can too. I have faith in you. So, are you ready? Do you have enough information? The answer is yes, by the way. Yes, by the way. Excellent. Okay, so go. Okay, so Michael are you ready? Do you know the answer or how to compute the answer? Well, I feel confident that I know how to compute the answer. I was trying to do it in my head, but then you know math. So two thirds of the time, Chris is going to tell me to see the chicken. Yes. And we worked out it's on the top of the screen there that I score four on average in that case. So I would write down like two-thirds times 4. Okay. So two-thirds x 4. + one-third times what I get in the case where I'm told D, which we did clearly last time and got 7. So it was one-third x 7. Okay. So now you can see why I was having a little trouble doing it in my head. So eight-thirds plus seven-thirds is like fifteen-thirds, which is like 5? That is correct. Yes. Good job. So that's so cool. So that's not quite, actually, we're still better off by both chickening out. [LAUGH] But maybe we have some dignity this way we have less utility. Wait, wait two things, let's point this out. You skipped something. I did? 5 is greater than 4 and two-thirds. Five is greater than 4 and two-thirds, so we're better off following this than choosing the stochastic equilibrium. Which was the only fair one that we had had at this point. Right. So this is a way that we can actually both do the same and do better than the other way that we can both do the same. Right, so this is better than that But you pointed out that we're still better off when we both chicken out. Yeah. So why won't we both chicken out again? In the case of this particular equilibrium? Just in general. Why would we not both chicken out? You just said we're still better off by both chickening out. And you're right, because 6 is greater than 5, which is greater than 4 and two-thirds. So why won't we both choose to chicken out? I'm not sure. It seems to me that Chris could tell us our cards. And we could just yell over the wall, we're better off just chickening out. Don't don't D on me And what's going to happen if I know you're chickening out? You're going to D on me I'm going to D on you I see. I'm going to not take that risk Because it is not an equilibrium. I see So what's nice about this is that it's better than this. 5 is better than 4 and two-thirds. I see. I see what you're saying. You're saying that 6, 6 is a fine score, but it's not an equilibrium. It's not an equilibrium. And 5, 5 is a pretty good score for an equilibrium. It's pretty fly for an equilibrium. Yeah, that's exactly right that. That's cool. So we got this power out of the magic that is Chris. We got the power out of the magic that is Chris. He is our little correlated thingamajigenbob, our object that provides enough information to the players so that they know enough about what's going on to make a decision that's better than both of them. But again, remember what's important here, right, is that in the correlated equilibrium case, neither side knows what the other is doing but knows something about what they could do because they know something about the card. Bayes' rule. That's right because given the card, I know something about the others. Given my card, I know something about your card, okay, uh-huh. Okay, so correlated equilibrium better than natural equilibrium. Now it turns out, in this particular case, it turns out that there are some neat little facts. But before I go in this neat little fact, set of facts, I want to point out to you that we do this all the time. That Chris is actually a stand-in for objects in the world that provide information all the time. I find that hard to believe. So let's go back to our original game of chicken. In particular, I just met Chris for the very first time yesterday. So he can't that common. No, this is the ideal Chris. [LAUGH] So do you know in fact what the ideal Chris is in the game of chicken? The ideal Chris in the game of chicken would be like a random number generator that is on the street yelling cards at us. Close, we actually have that. Do drive, Michael? I do drive. I don't drive a car that's as nice as your car but I drive just fine. I didn't ask you for a value judgment about how well you drive, but you do drive, right? Yeah. Okay, and I'm going to claim that every day when you drive you're actually playing a game of chicken, especially whenever you come to an intersection. I'm pretty sure my parents don't look like this. [LAUGH] Well, that's just the wrong universe. All right, so I will, okay. So when I come to an intersection, it is definitely the case that I can either stop at the intersection or I could go through the intersection. And I'm better off going through it, than I am just stopping and waiting. Mm-hm. And I'm really bad off if I go through but at the same time someone else is going through. Yep, you're basic playing chicken and dare all the time, except instead of going at each other directly, you are going at each other at let's say, right angles for most intersections. And the good case is I get through without having to stop. The bad case is, I wait forever while people go through. And even worse case is we got to go through it once and we crash. This is exactly the game of chicken. So how do we avoid this? Well, we look at each other. No. There's a stop sign there. And what is a stop sign there for? Make me stop and look. To tell you what to do. [LAUGH] But it always tells me the same thing. Yes. It's always telling me to stop. But we know what the rules are. The rules are, with a particular stop sign, if you stop, you need to stop and look to see what else is going on. We have a more generalized form of this stop sign though. In busy intersections in the middle of the cities You're thinking of traffic lights. That's right. No, traffic lights aren't random. Well they're sort of random in the sense of when you show up to them. Interesting. And so it gives me one of two signals basically dare, meaning go, meaning go through the intersection. That's green. Or chicken, meaning stop, meaning wait at the intersection. There's also one in the middle, by the way, which means go faster. But that's not really important, let's just stick with the red one and the green one. So what is this traffic light telling you? It's telling me my half of the strategy. We're correlating our behavior with the other drivers. Right because when I see red, what do I know people coming the other way see? I don't know. I guess if I see green and I know that everybody else has the red, so it's clear for me to go. Right. So that's the really important thing is it won't show us both green. Right, if we have a chance of crashing. And that's just like showing us both D. Yep. Which is the thing that doesn't happen in the list of cards. That's exactly right. So in fact- That's crazy. All of this is just a bunch of probabilistic blah blah blah blah. It's really just a traffic light. Okay. And these things are very important. And we all sort of a obey these things because it mostly makes sense. Even in weird countries like the U.K., where- [LAUGH] you're allowed to run through a stoplight or at least to go through a stoplight as opposed to sit there forever at certain times of night when no one else is coming. You still stop because the red light tells you to stop because that's the right thing to do. And if you don't do it, there's chaos. The point here is that we create objects in the world that provide each of us with some information about what others are doing, which allows us to play these kinds of games without necessarily crashing into one another. We've come a long way since the 50s. [LAUGH] I see. Okay, so let's review quickly. Okay so by way of wrapping up on this let me just point out three quick facts about correlated equilibrium. Cool. Okay, so here's the first one, correlated equilibria or C.E. as a cool kids call them can be found in poly of the time. So this is a good thing even though I didn't tell you how I found the one third, one third, one third, zero correlated equilibrium in the previous example. You can find that in a reasonable amount of time. I found it in even more reasonable amount of time, which is I looked at the example and I wrote down the numbers. [LAUGH] Which is also polynomial. Okay, in fact it's linear. Okay, so. And that's not so true of Nash equilibria, that Nash equilibria are hard to find. Yes, they are which is unfortunate. No but it, okay, it gets a nice advantage of correlated equilibrium. That's true but Nash sounds so cool. Well in any case. Okay, the second fact is all mixed Nash equilibria are in fact correlated. So this means that correlated equilibiria exist. That comes from what we discussed or that there's always at least a possibly mixed Nash equilibria. Cool. And of course pure Nash equilibria are themselves mixed Nash equilibria. Therefore correlated equilibria exist. Gotcha. Okay. Finally. Wait, pure in the sense of the original definition of. Right exactly. Okay. And then the third one is, and this kind of follows from probability, all convex combinations of mixed Nash equilibria are in fact correlated. Whoa, okay. There's a lot there and we could talk about it forever but the basic idea is that correlated equilibria is kind of cool traffic lights are not a bad idea and coming up with them isn't impossibly hard. Unlike all that other stuff you talked about before with POMDPs where there are undecidable and life is terrible at least here we can actually find pretty good correlated equilibrium. Or at least we can find correlated equilibrium. That's a relief. It is a relief. Okay, so I think I'm ready to go on to the next thing. Here's sort of the last thing. I guess I'm kind of done with solution concepts, right? So I said that we've done Nash Equilibrium. We did Subgame Perfect. We have now done Correlated Equilibrium, and we're not going to talk about any of these other things. So I guess we're done. I was really hoping that we would talk about at least the Coco thing. I think we said in a summary that there's more to say about that. Well there's more to say about everything, Michael. But that doesn't mean that we have to say it. Just because we can do something it does not follow that we should. So but, I mean, Coco is interesting in some of the same ways that Correlated Equilibrium are interested in. Sort of sort of goes a little bit beyond it in some ways. I feel like it would kind of round out the story really nicely. Okay, well, you know I'm a big fan of story rounding-outing. So [LAUGH] With TD TD MDT. I am more than happy to let you do that. Would you like to? Sure! Yeah, I can do that. Okay, well then, here's the pen. Great, so if you just don't mind. Wait, no, aah! [BEEP] Great, now I will take it from there. All right, awesome, I get to tell you about coco values. So coco stands for cooperative-competitive values. Coco stands for that? Well, the coco part of coco value stands for cooperative-competitive. Okay. And this is an idea due to Kalai and Kalai because you need as many of those as you can get. Mm-hm. Here is an example of coco values that feature some people that you may recognize. So this is Curly and Smooth and the bananas. So Curly and Smooth would like to get bananas, turns out Curly is too short to get any bananas. So true. And Smooth is tall enough to get two bananas. Yeah, it's very. But if Smooth stood on top of Curly's head, then he could actually get four bananas, he could reach these two additional high bananas. So it's like a Tuesday. So let's set this up as a matrix game. Here's a matrix version of the game. So this is suggest to you and you have two choices. You can just reach for the bananas yourself, or you could stand on the top of my head and reach the other bananas. So you're my friend in this. Sorry, Curly, you can stand on the top of Curly. Not you, I mean Smooth can stand on top of Curly's head. Okay. And Curly has the choice of either [LAUGH] allowing that to happen or not, okay. I like this game. Yeah. Believe it or not, I did not make this up for the purposes of this class. And so we can ask some questions about this. So for example, what's the nash? I'm sorry, you're actually asking me [LAUGH] what the nash is? I guess I should- It's a rhetorical question, except it's not rhetorical. It's just a question. Pure nash? If you can find one, sure. Okay, well, is there a pure nash? I mean, I've just seen these numbers for the first time. Yeah, but you shouldn't. Okay, sure. So I can just ask the question, all right so, if we do don't boost and reach, would either want to move? Well, it doesn't really matter if you go from reach to climb. You get the same value. Right. And it doesn't matter if- No, wait. No, you, sorry, you, right, you, Smooth. It doesn't matter to Smooth- It doesn't matter to Smooth once if you're in don't boost, reach, and if you're in don't boost, reach, it also doesn't, wait. No, I'm sorry, I take that back. It does matter. It does matter, I'm reading the numbers backwards. So, the first one is for Curly, and the second number's for Smooth. See, I put them in the same order as there, where they're standing. That makes perfect sense. So if you have 0,2, so then if we're stuck on don't boost and Smooth can move from reach to climb, Smooth would not do that because you're going from 2 to 0. Meanwhile, if we're in don't boost, reach, and Curly can change, then Curly would go from 0 to 0. And it doesn't really make a difference. I guess don't boost and reach is a Nash equilibrium. So, let's see, I think there's probably there's a symmetry thing there. So we know don't boost and climb is not a Nash equilibrium. For the same reason that 0, 2 is one, 0, 0 is not, because I'd be going from 0 to 2 if I were Smooth. So there's no point in worrying about that. What about boost and reach? Well, let's see. That's 0, 2, and so Curly doesn't need to move, doesn't have an advantage to moving from boost to don't boost. But Smooth would want to move because you'd be going from 2 to 4, so that is not Nash equilibrium. Okay. Now let's look at boost and climb, so boost and climb, let's see, it's 0, 4. So if I went from boost to don't boost, I would still stay at 0, and so that doesn't help Curly any. And I wouldn't move from climb to reach because I'd be going from 4 to 2, and so therefore, that is also a Nash. All right, now, which Nash equilibrium do you think would be more beneficial? Well for Smooth, it would definitely be boost and climb. Yeah. What about for like the society of these two people, like if you just add up the total retrieved bananas? So that would be 2 versus 4. Yeah. So in fact again, boost and climb would be, on average, more benefit. On average. Or the sum for the society. Yeah, right. So on average is all the same thing. Right, but it sort of doesn't matter to Curly, but it would matter to Smooth, and it's also the case that to Nash, it doesn't really matter. Both of these are Nash equilibrium, so they're both perfectly acceptable from a Nash equilibrium sort of point of view. Yeah, this is like the example that I did with chicken, where you have two Nash equilibrium, and they're both kind of equally good from Nash's point of view. Although in that case, they're very different from the two players' point of view. Here, I suppose either way, Curly gets nothing. So really, it's just about making Smooth happy. I like this game. [LAUGH] So okay, sure, fine. So that being said, can we see anything about correlated equilibria? I mean, I'm sure we can, right? We can, any makes Nash equilibrium is a correlated equilibrium, but I think this one's a little different right here, right? Because no matter what happens, Curly gets 0. So that means that anything's going to be equally okay to Curly. Right. So Curly is not really going to participate in the equilibriumness or lack thereof. Right, so really, this is all about getting to boosting and climbing. Yeah, though I think you said that any convex combination of a Nash is a correlated equilibrium. That's right. So it should be everything like 0, 2, 0 to 1, 0 to 2, 0 to 3, 0 to 4, 0 to 5, 0 to 6, all the way up to 0, 3, 0,4. We know that all those have to be Nash, correlated, they have to be a value of some correlated equilibrium that is like 0, 3, for example. Maybe this is not a great example. But I want to say if we were in the situation, what would we we actually do? I mean if it really is kind, if we're doing don't boost reach for example. What would you do to encourage me to maybe participate in the other nash equilibrium? I guess I would just state that's what we're going to do and you have no reason not to do it. Yeah but I'd be like I have no reason to do it either. This is true, but since all the utilities are inside the Matrix, might as well just do it. I might as well not do it. Well, you gotta do something. Well, I'm going to don't boost. But why? They're both the same. They're both 0. I thought you wanted to make me happy? That's not game theory. Me making you happy is not game theory. Game theory is about utilities. So is there anything that you could do to change the utility for me? Sure. I could offer you a banana. Aha! Could share. Yes, and so the essence of what happens in Cocoa Values is that we're going to talk about making side payments. So the punch line is going to be we're going to do this thing that actually benefits us mutually. Like the sum of the payoffs is maximal. And then we're going to dispense the spoils. Right, we're going to get as many bananas as we can. And then give them out in a way that's fair sort of. A priori might not be so obvious what would be fair. Like maybe I would say I'd hold out for no I'm not going to give you boost unless you give me three of the bananas. You can keep just one. But it's going to turn out in this particular case that one banana payment actually is justifiable. That's cool because that's what I was thinking. I was thinking, look man I'll give you one banana. [LAUGH] Now what if this was forty? Would you still feel like one banana would suffice. For me? Yeah. But for me to act to actually give you a boost? I guess you would expect more. Maybe more. So I don't know that I really thought this through. But if, when the number was four like look I can get two if I want to. So, really this is about why don't we split the extra benefit? There, nice nice. Okay and that is a really powerful intuition for understanding how these things go. Okay. So here's the actual definition of Coco values that we're going to use and they come from the [INAUDIBLE] work. So the coco value of a game is going to be defined like this. Let's imagine that we've got a general sum game with a notation I'm going to use is U is the payoff to you and U bar is the payoff to not you, right? The other person. Yeah, that makes sense. Although I like that you use the bar because that means complement. Yeah. So you're saying we complement each other. This has nothing to do with me. It said you. It has something to do with you [LAUGH]. Okay, now you're just confusing me with your pronouns. All right, sorry. So I should have called the other matrix I. No wait, that's not going to help. All right, so the general sum game that we're actually solving here is U, U bar. And why don't we just compute say a nash equilibrium of that general sum game. Well there's a bunch of reasons, one is that they're hard to compute. Another is that maybe it would be indifferent to getting those extra bananas and sharing them around. So what we're going to do is we're going to define the coco value and we're going to use the coco value to actually divide the spoils. Here's how the coco values defined, we're going to take the general sum game U, U bar. And we're going to turn it into sort of redistribute all those utilities that are in those two matrices. To create two matrices, one that's purely cooperative and one that's purely competitive. We're going to create one game by thinking about the average values that the two of us would get for each of the different possible joint actions. Sure. So imagine that we would just get whatever we get and then we split it evenly. Okay. All right, that's very cooperative, right. It's very cooperative. And in fact mathematically, it's just U plus U bar over 2, it becomes the matrix payoff that we're both going to abide by. It's injuries more. Purely competitive version is for us to look at the difference between the utilities that we get and split that in half, right? So it ought to be the case that if we add these two matrices together, U plus U bar over 2 and U minus U bar over 2. What we get U for my perspective, sorry, from U's perspective. And we get U bar from the perspective of the other player. So we recover those same utilities from the general sum game. Mm-hm. But again, we've redistributed into these two matrices, one of which is purely cooperative, we get the same payoffs. And the other one is pretty competitive because one of us is playing U minus U bar over 2. And I was playing the negation of that which is U bar minus U over 2. Right, so what's good for you is bad for the other player and vice-versa. Okay. In particular, this is zero sum game now. I was actually going to ask what would happen if you were playing a zero sum game? Then U plus U bar would always be zero and in fact, this would say you do minimax on the gap. U plus U bar would always be zero. Yeah, once we've redistributed these into a new U and U bar, yes. And so right and so what's the Nash like thing with the equilibrium like thing to do when we're playing a purely competitive game and that's to take minimax. So we're actually going to compute the minimax value of that game and we're going to compute match the max max value. In other words, which cell has the highest sum payoff or average. If you say sum and average is the same. Find the cell that has the largest thing and say okay, well that is also a Nash equilibrium of the purely cooperative game. because if we choose the square that has the highest payoff out of everything, then we're going to have no incentive to switch to something else. That's right. How do we compute the cooperative equilibrium? You just replace all the things with their sum and take the biggest one. So it's like taking a max of all the cells on matrix. A max of all the of the new sales. Of the new meter, that's right. And what about minimax? Do you know I remember how to do that? No, I do, I've tried to remember which one is which, well, let's pretend I don't remember how to do it. Anything that's not a dynamic program is a linear program. And truer words never been spoken. [LAUGH] Because it's all about programming. [LAUGH] All right, so in particular, you can set this this problem up as a linear program and solve it in polynomial time. So this is efficient. At least you know in a formal mathematical computer science-y kind of sense. And so is this. Right. So we took up a game that we actually don't know how to solve because computing a Nash equilibrium can be really hard and we've broken up into two. Again, just by redistributing the wealth, redistributing the utilities. Mm-hm. We actually split it up into two games that are the same amount of utility is being spread around. But we can actually compute the equilibrium value for both of them really easily. And then we're just going to add them together. Okay, and then what does it tell us to do? Good question. So the way this actually plays out, it's easier to see in the context of an example. So let's go back to our example from the banana game. Okay. So here is the U and the U bar matrix from the banana game. This is, actually I wrote them backwards didn't I? Doesn't matter. So there's one player who gets 0 no matter what. Yep, that's the friend, that's Curly. Yeah, that was Curly before. And then there's one who can get either 2, 0, or 4 depending on which cell they end up playing. And we said that the Nash equilibria were these two corner cases, the 2 and the 4. So that gives us the purely cooperative game, which is the average of the payoffs for the two players. Which we're going to compute the value of using maximax and the purely competitive game from the perspective of the U player. So U- U bar / 2, which looks like this. So we're going to try to resolve the value by minimax. Then there's the same quantity from the perspective of the U bar player. So it's just the negation of that one. But we're going to compute the minimax value for each of these different games. So from the perspective of the U player, what's the maxmax value and the minimax value for these games? So let's start with this one. So what's the maximizing value? It's the thing you're pointing at. And that's the case where U plays the second row. And U bar plays the second column and they get these payoffs. Right we get four bananas. Four bananas that we're going to divide amongst us. Sure. So the minimax value, so what's the score that the row player, the U player the maximum score that it could get for itself. Minus one. You get a minus one. Right. So how does it do that? By picking this cell, the top row because the top row if you're trying to minimize my gains, you're going to choose the left column. If I choose the first row. If I choose the second row, you're going to choose the right column. I'm going to do even worse. Right. So the best thing I can do is choose the first row. And I know that, so I'm going to choose the first column. Good. And if we negate that, we get a completely analogous analysis. Right. So, which column do you choose? This or that, given that you're trying to maximize your value. It would be great if you could choose a second column. But I'm trying to minimize your value. So I'm going to choose the first row you can choose the second column. So the safest thing you can do is to the first column and I'm going to need to choose the first row to keep you from switching to go down to that too. Right. All right so from the perspective of U, the coco value is the sum of these two things which is one. And from U bar's perspective the coco value is the sum of the min max game and the max, max game which is going to be three. Right. Now we have to define side payments. So U's side payment, p, is the coco value from U's perspective minus the value that U gets in the equilibrium which in this case is zero but in general it's whatever the utility is according to U of the utility maximizing. Joint action and side payments for you bar are the coco value from the perspective of Q bar minus the value that U bar gets in the utility maximizing joint action. So, in this case that's one for you, minus how much does U get in that game 0. So it's going to be 1. Whereas, from p's perspective, the coco value is 3 minus the payment that U bar gets for playing the utility-maximizing action, which is 4. So we get minus 1. And that makes sense because it better be the case that they are opposite and equal. You know the transfer and now you've answered my question who gets to transfer money from the other. That's right. So, this the U player is going to get one and the U bar player is going to give one and that's the one banana exchange. So to wrap things up from the perspective of coco values, we learned a couple things. We learned that they're efficiently computable because we can do it with a simple maximization and a linear program added together. What we end up getting is a behavior that's utility maximizing for the two players and then they just split it up afterwards. You can think of it as actually decomposing a game into a sum of two games, one of which is purely cooperative and one is just purely competitive. We're just kind of shifting around the utilities. But we do it in a way that leaves us with two really nice games to work with. The result of that is unique. There is a unique coco value for each of the players. Unlike a Nash where there can be multiple different possible values depending on which Nash you pick, there's just the one coco value. It's very convenient. That is convenient, because that means there's an answer. There is an answer, yeah. We've discovered that, in fact, you can generalize this notion, not just to matrix games like we talked about, but actually to stochastic games in general. And we get an algorithm that we called coco-Q. Coco-Q is a Q learning like algorithm that uses coco values to learn a strategy in a stochastic game where they actually can make little side payments after each move. So the two players move, and then they divvy some stuff up. And then they move again, and they divvy some stuff up. And those side payments actually end up being sensitive to issues like, we just got really unlucky. Or I'm about to take an action which is very risky. And so, you're going to have to pay me to take that action and the payments have this nice interpretations in terms of kind of paying people for taking risky actions and giving them the benefits of things. It's neat because this actually converges, this Q learning algorithm converges inspite of the fact that the coco operator is not a non expansion. We've talked about non expansions in the context of generalized Q learning. This is a case where that analysis actually fails. But there's a different analysis that you can use to show that the algorithm actually converges so that was surprising to me anyway. I spent a lot of time worrying about non expansions thinking that they were the answer to everything and they're going to do everything. They are the answers to everything except things that are- Just not this thing. Cuckoo for Cocoa Puffs. [LAUGH] One thing that's worth pointing out is that what you get at the end of this is not necessarily in equilibrium, which is to say once we've computed what the cocoa values, are I might not want to play that game. I might not want to do the side payment. I would rather get my value in a national liberal or something like that. And so that means that to really make this work, this side payment mechanism has to be binding. They have to agree in advance that, whatever happens, we're going to do what the coco value says we should do. And it's somewhat beneficial, but making this binding agreement in general is not beneficial. It could be for some players, they'd rather just not do it, so it's not equilibrium in the same sense as like Nash and correlated. So we're going to have to hire police officers? We're going to have to hire police officers to divvy up our bananas. Okay, so just like the real life. So thank you for letting me have a chance to come in and discuss cocoa values which otherwise, we would not and got into. Well, I guess also should I learn from this the basically we should always be doing cocoa or cocoa? Yeah, we should always be doing cocoa and hiring police officers things, make things binding. Well, I don't know. I mean in general, you tend to get a higher sum of values from the coco value so paying the police officers probably makes sense. So it might be worth hiring them. For specific games, it's not necessarily true and it doesn't necessarily kind of fit. Another problem with the notion of coco values is that it doesn't really generalize if you've got more than two players. So this is specific to the two player setting. If you have three players, you can't split it into, I don't know, three matrices one of which is cooperative, another which is competitive, and the other which is I don't know, combustible. [LAUGH] Well, what if you did all pair-wise games? You'd have to get extremely lucky. It doesn't seem like there's a natural extension of this that works for more than two players. I mean it's an open problem and so no one's shown that you can't do it. So that could be a homework problem. Yeah, that's cool and then anyone who get, PhDs for everyone. [LAUGH] PhDs for everyone. Wouldn't that be nice? That would be nice. Okay so we've gotten through coco values. It's good, so we work together and now we've done two solution concepts in some detail. Yeah. Everybody wins. That's the thing and now we can actually do side payments to kind of deviate afterwords. So what are you going to give me? I'm going to give you back the pen [SOUND] so that you can wrap up the section three one [LAUGH] of game theory. Sounds good. So now that I've got the pen back, we're going to wrap this up by talking about something we promised to talk about at the end of the last class, which is Mechanism Design. Cool. Right, this seems reasonable right? It's like mechanical engineering. It is not like mechanical engineering except in a very abstract and analogy sense. Okay. So what mechanism design is it's kind of the art the science and engineering, dare I say it, of creating games to elicit certain kind of behavior. So, so far what we've been talking about with our solution concepts and all the things we be doing in game theory one, two, and three so far, is we've talked about what happens when we play games. And at least stepping back a little bit what we get when we play games is we get certain predictable behaviors and those behaviors are natural. They're natural equilibria or maybe they're correlated equilibria or maybe they're cocoa whatever you call equilibria and other things we haven't talked about. But here what we want to do in Mechanism Design is we want to make a game so just play a game but again with the end goal of getting some specific behaviors, okay? Does it make sense? It feels kind of like inverse reinforcement learning in that we're going from the behavior that we want to a game that would make that behavior whereas in regular game theory we go from the game to the behavior. Okay, I like that if I going to get so much, I will draw the arrow that way. So here, we have a behavior we want and we're going to come up with a game that would actually lead to that behavior. I like that. And then, once we've made a game, what will happen is we'll get people to play that game, and then that will generate the right behavior. Nice. Okay cool, so it's like a sort of cycle of life thing. [LAUGH] Yeah it's not a cycle though but sure. [INAUDIBLE] Yeah, but I don't think that arrow means anything. It means it's the identity function. Lead to behaviors which lead to a game, sure. So there are lots of ways we could talk about this but I think for the purpose of this class, I think the most interesting way to kind of get across mechanisms design is just to give a few examples. So I'm going to give two, or maybe three examples, and sort of work through what is sort of easy and hard about mechanism design. I try to give you a feel for why this is actually important and how, by making little sort of specific decisions you can have a huge impact on the behavior that people have. And in fact, you could argue that everything that happens in politics, the laws that we make, they're all about mechanisms. They're all about figuring out how to influence behavior. They're all about incentivizing peculiar behavior. So the big sample that people talk about every four years when there's a presidential election and worry about, you know, the budget is one of the biggest sort of mechanism design things implemented in the last several hundred years, which is the mortgage deduction. So, if you buy a house and you take on a mortgage, you get a huge deduction, and why is that a good thing? Well because it makes buying house cheaper, is what actually encourages you to buy a house. And we think that's a good idea for a bunch of reasons but it doesn't even really matter why we think it's a good idea. We have actually constructed something to encourage people to buy houses. Okay. So, let's see if we can kind of do this in the small and kind of think about how you might go about thinking about how you would generate certain behavior in people. Okay, sure. Good. Okay Michael, so here's the first sort of real example that I want to give you and it's going to be in peer teaching, which hopefully is near and dear to some of our listener's hearts so here's the idea. The idea is that we've got a student. Let's called the student curly, and we've got a teacher. Let's called his teachers smooth and there's a bunch of questions, okay, on some subject. Pick a subject you like. Dinosaurs. Dinosaurs. Okay good, so if you want your questions on dinosaurs, you should draw dinosaurs. Okay. There I've drawn a dinosaur in white ink. Okay, so what we have here is we have a bunch of questions about dinosaurs from the It's a snow dynamicus. A snow [LAUGH] Okay, let's try that again. So what we have is we have a bunch of questions, and let's just say for the sake of this discussion that we can sort them, and order them from really really easy questions to really really hard questions. And what I really mean is that down here, people with very little knowledge about dinosaurs can answer people up here. You really have to have learned a lot about dinosaurs, and it kind of moves between the spectrum from easy to hard. That make sense? Yeah, I guess as a coarse approximation. Sure, and so what we're trying to figure out and this is important the goal here is to figure out what incentives we can give to both the student and to the teacher. So that learning happens over time so the goal is learning and what we need to do is design mechanism. And mechanism in this setting means like a payoff function essentially. Yeah, because we're talking game theory, so in the end we're always going to be talking about some kind of payoff. Some way providing utility for taking some kinds of actions in the world. Now, it's important course this is not just simply a one shot game. This really is a case that room will be taking actions over a period of time because of course learning is going to happen over a period of time. And we want to figure out how we're going to get someone to kind of get out these questions. This is all we've got right. We've got learning, we got a teacher, we got a bunch of questions we know they're easier or easy and hard and we're going to try to figure out what we want the teacher to do. And what we want to learner to do so the learning happen, okay. So we need to design a mechanism that will ensure the goal. And just to be clear is your assumption that answering questions actually helps you learn or is this just assessment? This is a sort of assessment. This is assessment but of course if you can reliably answer questions then it suggests that you know something. Okay. Does that make sense? Yeah, I guess I'm just wondering to what end, right? So, what kind of questions should be asked. Ones that help me to figure out that you've learned. That you've learned. Yeah. Okay. Now, other stuff can be happening here. And there's you know whole complicated things. Readings can happen in between. There can be tutorials or whatever but we're trying to make this game really sort of simple to illustrate the idea. So in between questions things might happen. Okay. And perhaps answering the question requires that you go off and you do some reading or do some work. Which could promote some learning. Which could promote some learning. Some of these questions might be. Take implement five supervised learning algorithms on two datasets that are interesting run them and write a talker to say for example. My gosh, that's a terrible assignment. Yeah, but you will learn something in. Yeah, you'll learn that some people give terrible assignments that cause you to learn. I think that's what really important here. So are you with me? Kind of understand what it is we're trying to accomplish here? I think so, yeah. Okay, cool. So let me give you a proposal for some instances and you tell me what you think is going to happen. So here's a proposal. I'm going to give a point every time the learner answers a question correctly. That makes a lot of sense. That means a lot of sense, so you basically get credit for being right. The learner gets credit. The learner gets credit for being right. Okay. I'm also going to give + 1 to the teacher, and for the teacher, I'm also going to give them credit plus one every time the learner answers correctly. Good, so their incentives are aligned. Yes, their incentives are aligned. The learner gets credit for demonstrating they've learned, and the teacher gets credit for demonstrating they the least know something. So, what do you think is going to happen here? That learner's happy getting answers right, and the teacher is happy when the learner gets answers right. So they should work together to maximize, I guess not necessarily learning, but ask easy questions. If you ask easy questions, then the learner wins and the teacher wins. That's exactly right, I think. So hopefully it makes sense. So if let's say this question or questions way down here are so easy that anybody can get them right. Then if I ask questions down here, the learner will get them right. So it's good for the learner, but also because the learner will get them right that's good for the teacher. Right, but that doesn't seem to really make sense for this problem, in that we really want to be asking questions roughly at the level where the learner is. Well, that makes sense. But let me ask you one other quick, before we get there, let me ask you one other possibility. Since this doesn't give us what we want, right, basically this says always stick to easy questions. Right. And we'll both do well. What happens if I say okay well the problem here is I've incentivized the learner in a way that kind of makes sense, right, learner wants to get a correct answer to good questions, right. But I've incentivized the teacher to go easy on the student. So why don't I make it so that the teacher loses a point every time the student gets something right. So now, the teacher is not incentivized just to give easy questions. What's going to happen here? Well, part of me feels like that now, it just seems like that the right thing for the teacher to do then, from the teacher's perspective, is give hard questions. The learner should try to answer them, but learner's not going to be able to answer. Right, so what I should do is just immediately go to all the really, really hard questions about dinosaurs. You can answer them. You don't have the background and the sort of fundamental math and calculus to answer the questions about dinosaurs. And so you're going to keep getting them wrong, but the teacher is going to do just fine. And in fact, not only will the teacher lose a point every time the learner answers something correctly. Let's say actually, the teacher gets a point every time the learner answers incorrectly, which is sort of the same thing. Yeah. But allows me to say the teacher racks up points- Yes. By making you do things that are wrong. Okay? So this is just kind of broken, right? The kind of obvious thing to do here where I'm going to give the student credit for demonstrating thing, demonstrating they've learned things, they have knowledge. And give the teacher credit for either demonstrating that the student has knowledge or doesn't have knowledge, gives me bad answers, and in particular, doesn't help me with the goal of learning. It doesn't help the student and it doesn't help the teacher. It just gives them whatever credit they're supposed to get when they can bother to get it. So ok, if the teacher getting + 1 for the learner getting something correctly ended up making things too easy, and a minus 1 made things too hard, then right in the middle is a zero. So what about the teacher get zero for learner answering correctly. Well, the teacher get zero for the learner answering correctly, then it doesn't matter what the teacher does. So, maybe the teacher can do the right thing now. But why? It's just like the coco example you've given before, what reason do I have. And we certainly don't want this to be the teachers high payment. [LAUGH] Although. Would that work in this case? No, because now that the learner would want to get to maximize the side payments, then we're back to the original problem that you said. Which is, if the teachers paid off for the learner giving correct answers, then the questions are just going to end up being too easy to be meaningful. That's right. And there's really no side payment for the teacher to give the student. But you said something earlier that I really like. You said, what did you say, you said something, before he went down this path of the minus one and getting hard questions, you said something about what the teacher ought to be doing. The teacher ought to be asking questions, I don't know, so that the learners kind of getting half of them right. So I like that, so let's sort of take a step back and kind of see what that would sort of mean. Let's imagine that these spectrums of questions really do line up with not just easy and hardness, but actually line up with understanding. Which is sort of the assumption that we made, right? That anyone can answer the easy questions, but as you know more, you can answer the hard questions. That means the chance of you being able to sort of, these questions are aligned, not just in terms of easy and hard, they're actually aligned with some underlying notion of what you actually know. That your knowledge is, which I'm just writing as understanding here. Okay Okay, now furthermore let's imagine you're understanding were here, okay? This is really where you are on this understanding scale. Where Curly is. That's where Curly is. Okay, in this linear scale of knowledge Yeah so this linear scale of knowledge about dinosaurs that apparently requires some background in calculus, you're basically here on this linear scale. Well what would that mean for you to be right here. Well it probably means that for questions way over here, those really are easy for you and you'll always get them right. And for questions way out here, they're too hard for you and you're probably going to get them, almost certainly get them wrong. But one of the interesting things about being right at some particular level of understanding is that you will occasionally get some of these things just on this side of your understanding right. And you will occasionally get things just on this side of your understanding wrong. Really kind of understanding is like a probability function. And I'm going to pretend it's Gaussian, because that's what we do in machine learning. And just kind of say the probability of you getting a question right depends upon where your understanding is. And as you move far in one direction you will always, always get them right. Far in another direction you will always, always get them wrong, but right around your level of understanding there's some noise there. And you'll sometimes get the ones that are just beyond your understanding right, and sometimes get the things that are just below your understanding wrong. We actually talked about this in the machine learning class you might remember when we talked about self play and learning how to play games. Do you remember this? Was it a racquetball example? Right we used a Racquetball example and we talked about how if you play someone who is much much better than you are then basically you never learn anything because you always lose. That's like the hard questions. Those are like the hard questions. And if you play someone you're much much better then you also never learn anything because you always win. That's like the easy questions. Right, and in fact the right thing to do is to play someone who's just a little bit better or just a little bit worse than you that is right around where you are. And then you'll actually learn something because the exploration makes sense. And sometimes you get positive feedback, sometimes you get negative feedback, and you learn how to modify your actions and to learn. Okay, that makes sense to me. Right, and so by analogy that's what's happening here. If I can give you questions right around where your level of understanding is, then you might learn something. Good. So given that highfalutin boughten taking it all the way back to racquetball. What do you think a good set of incentives might be? I don't know I mean it's like, what I wanted to say was, well you want to be getting like half of them right. And so we could pay off the teacher. Well we I guess we pay the learner off for getting things right because that's really what the learner should be striving for. But we can pay off the teacher for keeping things around a half. But that doesn't really make sense because then the teacher could do like I'm just going to randomly choose from a really easy problem or a really hard problem, and on average that will be a half. So it'll really not be asking any questions that are in that happy zone. That's good so actually what you just said is pretty important. And it seems like a really bad idea. No, no it's actually really important. You said something really important. Let me point what you said. You said that if I tried to pick a half or any distribution really. Okay. Then he teacher can always just choose questions to kind of get that distribution. And sometimes they'll be incentivized to, I'll just pick easy ones or hard ones where I know I'm going to get the distribution that I want. And that sort of misses the point. But what if I don't allow you to pick the question what if instead I require you to tell me where this line is? So, the teacher isn't getting to pick questions anymore? No, the teacher's going to do, the teacher is going to say, here's where I think the level of understanding it. Kay. And then, questions will be generated accordingly. Ooh, maybe from Gaussian. Maybe from a Gaussian, and in fact, what I'm going to suggest we do is that we're going to say you need to pick a line. Now, why should you pick one line over the other? Because I'm going to set up the incentive, so that you get plus one any time the learner answers correctly something they should get wrong. But you also get plus one when ever the student gets something incorrect that they should get right. So, what does that mean? So, if you say, well, this is where their level of understanding is, what you're really doing is you're kind of picking two questions, right? You're you saying, look, this is a question that they should get right and this is a question they should get wrong. But if you actually identified where their level of understanding is, then, sometimes they'll get some of these things wrong that they should get right because they're easier, and sometimes they'll get right some of the things that they should get wrong. Sort of set up the problem that way. Okay. Right? And I'm going to get credit for that. You, the teacher. You, the teacher, I'm the teacher, going to get credit for that. Well, let's see, what does that mean? Well, that means, in order for that to work, I must have identified, more or less correctly, where your level of understanding is as a learner. because if you set it too high, the learner's always going to get things wrong, but there are things that the learner should get wrong, so there's no points to the teacher. And if the teacher sets it too easy, the bar down towards the bottom, then the learner's never getting anything wrong. And when he gets things right, it's things that the learner should get right. And again, should just means below the bar set by the teacher. That's right. So, you'll get some points by doing that, but you won't get as many as if you get it right where they ought to be. So, the maximum comes when you've done this 50-50 thing, right, where you're getting points for the learner stretching a little bit and getting a little bit above and scoring well, and you're getting points for, I guess tricking the student in a sense. [LAUGH] I wouldn't say tricking, what you really is you're sort of testing where their understanding really is, right? So, if it's the case that if identified this correctly, will still get some things wrong, we would think you would get right in, you'll get some things right that we think you would get wrong. You really want to do it on both sides. And so, if I move this line too far to one side, I only get points for one side of the bar. If I move things this far, all the way over to the right, I only get points for the other side of the bar. I want to get things for both sides of the bar. Mm-hm. So, that's only going to happen, if I can identify roughly where your level of understanding actually is. That's clever. That is kind of clever. Did you come up with this? I came up with this by listening to a guy named Jordan when he told me this is what he ended up doing with real class of, I think, fifth graders. Interesting. Now, by the way, there's something subtle here, we actually aren't finished with this. We just described this as if it only happens once. Really, what we did is we spend our time talking about the teacher, and that makes sense, because the teacher is this complicated incentive function, right? So, the teacher's going to get credit for basically identifying where your level of understanding is. So, let's say I've done that, I know where your level of understanding is. I put the line here. Good. What should you, what it's going to happen for you as a learner. Your goal is still to maximize your points. So, how do you maximize your points? Well, learner's perspective, just try to always answer correctly. Right. So, how are you going to start answering things correctly, getting more and more things right? By learning? By learning, by actually moving your actual bar to the right. I see. Because then, you will not get these things wrong anymore, and you will get these things right. And so, you will increase the number of points that you get. Got it. So then, this will cause you to learn more so that you can get more things right. And then, what's going to happen to the teacher? Teacher's going to be unhappy, but the teacher could move the bar to the right. Right. To kind of catch up to where you actually are. Right, so the learner is incentivized to learn, and the teacher is incentivized to understand really what the learner has learned. And they will keep pushing each other until you've learned everything. Ooh, and then, no one has any incentives anymore. Yeah, but at that point, you know everything. And you're a teenager. [LAUGH] Well, I was just worried about this case where, so then the learner feigns ignorance for a little while so the teacher can move the bar down, and then, starts getting things right and wrong again. Yeah, I don't know, I guess that's not really a concern. Yeah, I don't think so. Okay. And even if it is, the teacher will immediately figure this out, and the learner will be giving up points in the meantime. Yeah. It's not really the maximal policy. I mean you'd have to get it exactly right. Right. It's just that once the learners learned everything, the teacher has no incentive anymore. Sure, but then once the learners learn everything, then you're done. When the learner is ready the teacher will appear. And when the learner no longer needs the teacher, the teacher will be killed by somebody and then you have to go off and learn Kung Fu. I think you're actually taking the plot of Kung Fu. [LAUGH] I am, I am. Okay. Well anyway I'd like to think Jordan Pollock for this. I really love this example. I was really the first example of kind of mechanism design that really sort of hit home for me. Thanks. And it seems relevant to this kind of online teaching scenario that we're in right now. Yeah, I mean it's not enough for, I mean I know I'm sure some people taking this class have actually done peer grading. And there is always this question of why won't everyone just give everybody good grades. You have to construct something so that that doesn't happen. Good. Okay, so, I'm going to give you one more example, I think. And then I think maybe we'll sort of cover the space enough. What do you think of that? That would be great. Okay. Okay, so here's another example of mechanism design and I'm going to basically break it into two bits. I'm going to see if I can remind you of a particular mechanism design that somebody famous once used in a story told in the Bible, and then point out that that's kind of ridiculous. [LAUGH] And then we're going to come up with something slightly better. Does that seem fair? I'm interested to see this, sure. Okay, so this is the story of King Solomon. I drew King Solomon to look remarkably like Curly. Fun. Yeah. I get to be King Solomon, finally, after all this time You get to be King Solomon. So do you remember the story of King Solomon? Yeah, he was wise. Okay, okay, let's go with that. So then tell me the story of King Solomon and the two women and the baby. All right, so once upon a time King Solomon was hanging around on his throne doing kingly things and he would answer questions for the various people in the country that he ruled, Judah, maybe? Sure, let's go with that. But he was known for being very wise, so people would come up to him with very difficult problems. So one of the difficult problems was two women came to him with a baby and each of the women claimed the baby was her baby, and they could not come to any kind of agreement. One of them it was actually the baby, the other one was someone who really wanted a baby, couldn't have a baby and then took someone else's baby. And so this is not the sort of thing you want to get wrong, giving somebody's baby to somebody who's not their mom. Right, so, let's see. So there are two women, we gotta give them names because I'm having a hard time keeping track. So, let's just call them A and B. A for Alice and B for- Bob. Bob. That makes perfect sense. Wait, no, Beth. Beth, okay, so Alice and Beth. And then there's a baby. And so Alice comes to King Solomon, says Beth stole my baby. And then Beth comes to King Solomon and says, Alice stole my baby. And then the baby just goes, ga, because that's what babies do. See this little curly hair thing? Yeah, that's cool. And now King Solomon does what? So King Solomon checks whether the baby is named A or B and then can actually trace the genetic roots of the baby back to the appropriate mother. No, that was an episode of CSI. What does the story claim happened? Yes, I'm sorry. So King Solomon says, all right, I've thought about this a lot and I've decided that the only fair way of dividing up the baby is by dividing up the baby, giving half to A and half to B. And so that was what he claims he's going to do, and he calls for his royal sword person to bring a sword and he gets ready to cut the baby in half, at which point A- [LAUGH] I recognize that royal sword. [LAUGH] It's a Jedi. All right, so the royal sword person says, okay, I will bring you the royal sword, and Solomon gets ready to cut the baby in half. And suddenly woman A cries out, no, don't do it. B can have the baby. No. Yes. And so B gets to keep the baby. I don't think that's what happened. No, so then King Solomon says aha, I set this up all along. I never really was going to kill the baby, let's say. And instead what I wanted to find out is who really cared for the baby, this particular baby, and that would be the particular baby's actual mother. So the thought was that the mom would rather have the baby live and be with a different mom than be sliced in half, which would probably complicate the baby's life. Yeah, actually I think it would simplify the baby's life, but okay, I'm with you on that, right. And then King Solomon could just determine that, in fact, A was the mother. And, yes, sewed the baby back up and gave the baby to A and sent B to prison? I don't remember. I don't know. Maybe he had a baby with B for B's sake. I don't know. That's not really part of the story in my head. The point, A leaves with the baby, A is happy. B is disappointed but not so disappointed because there's plenty of other babies to steal. Right, and then of course there's the poor swordsman who didn't even get to cut up a baby. [LAUGH] Okay, so this is- He does look disappointed. I do, or he does, sorry. He does look very, very disappointed. Very sad, in fact there's a single tear. This is mechanism design, right? King Solomon constructed a game with payoffs which somehow are supposed to capture what's really going on with the mothers. And based on the action was able to kind of determine what their true preferences were so really this was a game about eliciting preferences. And, in fact, that's exactly what you said. You said, by their reactions, by their behavior, King Solomon was able to figure out that A preferred that the baby live and have B get it, him. Let's say him, while B preferred mainly that A not have the baby. And by once getting preferences and saying well that all those preferences only makes sense for the real mother and the fake mother. Let's just go with that and so now I can determine the real mother is. Okay Right and King Solomon's was. Now I'm going to make a claim which is that this is insane and doesn't actually make any sense. So let me ask you this, Michael. Sure. You're a rational kind of guy. I'm a wise guy. Your definitely a wise guy. Let's imagine that you were B, lets say B is not the real baby mother, not the real baby mama. Okay. So B is not the real mother, but let's say that you're B. Okay? I want to be the king but okay sure. And King Solomon. You can be both. And King Solomon makes this offer. I will cut the baby in half. If your goal is to keep the baby or at least to make certain A doesn't get the baby, what should you actually do? All right here's what I think you want me to say but I'm not sure that I completely get it. So B Is as wise as King Solomon, because game theory assumes we have rational agents, etc, etc with infinite processing power. So B is at least as smart as King Solomon, and so B says, okay, King Solomon is going to use whatever I say to decide whether or not I'm the real mom. And in particular, if I say don't cut the baby, he'll think I'm the real mom. So B will cry out do not cut the baby. No. And now King Solomon's back where he started and that he has two identical women and two identical reactions. Right. And so if both of them say no, then King Solomon knows nothing. But A being a extremely rational person as well says, King Solomon will know that the person who actually is not the mom is going to say no facetiously and so only the real mom would say, yes cut the baby in half. No. No? No because if that were true then the other thing would, then the story would make any sense. Actually the story doesn't make any sense because first off the idea that someone would say you're going to cut the baby in half I'm fine with that. [LAUGH] As if people feel like that's a caring mom none of that makes any sense but don't have to go we can just talk about game theory and there's a little matrix here and code The best action for both of them is just to both say no. So you're saying King Solomon set up a kind of a mechanism design problem or set up a mechanism. And it was a flawed mechanism because if you understand the mechanism you know the right thing to say is just no and so no and then he has to flip a coin to decide who gets the baby? Right. And then you're doing pretty okay in that game, certainly better than the alternative, which is to say yes. Think about it. If your action in some sense is either say no, or to say yes, it was my baby, or it's the other person's baby, the equilibrium here is sort of both, if the other person says no, there's no reason for you to say yes. Right. And if the other person says yes. Then you should say no. Right, so that's a dominant strategy. It's a lot like chicken and dare. Chicken and dare has this sort of thing where you want to be the daring one if the other one's the chicken. This is like, everyone says no. Yeah, okay. Fair enough, but you basically both want to say no. There's no circumstances under which you want to say cut the baby in half. All right, all right. That's awesome. Yeah. I mean so the problem with this of course is that none of these people study game theory. So they maybe hadn't thought things through and they were just giving their original reaction. Well I'm pretty sure in the real world even people who haven't studied game theory would know better than the say sure cut the baby in half. That's just not socially acceptable. Let's go with that, I mean I don't see anything better than this. But if you think you can out Solomon Solomon, more power to you. Well, I believe that there is a way out Solomon Solomon and it really boils down to noting what Solomon was trying to do. What was Solomon trying to do? Solomon was trying, they had private knowledge of whether they were moms or not, she was just trying to get them to tell the truth. Right, and what he was trying to elicit in this particular mechanism was what their preferences were. So, this sort of private notion of whether they were the mom or not, what was happening with what King Solomon was doing was this kind of assumption that each mom held the baby and sort of with some kind of value. So mom A in this case, woman A in this case, loves the baby and assigns some kind of value or utility to having the baby around. And woman B also has a value or utility for having the baby around versus having the baby end up with the other woman. Right. And in particular, there's a value of the baby to the true mother that is higher than the value of the baby to the fake mother. Maybe, okay. And in this case, that better be what King Solomon thought. Because otherwise why would the fake mother be okay with cutting the baby in half? Again, not necessarily okay, just not as horrified. Right, well, that means there's a lower value there. Right. So there's two values that are that are actually going on and let's say there's a value of the baby for the real mother. And there's a value of the baby for the fake mother. And really the only thing that we have to sort of assume here is that the value of the baby to the real mother is greater than the value of the baby to the fake mother. Okay, which is also not so clearly a good assumption? Sure, but it certainly drives what's going on with King Solomon here. If value is like willingness to pay, it could be that B is really rich but isn't the mom and A is poor and can't really pay very much. But okay, all right, let's just go with it, so. I mean if- We want to know who wants the baby more. Right, who wants the baby more, sort of the intrinsic value of the baby to each of the women. And if you assume that the true mother cares more about the baby, that is has the baby at a higher value. Then the fake mother then believing this to be true makes sense. Okay, I'll take it, sure. And if that's not the case, then you're kind of lost anyway. So we might as well go ahead and just sort of assume that this is true. Anyway, if the fake mom had that much money, she'll just buy a baby. But that's what she's trying to do [LAUGH]. No, she stole the baby. Good point. Okay, good. So let's assume that we really believe that there is a value to the baby to the real mom and it's greater than the value of the baby to the fake mom, okay? So I'm just going to assert another kind of mechanism that King Solomon may have used. Okay. And I want you to walk me through it and see if you believe that this is good, bad or the same as what King Solomon actually did in the story. Okay? Yeah. All right, so let me write that out. Okay, so I've written down a sort of flow chart here. Okay. An alternative for King Solomon, that I'm going to claim better than what King Solomon actually did in the story which is threaten to cut the baby in half. It's certainly more complicated and sort of. Well, you think it is but maybe it's not. [LAUGH] So I'm going to read all this to you and we'll just kind of go through it and you'll see if you understand. Now, remember we're starting out under the assumption that the value of the baby to the real mother is bigger than the value of the baby to the fake mother. Okay. Okay, now here's step 0. And again value here is like willingness to pay. Yes. But we're just going to go with that. Yeah, we're going to go with that. So here's step 0. King Solomon is going to choose a fine. Okay. We'll call it F. That's greater than 0 but meant to be small. Okay. So, you know something like a dollar. Now what I mean by small? I don't just mean something like a dollar, I mean F needs to be greater than zero, so there needs to be some cost to this fine. Mm-hm. But it should be smaller than either of the values v real or v fake. Okay so B real bigger than be fake bigger than F. That's exactly right. Okay. All right, so King Solomon picks the [INAUDIBLE] and by the way tells the two women this entire process that I'm I'm going to another this is a surprise. They both know this is what they're going to have to go through. Okay. Okay. So King Solomon pick some value F. Then in step one King Solomon asks A is this your child? If they says no then B gets the child if A says yes then King Solomon then turns to B and says Is this your child? If B says no then a gets the child if B says yes, then King Solomon. So somebody lying at that point someone is lying King Solomon's not having any of it and tells a that a will pay F. Wow, okay. Okay. And then, requires that be an ounce of the value v. Some number V that is greater than F okay. Okay. Then the next step King Solomon turns back to A and he says. [LAUGH] How about now? Yeah. How about now? Is this your child? And if A says no it's not my child then B gets the baby, but has to pay $V. Okay. If A says yes then a gets to keep the baby. A have to pay V, and B has to pay F. Okay. I'm going to claim that this is better and then claim here and then cut the baby in half. At least in the listening true preferences. Interesting. That make sense. It's intriguing. Well, so. I mean this still feels, this feels weirdly asymmetrical. Well, it is in some sense asymmetrical But that's because there are underlying values are asymmetrical. No, I mean the end of the day either A or B got the baby with no fines if somebody fessed up early. If they make it past the you guys are arguing stage three then one possibility is a pays A fine. And B pays V and gets the baby the other possibility is that A pays the fine. And A pays V, and A gets the baby, and B pays F. Now after King Solomon tells A they're going to pay the fine. If we get down to here, A instead pays V and B pays F. So A doesn't pay F+V. One of them is going to pay half and one of them's going to pay. Okay,k that feels more symmetrical in short. Okay, good. Okay, so how would you convince yourself of the veracity of what I'm suggesting, that this will do a better job of eliciting true preferences of the mothers? I guess I would think through what the different, I don't know, like, ways the game can play out might be. Okay, cool. So we're going to go under the assumption so. Let's see one of these mothers is really one of these mothers is fake. So I mean I suppose it's possible that they're both fake. Let's not go. Yeah, let's let's not go that's not that's not allowed one of them is real one of them is fake because this is King Solomon and King and Knight shone like. Okay so we're going to do that. And I want you to figure out what would happen if we got all the way down to step five and A is let's say, the mother. Okay. So what would happen step five? A gets the baby. A gets the baby. A pays B and B pays F. So F is basically a penalty to B for lying. Right. And A [LAUGH] is worth the penalty to A. But it should be, well, it depends on how much B sets the value of V for. Right, so, what would B set the value of V for? What are the possibilities? Okay, it could be like, 1 million dollars, right? So that would definitely discourage A from saying yes. Okay, so I say you pick the million dollars because you think a million dollars is more than the value of the baby to the real mother? That the idea here? [LAUGH] Yeah, I guess so. Okay, so some value of V greater than V real. Okay. Which for the sake of this discussion, let's say is 1 million dollars. Okay, excellent. So, I'm going to claim that B would never do that. B would never? So the disincentive for B for that is that A can be like, you know what I can't afford a baby at a million dollars. It's more than the actual value of my baby to me. You get the baby, but you're going to have to pay the million dollars. Right, now, that's not just something that could happen, that's something that would happen right? Right. Because we've tied everything up in the value. So if V Is bigger than V real, then we get down here and ask is it your child? A will say no because paying V for the baby is more than what the baby is worth. And so B will get the baby, but B will pay V. What do we know about V? V is bigger than V real, which means it's also bigger than V fake. Got it. So okay, so we're assuming that B has a value of B fake. Right. So- Well in this case we're assuming A was the real mother and B was not. A was the real mother and B was not. Yeah, yeah, that's right. That's what I said, right? So B has a value of V fake. So then what value of V would B pick? So, B shouldn't be willing, I mean B doesn't want the baby for more than V fake. That's how much B values the baby. And if B suggests a value of V or announces a value of V in step 3, that's bigger than V fake, B might get, you know, B is at risk. Right. Right, if getting the baby and overpaying for it. Right, and having to pay more of this. Economists are weird. Economists are weird. But that's okay. So that means that not only will B not pick something bigger than V real, V would not pick anything bigger than V fake. Yeah, and in fact, in principle, B would rather pay up to V fake to get the baby. Anything less than that and if B doesn't get the baby, B.'s going to be sad for not having offered more. So at the end of the day, B's going to pick V fake as V. Okay so if B picks V fake as B, then what do we know is going to happen at this question, when A gets asked if it's your child? A's going to say yes. A is going to say yes. Why? Well because if the value that B announced is V fake and A would have regret for not getting the baby, yeah, for not getting the baby, and not bidding up to V real, no, bidding is probably not the right word, but let's go with it kind of suggesting it, or being it, A would have been willing to pay up to V real for the baby. So A would be happy to pay V fake for the baby. because V fake's less than V real. Yeah, so A will, in fact, say yes. A will get the baby, will pay less than the value for it. And what happens to B? What happens to be B? B pays a nominal fine. Right, but that's greater than 0. So if they both thought this all the way through, now we can unroll back all the way to step 2 where B is asked, is this your child? Now B is saying, I could say no and A gets the baby and I just walk away. I get zero. Or I could say yes and I'm going to end up paying a fine. Right. But least A is going to have to pay a lot of money, so ha. But it's all built into the value here,all right. So that means that since B is never going to be willing to assert a V greater than V fake, which means that B is never going to be able to make it so expensive that the real mother would say no, B has no reason to ever let it get this far, because B will pay something. B will get minus F, which is less than 0, which means if we ever get to step 2, B will bail and say no. I see. Now what if A is actually fake and B is real? But that's not the truth. It could be the truth, we didn't know that. We went to mechanism to work either way. That's right. Okay, so all right, so let's go to the bottom again. So if B is the real mom, B has a value of V real which is bigger than V fake which is A's value. So when B, announces the value, B probably should announce the V real. Because again B is going to want to spend that or wouldn't want to not get the baby. Having not spend that amount we would have regret for that, and so B is going to basically be truthful right and say what B's true value is. Which in this case is V real. At that point A has a choice. A can say yes and is going to have to pay more for the baby then the baby's worth to A. Because V real's bigger than V fake. Or say no and walk away having paid the fine but B gets the baby but at a very high price. But it doesn't matter because F is still bigger than zero. I see, so now what you're saying is, so that is a little bit weird and asymmetrical, right? So B gets the baby for V real. But if it was A who had the baby, then A's going to get the baby for V fake which is less. Right, but it doesn't matter, Your point is that because of this fact, because of the fact that if A lets it get this far, knowing that A's not the mom, it's going to cost A the fine. Right, and A will not get the baby. So that means A doesn't want to get this far, which means when asked the question, having thought it all through, A will just say no. So in fact, by saying this is the process we're going to go through when asked the question both will be truthful. In fact, all the questions will be truthful, all the way up to question one. What? All the questions. So any question that's asked is going to be answered truthfully by this design. That's right, I'm sorry, you were saying all the way up to you're going counting backwards that's right. Now well up to one. Yes up the screen to one. Right so that's exactly right. Without knowing what V real and V fake are but with everyone understanding that the baby is valued more by the real mother than the fake mother. This mechanism set them in such a way that it makes no sense for the fake mother whoever it is to ever let it get this far ever let it get to step three. So as a result when forced to ask the question the fake mother has to bale. Got it. Before it gets to three because otherwise Fake mom will lose. Clever. Therefore this mechanism will in fact elicit the true values of the baby and will allow King Solomon to know the right thing. Because of this King Solomon knows that they will answer truthfully and will find out who the real mother is. And the only person who loses in this Is the poor person of the sword. Again. So sorry. And there you go. So I think that that's a pretty interesting example of how you might kind of do mechanism design. That is really interesting. Yeah and so I slipped and called it a bit at one point because it sort of reminds me of what people do in auction design. Right. When auctions are designed it's often the case that you want to set up an auction so that people reveal their true values for the for the goods that are available and that they'll bid their true values and that the winner the one who valued the most will actually end up winning at the end of the day. Right. You try to set up a structure so that people will basically pay what it is they really want or at least will announce what it is that they really want. There are lots of variations on this and in fact some version very similar to this is sort of what e-bay does but without babies the baby in this case is the prize E-baby. E-baby. Wait a second. We have a website. Let's go get rich. [LAUGH] I think that's illegal. I think it depends. But I was just going to say that if you switch the As to Bs and the Bs to As then eBay is baby. I do not understand how you do stuff like that. But I'm impressed. All right. Okay so here we are so yeah auction and by the way you might think that this would be a natural segue to talking about auctions. But that'll be our third class that will never actually do. [LAUGH] Seem fair? At least for being honest we're getting our true value. [LAUGH] That's true. All right, so with that, there are lots of other examples that I could come up with and we could talk about. But I think that, I think I've sort of made the point here. Yeah, that's really neat. I mean, again, it feels, not to be dissing on my homie, King Solomon, but- Namesake. Namesake. Mm-hm. Isn't that what homie means? In some, in Yiddish, yes. [LAUGH] Okay, fine. So, but again, if B is rich enough, B is just buying the baby and not even paying A appropriately for the baby. Well, if B is rich enough, B probably just bought a baby. Yeah, but bought the baby for the, [LAUGH] for what, for the value, for B's own value of the baby. So, it has to be the case that V real is greater than V fake for all of this. Yeah. But let's just say it is. And if it's not, then we'll come up with some other design. And actually, by the way, there are mechanisms for solving this where it may be the case that fake's greater than real, but you sort of limit the possible values that each might get, you kind of think there's like a distribution of them. And there are clever things that you can do to sort of make this work. But well beyond the scope of this single slide, which we've done all of this on one slide, so it's well beyond- All of this and a little more. Yeah, there's just no more room on the slide, so I can't talk about it anymore. [LAUGH] So, I think speaking of game theory and true values, we have a problem. All right, what's the problem? The problem we have is that we've now come to the part of the lesson where we have to say what have we learned, and one of us drives and the other talks. Great. But we both did things in this lesson. So, I'm going to say that we kept switching turns in order to make parity work. You should do the what have we learned. All right, so [LAUGH] what have we learned? I think you basically convinced me to do the writing and now you're the one who has actually think about what we learned. No, no, what we'll do is we'll end up sharing. So you tell me what we've learned and I'll you if that's right. So there was game theory one and game theory two and they had their own what have we learned along the way, right? Right. All right, so then what happened when we came back, there were Scooby Doo noises. There were Scooby Doo noises. Then we talked about- Solution concepts. Exactly, and we listed a bunch, but we focused in on two of them. Right, and I talked about correlated equilibrium and the relationship to Nash equilibria. And sort of how they're awesome and efficient to learn or efficient to compute. What was the main idea for the equilibrium? What was the thing that was introduced that makes it work? Traffic lights. Better known as a shared source of randomization. Yes, which I don’t think I ever actually said, but yes. I heard it implicitly. Yeah, it was implicit, a shared source of randomization. In other words, that one-third, one-third that we did with the cards was something that we both knew that kind of allowed us to correlate possible outcomes. And that is again, a shared sort of randomization. And I think even convinced you that traffic lights are a kind of randomization from your point of view when you get there. So it ends up being an easier solution concept to find the values for, but it also is it requires more power. If you don't have your source of randomization, you can't even use this. That's right, but it is powerful and it allows us to often come to a better outcome than we would with a Nash equilibrium. Some definition of better. Higher expected utility. For both players, for one player, for the average player. Well, that's on the game. And I think it generalizes to arbitrary number of players, but the game made to see themselves get really big. If as the number of players grows. But that's true anyway. Yep, exactly. Okay. All right, and then I talk about a solution concept called coco values which had an even stronger assumption. Not just that there's a source of shared source of randomization, but there's actually binding side payments. Meaning that there's a mechanism for deciding who gets what once the actions have been taken and utilities have been distributed. Right, and this encourages a player who might not have a reason to do any particular equilibrium. To pick one that is beneficial to one of the other players because that person will share in the increase benefit. Right, so it encourages a pareto optimal response that is to say that in particular they're going to choose the outcome that has the largest total reward and then they're going to divide up that reward. Did you mention greater optimal when we were talking about this? No. It was implicit much like shared source of randomization. Both these algorithms it turns out have been used in the sequential games setting, the games. Most solution concepts are usually useful in all those things. You should mention one other thing which is we actually pointed out that something that we talked about last time was in fact a solution concept. But Nash. Nash and? Subgame perfect. Right. Right, by the way worth pointing out. That the King Solomon alternative. That I described before is subgame perfect. All right. Well so let's flip over to that. Which is mechanism design. We talked about. Two things in particular. We talked about peer teaching. Right. And we talked about King Solomon. Dividing up babies. Baby division. Never ever divide by zero. Now what's interesting about mechanism design, sort of the flip side or compliment of what we normally think about in playing games is that one could look at a lot of these solution concepts as in fact being mechanism design. Right, you create coco values as a way of encouraging certain kinds of behavior by setting up binding side payments. You think of correlated equilibria as working because you've created a mechanism, this shared source of randomization that will encourage behavior that it wouldn't otherwise encourage. Therefore all the solution concepts can be thought of, sort of, at least all the ones beyond the kind of obvious when the natural delivery in any way I think about that, can be thought of as a kind of mechanism design. These extensions, these things that we introduce are to induce certain kinds of behavior. And so, even if we didn't want to think too hard about mechanism design, I wanted to believe it wasn't really a part of reinforcement learning. It really is, because all of these things that we come up with as these sort of extensions solution concepts are really about getting better outcomes. And they're all about driving behavior using rewards. Right, which is so much better than Driving Ms. Daisy. So I think that's everything that we've learned. Okay, so what's next. I think we're done actually with the material. We have kind of a wrap up that we need to do like an outtro and so yes we're getting down to the to the near terminal state. Nice. Well I think it's good well this is been fun I mean I guess I will get to see you again after this because we do have a few things that we need to do, but I think we've covered all the material. Wow. Well done. Great. Well, good job everybody. Yeah good job, I guess I will see you later Michael. Bye Bye. [NOISE] Anybody there? Is that you, Charles? It is, maybe. Who's asking? It's me, Michael. I was going to do another lesson. Ooh, I like lessons. How are you doing Michael? Yeah, I'm doing all right. I notice that my favorite letter is on the screen. Yes, Charles like the c. Si si si, Senor. Yes, so remember we had a lesson that was a triple a. So this is the triple c lesson. That makes sense. [LAUGH] It would make sense. What about the triple b? Triple b happened, but you have to pay premium rates to see that one. Nice, we quadruple your tuition. No, there's no triple b at the current time, but we can think about that for later. Maybe for our third class. So, let's see. So we talked about game theory. And one of the cool things about game theory, is it let us reason about sequential decision making in agents and learning, in the context of there being more than one of them in the world. Right. So this is something that I wrote in my dissertation, is that in some sense MDPs, are very solipsistic. Wow, did you actually write that? I did. It's in my intro. Do you know the word? Of course I do. Okay. Do you want to define it for everybody? No, because that actually would be ironic. Okay, fair enough. So the idea of solipsism is that you're the only thing in the universe, everything else is just kind of there. Yes. But you're the only real thing. And so one nice thing about the game theory setting, is it actually lets us admit that maybe we're not alone. So what I'd like to do in this lesson, is actually dive in even more deeply to that idea, and talk about how the MDP or the mark off planning and learning type framework, can be adapted to be able to talk about coordinating between agents, communicating between agents, and actually coaching between agents. That sounds perfect. That's actually the kind of thing that I like to do, and I like to think about as part of my research. So I'm looking forward to learning more. Excellent. So the way we're going to do that, is actually look at different ways that Markov models can be kind of modified and redefined, so that coordination and communication, and so forth, is a natural property in the model. And we'll look at a couple different models. The first one is called Deck Palm PP's. And the second one is called Inverse Reinforcement Learning, and they kind of get at these different issues in slightly different ways. Good. I think, well we'll do that. We'll talk about it some more, later. OK. To try to motivate this idea let's look at a little example. So let's imagine we've got two agents, agent 1 and agent 2. And agent 1 has kind of a goal or some kind of reward function that it's running. And to make that reward function have high reward, agent 1 needs an apple. Now the apple is not far away but it is very close to agent two. So what are some things that agent one could do to satisfy its goal. It could walk over to the apple. Good, anything else? Well, once it gets there can say I thought you were giving me an iPad and then kick agent 2 and run. [LAUGH] I see, wrong apple. This is the food kind of apple. But, you're right that probably this particular agent 1 would would like an Apple watch. Yes, well actually this particular agent 1 has an Apple watch. Then let's see, the other thing that could happen is agent 1 could ask agent 2 To bring him the apple. Good and I guess presumably each one could just also just do nothing and just wait for agent two to for the apple to just show up using artwork. Like maybe agent two would bring it by- I guess it maybe not apples but like all men's there's certainly agents that bring each one home mints and ice cream. Yeah. Have you got ice cream in while? Yeah. Okay, and then yes. That's too bad looks like your looks like your Apple Watch exploded. That's okay, I'll just get another one. Yeah, because this one's broken. Anyway, when we're doing planning or sequential decision making we need to decide amongst these different options if you will or different courses of action that we could take. How do we usually do that in the framework that we've been talking about? Well, if we're living in an MDP like world we actually define the MDP and I think the secret sauce there is to figure out what the right rewards are. Right. And the reason I said it that way is because in the game theory case we usually call those utilities and not rewards, and I just want to differentiate between the two of them. Okay and then what we do with those? Well once we have rewards or utilities we then find a policy that maximizes them. We maximize our reward actually as we maximize our utility. It seems like we could do this sort of thing in the scenario where we've got multiple agents and they need to coordinate and communicate with each other. But it turns out it starts to get a little bit hard to define exactly what the costs are, and was the likelihood that this is actually going to work and so forth. So what we're going to do is actually think about just like we did when we talked about POMDPs. We said that they allow us to strike a balance between actions to gain reward. And actions to gain information and we don't have special like a special other way of reasoning about that all of that being folded into one model. Where actions are being taken and they might have informational impact and they might have reward impact. The informational impact is subservient to the reward in a sense right so the agent just tries to figure out. I will take an action to gain information if it helps me get more reward in the long run. And so, the DEC-POMDPs is a similar sort of idea that's going to let us put coordinating and communicating along with actions that they may become actions just like everything else like this. Ask for it becomes a kind of action and walk to Apple becomes a kind of action, and the agents can reason about whether that's a good idea or not depending on how to maximize their utility. Sounds promising. So DEC-POMDP, I've written out the expansion of what it stands for. Decentralized partially observable Markov decision, and then I kind of ran out of space. Nah, the P doesn't matter. But you know that it's a POMDP, decentralized POMDP, and so the quantities that define the DEC-POMDP are very similar to the quantities that define a POMDP, we've got a set of states, transitions, rewards, observations, observation function. The major difference is that actions are actually taken simultaneously by a finite set of agents. There isn't just one driver there is actually a whole, you know, too many emcees, really, not enough mics. Well done. There's a set of agents who actually get to have some say over how the transitions works. So we're going to say capital I is that finite set of agents, actions available to an agent is A sub little i for agent I. And the transition function is a function of the joint actions taken by all the agents simultaneously. So instead of just going from state to a next state through an action, this is actually, you could think of it as a vector with one action for each of the agents in our set. All right. That makes sense. So that really is very similar kind of quantity but we break up where the actions are happening. They have to happen jointly. And the agents themselves, their observations about the environment, are also distributed, in a sense. Agent i gets to gets to see something that's a function of the agent and the state of the world. All right, so, this is a model. In this model, all the agents are acting cooperatively, because there's one shared reward function that everybody gets. If we actually split the reward function so there's a different one for each of the agents, we get yet another model they're not going to talk about caught up. Partially observable stochastic game or POSG. POSG. POSG. I'm positive that would be interesting. Yeah, yeah so we could POSG this discussion at this point and have a side discussion. [LAUGH] More puns. The P stands for puns. [LAUGH] So, this makes sense and sounds great as a formalism goes, but I'm kind of struck. When you brought up the stochastic games thing, I was thinking the whole time, this sounds like game theory stuff. Yeah, yeah. Right, it's sort of clearly game theory when we have separate reward functions, because every agent has its own personal interests. Right, so, the DEC-POMDP setup is a kind of common interest game, might be a word for it, the notion that there's agents acting independently, but they're all acting for the same reward. So, we could model this as game theory, or is this some different way of thinking about it? It's part way between game theory and POMDPs, yeah. It has elements of both, and in fact, I guess it subsumes both. So, you can represent any POMDP as a DEC-POMDP with just one agent. And you can represent any common interest game as a DEC-POMDP with complete observability, or maybe no states, that sort of thing. So here're some of the properties of DEC_POMDPs that probably worth knowing about. As you pointed out, there's elements of game theory and POMDPs mixed together deliciously. And as a result, member POMDPs, well like I told you the POMDPs are not computable. So DEC-POMDPs are also not computable, but in the finite horizon, POMDPs are P space complete which is probably definitely kind of hard. DEC-POMPDPs are actually known to be NEXP-complete which is non-deterministic. It's like NP except with an X in the middle, non-deterministic exponential time complete. That sounds difficult. Yeah, I mean so the best that we know how to solve non-deterministic exponential time problems is double exponential time. The shortest that it could possibly be solvable is single exponential time. [LAUGH] So it's somewhere between single and double exponential. Like 1.2 exponential. Well, it could be, could be 1, could be 2, could be 1.8. As long as we have choices. One of the the beauties of this model, one of the reasons that people in spite of this computational challenge, like to think about it, is because, just like POMDPS are useful because they let you think about actions to gain information, DEC POMDPS let you think about actions to communicate. An optimal solution to a DEC POMDP has to balance the cost of communicating with the cost of not communicating. So is it worth saying this at time? Okay, that makes sense. That sounds like some of the other discussions about POM DPs, and exploration versus exploitation trade offs. Right. But it gets into some of these interesting questions about communication. So this is model that's fun to think about if you're a multi-agent person or maybe a natural language person or a met natural language dialogue person where you actually have to reason about what's worth saying. Okay. And there are some algorithms and heuristics that are known, there are some applications that are known and just the last thing I want to do is run us through a quick example, so you can have a concrete example, what it feels like to be in a DEC POMDP. Okay. Right so this should be a relatively simple example, very tiny little example, but it may be challenging. So here's what's going to happen. We are in a little four-room museum, and we're interested in together exploring the museum, but the first thing we have to do is actually end up in the same place. So we each know what state, we are in state four by the way, but you don't know which state the other person is in, you know where I am for example. I know where I am, but I don't know where you are. We win if we end up in the same state you can go basically north, south, east, west or stay put, and if we're going to move simultaneously, if I turn ends, and we're both in the same place we win okay. So normally you would have to issue these commands into the environment. Right? You walk around by yourself. But you need to tell me what, what you're going to do, so I can keep track. Okay, and all I can do is move north, south, east, and west? Or you can stay put. Or I can stay put? Yeah Okay. And for what it's worth in the DEC-POMDP model, It's a planning problem. So we can actually make up a strategy in advance. As long as we don't use it as part of the strategy. What state where, we're starting in, because we don't know that. So we can talk about what our strategy would be? Yeah. Well I can think of sort of three classes of strategies. Okay. So one would be one of us decides never were to move so that the let the other person move around. Good. That way we can't just miss it to other right, and so I could not move or you could not move right. I should see those two separate things. The other one would be we just agree on a set of actions that we will take that guarantee that we will end up in the same state. Well and maybe one way to do that is, we'll pick out like a focal point will pick a room that we should both go to. Right, I was thinking three because three is my favorite number. Nice. Okay. So, which of these strategies do you suppose is better? Well, It depends on exactly how rewards and discount factors are defined. Yeah, that's true. But if we just think about cost somehow being how long it takes us to get there. Okay. I actually guess. Going to the same room is probably the best one, because no matter where you are long as you can either take zero two steps to get there. One step to get there or two steps to get there. Good, good I think you're right actually, so let's do that so we defer to sign in advance that we're going to converge on room three. And you wake up, and you find you're in room four to start, but I don't know that. But I know what room I'm in, and now we both get to choose an action. And I would choose the action left. Assuming we aren't in the same room. I would choose the actual left. Okay. All right, and I will tell you that I put you in room three. and I take an action that is trying to also move me towards room three, and it worked. We're both Room three. We're both there are three. Hey what you want to see first. Room three. [LAUGH] This is the Salvador Dali room. Excellent. All right, so this worked out actually really well for us, because you can imagine if this is a much more complicated domain, if there's a lot of stochasticity if our task was more complicated. It might be very difficult to know what to do. For example, what if you thought you were in room four, but you were actually in a room two. And because you know partial observable states being what they are. So, you might not even know that you're not in room three yet, and it might be very difficult to get there and know that you got there. Well, I would hope that there would be some room Where we know whether we're in that room or not you never sure. I dont know I mean whenever you do this in a real museum right that's what you do look just meet you at the entrance. Well I thought it was the Jurassic room, but it was actually the Cretaceous room. This is where pick an intruder the entrance or something. I see. Yeah like a good focal of so like one like the entrance. No three was the answers as weapons. All right. Done and done. All right, so Dec-POMDPs give us a way to optimally trade off acting and coordinating and communicating, and I think we can now move on to the topic of coaching, so communicating and coaching. So let's imagine instead of the scenario we talked about before, where we had an agent that himself wanted an apple and wanted to somehow use communication to help bring about a positive outcome. Let's look more particularly at this case of, okay, agent 1 needs agent 2 to learn a particular task, like how to fetch an apple, all right. And so what we're going to try to do is figure out how can agent 1 convey the task to agent 2. If agent 2 has its own reward function, it has its own task. So we're going to try to create a a task or reward function for agent 2 in some way that is consistent with agent 1's goals. Okay, so two things. One, so when you said, we're going to try to create, you mean like agent 1 should try to create a reward function for agent 2, right? Yeah. Okay. Or to convince agent 2 to create such a reward function on its own. Okay, and so second, I think the first thing agent 1 should do is not tell agent 2 that he's trying to teach him to fetch things. I think he should probably. Yeah, fetch is not going to happen. Right, you should probably use another nicer word. But okay, I think I'm with you. Yeah, that's why I said get. Yeah, except you said fetch. [LAUGH] Yeah, it felt like fetch was a good word. All right, yes, okay, so I agree with you on that. Okay, good. So okay, so then I think I've got the task. I want to somehow get agent to get a reward function that will get it to solve some task that I wanted to solve or him. Yeah, exactly, and so there's a bunch of different ways of thinking about this, but we're going to dive into a particular one that I think is really interesting and actually quite useful called inverse reinforcement learning. What, okay. All right, so let me set this up for you. All right. Okay, so what is what does this diagram look like to you? We've got this box in the middle labeled RL. I won't tell you what that stands for. And rewards and interaction with the environment come in and behavior comes out. So it looks like that. What? Yeah, that's what it'll look like, it looks like that it looks like that. But what process does that remind you of? I'll give you a hint. Reinforcement learning. Yeah, okay. So that's reinforcement. [LAUGH] Well, I want to contrast it with inverse reinforcement. I see, okay. All right, so this if this is reinforcement learning in a nutshell, then- It is? Is it? Yeah, environment and rewards come in. Yeah. And then behavior comes out. Right so now I'm going to invert that, right? So see how I change things? I added the word, inverse. I put an I in front of the RL and I flip these two arrows. So now in inverse reinforcement learning, the learner is experiencing the environment and externally from an external source is experiencing behavior. So this is coming from those same kinds of demonstrations that you were talking about and from that it has to actually infer a reward function. So let's do this as an exercise. Okay, that would be kind of. Not a quiz. No, no that would be wrong. That would be wrong. We're in a grid world. Okay. And you have to learn a reward function. You have to learn what's good or not good to do in this grid, and you're going to get to learn it by watching me. So I'm going to control the agent. The agent's name is Frank. Okay. So he's going to go like this [SOUND] and then he stays there. Okay? Okay. So I claim that you can actually make some very good educated guesses about how rewards work in this environment, just based on that one little demonstration. I think that's probably true. So is there anything that the agent finds positively rewarding? Green. Green. Yes, good. Is there anything that the agent finds aversive? So, I could guess that blue is aversive because you avoided the blue, but that also happened to be one of the shortest path to get to the green. Yeah, that's true, that's true. So it might have just been a coincidence, but I think you have some evidence that maybe blue was something I was trying to avoid. Right. When I- What about red? So red is not something you're trying to avoid because you could have avoided it. That's right. I don't know about orange because you couldn't avoid orange to get the green. Excellent, and that was exactly the inferences that I was hoping you could make. So, again, you can learn a tremendous amount about what's valuable and what's not valuable by, not just the path that was taken, but, in a sense, by the paths that were not taken. And what the other options are or were and what the other options weren't, right? So you couldn't learn anything about orange. It could be that he hates orange, not as much he likes green. So Frank was willing to go through orange and it didn't matter where. But red, yeah, totally could've gone around red and didn't bother. Right? So that you can avoid blue and red just by going like that, but he didn't do that, so he probably is indifferent to red or something. So, yes, it's actually a really cool idea and we can actually learn things about what people want to do and what they don't want to do. How tolerant are they of like sudden stops in a car or a bumpy road? That sort of thing. How do you trade that off against how fast you're trying to get to your destination? It's hard to get people to tell you numbers for that, but it is actually not so hard to have them demonstrate, and then infer these kinds of values from their behavior. So, obviously, as we're sitting here, we can't really get into the details of the algorithm. But just to give you a kind of a hint of the form of this, there's an algorithm that I worked on called Maximum Likelihood Inverse Reinforcement Learning, but there's a whole bunch of different algorithms. This one I happen to understand the best because I helped make it up. And the basic game is here, guess a reward function. In this case, we were guessing reward functions that mapped colors to values. We then compute an optimal policy with that reward function in mind. Measure the probability of the data of the behavioral trajectories that we saw given that policy and then compute a gradient on how can the value of the reward change in a way that would make the data more likely? And then we just go back and do that again until we find something that is a local maximum. That makes sense, how does it work in practice? It seems to work pretty well. It can be slow and it can get stuck in local minima, but we haven't really had significant problems with that. It actually pretty consistently comes up with a reward function that's pretty solid. There's tricky things like keeping in mind the fact that, when I showed you the trajectory, I went through green and then I stayed there. So how do we represent that? Is this an absorbing state? And if so, do I just include it once in my trajectory? In which case, it just saw one little green, in which case it's not so excited about green. Or is it really important and it kind of sat there forever? In which case it really didn't see much of anything else. So getting getting those terminal states right can be a bit of a trick. That makes sense, okay, I like that. All right, so even though that example that we just talked about is something that I drew up on the screen. I decided I would actually turn that into a real example and ran it in Burlap with the implementation of MLIRL that's there. And this is the result. It's actually kind of neat. So, here's the trajectory and the MDP that we ran in, and here's how MLIRL decided to actually assign rewards to the different colors. So you can see that the cell that's green is assigned very high positive reward. So it is drawing the agent over to that that is a positive place to be and you can see that the blue squares are actually fairly negative. The ones that correspond to these blue puddles are given a not insignificant negative reward so they are actually avoided on purpose. One of the things that's interesting is as we talked about there's no set of evidence in this trajectory that red is good or bad and in fact it ends up getting pretty much the same reward as the background, basically zero. One thing that's a little bit more interesting is that it actually concluded that this orange strip is somewhat aversive. It gave it a slightly stronger negative reward to try to get it to avoid it and I guess that was because the demonstrated path didn't have the agent spending multiple steps there and it could have. Yeah But it didn't so maybe it was a bad place. Right there's lots of ways that it could have got. You could have come all the way over to the origin going straight down. Yep. And since it didn't it maybe try to pick a path that minimized orange. Right and so it just kind of went with that and gave it a slightly negative reward. If we gave it a bunch more examples and we could see negative whether that rather than orange really was consistently a good thing or a bad thing,it would get fixed with more training data but it's pretty neat how much you can pull out of actually one single example. That actually that's actually kind of cool and I do like that gets slightly negative. It doesn't believe that you're trying to avoid orange just that maybe you are because if you weren't trying to avoid orange. I guess if I thought about all the path that could get me from where I started to where in that up. There's a lot of them that involve touching orange more than once and since you didn't pick that one maybe you're trying to avoid touching more and more than once and since are all the same thing that would be evidence that it's something to avoid although it's only very slight evidence. So that turns it into slightly negative that's kind of cool that's kind of amazing. Yeah I think so too I mean and one of the things that I really like about this is the notion that It's actually learning, not just from the positive example of where it went, but in a sense from all the different paths that it didn't take. Right. Right and so the fact that it didn't hit blue, it used that as data. So it actually is combining this notion of your planning in the full MBP. Well, we get to see one trajectory through that, so what is that telling us about other parts of the MBP? So I guess the short version of this is you can communicate a lot even by communicating a little. And an even shorter version of that is less is more. Well no I wouldn't say that at all. [LAUGH] But you just did. I did, but I did then I read it and I realized that that's not what you meant or that's not what I wanted you to mean so yes that you can communicate a lot with a little. Less is enough maybe. I like that. That's like eight is enough and eight is less the nine so less is enough. I like that. And that pretty much sums up the the three Cs that I wanted to talk about. That sums up the different things wanted to talk about in triple C. So we talked about coordinating between agents. We talked about communicating between agents. And we talked about coaching or kind of teaching, instructing between agents to convey an idea of a reward function. So what do you think? Are you ready to through and kind of fill in the topics that we touched on in each of these? I'm going to say no and- I don't think that's on the script. You're supposed to say yes, and we're supposed to go through it. I'll tell you why because I think this is really cool, and I think we're giving it short shrift. I think there are several more examples that we could go through that would really kind of drive home this point, particularly the coaching and communication point. And I think we should do it. So let's do some more of these. So what you're saying is you didn't think I lectured enough, and that I should now on the spot make up more material. That's kind of tough. You don't have to make up more material. I mean, the material is out there, you can just very easily describe some stuff that we know about. How about doing that? No? [LAUGH] I'd rather not, I mean I feel like we covered the topic and we should just go with it. Okay, I think I can prove to you that we haven't covered the topic. And the way I could prove it to you is simply by covering more of the topic. Okay, that's fair. All right, so let's do that. So you know what, here, I'll make it easy for you. I have two or three things in mind that I think that we could talk about that would really kind of drive the point home, and I will talk about them happily. All right, I appreciate that, and I would be very interested to hear what you have to say about these topics. So why don't we stop for now, and then you can go research those topics, and we'll come back and talk about it. Nope, I'm ready to talk about them now. Wow, okay, that's impressive. Thank you. So you just want to take over. Okay, so the thing is I'm actually in Rhode Island right now, and so it's not clear to me how we can just kind of switch places and let you drive. Well, why don't we use the magic of technology? So here's my idea, Charles. You know how in Star Trek they have a transporter room? Yes, I do know that. So let's see if we can do a transporter slide. I like that. It's like the Electric Slide. Yeah, all right, so I'm going to dematerialize, rematerialize someplace else. And you'll rematerialize here, so you can drive the rest of the lecture. Makes perfect sense to me. I don't see anything ridiculous about that at all, it's good. [SOUND] Did it work? It worked. That's awesome. All right thanks Michael, I think it is awesome. No, no that's really cool, and you know what else? What? I got your almonds. [LAUGH] Curse you Kahn! Okay, Michael, so I wanted to, before diving into a couple of examples, I wanted to remind you of a conversation that we had earlier. Does this little picture look familiar to you? It looks like the Monte Carlo tree search stuff we talked about in the options lesson. It actually is. So, here's the point that I wanted, I made the point at the time, but I wanted to really kind of drive it home here. If you recall, I asked you to come up with some options for solving Pac-Man. And you came up with four of them. Do you remember what they were? Yeah, I think they're on a slide, fortunately. I just have to remember what they mean. It's like eat with three different bullets, and they're getting successively larger. But I think it's actually food pellet, power pellet, monster or ghost, and then avoid ghost. And avoid ghost. Right. And if you recall, at the time we briefly discussed that these things really do look like options. And this last one doesn't really look like an option in the sense that it doesn't have a goal and there's something you can succeed. In some sense you can only fail. And this turned out to be interesting for a couple of reasons. One, just because you came up with them, and in fact we've done this study multiple, multiple humans at various levels of expertise. And, basically, they all come up with the same four. Sometimes they'll come up with a fifth that kind of distinguishes between eating a real ghost that's edible and a ghost that isn't edible, and that kind of thing. But basically, they come with the same four. And the point here, and why I want to bring it up is that if you just ask people to give you what they know, you ask them to communicate, you ask them to coach an agent. They will often come up with things that are different from what your machine expects. And the sort of claim I want to make is that often in research what we do is we come up with wonderful machine learning algorithms, decide what information they need, and then try to force humans to give us that information. But actually, human beings will naturally sometimes give us that information, but often give us different kinds of information. And if you look at it the wrong way, you would say, well, look, these three are useful. This last one can't be implemented, so I'm going to ignore it. But in fact, even in the Monte Carlo tree search example, this last one, this constraint, actually helped us develop a better algorithm because it gave us a better rollout strategy. And, in fact, as the students listening will know, because they've read the paper on this that we made available, that it actually improves performance an amazing amount. So, the point of this is that when we ask humans to communicate, to be a part of the sort of team of how we teach and demonstrate to our agents, we need to meet them where they are as opposed to try to force them to meet us where our algorithms are. And I want to give you just a couple examples of that. That sound fair? Yeah, that's really cool. Cool. All right. So, the first one's going to be something called policy shaping. And as a preview, just want you to know the whole point of this is to say that all the stuff you talked about with the word shaping was wrong. Yeah. Let's take a look. Okay Michael, so I have brought back yet another slide, slightly modified, from the discussion on options and constraints. Do you recognize it, you remember this? Well, it's got the Pac-Man thing on it, and it has the four options or whatever that we just talked about. And it has the beginnings of a stoplight. Right, exactly right, so in fact I told you a little about this experiment that we did, where we asked people to come up with these options. But there was a second part to the experiment. There are two buttons, this is me attempting to draw what a button looks like. There's a green button and there's a red button. And what you're being asked to do is to watch Pac-Man over here run around executing actions including these options you came up with. And then occasionally you're supposed to hit a green or red button. And what do you think that the green and the red stand for? Probably green is Pac-Man did something good, and the red is the Pac-Man did something not so good. Right, that's exactly right. So this is supposed to mean good. And this is supposed to mean bad, just like you would expect. We had people do this, and they did it. They would say that's good, that's bad, that's bad, that's bad. That's good, that's good, that's good, that's good, that's good, that's good, that's good, that's bad, and so on and so forth. Now, here's my question to you. Given that people have done all of this, and they've given us lots of data on whether Pac-Man did well or did poorly. What would be the obvious thing to do with that information? The obvious thing would be, it seems like the right thing. Which is to, if you treat the good as a plus one and the bad as a minus one, and have the agent just maximize for reward. That should basically be reinforcement learning. Right, and that makes a lot of sense, right? So since we do reinforcement learning, we'd want to turn this into a reinforcement learning problem. And we would say well, every time you said something was good, I'm going to treat that as a reward of say one, it doesn't really matter. And bad say is something like that, and you get a reward for say, being in a state in taking a particular action. And that's a perfectly reasonable thing to do. And in fact this ends up being a way to get human beings to help us to automatically do reward shaping. And in fact you can show this works pretty well. And again, there's been a lot of work on this, we have provided some papers on this for the students to read about. And you do better, as you might imagine, by treating these things as reward shaping than if you didn't. Because really they're hints, as you move along, about where you ought to be and the kinds of things you have to do. But remember what I said before, that human beings actually are trying to tell you something. And they might be trying to tell you something different from what you want them to be. So, if I think back to what you just said, what you kind of said is, well look it's a reinforcement learning problem. Reinforcement learning problems need rewards, and so we should convert these things into rewards. Well, it turns out that in practice, when you actually talk to humans, and work out what they do, they're not actually doing this. When they say good and they say bad, they're not thinking in terms of actual rewards, plus one, minus one. Or plus ten and minus nine, and these kinds of things. Because of course they're not reinforcement learning experts, they're not thinking about rewards. They're actually giving you a different kind of information. So here, let me just ask you. If I told you in English, that you should hit the green button whenever the agent does something right. And the red button when the agent does something wrong, then what kind of information am I conveying, other than a reward value? Well, one way to put it, is your you're giving commentary on the behavior or the policy. That's exactly right, that really in fact, it turns out, you could convert these things into rewards. But you could also convert them into something that's a bit more direct. You can just say well you're actually telling me that in this particular state, I should in fact take this action. Or that in this particular state, I should not take this action. And this is direct policy advice, as opposed to reward advice. And so maybe, what we're really doing, is not reward shaping, or what humans are doing is not reward shaping, it's actually a policy shaping. Policy shaping. Policy shaping. Did you make up that word because I'm not sure I understand what that word means. So I did not make up the word policy and I did not make up the word shaping. No I know that. [LAUGH] No, I'm sorry that the combination of those two words. Well I'm not going to say I made it up although maybe I did, one of the papers that we're going to read is actually a paper that I was a part of the author team on. And it's all about introducing the notion of policy shaping. And it's really a very simple idea, it's sort of what I just described, that what human beings are actually doing is giving you direct policy advice. And so they're trying to shape what the policy should be directly, as opposed to trying to say something about what rewards ought to be. Does that make sense? Yeah I think so. I mean I guess now, before I was thinking about shaping differently. But now it's sort of feeling like sculpting to me, like you have a policy out there and you're going to apply pressure and smoothing in different places to have it be a little bit different. Right, so think about it this way. There's actually I think a direct way of doing this. When you do policy shaping, what have you actually done? What you've actually done is you've taken whatever the reward function was in say some particular state or some particular action and let's say was zero. And what you said is no, this is the wrong answer, the wrong value, it should not be zero here. It should actually be something else, it should be, well I don't know, three so you've actually replace the original reward function R with another reward function R prime instead this is actually the reward function that you should be learning over. And that's really what reward shaping does for you, right? I mean, if you think back to the examples that you were giving with all the reward shaping, you were replacing reward values, agreed? Yeah, though in the case where we were doing potential base shaping, we also had to make sure that whenever we sprinkled some positive reward in, there was negative reward someplace else to balance it out, so you didn't get new positive reward cycles happening. Right now that's true but regardless of where you sprinkle them and what kind of constraints you put up you still took what was originally a reward value of something like zero and replaced it with another reward value I agree with that. Plus three, minus three, seven, doesn't matter. So you change the reward function. So in exactly the same way policy shaping is about changing the policy. There's some policy that the agent currently has and you're saying actually in this state, rather than whatever it was you think you should do, you should do in fact this. So maybe I'm confirming what you already had, maybe I'm telling you to do something different, but regardless I'm overriding your policy function. In the same way that in reward shaping you're overriding the reward function. Okay, that makes sense. Right, so this is what policy shaping is all about and it happens to line up with, it turns out in practice, the way people are actually trying to convey information. And so then the real question is, what do you do with this? How do I take this information that's being given, if it's not being given in the form of rewards, we know how to do that, but it's actually being given in the form of a policy. What should I actually do about it? So I was interpreting what you described as saying, if a person gives positive feedback for an action in a state and it's not the action that the agent would have taken that state, then just replace it. Just make a modification of the policy so that, that's the action that's taken. Is that the way to do it? Well, ideally that is what you would do, right? If a person tells you, look, you should go up here, or you should eat a ghost here, you should just do it. But that actually requires something be true, which as the lawyers would say, assumes facts not in evidence. And that is, that the person's actually right. What if the person's wrong? Yeah, but certainly we should at least have that as our assumption. I mean, if we don't think the person is right then this is a really easy thing to do policy keeping. Just ignore the person. But what if the person is right most of the time, but can occasionally be wrong? I see. But what if the person's right but actually meant to say at the time step before? I see, so maybe there's a way to use all these different things as evidence and then to combine them together to get, I don't know kind of a best guess as to what the right answer is. Exactly and that's why it's policy shaping and not policy doing. So I'm going to try to give you a little quiz and see if we can kind of work out what the right thing to do is under the circumstances, where a person might tell you to do something and might not always be right about it. Seem fair? Yeah. Okay Michael, so here's a quiz. What's the answer? I'm going to say seven. That is actually correct. I don't know how you got it. All right, let me explain a little bit about about what's here on the screen. So we're going to pretend you're in some particular state. Let's call it S okay? And the agent has taken an action and there are three possible actions the agent can take. X, Y or Z. Got it? And a human being has pressed the green button or the red button and we visited this state maybe many times, and taken at each of those actions maybe many, many times. And what's going to be in this table is the number of times when we've been in that state, and taken a particular action that the human being has said either good, that's the right action. Or bad, that's the wrong action. Now, what I'm going to ask you to do is, I'm going to put some numbers in here, and then what I'm going to ask you to do is tell me what's the probability that a particular action X Y or Z is in fact the optimal action. Given that the human being has told you is or is not and the last little bit of information that you need is the probability. That a human is actually correct whenever they give you a green or red signal. Okay, because remember last time we set this whole thing up by pointing out that a human may not always be right. Okay. So for the first quiz, I'm going to tell you that the human is always with 100%, 1 probability, absolute certainty, correct. Okay. And I'm going to tell you that when we've been in this state, the human has told you exactly once that X, is in fact the correct action, has never told you that for Y or Z, and has never told you whether any of the actions are incorrect. So I want you to tell me the probability that x is the optimal action, y is the optimal action or z is the optimal action. Given the information in this table. Got it? I think so. Can I ask some clarification questions? If you insist. [LAUGH] Well, so I don't think it matters, but is there any kind of prior? We're just saying that sometimes the agent takes, x, sometimes y, sometimes z. But other than that, the agent has no reason to believe x, y, or z, one is better than the other other than this feedback that we're looking at. Yeah, for the purposes of this discussion, yes. And then, I had another question which is you're saying that the person says whether such-and-such is a correct answer. Could it be that two of them are correct answers like there's two perfectly reasonable ways to act from the state both X and Z are fine. Is that the case or is it exactly one is the right answer. So that is an excellent question Michael and I'm going to say you tell me. Boy, if there's no prior, there always is a prior so I guess it to you like a uniform prior. There's no reason to think that one is more powerful than the others. All right and then if it could be the case, that multiple of these answers actually can be correct. I think we need to we need one more piece of data if we're going to do some probability reasoning. Which is not just what's the probability that the human is correct, but what's the probability that humans going to give feedback. Well, I'm going to claim it actually doesn't matter. When I say there's a 0 here., that means that the human doesn't have to give any feedback at all. So we've imagined in this case we run this we've been in state s exactly once and we took action x. And human said that was good, and that's it, and so that's why everything else is 0. Another way of putting it is, for the purposes of this, whether the human tells you an answer or not is independent of whether they've ever told you anything before and whether they'll ever tell you anything in the future. Sure, okay. But I feel like I'd still like a probability for that. I guess what you're saying is maybe it's all going to cancel or something. I mean, there is what seems to be a really obvious answer. But I'm just, I feel like you could also be setting me up for a trick, and I just want to make sure I've got all my basis covered. Michael have I ever tricked you, ever. Yes. That is false. I feel like you're tricking me right now. Your parent know here is noted. Michael, I'm only about helping you to learn. So are you ready to go? Yes. Excellent then go. All right, you ready, Michael? You got an answer for me? Sure, I have at least one answer, maybe two. Okay. All right, so in the case where there is only allowed to be one right answer, it seems pretty clear that that answer has to be x. And why is that? So the reason that is is because the evidence that x is a correct answer is perfect, right, because the person said x is a correct answer at least once. And the person's never wrong according to the little assumption in the bottom left and so nothing else really matters that combined with the idea that only one thing can be correct, it has to be x. Okay, so then what would I put in the first box? One and then zero and zero for the other guys. Right, and, in fact, one way of thinking about this is that if you know there is only one optimal action, then if I tell you that x is optimal, then it's as if at the same time, I actually told you that y is not optimal and z is not optimal. I see. That's really useful as a general kind of rule. And so that's all consistent and it's great and had you told me that x was said to be correct and y was said to be correct, then I would have divided by zero someplace and been very unhappy. Yes, and dividing by zero is bad. Very bad. All right, so you said you had maybe two answers, did you just give me three answers or did you mean something else? No, no, no, no, sorry, no, I have probabilities for all three boxes, that's one answer. I was trying to also think through the other case. You said that if we allow for the possibility that each of the answers x, y, and z, each one independently is either right or wrong, then we have perfect information that x is correct. But that doesn't put any constraints on y and z. And in fact because we got no feedback of any kind, there really isn't anything telling us to move away from our prior of whether y is correct, let's say just by itself. I mean z could be correct or not, y could be correct or not, and x could be correct or not. X is definitely correct. Y, we don't know anything. I mean if you erase that sort of green one that you wrote up there. Right, in fact let me do that. Nicely done. That, to me, would imply that we're at our prior for y and z, which is a uniform prior, so half. So I want to say one-half, a half. Right, and we would accept either answer. Okay. And this makes perfect sense, isn't that neat? Yeah, in one case, it's like a multinomial because all three of them together have to add up to one. And, in the other case, it's more like a bunch of binomials, where each one individually has to be between zero and one. Okay, that's a great point, Michael, let's come back to that in a moment. I want to give you one more quiz. Because you like quizzes. I do. It makes me understand whether or not I understand. Wow. Okay Michael, so here's the same quiz I gave you before, except now the probability that the human is correct is different. It is 0.8. But otherwise the feedback that we've gotten is the same. The feedback we've gotten is the same. We've gotten one feedback on. {LAUGH} And are we doing the case where exactly one answer is correct or all three of them can be correct? Whichever one you want. Let's say only one can be correct. Sounds good to me. All right. In fact, I'll write that down. Only one can be correct. That reminds me of a movie. I don't know what you're talking about. Okay, you ready? Yeah I'm going to go get a pencil, I'll be right back, but yeah I'm ready. Okay, while he's getting a pencil, everybody else go. Okay, Michael, so you got an answer for me? Yes, though it was harder than I thought, actually. Well, you're welcome. [LAUGH] All right. So the question is what's the probability that x, y, or z is the right answer? And I had said that I wanted to do it in the form where It's either x, y, or z. So this is a multinomial over x, y, z. Mm-hm. And what we know is that the probability that the human is correct is 0.8. Yep. And so, my intuition was that it should be 0.8, 0.1, 0.1. Okay. Because 0.8 is the probability that the first one's correct and then I wanted to split the probability amongst the other two, because they're, we have exactly the same information about y and z, so they need to be the same. Then I thought okay, well, based on the way that you were talking about it last time, it might also make sense to say that it's .8.2.2 Mm-hm. Which of course isn't a probability distribution. So then we need to normalize it. Right. And I was pretty convinced that that couldn't be right. But then I worked through the derivation using Bayes Rule and it in fact is correct. I mean I guess I think what it comes down to is the same reason that the the Monty Hall problem is difficult because you have to be crystal clear on what your generative model is before you can even start talking about these kinds of probabilities. That is excellent and in fact I'm going to write that one down. Crystal clear and [LAUGH] There's an irony to the way you wrote that Thank you Bay's rule and generative model. Years later we will understand this. Right, sort of that I mean I think this is a really important point because when we were working this out originally, one of the problems that we had is that we kept changing what we were trying to actually compute. You know the probability that an action is optimal is exactly the what you're trying to answer. You're not answering other questions which you might find yourself wanting to answer along the way. So that what you found really curious how you you ended up with something. So the generative model I looked at a couple but the simple one was basically, first, let's imagine that the right answer is chosen and the prior is uniform, so it's a third a third a third for x, y, and z. Then we imagine that we're going to be asked is x the right answer and we're going to give an answer. Okay. And then answers either going to be yes or no. Right. And so what's the probability that will be yes or no in each of the three cases. So with a third probability, it's x times four fifths probability. The answer will be yes. Okay, and that four fifths is because .8 is a probably of humans, correct? And one third of the time x will actually be correct. Is that what you tell me? Yeah, exactly. Okay. And then what about the Y case? The y case is the probability that it is why is a third and the probability that the human would have said yes, in the case where x was the thing that was run, and y is the actual answer is one fifth. Yup. And then finally for z it's the same analysis, one third times a fifth. Right, and notice here that y and z have the same, sort of this the same calculation. You don't take that and sort of distributed around them and you got the distribution because of the one third here. Right, yeah. If the answer really is y, the probability that the person would have said yes to x is just a fifth and that also goes through in the z case. Correct. So those actually split the probability space. Those are the exhaustive and non-overlapping cases that we need to consider and then the last step is going to end up being just to normalize. And so we actually have the thirds all cancel, the fifths all cancel and we have 4, 1, 1 which is the information that we need. Well basically four sixth, one sixth, one sixth or two-thirds, one sixth, one sixth. Okay, right what are the, where the sixth come from? So once you've cancelled the fives in the threes you get 4, 1, 1, which is like what you call if you're trying to get information and that is up to six. Six, right. So when we normalize it four over sixth, one over sixth, one over sixth. All right, so that would be two-thirds, one-sixth, and one-sixth. [LAUGH] That's not a six. Yeah that's a six. Which efficiently large squiggle, that's a six. So that was surprising to me, that was not where I originally went. I didn't see the two thirds popping out because really like you said there's no where's the thirds. There's nothing in the set up of the problem that had thirds in it. Right and so the way that I come to that answer is I think about counting. So before when I said well if I know there's only one and you tell me that one. The x is actually right, that's a one. It's like also telling me, y is not it, and z is not it. And kind of the intuition here is that if the probability of human correct is 0.8, it's almost like I'm giving eight-tenths of a count. That's like saying that you got eight tenths of an x, of a count going towards x and then 0.2 for y and 0.2 for z, which you'll notice that if you normalize that out, works out exactly to be two thirds one sixth and one sixth. Yeah so that counting thing turns out to be very powerful and we can derive a more general version of this. So this quiz is actually, both of the quiz I gave you were actually designed to be easy. [LAUGH] Yeah and do you know what makes them easy? What makes them easy is that we only had one, one piece of data. You see you try to take action x in state s and you ended up being told that was good. But what if you had done it ten or twenty times and sometimes x was said to be good, sometimes said to be bad, sometimes y was said to be good, sometimes said to be bad. Then this kind of natural, this sort of process you went through still works but you have to keep track of a lot more things and this counting argument actually helps. And it comes from exactly the crystal clear generative model that uses Bayes rule that you came up with. So I'm going to actually write it down for you and we can talk about it for a little bit, okay. Yeah no I'd like to see the general form. Okay Michael, so what I have here on the screen is two formulas that tell you how to compute the probability that a particular action a is the optimal one given some evidence which I'm calling the d sub a. That is the, d is the data set of labels a number of times you said yes or no for a b in the optimal action in both the general case where there can be any number of policies that are optimal, that is any number of actions in a particular state that could be optimal versus the case where there can be only one, the Highlander case, okay? So I'm just going to kind of read this out to you and you can just ask as many questions as you want but we'll sort of see where it goes. So, we should read the first equation in the following way. So all of these are for a particular state and you just can apply this in a given state because we're living in Markov decision process and then we'd have to worry about all the other states where by the state we happen to be. This is the probability for a given state, action a is the optimal and given a sequence of labels that say, yes this is optimal, no this is not optimal is equal to this quantity C is just the probability, the human is correct, which was 0.8 in our last quiz and was one in the quiz before that. Raised to the power delta sub a. Now what is delta sub a? Well, delta is just the difference between the number of times you said yes, and the number of times you said no to action a. Now what does that really mean? Well remember as you said, when you were kind of thinking this out loud for the first one, that what we really got is multinomials and binomials. A binomial is a special case of multinomial. So what we really has is multinomials. What we're really doing is flipping coins. So every time you sort of flip a coin, you're getting some evidence of whether the coin comes up heads or tails more often. And so what you have here is delta basically is a sufficient statistic that summarizes the entire sequence that you get. Now what this means in practice, is that every time I say yes and no, it's as if they cancel each other out. They drag me back towards the uniform distribution, or whatever your prior happens to be. Now a lot of people find it difficult to wrap their heads around. It feels like if I flip a coin twice and it comes up heads once and tails once, that's evidence that it's a half. But if I flip a coin 1000 times and 500 times it says yes and 500 times it says no, that's somehow more evidence that it's a half and that's true but actually in both cases, the maximum likelihood answer is still one half. It's just in the second case, you're even more confident about it and what we're doing here is we're measuring basically the mean, not the variance. Does that make sense? Yeah. I think so. Right. So, in some sense, it's not telling us how sure we are of the answer. It's really just saying what the answer is that best guesses the answer. Exactly right and and what this delta kind of summarizes for you is the only thing that matters is how many times you said yes versus how many times you said no, not how many times you said yes on its own. So, in the general case, what we're really saying here is something kind of neat which is that we can we can treat every single action as if they're independent. And so the probability that a is optimal depends only upon what you told me about a, it doesn't depend upon anything else. Because as we did in the first example, if you tell me that the first action is optimal but all of them could be optimal. There's multiple optimal policies, then the fact you told me something about that action doesn't actually tell me anything about any other actions. And so what we have here is, basically the probability that I would see that many times you said yes normalized by the probability I would see that many times that you would say yes versus I would have said all those times and been wrong about it. So C to the delta a is a sort of under the assumption that you are correct. And 1 minus c is under the assumption you are not correct. And that just falls out of the binomial distribution. And if we plug in our original answers, you would see that you end up with the same thing that we got before and the special case where you've only seen one label. You with me? Yeah, that's cool, yeah I think so. So, because it's the binomial case, one per action, we're not looking at any other actions. So the denominator there comes from essentially, the count of how likely it is that we'd have those, that number that delta number versus that if we were wrong, we would have that delta number the 1-C. So yeah, I think that feels right to me. Now, I get slightly more complicated in the case where there can be only one policy. And that's because for the very reason that we described in the first quiz, where if I know with certainty, there's only one policy, and I know with certainty, the human gives me the right answer, the moment you tell me that some action is correct, I now know something about the other actions. So it becomes complicated. Somehow, what you've told me about a second action tells me something about the first action. And when you go through all the math, it's still a multinomial. And when you go through all the math and do the derivation, and the derivation's available for the students, you end up with this quantity. That the probability that a particular action A is optimal given a sequence of labels is what we've seen before, it's c raised to the delta sub a (1-c) raised to all the other evidence that we have from the other actions. And then there's a normalization factor which I don't write out here. And this corresponds to what we did in the last example, where we wrote out 0.8, 0.2, and 0.2 and then normalized everything out. And let me make sure I understand that. So we have 1.2, the delta is 1. Yes. No, but it's 0 for the other guys? What's delta j? Delta j is for the other actions. Yes, this is the difference between the times you said yes and no for every other action that isn't the particular action you're asking about. So this provides evidence from the other actions. I see, and then we're going to normalize. So maybe that squiggle equals wants to be like an alpha, like a proportional to symbol? Yeah, sure, I usually write squiggle, where I grew up in the South, that meant approximately. But there, that make you feel better? Much better, because it's not really approximately, it's actually proportional to, which means we're going to have to normalize afterwards, so that we get the 0.8, we get a 0.2 and a 0.2. And then when we normalize it, we get the 2/3, 1/6, 1/6. Awesome. Exactly. And the right thing kind of happens there. And you could write out what the normalization factor is, and then we don't have to have an alpha here, we can have an equals. But it's big, long, and hairy, and it's just much simpler to actually ignore it, do this part, which is really easy. And then do the normalization afterwards. More to the point, since what we care about is a policy, we only care about which one's maximal. We don't actually care about the actual probability. I'm forgetting why we care about a policy. Remember. because we're trying to, because ultimately, we're combining this information to figure out what action to take. Right. Got it. If all we're doing is listening to human beings, then when a human tells us something, we want to pick the action that's most likely to be optimal. And so it doesn't matter what the actual probability is so long as we have the ordering right. And so we don't have to normalise. We don't have to know the probability, we just have to make certain we get the maximum one right. So interesting. So, I'm wondering whether this, if we have a C that is aspecific, which is to say the probability that I get it right if it's one kind of thing versus the probability I get it right if it's some other kind of thing, we can probably generalize that C to having a different C for each of the actions. Yeah, we could. I don't want to, but we could. Actually, I think the more likely case in some sense is a weird one that seems counter-intuitive, which is not for every action to have a different C. But what's the probability that I would say something is right, versus the probability I would say something is wrong? Those don't have to be the same probability. This C actually represents the probability that the label I give you is correct, but there are actually more possibilities. There can be, there's a difference between a false positive and a false negative. Got it So, we're treating it as if there's only two cases here, but there's actually four. Okay, that's fair. But no one wants to do that. So, the last thing I'll say about this before I move on to my last slide is notice that I've acted as if there's only two cases here. There's the case where any policy can be optimal, and then, there's a case where only one can be optimal, but there's actually spaces in between here. So, you could imagine a case where you might say, well, there can be more than one optimal policy but it's more likely there's only one. It's slightly less likely that there are two. It's slightly less likely that there are three and so on and so forth. And you can have a prior over any of the combinations of actions being optimal and that prior need not be uniformed. And in fact, even here, we're actually assuming a distribution over all the possible two to the N minus one policies in such a way that each action independently is equally likely. You could set things up so that, yeah, sure, it may be the case that there's more than one optimal policy, but A alone is much more likely than A and B which is even more likely than A, B, and C and so on and so forth. Does that make things much hairier in terms of the formulas? Much hairier. I mean the reason we end up with something that's this simple over here, and this simple in the general case, and this simple in the only one case is that we get to eliminate all the common priors. I see. And if we can't eliminate it, we gotta carry them around with us. Neat. Okay, cool. Okay, that's cool. Now, does this tell us what to do also if we have a prior that some action is correct, because, let's say, we've been learning for a while like, yeah, that really seems like the right thing to do, and then the human is telling us something else. Can we factor in the information that we've gotten from our own experience? That is exactly what I was going to bring up next, so why don't I do that? Awesome. Awesome. Okay, Michael. So, let me sort of jump in on that last thing that you asked me. We've just been talking about how to compute probabilities of optimal actions based upon what human beings are giving us. And that's fine. But it turns out we actually have multiple sources of information, and we would like to be able to combine them. In particular, we have some policy that we've learned that gives us a probability distribution over actions that we learn from a human, that's why it says pi sub H here. And you notice here I'm treating a policy as a probability distribution over actions, and that's all neat. And these, in fact, are the numbers that we came up with in our last quiz. But while the agent is running around in the world. The agent has information from running around in the world, right? There were rewards. I've actually gone through and played Pac-Man. And I've been eaten and I've won and I've gone through rooms and I've done all these kinds of things. And it seems kind of silly to ignore what I actually know about what's going on in the world, particularly in the case where human being might, in fact, be incorrect sometimes, or may really believe that the optimal policy is one thing, but in fact, for the underlying NVP, the optimal policy is something else. So, I just want to ask you a couple of really simple questions. Let's imagine that I've got some algorithm that runs around in the world, on its own, and learns distributions over actions in every given state, okay? And it says after a little while that the probability that action x is optimal as 1/3, action y is optimal as 1/3, and action z is being optimal is also 1/3. And so now I've got two sources of information here, what the humans told me, and I know that's 2/3, 1/6, 1/6 because math, and I've learned from experience so far that all actions are equally likely. Which is pretty non-committal. It is pretty non-committal. In fact it might be what happens when you start off. If you had to choose an action in this case, which action would you choose? X. Why? Is that the quiz? No, that's a question. It's Y? Wow, I had no idea why it would be Y. Explain to me why you chose X. Sorry. I see, why I chose x? Because well according to the algorithm, it doesn't really care, doesn't have any reason to believe one over the other, but the human has reason to believe it. I guess there's one missing parameter maybe, which is how much we should believe the human. And so, if we believe the human not at all, it's still not being bad to go for X. But here's the thing. We actually have that parameter. That was C! That was C, and in fact, it doesn't matter what C is, because that's all inside this distribution. This distribution, two-thirds, one-sixth, one-sixth, actually already incorporates all the information about the uncertainty of the human. I don't think it does. Yeah, absolutely does. The probability that X is optimal is in fact a function of the data that the human has given us. And how likely they are to be correct in the data that they've given us. But it seems like it could change over time as we've come to believe the algorithm more. Then we should believe the human less. It's really interesting you say that because all the other various techniques that have been out there in the world actually do that. They actually try to capture what the human has said and what you've learned in the world and then over time believe the human less and less often because whatever the real world is telling you is got to be correct. What's actually beautiful about this particular mechanism that we described in the last couple of slides is that that parameter c, that confidence that we have that probability the human is correct actually sort of captures everything we need to know, if we are completely certain about the human then these numbers will reflect that by peaking over whatever action should be optimal. And if we're uncertain about the human, then these things will tend to stay towards the uniform. Let's imagine, for example, that there is truly only one optimal action. Then, as we spend an infinite amount of time running around in the world, our notion of which action is optimal over here, if we can convert them into actual probabilities, will drive whatever the optimal action is towards one. And that just comes from all the stuff we've been talking about with reinforcement learning for all the time in this class. If we can get to an optimal policy by just experiencing things in the world, then eventually, these numbers will reflect that. And it doesn't matter what the human thinks, the real world will drive the correct answer towards one. So, you're saying that c doesn't change over time? c could change over time, but let's just imagine it doesn't. Let's just say, we pick a value for c and it comes out this way. If you want to say that c changes over time that's okay, but then we're going to deal with that question by having these numbers here reflect whatever we think c happens to be. Remember, we can keep track of all the labels that we've seen and for any given value of c that we have, we can just change these numbers appropriately. Okay. So, it sort of doesn't matter. At the time we ask this question, I've got a distribution of reactions according to the human. I've got a distribution of actions according to my experience in the world. It no longer matters what I think about the human because that is captured by this distribution. So I can just ask the question, what should I do? And here you think the answer is x. Yeah. Now, what if I had different numbers here? Then I still think it's x. Yeah. [LAUGH] What if it is this? Well now I could certainly justify why. Why. Yeah. [LAUGH] Tell me why you think y. So okay, so are we supposed to imagine that the pi sub A is, can the algorithm be wrong? The algorithm could be wrong. But in the same way that we don't have to worry about what's wrong with the human because the distribution captures it, let's make the same assumption here. The agent, well this distribution is going to capture all of the uncertainty about the agent's beliefs. So maybe we'll never actually see 0, 1, 0, but we'll see numbers that are close enough that I'm just going to write out 0, 1, 0. Boy, there's a lot of ways to combine these numbers together in which y ends up being the winner. But it strikes me that in particular, if we believe pi sub A, then pi sub A is absolutely certain that it's y. So it almost doesn't matter at this point what pi sub H thinks, what the human thinks. Mm-hm, and by the way, with the same argument, if it's the case that c is equal to 1 for the human, that the human is infallible, then it really is not going to matter what the agent thinks, because the human's infallible. Well, unless they, [LAUGH] unless they disagree. But then we have to really believe that probability is correct. Right. Either the probability is correct or it's incorrect. And for the purposes of answering this question, let's just say the probabilities are correct. In so far they are not it will be reflected by these distributions. So we can never have 1, 0, 0 on the top row and then 0, 1, 0 on the bottom row, because that just violates the laws of reality. Right. So, you would argue for y? I would. I think that you've been using intuition so far and I want you to do a little bit more than intuition, so I'm going to do that by writing out a quiz. So Michael, here's the quiz. What we have here is a probability distribution from the human over optimality for each action X, Y, and Z. And we have a similar distribution for each action from the agent for X, Y, and Z. And I want you to look at these numbers and tell me which action I should actually choose, given these numbers. Should I choose action X? There's a radio button for it. Action Y, there's a radio button for it. Or action Z, here's a radio button for it. Got it? You understand the quiz? I hope so. Excellent. So then do you have any other questions? The answer is no, by the way. No, by the way. Excellent, then go. Okay Michael, you got an answer? I do, I don't feel very solid about it but I do have an answer. Okay, what's the answer? So the human thinks X, and the algorithm thinks Z, and the algorithm's more sure. So Z. Mm-hm. But, the human's also pretty sure Z's not it. Yeah, but it's not as you think why could it be why no because it's just as unsure it's- Okay. So you think the answer is easy and by the way. Yes, so I'll help you in your derivation by telling you that that is in. Is that correct? Sure. Okay, that helps to some degree. Yeah, it's much easier to prove things if you know they're true. That's not necessarily true. [LAUGH] No no, I think it's a well established fact that if you know something is true, it's easier to arrive than if you're not sure whether it's true. So the point is, if I want to try to be systematic about this I need some kind of generative model again. And- Yes. I could imagine two different generative models. So one generative model says here's what we do. There's the there is the right answer. The way we're going to pick the right answer is by first picking who's going to get to decide the right answer the human or the agent. And they're both correct so we do 50/50 on those, then we choose according to the ages probability what the right answer is and then we see her right. So what that would do in terms of setting up all these probabilities is we average the two different X numbers the two different Y numbers and the two different Z numbers. And I want to say in the normalized but I don't think we need to at that point. I think they'll already be set up to sum to one. Another way to think about it is that, essentially, what's happening is both H and A, both the human and the algorithm are just correct. They're choosing things according to those probabilities. And reality adds the constraint that they have to agree because there's just one right answer. So then we could say, well what's the probability that both of them choose x or both of them choose y or both of them choose z? And that's the pairwise product of the two x numbers, the two y numbers and the two z numbers normalized. Right. So in fact, that is in general the way that you combine multiple sources of information when they are represented as probability distributions under exactly the assumptions that you mention. The fact that there's a human being involved and there's an agent involved is neither here nor there. If you've got two probability distributions that's called one in two, and they are estimated from samples. And in some ways the human being was estimating from samples we don't know what they are. But, this distribution reflects that and the other ones estimated from samples. If you want to know what the maximum likelihood action is the maximum likelihood coin flip. However, you want to think about it you basically just take their products. So the action that I should choose is simply one that maximizes this expression the one that maximizes the probability that A. Or that particular action is the optimal one given what pi one says and what pi two says. And in fact, what this exactly captures is the probability that they will agree. All right Michael, last example I want to give you is something that comes from the world of what's called drama management. And the way I'm going to introduce this to you is I'm going to point out what we've actually been talking about so far. So I'm cheating a little bit for you in helping you with what have we learned part of the lesson. So I'm going to claim that if you think about at least the last few things about communication and coaching particularly on the coaching thing with human beings we've kind of talked about three different ways that a human can communicate to an agent. The first is demonstrations. You show the example of one particular drawing through a grid world to do inverse reinforcement learning and that was a demonstration. And I can give you multiple demonstrations. And we talked a little bit about rewards and that what a human can give you is rewards for various things that you do and that's a kind of reward shaping way. And then last thing we talked about was policy shaping where the human is communicating to you is not demonstrations and not reward values, but actually telling you what the correct policy thing is in response to things that you've done. And the last policies is different from the first demonstrations in that in the first case you would actually demonstrate what the right action is. But here with the policy shaping you're just giving critique that gives information about whether a particular policy is correct or not. Does that make sense? Yeah, that's interesting there's these different kind of mechanisms for giving feedback. Right, so I want to talk about another sort of model and way of doing coordination. This time between a human and an agent who's interacting with another human, right? So in the cases we described before there were two. There was the human and there was an agent, but here I'm going to say there's a third person. So I'm going to replace the human with what I'm going to call a Player. For example, a player of a game like Pac-Man. The Agent, which in this case would be Pac-Man, and a third human, I'm going to call the Author. So in the world of drama managment there's this basic idea that there are these things like Choose Your Own Adventure stories there's ways of seeing tasks in the world as if they are a games. Where there's some Author, someone who designs the game designs the system whether it be Pac-Man or a story or a movie or anything if you can imagine any sort of interaction in the world and wants to build an Agent, which is actually a system, which interferes with the third agent so to speak which is the Player, the person that's controlling what's going on in this world. So, in this Pac-Man example the Author would be the person that created Pac-Man. The Player would be the person controlling Pac-Man running around in this world and eating. And then the Agent would be the actual Pac-Man game itself, and that Agent might have the ability to change the way the ghost behave. To set up another level that has different properties or whatever it wants. And the goal of the Agent is to create an experience for the player that is consistent with whatever it is the Author wants. Does that make any sense at all? At a high level. I mean, how does the Author express what he or she wants? That's a good point. So, he Author wants to the Agent something, the Player is going to be interacting with the Agent and the Author never gets to play with the Player. How does the Author express to the Agent what it wants? Well, that's what I'm going to talk about but let me just kind of give you a little preview. Imagine that the Author says to the Agent. I want the Player to be able to win every level, but only after they died several times in that level. I want the Player to feel like they have to learn how to play better. So the game has to be hard, but it can be so hard that the Player can never win. Okay. Now I haven't told you how to communicate that with numbers, but you can imagine that that's an objective function. It's a thing I could write down that I keep track of how quickly a Player can go through a level. Whether it gets better over time. I could come up with a function that tries to capture elements of that either directly through the experience or by describing properties of the game, properties of the Pac-Man level that need to be traded off one another in terms of things like difficulty. Good. So there's lots of ways of doing it and people have been solving, worrying about this problem for many, many years now. And I want to describe a particular mechanism that combines this notion of demonstrations and rewards so that you can come up with a way for this Author to convey information to the Agent so that the Player has an experience that is consistent with the Author's intent. Okay? Neat yeah, that seems to fit with a couple of things we've been talking about in this lesson. Excellent, so let's dive in. Okay Michael, so I think I have basically one more thing that I want to say before I start writing out little formulas and trying to convince you that this is something that we can do with some of the framework and infrastructure that we've built so far. And I want to do it by diving in and sort of doubling down on this idea drama management, this idea of going through a story. So you're an AI person and you need to kind of think about how you're going to build a system where a player can learn how to go through a story or can go through a story and have a good time. So the first question I'm going to claim you're going to have to ask yourself is well, what's a story? What's a story really? And for the purposes of this discussion, I'm going to just assert something and you can tell me if it makes sense to you. And that is that a story, it turns out, is just a trajectory. It's just one of any possible many number of possible trajectories through a space that is made up of plot points. So, does that make any sense to you? Do you know what I mean by that? So you're being abstract about what stories are. Yes I'm being very abstract. That abstraction is important because we're computationalists. And plot points, I guess plot points makes me picture a graph where there's points and there's edges or there's vertices and there's edges. So plot point is like a vertex?. Yes, right. So a plot point is like a little circle, and that plot point might represent something like, I don't know, what's your favorite kind of story? Mystery. You like mysteries. Okay cool. So I have a plot point, that's something that has to do with the mystery. So what happens in a mystery? So there's a murder. What? Yes that's a plot point. There's another plot point maybe and that plot point is there's an accusation. There's another plot point which is a weapon is found. And there's a whole bunch of these and if they're set up right and they're sequenced just right if of all the possible paths of all the possible plot points were set up just right. So you see the murder, you see the accusation, a weapon is found and so on. Then you have something that looks like a murder mystery. Now I want to point out that it's possible to have good murder mysteries and bad murder mysteries. So if you think about the way a murder mystery works right, it's got this really neat structure. There's a murder. Nobody knows who did it, that's what makes it a mystery. And then eventually you discover that, every single person in the house had a reason to kill the person who was killed. And you find a weapon that points to a particular person. It's, Michael. [LAUGH] Everyone know it's that person and then that person gets killed. That's Michael again No! And then you know therefore it wasn't Michael. Or Curley. And so, then you keep going And you discover it was in fact not Curly. And then you find out who the real murderer is and there's a whole bunch of things that happen very quickly. And it turns out it was the butler all along. That would've been my guess. And that makes it a good murder mystery. Now here's a bad murder mystery. There's a murder and you happen to see it and so you know who the murderer is. [LAUGH]. So then a murderer has to go off and start killing you and everyone else they can. You run away and you know what you've got instead? A chasing movie. You've got a horror movie. So the difference between a murder mystery and a horror movie, and not that much difference, it's really about whether you know who's doing the killing or not. So if I have all these elements lined up in some kind of virtual game. Depending upon what the player of the game is doing I may have to interfere, interact with what's actually going on underneath so that the player experiences a murder mystery as opposed to experiencing a horror flick. And when you say you might have that, you mean the drama manager. Right, because the agent can describe what it means to be a murder mystery. And as a player, the author can't control the players. The player's going through. The system may have to interact with that in order to make certain that you still get a good murder mystery as opposed to something else. And I'm going to claim that if we think of these points as being key things that happen in the story and some sequences are better than others then what we end up with is a story is a trajectory. It's a trajectory through these plot points. And that a good story is a particular kind of trajectory. And there are bad stories which are other kinds of trajectories. And we need to figure out how to influence the player in such a way that we go down the good trajectories. Instead of the bad trajectories. And can you say a word about what the player's doing here? Well so it would depend upon the situation. But imagine that. I mean you play games like Zork. [LAUGH] Sure. And various other things. Right? Sure you have vague memories of Hitchhiker's Guide To The Galaxy. And those like text adventure games. You're interacting with the system somehow and you decide to go north or you decide to open the door, you decide to talk to somebody. It's just like you choose your own adventure story, that's the kind of idea that I'm trying to evoke here. Interesting. Okay, and so you're saying that we could think of this as a kind of MDP and then do reinforcement learning in it? Yes, that's what I'm going to talk about. I want to convince you, if you buy this idea of story, as trajectory with plot points and it's actually pretty easy to turn it into an MVP. Okay, I would like to hear about that. Okay, let's do that. So we can take all of that in just turn it into an MDP. I think it's pretty straightforward to think how you'd turn this kind of story management system into a market decision process, right. So what are states? Well, we said that last time. States are just partial trajectories or partial plot sequences. Actions, well, they're the story actions that the system can take. Your model is still the same model that we're using before except it's a player model that means you know. We've seen a sequence of states. Or we've seen these trajectories. Some action is being taken, and I want to know what plot state we're likely to be in, given that the player is doing something. So it still looks just like a transition model. And then the rewards mean exactly what they meant before, except here they're actually standing in for some notion of what some writer of a story thinks of as a good story. And you can somehow formalize that so that you can plan? Yeah, well, you could, but that gets us to a whole other discussion about where rewards come from, which is probably a discussion that we should have someday. So maybe we'll do that towards the end of the class. But just imagine the right now you have the reward function the same way that you would with any other MDP and you're really worry about where it comes from it's just there. Okay and given that these trajectories, or sorry, the states are now these full trajectories. It seems like that makes some things harder because now there could be a lot of them but on the other hand you can never go back to a state you've been in before so it's acyclic. So you can actually solve it more efficiently because you don't have to run, I mean basically one value duration and you're done. Well, that's actually kind of neat that you said that. Because I was about to say that there are two problems with this. The first problem is, well, now that you don't have states, but you have sequences of states, that's a lot of new states. How big do you think that is? I'm going to say very. It is very, very big. In fact, we can be even more quantitative in that. It's actually, hyper exponential. It's like 2 to the 2 to the f. Where, in terms of the length of the sequence? Yes, of all possible sequences you can have the number of states. Even. Okay, yes, yes. Okay, all right. Because the number of states is sort of in factorial and all the different kind of that we can have but we don't. All the states don't have to actually be there and so at the end of the day it ends up looking something like 2 to the 2, you know. And be really really really big. So the space that we're looking over of all possible trajectories where things might be there things might not be there gets really really big very quickly. So that's one point that you said. But on the other hand it turns out that there's structure there that we're going to be able to take advantage of. And, in fact, that's what we do and why we're bothering to introduce this formalism that I mentioned earlier. So let's do that. Actually before we do that, let me mention one other problem with thinking about things as Markov Decision Processes. And it's going to be a problem with the strength of a Markov Decision Process. So what is the goal of reinforcement was the goal of solving an MDP? Maximize reward. You maximize reward. So, in fact, it turns out that if you take this sort of notion of storytelling and how I can get someone to do a choose your own adventure and I've got rewards. I've got some evaluation of what best stories are. What's going to happen? What is the system going to learn how to do? Make the author happy? Make the author happy. That's exactly right, but not necessarily. Make the player happy. So what ends up happening with Markov Decision Processes, using sort of modeling as Markov Decision Processes in the obvious way, is that you force the player to experience the best story. No matter what the player does. No matter whatever choices you're trying to make. Nope, this system is going to make certain that you're going to enjoy yourself. Dammit, and enjoying yourself means whatever the author thinks is enjoying yourself. So like grad school. It's just like grad school. No, no, no, that's not right. Why is that a problem because the the author. I mean in a book that is certainly what happens. The author gets to choose what you can experience. And that's a good argument, but if we're bothered with this whole kind of interactive entertainment, choose your own adventure story, then if you don't really have any choice in what you do, then it's not really a choose your own adventure story. Got it. The basic idea here, is because there is a reward, and because all the algorithms we've ever thought about with Markov Decision Processes are about maximizing long term award that it's finding an answer. It's going to find an answer that's going to force you down a particular path. But it turns out we can relax that need to find the best answer and actually end up solving hyperextend is a problem at the same time. So, let's do that. So in order to deal with those problems, we're going to introduce an extension to markup decision processes called TDD MDPs as opposed to plain old MDPs. TDD MDPs. TDD MDPs because it was off the tongue. [LAUGH] What does it mean? Well it means Targeted Trajectory Distribution Obviously. And that actually you can see that when you think about how we go from an MDP to a TTDMDP. So what we are trying to do is we want to make explicit those things we said before, so instead of having states, we now have Trajectories and that's just all partial plot sequences that we can't have, that we can't reach, that we can't get to, so it's the sequence so far, it's the story so far. I try to go back and forth between trajectories and sequences as opposed to stories because this is actually more general than the story example that we're using. Actions are exactly the same as they were before. We have a model, but now because we have trajectories instead of states, we have a model that says well, if I'm in a trajectory and I take a particular action, what's the next trajectory that I will see? But it's still basically the same things except that really it elevates this notion of trajectories to sort of to a first class citizen and that we're reasoning over trajectories now. And now instead of having a simple reward, we have a target distribution. Now here I've made a sort of capital T instead of a lowercase T and that's because the lower case T here was sort of standing for trajectories the capital T stands for final trajectories you know that is ends of stories. So you don't have a distribution or I guess because they have a probability of zero for a partial dish but all which we really care about is stories that we care about. So things that have higher probability are stories that you might consider good and things that have lower probability, stories that you might consider bad or less desirable. Does that make sense? I guess so, but why not just pick the one that we want and just and call that I guess the distribution where the right story has probability of 1 and everything else is 0 seems to make sense. Right so that's a sort of strict generalization of what we were talking about. This is a strict generalization of what we're talking about before. So this allows us to say, some things are better than others and it turns out by relaxing this hard constraint that I'm trying to find, I'm trying to maximize reward but instead I'm trying to match a distribution. It allows more replay ability, allows players to do a few more things, because now our goal is not to find a policy that maps states to actions in order to maximize long term reward, it's to find a probabilistic policy. That is a probability distribution over actions, okay. And what is the optimal policy? It's the one, that if I use this probability distribution, would lead to be matching the target distribution. I see. Right? And just just so that I'm still on board here. The actions here are the things that the story can do? Right. The story or the story manager or the system can do. And the uncertainty for that thing. The uncertainty is, well, what is the player actually going to do? We don't know. Right. Okay, so it's not the players actions it's the story's actions Right, exactly it's story actions or system actions or manager actions or lots of ways to sort of think about. Things like we're going to make a wolf appear right about now. [SOUND] Yes exactly, that was me doing a wolf. Shoot I was really scared. Or I'm going to make a key appear or I'm going to unlock a door. I'm going to lock a door because I don't want you to go in this room. Got it. Right. So, these are the kind of actions that you might be able to take in the system. The model says well if I'm in some trajectory and I take one of these story actions, what plot point will I end up in next? Which rejectory will I see next? That depends upon the player and so all of your uncertainty here, sort of probably comes from the player. That's where our inter P comes from. Okay. And I'm trying to get some stories and some stories I want to happen more often than others, but this allows the player agency sort of published. Now there's a lot of math that I could do here, but at the end of the day what you might think is that, well now that I no longer have this sort of simple constraint of maximizing my long term expected reward but I have probabilities and randomness and probabilistic policies. This is going to be harder, much harder to deal with. But it turns out it's not. This is often the case in the sheen learning in AI. If you relax a hard constraint and make it a soft constraint you make it something that's more about probabilities and probabilistic sort of search, you actually get more power out of it and you're able to solve hard problems. So without going into the details I can point you to a paper that you can actually series of papers that you can read here. It turns out that by going from a hard constraint to probability distributions, you can actually solve TDMDP in linear time. The linear in what? Linear in the length of a story. That seems unlikely. Well, I can prove it to you, but instead I'm just going to have to read the paper. All of this kind of induces a tree, right? You start in some state, there are several actions you could take, which gets you to another set of states, and so on and so forth. And this tree really, really, really, really kind of builds up. [LAUGH] And you have a whole bunch, in fact, a hyper exponential number, of possible stories. Well, if I have a probability distribution for all partial trajectories, then it turns out that whatever state I happen to be in right now, let's say this one, I don't actually have to care about any of the rest of the tree. It doesn't matter because I can't get there. See as you pointed out, it's cyclical. So, even though I can get from one plot point to another, I can't see a sequence that doesn't involve going this way. Sure. Does that make sense? Yeah, that's right, but underneath that node, there's still an awful lot of other nodes. But if I have a probability distribution for all of the next states I could get to, then I simply have to execute the action. And then I will end up in one of these states, and when I do, it doesn't matter what the other ones were. And so all that's ever happening is that I am following one particular path down the tree. And how do we get from that target distribution that kind of specifies the goal, to a target distribution over all the intermediate nodes of the tree. Right, that's the hard part. Okay. But it turns out there are easy answers to it. Okay. I'm just not going to tell you what they are. Cool and have you used this for actually making stories? Yeah, we actually done this for making stories and actually shown that you can get people to feel satisfied in their stories or experiences that they're having. And you can do it in real-time, on the fly and and I'll just leave you with this little teaser. It involves using Kubrick live or diversions. [LAUGH] Is not exactly the most exciting teaser. I thought you we're going to say something about wolves again. Well in the end that's all a wolf is. [LAUGH] I would love to talk about it, but I think it's probably beyond the scope of the class. But we'll put in the resources link a series of papers that talk about this. But the real answer is that, any time you're trying to match probability distributions, you care about KL divergence. And KL divergence is just a bunch of sums of probabilities, and sums mean linear independence. Independence means I can do them in any order. So I can basically just ignore all the things that I don't actually see, and I only ever see a finite and not just finite but linear in the number of plot points at any given time. And the hard part is as you recognize is figuring out what probability distributions are for the intermediate node. I mean that can be very hard because it can require computing the whole hyper exponential tree. Or can be very easy if you have a nice function that can map partial trajectories into probabilities or some other kind of value. Cool, okay. Okay, Michael. So with that, I think we're kind of done with this lesson. Those are the three things that I wanted to mention and I think we can now go back to where you left off and talk about what we have learned, except I have the pen. And so you have to tell me what we've learned. I don't have to tell you. This is true. So now when you see the three things, what were the three things that you talked about? Your trying to trick me into telling me what we learned. [LAUGH] Yeah a little bit. All right, so. All right. So when I had the pen, we talked about. Well two main things, well the overall topic is the CCC. Right. Which is Comedy Central Committees. [SOUND] No! Closed caption clowns. No. So, coordinating, communicating, and coaching. Right. Ha! I think what we learned is that alliteration can be fun. Well, sure, and in particular, I talked about two particular models. One was the DEC POMD VP model. Which least at least let us define or ask questions about what it means to coordinate and communicate optimally when we've got agents that don't share a brain but they do share the reward function so what's good for each one of them is good for the community. Right. Shared our distinct brains. I like that. Then we talked about the inverse reinforcement learning idea which was that we could actually use examples of behavior demonstrations of behavior to infer reward functions and then once we have those reward functions we could try to optimize them in other settings. Yep we go from behaviors to rewards instead of going from rewards to behavior. And that is what makes it inverse. And having those rewards means that it generalizes to other settings as well. Right, and I think that a point that I made on that, when I started taking over the pen, was that this actually is a kind of reward shaping, by getting human beings to give us hints about what we ought to be doing. Okay, but in the form of behavior demonstrations. But in the form of behavior demonstrations. And that is a nice little point worth making, which is demonstrations are one way to do coaching. What were the other ways we did some coaching? So, yeah demonstrations is what we did and then there was this kind of policy feedback policy. I guess you called it policy shaping. Right. But it was a sort of idea that you can get feedback on the actions being taken by the agent and that can be incorporated or combined with other suggestions as well as what the learner is picking up on. And so between reward shaping, demonstrations and policy shaping, we got a nice little set of ways that human beings can communicate to agents. So let's see. I know I did more than this. There were some examples that I used. What kind of examples did I use? Well, in the policy shaping setting, we talked about Pac-Man. Pac-Man Yeah. So we learned that Pac-Man is like the one example you should use for everything. Sure except for what we did at the end, end which was drama management. Right, right we talked about drama management. And in particular, [LAUGH] yeah. In particular we introduced TTD-MDPs. Where TTD MDP has something to do with trajectories and drama. Targets and stuff. That's exactly right. And I think we had a nice little conversation here, tying some of these things back to things we did before. We actually returned and brought up again constraints and options as a way of doing coaching and communication. And try to really drive home the point that there are some human beings and they think a particular way and then there are machines and they think a particular way. And we should try to move the machines closer to the humans. Rather than forcing the humans to come all the way over to the machines. Yeah I like that as a philosophy. And that's really what I think is important. Yeah that's a good philosophy. Well I think that covers everything Michael. Nice. I feel pretty good about that. Nice, so does this mean we're done? I think this means we're done with the class, kind of. Really? So maybe we should do some kind of wrap up to see how everything ties together. Yeah, let's do that. That way I don't have to say goodbye, why don't you come down to Atlanta and maybe we can record some stuff together. Great I will have done that. Perfect. Or is it perfect or is that the past perfect poo perfect. Poo perfect. Yes the perfect good perfect. Okay Michael, well I will see you in Atlanta soon. Bye Charles. Bye. Hey Charles, this is when we get to talk about reinforcement learning. Hi Michael. This is when I get to hear about reinforcement learning. Wow. I'm glad we're on the same page. So- Are we on the same page? Is this all of reinforcement learning or is it just the reinforcement learning basics? We're going to start with the basics. Oh, okay. I can't wait to hear what that is. So the first concept to try to understand when you're doing reinforcement learning is that a lot of it takes place as a conversation between an agent and an environment. Okay, so like right now, you're the agent and I'm the environment. Actually I think I'm going to have you be the agent. Okay. And we'll just imagine some kind of, I don't know, like a video game environment. That sounds reasonable. By the way did you notice I've lost weight? [LAUGH] Oh, good job, how did you do that? Well, I got drawn as a stick figure. [LAUGH] That's fair. So here we are, the agent and the environment, and the conversation basically talks about what is going back and forth between the agent and the environment. So, the environment is going to reveal itself to you, to the agent, in the form of states, S. You then get to have some influence on the environment by taking actions, a, and then you also receive back, before the next state, some kind of reward for the most recent state action combination. Okay. Fair enough. So this is the same kind of elements that we have in an MDP. But the important thing is that instead of just being given an MDP as some kind of a graph structure and then we get to compute on it. Really the computation's happening inside the head of the agent and the information about the environment is really only available through the course of this interaction. So does that make some sense? It does make some sense but I guess how is that any different from the MDP? Well, it's the same story as how a policy interacts with an MDP. Right, where this is playing the role of the MDP, and this is playing the role of the policy, pi. Mm-hm. But now, again, the computational aspect of the system here, the agent doesn't know the environment, it's not living inside the agent's head. Instead the agent is just experiencing the environment by interacting with it. It can then you know if it so chose build some kind of a model of the environment in its head and then think about that. But what's in the agent's head and what's in the environment are two different things in this set up. Okay fair enough. I get that. So maybe I can make this a little bit clearer. So let's actually put you in this environment. What do you say? Okay, metaphorically? No, let's just do it. Sure. So, we've transported you into an MDP, or actually we didn't transport you into anything. We're giving you the ability to interact with an MDP, and here's the MDP. Now, remember I said the MDP, the environment has states and actions. Now, the states in this case, are going to be visualized to you in terms of this sort of Mondrian looking picture in the middle of the screen. Mondrian? I'm going to go with that. Okay. And it's like colors and squares and things. And the actions are the numbers one through six. And unlike in a regular video game, you don't know the rules. You don't really know how things work. You don't even know what it is you're trying to do, but what you can do is start playing and use that experience to start figuring out how things work. Okay, so it's just like grad school. [LAUGH] Maybe. All right, so you have to now choose actions, and I'll enter them into the MDP and we'll see what happens and then you can see if you can win or not. Okay. Get my reward. Three. Three. I did it. Just nothing changed. Five. I did it. Nothing changed. Six? I'm back where I was. That's true. One. Nothing changed. Two. Three. That's a new state. It is a new state. Three. Oh, so I figured something out then. Do you want to tell me what it is? No, because I forgot what I did [LAUGH] three steps back. Okay. I've decided that three means move to the right. Okay. And something moves down, but I can't remember what I said. I want to say it was two, but I guess I could try that. Two. Yes. Now I need to figure out how to go back up again. One? Nothing happened. Four? Four? Four, Five. Nothing happened. Six. So I think that's visually very attractive. Thank you. Yeah, cool. So I think my actions mostly moved the orange square around. And now I just need to know what [LAUGH] it's trying to accomplish. How about two, two, two, two, five. Nothing happened. Six. Nothing happened. Four, four, three, three, three, two, two, three, one. Nothing happened. Four. Four. Four. Four. All right. So you seem to have at least gotten the feel for the idea that there's four directional actions for the orange square. Yeah. In fact maybe we should actually notate them just to make it a little bit easier to hold in your head. Okay. But you do notice that there are some actions that you haven't quite gotten to do anything yet. Yeah, and now that I don't have to remember what they are. I mean sure two, three, four and six math exactly as you would expect them two, two, down, right, up and left. But I was having a hard time keeping them in my head. So let's try six. Six, two, two, six, six, four, four, one. Nothing happened. Three, six, five. The little yellow circle thing. Or green. Green is good. Okay, the green little circle thing shrunk. Oh, three. Oh, that's interesting. [LAUGH] Two, two, three, three, three, four, four. I should do something now. One. Success. Yes. I get it. I think I even understand what the game is. Okay, let's show you the game again and maybe you can explain it. All right, so here's another state. I just transported you randomly to another start state, and you claim that you understand how things work now. So can you explain? I think I do. The goal is to go get the little yellow circle or the circle which was yellow to begin with the last time. I don't know if that means anything. And I I think it was green last time. Was it? And you want to pick it up, which was I think 5. And when you do that, what I don't yet know is whether it will change color. But whatever color it is when you pick it up, you want to take it to the square that matches that color. Okay. Then put it down, which is Action 1. Now I should say, because I didn't say it before, that what I thought the point was the first time was just to get the square and the circle in the same place. The square and the circle. Yeah, you did that. But you didn't get any reward for that. Right, so then I figured out maybe I didn't do anything but at that time I was also just trying to figure out what all the commands did. Good. I was doing a lot of exploring. All right, so learned a bit about the transition function, you learned a little about how actions change the state and you also learned a little bit about what states are rewarding. Yes. And so now you think even though things are a bit different you think you can actually get kind of on a minimal path to a gold state now? Sure. So from here I would go 3,3. Then I would go 4,4 then 5 which keeps it yellow. So then I need to go as quickly to yellow as I can. There's a lot of ways of doing that but let's just do 2,2. 6,6,6,6. 2,2. And then that doesn't work. Is it possible you forgot something? Oh, 1, I forgot to put it down. Boom! Done. All right, so good excellent job. So that was an MDP and in fact, you solved that a lot faster than any of the algorithms [LAUGH] that we're going to be talking about. Because you made use of all kinds of interesting background information about how you can- What navigation does and how it's consistent across states. But generally in the MDP model and the reinforcement learning model, we don't necessarily assume that. It could very well be that what you're- In each state, the actions all do some other kind of crazy different thing, and you have to learn that mapping. Right, so let me say two things. Actually three thing. One, is when I trying to figure out what the actions were, I considered for a moment, even though I had already figured out that 3 moves you right, I considered trying to go right a couple of times just to see if there was any randomness [CROSSTALK]. Mm. But then I decided you wouldn't do that to me, you're too nice of a person. [LAUGH] And so I decided they were deterministic and then do that. But you point out that yeah, I did it kind of fast because I had a lot of background knowledge. I'd really say that it was more about assumptions. Okay. And in fact my first assumption was wrong. Oh. The goal was just to get in the same place as the circle and that was the whole point. That's true. And then the last thing I'll say is that as long I've been thinking about reinforcement learning. As long as I've been thinking about search algorithms and AI. As long as I've been machine learning I don't think I ever really appreciated how incredibly frustrating it is [LAUGH] when you don't actually know what the rules are and you're just trying to figure out what's going on. And I now am going to be much kinder to all of my search algorithms. That's what I was hoping you'd get out of that. Thank you. Yes. So sometimes it's really frustrating watching these algorithms do what they do. And in some cases just repeatedly bashing their heads against the wall. Which for what it's worth is something that you didn't do. When you were moving around in the grid, you were purposely avoiding these black lines. Do you know why? [LAUGH] Maybe those are actually reward lines. If you bump into them you win. Yeah, but every time I've ever played any game, they're walls. Yeah, they look like walls. And, in fact, they are in this case. So that again, that was an assumption that you brought to bear. But, right, so when you watch a learning algorithm, reinforcement learning algorithm actually interacting with a grid like this. It doesn't have those biases necessarily. And so it can actually bash its head against this wall, and then maybe this wall for a while, and hey, this wall looks really fun. And so, yeah, it's good that we have a little bit of empathy for the poor reinforcement learning agent that's trying to master this environment given very complex and hard to parse apart representations for the actions and the states. Hmm, that's very good. Probably the only thing more frustrating than my trying to play this was all the people watching me trying to play this. Oh, I think they loved it. For what it's worth, we can make this available. [LAUGH] Who would play it on their home computers? [LAUGH] Yeah, but now they know the answer. Can you just like randomly change what the actions are? Let's say that I did that. Okay, that was actually quite good. I'm glad you didn't let me know what it was before hand. [LAUGH] Do you recognize it now? Do I recognize what now? It's a taxi cab to me. I almost said that [LAUGH] and then realized that would break the illusion. [LAUGHTER] So when I had Molly do this a long a time ago, and I asked her at the end. What is exactly the task that this is supposed to be doing? She's like well you have to rescue the dog, and it just wants to go to its house. Wow, that is- Yeah, I thought that was kind of awesome. We should try that with my daughter and see if, assuming she can sit still long enough, and see what she would come up with. The other thing is when you had in this position and you really like the kind of symmetries of it and stuff. We actually did this in a mall in New Jersey where we had people come by and just see what they would do. You'd give them the game and they'd. And there was one woman who was a real estate agent, who could just not get over the idea that the actual goal is to fix the Feng Shui. So if you look there's a color in every corner, except the blue is not quite in the corner. So she kept trying to move the blue into the corner. And when I told her that she should stop doing that, she just kept doing it because [LAUGH] she was convinced that the task was one of feng shui. I like that. Yeah. And that tells you everything you need to know about why our real estate market is the way it is. [LAUGH] And New Jersey. All right, so our goal is going to be to try to create learning algorithms that are not quite as smart as you were in that example, but trying to approximate that, getting as far along the rock to Charles continuum as we possibly can. [LAUGH] Okay, we want algorithms that are Charles complete. No, I think you're probably still going to beat the algorithms that we're going to talk about in this class. Someday, someday. All right, so the important thing is that we have to figure out, what it is that we are trying to learn? So in this case what we're trying to learn is a behavior, we're trying to learn the way of interacting with the environment that obtains high reward. And it turns out that there's different ways to think about this depending on what kind of behavior you are looking for. So one classical kind of behavior that is very common in the AI literature is the notion of a plan. So plan usually refers to a fixed sequence of actions. So the kind of thing that says, okay I see where I am now, this is the sequence of things I'm going to do. I'm going to go left, right, up, up, up, down, pick up, left, left, down, down, down, down, down, drop off. Something like that. So that plan, once you've chosen the plan, you can just execute it. It's just that fixed sequence of actions. And this happens sometimes in real life, there's certain kinds of behaviors that we just have. It memorizes a sequence, but in fact what's the problem with this? Why can we, in general, just execute plans in order to satisfy the kind of goals that we're trying to satisfy? Well, so, I'll say two things. One is, I was actually executing a plan once I figured out what was going on. Mm. Right? Particularly the- Yeah, that's true. Right so, part of it is, during learning, you don't know stuff, so you don't know kind of plan to execute. For this slide I'm talking about behavior structures for after learning. So during learning also sorts of weird things have to happen. You have to try stuff out. You have to adapt. But once you've figured out what you want to do, you could try to represent it as a plan but the problem with that. Okay, so it is true the environment that I showed you has a property that makes plans actually okay [LAUGH]. Yeah, it's deterministic for one thing. So that was the second thing I was going to say, which is that the plan worked because everything did as I expected it to do. It was deterministic. Good, good. And so in stochastic environments you need to be at least a little bit of aware of your surroundings, you need to move around with your eyes open. And so one step in that direction is something called the conditional plan. So you can think of a conditional plan as actually being a kind of program where plan is a fixed program where it just executes a sequence of statements. A conditional plan can have if statements in it. You can sort of see what's happening in your environment. I'm going to do this, this, this, and this. And if I get to this point and such and such is true, then I'll go this way and start this action, otherwise I'll start these things, these actions. Right, so could one of your if things be, if everything works out the way I expected, keep going. If it ever doesn't, then stop and try to figure out a new plan? Usually that's not thought of that way. A conditional plan would be something where you have that sequence, and then there's a branch point, we ask a question. And we could, if everything's gone okay, continue the way we've been going, but if not, then a conditional plan has an else statement, right? Mm-hm. It tells you what you should do in case that that's not true or in case it's something else that's true. Okay. What you're talking about is something more akin to dynamic replanning, where the idea is exactly what you said. We're just going to continue along the plan, making predictions about what's suppose to happen next. If those predictions are violated, we just generate a new plan. So there's nothing in the plan that says, here's what I do in case things break, instead, we wait for something to break and then we react to it. Okay, fair enough. So that's just in time conditional planning? Just in time planning really. Hm. because again the conditionals aren't in there. Hm, fair enough, fair enough, I like that. All right, now stationary policy, which sometimes in the literature is referred to as the universal plan, is actually what we talked about in the context of MDPs, so mapping from states to actions. And this is a very powerful sort of thing because it can handle stochasticity really well. Every time you take a transition, you go back to your table, you go back to your policy to say, all right, here's the state that I ended up in. What's the right thing to do from here? It is kind of hard to convey these to people, though. Do you see why that might be? Yeah, so one way it might be is because, I will claim that a stationary policy or universal plan is just a conditional plan where you have an if at every single possible state. That's true but it's actually a little bit more constrained than that. Which is it's the same if at every possible state. Right. So no matter where you came from, no matter what you were in the middle of doing, once you discover where you are, that's going to determine what your next step's going to be. Right, and that's a lot of if statements. It's a lot of if statements, yeah. And right, so if you try to convey it, you have to actually tell someone what all those if statements are. It's sort of like if I want to give you directions to my house. A plan might say go left, go right, go left. Go down until you get to Maple Street, make a right. A conditional plan might say something like, go to the highway. If you get there and it's crowded, then go these back roads, otherwise, go on the highway. [LAUGH] And a policy would say something like, for every location in my neighborhood, here's the direction that you should turn. And yeah, and that's going to be very large. Right. And in some ways not very useful. Well, [LAUGH] define useful, right? So it's very useful in the sense that it is very powerful and it can handle any kind of stochasticity. And there's always an optimal stationary policy for any MDP. Okay. So that's a really nice property. You never have to look beyond the stationary policies to find something that's optimal. Okay, so then I take back what I said. It's not that it's not useful, it's that, in fact it's very useful. It's that it's not very compact, or as you have up there, it's very large. Very large. So you have to say a lot. Yeah, so that's exactly right. All right, but now for the learning algorithm, it could mean that it has to learn a lot. It has to discover what a reasonable thing to do is in every one of its possible states. So that could actually make the learning problem difficult. But anyway, I just wanted to at least raise this issue. We're going to be focused, almost entirely, on these stationary policies. Because they do allow for the possibility of expressing that optimal behavior. Cool. We're going to want our learning algorithms to return optimal policies. Right. But to do that, we're going to have to say what we mean by optimal. So I know we talked about this in the context of MDPs, but I thought it might be worth revisiting just to see that there's different choices we could make. And the choices that we make have implications about what it means to be optimal. Okay. So here's, in particular, what I want to get at, let's say we have some particular policy that we're going to follow. By following that policy from some initial state, we're going to have a sequence of states, actions and rewards. But there's actually multiple possible sequences of state, actions and rewards that can happen because the domains are stochastic. Right, right. So let's say that we've got some particular policy that we want to evaluate and it generates this sequence of states with this probability, and this sequence states what this probability, and this sequence of states with this probability. How do we turn all that possibilities into a single number so that we can say, oh, this policy is this good, has this value, and the other one is a smaller value so I like this one better? So here's this four steps that we need to do to take what was once a sequence of sequences, or a set of sequences of states, and turn them into a single number. So the first step is, for each of the state transitions, we need to turn it into actual numbers, the immediate rewards. Sure. So in this case maybe landing in a red state gets us -0.2 and landing in a green state gets us a +1. What is it in the definition of the MDP that allows us to translate between state transitions, from some state via some action to some next state, to actual values, actual numbers for that transition? Oh, it's our reward function. Exactly, that basically just gives us strings of numbers. Right, so what was strings of states now becomes strings of numbers. And those are infinitely long, right, because this process can continue in an unbounded way. If we're talking about the infinite horizon, we just leave it that way. But if we're talking about some kind of finite horizon, like I don't know, say truncating it at 5 steps, we can actually take the sequence of numbers that results and cut it off after just 5 transitions. Mm-hm. Now we need to take that list of truncated numbers, and for each of the sequences, turn it into a single number for that sequence. It's called the return for the sequence. So how do we do that? Yeah, often what we'll do is actually add them according to some kind of discount factor. Mm-hm. If r sub i is the reward that we got on step i of the sequence, we might want to actually sum up the discounted rewards. Right, and my recollection is that that's a geometric sequence so it allows us to do infinite sequences in finite time. Or at least infinite numbers and make it a finite number, so to speak. That's right, and that's a good point actually because we might be doing this finite or we might be doing this infinite. In general, we do it up to the length of the horizon. Mm-hm. All right, and so let's just say for concreteness, you can imagine having a discount factor of 0.8. And that gives us a way of summing up the numbers that have been generated by these states and getting a single number for each sequence. And then finally, we have to take those multiple numbers, one number for each of the sequences, and turn it into a single number that summarizes across all of them. I see, so you would just average them according to how likely the sequences are. Good, so it's also taking the expectation, all right. And so I've annotated these sequences with their probabilities. So what you should be able to do at this point, is actually, according to these parameters that I filled in, say what the value of this policy is. Assuming that these are the three possible trajectories that could result, what is the single number that actually summarizes the outcomes of this policy? Right and so that's what, when we talked about MDPs, is long-term expected reward. That's right, or value or utility. Mm-hm. All right, so can you work out what it is for this case? We're going to make that a quiz. Oh, okay, let's pretend, oh wait, I've got one more thing. I think I know what the answer is, but just to force you to be absolutely concrete. I know what happens when you hit a red circle, I know what happens when you hit a green circle, what happens when you hit a white circle? Good question. [LAUGH] They look like 0s, let's just make them 0s. So they're not going to come into play, except for the fact that they're 0. I like it. All right, so you should be able to reduce this entire sequence to a single number, and you can put the number in the box. All right, sounds good, let's do it. Go. Okay, Charles, how we going to reduce this to a single number? We're going to follow those steps that you have outlined. I don't know the answer, but I think I know how to get to the answer. All right, let's do it. Okay so, well first off, are we doing finite or infinite here? T = 5 it says. T = 5, so we're definitely doing T equals, that's what I thought, just want to be sure. Okay well, so all I have to do is actually turn everything that I see into, turn the state transition into immediate reward, so I can do that. Would you mind writing some stuff for me? Not at all. Yeah there you go. So let's do ones over the greens and minus 0.2 over the reds. But actually I don't really care about the one you just wrote down because it's more than T = 5 away. All right. Well that's going to come up in the next step. Truncate according to horizons. So one, two, three, four, five, cut. Mm-hm. One, two, three, four, five, cut. One, two, three, four, five, cut. Now [LAUGH] I was a little bit sloppy with how I defined the five. It could be that you actually put it one step deeper. But I was careful to pick a horizon link. So that it wouldn't matter which way you interpreted it. This would just add zeros to it. It wouldn't change anything. So anyway here we are cutting it at five. Okay, pretty smart, Michael. All right, cool. So now that we've done the truncating, I just need to summarize each of the sequences in turn. So I know that whatever I would give for the first value, would be 0.8 to the zero or to the one. Where are we starting from? That's another good question. So that's going to actually change the answer depending on how people interpret that. I would say, to take this literally, that we're talking about the first reward being r1 and then the next one being r2, r3, r4, r5. And the gamma corresponds to the index number here. That doesn't seem right. Yeah, but it is what it is. Okay, seems like it should be zero, one, two, three, four. Oh, I see! If you call the first one zero. Oh, I like that better! Mm-hm. [LAUGH] But probably for the quiz, we should accept either way. Sure. Or either way. Or either way. Okay, so now that we know what all the words for the rule was, that's pretty easy. Actually, I think that's going to work out pretty well for us. So then for the first sequence I know it's going to be 1 times 0 plus 0.8 times 1. And actually everything else is 0. So. Yep. As I explained it. So this first sequence actually has a return of 0.8. Right. So the second sequence is pretty similar. It's going to be 1 times 0 plus, blah blah blah, so that's 8 to the third, or pointing to the third, times 1, which is some number. Okay, so the return for the second sequence is 0.512. So for the third sequence, we're going to do the same thing, it's going to be 0.8 to the second power, times 1. First power To the first power, thank you very much. 0.8 to the first power times 1 which is, in fact, 0.8, + 0.8 to the fourth power x minus 0.2. All right, so we got 0.8, 0.512, and 0.71808. Mm-hm. And so, we just have to add those together, get an average, but a weighted average. Mm. So it's going to be 0.8 times 0.6 plus 0.512 times 0.1 plus 0.71808 times 0.3. Which is obviously. All right, so when you've just worked out, then you get something that's very close to 0.75. So that's the summary of this particular outcomes of this particular policy according to these various parameters that we've chosen. So at this philosophical level there's one last thing I want to talk about. We just talked about evaluating a policy but remember in the reinforcement learning setting, the policy is what gets output as a result of the learning process. So what if we want to actually evaluate the learner? How good is this learner? Can you think of some ways that we might be able to decide how good a learner is? Sure. It's as good as its policy. So that seems like a good idea. That a good learner is one that returns a good policy, sure. What if we have two learners that return the same policy say the optimal policy? Oh okay, well I can think of two responses to that. One is who cares they're both the same and the only thing that matters is how good the policy is that you come up with. Or I could come up with another answer which would be, I will take the one that does it more quickly. Quickly in terms of what? Let's say time. [LAUGH] I think that's what quickly means. Yes, but time of what? There's lots of different kinds of time that matter. So one in particular is computation time? Yeah, wall clock time. And that seems like an important one, right? We want one that's going to be more efficient in terms of the computations that it does in the process of learning the policy. But there's another kind of time that actually matters a lot in the reinforcement learning settings, so I think it's worth mentioning. Okay. So the other kind of complexity that turns out to be really important in this learning setting is experience complexity. Do you have a sense of what that might mean? How complex your experiences are? I'm going to say no. Oh, okay, I get it. So computational complexity is sort of how much computational energy you have to put into it. So this would be how much experience you have to have in order to learn. So it's the amount of data that you need. Yeah, so if two different learners can learn the same optimal policy, but one only requires a couple interactions with the environment to do it, and the other one requires a billion, the one that has the lower experience complexity should be preferred. Though of course there's trade-offs. If to do that it actually has to compute for 2 billion years then that seems maybe not so good either. So there can be little trade-offs between how much time that the learner takes in terms of interactions with the environment and how much time that it takes in terms of the computations that it does between interactions. Hm. Do you know what this reminds me of? It reminds me of two things. It reminds me of MIMIC because that was the exact trade-off that we were making then. Oh, yeah. Okay, sure. And the other thing is that it reminds me of the theory lecture that we did in the Intro to Machine Learning class where we talked about sample complexity. Yes, that's good. That probably is a better name than experience complexity. Let's call it sample complexity. Yeah, because it really is about the data that's necessary to achieve the learning ends. All right. All right, so this kind of sets the stage for what reinforcement learning is, and how compare different learners to each other, and how we compare the outputs of those learners to each other. What we're going to dive into next is actual algorithms for doing this kind of learning. Yeah that sounds good, can I ask you a question? Sure. So, why aren't we talking about space? Ooh, so space complexity. So that's a good question. In computer science, usually computation time and computation space are the two dimensions along which we evaluate algorithms. Here I guess we could do that too. You could talk about the space that an algorithm uses. Our learning algorithm uses, but it's generally not that interesting. Generally we don't, we're not limited by the space complexity. We usually run into issues more quickly on the computational or the sample complexity side. So, yeah, so next time we're going to dive into algorithms and talk about temporal difference learning, so why don't we just sort of summarize the philosophical stuff that we talked about just now. Okay. So what have we learned? So, I think we learned a bunch of stuff, but we really learned three classes of things. We learned sort of what reinforcement learning is. All right, that's one class of things. Yeah, and as you wrote up there, we realize RL is about agents interacting with environments and getting rewards and figuring out what the world is. And by the way, as a part of that, we learned just how frustrating it is to actually do reinforcement learning rather than just solving an MDP. I agree with that. Okay, the second thing we learned or we talked about was a description of what a policy is and the different ways you might go around solving these RL problems and evaluating them. What does it mean to have a good policy? And then the third class of things that we learned was about evaluating learners. And we realized that there's lots of different ways to evaluate learners just like there's lots of different ways to evaluate policies and plans. You can talk about truncating them or doing things that are infinite. We can talk about whether a learning is fast, whether a learner is efficient in a different sense like with sample complexity whether it actually returns good values. If you think about it just in terms of it's output or how it got to its output. Right. And so we're going to make some choices so that we have concrete algorithms that we can present and analyses that we can present. But it's worth keeping in mind that some of these things are kind of arbitrary. I mean, they're justified. But there's other things that would also be justified. That seems fair in machine learning All right, well that all make sense to me Michael. So, I guess we'll continue this conversation next week. Sounds good. All right, have a good day. Hi, Charles. Hey, Michael. How's it going? Good, do you want to learn about temporal difference learning today? You know what I'd like to do? I'd like to learn about temporal difference learning today. Perfect, because that is the plan. And in fact, I need you to actually take a little bit of a break and read Sutton. What you mean the entire thing? Yeah it's just a paper. There's the Barto and Sutton book, and Sutton and Barto book, but this is actually the paper where he introduces temporal difference learning. Okay so you want me to go read that now. Yeah. I can wait, you can just pause me. How about I pretend I read it and then you take me on a odyssey of discovery. Perfect. All right. To introduce you to temporal difference learning. It helps to have a little bit of context about reinforcement learning in a more general way. So we can see how the different methods for doing reinforcement learning fit together. Okay. So, I'm going to think about reinforcement learning this way. Remember we talked about SARS, the state-action-reward sequence. So, I'm in a state. I take an action. I get a reward back. I end up in a new state. I take a new action. I get a new reward. And this just continues, so I put a little Kleene star after it. To indicate that this is some sequence of state, action, rewards. That's what comes as input to a reinforcement learning algorithm. And what the reinforcement learning algorithm produces as output is a policy. Sure, that makes sense. Hey, I got a quick notational question for you though. Sure. Should that really be a Kleene star, or should that be a plus? Well I'm being a little bit lax with notation here. But I'm imagining that, even if you give a reinforcement learning algorithm no history at all. It could still come out with an initial policy. Sure that makes sense. In fact, I guess that's true for every algorithm we've ever talked about. I think so. I mean it could be just be a default policy like in all states take whatever the first action is. But the fact of the matter is over time. The interesting case is when we've had some amount of history and we're trying to map that to some behavior. Okay I'm on the same page. So the way I like to think about it, is there's three main families of reinforcement learning algorithms. So, the one that's most like what we've talked about so far is what we could call model based reinforcement learning algorithm. So, model based reinforcement learning algorithms takes the state action reward tupples that it gets, and sends them to a model learner, which learns the transitions and rewards. Those transition and rewards, once you've learned them, you can put through an MDPSolver, like we talked about, which could be used to spit out a Q*, a optimal value function. And once you have the optimal value function, you can use, just by taking the argmax of the state that you are in, you can choose which action you should take in any given state and that gives you the policy. So it's still mapping the state action reward sequence to policies, but it's doing it by creating all these intermediate values in between. Cool, and let me just add one more thing, which is that the model learner takes the history that it's seeing. But it also takes its current estimate of transition to rewards to produce the new estimate of the transition rewards. So, this is how I want to kind of represent the learning piece of the sequence. Does that make sense? All right, so this is one type. Now let me show you another type. So, the second class of reinforcement learning algorithm that's important to think about. They're referred to as value-function-based, or actually, sometimes model-free. So, the beginning and the end of it are still the same. We're taking sequences of state action rewards and producing a policy and we even have this Q* in between that we generate the policy from using the argmax. But now, instead of feeding back the transitions and rewards, we're actually feeding back Q*, and we have a direct value update equation that takes state action reward that it just experienced, the current estimate of Q*. Actually, it's kind of more Q than Q*, and uses that to generate a new Q, which is then used to generate a policy. Yeah. So instead of explicitly building a model and using it, it just somehow directly learns the Q values from state actions and rewards. And then the third class of reinforcement learning algorithms comes from the idea that you can sometimes actually take the policy itself and feed that back to a policy update that directly modifies the policy based on the state action rewards that you receive. So, in some sense, this is much, much more direct, but the learning problem is very difficult because the kind of feedback that you're getting about the policy isn't really very useful for directly modifying the policy. So, here's these three models all together so you can compare and contrast. You can see, in some sense, they're getting simpler as [LAUGH] we go down to policy search. But you could also say that the learning is more direct. And as we go up this way, the learning problems become more supervised in the sense that you can imagine learning to predict next dates and next rewards from previous dates and previous rewards to learn T/R pretty directly. It's basically a supervised learning problem. You get to see what the output's supposed to be. Here, you don't quite get to see what the output's supposed to be, but you do get values that you can imagine propagating backwards. And here, you get very little useful feedback, in terms of how to change your policy to make it better. Wait, so, the way you put that suggests that you should always do one. Well, so, yeah, but look at all the extra stuff that you're doing in between. So, in fact, different problems have different trade offs, as to whether or not which of these is the best thing to do. A lot of the research has been focused on number two, and that's what we're going to be diving into, because in some ways it strikes a nice balance between keeping the computations relatively simple and keeping the learning updates relatively simple. But there are plenty of cases where you'd rather do one and where you'd rather do three. I look forward to you convincing me that that's true. All right, so that brings us to temporal difference learning. So, temporal difference learning, the algorithm we're going to look at originally by Sutton, is called TD Lambda. This is a little Lambda symbol. So TD stands for temporal difference. And what really it's about, is learning to make predictions that take place over time. And so as a concrete example, a concrete way of looking at this, we're going to look at how we can from a sequence of states, state zero goes to state one, goes to state two, goes to let's say a final state. And each time there's a transition, there's some reward that's associated with that. We're going to try to predict the expected sum of discounted rewards from that. Okay. Which is what we're trying to do anyway. Yes, so this is going to be a really important subroutine for trying to do reinforcement learning, because we're going to use the notion that we can predict future rewards to try to better choose actions to generate high rewards. Okay, that's fair. All right, so for the kind of prediction problems that we're going to be learning with temporal difference methods, it's helpful to have a concrete example to kind of ground out some of these terms. So, let's take this as a Markov chain. With states S1, S2, S3, S4, S5 and SF for the final state. Okay. I've labeled each of the transitions with the reward that happens when you make a transition from that to the next. And this particular transition out of S3 actually is a stochastic transition, with 90% probability going to S4, and ten percent probability going to S5. Okay, that makes sense. And what we'd like to learn here is a value function, a function that maps the state to some number, where that number is set to zero for the final state and for every other state it's the expected value of the reward plus the discounted value of the state that we end up in, averaged across all the different states we might end up in. Okay that makes sense. Great, okay, good, then let's have a quiz. [LAUGH] All right, and here's the quiz. What we're going to do is, since you understand this so well, figure out what V(S3) should be. So what is the value of state 3, given this Markov chain, and just to make things a little bit nicer, more friendly, just assume the gamma is one, so you don't have to multiply by funny numbers. Oh, that is very nice of you Michael. All right, go. All right Charles, let's work this out. We're going to work this out? Well, you're going to work this out. Ok well, so I'm looking at this. I actually think this ones pretty straight forward to do once you have the first insight. The first insight is, you want to go backwards. And the reason you want to go backwards is because I already know what the answer for the final state is, and I can go back one step and use that to figure out what the states before are, and so on and so forth. So with that, why don't we actually be explicit about it then. So. Yes. The value of s of f would be zero. That's a given, that's pretty easy, okay. So then I can ask, well what would be the value of s4 say. So s4 is the reward you get for leaving that state and taking the action. Plus gamma times the state you're going to end up in. Well, so that would be 1 + gamma x 0, which is 1. Well, that's pretty good. Okay, and then for S5 I can do the same thing. In this case it would be 10 + gamma x 0. Which of course is 10. All right, am I right so far? You are right so far, but that's not the question that was asked, it was S3. Oh, okay. Fair enough. All right, so let's do S3 then. So S3 would be the reward that I get, which is be 0 + gamma times. The value of the state that I end up in. Well, if I end up in state 4, that would be 1, but sometimes, I'll end up in state 5, and that would 10. So 90% of the time I end up in 1. 10% of the time I end up in 10. So I multiply the first one by 0.9, the second one by 0.1. And then I add them up and we go from there. And that number is .9 + 1 is that right. Which is 1.9. And that was easy because gamma was 1. Nice very good. So I think actually might as well go ahead what S1 and S2 would be then because why not. Okay. All right the value of S1 would be 1 Plus gamma times the value of S3 which is 1.9. And so that would be 2.9. And S2 would be 2 + 1.9, which is 3.9, so this is good. So if there was some state zero, and I had a choice between going to state one and and state two from it, I should go to state two. Yes, because it's just the same expect you get that extra one bonus going out of state two. Right, okay that makes sense. Great. Nicely done. Thank you. So let's think about how we can get those same values from data. Instead of you knowing in advance what the model of this particular markoff chain is. So let's imagine that in fact, what happens is we're going to do a series of simulations starting from S1. It's going to hop around and with some probability it's going to hop. It's going to go S1 to S3 and then either S4 or S5 and then to SF and then it stops. And so let's imagine that we've done that and these are the episodes. These are the sequences that come out of it that. So the first time we run it it goes from S1 to S3 to S4 to SF. The next time it goes S1 to S3 to S5 to SF, then it goes S1 to S3 to S4 to SF, and so on. So what I'm interested in is how we would actually estimate. Develop an estimate of the value of state one. Given just this data after say the first three episodes, or the first four episodes. The fifth episode doesn't involve S1 so presumably it shouldn't. Well, we'll get to what happens then. But first I want to think about after the first three or after the first four. That makes sense? Sure, I can do that. Excellent, all right, so do that. All right, go. All right, so do you have an answer for this quiz? I don't but I think I know how to get one. Let's go for it. Okay, so basically your asking me to estimate from data. So you really, what your really doing is asking me to do an expectation, right? Right. because that's what we're trying to estimate is an expectation. Uh-huh. Right, and the way you do that is you just average things. Yeah. So I should just be able to do that. So I can look at episode one and I can see what the value for S1 turned out to be. And because you were so nice and made gamma equal to 1, I can basically just add up the numbers on the arrows, right? Yeah. So the value to S1 in episode one is 2. Good. And the value to S1 in episode 2 is 11. Good. And the value to S1 in episode 3 is 2 again. Good. All right. So then if I wanted to know what an appropriate estimate was after 3 episodes, I would just average those three numbers. I would get 5. Is that right? Yeah, so 15 over 3 which is 5. Nice. Okay, so then for 4 I see 2 again and so that would be 17 over 4. Good. Which is a number. [LAUGH] 4.25. Good. Yeah, that's what I was looking for, nicely done. Oh, so in fact I could do this for one episode, or two episodes. That's right, in fact what we're going to work up to next is how we can incrementally compute an estimate for the value of a state, given the previous estimate. Given after 3 episodes, how do we get 4. And 4 how do we get 5? And 5 how do we get 6? And then 10,000 how do we get 10,001? Well, can I ask you a question? Sure. I'm curious how close we came. You go from episode 3 to episode 4 you went from 5 to 4.25. That's kind of a big jump. So, do your remember from the last quiz, what the last value of S1 was? I think it was 2. something. 2.9, that's right. Oh, it was 2.9. Yeah, so in principle, with an infinite amount of data, that's what we should get. We should get 2.9 as our estimate for S1. In this particular case, we're actually a little bit high. Why do you suppose we're a little bit high? Because we just haven't seen enough data yet, we saw 11 once. And, so it looks like for 3 episodes, 11 happens a third of the time instead of a tenth of the time. That's right. Yeah, so we have kind of an over-representation of the higher reward. So our estimate ends up being skewed high. Okay, that makes sense. In the limit it should all start to work out and we'll get one-tenth plus elevens. But in this case we got, I don't know. Let's call it lucky. We got a higher reward than the expectation would predict. Hm. Okay. Okay, Charles. I'm going to see if I can get you to derive a piece of the temporal difference equation just by working through this example again in a slightly different way. So from the last thing we talked about, we said that the value estimate for S1 after three episodes was 5. And then we ran an episode, and the return of that episode, sort of the total discounted reward for that episode, episode number 4, starting from that state S1, was +2. Could we just from this information figure out what the new estimate's going to be? Sure. So let's see if we can do that. Let's work through that, step by step. Oh, okay. Well I can cheat right, by just, I mean you have everything indexed by time so I know that five is an estimate of the last three, and four is an estimate of the fourth one. Right. So, I should be able to, you know, weight the first one with three, and the other with one, or something like that, and that'll give me the right answer. All right, so what do you mean by weight? So what you mean is since five is the average of the first three episodes then 15 is the total from the first three episodes, right? That's right. I've seen it three times. Good as if we had seen five three times. You know the details actually varied from that but that's okay because averages work out nicely. That's right doesn't matter. Then we got an additional reward of two and now we want to know the average over four steps right. So that gives us 15 + 2 / 4 is 17/4 which is just the answer we had in the quiz. In fact that's exactly how I did it. I just took the 15 over 3 that you had written down and just said, oh so that would be 17/4. Good. So let's generalize this a little bit and put in some letters because you know math. So let's say instead that we're talking about the previous estimate, which was from time T minus 1. Then we get a return on step T of T, and now we want to compute the new estimate, VT of S1. So we should be able to do that same kind of calculation with these quantities just symbolically. Okay so you want to try that again? Sure. So I think I know what the VT of S1 is equal to. (T-1) times V sub T-1 of S1 + R sub T of S1 divided by T. Good. Which was four before, but now in general it's T. Excellent. All right, so let's now do a little bit of algebra on this. Oh, sure. So first I'm just going to divide through by this capital T, and split it into two pieces of the sum. Okay. All right, so then we can regroup things as follows. Let's see, so I guess, mainly the idea is that we split this T-1 over T into a T over T, and a -1 over T. So that gets us (VT-1(S1)) + (1 over T (RT(S1))- (VT-1(S1)). So I wrote it kind of funky. I introduced this additional learning rate parameter, alpha sub T, the learning rate at times step T, but it's really just playing the role of one over T here, the averaging. But there's an interesting thing that popped out of this particular way of rewriting it. In that we are looking at kind of a temporal difference that our update to the value is going to equal the difference between the reward that we got at this step and the estimate that we had at the previous step. And so this difference is actually going to drive the learning. So if the difference is zero then things don't change. If the difference is big positive then it goes up then if it's big negative then it goes down. And as we get more and more episodes this learning parameter is getting smaller and smaller so we're making smaller and smaller changes. We're kind of honing in on the actual true values. So this actually looks just like the update rule for perceptrons. Yes, yes, and in fact that was I believe that what Sutton was trying to do. He wanted to make temporal predictions and he wanted to put it into the same kind of algebraic framework that people were familiar with from perceptron learning and from neural net learning. Right, so if we did it that way then RT minus V sub T is the error. Very good. Yeah, that's a great way to look at it. That we're making our update based on moving a little bit in the direction of there. Right, moving less and less and less as time goes on, which is just what you're supposed to do at the learning rate. Exactly. So, let me asks a question along those lines then. So if alpha sub T was not 1 over T but something else, like I don't know, 1 over the square root of T or something. Some other way of doing a learning rate. Uh-huh You would get a slightly different answer at any given time. But would you expect to kind of converge to the same answer ultimately? That is a really good question. In fact that is the next question I was going to ask. Oh wow. I feel better now. [LAUGH] So here it comes. So that was very astute of you. And in fact there's a very general property that could be stated about learning algorithms of this type. So if we have a learning update rule like the one that we have, which is that the value at time T is the value at Time T minus one, plus some learning rate times the difference between the new estimate and the previous estimate. Or the new observed value and the previous estimate, then this Vt will actually go to the true expectations, the actual average value of the Rs once T is big enough. Okay, so that's good. That means it's actually learning the right thing, but there are conditions that we have to put on the learning rate sequence. And the learning rate sequence has to have the property that if you sum up all the learning rates, it actually sums up to infinity. It's unbounded. it diverges. As opposed to greater than infinity. Oh, good point. Yes, that it's equal to infinity. Very good, it's actually greater than any finite number. Anyway, yeah it diverges, but the square of the learning rates, if you sum those up, it's less than infinity. And we're not going to get in the details about why these are the standard properties for making this kind of learning rule work. But the first approximation, the learning rates have to be big enough so that you can move to what the true value is, no matter where you start. But they can't be so big that they don't damp out the noise and actually do a proper job of averaging. So, if you just accept that these are the two conditions that make sure that that's true, that gives us a way of choosing different kinds of learning rate sequences. Okay so what's some alphas that tease to satisfy that and some that don't. That was a really good question so let's turn it into a quiz. So here's some five different examples of learning rate sequences. That you might consider using in one of these kinds of learning algorithms. There's our friend 1/T which came out of the analysis that we did to get out the average. So we're hoping that one should really work. But then let's look above and below it so 1/T squared. 1 / T to the two-thirds. 1 over square root of T. And actually, it's sometimes pretty popular to use learning rates that are just a constant. Just a small number that just stays constant over time. And so what I'd like us to figure out is which of these five rules satisfy the two conditions? In other words, that the sum of the learning rates is infinite. But the sum of the squares is finite. Okay, we can probably do that together. Cool. Okay, well go. So to actually figure this out we're going to need to check these two conditions. So we might actually find it useful to check each one of them individually. So here's the sum of the alphas, and here's the sum of the alphas squared. Okay. So how are we going to start filling some of these in? Are there any of these cases that we actually know what the sums are going to be? Yeah, two of them I think. Okay. At least. Well, let's go ahead and get the easiest one out of the way first. So I think that's number, well actually I think two of them are equally easy. Let's do number five. Great. So we know that that's going to be infinite, the sequence will be infinite because it's a constant. Yep. And we also know the square of which is just another constant, and that's non-zero, so it's also going to be infinite. So what does that tell us about this learning rate? It says if your learning rate doesn't get smaller over time, then you're never going to convert. That's right. So this one, this learning rate, doesn't work as far as the theorem's concerned? Right. You said there was another easy one. So number two I think is easy because you told me last time that that one works. [LAUGH] Yeah I guess that's a reasonable way to do it. Do you recognize this, the sum of these? Do you know what this is called if you sum up alpha to this sort of form? Isn't it a geometric series or is it harmonic? Which one is it? Harmonic. [SOUND] Yeah, that's right. That's our harmonic sequence and the sum of this up to T acts a lot like log of T, so if T goes to infinity, we do get infinity there. Okay, and we also know that the square of it is less than infinity, because you told me so last time. Yeah, though that's again, not the greatest reason. It turns out that any time that the power that we're raising, the T to in the denominator is bigger than one, it's going to converge, and so that gives us both of these boxes, right, because it's the same thing. Mm-hm. This problem in particular, this one it does, it converges, the value that it converges to is really interesting. I don't know if you know this, but if you sum up the infinite sequence of 1 over T squared, you get pi squared over 6. Oh, right, yes, that's obvious. No, it's not obvious. Well, explain it to me then. I can't, it's bizarre. It's like, where's the pi? Where's the pi? I want some pi. I ate it. All right. So this problem was known as the Basel problem, and for hundreds of years people didn't really know how to solve it, and Euler worked on it and solved it, but he didn't solve it validly, he eventually came back and solved it right. But it is, it's kind of a remarkable thing. Pi's come up when you're doing areas of circles and things like that, there's no circle here, so it's kind of interesting that you get a pi squared. Or r squared, right? Pi r squared [LAUGH] Pi r squared Let's hear it. Sure, but the pi's not squared, it's weird. I agree it's weird. I mean, I think I vaguely remember this from calc two or something. Cool, alright, well anyways, so that's just sort of a fun thing that I find to be fun, but the point is that, so let's look at the first row here. If we square T squared, we get T to the fourth, which is bigger than 1 over T, so that one's also going to converge. So what does that give us in terms of answers here? No and yes. Exactly, right, it's because we need it to be infinity and then less than infinity. One of these boxes we've actually figured out, because this 1 over T to the 1/2 or the square root of T, if you square that, that's 1 over T, so this squared box has to be the same as this one. So that's infinity, and actually that tells us the answer for this one even before we figure out the rest. Yeah, I guess so, but you gave the answer away a long time ago. How so? Because you said that if the exponent was greater than 1, then it would converge and if it was less than 1, then it would not converge. So I can just look at the numbers or the exponent for number three and number four, and I already know the answer. So let’s just fill them in then. Go ahead, what do you got? Okay, well, let’s see. If the exponent is greater than 1, then it will converge. Less than infinity, that's true for neither 2/3 or 1/2. So these are infinities? Right, and you just have to square them and ask the same question. Good. So we squared this one, and that gets us 1 over T, which also has an infinity. Right. And you square this one and you get 1 over T to the 4/3, which is bigger than one, so that one should converge. Right. So interestingly this one works, this one works, and this one doesn't. Right. So I think this leaves us in a position where we can actually state the TD(1) update rule. So it looks a little hairy, but we're going to be able to go through it and see that it does something very sensible, or at least somewhat sensible. So this is the structure of the rule, of TD(1). So for each time we start a new episode, we're going to initialize this thing called the eligibility, e(s) for all the states, to 0. So we just start them off as ineligible at the start of the episode. And we start off our estimate for the value function for this episode, to be initialized to whatever it was at the end of the previous episode. That makes sense. So then, each time we take a step within the episode, a step from some St-1 to some state St, getting reward rt, what we're going to do is first, update the eligibility of the state that we just left. Then go through all the states, compute this one quantity which is the same for all states, this is the temporal difference. This is the sum of the reward plus the discounted value of the state that we just got to, minus the state that we just left. And we're going to use the values that we had from the previous iteration, previous episode. That's going to be our temporal difference. And we're going to apply that temporal difference to all states, proportional to the eligibility of those states, and of course the learning rate, because we don't want to move too much. And we're just going to update them by that. We're going to increase them by this temporal difference times the eligibility. Then we decrease, or decay, the eligibility for those states. And then we're back up to the next step. Okay it's got all the right letters so I think I can imagine that this will do something. So, the for all S part, does that have to be done in specific order? No, because the S's are all being done in parallel here. So the value at state S is going to be updated based on this quantity, which is the same for everybody, doesn't depend on which S we're currently updating, and this quantity which is specific to the S that we're looking at. So it's all kind of happening in parallel without any dependencies between them. Yeah, okay, I see. So because there's an order to the states and the way we're going to visit them, you basically get to see that value kind of applied everywhere. Okay, okay, I think I understand. Okay, all right, so let's kind of step this through in a tiny little example. And then see the magic of it, see that it actually produces the same outcome that we would have gotten with the update rule that had talked about previously. Okay. All right, so let's walk through the pseudocode with a very tiny little example in mind just to see how the value updates work. So what we're doing here is we start off at the beginning of an episode. We say that the eligibility for all of the states is 0. Zero. And the new values, the new value function is whatever the value function was at the end of the previous episode. So we just make a note of that. What we're going to do for this example is just keep track of the changes. So this ultimately it's all going to get added to whatever the previous value was. All right, so now we make a transition. And the first transition we make is from S1 to S2 to with reward r1. What does that do to the eligibility? Well, for S1 it sets the eligibility to 1. Good. All right, then we're now going to loop through all the states, all of them, and we're going to apply the same little update to all of them. And the update has the form, whatever the current learning rate is times the reward that we just experienced, the R1, plus gamma times whatever the previous iteration value was for the state that we landed in. Minus the previous iteration's value of the state that we just left. All right, so this quantity is going to actually get added to all the states, but the amount that it's going to get added to all the states is proportional to the eligibility for that state. And since, at the moment, states S2 and S3 are ineligible, they have their eligibility set to 0, really nothing's happening there, we're only making an update with respect to state S1. Oh, okay I see. So then why are we bothering? Well, it's not clear yet. For now, let's just make sure that it does the right thing. Then we'll talk about why it's a reasonable thing to have done. Okay, I trust you Michael, implicitly. All right, so that's the changes that we've made so far to the new value function for S1, S2, and S3. All right, now we take the next step which is from S2 to S3. Oh wait, no, sorry. Before that, we have to decay the eligibilities for everybody. So the 0s decay to still 0, and the 1 decays down to gamma. Right. Now, we've done that update from the step that we just took, we take the next step, S2 to S3 reward r2. So, what does it tell us to do? Well, the first thing is, we have to update the eligibility of the state that we just left, so that bumps S2s eligibility up to 1. Then we go through everybody and we add this quantity again. So what's the new quantity? It's r2 plus gamma times the value of the state that we just left minus the previous value of the state that we, no sorry, the state we just got to and the state we just left. All right, so this is this update that is now independent of which state we're actually changing. We're going to apply that everywhere. But we're going to apply proportionally to the eligibility. So the eligibilities are at the moment gamma 1 and 0 so gamma 1 and 0. So we just have to add this into the other things. What does that look like? So through the magic of cut and paste, we can actually add that same complex quantity to both these updates here. And in particular, we're multiplying by the eligibility. So for the first value, we have this extra gamma in here. For the second one, nothing. And for the third one, it gets multiplied by zero, so I didn't even bother to write it. So this is starting to look a little bit hairy, but that's okay, because we're going to do some simplification. Okay. So here's the interesting thing to observe. That previously we added in this value of S2, according to the previous value function, times gamma. And now we're subtracting away the value of S2 from the previous iteration. And we're multiplying that by this gamma and this alpha. So they actually match exactly. This thing [SOUND] and this thing [SOUND] cancel. That's beautiful. So now I can rearrange the remaining terms and make it look a little bit simpler. All right, and this is the simplified form that we get. It compactifies really nicely actually and so we have something that's of a similar form that we had before. But the point is that mainly what just happened is we regrouped terms and the S2 related stuff in this term, in this update here, cancel. So the last thing we do after we've done all those value updates is we update the eligibilities and we're ready for the next step. The next step takes us from S3 to SF. Then we get reward R3 when that happens. The first thing that happens is we update the eligibility for that state. Then, we compute this quantity that we're going to add to all the state updates. This gives us R3 plus gamma times the previous value of the state that we just ended up in minus the state that we came from. And this update is going to get applied to all the states, proportional to their eligibility. So, we can do that same thing again, just add this quantity into all these things and simplify. Does that sound okay? It does sound okay and I think I finally see where this is going. So, if you look at the pseudo code right, you say for all s, and you update the value of the state, if you look at the stuff that's multiplied by alpha sub T, all that is basically is your error. Right? [CROSSTALK] Right, that's why this is TD learning. Right, it's what you thought it was. The difference between what you thought it was and what it turned out to be when you actually saw the reward if you assume that your future guess is about the values are right. Exactly. So, because of that cute little compactify telescoping thing that you just did, I think without actually writing everything out, I can guess what the next values are going to look like. So, and tell me if I'm right. The delta for VT of S1 is basically going to have seen one more thing. So, it'll end up being R1 plus gamma R2 plus gamma squared R3. And where does that come from? Well, that comes from the term that you just added over there. The R3 and the eligibility for state S1. Right, which you've managed to make the gamma squared. So, the eligibility isn't just whether you've seen a state before. It's actually keeping track of how long ago you saw that state. Good. Right and so, if there was another state after SF and we kept going, we'd see it get that would become gamma cube, then S2 would become gamma squared, and S3 would become gamma, and so on and so forth. That's quite clever. Okay, so you would see R1 plus gamma R2 plus gamma squared R3 plus gamma cubed (VT-1(S4)). Well, there is no S4 to the final state. I'm sorry, SF, thank you, thank you very much. SF minus (VT-1(S1)), so that stays. In all that is is the error, it's what you're guess was of the value. And what you actually ended up seeing, which was all the rewards that you saw plus whatever future value there's going to be. Good. All right, we may have done a little too much jumping. So, can I fill that in with the way that you just described it? Okay. So, some frustrating amount of algebra and rewriting later, it seems to have fallen into this pattern and this is what you're talking about, right Charles? Yeah, that's exactly what I was thinking. So, we have this sort of temporal difference thing. So, we can now state a claim which just follows very clearly from the example. And that is that this update, this TD(1) update that we just talked about, actually does the same thing as the outcome-based update, which is to say, we wait to see what all the rewards, the discounted rewards, are on the entire trajectory, and then we just update our prediction for the state that they started from with those rewards. Keep in mind that these things are zero. The VT minus one of SF is zero, so this kind of goes away, and this kind of goes away, and this kind of goes away and we're really just talking about the discounted sum of rewards minus the old prediction. The discounted sum of rewards minus the old prediction. The discounted sum of rewards minus the old prediction. So, at least in the case where there is no repeated states, we visit a state more than once along a trajectory. Everything beautifully cancels and what you get is exactly the same update. The TD(1) update is exactly the same update as just waiting for the outcomes and updating based on that. That's actually really impressive. So what happens if there are repeated states? So if there were repeated states then, oh I see. So let's imagine that we did outcome based updates sort of then we saw repeated states. And all you're really doing is ignoring anything, oh I see, okay, okay. How about this, Michael? Tell me if what I'm saying makes sense. Sure. So if we were doing outcome-based updates, then basically, you're just looking at all the rewards that you see, the values that you kind of expected along the way, and it's exactly what you have written down before. But if we had a repeated state, then you're basically ignoring anything you learned along the way. So [CROSSTALK] During the episode, right. During the episode. So outcome-based updates learn nothing during the episode. So if your sequence up there, instead of being S1, S2, S3, SF, had been S1, S2, S3, S1, SF, then what would have happened is I would have seen the rewards that I saw, r1, r2, r3, and then some other reward that I would see. Let's call it r1 prime or something, I don't know. I would be pretending as if I didn't already know something about state one, because I also saw it go from state one to state two, and saw a particular reward r1. Yes. I'd be ignoring that. So what this TD(1) update lets you do is, when you see s one again, and sort of back up its value. You've actually captured the fact that last time you were in s1, you actually went to s2 and saw reward r1. Right, exactly, so it's like the outcome-based on updates now with extra learning or inside the episode learning. I like it, beautiful. So is that better? Let's say yes. Excellent. That makes me feel good about myself. All right, Charles. So now that I've talked through how we can compute these outcome based estimates with these TD1 estimates, I'm going to try to tell you that it's not exactly what we want. I'm going try to explain why TD1 kind of is wrong sometimes. Okay. To do this we're going to go back to the example we've looked at before where we've got this markup chain with rewards and we're observing some trajectories through it like S1, S3, S4, S final. And so we observe a bunch of those and then the last one we observe starting from S2. S2, S3 S5, SF, okay? And now what we want to figure out is what does TD1, or an outcome based estimate, give us as an approximation for the value of S2 given this set of data? And then we'll compare it to what we get with the maximum likelihood estimate. Okay. Wait, you want me to do this? Yeah. Oh, okay, so it really is a quiz? Yeah, that's what this word is. All right, fine. I think I have some questions, but in fact, let me ask some of them now. Yeah, I think we should make sure it's clear enough that you can work on it. Okay, so you want me to tell you what the value of S2 is or what I would estimate the value of S2 to be using sort of the TD1 outcome based estimate. And there it should be the average of the reward sequences that we've seen upon encountering S2. Okay that makes sense but that has to start from somewhere. Right, the equations require you start from somewhere? We can just average it across all the encounters that we've had of that thing. Since we're averaging the first time we see it, that is the estimate. The second time we see it that's half of it. Okay that's fair. Okay I can deal with that. And by maximum likelihood estimate you just mean the value that best fits the model, which in this case is the MDP itself? Yeah, so given the data that we've seen so far, you can make an estimate of how likely it is to go from S3 to S4, S3 to S5, S1 to S3, S2 to S3. So you can use the data that you’ve seen to estimate the probabilities in this model, which aren’t going to be exactly what they show here because this is the true model. But we can estimate it based on the data. And then we can say, okay, given that estimate, what is the value of S2? Okay, that seems reasonable. Okay, I can do that. All right. So but you should probably also ask what the discount factor is. Oh, yeah, what’s the discount factor? Good question. Just for simplicity we'll go with gam equals one again. Okay, go. Okay, so let's go through this. Good. So let's look at the outcome based estimate. Good. I think you kind of gave me everything I need here. [LAUGH] So- Yeah, it's possible I spelled it out too detailed. You can never spell that out in too detailed way. So if I look at it, we encounter S2 exactly once! Exactly once. And so the average of one is pretty easy. It's that thing. Okay, and what is the estimate for that one? Well, 2 plus 0 plus 10 which is 12. Because gamma's one. Yeah. Because gamma's one. So, good. So, this is saying, because we only ever encountered S2 once and the one time that we saw it we ended up with a reward of 12 getting to the final state. That's going to be our estimate. That's what TD one would actually do moduled learning rate and how you initialize the values and so forth, but that's sort of what it's trying to be like given this data. Right. All right, so now, does that match with the maximum likelihood estimate, the estimate that we'd get if you use the data to build a model and then solved it? So for the maximum likelihood estimate, All I really need to do is to compute, let's see, the probability from going from S3 to S4 and the probability of going from S3 to S5. Because everything else is deterministic, and in all five of those transitions, I get to see every state and every reward. So I have everything that I need. So let's just go ahead and estimate the probability. So looking it through I go from S3 to S4 three times. 2, 3 out of 5 times we're in S3, good. Right. So that would be .6. 3/5 is .6 yes, got it. And by process of subtracting that means that going from S3 to S5 happens .4 of the time. Yeah. And we could compute that directly too by just counting, we get twice out of five times. So those aren't right. It should be 0.1 not 0.4, and it should be 0.9 not 0.6, but hey, that's the data that we've got. Yeah, you're right. It's much more even, but S4 is more common than S5. Right. Okay. Good. So with that I can just propagate values backwards through the NDP, just like we did on the final exam for 7641. Okay. So. Good. So let's see. The value of SF is 0. The value of S4 is 1. 1 more. 1 plus 0. And the value of S5 is 10. Good. Plus 0. All right. So, and that's all actually correct. Yeah, that is actually correct. Thank you determinism. Okay. [LAUGH] So S3 then. .6 of the time we'll get a value of 1, and .4 of the time we'll get a value of 10. So that would be .6 plus 4. 4.6? That is indeed 4.6. Cool. All right, so who cares what S1 is but I guess we can go ahead and compute it. It would be 5.6. Good. because it's plus 1, 4.6. But the actual question was about S2, dun dun dun. And so it is 6.6. 6.6, all right. Truth by addition And it works out pretty well. Okay, so 6.6 is very different from 12. Sure. I'm not sure either one of them is right, so let's see. If I recall when we went over this MVP before using the actual transition probabilities, .9 and .1. We figured out that the value of S3 was 1.9? Yep. Yeah, and we can check that. That would be 0.9 times 1 plus 0.1 times 10. So yeah, that'd be 1.9, which means the value of S2 would be 3.9. All right, so the actual value's 3.9. The maximum likelihood estimate is 6.6, which is more. But the outcome-based estimate is 12, which is a lot more. Right, so everybody's wrong except for 3.9. But maximum likelihood estimate is less wrong. Right, and so why in this case? What's the difference between these cases? First of all, why are these so far off? And second of all, why is the outcome-based one even further off than the maximum likelihood? Well, I think it’s pretty simple, right? When we computed the outcome-based one, we basically only used one of the five trajectories to propagate information back. Mm-hm. But when we did the maximum likelihood estimate, we used information from all five of the trajectories. Even though we only encountered S2 once. Mm-hm. And so that's just more information and it's more accurate, and you would imagine that if I triple the amount of data the 6.6 would very quickly get closer to 3.9. And the 12 might not move at all if I never see S2 again. Hm. Good. So this maximum likelihood estimate is actually using more of the data. I think that's accurate. The reason it's higher than it should be is because we got, I don't know, lucky I guess, and had a bunch more transitions to the high reward state, S5, than we would expect on average. And we got really unlucky, or really lucky, depending on how you think about it from this one trajectory we did from S2, we got the unlikely outcome which is high reward and so we ended up with an estimate that is way higher than it really should be. Yeah, but look. Let's imagine that we had gotten lucky the other way, there's only one other possibility, right, S2 to S3 to S4 to SF. And if that had happened then we would have seen the value of 1 instead of a value of 10 and we would have had 3. That's right, and on average it's going to turn out to be the right answer, this 3.9. But this estimate is based on the limited data that we've got. Right. Okay, fair enough. All right, but the punch line here is that this TD(1) idea, it's not using all the data, it's using just individual runs. And so when a rare thing happens on a run, you could end up with an estimate that's quite far off, and it could maybe stay that way for a long time. Whereas the maximum likelihood estimate seemed to push you in the direction of using the data that you've got to do the best you can. And so what this is pointing out is that maybe we don't want to just do TD(1). There's some nice things about TD(1), but maybe we can shore up some of these other issues and come up with something even better. What, without actually building an entire model? Yes! Okay, well I look forward to that. All right, so here is a rule we're going to call the TD (0) rule, which gives it a different name from TD (1). But eventually we're going to connect the zero and the one together. And the rule looks like this, not so unfamiliar looking. That the way we're going to compute our value estimate for the state that we just left, when we make a transition at epoch T for trajectory T, big T, is what the previous value was. Plus we're going to move a little bit in the direction with our learning rate toward what? Toward the reward that we observed, plus the discounted estimated value of the state that we ended up in, minus the estimated value of this state that we just left. So does this look familiar? It's got Vs, and rs, and Alphas is in it. And so- [CROSSTALK] Well sure, all right, but let's maybe we can be a little more precise about this. So what would we expect this outcome to look like on average, right? So the thing that's random here, at least the way we've been talking about it is, if we were in some state St-1 and we make a transition, we don't know what state we're going to end up in. But there's some probabilities of those. So basically this update is being done more often in the states that we end up in more often, the more common Sts, and less often in the Sts that are less common, or less likely. And so really if we take the expectation of this, what is it going to look like? If we repeat this update over and over again, what we're really doing is sampling different possible St values, right? So really we're taking an expectation over what we get as the next state of the reward plus the discounted estimated value of that next state. That seems right, that's kind of what we want. Yeah, so this is exactly what the maximum likelihood estimate is supposed to be. As long as these probabilities for what the next state is going to be match what the data has shown so far as the transition to St. Is that what that blue stuff above the equations is about? Yeah, so here's the idea, is that if we repeat this update rule on the finite data that we've got over and over and over again, then we're actually taking an average with respect to how often we've seen each of those transitions. So it really is computing the maximum likelihood estimate. So this does the right thing. Oh, okay. So that finite data thing's important because if we had infinite data, then everything would work. So right, in the infinite data case this is also true, but also TD(1) does the right thing in infinite data. Kind of everything does the right thing in infinite data. Yeah, everything's getting all averaged out, and it'll do the right thing. But here we've got a finite set of data so far. And the issue is that if we run our update rule over that data over and over and over again, then we're going to get the effect of having a maximum likelihood model. And that's not true of the outcome based model. Because in the data that we just saw where we were only saw one transition from S2 to anything else, we can run over that over and over and over again. But the estimate is not going to change, it's always going to be exactly that. Okay, that makes sense. Okay, sure, sure, I'll buy that. We can contrast this with the outcome based idea where we're not doing this sort of bootstrapping. We're not using the estimate that we've gotten at some other state. We actually use literally the reward sequence that we saw. And so as a result of that, if we've only seen a reward sequence once, like in the case of S2, repeating that update over and over again doesn't change anything. Right, sure, sure, right because the expectation is the expectation. Right, whereas in here, we're actually using the intermediate estimates that we've computed and refined on all the intermediate nodes. All the states that we've encountered along the way to improve our estimate of the value of every other state, right? So this kind of is more self consistent, right? It's actually kind of connecting the values of states to the other values of the states which is what you'd want. And this is more just literally using the experience that it saw and ignoring the fact that there's these intermediate states. Okay, well, when you put it that way, it makes it sound like you should always use TD(0) and never use TD(1). Yes, but that's oversimplifying. Okay, can you under-simplify it? [LAUGH] We'll get to that. Oh, okay, good, I look forward to it. All right, so now we've talked about two different algorithms, TD(0) and TD(1). And what we'd like to do next is show that, in fact, there is a larger algorithm that actually includes both of those as special cases. And so we're going to write that as TD(lambda), and it's going to have the property that when lambda is set to 0, we get TD(0), and when lambda is set to 1, we get TD(1). But we also get update rules for all sorts of in-between values of lambda. I don't know, that sounds kind of like witchcraft to me. [LAUGH] No, no, no it's just algorithm design, it's all good. Hm, all right, well I'll believe it when I see it. In particular, what's going to let us do that is the fact that both TD(0) and TD(1), the way that we were writing them, have updates that are based on differences between temporally successive predictions. Temporal differences, that's the TD part. And so we can actually unite them into one and, I don't need to say that again, because I've already said it. All right. So I copied the algorithm back down here. And do you know which algorithm [LAUGH] this is? And the hope is that it's a little bit challenging because, in fact, if it's TD(0) or it's TD(1), they actually look an awful lot the same. They have a lot of stuff in common. The update rule itself looks almost identical, but they are different. So which one is this? I'm trying to remember. And let's see, well, let's see. There's a 0 in one line and a 1 in another line so that's not going to help me. I was hoping I could the number pf 1s and 0s. [LAUGH] Actually, there's a bunch of 1, right. There's a (St-1), and there's another 1, and here's another 1, and here's another 1, and another 1. Yeah, but most of those are -1s, not +1s. I mean, you didn't ask me if it was TD(-1). So maybe this is TD(-1). Oh, that probably suggests that the way I was going about this was wrong. [LAUGH] I think it's TD(1). All right, and you have any particular reason for that? Because I recall TD(1) had a bunch of e's in it. Yes, it is easy to tell the difference. So e here, the eligibility, is something that doesn't show up in the TD(0) rule the way that we wrote it, but it does show up in TD(1). This sort of idea that what we're going to do is we're going to update the values for all states, all s. And the amount that we do this same update varies depending on the current eligibility of that state in question. Okay, that seems reasonable. I feel better now. All right, and so to highlight the similarities and differences between TD(1) and TD(0), I took the TD(1) algorithm that we just had and shrunk it and moved it over to the left. And then made another copy of it, put it on the right, and made the minimum changes necessary to turn TD(1), what was TD(1), into TD(0). And so mainly all that it involved was, any time the eligibility was mentioned, I kind of get rid of it. So I got rid of it from there, I got rid of it from there. And I don't multiply it here against the temporal update, and I don't have to update it here. And the only other difference, other than just getting rid of all references to the eligibility trace, is where as in TD(1) we do this update over all states, in TD(0) we just update the state that we left most recently. So I changed that loop. [LAUGH] Instead of making it a loop, it actually is just says for all s=st-1. In other words, we are just going to do this update for S=St-1. All right, so now we are ready to describe the TD(lambda) rule. What I have done now, is I got rid of those words and I took the TD(1) rule and copied it up here. So this is currently TD(1). And what we now need to do to it is make changes to it, small as we can, so that it turns it into T (0) when lambda is equal to 0. So we need to introduce a lambda. So here's what I'm going to do, just to kind of cut to the chase. I'm going to take this gamma decay of the eligibility. And I'm going to change it to a lambda gamma decay of the eligibility trace, right? So all I've done here is, it's exactly TD(lambda), except I just threw in an extra lambda here as a multiplier in the update of the eligibility role. And boom, we're done. Okay, so wait, okay, right. So I was raised to never put two Greek letters next to one another. But this does, do I buy this? Okay so if lambda is 1 then, well, nothing's changed, it's exactly the same algorithm as before. Right, great, because [LAUGH] if lambda's equal to 1 and we're just multiplying this lambda in there, then nothing's changed. So, boom, it really is, and it was TD(1) and it is TD(1). Right, so let's see, what happens is lambda is 0? Well, for the thing we just looked at, that means it's 0. So you just set yes, the eligibility trace equal to 0. Right. But, you started out with it being equal to 0 too so let's go back to the top. So let's see, for all S if S is 0, which is where we started out at the start of the episode, that's nice. Then we update e(S sub t -1) to be e sub s(t+1). So now e sub s(t) = 1. Right, good, okay, so the eligibility of the state that we just left gets bumped up to one, that's right. And everything else is 0. Now we loop over all states. Right, and we update V sub t(s) to be this thing times, well, it'll be 0 for everything except for S(t-1). Good. And then we will take the eligibility for that state and set it equal to 0. Right so, iin particular on this update, the eligibility is 0 for all the S's not equal to S(t-1), so nothing gets changed there. And for the one where it is equal to S(t-1), its eligibility is 1. So we just do it, we just update based on that, which is exactly what we have for the TD(0) rule. And then right, as you say, we zero out the eligibility and start it all again. So it's almost like, it just briefly remembers where it needs to update and then it loses that memory right away, goes right back into forgetful mode. Although that won't matter anyway because you're doing it for every S once. And then you'll pop back up and do the episode again, and you start all over again. It's like an invariant, and so. Oh, but that's only at the end of the episode, so we really only pop back to, right, when we're doing these updates, this is after each little t transition, so This part we're actually going to execute over, and over, and over again, until the end of the episode. So we don't zero out the eligibility until the end of the episode. [INAUDIBLE] In fact, using lambda equal to zero here, it has the same effect as it, so we don't really need to do that. Exactly. Okay, so then the right thing happens. Yeah, so when lambda's equal to zero, boom we get TD(0), and when lambda's equal to one, boom we get TD(1). And so, I guess what makes this interesting is not just that we can cleverly shoehorn two algorithms into one, but that it allows us to actually use values of lambda that are in-between which gives us kind of elements of both of these. It has TD0ish things to it and TD1ish things to it, and what we're going to look at next is how to think about this rule. This generalized TD lambda rule, so that it kind of makes some sense so that it kind of grounds it into quantities that we can understand. I look forward to that. So turns out if we want to understand td lambda, a really useful of studying it is by studying naught td lambda. So in particular, we're going to look at k step estimators for a moment and then we're going to relate that back to td lambda. So here what a k step estimator is. So let's say, if we're trying to estimate the value of a state, st. So it's a state that we just are leaving as a function of what happens next. So let's look at this first estimator here. It says that we're going to estimate the value of a state that we're leaving by moving it a little bit in the direction of the immediate reward plus the discounted estimated value of the state that we landed in. And we're going to just move a little bit in that direction. So, do you recognize this? Have you seen this before? I believe I have seen this before. What do we call it? I don't remember anymore. Oh, well that's fortunate. But I've seen it before, it's just like a one step look ahead sort of thing. Yeah, exactly which is what we're calling td0. All right. Yes, yes, that makes sense. And in fact, you're exactly right. That the way to think about is there's a one step estimator. So it's a one step estimator. In that, we'll use the immediate reward that we got plus the value of the state that we land in. But let's contrast that with a two-step estimator. We call it e2. Estimator 2, where what we do is to estimate the value of a state that we're leaving. We're going to move a little bit int he direction of the immediate reward that we received, plus the discounted reward that we receive next, plus the double discounted value of the state that we landed in two steps from now. So now instead of where we were before, we did an estimate by taking one reward. And we estimate the rest of the sequence using our current estimates. Here we're taking two rewards, and using that as our immediate estimate plus our estimate of the future taken as the state that we reach two steps from now. So does that make sense kind of as an extension of that idea? It not only makes sense though, if e1 is at one-step look ahead, then e2 is two-step look ahead. Yeah. Yeah exactly. So we used two steps, two real steps followed by this kind of look ahead thing. And we can do that for three, right here. And we can do that for k, where we're estimating the value of a state by k rewards summed up here, plus and then we estimate the value of the future by looking at the state we end up in k steps from now, and discounting that by k steps. And in fact, if we continue doing this arbitrarily, we eventually get to e infinity. Where, let's see what would happen int that case. So we get the sum of rewards, and it keeps going and it keeps going. And eventually at infinity, we look at the state that we end up at infinity, which of course, isn't a real thing. But then, it's okay, because we discounted by gamma to the infinity, which is zero. So this piece of the term actually drops out, and we just end up with this infinite sum of rewards. Now, this last estimator should look familiar to you too, Charles. Yeah, that looks like td1. Good! So now we've got td0 on the one side, as a one step estimator and td1 on the other side, as an infinity step estimator. And we've got these other things in between that we don't really have or possibly even need a name for. Well, that makes a lot more sense than with the title k-step estimators. I was worried this was going to turn in some kind of. Commentary about dub step. Oh, yes, break it down. So that's not going to happen in this case. [LAUGH] It's probably for the best. So what I need to do next is actually relate these quantities to td lambda which we're going to show is a weighted combination of all these estimators. All infinity of them? All infinity of them. Oh, that's pretty impressive. So here's how we relate the K-step estimators that we just talked about with TD lambda. So the idea is this, we're actually going to be using a weighted combination of all of the estimators for any one of these TD lambda algorithms. And the weights are going to look like this, one minus lambda for the one step estimator. Lambda times one minus lambda for the two step estimator. Lambda squared times one minus lambda for the three step estimator. Lambda to the K minus one times one minus lambda for the K step estimator and then all the way down to the essentially lambda the infinity, but of course that wouldn't, literally doesn't get any weight. Wait, what's Except, well, let's leave it at that for now and then we'll explain why that's an okay thing to say in a second. Okay. So, so far everything you said makes sense except you haven't told me what you're weighting exactly. Oh, so what we're going to do is when we move the value, when we change the value of a state, we're going to change it by all the different estimators. All the different estimators are going to give us their own preferences to what we should be moving the value toward. And we're just going to take a convex combination, a weighted combination of all of them. Each time that we do an update for the value of a state. Oh, I see. Now we don't literally get to lift all these and update based on them. But equivalently this is what we're doing. So there's a couple things that we should probably check. One is we should probably make sure that these weights sum up to one. Mm-hm. So, how do we do that? Let's see. You write a summation? Good idea. So the summation is the all the different estimators from one to infinity of the weight applied to that estimator, which this formula works for all different values of K. For K equals one it's lambda to the zero which is one times one minus lambda. So we can now factor out the one minus lambda. As long as lambda's less than one, we can use this as a geometric series. And we get one over one minus lambda, and that's going to cancel with this one minus lambda and so the weights sum up to one. If lambda equals 1 though, what happens then? These are all zeros and this infinity one is one to the infinity which is just one. So when lambda is one we put all of our wait on this last estimator. And what happens if lambda is zero? Sp if lambda is zero in this case, we get a weight of one on the first estimator and zero on all the rest, which I'll write this way, Mm-hm. And when lambda is one we get zero for all these except for the last one. Which let's say at least in the limit would be one. So this is actually as we said this is giving us TD(0) and this is giving us TD(1). So if we just use lambda in place of one and zero, which works, then we get td lambda. But the important thing is for in between values of lambda, like a half, or .3 or something like that. We actually get weights on all of these to some degree. And it's combining all those different estimators together. And that seems okay. Any good characterization of what that weight looks like. So if lambda were equal to one-half, is that exactly halfway between TD(0) and TD(1)? Well, when lambda's a half, what do our weights look like? We get a weight of a half on this one. Mm-hm. We get a weight of a half times a half, or a quarter, on this one. Mm-hm. We get a weight of a half times a half times a half, or an eighth, on this one. So we get this sequence of decreasing weight, exponentially decreasing weight. Is that halfway between these two things? I don't know. I suppose you could define it to be. But it is just what it is. It's a combination of a whole bunch of estimators together. Right. That eventually gets to zero. Sure, but again, we're combining the outputs of all these estimators. We're using whatever update they're suggesting and moving a little bit in the direction of the first one and a little less in the direction of the second one, a little bit less in the direction of the third one and so on. So, yeah, in the limit we're not actually doing the e infinity estimator at all, but that's okay because we're doing all these estimators that kind of lead up to it. So I don't know, does that help you understand TD(lambda) better? This is the way I think about it, is to say well, it's a little bit hard to interpret but what we're really doing is doing a weighted combination of a bunch of step estimators instead of this sort of strange, continuous parameter. Well, I guess so. I mean, when I think about it the way you put it I guess what you're saying is, look, you could do one step look ahead or you can do two step look ahead. Or you can go all the way into the end of your episode which may last forever and you can pick any of those. Or you can say each one of those gives me some information and I should just figure out some way to combine them. And I can pick various ways for combining them and I know particularly know how to turn the way I combine them into simple English sentence. But I am finding ways to combine various ones of them and I should pick some values of lambda that seems to work for the problem that I care about. Yeah. So here we go. All right, so now given that we don't actually have a crisp characterization of what something like lambda equals one or sorry, lambda equals a half, really gives us in words. Why do we care about these intermediate values? So I'm going to make a transition and talk about next what happens empirically when you use TD lambda. Okay. All right, so let's see if we can think through, or maybe guess what's going to happen if TD lambda is run as an update algorithm on some finite amount of data. So we it, we give the system some finite amount of data, and then we run TD lambda with different possible values of lambda. And then we ask, alright, the V that you've learned, how far is that from what the V should have been, if we had infinite data? Hm-mm. Alright so, I don't know, let's see if we can figure out, or maybe even guess, what this might look like. So, let's say if we use lambda equals zero, so we do TD zero. So after some finite amount data, there's going to be some amount of error. Let's just you know, I don't know, call it that. Okay. Now it turns out, empirically, if you play with this, it's pretty typical for TD one to give you worse error, given finite data. And now the important thing is what happens in between. So it could be like a straight line if it was literally interpolating, it could be kind of bowed up, but it's actually typically bowed down. So it's not so atypical to see a curve kind of like this, that gets better. The amount of error goes down as lambda gets to some intermediate value let's call it, I don't know .7. It depends of course on this, the particular problem. And then kind of start to shoot up again and it eventually hit the error for TD one as lambda approaches one. And so this is kind of the justification for why we want to consider TD lambda. That it's actually combining these two different sources of information. What happens at the end of the episode and what happens at each step, in a way that actually takes advantage of both. It gets you the benefits of both. We're rapidly propagating information and we're not getting a biased estimate, because of the way that we're using one step information. So, you know, it has some attractiveness to it. Huh, did you pick 0.7 on purpose? I remember reading a paper once that suggested that basically everyone uses lambda equals to 0.7. I typically see 0.3 to 0.7 as values where things tend to work out where this bottom tends to fall, somewhere between 0.3 and 0.7. Personally I always use zero, because it works better with the control. I have had better performance and better understanding of what happens in the zero case. But it is true that if you actually run it with a finite amount of data, it tends to bottom out somewhere between 0.3 and 0.7. Hmm, okay, that's worth note. So that's actually all I wanted to tell you about TD lambda. I wanted to give you a sense of how the equations are defined, and how they're derived, and empirically, why this is something we actually care about. So, maybe we should sum up? I'm big fan of sum. Is this going to be an infinite sum? [LAUGH] Thank you, I've been here all week. Okay Charles so we've been talking about temporal difference learning. And in particular various flavors of the TD lambda algorithm. Can you remember some of the important things that we've touched on? Well I think you just said the big one. That all this discussion has really been about temporal difference learning. And in a nutshell what's a temporal difference? Well it's the difference between things you see on different subsequent time steps. What kind of things in different time steps? Reward. So, sort of the difference in value estimates as we go from one step to another, right? That's exactly right, because value is just a nice summary statistic for long-term reward. Good, all right. But actually, I think it was important that you started off talking about this is the thing that we wanted to do, but first you put it in context. By talking about different ways that we could solve the reinforcement learning problem. As I recall, there were three ways to think about it: model based learning, value based learning, and let's say policy based learning. That's right and TD methods kind of fall where? In the value based. All right then, we dove into the update rules themselves. So what do we talk about there? We actually deroved? [LAUGHS] Derived? Either one of those things. It's just semantics. This kind of incremental way. Of building up estimates of the values, and I think you called it outcome based. And I suppose all that really means, of course, is that you actually look at the outcomes of what you experience from different trajectories. And episodes through the space, and you use that to build up your sort of estimate of what the values of various states are. Yeah, and in particular, the idea of an outcome based says, well. You can kind of treat reinforcement learning as a kind of supervised learning. Just wait for time to stop and see what happened, and use that as a training example. But we can't really wait for time to end, because it's kind of too late to use any of the information there. So what we want are incremental methods, methods that can actually do updates along the way. That would be equivalent to waiting until the end, but actually gives us results sooner. Right, and there was a technical point that you made that I guess is something that we learned about. Which we had talked about in the old machine learning course but we didn't go into any detail about. Which is this sort of incremental update by making it look like a supervised learning rule, just look like the normal perceptron update rule. And it had a learning rate. And in order for us to be sure that we're going to learn something that we wanted to learn, that learning rate had to have certain properties. Great, that's exactly right. And in a nutshell, what were these properties that the learning rate had to have? Well, let's see, that the sum over all the learning rate values, so let's call that Alpha T for time, needed to be infinite. And that the sum of the learning rate squared needed to be finite. Yeah, so intuitively we want a learning rate sequences that would allow us to move the value to wherever it needed to go, to converge. But would dampen overtime, decay so that the estimates wouldn't keep bumping around as a function of the noise. Right, I think that's great. Well, anyway I think that pretty much brought us to the end. We took a lot of slides to get there, but it turned out the algorithm that we derived looked a lot like something we call TD(1) give it a nice little name. And TD(1) was very nice except it was inefficient. In the way that it used data. Yeah, I don't know that everybody would agree exactly with that way of characterizing it. But it is true that it had this sort of weird property that it was, there's a lot of variability in it. There was a very high variance, because if we had one run, we had a value that we're estimating from one place and. The estimate was based on a very noisy example, then we're going to have a very noisy estimate. It wasn't going to be able to use the rest of the examples that it's seen to kind of smooth this out. Right it didn't use information along the way. It used information sort of at the end of every episode. And that's what made it sort of data inefficient. And I guess in the end that gives you high variance. But we came to the rescue and when I say we I mean you came to the rescue with TD(0). Which has the nice property that it gives you the maximum likelihood estimate. I prefer to think of it as the Micheal Litman estimate. Yeah, I know you do. I've see how you snuck that in and then it got us to the very end which is good because you're running out of space on the slide. We generalize TD1 and TD0 into something we called TD lambda. That has the property that when lambda is zero we get TD(0) and when lambda is one we get TD (1). What about the intermediate values? Well, then that just gives you dubstep. Wait what? It gives you dub step. You get the dub step look ahead. [SOUND] [SOUND] [SOUND] [LAUGH] I see that's right, multiple steps of updates. I see what you mean by dub step, like double the number of steps. Oh yeah I like that. That sounds like I meant to do that. Yeah let's go with that. And there you go, I think that's just about everything. Oh, one more thing, is that you talked a little bit about different values of lambda and why you might choose one value over another. And it turns out that values of lambda between 0.3 and 0.7 empirically seem to work pretty well. Yeah, because it's somehow making a nice trade-off between estimating the right kind of quantity and estimating it using the values that you get from looking ahead. Right, and first off that's neat because otherwise you wouldn't need to have TD lambda. But it's in some ways a little surprising. Why is that? Well because it kind of just works out the way that you would want it work out. You went through all this trouble to kind of generalize between things. You have this kind of highly non-linear way of combining all the different k step look aheads. And it terms out that combining them so it's not quite 0 it's not quite one is better than either 0 or 1, and often that's not how it works. Right, I mean often it's like a line from 0 to 1 is bad. It's like a line, or it gets really better, then it sort of flattens out towards the end, like you said. But, in this case it just gets better, and better, and better, and then it starts to get worse when you go from one end to the other. Yeah that's right. So often when you combine two good things, you get the worst of both. In this case, it does somehow manage to get the best of both. Yeah, that's pretty cool. All right, so what I'd like to dive into next is a deeper understanding of. How we could show that these kinds of methods actually converge, and they actually produce optimal estimates and optimal behavior in the limit. And so we're going to take a step back and look at the Belmont equation again to do that. Oh I like it. I love the Belmont equation. All right see you then then. All right see you, bye. Hey Charles. Hey Michael, how's it going? Not too bad. I'm very excited because today we get to talk about TD with Control. Okay, is that what that yellow thing is? That's intended to be a lightning bolt. I don't know, somehow it just convergence, TD with Control, it sounded like a superhero movie to me. Okay. That makes perfect sense. Why are you excited about this? Well so, what we are going to talk about is how to not just learn values learning update rules like TD, but to actually learn the values of different actions we might take as a way of figuring out the right way to behave. Okay. I like that. Yeah. And we'll even get really close to proving that these methods converge. That is to say that, given enough data over enough time, they actually do the right thing. Okay. By the way, did you know that Convergence is the name of the 2015 company-wide event in DC Comics? Whoa! Okay, so it is a superhero thing. Yeah, well done. Does that mean we get paid for using their stuff, or does that mean that we have to pay for using their stuff? I think it means that we have to pay them so- Oh, whoops. Time to raise tuition. Can I do that? There you go, now everything's taken care of. Tada! The difference between using reinforcement learning for control and using reinforcement learning without control is basically whether or not there's actions that are being chosen by the learner. And so just to make that make sense, I'm going to swap back in the notion of Bellman equation. So this is the Bellman equations without actions, this is what we've been talking about for the last few videos. Do you recognize the Bellman Equation? The value of a state is the reward plus the discounted expected value of the next state. Yeah, I mean usually you write that with an a in there. Right, right, but I said no actions. Okay. We're going to put an a back in in just a moment. Okay, that'll make me feel better. Certainly when we were talking about TD we didn't really have much of an ocean of actions. We were really talking about sequence of states that were being experienced and the rewards associated with them. And in fact the update rule that we talked about, this is one of the update rules that we talked about. Says that if we go from some state and we get some reward and we end up in some new state, then what we're going to do is our new value function estimate is going to be. The, well, it's going to be the same as the old one, except for in the state that we just left. Where we're going to move a little bit towards the reward we got plus our prediction, our discounted prediction of the state that we just landed in. So what update rule is this of the different update rules we talked about? TD(0). Yeah, TD(0), factorial. So what we need to do now is get control back into the picture here, and so we're going to do that now. So, here's the same idea the Bellman Equations and the learning update rule but his time with actions. Actions. So, in the action form of it, we have this Bellman equation, the Q form of the Bellman equation Q(s,a) equals the immediate reward plus the discounted, expected value of the maximum action in the resulting state. Okay, so, when we actually land in that new state S prime, we choose the action that is best for that state. Right. But otherwise, this is really the same kind of update as before. And so, we can translate this into a TD0-like rule using the same sorts of idea so that we're going to have a new estimate of the value function, and the way that we're going to change it as we go from one time step to the next time step as it's the same everywhere except for the state and action that we just left. We just took action At-l and state St-1. And so that one we are going to update. We are going to move a little bit in the direction of the immediate reward plus the discounted value of the state that we landed in St where the value is taken- Is that better? [LAUGH] [LAUGH] It's taken with respect to the best action that we get from that state, okay? So, this is, again, the same update rule. The only difference is we got this extra little max in here to simulate the idea of choosing the best action from each state. All right, so, what I'd like to argue here is that this Q learning update rule is actually taking care of two different kinds of approximations all at one time. So, let me step you through what those are and then what we're going to do is understand how each of them works independently, and then use a result that says if these two things work independently, they're going to work together. So, all right, here's the first idea. Notice that if we knew the model, if we knew the transitions and rewards, then when we go from some state, take an action, get a reward, go to a next state, we could actually use that to update the Q values simply by applying the Bellman equation Right All that's really happening there is we take the old Q value estimate and we update it in the way that the Bellman equation says we should update it at that particular state connection pair. And so we haven't quite argued that this works. We said that it works but I want to dive into a little more detail about why this works, but I want to show that there's a second kind of approximation that's actually being used at the same time. So, here's the second kind of approximation that's going on with the Q learning update. It says that if we knew Q star already, we could use that to learn Q star, which now that I say it out loud, seems kind of tautological. I think, it, currently, is. But don't mean it quite so directly. What I'm saying is, let's say that we run the Q learning update like this. We say, we're in some state. We took some action. We got some reward. We ended up in some next state. We're going to update the Q value for the state action pair that we just left, moving it a little bit in the direction of the reward that we just got, plus the discounted value of the next state, St, which for the purposes for this little update rule, we're going to use the actual Q values to get that estimate. So, we're kind of cheating here. We're saying, we already know the Q value at least for the next state, and we're going to use that to update the Q value for this J action pair that we just left. Right, and that would work because since we keep applying truth to it, Q star being true, eventually, you have to kind of get closer and closer and closer to it. Yeah, and do you remember there's an important property though that we have to have in terms of this learning rate. Proper subscripts? No, you can get subscripts wrong and you can always fix it in post. Well, what's that property, Michael? Yes, so what is the property? This is just like in the TD row where we looked at, where we said that, if we have the learning rate sequence sum to infinity but the square to that sequence be finite, then you know good things happen we actually convert it to the right answer. Right. So, in this particular case if we have, if we know what the target is, if we know where Q star is supposed to be for the next state, we can use that to actually, what we're really doing here is dealing with the expectation over next states. And we handle that by using next states sampled from the distribution, because we're actually acting in the world, and instead of actually computing this sum, we just do this successive averaging thing and that's going to work out. Right and so it's insanely high probability you will move towards the expectation. Yeah, like, probability is seven. Yeah, that's pretty insanely high. That's pretty insanely high. Yes, so the piece that we really don't have yet to build on is this first piece. What is it that's making the Bellman equation update actually converge on the solution to the Bellman Equations or Q star, and, so we're going to prove that next, and then we're going to show, or actually, assert, that these two things can be tethered together in such a way that they're both going to end up converging in tandem. Nice. To make the notation actually come out relatively cleanly [LAUGH], to minimize the number of subscripts that I am going to mess up, we're going to introduce a notation called operator notation, functional notation. And we're going to introduce a particular, a Bellman operator. So Bellman operator is going to be a mapping, a transformation, from value functions to value functions. You give me a Q function, the Bellman operator will give you back, possibly, a different Q function. Okay. So it's a function from Q functions to Q functions. Does this operator have to have any particularly interesting properties? We're going to prove that it has some properties, but first let's define what it is. Okay. Boom! Oh, okay. So here's how we're going to define this Bellman operator B. We're going to give it a Q function, and the new thing that we get out, this B applied to Q, has the property that at state action pair (S,a). It's equal to the immediate reward plus the discounted expected value of the next state, where we look up the value of the next state using whichever Q function we were given as input to the operator. So, Q goes in, BQ comes out. BQ is a new value function, a new Q function. Do you think you get that? I think I do. I guess, I don't know why we're going through this yet. Oh, I'm going to tell you that quite yet, but I just want to make sure that you get it by quizzing you. No! This notation is very succinct. And so we can actually rewrite some things that we've talked about already in this very nice operator based form. And it makes it look smaller, and neater, and easier to work with. So just to try to prove that to you, the equation Q star, that's a value function, equals B applied to Q star is another way of writing something that we've talked about a bunch of times. So I was hoping you'd be able to fill that in here. And also, the notion that if we define some value function Qt to be, like a whole sequence of value functions, where each value function Qt is defined as the Bellman operator applied to Q(t-1). These both are things that we’ve talked about and have names, and I just want to try to get you to connect those to each other. Okay, I will do my best. All right, so do you see what to do here? No. Okay, all right, so here's my suggestion. So remember this B thing is just a bit of notation here. And what the notation is saying is that whatever value function you give it, you can replace this whole thing with a value function defined like this. Right. All right, and so imagine what happens if you actually plug all that in, if you actually take this BQ* and replace it with the definition according to this operator definition above. That says that Q* is actually equal to, well that's the definition of Q star. Do we have a name for that? Q star? Sure, we have a name for Q star, but the equation that defines Q star. Well it's the Bellman Equation. Yay! That was easy. Okay. And you don't have to put the exclamation mark to get the full credit for that one. Oh, I don't know. I think you should require they put the exclamation mark or they don't get full credit. No, I think it's okay. All right. In fact, anything that has Bellman in it should be fine. Anything that has Bellman in it? Yeah, like the Bellman Identity. Or, the [LAUGH] I don't know. I was trying to think of other sequels that have. The Bellman Ultimatum. Yes, Bellman reloaded. All right, so, good. So with that kind of thought process in mind can you do the next one? So we have some Q function, we apply the Bellman Operator to it and we get some new Q function. Bellman update? Yeah, it is a bellman update. But this now you can think of as an algorithm. It's going to actually generate a whole sequence of value functions one from the next. PV? [SOUND] No. No. Well then I don't know because I'm trying to, oh, wait. This is value iteration. Whoo! Yeah, I think I need extra exclamation marks with that. That makes perfect sense. Yeah. So value iteration is this big algorithm, and it probably takes a page of code or whatever to write, but in this operator notation, boom. You start off, probably to be more precise, you want to say something like Q0 = 0 value function or something like that. You need to initialize the value function. But then after that, you're just generating value functions from other value functions. That's exactly how value iteration works. Yeah. And it would work even if Q0 weren't last to 0. That's true. Yeah, but if we want it to be a complete algorithm we'd have to say where it starts and then how we generate new things from old things. It just starts with whatever happened to be in memory at the time. I see. [LAUGH] Yeah. That could run you into trouble, but probably shouldn't. As long as the types are correct. Yeah, well it doesn't matter, they're just bits. They're ones and zeros. Yeah but they'd better be interpreted as floating point numbers. Because if you interpret them as imaginary numbers or something like that it could make the Bellman equation act strangely. It doesn't matter, the operators will do the right thing, they'll just interpret them as numbers, or doubles. Perfect. Okay, well then we're all happy. So I guess what I was hoping to do with this example is, you know convince you to develop an appreciation for this particular notation, this Bellman Operator notation, because we can write stuff that we care about really, really syncly. Yeah, yeah okay I'll buy that, and it means that you'll be able to fit everything in on a slide. [LAUGH] Which will be good, and look at how less, I'm messing around with subscripts, it's very nice. I like your subscripts. So we're going to define a special kind of operator called a contraction mapping, and we're going to say an operator's a contraction mapping if it has this property. So B is some operator, and it's mapping value functions to value functions. If it's the case that for all value functions, so all value functions F and G, and some multiplicative factor gamma which is less than 1. If it's the case that the distance between B applied to F and B applied to G has to be smaller than or equal to gamma times the original distance between F and G, then we say that B is a contraction mapping. It's causing things to get closer together, tighter together. So this notation here is sometimes called the infinity norm, the max norm, and all it's doing is saying, this is a function of states and actions, so which is the largest? What's the largest state action pair in terms of absolute value? Okay, so that's just saying, what's the maximum value that the function Q can have? Or that it currently has, yeah that's right. So when we write F- G max norm really what it's saying, what's the biggest difference in Q values between F and G? And this is corresponding Q values. Mm-hm. So F- G is saying what's the biggest difference between these two Q functions at any state action pair? Whatever that largest difference is, we're going to multiply it by something that makes it even smaller. And we're going to say that, for it to be a contraction mapping, is has to be the case that, if you apply that mapping to F and you apply that mapping to G, the distance between the resulting functions is even closer together than they started. And that makes it a contraction map, okay. Yeah this is how we're going to define contraction mapping, and this is going to give us all sorts of nice properties, and it's also going to happen a lot in the kinds of operators we get in reinforcement learning. So just to make sure I have this right, since the infinity norm finds the biggest value and here you define the difference between two functions, at least the infinity difference between two functions, being wherever their greatest distance is. It's certainly the case that over time as we apply this operator over and over again, the point where their maximum is, the state action pair where their maximum is, might change every time, right? Right. That's right. So this max norm is computing for the specific value function that it's given where the biggest absolute value is. And that could be different on this side of the equation from this side of the equation. There is no reason that it needs to be the same over and over again. And in fact when we're using B to be a value iteration operator it does tend to move around a lot. Okay. So you would expect it to move around a bit, at least as you're going through value space. Yeah, exactly. However, let's not worry about that quite yet, let's actually do some contraction mapping examples on just scalar values where it's really easy to reason about them. Okay, yeah I think that would be cool. All right, so we're going to play around a little bit with this idea of contraction mappings, but in a very simplified setting. We're going to look at, instead of Q functions, we're going to look at just scalars, just real numbers X. So B is some operator that is applied to a number and it spits out a number. Mm-hm. And I've defined 4 different kind of Bs here. And some of them are contraction mappings and some of them are not. And we have to figure out by using the definition of contraction mapping which are which. Okay. So let me just make certain that I have this. So the way you defined contraction mapping before, you said there would be these two functions. And you basically want to see if they get closer and closer to one another. Yep. As you apply the operator. But here the function is just a thing, instead of being a Q value with all these indices, it's just a number. Yeah, you can think of it at being a single state, single action, [LAUGH] right. It's like an MDP that has one state and one action. So it's just a number. Okay. So it's just a number. All right, and so I just want to see if there's some gamma that I can choose between 0 and 1, inclusive of 0, not inclusive of 1, that as I apply these operators X/2 X+1 X-1 and (X+100) * .9 that will just get smaller and smaller and smaller and smaller and closer and closer closer together. Okay I think I got it. Okay. All right let's do the quiz, go. All right, you're ready to answer these questions? Yes, I think I am and I think I know the answers. I think they're yes, no, no, yes. Great. That's actually correct. You want to tell us how you can actually argue that? Well, no but I'll do it anyway. So let's take the ones that I think are pretty easy, two and three so without even coming up with an example, I think it's pretty easy to see that those are contraction mapping. And here is why. So let's Imagine I started out with two functions. Let's say they were six and eight. Okay. So, we are going to say X equals 6 and y equals 8. Yes, but Y is the other function. Well, what's the difference between them? It's 2. So just let's make sure we are applying this in the right context. What we want is that the distance before we've applied any operator to them is going to be bigger than or equal to the distance between them after we've applied the operator. Right. And it has to be the gamma's less than one. This actually becomes important. That's right. All right, so you did x-y is 2, uh-huh? Right, well x-y is 2, well when I add 1 to both of them that'll just become 7 and 9, but the difference is just 2. Okay, so I do buy this, that adding one to something shouldn't be a contraction. But it seems like subtracting one from something should make them smaller. Right. Except it makes both of them smaller in exactly the same way. And so, you're basically keeping the distance between them constant. Good point. And in fact the same example works here. So, 6 and 8 are 2 apart. 6 minus1 and 8 minus 1, or 5 and 7, are still exactly 2 apart. So, 2 is not less than or equal to any gamma less than 1 times 2. Right. So, yes, I agree that these are not contraction mappings and the proof basically involved showing a particular value of X and Y such that they're not getting any close together, they're staying the same distance apart. Right and I think you can even prove that by just basically writing down the equation that relates X and Y together. Y is equal to X + 2, if I subtract a constant from both sides or add a constant to both sides it doesn't change anything. Very good. All right. So for the first one, x over 2. Here's my thought process, and kind of the proof that I ran through in my head, and then the realization that I had that made four actually easier as well. Okay, so why don't you erase that? And I'll give you an example of what I was thinking. All right. So the x over 2 thing, I thought are you going to just keep dividing by two. So let me come up with an example in my head. And I thought, oh, I should use a power of two because then I can keep dividing by two. Hang on. Hang on. Well okay we'll let you step through that. But that's not going to get us all the way there. No it's not, but- Okay, let's do it anyway. Well I was just going through my thought process. Okay. Great. So just for the sake of thinking out loud, let's say x is 16 and y is 32. So what's the distance between them? Well it's 16. Good. Now if I divide both of them by 2 I get 8 and 16, Right. And the distance between them is 8. Good, all right, and 8 is definitely less than 16. Right, but then I realize, oh, this is actually pretty simple, I'm glad I chose a power of 2. That basically the distance between x and y at any given, let's say, time step, I chose x to be some 2 to the n and y to be 2 to the n plus 1. And then if I divide both of them by 2, then the new distance will be 2 to the n minus 1 and 2 to the n. Sure, but this is only showing that it works for things that are like consecutive powers of 2, which is not what we need. We need it to hold for all x and y. Right, but that was the process that I went through in my head to kind of show that this will keep working. And then I realized what's really going on is I'm taking my value X and making it smaller. Sure. But that's true of number three also. That's true, but I'm making it smaller in a geometric way. [LAUGH] Okay. You don't like it there? To me, the easiest way to step through this is to do the algebra, to say that Bx is x over 2, By is y over 2. Yeah. So the absolute difference between them is actually equal to 1/2 of the distance between X and Y. Yeah that's fair. So I'm just factoring that. And that gives us exactly what we want, which is to say that the difference before compared to the difference after applying the operator is less than or equal to some less than one value multiplied by the old distance. Yeah, that makes sense because that's exactly what I was doing when I went from 16 to 32 to 8 and 16. Exactly, yeah. I noticed the distance was cut in half every single time. Right. Okay, but that general thing is going to work for number four as well, right? So the first thing to do to make that really easy is to realize that although you've written it as X plus 100 times 0.9, so I could rewrite that as X times 0.9 plus 90. Good. Right. So the X times 0.9 part is going to keep reducing by exactly the same argument that you did before. I could take out the 0.9 and show that it's just going to keep getting smaller. So that parts going to get smaller and since I'm adding a constant every single time, it's not going to change the actual distance. Good. So it will continue to get smaller. Yeah, that's right. And so algebraically, if we have whatever X and y were before, after we applied this operator, as you just pointed out, it's actually going to be 0.9x minus 0.9y, plus 90 plus 90, but then they're going to get subtracted. Ans so this quantity is equal to .9 x minus y. You just factor that out. So whatever the distance was before is actually contracting a little bit, it's getting a little bit smaller, nine tenths the size of what it was before. Right. And so, there are two things there. One is, adding a constant, whether the constant is positive or negative, doesn't change the distance, because they're going to cancel each other out. That's why two and three don't work. And then multiplying your function by a number that is less than 1 will make them get closer and closer together. Which suggests that if we were instead to multiply them by a number greater than 1, or actually 1 or greater, you would end up with something much much much worse. If we get bigger and bigger and bigger it would be an un-contraction mapping. [LAUGH] Expansion mapping. Yes, so expansion mapping, I like that. Great, okay nicely done. Cool. All right, so one of the reasons that contraction mappings are really cool is because they have some very nice and convenient properties. So let me state those properties and then maybe we can figure out why and how it has these properties. So the properties are B is a contraction mapping, we know that B is a contraction mapping, and that means that any two functions that we pump through it are going to get closer to each other than they were before you pumped them through. Then first of all the equation F* = BF* sometimes called the fixed point equation, has a solution that there is some F* that you can plug in to B that causes it not to change. All right, so whatever F* was before after we put it through B it stays the same as it was to fix point. Mm-hm. And there is just one of those, it's unique. So just setting up this equations says okay, it has a solution and we could just call it F* because there is only one that works. Okay. And a second property is that we can actually converge to F* by doing this kind of value iteration idea. That we start off with some value function. We push it through the operator. We get a new value function out. We push that through the operator. We get a new value function out. We just keep thing doing this over and over and over again. And at the sequence, the value functions, is converging in the limit 2, the solution, F*. Okay, so that makes sense Michael. Here's a question for you. If 1 is true, and I know that B is a contraction mapping, then doesn't 2 follow from that? Well, 1 is true. [LAUGH] But yeah, it's actually not so hard to see 2 being true given that. So, here's the idea. So let's say we have some F* and we know that B is a contraction mapping. Then let's look at what happens to Ft minus 1, compared to F*. All right, so by the definition of a contraction mapping, we have this property. That there's some gamma less than 1 such that for any two value functions, they're going to get closer together after we apply the operator. But the two value functions that I choose are F of t-1, the value function that's coming out of this sequence, and F*, the fixed point. Mm-hm. All right, so just by virtue of being a contraction mapping, we have to have this be true. But let's look to see what this is actually equal to. So what's BFt-1? Ft. Yep. And what's BF*? F*. Yeah. So we're getting closer and closer and closer to F*. This sequence, however far if was from F* before we do the operator, the next time we do the operator it's at least 1- gamma of the way to F*. So we just keep repeating this over and over again and they're going to be equal. Right, that makes sense. Okay, so if B is a contraction mapping and a unique fixed point exists, then you have to get there eventually. Right, and then showing that the fixed point is unique is the same kind of argument as here, like if we have two different fixed points then, right. So if we have two different fixed points that would mean that we'd have an F* that satisfies an equation like this and a G* that satisfies an equation like this. And that means if we take F* and G* and we put them through the B operator they're not going to get any closer together because they're going to stay exactly as far apart as they were. And that violates the notion that it was a contraction mapping. So there can't be multiple fixed points. That's beautiful. All right, so now that we have a notion of contraction. We should apply a contraction to our favorite friend the Bellman operator. I do not use contractions. Oh, I see what you did there. Thank you. My spouse is an obstetrician, and so sometimes she worries about contractions as well. I think we could probably do this for an hour. Let's say we contract it. Nicely done. So good. So right, we want to apply this notion of a contraction mapping to the Bellman operator. In other words, we want to show that if we have a Q function Q and we apply the Bellman operator to it. Then, well in particular, if we have 2 Q functions, Q1 and Q2, the distance between them. After applying the Bellman operator is less than or equal to gamma times the distance before we apply it. So by applying it we move these two Q functions closer together. Sure. All right, so the first thing we're going to do is unpack the definition of the max norm. And so in this case, in the case of Q functions. What that really means is we're going to maximize, over all state-action pairs. The Q value of each of those state-action pairs' difference and absolute value. And that leaves us in a good position to be able to unpack the definition of the Bellman operator itself over these two different Q functions Q1 and Q2. So all I did here is substitute it in for the Bellman operator. The definition of the Bellman operator as it's written up here. Mm-hm. Alright, since we're computing the difference between Q values for the same state action pair. They're going to have the same reward and then they're only going to differ really in the value of the resulting state. Mm-hm. So we can cancel out these two rewards. We can actually factor together these gammas and summations, which are the same in both cases. And we get down to the maximum difference over all the state action pairs. Or the absolute difference between the discounted expected value of the next state. And the value of the next state being for Q1 the max over all actions of the values of state s prime at Q1 and for Q2 The maximum over reactions of the values at state s prime for Q2. Oh, because you can. So why can't we just keep factoring it? Yeah. because they're different a primes. Right. So do you see why I didn't go one more step here and actually combine these two maxes together. Even though we combined these two sums together? Yeah, because, even though you wrote a prime both times, they're two different a primes. Right, right. So this is the action that maximizes the q value for Q1. And this is the action that maximizes the q value for Q2, and they may have different max actions. Right, even though they both end up in this prime. They start in s prime. That's right. Well, they start in s and they end up in s prime. And then we're choosing the action in s prime. That's right. Okay, yeah, that makes sense. So let's just make a little room here and we'll copy this equation down one more time. But this time what we're going to do is instead of considering this weighted combination of the Q values at state S prime, weighted by the probability of reaching S prime. Let's just make it simple and just worry about the S prime where the difference is largest. Since this is a weighted average, or a convex combination, it can't be any larger than the biggest difference at any S prime. So we're going to do that, we're going to copy that down and max it over S primes. And we don't need to include the max over a and S anymore, because you'll notice this expression doesn't even have a and S in it anymore. So actually, by considering this kind of worst case version we don't have to. We don't have that max anymore. And that gets us from an equal to a less than an equal This gets us from an equal to a less than an equal that's right because we're bounding it. It could actually be a bit wider here. We're considering the worst possible next state in terms of the difference between these Q functions. Okay. And now you can see that it's actually getting pretty close to where we want to end up. The main difference, in fact, let me rewrite this max norm equation out. So when we write that out we get gamma times the max over state action pairs. And I'm going to write s prime and a prime to make it easier to match up with the other equation. It's however we're indexing over the state action pairs that make up Q1 and Q2. So this max norm is really just the max over those of the difference between the two Q functions, and this is where we are at the moment. We have something like gamma max over S prime, and these maxes are trapped in the absolute value. It would be really nice if we could move these maxes out of the absolute value, and have it this way so now the a primes actually match up. Here this is the max action according to the Q function Q1 and this is the max actions according to Q function Q2 and it would good if we could just talk about them together. So this is kind of to me anyway this is the most fun most exciting most interesting step because this turns out to be true. [LAUGH] Right, that we can actually move these maxes out, and then we get a valid sequence. So I'm going to focus in on this step in a moment. But let's just accept it for the time being, that we can move these maxes out. Now we've actually completed the algebra that we needed to complete. The difference between the two Q functions after we apply the Bellman operator is actually less than or equal to gamma, something smaller than 1, times what the distance was before we applied the Q operator. I'm sorry, the B operator of the Bellman Operator of a value iteration. Wow, so Bellman and value iteration converges, except for this magical step you're about to explain to me. Except for the magical step, yeah. And the magic step is fun because it's, I alternate between as seeing it as completely obvious, and of course this is true, and being completely mystified and don't understand how it could possibly be true. And my brain just sort of flips back and forth between those kind of like a Necker cube. So let's dive in and make sure that this property holds, knowing that once we establish that we get the whole chain, we get the whole contraction operator thing. We get all the power of contraction operators in when we're doing Bellman Optics. This is the property that we would like to hold for Max. Max needs to be, I'm going to call it a non-expansion. It looks just like the contraction property except there's no gamma multiplier here less than 1. So it could be that we have equality here, and it could be that we have less than. But we have this kind of non-expansion sort of thing going on that when we apply Max individually to these two functions f and g we get something that's going to be. The difference between those is going to be closer together than if we actually just take the Max over the entire differences between f and g. So I don't know how crystal clear this is but I think it might help to do a concrete example. It is not crystal clear. Wait, quick clarification question, so you have max a f(a) max a g(a). But just like before, those are actually two different a's. That's right. In fact, there could be three different a's. because this thing on the right hand side could be yet a different a. A a a, okay. So this is all very abstract. So let's actually just write down some concrete numbers. And imagine this is, we have two functions, f and g defined over the same set of a values, a1, a2, a3, a4, a5, a6, a7. And I don't know, just fill them in this way. F of a3 is 1, g of a5 is 8 and so on. Mm-hm. Do you get that? I do. Good. So, quiz. Just fill in what these quantities are in the case of these particular numbers. Just make sure. You know, we want to prove that this holds for any f and g, for all f and g. But let's just make sure that it holds for this one first. Okay. I like this quiz because this one I think I can actually do. [LAUGHS] It's not hard but it starts to make you think about the relationships between the numbers and the two functions. Well, you can try to make me think about it. All right. Go. Can you compute these quantities for me? Sure so I gotta find the max a of f of a, so I just have to find the maximum value for f and that is 9. Happens to be a2. That's right happens to be a2. And he maximum value for g is 8 which is at about a3 and a5. Good, all right so then what goes in this box? 9 minus 8 which is 1. The absolute value of 9 minus 8 which is also 1. Yes, that's amazing. And the other box? So that requires a little bit more, I have to figure out what the maximum absolute difference is for each a. So I'm just going to do that. I'm going to start at a1. So 7 minus 5 is 2. 9 minus 3 is 6, 1 minus 8 is 7, for the purposes of this discussion. It's absolute value, it's fine. It's absolute value. 0 minus1 is 1, 6 minus 8 is 2, 3 minus 5 is 2, and -2 minus 5 is 7. Good. And let's see the maximum value there is 7. All right. And just to make sure is 1 less than or equal to 7? Most days. There we go. All right so good, so we verified that this theorem at least holds for this one example that I kind of made up at random. Well and then we just simply use, just simply say it's true for one example. It's true for two examples, and then we're done, right? This is, what, induction? Yeah. Yeah, I don't think we can do that. I think we actually have to do it by deduction. Ugh, okay. So here's what we're going to do. It turns out that it's sort of complicated to keep track of where all the max's and the differences are in absolutes. So we're going to make a simplifying assumption, which is, that the largest value in our f function is bigger or equal to the largest value in our g function. And I'm going to say we're going to assume that without loss of generality. Why is that okay? Because if it weren't true, we'd just swap f and g around. Yeah, that's all. I mean, we'll just call f. Of the two functions that we're comparing. Call f the one that has the larger max or larger or equal. I guess, if they were equal. If the maxes on the two were equal, then it doesn't matter. But we want f to have the larger of the two. What's that's going to do is going to simplify some of our thinking with the absolute value symbols. Because otherwise, the case analysis get's really tedious. Hm, we can't have that. Okay, I can't wait to see where this goes. All right, so ultimately, we're going to be trying to compare this quantity to this quantity. So let's start off writing this quantity on the left. And then we're going to reduce it step by step until we get the quantity on the right. So here's where we start. What can we do with this? Well given that we know that the max of f(a) is bigger than the max of g(a), because that's we were assuming when we named f and g. We can get rid of the absolute value signs, yay. Mm. Now, because we're finally going to come clean on this issue that you were talking about before, which is that these are kind of different a's. And it would be good, if we gave them different names. So let's actually do that. Let's actually say that, a1 is argmax f(a) and a2 is argmax g(a). Mm-hm. So now we can rewrite this different, max a f(a) minus max a g(a), much more simply. And it's just the difference between the corresponding values in the function. Sure. All right, so if you let me get away with that, now you're in trouble, because the next step is going to follow pretty easily from this. But it's going to take us into a whole new realm. So now, we said that this quantity is equal to the difference at a1 of the f function and the value at a2 of the g function. So since this a2 was chosen to maximize the g function, then any other a that we could substitute in here, let's just say a1, can only make this smaller, or it can make it no bigger. Right. All right, so the g(a2) versus g(a1), g(a1) is going to be smaller than or equal to g(a2). Yeah. But since we're negating that in this equation, that means it can only make this quantity smaller than or equal to the previous quantity. That's That's kind of amazing. That's a neat trick. Right? But intuitively, does this piece feel okay intuitively now? Essentially what we're doing is, we're taking the place where f is maximized at a1, and the place where g is maximized at a2. And we're just sliding over to a number that's- Well here, think of it this way. So I find this little piece helpful. So because of our, without loss of generality statement, if we think of this as a number line, we have the f value at a1 is the largest thing of all. [LAUGH] It is the king. Mm-hm. The g value of a2 is Some other large value, but not as large as f(a1). So if we switch from g(a2) to g(a1), we have to be going down or at least not up. Mm-hm. And so if we look at the difference between these two quantities, And the difference between these two quantities, the difference can only have gotten larger. And that's what this equation is showing, that the difference can only have gotten larger when we substituted in the a1. Well that's not true. It could stay the same. So f(a1), g(a2), and g(a1) could all be the same number. But g(a2) cannot be bigger than f(a1), and g(a1) cannot be bigger than g(a2). Good. No. That's really clever. This is cute. This gets you a way of going from two completely different variables to one variable. Right. That's awesome. Now we're almost home free. So the first thing I want to point out is that we can reintroduce these absolute value signs without killing anything, because we can. This was a positive quantity and so taking the absolute value doesn't change it. So the last little thing to point out is it if we take the max of this same quantities substituting in different values of a, this can only get larger or no smaller because this a that we're maxing over includes a1. So we can definitely achieve at least this value, but it's possible that some other action will let us achieve an even larger value. But by maxing, we can't have it achieve a smaller value. I mean it can achieve a smaller value. But it will at least the max will be at least as big as what we get from a1. Right. And with that in place that actually completes the chain of inequalities from here to here so we get this non expansion property for the max operator. Here the max operator is being used here as sort of a way of summarizing these functions F and G. Okay so let me just say that last thing back to you to make certain I've got it. So, I'm with you all the way to F of A 1 minus G of A 1. You went to the absolute values sign, that's pretty easy because you already knew that value was positive, that difference was positive by construction. And then that last step you noticed that the difference between A1, F of A1 and G of A1- 1 is some number. You're maximizing all, over all possible a's you could stick into f and g. It has to include a1 by definition, which means the max might be bigger. But it couldn't be smaller than the a1, and so therefore, you're bounded from above. Excellent. Good, good, good. Yeah, well said. That's really cool. Now I've forgotten why we did this. [LAUGH] I find that's often true for me as well. This requires enough of my attention that I lose context. But the context was that we had this step in our proof that the Bellman operator is a contraction. We needed it to be true that summarizing the Q values at a given state using a max, was going to leave us with something that is no farther apart than the maximum difference between the two functions that we started with. Okay. Yeah, I remember that. Maybe you can show me the equation again. And in particular, it was this step right here where, essentially, we had to, one way to think about it is move this max out of the absolute value. So if we compare that to what we just proved, that that's what we did. We kind of moved the max out of the absolute value. Right, that's awesome. Good. Okay. So I get it. So the Bellman operator is a contraction operator value duration works. Yes indeed. Now I went through a phase of my life where I got really fixated on non-expansion, and identified a whole bunch of other operators other than max that are also non-expansions. And that actually gives us a way of analyzing a whole bunch of other kinds of decision processes. So not just these standard mark-off decision properties where we're trying to maximize expected reward, but we can do a lot of other things and this same kind of proof is going to end up holding. So we're going to get back to that in a moment, but first I want to talk about how we use this property now that we've established it to prove the convergence of Q learning, to prove that this interesting way of updating the Q values over and over again In the face of experience actually does, in the limit, lead you to getting the solution to the Bellman equation. Okay. What I would like to do know is step through a statement of a theorem. We are not going to prove the theorem. That's really up to people like. But we are going to state the theorem and understand the theorem well enough so we can apply is to showing that Q learning converges. Okay. So, the first thing we are going to do is just to make things simpler to write down, we're going to say that Q learning and update algorithms like that are going to update all state action values on all time steps. However, if it's a state action value that doesn't actually correspond to the current state action pair that we just experienced then we just set the learning rate to 0. Right so that just means leave the Q values alone except for in the state action pair where you actually just experience and got a transition. Okay. This is the beginning of the state into the theorem so we're going to say B is going to be some contraction mapping. So this is going to be the Bellman Operator ultimately. Q star equals = BQ star. That's the fixed point. That's the solution to the Bellman Equation. So Q star is like Q star. Okay. And let's imagine that we've got some sequence of Q functions. That starts off with Q0 and the way we're going to generate the next step from the previous step is we're going to have a new kind of operator, B sub T. B sub t is going to be applied to Q sub t, producing an operator that we then apply to Q sub t and that's what we assign Q t plus one to be. So in the context of Q-learning, this is essentially the Q-learning update, but we're going to separate out the two different Q functions that are used in the Q-learning update. One is the past Q function that we're using to average together, to take care of the fact that there's noise in the transitions. And then the other Q function is the one that we're using in one-step look ahead as part of the Bellman equation. But we'll get to that in a moment. But here's the cool thing. That this sequence of Q functions, starting from any Q0 that we want, as long as keep applying this, is going to go to Q star, as long as we have certain properties holding on how we defined these B sub t's. So here's the first property. First property says that for all Q functions, U1 and U2, and see I used a Q function and U together. Uh-huh. Just because you thought it was funny that I kept saying Q function and Q. So now I said Q function and U. [LAUGH] All right, for all Q functions U1 and U2 and state action pair SA. This is true, all right, what the heck. What's happening here, we're taking this B operator, applying it to U1 to get a new operator that we apply to Q star and we compare that to that same Operator Bt applied to U2. Whatever operator we get out of that we also apply to Q star. What has to be true, what we need to be true, is that this is going to be less than or equal to one minus the learning rate, times the difference between the U functions. So this is a non-expansion property again. Right, that the update rule, the Right, that the update rule, the operator that we're using to modify the inner value function at the B, is going to have this averaging property to it that what we're ultimately doing is computing the expected value. It's not exactly what this says, but that's how it's going to be used. Okay. So the important thing here is this just doesn't make things get farther apart. Okay, by the way, I just want to say I really appreciate your proper use of farther instead of further. It makes me very happy. I try really hard when I'm talking to you. Thank you, I appreciate that. All right, so here's the second condition that has to be true. That for all Q functions Q, Q functions U, and state S action a, we need another kind of, well in this case, contraction property to hold true. There's a, our gamma is sitting in there, gamma is less that 1. See. Mm-hm. And what we're saying is that if we hold fix this U value, this value that plays this role in the operator, and the other 1 is Q* and Q That we actually get a contraction of Q towards Q* by applying this operator in both cases. All right, so [LAUGH] we're almost there. Lastly, we're going to need our standard condition on the learning rates to hold, right, which is that they sum up to infinity and the square sum of less than infinity But there's something hidden in this definition. It took me a long time when I read the first proofs of this, like, no this can't be true. In fact, what's happening here is that given that we set the learning rate to zero, when there's state-action pairs that we're not visiting. The only way that this sum of learning rates for state-action pairs is going to be infinity, Is if we visit every state action pair infinitely often. So not only is this saying something about the learning rates have to change. It's giving an additional constraint on how we have to visit state action pairs. Wait, so then if that's true that gives you infinity why doesn't that give you the squared? Why does that still get you the square less than infinity? So the infinity and the square less than infinity is here, because we're going to be doing the standard kind of averaging trick, and we need the learning rates to decay the right way. But what I'm pointing out is that in addition to that condition that we generally we need when we're doing convergence of stochastic things. We also have an additional constraints that's hidden in here. Which is it's saying, we have to visit all state action pairs infinitely often. If we don't, then the sum of the learning rates is not going to be infinity, it's going to be something finite. Oh, I see. No, no, no, that's right, that makes sense, that makes sense. Oh, so cool. So basically our algorithms will converge so long as we take an infinite amount of time. [LAUGH] Well, convergence is an infinite amount of time kind of thing, just to be fair. Mm-hm, mm-hm, no, that's very, very satisfying. Later in the class we'll talk about some results that actually hold, believe it or not, infinite time. But this, yeah, this result is intended to hold in infinite time. I'll believe it when I see it. Alright I want to try to explain this convergence theorem specifically in the context of Q learning. It's intended to be general and apply to lots of algorithms of this form, but mostly we want to use it to show that Q learning converges. So let me express Q learning as an update rule. In the form of this kind of weird Bt operator notation that was used in the theorem. Okay. So Bt is the operator that we're going to use to update the Q function on the t timestep. And so, given that we know that we're on the t timestep, we know that we're in some state St minus one, took sub action At minus one, ended up in new state St, having received reward Rt. Alright, so, what I'd like to kind of argue is that this strange definition of operator Bt Q is exactly the Q learning update. So, let me try to argue that. So this looks a lot like the Q learning update. Saying that we're going to take the Q function at (S,a) and I'm going to move it a little towards the immediate reward plus the discounted value of the next state. That much toward where away from that old Q value was. So, the only thing that's different between the Q learning update and this is that we've separated out two different q functions, this w thing and this q thing. Q is using to play the role of essentially the q function in the state action pair that we just left, and w is playing the role of giving us an estimate of what the q value is at the state that we just arrived in. And in regular q learning those are the same. But we're now separating them out because the theorem is going to find that easier to swallow. Okay. All right so you buy the fact that this is Q learning just in a slightly different notation? Well it certainly is if w and q are the same. Which is what you just said. Yes, certainly the case now that w and q are different but when, oh are you are saying it is q learning when w and q are the same? Isn't that what you said? Yes. Okay, so yes, that's what I'm saying. Great. All right, so now what we want to argue is that this definition of the update rule causes the things that we needed to be true for the theorem to be true. All right, so let's start with condition 1. Why is this true for the learning rule defined this way? Is that a question? That's rhetorical, right? It was intended to actually be a question. So let's look at what needs to be true of this learning rule, what needs to be true is that if we have, essentially if we did this update in the form of knowing what Q* was and use it as a one step look ahead here, then what do we want to be true? What we want to be true is that if you give me any two value functions, they're going to get no farther apart. In fact, they're going to get closer together if the learning rate, if we're actually updating that particular state action pair, the learning rate is non-zero, they should be getting closer together. So why is that the case? Why is it the case that if we do this update at a state action pair, doing our look ahead with Q star, that everything is sort of moving towards one particular answer. It's moving towards a particular answer. Do you know what it is? Q star? It is moving towards Q star, but in particular it's trying to compute the expected value of this one-step look-ahead where the expectation is taken over all possible values of St. Mm-hm. So it's essentially doing the averaging part of the Bellman Equation. And the reason that works is because well, that's sort of how things converge if you let learning rates converge. We've talked about that in previous lessons. So all this rule is saying is that if we knew where we were going, we could use this update rule to average out all the stochasticity that we get from on different trials taking a step from a state and ending up in a different state, because of the stochasticity of the Transition function. That's all this is saying, is that we can smooth out these stochasticity transition function using the rule that we worry about. Okay. That makes sense. So that explains how that first condition is satisfied that it's basically taking care of the averaging over noisy transitions. This second condition is going to turn out to be, well, let's find out what it's going to turn out to be. So what we're doing here is we're saying let's apply this operator, this BTQ operator, or BTQW operator. And the condition that we need to have hold is that for any Q, U, S and A, that if we hold this U fixed, which the Q in the equation is going to turn out to be the Q itself, and only vary how we're computing our one step look ahead. How we're estimating the Q values of states that we are moving to one step from now. That the difference between those two Q functions, in this case Q star and Q, is going to get smaller after we apply this operator than before, which is an example of what property? What is it about the operator that we're counting on, for this to be true, that before we do the one step look ahead, there are some distance apart, but after they're closer. Actually I'm not sure. So however far apart the value that we're using as a one step look ahead, however far apart it was before, that after we apply this rule now it's closer together. Right and it will be because you're moving in the direction of what the one step look ahead operator said. Right and so what property the one step operator, what property of this equation is giving us that? It's the contraction property. So the fact that this update has the form of a contraction is what gives us this property being true, that after we apply an operator things get closer together. And then the third condition was just the condition on the learning rates, and we need that for any kind of Q learning algorithm. Anyway, at the end of the day, by separating the Q learning update equation into using two different Q functions in two different ways, we can prove properties of both of them. One is playing the role of averaging out the stochasticity so that we're sort of acting as if there are expected values in here, even though there's not. And then given that, the other one is acting like the value iteration operator, the one step backup, and is contracting with respect to the value function that you used. So again the Q functions are being used in two slightly different ways. We proved that both of them hold, and therefore this theorem gives us that the whole iterative algorithm is going to converge. Right. So this is all really just a long way of saying hey, look, because Q learning can be forced to fit into the scheme, it's guaranteed to converge in the limit. So Q learning converges? Q learning converges. I guess we knew that. Yeah, we knew that, and in fact there's lot of literature where people have proven essentially this result over and over again in slightly different ways using different tools. The thing that I like about this proof is, well, first of all I read it because it's in a paper that I wrote. [LAUGH] But second of all, it breaks things down in this sort of modular way that we're going to be able to reuse to prove the convergence of other Q learning like algorithms. And that turned out to be important to prove, for example, that Q learning in game scenarios also converges. Huh, but that's in the future? That's in the future. Are we going to re-prove this all over again when we get to the future? No, we don't have to, because the generalized theorem holds in both cases. Nice, nice. That's the beauty of the generalized theorem. We never have to look at this again. [LAUGH] Well played, sir. All that's going to matter is, well it's going to matter that the operators that we use for updates are contraction mappings. Okay. And as long as we take care of that, things are going to be great. Oh, sorry, non-expansions, as long as the thing in this position here is a non-expansion, all will be well. Good, good, so well played. So we went through all of this so we never have to go through it all again. I like that. That is exactly the thought, yeah. And in fact, even missing all the details of this, I just wanted you to be aware of the fact that these details exist. The way that we actually use this is at a much higher level of abstraction where we just say, hey, we can write down what we think the Bellman equation is. We can prove that the operators that we're using here are non-expansions, and we're done. And we inherit everything else? Yeah, it's just like knowing that the sum of the alphas should go to infinity but the alpha squared should not go to infinity. Once we know that, then it helps us to decide whether to choose this learning rate or that learning rate. It's the same thing, we can't just substitute max with any arbitrary thing we want to because it might be expansive instead of non-expansive. But so long as we're smart about the things that we pick, we know that things are going to converge. Exactly and so let's maybe go through some examples of that as a quiz. Sure. All right, so now we're going to take what we just learned, and think about what generalized MDPs might need. Generalized Markoff Decision Processes. So the Bellman Equation for generalized MDPs looks like this. Q*(s,a)=R(s,a)+Gamma times kind of a summish thing over s prime of a maxish thing over a prime of Q star s prime a prime. So basically all I'm trying to say here, is that we can take out, we can substitute various things in for this plus operator. And this times operator and we get different sequential decision making problems that come out of it. So just to make sure that you understand what I'm talking about, let's do the first one together here. The first idea here is that we're going to replace this plus symbol with a sum over all s prime of the probability of reaching s prime times then whatever function we're plus-ing. Which in this case is the X thing, applied to Q star so the X thing here is written to be the max operator, so max over all next actions. Well, it says F S primate prime, but here we're going to be substituting in the Q star s prime a prime. So, if we were actually to do the substitution in the first line, put this summation thing instead of the plus thing, and this max thing instead of kind of the times or X thing, what decision process sort of thing would we get from that? That's just a regular MDP. Yes, that's just how I would have said it. So, it just gives us back the ordinary Bellman equation for MDPs. It's just, again, kind of a weird way of writing it, but the point is that we're going to allow different kinds of operators to be substituted in here and we're still going to get very reasonable things out, things that we can run value iteration on, things that we can run q learning on and so forth. So, if you think you get that let's do the rest of this as a quiz. Basically what I'd like you to do is for each line here, if we were to do the substitution, where the plus operators define here and the time operators define here, what kind of decision process would we get out of that? I think I follow that. I'm looking over these to make certain it all makes sense to me. I think I get what you are saying. In the regular MDP we have an expectation over maxes, and so the first line is basically an expectation over a max. So that's what a regular MDP is. So the next one is a min, instead of an expectation, over a max. And then in the third one's an expectation over a peordida. Oh yeah, sorry. I just made up some notation there. Let me explain what that means. So, the basic idea is that we're going to take a weighted combination of all the actions and the weights depend on the rank of the action. And in particular we can have a weight for the max and a weight for the min and a weight for the second largest, the third largest, the fourth largest, and those weights are consistent. They're going to be just a constant. Oh, okay. Does that imply that we have, no matter where we are, we have the same number of actions? Yes. Okay. Is there any relationship between those weights? They can be negative, they can be a million, they can be anything you want? Oh, sorry. Yeah, good. So the row of the weight for a given rank should be It should be a convex combination. They should all sum to one and be non-negative. Okay, okay, okay. Sure. So it's like doing an expectation but based on how good the action is. Sort of. Yeah where it falls in the list yeah. So this actually generalizes the first line max. Because max is what we get if we put the row of the largest rank to be one and the row everything else to be zero. But it also let's us get min right. Where we have the row of the smallest rank equal to one and everything else zero. And everything in between, like uniform waiting over all the actions. Okay, I think I've got it. And let's see. Let's double check the last one. The last line, certification over minimax. I think I know what that means, okay. Well, just to make sure. So it's a little bit weird. I'm kind of abusing notation here, because minimax is usually defined when you've got two different actors, kind of a player one and a player two. And here I'm just writing it as if it were one actor. But, you should think of it as, this a prime represents a joint action by the two players. All right. Okay, I think I got it or at least got it enough to work it through. All right. Why don't you give it a shot, and if you don't get exactly the way I was thinking about it, that's okay. It's mostly to just sort of get you thinking about how we're substituting in different pieces to get this generalized MDP. All right, I like it. Okay, go. Okay Charles what do you think? About what? Let's just go through these. The ones that don't have names yet and give them names. Kind of think through what it is that they are like and label them. Okay sure we can do that all right. If the first one is a regular MDP and and what makes it a regular MDP is that you're doing an expectation. Over the next best action that will be taken, right? Right, the best next action I would say. Right that's the right way of saying it. So, here the only difference between the second one and the first one is that instead of doing an expectation you're going to only pay attention to the worst possible state you could get into. Mm, okay. So we're taking out the actual stochastic dynamics of the MDP and replacing it with something that is minimizing. Right and so that means if we were thinking about the function at the very top, the Q star, is not kind of the value of taking an action in a particular state given that you are then going to do the best thing from that point on. But it's sort of the value you would get is you always assumed the worst thing could happen. Mm-hm. And if that were true everywhere, then you would be valuing things by the worst possible outcome. So that would be a kind of, I don't know what the right word is, maybe something like a pessimistic MDP, or a risk-averse MDP, something like that. Good. Worst case MDP, maybe. Mm-hm yeah. It's also related to, in control theory, the notion of H infinity control. That H infinity control basically says, I don't know what's going to happen so let me assume that the worst thing's going to happen. Okay so pessimistic sort of describes the outlook on the world of this equation but is risk averse the right thing? This suggests that I will end up trying to choose actions that put me places where the least bad thing could happen. Well, I mean, I wouldn't quite think of it that way. So, it's almost as if the environment is out to get you. So, instead of just randomly transitioning you to some state it's always putting you in a state that makes you the least happy of all the states that you could go to. Oh, so grad school. [LAUGH] Well, it depends where you go to grad school. Like, Rhode Island's a pretty good state. All right, so we're going to be placed in a state that is worse for us, and once we get there, then we get to choose an action. We get to choose the action that's best for us. So it's also almost a kind of game, where we get to make a move, and then the environment makes a move, and then we make a move, and then the environment makes a move. Okay, I like that, I like that. But that means you would want to choose the action that would get you to the place where kind of the least worst thing can happen. Yeah, that's right. Right, so us, the agent, the ones who's doing the deciding, is trying to be risk averse relative to what the environment might do to it. Right, sure that makes perfect sense. So risk averse is not a bad description. Good. So other answers I would accept here include things like adversarial dynamics or two player game, things like that. I guess you could also, notice the way this is written here. This is the min over all next states so this is really going to always transition us to the worst state. Not the worst state that makes any sense, because we didn't do something like for all states in the set of states that are reachable with probability greater than zero, but that's a little bit more constrained. We didn't constrain it, so really it could just be MDP that makes you sit in the worst possible place, and just pummels you over and over again. But I don't think there's a nice succinct name for that. I thought that was pretty succinct, what you said. All right, good. All right, you ready to move on to the next one? Sure. Okay, so the next one, we've got our expectation back. That makes me happy. Yeah. So, we've got our expectation back, but now what we're doing is we're going, the next thing that'll happen is rather than doing the best action from that point on, we will take sort of the average over all possible next actions. Mm-hm. But give more weight the better the action is, if I remember what your row of ord actually does. That is a nice way to do it, but this is not written to be that way. It's just weighting by the ranking. So it could be that we put higher weight on higher rank things. But it could be we put higher weight on lower rank things too. So this is a very general expression. But you're right. I mean, this would be useful in the case where it's doing some kind of, I don't know, maybe a generalization of epsilon greedy exploration. The idea that we're going to, with highest probability choose the max, and with lower probability choose things that aren't the max. So it generalizes that idea. Right, and so actually that would be nice because one of the things about the way we go around learning MDP's and do Q-learning and value iteration is we kind of assume we're always going to do the next best thing. But since we still have to do exploration as well as exploitation that isn't actually what we do. So this would allow us to do exploration. Well, it's not that it's allowing us to do exploration, it's actually defining the values, so they incorporate exploration into them. Right. So one way of thinking about what this is, is an exploration sensitive MDP. Right, that's exactly what I was going to say. Oh, awesome! Well, just to finish this thought. The values that you define for the states include in them the probability that you might take a non-maximum action. That's the sense in which the values are exploration sensitive. We can explore in, say, Q-learning and still use this first update rule to try to figure out what the value is for the best policy. This is actually trying to figure out what the value is for the policy that is exploring. Right and in particular the policy we're going to follow as opposed to the policy we could follow. Yeah, that's a good point. Right, I think that's important. Yeah, so this is kind of neat because unlike regular Q-learning it's actually figuring out the value, it's on policy, it's actually figuring out the value of the policy that we might really be taking. Whereas if we were to literally always take the max, we won't learn the right value in the case of Q-learning because we're going to get stuck not exploring. Okay. I like that. I can't think of a better name. Are there any other names for this besides exploration sensitive? Well as I said it generalizes max and min. [LAUGH] One of the things that I really like but have absolutely no use for is that you can imagine an update role where you always are trying to choose the median action. kind of not too good, not too bad, middle of the pack. I guess it's kind of like the decision process for people who just want to be mediocre and not stand out. So I'll say mediocre. [LAUGH] Mediocrity-sensitive MDPs. And there's another case where I found use for this, which I think is kind of interesting. So imagine that we've got a huge action space, where actually computing this max is going to be difficult. Okay. So instead of actually running through all possible actions to compute the max, we're going to take some random sampling and then just compute the max of that random sample. So does that make some sense as an algorithm? Like instead of taking the max over everything, we'll just take a max over a handful of things. Sure, that makes sense if you have, for example, an infinite number of actions. Yeah, that's right, but even just a very large number of actions though. The first time I heard about this was in the case of an MDP that was being used to define scheduling actions for the Space Shuttle, back when there was a Space Shuttle, and there was just so many of them that going through them was intractable. So this is Tom Dietrich's work. He actually randomly sampled some of the actions and then chose the best among that sample. So what that gives you actually is there's some probability you will choose the actual max. But there's also some probability that you miss the max and instead get the second largest out of all the actions. And there's some other probability that you get the third largest of all the actions. It's more likely that you get higher ranked actions than lower ranked actions because of the subset that you choose, you're always taking the max of that. But it's not going to be with probability one, so what you end up with is some kind of distribution over the likelihood of choosing the max and the second highest and the third highest and so on. And this equation generalizes that idea and shows that if we use this as our update rule for something like value iteration, we get a well defined answer. This will converge to something. It's just a little bit hard to say exactly what it converges to. It's not the best possible policy. It's more like the policy that we get when we always choose the best action from a random sample. Right, but if they are a bunch of pretty good actions then you may not get the optimal policy but you'll still get a pretty good one. It's like be satisficing as opposed to satisfying. Ooh yeah. I would accept satisficing as an answer to this one. That's cool. Okay. Last one. So, the last one we have our expectation again, which I like, and now we are going to do minimax. What you said, before we started going through the quiz, is that you are kind of treating this as a joint action space. It's sort of you taking an action and then your opponent taking an action and so on and so forth. Really, I don't know what I would call this, but it looks a lot like just regular old game search when you have zero sum game. Yeah, zero sum game, that's exactly what I would call it. And we're going to talk a lot more about variations of algorithms that act like this that deal with the case when there's multiple agents. But this was sort of a nice setting to bring it up because the basic result says that if we've got a generalized MDP. We've got this operator and this operator defined in such a way that it's a non-expansion. And it turns out all these things, mini-max, this rank-weighted idea, max, the linear combination, expectation, min. All these things are all non-expansion. So all these things value iteration will do something reasonable. It'll actually come out with a fixed point in the limit. So actually min, max, all of these things and ord, row of ord, they're all convex sums. So does this mean that all convex combinations are non-expansive? So that's a great question. They are all convex combinations, that's true. It is not the case that arbitrary convex combinations result in non-expansions. In particular, if the weights of the convex combination depend on the values, the actual magnitude of the values in some way, then we need not get anything that is a non-expansion. Okay. So, that's good to know. Yeah. And that actually comes up in a really important setting, which is if you're doing not, say, epsilon greedy, exploration which corresponds to the third line here. But instead we're doing something like Boltzmann exploration we could run into trouble. So, I think that it's for this kind of introduction to generalized MDPs and I think that actually finishes the section on convergence. So you're ready for some recap? Yes I love recapping. All right, Charles, so what did we learn on this extended unit on convergence related stuff. Well, I think the first thing we learned was Convergence. Convergence is a thing and it's something that we would like to be able to prove in our other algorithms, and we can in fact prove them from other algorithms. Like what? That for example Q-learning actually converges. It doesn't just converge to something, it converges to Q star. Oh yeah, that's pretty good. Now we did that in the form of a generalized theorem that broke the Q-learning rule down into actually kind of two little sub-update rules. And the proof basically says, well if both of the sub-update rules do the right thing, then the overall update rule does the right thing. And that got us the Q-learning converges. And as you said, not just converges, but converges to the optimal Q function for the corresponding MDP, which actually is pretty amazing when you think about it, right? We have all these algorithms for solving MDP's and Q learning is like one line of code that just says, whenever you make a transition, do a little update like this, and nonetheless, that actually works in the limit it computes Q star. It's almost too simple not to try Q learning. Right, but it's generally the case that simplicity is often the right thing to do we call that elegance. Ooh, nicely said. What else have we learned? I learned some other words like nonexpansive. Or even better, contraction. I do not like that word. Really? It makes you uncomfortable. [LAUGHS] Okay, so the last thing that I remember is talking about is speaking of generalized, generalized MDPs. Right, generalized MDPs have these nice structure that as long as the operators that are inside of it that are doing updates or non-expansions then we get these other kinds of convergence. We get the convergence of value iteration. We get the convergence of Q learning. So, we can actually combine these generalized MDPs with the generalized convergence theorem and kaboom. Yeah. And that's actually pretty cool because it would allow us to think about different kinds of decision processes. Different situations we want it to be in and we would know that all the math would go through and so we could just do it. All we have to do is worry about whether they're non-expansive or not. Great! Exactly and there's a whole set of things that we've shown are non-expansions. Oh, maybe that's even worth remembering. What are some things that are non-expansions? A max. That's a good one. Min. More generally, order statistics. Yes, order statistics. Fixed convex combinations. Uh-huh. That's my favorite. Actually, I think all the things are kind of variations of those two. Yeah I think so. You can get fixed convex combinations of order statistics, so we can actually combine those two things together. Oh, that's cool. Is there anything else? I think that's all that we covered. Yeah. Cool. But I think importantly, you said something that I think was important, which was that, not just any convex combinations, but it has to be fixed. That if the value of it can change, like with the Boltzmann exploration, then you do not know that it will converge. That's right. It's actually kind of, maybe an interesting open problem. It's definitely the case that we can prove that Bolton updates are not non expansions. They can be expansions. But that's not quite the same as saying that therefore it doesn't converge, and there are some examples of Q-learning-like update rules that have update rules that are not non-expansions and yet it converges anyway. There's the one that I was involved with really recently that we called coco- Q learning. Of course you did. Which is a two player game set up. And that one you can show that it's not a non-expansion, but it converges anyway. Phew. Okay, so one last thing and I think we've covered everything but one last thing and I realize we have an open question. Okay. So up in the very first line I we have convergence. And it's not just convergence for Q-learning but the Q-learning actually converges to Q-star, which is mildly miraculous. All these other things we were looking at, these generalized operators and the order statistics stuff, those things converge, but do we know what they converge to? Right, they converge to that which they converge to. So in general it's not Q* because we're actually defining a different set of Bellman Equations. So it actually is defining a different fixed point. In some cases those fixed points have good names. [LAUGH] In some cases not so much. But do they converge to something that looks like the corresponding Q*? If you know what I mean, do you know what I mean? Yeah, yeah yeah. So the idea is that they converge to something that makes sense with regard to how the Q equation is defined. So for example if we replace the max in the action selection with some fixed probability distribution over actions. That's kind of equivalent, or maybe even is exactly equivalent, to having a fixed stochastic policy. And what we get convergence to in that case is the value of that policy. Okay. So that's good. That's a big win. All right. So, did I forget anything? No, these are the things that I think were the critical high points of this fairly formal section, right. We did a fair amount of proving of stuff. I think we're going to do a little more of that before we get into some more implementation and practical stuff. Okay cool so then you'll be doing those. Yes. That's great well then I look forward to next time. Awesome. All right. Thanks Charles. All right. Bye. Hey Charles. Hey Michael, how's it going? It's not going too badly. I'm kind of excited. because what we're going to do in this section, is take the previous two sections and kind of mash them up. Oh, I like that. I always liked mashed sections. So, in particular, we're going to use some of the ideas from convergence to better understand value iteration, linear programming, and policy iteration, three algorithms for solving MVPs. So I decided we could call it Advanced Algorithmic Analysis, or AAA. Oh, I like that. And I like that you have a scarlet A. Yes. That's a nice allusion. That's another A. Yes. So last time we were talking about how contraction mappings make it so that value iteration converges you know, in the limit that we end up solving the Bellman equations. I'm not going to want to say too much more about value iteration, but on one slide I want to at least summarize a couple interesting things that are worth knowing. All right, so here's the first one. If you think about QT star, so that's the Q function that we get if we run value iteration for T star iterations. Okay. We know that it converges in the limit. We know that QT eventually goes to Q star. But here's kind of an interesting thing that we know about the finite horizon. So there is some T star, less than infinity, that's polynomial in the number of states, the number of actions, the magnitude of the rewards in the reward function, the number of bits of precision that are used to specify the transition probabilities, and one over one minus gamma. So that, if we run valuation for that many steps, the Q function that we get out is Q sub T star of SA. If we define a policy, pi SA, which is just the greedy policy with respect to that Q function. That policy is optimal, okay? So what we're saying here is that we know that in the limit, if we run value iteration for an infinite number of steps, then the Q function that we get out at that point, the greedy policy with respect to that Q function, is optimal. But what this is saying is that there is some time before infinity where we get a Q function that's close enough, so that if you do the greedy policy with respect to it, it really is optimal. Like all the way 100% optimal. So what that really means it's polynomial and the things you would expect to matter. And so that's saying that you can actually solve this in a reasonable amount of time. Yeah. I mean any time I see the word polynomial, I'm like okay, what you're saying is, this is reasonable. Yeah, now the trick in this particular think. The reason it's not just polynomial is this one over one minus gamma. So as gamma gets close to one, this blows up and that's not polynomial bounded in say the number of bits that it takes to write down gamma. So it really is exponential in terms of the number of bits it takes to write down the whole problem. Mm. But even so, if you take gamma to be some fixed constant. Then one over one minus gamma might be really big, but it's some fixed constant and we're polynomial in all the rest of this stuff. So, what this really all boils down to is the idea that once you fix an MDP and the number of bits of precision that your using to write it down, then there's going to be some optimal action and the might be other actions that are tied for optimal in any given state. But the second best action is going to be some actual bounded amount away from optimal. So, it can't get arbitrarily close to optimal. It's going to be some distance away. And what that gives us is that when we run value iteration enough, eventually the distance between the best action and the second best action gets bigger than that gap. And once it's bigger than that gap, then the greedy policy's going to be optimal. It's going to choose the actual optimal action in all states. So VI converges and it converges in a reasonable amount of time. It's not that it converges in a reasonable amount of time. The greedy policy with respect to value iteration converges in kind of a reasonable amount of time. Okay. That's a much better way of saying it. Okay. I buy that. This turns about to be a consequence of Cramer's rule, which we're not going to talk about. But this is why, once we write down everything with polynomial precision, that we are guaranteed to get some fixed size gap, between the best and the second best. It always amazes me how you manage to get Seinfeld into our conversations. [LAUGH] Kramer rules. [LAUGH] So here's another thing that we know, that is if we run value iteration, we get a series of value functions, say Vt. If we compare the value function we have at some time step, t to the same value function that we have one time step later, Vt plus 1, that if it basically has stopped changing. If the change, if the amount of change from Vt, sorry, from t to t plus 1 is less than epsilon for all states in the MDP, then the largest difference between the optimal value function and the value function that we get, sorry for this. But, by taking that value function, taking the greedy policy with respect to that value function and taking the value function of that greedy policy is small, right, so we're going to be doing almost optimal from all states. And the smallest here ends up being 2 times that epsilon times gamma, divided by 1, minus gamma. Are you sure that's intuitively obvious? [LAUGH], well maybe not intuitively obvious, but the Sing and Yi proof of this is not so complicated, not so different from the stuff that we've been doing. And you get the 1 over 1 minus gamma because this little bit of epsilon that we could be off, could get magnified over every single step as we think about this into the future. That's why we get a 1 minus gamma in the denominator. And then 2 epsilon, because I don't know which side of the epsilon we're on, times gamma because it doesn't take place right away, it's one step from now. So it does kind of make sense. But the real important upshot of this is the notion that if you can get a good enough approximation of the value function. Then you're going to have a good enough, well, then you can at least put a bound on how far off you are in terms of following the resulting policy from how far that's going to be from optimal. And furthermore we don't even need to know how close we are to V star, all we need for this result to hold is, we look at two consecutive iterations of value iteration, right, which that we have. We're running value iteration. We know what it was a step ago, we know what it is now. And so we can compute this epsilon and that gives us a bound on what if we just stop now and follow that policy. So that's a really useful thing. Otherwise, knowing that valuation converges in the limit is not super duper helpful, because you're never going to get to the limit. And knowing that value iteration eventually is optimal, even if finite is still not that helpful, because you don't know when that takes place. So what this is telling us is, we can actually test on our own, when is a decent time to stop. So that's cool. So here's something I just noticed. If I look at both your 1 and 2, I noticed that they both would encourage you to pick small gammas. Mm. Right. Yes, it is certainly the case that if gamma is really tiny, like if gamma is 0 then this bound stays small. This bound stays small. It's all good. The closer gamma gets to 1, the more things are problematic. But what do you infer from that? Well, mainly why is that? Well, that's because gamma really tells you what your horizon is. And so the smaller gamma is, the sort of further into the future you don't look. [LAUGH] If that makes sense. And so that makes sense then, right? That if I'm not really worried about the future, then it doesn't take much for me to be doing what appears to be optimal. Yes. That's right. Right, it's easy to optimize over a very short horizon. And the horizon, the effective horizon is always something like 1 over 1 minus gamma. That's kind of how far into the future do rewards actually matter. Right. And so right, so gamma closer to 1, is really looking ahead a lot further. And that's why it's a harder problem to do. It's harder to get things exactly right. So then why, in practice, don't we always set gamma to be really, really small? Yeah, that's where I thought you were going to go, so it's a great question. I mean one of the things that happens if you set gamma to be something very close to 0, is the agent who's acting in the world acts very myopically. Very short term thinking. So, I feel like eating now instead of finishing recording this lecture. So I might as well do that. Right, so you get shortsightedness. I am eating now. [LAUGH] Well that's good for you. I am not eating now. I would like to eat now, but I won't because I'm sufficiently farsighted to know that just getting something yummy to eat right now is going to interfere with my ability to finish this course. Because our wonderful and talented video editor eventually going to stop [LAUGH] processing this stuff, and then we're hosed. So, yes. Gamma, smaller gamma means easier problem, but more short sighted behavior. Closer to one gamma is looking ahead more into the distance, but can make the computations more difficult. So it's basically what these two results are saying that there is a trade off between horizon and sort of feasibility of solving a problem in a reasonable amount of time. Yeah. That's a good way to say it. So here's a third useful fact, that I probably should have put first now that I write it down. And that is the idea that, if we start off with sum Q function, Q1, and we run k steps of value iteration, in other words we apply the Bellman operator k times, that the k step Bellman operator is a contraction mapping. And the index of contraction is like gamma to the k which is a much, much smaller number. So running k steps of value iteration actually pushes your value functions much closer together than let's say, a single step value iteration. Which is really what you'd expect. That is, although that's even better, right, because it's not just like linear. It's not like linear. Right, it's like gamma squared, gamma cubed, gamma to the third, gamma to the k. Right, and that is the index of contraction so we run ten steps of value iteration, that's like running one step of ten step value iteration. So again, it's not super surprising but what's nice about it is it gives us a way of quantifying how long we run value iteration and connecting it with how close you are to optimal after you've run that far. And so, this kind of fact is underlying these other things that I wrote number one and number two which is probably why I should have set the third one first. That's okay, we can fix it in post. So we talked a little bit about value iteration. And we said, that eventually, it identifies an optimal policy and polynomial time, in 1 over 1 minus gamma. And I pointed out that, 1 over 1 minus gamma isn't really a polynomial in the number of bits that it takes to write down gamma. because you can, with every few bits, specify a number that's very, very close to 1, causing this value to explode. So, in fact, value iteration doesn't give us a polynomial time algorithm for solving MDPs. There is only one way that we know to solve MDPs in polynomial time and that is, [SOUND], Linear Programming. Okay. Do you know what Linear Programming is? I'm going to say yes. All right. So it's optimization framework, in which, you can give linear constraint in a linear objective function. And in polynomial time, as long as the number of variables and constraints is polynomial, get a solution. That's exactly what I was going to say. Yeah, well, this is why I was going to say it for you. Thank you. So if we're going to use this idea to solve Markov decision processes, we need a way of encoding a Markov decision process solution as a linear program. So we need to encode our MDP as a linear program. So linear constraints and a linear objective function. So how do we know how to solve a MDP? We just have to solve the Bellman equation. Bellman Equation. Right. I can write the Bellman equation in terms of the value function as a set of equations where the variables are the value function that we're interested in. So for each state s, we have a variable v sub s. And we relate the v sub s' through this equation to the v sub s primes which are themselves just v sub s', just ordered in a different way. And that seems really good, right? We have a set of constraints. And if we could just solve this set of constraints, we'll be good to go. So are we done? Do we have a way of doing this with linear programming directly from this expression? Not if max, unless max has become linear since I last looked, no. Right, which they're not so much. Yeah, so since the max over actions in not a linear operator, this isn't translatable directly to a set of linear equations and a linear objective function. We have a system of non-linear equations, but that's not so good. So, the cool thing about max is that we can actually think about max as being specifiable in terms of a set of linear constraints, and a linear objective function. Let's think of it this way. Let's say, that we're trying to compute a max, and this is just as a simple little concrete example. Here's a thing that we're trying to compute the max of. And the max is going to be some value x. What is the relationship between x and all these values? So what's the relationship between x and -3? X is greater than -3. Greater than -3, or in general, we could say greater than or equal to. Mm-hm, I knew that's what you were going to say. Yeah, you know, it's always nicer to have or equal to's, it's more inclusive that way. Mm-hm. And we can say the same thing about 7, right? X is going to be greater than or equal to 7. And x is going to be greater than or equal to 2. And x is going to be greater than or equal to 5. And how do we know this? Because it's the max of those. So it has to be at least as big as any one of them, right? Right. By the way, your bottom thing says equals. Greater than or equal to. Mm-hm. It could be equal to 5, if I get rid of the 7. Yes. [LAUGH] All right, so this is a set of constraints, of inequality constraints. Right? So is it true that now, the solution to this set of inequality constraints is exactly the max? No. And why is that? Because 9 is also greater than or equal to all of those things. It is! And 9 is the max of these things. No. No, that's not quite true. No. So what do we have to say? We have to say that, it's bigger than equal to all these things. And of all the things that are bigger than equal to all these things, we want the smallest such x. We want the x that is the smallest possible while still satisfying all of these constraints. In this case, that's going to have to be 7. Right. Right, because 7 is bigger than all the other ones, and you can't get any smaller without getting smaller than 7. Right, and of course, min is a linear operator so we're good. No, no, no, min is an objective function. This is a linear objective function. This is okay. Good, good, good. That's what we are going to do. We are going to generalize this idea to the equations in the Bellman equation in a very similar way. I look forward to it. So we can get part of the idea really simply by just replacing this equation with something that says for all state and action, the value of the state is bigger than or equal to this expression. This is essentially the q value, for each assay this is the q value assay. So that's good and know we have to add that min thing. So what do we minimize here? V. So we can't just minimize V like that, because that's a vector. Right. And we can't just minimize V sub s because s is unbound here [LAUGH]. Right, it's like which s do we want to minimize? So we kind of want to minimize all of them somehow. So the way we're going to write that, if this turns out to work. Is minimize the sum. So that's going to put pressure on all the individual Vs to be as small as they can be, so that it actually equals the maxes, it isn't just an upper bound on the max. Right, because if any one of them isn't, then you don't have the minimum sum. Right, then you can always move it down a little bit. Right. So this actually is a linear program, that is the solution to which is equivalent to the solution to the MDP. So we can just write down this linear program and give it to a linear program solver that runs in polynomial time, and boom, we get our values V sub s. How do we get our policy from that? We just choose the action that always in expectation gives you the best value, just sort of one [INAUDIBLE]. Yeah, we choose the greedy policy with respect to that optimal value function. So this is cool. This is sometimes called the Primal. I guess because it lives out in the woods. [LAUGH] Representation of the linear program for solving MDPs. And this is a perfectly reasonable [LAUGH], this is actually very useful from a theoretical perspective. I haven't seen too many people really use Linear Programming to solve MDPs. It's got some overhead associated with it that seems to make it a little bit difficult to get it to run competitively with the other algorithms. But it's okay, especially if you have a really good Linear Programming package. And if you happen to have other kinds of linear constraints that you need to throw in, this is a very convenient way of solving slightly more difficult problems, MDPs plus a little bit of extra constraint. So, does that make sense? Are you good with that? Yeah I'm good with that. All right, I want to say one more thing about Linear Programming by switching away from the Primal to what's called the dual. So, Linear Programming has this nice property that you can actually [LAUGH]. Any linear program, you can change the constraints into variables and the variables into constraints, and you get a new linear program that's equivalent to the old one. So this is just true in general for Linear Programming. It's linear programming duality. And sometimes it's a really useful way of actually solving them and for putting bounds and constraints on the solutions. But in the context of MDPs it actually does something kind of cool, so I want to show you that. Okay. Is it going to be as cool as what happened when we did support factor machines? I don't remember that. We did the same thing. We wrote down a series of equations and we solved for the dual. Nice! So, yes, it'll be the same amount of cool as that. Excellent, excellent. I look forward to it. So the process of producing the dual of a linear program is just a mechanical process. There's a series of steps that you go through where each constraint from the primal becomes a variable in the dual and each variable in the primal becomes a constraint in the dual. And certain maxes become mins and bounds become objective function things. So what I've done here is I've actually gone through that process, which we could go through in detail, but let's just pretend that we did. [LAUGH] And we create a new linear program from the old linear program and the new one is called the dual. So let's take a look at this dual in detail. Because it turns out that the variables and the constraints actually have some very nice interpretations that are worth thinking about. Here in the dual, the thing that we're trying to optimize is we're trying to maximize the sum over all state-action pairs of the reward for that state-action pair times this value little qsa. So little qsa, probably not a great name for it because it's not the same as a Q value. But it's something. It's something that we're multiplying by the rewards and we're trying to maximize it. So one way I like to think about qsa is as policy flow. Sort of how much [LAUGH] agentness is flowing through each state-action pair. If it follows the policy it's going to spend some time running around in the environment, passing through each state-action pair. And each time it passes through a state-action pair, we're going to get the reward associated with that state-action pair. And what we want to do is maximize that reward. So from this sort of, policy flow concept, it sort of makes sense what it is we're trying to maximize. It's in some ways even more intuitive than in the primal, which we were saying we're trying the minimize the value, which of course we're not really trying to minimize the value. We're just trying to minimize it so that it doesn't end up being an upper bound on the value. Here we're actually trying to maximize expected reward, which seems like a really good thing. Right, right, right. Okay so this is now going to be subject to the following idea, that for each state, in this case it's easier to think of them as next states. For each possible next state, we want it to be true that the amount of policy flow through that next state summed up over all the actions that are going through it. That should be equal to in some sense the number of times that next state is visited, which we can get by summing over all states we might have started at, and actions we might have taken form there. The policy flow through that state-action pair times the transition probability that would send us to s' as a result of that. And we're going to also include in this equation sort of the sense that we can also start a given state-action pair. So we're just going to stick a 1 in for that. So there's some policy flow that we're injecting into each state-action pair of the system. And we're going to add to that any policy flow that would be coming through it through other parts of the system. So it's kind of like we're dumping policy flow all over our MDP. And then letting the MDP kind of pump around what's left. And it's discounted, there's a little discount here. So there's actually evaporation or something happening all over the place. And what we want to know is what's the way of letting the flow go through, kind of deciding at each state which action it should flow through, so that we ultimately get the maximum possible reward. And I think just one additional constraint here, just to make sure that the policy flow is not negative, so that we can't introduce a whole lot of policy flow in one place. And then drain it out some other place to make things balance. You could think of these constraints as actually being a kind of conservation of flow. So the amount of stuff going into s' has to equal the amount of stuff coming out of it. And that's all this is saying. It's just saying that that has to balance. It has to be a meaningful flow. Subject to that constraint, maximize reward. So, again, this is kind of a neat interpretation of what it means to solve a mark off decision process. But it just comes mechanically out of the primal version of setting up this as a linear program. Okay, does that suggest an algorithm that's different from the sort of algorithms you've been using so far? Hm, interesting thought. So, well for one thing, it is an algorithm in the sense that once we set things up this way, we can hit it over the head with the linear programming stick and a solution will come out. And in fact, it'll come out in polynomial time, which is nice. But you're right. I mean in some sense, if you think about value iteration as something that's propagating these values around, this has a different form. This is really concentrating more on the policy aspect of the MDP. And maybe we could make an algorithm that more directly kind of searched in policy space for the best behavior for the MDP. Let's do that. Charles I'm going to say that you invented an algorithm just now that we're going to call policy iteration. It's kind of like value iteration, except it's going to iterate a policy space. I'm so smart Yeah, though you invented it a little bit to late, because it dates back to 1960 or thereabout. Dijkstra? Dijkstra always has ideas before I do. Yes. Dijkstra's pretty clever, but this is actually Ron Howard. You mean the Ron Howard? The director? Oh, sorry. No. It's the Ron Howard. The algorithm designer. [LAUGH] Oh, that one. I always get them confused. One played Opie. And the other one proved contraction mappings. They're very similar things. So this is what I was imagining you were explaining. Take it step by step. So what we're going to do is we're going to start off picking an arbitrary Q function like we often do. We'll call that the initialization step. Then, we're going to iterate the following. We're going to take the t-th Q function. And compute its greedy policy. Call that pi sub t, then, that policy, we're going to evaluate it to get a new Q function, Q 2 plus 1, and then we're going to repeat, and iterate this over and over and over again. So each time we go around this loop, we're taking our previous Q function. Finding it's policy, taking that policy, finding its value function and repeating. Lather, rinse, repeat. Exactly. So unlike when I take a shower, we actually get convergence in finite time. [LAUGH] So in particular the sequence of Q functions that we get converges to Q*. Which is good, that's like how policy iteration works. But even better, convergence is exact and complete in finite time. I guess that was kind of true of value iteration as well. And it converges at least as fast as value iterationm in that if at any point we sync up the Q functions, we start value iteration and policy iteration from the same Q function. Then each step that policy iteration takes is moving us towards the optimal Q function, no more slowly than valued iteration does. Okay. So that kind of suggests that this is just way better. Yeah, isn't it way better? So, why is it not way better? There's kind of some excitement going on in here that we need to take into consideration. There is a bit of trade-off as you like to say. Mm-hm. What's the trade-off? So where's the trade-off here? We're getting faster convergence at the cost of greater computational expense. So in particular this step, this policy evaluation step that says take the policy and then work out the Q function for that policy. You can do that by say solving a system of linear equations. Or perhaps more commonly, by writing something like value iteration to completion. So in the inner loop of policy iteration is something that's an awful lot like value iteration. And so maybe it's not so surprising that it goes at least as fast as value iteration. It's doing a lot more work than value iteration. Each iteration of policy iteration is doing pretty much all the work of value iteration. Yeah, well, so, it just depends upon what you're counting. I say we just count the outer loop. Then we win. Or at least we don't lose. In fact, this is kind of an interesting outstanding question. So, we don't really know how many iterations policy iteration takes, so an open question is what the convergence time really is. We know a couple things about it, but it turns out to be fairly weak. We know that there are some MDPs. Such that the number of iterations, the policy iteration takes, is linear. It's at least as large as the number of states in the MBP, though I don't think anybody's actually shown like two times the number of states. So all we know is something really, really basic which is that it takes at least the number of states iterations, in theworst case. And we know it can't be any worse than number of actions raised to the number of states an exponential. But where is it's in between we don't really know. And so if it's closer to linear, then it's totally awesome and it blows the doors off value iteration. If it's more like exponential, then It's probably still better than value iteration, but it's definitely more of a wash. What I'd like to do, and [LAUGH] I'm guessing maybe you don't want me to, but I'm going to try to do it anyway, is go through the argument as to why policy duration actually works. I think ultimately it's not that bad, but we're going to [LAUGH] slog through it. And so, one of the things we're going to need is the ability to compare policies to each other, and value functions to each other, and so, we're going to introduce the concept of domination. Oh, yeah. [LAUGH] So, this is what domination [LAUGH] means in the context of policies. We're going to say that policy one dominates policy two if it's the case that for all states the value of that policy, at that state for policy one, is bigger than or equal to the value at that state for policy two. So, it could be that there's some policies where policy one does better in one state, policy two does better in another state, but if it dominates it, policy one has to do no worse in any state. And in fact, we're going to say we have strict domination. [NOISE] If it's the case that [LAUGH] policy one dominates policy two, and there's some state for which it's actually strictly better, it's not equality. And just for fun, we'll also introduce the concept of epsilon optimal policies. So, a policy is epsilon optimal if it's the case that the value function for that policy is epsilon close to the value function for the optimal policy at all states. And so what this does is it gives us a concept of bounding loss or bounding regret. So, we'll say that a policy is nearly optimal if the amount that it loses, essentially, per time step is very small. Okay. So, by the way, does this mean that if policy one dominates policy two, but does not strictly dominate it, they must have the same value everywhere? Yeah, that's right. They'd have to have- it would never be greater than, it would always be greater than or equal to, which means it's always equal to. So, two policies that, well, what you said. Oh, okay, good. Well, as long as it's what I said. In addition to this notion of domination, another thing that's going to be helpful is, if I tell you a tale of two operators for two policies. So imagine we've got policy Pi(1) and policy Pi(2). We're going to define the one step Bellman operator on those two policies. So the first operator B1, when we apply it to a value function V is just going to return for each state the immediate reward from that state. If we follow policy Pi1 plus the discounted expected value for following policy Pi1 with one step look up on V. So we're converting V, we're basically putting V through a one step value iteration with a fixed policy Pi1, and B2 is the same idea, same operator, except with respect to policy two. Pi2. Pi2. So, what we're going to be able to show is various kinds of interesting domination relationships between the value functions that we get out with each of these operators. All righ, t so now, we have got these two Bellman operators, B1 and B2. That each one is associated with the a fixed policy. So B1 makes an update, essentially following Pi 1. And B2 makes an update following Pi 2. So we know a bunch of things about these updates. So for example, we know that if we apply B1 to two value functions. What's going to happen to the value functions as a result? They will not move further apart. By a factor of at least Gamma. Unless, they're the same one. Right? If they're already perfectly together, then they won't move closer together. Because they were at a fixed point. But then their distance is 0. And we do get gamma closer to zero, because 0 times Gamma is still 0. Sure. So that's true, but I'm just saying, it's not that they always get closer together. Yeah, okay, all right. That's fair. Okay. That's fair. If they are not on top of each other, then they get closer together. And the same thing is true of B2. Yeah. But there's some other interesting properties of these that aren't necessarily true of the standard Bellman operator, but they are true when we used the fixed policy like Pi 1 or Pi 2. So let's step through what some of those properties are. And we'll use them to show that policy iteration behaves itself. Nice. So this is the property that we want to show that B1 and B2 have. So, if we've got two value functions, V1 and V2, not only if we apply these Bellman operators to them do they get no farther apart and actually potentially closer together. But they actually keep their same ordering. So let's say that V1 dominates V2. So V1 is bigger than or equal to V2 at all states s. Then, that still is true after we've applied Bellman operator B2 or Bellman operator B1. But we're talking about B2 right now. Okay, that makes sense and I think it follows immediately from raising Thursday to the Monday power. Oh, I see. No, that's supposed to be theorem. Sorry about that. Okay We don't do day of the week math until the next section. Okay, looking forward to that. So how would we show something like this is true? The standard thing that we always seem to do is go back to the definitions and just plug things through. And let's see what we get. Okay To show that B2 V1 dominates B2 V2, I'll just subtract one from the other and see if the result is always non-negative. Okay. So let's do that. We plug in the definitions. And I did some cancelling here, because it's, it always sort of comes out the same. This sort of notion that since we're talking about the same states and the same actions, these rewards are going to cancel. The Gammas and the summations and the transition probabilities are going to factor out. And we're going to end up with something, well, actually a lot like this. Gamma times the sum over all states of the probability of reaching the next state times the difference at the new state, s prime, between the value functions V1 and V2. Oh, okay. And what do we know about this quantity here? Well, we know that V1 is always greater than or equal to V2, which means that V1 minus V2 has to be non-negative. Good, and then we take a convex combination of a bunch of non-negative values. There's no way it could become negative. Right. It could be 0, I guess, but we know that we're greater than or equal to 0. It can't be smaller than 0. So that gives us exactly what we need. That if V1 dominated V2 before, B2 V1 dominates B2 V2 now. Nice. And if we replaced B2 with B1. This would also be true. Yes. Exactly so. The only thing that we're actually using here is that we have a fixed policy. It doesn't matter what the policy is. So B1 and B2 are the same operators except they just differ as to which fixed policy they are using. Okay. So both B1 and B2 have mono. That's perfect. No. They don't have mono. They are monotonic meaning this preserve this kind of ordering property. Not that they have mononucleosis, sometimes known as the kissing disease. Well, maybe where you're from. So we're going to need one more interesting property of this operator B1, but this one is specific to the context of policy iteration. So let's think about it this way. Imagine we've got some policy, Pi1, just like we've been saying, and associated with Pi1 is the operator B1. And let's say that Q1 Is the fixed point of B1 and how do we know that it has a unique fixed point? because you proved it earlier. Yeah, it was the contraction property of B1. Yeah. So let's imagine that's happened. We have solved out Q1, and this is sort of how policy iteration works too, right? You start off with the policy, you get the value function of that policy then we're going to take the greedy policy Pi2 with respect to Q1. So in the last slide or so Pi1 and Pi2 were just arbitrary policies. Now Pi2 is specifically the greedy policy with respect to the value function defined by Pi1. So then let B2 be the operator associated with Pi2 and this is the thing that we want to show. The Q function that comes from solving, for the the fixed point of Pi1, so the value function for Pi1 is less than or equal to B2 applied to Q1. Does that make sense? So if we take whatever we end up with after we solve for the value function for Pi1, we take Pi2s greedy policy and then we do one Bellman backup with respect to that and we get value function that dominates Q1. Okay, and that's because of other stuff on the slide. Yeah so we'll just work through that. So intuitively what we're saying is you do policy evaluation on Pi1 we get some value function Q1 and then we do one step of essentially value iteration using the greedy policy with respect to Q1 and that's going to make it no worse possibly better. And so the reason for that is well what is Q1. Q1, it's the fixed point of the B1 operator. So in this particular case it's the reward plus the discounted expected value for taking action Pi1 and ending up in some new state as prime and taking action Pi1 of S prime from that state. But think about what Pi2 is. We can actually substitute Pi2 for Pi1 in here but Pi2 is the greedy policy with respect to this. In other words, it is the policy that causes this exact quantity to be maximized. That's what the greedy policy does. At every state either we're going to do no worse than we did with Pi1, or we might actually do better by switching to Pi2, the greedy policy with respect to Q1. So at each state action pair we're doing no worse by doing one step of, well, one Bellman backup with B2 on Q1. So this is exactly the result that we get. That the Q function that we had before is less than or equal to the Q function that we had before pushed through the B2 operator. The Bellman operator with respect to the greedy policy with respect to Q1. Essentially the idea is if we are going to do one more update on Q1, and we are going to do that update with respect to a policy that we know is greedy with respect to Q1, then we are moving up in the world. Oh, I see, I see, I see, okay. What? Well, no, I was sitting there thinking well shouldn't this be greater than or equal to and then I think, oh no, I see, I was kind of reading it backward. Yeah, I wanted to write it as, because domination up to this point we used greater than or equal to, but it just didn't feel right to put it this way because it would have been B2 applied to Q1 is greater than or equal to Q1, which is true, but the ordering feels backwards to me. So this seems better, that we're going to take Q1, we're going to apply the B2 operator to it, and it's going to make things better. Write it the other way and see what happens. Sure. So, when I look at that, that says that applying the greedy operator to Q1 makes it better than it was before. All right, so that is more helpful for you read, that's really the same thing. So that's great. Oh no, it is the same thing. I just realized the whole time you were talking I had been flipping the thing in my head. Sorry about that. Oh no, no, it's not your fault. So now we have all the pieces we need. We had one property, we called it monotonicity. What, we should call this property something, too. Value improvement. Okay. So now with value improvement and other stuff like definition of Pi2 and B1 and B2 and Q1 and Q2 and the monotonicity property, we should have all the things we need to prove that policy iteration moves in the right direction. That it actually improves the policy. Or, in the case where the policy's already optimal, it can't improve it, but it doesn't break it. It leaves it the same. It doesn't ever make it worse. Now we have all the pieces to put together a proof of policy iteration. And since you have all the pieces, you should be able to do it yourself. So here's what I'm doing. Let's take one step of policy iteration, that is we start with some policy pi 1. And just for notation, we have Bellman operator B1 associate with that policy, and Q1 as the fixed point of that operator. And then Q2 is the next policy we get. It's the greedy policy with respect to Q1 that we had from the previous iteration. And just again for notation, B2 will be the Bellman operator for that policy, and Q2 its fixed point. And ultimately, what we want to show is that the Q function that we get for the second policy dominates the Q function that we get for the first policy. And so this is how we're going to do it. Were going to have a series of steps. And each of them has justification that we've been talking about recently. Just fill in the boxes. Good, I think I understand what you want and I think I can do it. Yeah and we'll try to be a little bit generous with what the names of these justifications. But I did give names of these steps in some of the previous slides. So I'm hoping that those words should still be jumbling around. Some of them anyway. The first statement we need to make this proof go is that B2, the operator B2, applied to Q1, dominates Q1. Right, which you just proved a slide ago, right? Right. And what did we call that? Value improvement. Good. Yeah, the fact that we're using the greedy policy with respect to Q1, one step of update with respect to that gets us a possibly better value function. Right. Great, now the next statement says that if we iterate this operator, so B2 iterated K times, applied to Q1, versus B2 iterated K+1 times applied to Q1, the K plus 1 one dominates the K1. So this is kind of step of a proof by induction. But what's the main justification here. What lets you make the statement that one additional application of B2 is going to dominate the previous adaptation. So I'm going to say that it's the other thing that you said because I only remember two things that you said. And so that would be monotonicity. It's the monotonicity of B2. Right. But if you've used up all these, the next two boxes are going to be hard. But just to go through why this is the case, what we've done is we've taken this first equation and applied B2 to both sides. Just over and over and over and over again. But since one dominated the other, after we applied B2 to it, it still dominates. Right that was like two slides ago that was the whole point of monotinicity. And they look very similar. Value improvement and monotinicity look very similar to one another. Exactly. And they all ground back in domination. Right, everything comes down to domination. Boom. Nice very nice. Okay cool. All right so we got value improvement although I guess that's more accurately value non-deprovement or something. [LAUGH] Just to be you know mathematically technical here and then we've got monotonicity. I like how you became more mathematically correct and less grammatically create. Well those things are trade offs, they're fundamental trade offs. Okay so we've got that. So we got value improvement, we got monotonicity of our value improvement so to speak. And so that's actually what the third thing is. It's basically. The monotonicity of our value improvement. You just kind of kept saying if I keep doing the second thing over and over and over again, it keeps getting better and better. Therefore the very last one that I do, has to be better than the very first, or at least no worse than, the very first one that I did. So I guess that's, there's a math term for this transitivity. A greater than B greater C, A greater than C. Oh, that's right. So transitive property of monotonicity non-deprovement. I wouldn't say that. It's what we get each time we iterate this B2 operator, we are getting something that dominates the previous value that we had, which dominates the previous one, which dominates the previous one, which eventually grounds out in this first statement Q1, so if you just chain all those together we get- For each of the possible ks the number of iterations that we could apply the B2 operator, we're just getting more and more monotonistically improved. Right. So that's, but just remind everyone that's just if a is greater than b and b is great than c, we know that a is greater than c. Right? Transitivity. Yeah, that's just using transitivity, but we're it over and over and over and over and over. Yeah, but it doesn't matter. A greater than B greater than C greater than D greater than E greater than F greater than G. Got it. Okay. And then the last statement says, okay, well now that that's true we know that Q2 dominates Q1. Right, and that's because you said way up there at the top that Q2 was the fixed point of the B2 operator and the limit of K approaching infinity Vk of Q1 is in fact V. Fixed point. Good. So it's the definition of Q2 as the fixed point, and this sort of notion that we can just keep repeating this operator and we get to the fixed point eventually. Does it matter that there's Q1 plugged in here? No. It does not. Because you defined it that way. Not that we defined it. Well, up to this point, we came through Q1. But ultimately, the fact that we get to Q2, doesn't matter that we started at Q1 because there's a unique fixed point. So no matter what value function we start at, if we keep applying these operators over and over and over again we're going to get to Q2. No matter what. And so we did. Yeah, but we want to be able to relate it to Q1 and doing it this way allows us to do that in four easy steps. Yeah, four slightly easy steps, one slide per step. But yeah, that's exactly right. So we related Q2 and Q1, which is to say we've related two adjacent steps of policy iteration to one another. Each time we do a step of policy iteration we get no worse. This doesn't tell us that we can't reach some kind of local optimum. Mmhm. But it is telling us that we're getting the worse as we apply this series of operators. That's nice. I like that. So this is a pretty straightforward derivation of fixed proof through transitivity of value non-deprovement monotonicity. Yeah, obviously. I like proofs. I like obvious proofs. [LAUGH] I like the word non-deprovement. We'll have to use that. That could be the name of our band. The really important thing here is that policy iteration does not get stuck in the local optimum. That is to say if there's any way to improve it will actually improve. It won't get stuck before. So, the important step in the proof for that is actually back one slide when we talked about value non deprovement or what I called value improvement. So I went back a slide so we could look at this together. So, this is the claim that I'm making, Charles. If it's the case that pi 1 is optimal, than the greedy policy pi 2 with respect to the fixed point of pie 1, is going to be the same. We're actually going to be stuck at the fixed point. Because that's the whole thing about a fixed point. But if it's not, if it's not then there has to be at least one state someplace where this value non-deprovement is actually value improvement, like strict value improvement for some state. Like that. I see. So what's really going on here is we should be more precise, maybe, about what value improvement really means, what value non-deprovement means. Okay. We're talking about it. Every time we talk about it, we talk about it in terms of one value function better than another so it's easy to think of it that way. But if that were the case, then I could have sort of cycles, right, where You know I have two states where I keep on each iteration swapping back and forth, which one is better than the other, and you'd still have domination in both cases, right? Does that make sense, what I just said? Wait, no, because domination is point wise, it's state wise. Yes it is statewise, but imagine I had two states, let's call them S1 and S2. And let's say for one particular value function, if I were going to compare, let's say Q1 and Q2 or V1 and V2, going to compare V1 and V2. It turns out that in S1 V1 is greater than it is in V2. So just making up some numbers. Right in the other case they're the same. The same. Yeah. Okay. Then I could apply an operator to both of them. It would also be the case, so here V1 dominates V2, right? Right. So here's another set of numbers I could write down where V1 still dominates V2. Okay. So rewrite the S1 and S2 again underneath. All right, so V1 still dominates V2. Right, so V1 still dominates V2 if I just swap the two rows around. And, so if you weren't thinking carefully about what value improvement or value non-deprovement meant, you would think that you could just go back and forth between these two different values where 10, 6 and 3, 3, those rows sort of swap, because in both cases you're still getting V1 dominating V2. But it's actually the case that the way this value improvement works, is that it's true not just for the overall value functions, it's true at every state. Oh I see, yes it can't be the case that we go from V 1 S 1 being ten, to V 1 S 1 being three, because it's actually non-deproving. Right. In other words, we can't go down, we can stay the same but we can't go down. Right, so the statement has to be the case for all the states, as well as the overall set of states. So once V1 is better than V2 in some state, it basically has to stay that way. Got it. So this keeps us from cycling back and forth. And this is what keeps us from getting locally stuck. That's my claim. Okay. All right so local stuckness doesn't happen. And in particular we get strict value improvement in at least one state if we haven't actually reached the fix point yet, and therefore each time we go through policy iteration it's actually getting better until it can't get better anymore. So one other interesting thing to point out, is that the number of policies, each time that we go through an iteration of policy iteration. We have a new deterministic policy. There's just a finite number of them, therefore; at some point we exhaust the set of possible improvements that we can make. And we have to have found the optimal policy. You know what that reminds me of? Yeah Oh so Okay. Reminds me of the proof we had for YK means how to convert Oh, yes, yes, yes. It's exactly the same argument. When we talked about k-means in the other class, we talked about how there's a finite set of assignments that can be made and each time you do an iteration of k-means, we get either the same assignment or one that has a better score and therefore, we're going to find the one with the best score. Well, we're going to find a one that doesn't change anymore. But the difference between k-means proof and this, is that you could get stuck in a local optimum. In K-means. K-means, but here you can. And I think the big difference is that you basically, it's statement of value non-deprovement is, at every single point, every single state, in this case. Where as with K-means, that wasn't necessarily the case. Okay, super. So do you feel like your grokking policy iteration now? Yeah, in particular I'm grokking that it actually kind of has to work. That's sort of cool. Cool, I think we can sum up then. Let's sum up. I like summing up. So in this section on advanced algorithmic analysis, what have we learned? Well we learned a bunch of stuff. It's a compact and dense lesson. I think that's pretty good. Oh that was a math joke, wasn't it? Yes it was a math joke. Well the first thing we learned is that you like alliteration. [LAUGH] This whole section was advanced algorithmic analysis. Okay. Mm-hm. And in particular we talked about three things, all ending in I. VI, LI and PI. [LAUGH] Yeah there's a problem with LI though. Yeah that it's not really a thing. Yeah, it's LP in your programming. But then it wouldn't end in an I. I agree with that, but at least it ends with P and the other one starts with P. There is some over lap. All of these things overlap with each other. If you call it VLP for very linear programming. No, It's just linear programming. Let's just go with that. Okay. Very linear is still linear. In any case, all right. So what did we learn? We learned a little bit about each of those things. So in particular for value iteration, we learned that it converges to optimal in a finite number of steps, or it gets bound towards that. Okay. Can we be more precise there? The value function doesn't converge in a finite number of steps. But the policy does. The greedy policy does, yeah. Right. But really, that's what we want. Yeah. I agree with that. Sure. But that's a good point. It's not that the value function itself has to, because otherwise we go all the way back to 'em' and have that conversation again. But that the policy itself will stop changing. The greedy policy will stop changing at some point and in finite time. Right. And it will be optimal at that point. Yeah, and so the value function itself doesn't necessarily converge, but it's going to get boundedly close to the solution. So that is to say, if we want to be epsilon close to the solution to the Belmont equation, we can define how many iterations of value iteration we would need to accomplish that degree of approximation. Right. And that's fine. Because in the end that's interesting, but what's really interesting is that we get a policy that we can act on. Because we want to maximize reward. That's just what we are about. That's what we are all about, really. So let's see. Then we talked about linear programming, which is another way to think about solving MVPs. What I remember most about it is you actually define the dual. Okay. That was really cool because it allowed us to solve the problem and like so many things that we do, it was a call back to our first class. And now I actually understand how support vector machines work. [LAUGH] Okay. I think you did that lecture, so I was hoping you understood it already. Well, there was just a point where you say, well or could you solve the dual, and then I rapidly wave my hands and said don't worry about it, it's a dual, it's a thing. Okay. But now we talked about the dual, so I think that that's good. I feel like the concept of dual is somehow relevant to superheroes. You think so? I don't know. You know way more about comic books than I do, but isn't there like every superhero has a dual, like anti-Batman or the dark Spiderman? Venom? So you're saying that primal and duals are like nemesis. Yeah, or mirror images of each other in some way. So the flow, the sort of policy flow idea, is like the nemesis of the value idea, or the mirror image of it. Yeah, sure. Let's go with that. I was thinking more of a hip-hop analogy, but I think that's pretty good. Oh, flow is a hip-hop thing. It's a hip-hop thing. And values. People in hip hop have values. That's right, so it just follows in one step. Okay, speaking in one step. So, then there's policy iteration, which I thought was kind of cool. We got a chance to prove a bunch of things about it. And I think the first thing we talked about there or somewhere in that mix was domination, which turned out to be really cool. Yeah and that comes up actually in all of the algorithms. Value iteration has some of that. Linear programming has some of that. And policy iteration. The proof of policy iteration depends significantly on that. Right. Speaking of things the proof of policy iteration depends on, we also talked about value non deprovement, or value improvement as I believe is the non-technical term for it. That's right and we argued that we'd actually get strict value improvement any time that we didn't have a policy that was already optimal. If it was already optimal then a step of policy iteration resulted in value non deprovement which is just our broken way of saying it doesn't get worse. It might not get better, but it doesn't get worse. Yeah. It's like a monotonically non-decreasing function. But the other thing is, we had a discussion that I think is actually pretty important, that when we talk about things like domination, you writing them down for good notational reasons, talking about an entire value function, but really this is about things have to also be true on a state by state basis. That's right. Because if that weren't true, then in principle, you could cycle, and then you get stuck in local optimum. But here, we don't get stuck in local optimum, because things also dominate on a state by state basis. Cool. Kind of neat. We also talked about monotonicity. And that you can contract it from kissing. Yeah. Oh, you said contract. Very well done, Mike. Oh, yeah. That was pun intentional. Sorry. Pun intentional. [LAUGH] Okay and really when you put all these things together. I think the real thing we learned in this lesson is that proofs do not suck. [LAUGH] So that's great. Because if I recall we started our last class with a little discussion about how I don't mind proofs as much as you do. That's right. Speaking of proof you forgot the s on proofs. See, I did a proof of your proof. That was pretty good. You proof read it? Yes. Yes. Nice. I feel like we have converged. [LAUGH] And that's what this was all about. That's what this is all about. Yeah, there's something very satisfying about getting to the convergence point. Very nice. Very nice. Okay well, I think that's everything that I was capable of learning in one lesson. [LAUGH] Yeah, all right, well so next time we'll start in with something else. I think we're going to talk about how we can modify reward functions and get different things out of them. Oh yeah, that's my favorite. Excellent. Well, then I will see you then. Hey Charles. Hey Michael. We've got a quick little mini-topic to talk about which I'm calling Messing with Rewards. Okay. And so just to give you a sense of what that's about, I mention Ng, Harada, and Russell. They wrote a paper that covers the ground that I'm going to talk about. And then to make it more vivid, I drew a picture of me messing with the reward function. And when I asked Google for a picture for this, this is what it came up with. So that's Debra Messing winning an award. [LAUGH] And that seems absolutely appropriate. because she's got the same look on her face that you had on yours. I know, I thought that was really interesting, I was surprised that Google did such a good job. I think Emma Church is getting better because of machine learning. Probably, probably reinforcing the learning. I hope you gave it a little reward. I forgot to do that. All right I owe it one. So I wanted to start off with question two, which is if we're going to be tinkering with the reward function, why might we want to do that? Why might it make sense to take a reward function for MDP and then turn it into some other reward function? Because it's easier? It's easier to change it than to not change it? No, just we might get a reward function that's easier to solve. This is the whole point which is to solve the MDP, we should just make that really easy. Yeah, so if we can find a way to changing the reward function in a way that makes it easier for the learner to work with. Then without actually changing what it's going to learn as a result. That's a win. Oh, I wasn't going that far. I just figured we'd set the reward to zero everywhere. And then everything would be easy. That would make it easy. But I think it should be easier to solve and similar to what it would have learned otherwise. Okay, I guess that's fair. That seems like less cheating. But it does invite a question, maybe we don't want to get into a philosophical discussion here, but where did the reward function come from in the first place? I mean, I think the way you pose this question sort of suggests that there's a true, original reward function. That's a good question. So, why wouldn't we want to change the reward function? because, we don't have one yet. Right. I think that is a really good point and you're right I'm not going to get into that. But this is a topic that you know that I have a lot interest in because you and I have been meeting on this subject for a year or so trying to figure out. If we think about reward functions as being ways of encouraging reinforcement learners to exhibit certain kinds of behavior. Then they're kind of like a programming language. They're kind of like a way of specifying behavior that then is going to get compiled by the learning algorithm into actual behavior. And so there isn't actually a lot of work on how to do that. If we have some idea of what behavior we want. What reward function would be appropriate to make that happen? So you're right. So in some sense I'm finessing that question and instead staying yeah, somehow we have reward function and we want to tinker with it. Maybe to make it easier to solve. Maybe make it easier to represent. Or, easier to reason about, or any of a number of possible reasons, but without actually undermining what it is that the behavior is supposed to be, that's being specified by that reward function. because, as you said, it's really easy to just turn the reward function into all zeros. And then all policies maximize that reward function. In which case, learning is done. Yeah, and then we solved a problem, just not necessarily the problem we were trying to solve. Yeah, that's right. And that's always an issue in AI, I think, we have to worry about turning the problem into an easier problem that isn't really interesting anymore. Right. So, by the way, something you said made me think about something. Well two completely unrelated things. So, you call the reward function sort of like a programming language. I like that. One nice thing about that is let us think about this easier to solve thing while still being similar to saying well, if you have a program that allows you to sort. Well there are many different programs that allow you to sort and some are actually more efficient than others. And they're more efficient both in terms of the time it takes them to run. More efficient in the space that they use. And they're more efficient in the sense that they're easier for a human being to write or to think about, even though they're ultimately going to exhibit the same behavior, which is sorting a list. So I like that, then it makes the question of why would you want to change a reward function make a little more sense? And that ties in to the second thing I wanted to say, which is you say easier to solve, but you haven't said what that meant. I can think of at least two things it could mean. Easier as in faster, or easier as in might be solvable at all. Like there maybe a reward function that becomes extremely difficult to solve. So okay, so I summarized what you were saying to me as first of all, if we think about reward functions as a certain way. At least gives us an idea about what semantics is. Like what is it that we want a reward function to make the agent do? And at the moment we're going to finesse that and just say, well, whatever it is that the old reward functioned me to do. But then we can also think about changing it in a way that doesn't change the meaning, but does make it more efficient. Either in terms of the amount of data it needs to learn or the computation that it needs to learn. I guess speed really has those two different pieces. Right? One could be how much experience does the agent need? And then how much computation does it take to actually carry out the learning process? And that can be easier or harder. If you don't have to plan ahead super long then it might actually be faster in terms of computation. And other kinds of efficiency include space, sort of the complexity of writing down the reward function. And maybe whether it makes a problem solvable at all. Maybe that the efficiency is the difference between infinity and not-infinity. Yeah, this is a pretty big difference most times, I mean in the limit. So good, all right. So let's think about some ways that we can actually change the reward function without changing what it is optimizing. So how can we change the MDP reward function without changing the optimal policy? So in an MDP, we usually define it with the states, the actions, the rewards, the transitions, and our friend gamma. Mm-hm. [LAUGH] What we're going to mess with here is the R, but we're going to leave everything else the same. So what are some ways that we might be able to change a reward function that actually wouldn't change what the agent would want to do to maximize the reward function. Well there's a couple of I think, obvious ones. So one is if you just take the reward function and multiply it by any positive constant. Everything should be the same. Right? Yes, exactly so. So in some sense the rewards are kind of unit less, so if you multiply them by 1,000, it still encourages the same behavior. It just seems like you're making more reward points, but they're not, it doesn't change the relative strength of different behaviors. So, good, good. That's definitely one. And I want to say this is true. Just like by multiplying by a positive constant doesn't make a difference, it seems to me that if you just shifted everything over by one or by two or by three, it would make a difference of adding a constant to the reward function. Adding a constant as opposed to multiplying a constant. Good, okay, good. We'll talk about that one as well. And there's a third one that we're going to talk about, that may not be so obvious. But do you have any other guesses? This makes me think it desperately looks like what we did in the other machine learning class with clustering. There has to be non-linear ways of messing with the reward function to get you the same answer. But I can't think of a compact way of saying what I'm thinking. Okay, so that's, that's fair. And we'll get into that in more detail. Ultimately, we're going to call it potential based modifications, or potential based rewards. And I'll define in a bit, but first I want to go through the, as you pointed out, the simpler cases and develop a kind of methodology for proving that we're not going to change the optimal policy. Okay. Okay, let's look at multiplying by a positive scalar. So I started out by writing out this equation Q, which you recognize as The usual thing. The usual thing, the Bellman equation, uh-huh. So let's imagine we've got some reward function R and it therefore has a corresponding Q function or solution to the Bellman equation. And this is going to tell us how to behave. If you know the Q function for a given reward function, then for each state you're going to take the action that maximizes the Q value in that state. So this is kind of capturing everything we need to know if we knew the solution here. But now what we're going to do is we're going to generate a different reward function, R prime. So R prime for a given state action pair is going to be defined as the old R, the old reward function for that state action pair, multiplied by a constant. And that constant should be positive. Why should it be not 0? Well, if it's 0 then everything's the same. Yes, if it's 0 then we shouldn't expect any of this to work. If you multiply all the rewards by 0. You lose all information. Yeah, very good. And negative, what do you suppose would happen with a negative? I'm going to say the opposite of what you want, except I'm not sure what opposite means in this case. That's right, it kind of flips reward and punishment so then you end up trying to find the reward function that maximizes pain, which could be an interesting exercise, but I think not what we're trying to do in this case. Yeah, it's just like grad school. [LAUGH] I don't think the sign is flipped for grad school. I think it's just- Oh, I'm pretty sure the sign is flipped for grad school. [SOUND] All right, so here's what I would like you to do. I would like you to make a guess as to if Q is the solution to this original Bellman equation, and R prime is our new reward function as a function of the old reward function. How can we write the new Q function, corresponding to this R prime, as a function of the solution to the old Bellman equation with this Q? So I want Q'(s,a) in terms of Q(s,a). And that's going to give us some insight as to whether or not we have a monotonic transform, that the action that was best before is still best now. So this one is intended to be kind of a warmup. So you might want to go just with your intuition on this. So what would you think the value of Q prime (s,a) is as a function of Q(s,a). Keep in mind that we scaled all the rewards up. Think of it like by a factor of a thousand. My intuition about that is actually pretty straight forward. What I want it to be is just I want Q prime of s,a just to be c times Q of s,a. In other words, if we double all of the rewards, it should double all of the Q values. If we halve all of the rewards, it should halve all of the Q values. That sort of thing. Right and that makes sense at least to me conceptually if nothing else. Reward is your reward and the Q value is the true reward you get. The summation of all of the rewards you're going to get. So, if I double the rewards, then I should double the value. Yeah that's good. So that is the right answer. And I just want to go through a quickie derivation to show why that's the case. Alright so here's how I like to think about this is what we want to show is that our Bellman equation for Q prime and R prime is correct, that it solves and we get a solution to the Bellman equation and since the bellman equation is unique, the solution to the bellman equation is unique, then we must have found the right solution. So what I did here was I took you know imagine the Bellman equation for q prime. It's like q prime s a equals r prime s a plus gamma q prime. So what I'm going to substitute in here is our guess as to what Q prime is so that is here and here. And our definition as to what R prime is, which is that. Okay so if this is true, then we have a solution to the Bellman equation. Right and it's derivitively true. Right. And so why is this true. Right because the C here we can pull it out of the max. That doesn't, again because C is positive, it doesn't influence the max. If c were 0 or negative it would mess up the max. But we can factor it out of the max because it's positive so it doesn't change the order. Of what's largest. It's going to then get multiplied by these C's, and it's inside the sum, but it's common to everything in the sum, so that gets pulled out there, so we actually get to move this C all the way out to here, and now, notice that we can factor it out, well, it's obvious now, I guess, right? We just factor the C out from both sides, and they cancel. So what we are left with is this equation, which is the Bellman equation that we had already solved. So this has to be true, so the original thing has to be true. Right. That makes sense. Yeah, which is probably the way you think about it. Yeah, in fact this is exactly the way I would think about it. Awesome! All right so now that we have this down we should be able to do these other modifications. The other messing around with the reward function next. All right, so the next thing we said we would do is consider adding a scalar. So again, I wrote the Bellman Equation for sum reward function R. So this is the target behavior if we use R as our reward function. And now we're going to replace R with a new reward function, R prime, which is the old reward function plus this constant C. So now on every step, we get an additional C reward that we wouldn't have in the original thing. So what does that do to our Q function? What's our new Q function as a function of the old Q function and this value C? It'll be different from Q(s,a). Are you going to tell me the answer? I will, but only after we do this as a quiz. Okay. Fair enough. All right so, it's just like what we did before except now we're adding a constant. So I suspect we ought to be able to use similar reasoning to get to the right answer. So can you think this through? I think I can think this through. So when we did the last one, you did a kind of derivation. I didn't really talk about how I came up with the answer that I came up with. But I did it basically just by substituting a few things in my head and seeing if it worked out. So when we did the multiplying one I said, well, all you're doing is multiplying R of sa times c. And since the sum of all those T of s of a, s prime times the max of a primes of those Q's, is really can be unrolled as a series of rewards. That means all you're doing is multiplying all of those rewards by this constant C, and so I could take it out and that's how I ended up c times Q of s of a. I think there's a similar argument here. See if this argument makes sense to you. If we took the equation up top and we said, what's Q prime s of a, equal to? Well, it's equal to R of s, a plus c. Right. All we're doing is adding a c. And then inside that sum, if we unroll the Q's it's just going to be a series of all the rewards that we see. Each time adding up a c. And so we can sort of separate out the adding of the c each time. And the problem with that, is that each time it's being multiplied by gamma. So what we're seeing is a bunch of Cs added over and over and over again. I mean, an infinite number of times, potentially. But each time, multiplied by gamma. Then gamma squared, then gamma cubed. But that's okay, because we actually have a formula for that. Good. It's the geometric series. Yes. And that's just 1 over 1 minus gamma. The sum of the gammas is that. And then you multiply by c. There we go. All right, so you're saying that the new Q function should be essentially the same as the old Q function, which is already kind of gathered up all the rewards, the R rewards. But we need to modify it by this infinite sequence of Cs. Which is C over one minus gamma. That is what I'm saying. Awesome. Is that right? Yes. So, let's do the proof. So again, the trick is going to to be, we are going to to write out the Bellman Equation for Q prime, substituting in, our guess for Q prime and our definition of R prime and then see if it works. All right, this is the thing that we need to be true. If the rest of it is true. That is to say, if it's the case that the new Q function is just the old Q function, then shifted by this c over one minus gamma. Then we need this equation to be true. So this is true, if and only if what? Well we have this c over 1 minus gamma, inside the max, getting added to this Q, s prime, a prime. How does that influence the max? It doesn't effect it at all as long as c is positive. Even if c is negative, actually. Oh you're right. Because it doesn't depend on a, at all. It's a constant. It's a constant so, we're adding this constant to all the different pieces of the max. So then the actual max we get into the end of that is going to be whatever the max was before, plus the same constant. So we come out of the max, but we're still inside the sum with the transition probabilities. So what happens to the transition probabilities here, is this gets multiplied by this, just before. Plus, it's also getting multiplied by this, but this is a constant, it doesn't depend on this prime or anything like that. And the sum that we're taking is a bunch of coefficients and all of that up to one. So really, this whole piece here is going to get promoted all the way out of the parenthesis. With the only difference that it's going to get multiplied by a gamma. So, what we really got here is a c plus c gamma over one minus gamma on this side, and a c over one minus gamma on this side. So in fact, here, let's do this, as a step, this is true if, and only if, this is true, which I got by subtracting from both sides the solution to the original Bellman Equation. Right. Right. We've got this Q, we got the R, we got this thing here. And all that's left are these c things. So is this true? Yeah. It is. And why is that? because you multiply everything times one minus gamma. Oh, good idea. [LAUGH] So this is true if, and only if, this is true, where we've multiplied through by 1 minus gamma. And this is true. Right, because? Because we've got this c on both sides, and we've got c minus gamma and a c plus gamma. No, sorry, a negative c gamma and a positive c gamma that cancel. Right, so c=c. Whew, that feels good. We solved everything. Not everything. So we handled the two relatively easy cases, though that second one I admit was a little bit more of a workout. The next one is going to be conceptually trickier. But the algebra's not going to be much worse and this is going to be in the context of reward shaping. So, have you heard this phrase Reward Shaping, before? It comes up often when people are talking about, wow, how did they train that dolphin to jump through a hoop? So it's tricky to imagine how you might have gotten a dolphin to jump through a flaming hoop, out of the water and then splashing down on the other side. Because it's not the kind of thing if you were to train it. Well you can't tell it what to do. Because they tend not to listen unless you go [NOISE]. And you can't reward it just for having done the right thing. Because if you've had this flaming hoop up in the air and you just wait for the dolphin to jump through it and you give it a treat. Plus one if it does it and otherwise you don't give it a treat it's never going to figure that out. It's never going to inadvertently hop through that hoop and say, hey, do I get a reward? So, how do you suppose they actually train dolphins to do this? I have no idea, but I'm going to guess about how they do this. Because I'm not as into dolphins as you apparently are. What I assume that they do is they have to coax the dolphin to jump through the hoop in the first place, by making it worth his or her while to do. So you kind of reward it maybe for jumping first and then getting near the hoop without the fire, with the fire. You just keep giving it kind of rewards for getting close enough until it rather than inadvertently jumping through the hoop, decides that that must be the right thing to do now. Yeah, I think that's [LAUGH] some approximation of what I imagine that they'd do. My understanding is that they'll start off with something like you put the hoop in the water. And you put a fish next to the hoop, right? So that the dolphin swims over and eats the fish. And then you start saying to the dolphin, well, you start changing when you pay off the dolphin. You don't give it fish for just touching the hoop, or for being near the hoop. You give it a fish for actually touching it with its nose. And then you lift it out of the water a little bit, and then it has to swim faster to get up to it. And then you lift it out of the water a little more and it has to jump all the way through. And then, yeah once it's all the way at the top eventually you can light on the fire I guess. I actually don't think I ever seen a dolphin jump through a flaming hoop. So we might want to extinguish some of that. But that's not really the point. The point is that it's this series of mini rewards along the route to the big payoff that helps provide the right kind of hints. That's cool. I'm going to say life. Like you don't have to just say grad school. Like most of the things we do are like this. We don't wait to get the big reward after we've inadvertently done the desired behavior. We get hints along the way. You've clearly never met any of my students. But okay. Since we don't have any robotic dolphins that we could be doing testing and training on, let's think through an example of doing reinforcement learning to try to teach a robot soccer player to score a goal. So, this is supposed to be kind of a robot soccer pitch. This is the goal in blue. This is the ball in yellow. And this is the robot in black. And what we really want it to do is score in the goal. And that's worth a hundred. Again, just like in the dolphin case, it's very unlikely that the robot is going to spontaneously drive around, drive around, hit the ball into the goal, experience the 100 and then say, okay, cool, that's what I'm supposed to be doing. So, what are some things we can do shaping-wise? How can we modify this reward function so that it might encourage something in the direction of scoring the goal? I think, there are a couple of things one might do. I'll try not to over-think this. Well, in order to kick the ball, the circle with its circle feet, it has to get near the ball, so I could imagine putting some reward for being near the ball, wherever the ball is. Cool. So, you think like plus one is enough for that or maybe? I don't know, but let's start with plus one. All right. How do we define near the ball? There's going to be some kind of epsilon ball around the ball? [LAUGH] Yeah, that makes sense. Or, you could go all the way around and think about the distance you are to the ball is inversely proportional to the reward that you get. Good point. All right, let's look at your way of thinking about it first. So, if we're getting a reward, the agent is getting a reward by virtue of being close to the ball. What's a good policy? A good policy might be to just sit in place or sit a little bit closer to the ball and just rack up the points. Sure, so then you'd need something else, like hitting the ball is worth some points. Aha, okay, that seems plausible, right. So, if the robot is getting additional points, additional pleasure for getting closer to the ball, and then actually hitting the ball, yay, that was really awesome. But, of course, by hitting the ball it pushes the ball away which is going to cause the near to the ball score to go down. So, it's going actually kind of be sad that it did that. Sure. But then, you would want to give it something for getting the ball near to the goal. So, just like being near to the ball is worth something proportional, inversely proportional distance, you want the distance of the ball to the goal to also be inversely proportionally related to distance. Yeah, that makes some sense! So, now, we want to be careful that the amount of bonus that the robot gets for pushing the ball toward the goal is going to be an improvement over the fact that it's now no longer itself near the ball. Right, so I don't know. Two times, or something. I don't know. I mean, I don't know exactly what the right numbers are, but there you go, something like that. So, it seems like we could play with these numbers for a long time trying to get the behavior that we want. And there are a couple of really interesting failure modes. So, for example, if we have hitting the ball being defined as making contact between the player and the ball. And you get, let's say +10 for that each time, ignoring these other factors for a moment. Then, once thing [LAUGH] that you often see when you ask reinforcement learners to learn in that kind of setting, as you'll see, robot walks over to the ball, touches the ball, gets its plus ten. And it turns out, if you want to get that plus ten again, the best thing to do is just back off from the ball a little bit and then make contact with it again and in fact really what you want to do is go up to the ball, and just shiver. Right, just like vibrate back and forth, [SOUND] so that you just keep getting plus tens over and over and over again. You can make a lot of money that way. A lot more than you can make by scoring the goal. Because you can do this, you know 30 or 40 times a second if you're a good wiggler, and so this ends up actually even though this additional shaping information that we're defining is giving good hints, it's also creating spurious optimal policies. Right. Well, it makes sense, right. People have been making money for thousands of years by wiggling. [LAUGH] I guess that's sort of true. But in this particular case, what we have to do is somehow make it so that you can't have a money pump. You can't do arbitrage. You can't, by doing the good thing, undoing it and then doing it again, get double credit. I see. So, a natural idea is to actually say, all right, if you're going to get plus ten for hitting the ball, then letting go of the ball should be a minus ten. Right? You should not be able to make a profit by repeatedly bumping into the ball. Does that make some sense? It does. My favorite part about that is how you called it an actual idea which, as opposed, makes what I said not an actual idea. But yeah, okay. Oh, I did not mean to imply that or even- That's fine. Say it. It's fine. It's fine. It's fine. I see what kind of rewards shaping your doing. It's okay. [LAUGH] In any case what I guess is happening, so that makes sense. So, you don't want to get into a place where you can get into a suboptimal positive loop. So, you're going to say, getting to the ball is good. Getting away from the ball is not good. So, you're going to learn to get to the ball and have to do something else, that will get you even more reward, make it worthwhile backing away from the ball. Right, good. And I like the thing that you said about not wanting to create a suboptimal loop. A suboptimal positive loop. [SOUND] Because a suboptimal positive loop is actually going to cause this spurious behavior to happen. If there is a way that you can do a sequence of actions and generate profit where there really shouldn't have been in the environment, then it's distracting, right. The agent is going to end up doing things that you didn't wanted to do. So, we have to avoid creating these suboptimal positive loops. And so a natural idea from, I'm going to say physics. And you can tell I'm an expert in physics, because I understand Bellman's equation, Einstein's Bellman equation. That is that the idea of potential functions. What I am going to do is define potential functions and then show how we can use them to do shaping without actually messing up the optimal policy. I look forward to it. So the idea of Potential-based Shaping is going to be this. Instead of giving little bonuses for good things happening, what we're actually going to do is put bonuses on states of the world. Then when you achieve a certain state of the world you get that bonus. And if you unachieve that state of the world you lose that bonus. So everything kind of balances out nicely. So we'll write down an equation for that in a moment but let's see if we can think about what state based rewards might look like in the context of this little soccer example we were talking about. Okay. So the official reward function has us getting a +100 for scoring a goal but we wanted to give hints. We wanted to give hints to the system for getting near the ball, hitting the ball, getting the ball near the goal. So we need to do that, we need to define that now in terms of the states of the world. So what would be some appropriate rewards that we could put in for that? Well, I mean, I think they would be very similar to the ones that we were talking about before, right? If you're talking about states, you want to be in the state of the ball moving towards the goal, or the ball being near the goal, or you moving towards the ball, or kicking the ball. Those are all states you would like to be in, right? Yeah, okay. All right, so yeah, and the way that we were writing that before, I think, was something like 1 over the distance between us and the ball. And 1 over the distance between the ball and the goal. But now we think of these as being bonuses that are tied to the state. Not values that we're getting for becoming close to the ball. So there's always this value, this potential that's keeping track of how close we are to the ball. And when we're close to the ball, it's a big number. And when we're far from the ball it's a small number. And the reward that we're going to get, is we change from state to state, how much more potential have we've gotten? So to make this all work out, what we're going to do is introduce state-based bonuses, the sort of idea that instead of just giving little bonus rewards every time that you do a certain thing, instead we're going to keep track of what the state of the world is. And as we move to states of the world that are more desirable, we're going to get reward for that. Which I guess is not that different from what we had before. But then we're going to have to subtract that off when we move away from those states. So the basic idea is we can still keep track of things like how near are we to the ball, and we can have a score that's associated with the inverse distance. But now the idea is that the rewards that we get for taking an action are going to be the increment that we get in this value instead of actually this value. Right, okay, so that's important, right? So that means if I went from being, let's say, 10 pixels away from the ball to 5 pixels away from the ball, I get an increment of, let's say, +5 or one over the distance somehow. But for simplicity's sake, let's say +5. But if I went from- Well, wait, these are numbers that I actually know how to do. So this is like 0.1 and this is like 0.2, so we'd get a bonus of +0.1. Okay, good, good, I like that. All right, but that would mean, then, if we went from 5 pixels to 10 pixels, we would end up losing 0.1. Right, and this kind of keeps the account balance in a good shape, right, because it's saying that, sure, we'll give you a little bonus for doing good things, but you can't just continue to get that over and over again. If you relinquish it, we actually have to take those same points away, so that we can give them back to you if you re-accomplish it. Would it be more accurate to say it's not so much state-based bonuses, which I think you could argue is sort of what we were doing before, but it's sort of changes-in-state-based bonuses? Ohh, good, yeah, I like that. So change-in-state-based bonuses. Yeah, I like that. All right, so we can continue to fuss with this, but I think it might be easier, believe it or not, to not get bogged down in details but actually talk about this at a more abstract level. Well, before you do that can I ask a question? Sure. So I did make the claim that we were sort of doing state-based bonuses before, we just weren't doing the change in state. But I'm going to claim that of the four things you have written up there, score in goal, near the ball, hitting the ball and ball near goal, is hitting the ball count as a state-based bonus or not? It probably would want to be like touching the ball, something that is actually something that's measurable in the state. Hitting is a set of actions or is a sequence of actions. Right, okay. So would that just be captured by near the ball? Yes. So okay, that's fair. Let's write down what the formal definition of these bonuses are, and then we can think again, if we want, about how we might define it for a particular problem. But let's look at the general form first. Okay. All right. Here's how we're going to define our modification of the reward function. In this case of what we're going to call potential base shaping. The potential is going to be defined by this function, Si is it? Let's go with Si. All right. Here's the idea. Once again, here's our Q function defined in terms of a reward function. And I did mess with this a little bit because I now made rewards dependent on the state that we end up in as well. because that's going to be helpful for keeping track of how rewards are changing. Wait, does that change anything? That doesn't change anything, right? Does it? It doesn't change anything in this Q function, no. So it's the reward plus the discounted value of the next state. And we average that over all possible next states weighted by the transition probability of ending up in that next state. So that's all the same. If we actually have a reward function that's not defined in terms of S prime, that's fine. This is just helpful for the notation. It doesn't actually change anything if we ignore S prime, the definition of R(s,a) Oh, and in fact, if R(s,a) Ddoesn't depend of S' then you're just doing a convex sum over R(s,a) so you want to put the R(s,a) so you'll get back to where you were before. Exactly right. So that's right the reward function pops back out which is sort of the opposite of what we just did we shoved it in there. Okay. Good all right and so now. Our new reward function, R prime, that we're going to use is going to be the old reward function, first state-action pair, minus the potential of the state that we're leaving, plus the discounted potential of the state that we're entering. So this difference here between the potential of the state that we end up in and the potential of the state that we left is the bonus that we get, the amount that we're getting warmer, we're getting closer to good. Okay, so looking at it. I see, I see, let me make sure I have the intuition here. Let's ignore the minus side of S and just think about the plus gamma syathus prime. You're saying that every time we enter into some state, s prime, we're going to get that little bonus, which is what we've been talking about before. But, now I gotta account for the minus side of this. Well, the minus side of this is just a way of saying yeah, but whenever I leave a state I have to lose that bonus, and that's your way of avoiding those kind of sub-optimal positive loops. Yeah, exactly right. It's sort of like, I don't know, good bookkeeping or accounting. We have to make sure that, sure we made a transition from state s to s prime. We get this advantage of being in s prime but we have to have lost the advantage of having been in s cause we're not there anymore. Okay, so this makes sense. Great, okay. So what we need to do now is figure out what does the new Q function look like when we've made these revisions to the rewards as a function of the old Q function? Okay. Oh, I think I know the answer. All right, so do you think you're ready for a quiz or do you need any additional explanation of what I'm asking? No, I think I know. We know either Q of SA is we're going to change the way the reward function works to have these potentials, and so that's going to change what Q of SA is. It's going to be now called Q prime of SA, and you want to know if there's some nice way to describe Q prime of SA I guess in terms of the old q of s of a. That's right, that's right and in fact there is a nice way of explaining it. Or it's not of explaining it but of defining Q prime in terms of Q. Okay I think I know the answer, we'll see. So you want to help us through what this could be? Yeah so I think I know the answer because I think I know how to get the answer because I think it's like other times we've done this sort of trick. And the key part is noticing that we are adding and then subtracting off the things that we just added. And then adding the thing we just got and then subtracting that off and so every time we go through a state action reward, state action reward, we're going to end up adding a potential and then subtracting the same potential on the next step. So here's a useful way to visualize that. So we're going to go through some sequence of states, S, S prime, S double prime, S triple prime,S quadruple prime, and so on. And in this Q version of the Q function, we're getting the sum of the rewards for those states. So we have like a reward that happens here, and a reward that happens here and a reward that happens here, but these are going to be discounted and summed up to get our Q. And now we're wondering well what happens if we replace this R with this new R prime. It still has R in it but now it's going to have this take away psi, give me some gamma psi prime. Give me some psi, give me some psi. Yeah exactly. High, low, on the side. All right. So, this first reward in addition to the R part we're also going to get the minus side and the plus gamma side, so let's just add that in here for the transition from S to S prime, but we'll also have to add it in there for the transition from S prime to S double prime. So, the second reward Is gamma times whatever that reward is, which is what it used to be, minus the psi S prime plus the gamma psi S double prime, and so on for each of these rewards, is going to get replaced by what it was plus the discounted psi minus the psi. Right. So if we look at that, so good, so writing that down confirmed my thought so. First thing you notice is that things start cancelling each other out. Right. So in particular this gamma psi S prime and this gamma times negative psi S prime disappear. Right and then that's going to happen again. It's going to happen again on the double prime. Right. [SOUND] [SOUND] And the triple prime [SOUND] [SOUND], and the quadruple prime, [SOUND]. And so forth, and then all we're going to be left with are the R's themselves, which we are writing as Q. And this weird little extra- psi(s) in the very beginning. Right, so then I think that we can write Q'(s,a) = Q(s,a)-Psi(s). Nice. And there's only one little bit of bookkeeping we have to argue our way out of. And that's the fact that we are getting rid of everything in the middle except the things at the beginning and the things at the very end, the thing at the very, very end. Right, so there's some kind of gamma to the infinity psi of S infinity kind of thing at the end. Right. But, if gamma's less than one, then something less than one to the infinity is just 0. Right, as long as our psi guess is bounded, which we didn't say, but it's true. It better be bounded, so then it's 0, so we can ignore it [SOUND]. And that's cool, because that's what always happens whenever you have a telescoping sum. Yes. Even though this was an infinite telescoping sum. Very cool. Does this make some intuitive sense at the end of the day? Are New Q function is just like our old Q function except it shifted downward by psi s. Like this downward kind of bothers me a little bit. Well, it's bothering me too, but actually I think it doesn't bother me now. So what your saying is the true value of being in a state and taking an action is actually whatever the value is minus the potential you're going to lose. Because you need to account for everything. You need to account for not just the reward you get for being in a state and all the discounted reward you're going to get thereafter, but for any potential that you're going to lose along the way. So psi is that potential that you're going to lose by leaving this state. So Q basically captures what you're getting for being in a state and then what you're going to get from that point on. Right? Assuming you behave in a particular way. Well what's going to happen from this point on? Well you're going to get the reward that you got in the state, like before. But then you're going to lose whatever magic psi we gave you for entering into the state. So we have to account for not just the present, but also the future. Right. So the future requires getting rid of the potential, so I think that's fine. It kind of makes sense to me, I kind of wish it weren't that way but it does makes sense if the reward is actually, you're going to get a little bit of extra bump in bonus. Another way of thinking about it is you got R' (s,a,s') = R(s,s)- psi(s) + gamma psi(S'). Well if you were to write down what R'(s,a) was, it would just be R(s,a)- psi(S) if you were to ignore the S prime part. because the S prime part only matters with the discounted future. So then Q prime is just the rewards that you would get, minus the potential that you're about to lose. So I think it works out just fine. It makes I guess so. I guess I just wondered, why wouldn't it be plus? [LAUGH] Why wouldn't it be the case that the Q value for being in a state is what it used to be? But we got this extra bonus for having gotten to s in the first place. Because we're going to lose the bonus. We're going to lose the bonus, and so we have to pre-lose it? Well I don't think you're really pre-losing it. I mean what's really kind of happening I think is that you've kind of got a new R of s of a. And that R is including the psi, and so you've gotta subtract it off to get back the original R you had. All right, so let's just, just to be certain that we're comfortable with this, maybe we should do the algebra to make sure that this all really does balance out. All right so I just written out our equation for Q' and substituted in the definitions that we think are true. What the new Q' is in terms of the old Q and what the new rewards are in terms of the old rewards. And so if this works. If we can do a bunch of if and only ifs that actually bring us to something that we know is true then we should be good. Okay. So All right how can we, we do have an equation for Q. We don't have an equation for Q minus psi of s. But maybe we can cancel some things? Oh, I can see two things we can cancel. All right. Okay, so the first psi of s on the left side of the equals sign? Yep. That matches up with the first psi of s right next to r. Yes, right there. Yeah, because we're summing this over all next states, all next S'. And this sums up to 1, it's a convex combination. Right, so that means, effectively you could take it out. It doesn't depend on S' anyway. And they go away. Good, so now we got Q(s,a) on one side. And then you can actually handle the next one. If you look at the next psi of S'. The gamma of psi S'. Well that matches with the minus gamma of S' inside the max because that max is multiplied by a gamma. Good. Right. Right, right, right. Yeah, so it's interesting. And the max itself is not impacted by this quantity because it doesn't depend on a prime. It is in fact a constant. So we can actually take out this constant. And it actually cancels this one. And look what we're left with. The original equation. Ta da. We're left with the original Q equation, which we know is true and so this is true if and only if the original thing is true. This is an actual solution to the Q equation. The Bellman equation and so, right, this is what happens to the q function. It's the old q function shifted down by the potential function. Right. And at no point do we divide by zero, so everything is great. Right. And how do we know that we still have the same optimal policy? In which case? So we know we have some optimal policy for this Q function. This new Q function Q prime has an optimal policy. How do we know it's the same? Well because it's just the original Q function and you're just being shifted around by a constant and so it doesn't change the max and therefore it doesn't change the optimal policy. Exactly. Awesome. All right, so this now gives us three total different ways of messing around with rewards to give us new q functions that don't actually change the policy. That's really nice. So, I have a question. Sure. You didn't write anything down here about any restrictions on the potential function. That's right. Does there have to be any restrictions on the potential functions? Maybe potential restrictions potentially? [LAUGH] I'm going to say no because it is all going to end up being okay and cancelling out. So, there is a sense that you've actually not changed anything by throwing in one of these additional potential functions. Maybe it can help us learn faster? Here, lets look at an example where this might matter. Okay. So it's helpful to think about how we would use this potential function idea in the context of Q learning, instead of just solving Bellman equations like we had a moment ago. So in the setting of Q learning, we're going to be keeping some Q function, Q(s,a). And each time we get a state action reward next date quadruple. We're going to update our state action pair for this state action pair we just left to be the, you know, little bit more in the direction of the reward plus the discounted value of the state we end up in. And this reward, now we're going to use the real reward, but now we're going to shift it around, mess it around with this potential function. So we're going to take the actual reward, subtract the potential for the state that we're leaving and add the discounted potential for the state that we're arriving in. And so this is going to be how we do our updates. And so if we run this version of Q learning, what should happen in the limit? The right thing. Yeah. So it should actually solve the original MDP, the one with just R here. And it should ultimately ignore the size. So there's a sense in which we don't really care what the size are. So what if we choose like this one? What if we say psi of s is the max over all actions of the optimal Q function Q*, for that state action pair. You mean the actual true value of being in the state? Yeah. Oh, well first off, if we choose psi to be that then we're already done. Well so, yes and no, right? We only can do this if we've already solved the problem. So admittedly this is an unrealistic thing to be able to do. But what does it do to Q? What does Q end up being in this case? Zero. Is that obvious? QSA Is going to be Q*(S,a)- psi(S), right? That's what we figured out on the last slide. And this psi(S) is exactly -max a Q* (S,a). Which is Q* (S,a). Not exactly, right. So Q(s,a), for the optimal action, is zero. Oh, you're right, I'm sorry. What I just said is true for when you take the optimal action. It is zero. Right. So Q(S, a) in this case is going to be 0 is a is optimal. And it'll be less than 0 otherwise by a consonant for each action otherwise. Right, by the amount that it's suboptimal. Yes, that's exactly right, by the amount that it's suboptimal. So this is kind of cool. So if you think about initializing your Q function to 0, which we often do, because what do we know. Then it should start off actually doing the right thing. It should maybe take some of the non-optimal actions, discover that they have a value less than 0, and then after that just take the best ones. So this give you an incredible leg up on the learning problem. One other thing I want to point out here. This is kind of cool and this is maybe why potential functions are helpful and why it can help speed up learning. Eric Rewaref showed that the series of updates that you get by running Q learning with some potential function is actually exactly the same. As what you get by running Q learning without a potential function, but with the Q-function initialized to what the potential function would have been. So you get the same, in the sense that, you get the same answer or you get the same in the sense that, it takes the same amount of time? It goes through the same series of action choices. So it's going to do the same things forever. But yeah, the actual q functions that you get are different in the two cases. Okay, well I meant the same thing in terms of policy, which I guess had to be true. Yeah, but not only does it converge to the same policy which we knew was true, but it's actually going to have the same policy at every step door in learning. Right. So that makes sense, okay cool. Oh sure, because psi is just a constant, really, with respect to any given state. Right, that's right, which is having an impact, this shift impact on what the Q values are converging toward, yeah. So that's kind of neat. Yeah, it's neat, and it's a cute proof technique as well. So, I guess, one way of interpreting this is yeah you know, potentials they're really cool. It's just like initialization. Another way to interpret this is, who needs potentials? You can always get the same effect by Q function initialization. Well, I think, it says something else too. It says that whenever you randomly initialize your Q function. You are, in fact, asserting a particular potential which just cannot be the right thing to do. Mm-hm. Wait, so what do you mean by that? Well, this argues that random, because sometimes, what are you going to do? You gotta start out somewhere. So you just randomly initialize Q functions. You put random values in every Q function. But that's like saying that, since that's equivalent to having a potential function, you're saying that where you randomly choose to put high values versus low values. And you're saying those are places you ought to be, or places you ought to start out trying to be anyway. So, that would argue against random initialization. Because it matters, and so if it matters, why be random? I mean, one reason to be random even though it matters, is because you don't know [LAUGH] If you always say give it 0, or if you always give it plus 1000 or something like that, you're biasing it as well in a very consistent way that's easy to fake out. Right? It's easy to make up an example MVP where that's going to be the wrong thing. Whereas, if it's random, you know, maybe it'll work. My guess is though that the you know, if you integrated over all the ones that were wrong versus all the ones that were right for any given random thing, for any given fixed MVP, if you didn't know what it was, that number's going to be big. But I think it argues for you shouldn't be randomly initializing Q functions. All right, that's not an unreasonable thing to say. I mean, because basically, you're injecting knowledge. And you should not inject noise. [LAUGH] You should inject knowledge. Right, actually that's exact the way we put it. Knowledge and noise don't both start within. Mm. Only noise does. [LAUGH] That feels like a saying. Knowledge and noise both sound the same, but knowledge starts with k, like okay. And noise starts with no, like N-O. Oh, I did not know that. Which is the beginning of knowledge. Okay, all right. I fear that we are off track. So, the punchline of this was, that well, your punchline was, you don't want to randomly initialize your Q functions. My punchline was just that you can actually inject various kinds of knowledge in here, and it could have an impact on learning time. This doesn't actually show that it has an impact, but empirically it often does. There are some results that actually show that if you initialize your Q function optimistically, but not too optimistically. You actually bound what the learning kind is going to be in terms of that. But that's kind of a story for another time. I think, at this point, we've said what we're going to say about messing with rewards. So, maybe we should do a what have we learned. Okay, I'm a big fan of that. All right Charles, so what have we learned? We have learned that unlike Texas, you can mess with reward functions. And what does that mean? Well, it means that you can take the rewards that you're given and you can modify them to still get the same policy but maybe to learn it more quickly. You can shape them. And thus shape the experiences of your agent. And I think we talked about three different ways that we can do that. Do you remember what they were? I remember one of them, potential functions. Sure, that was the last one. So, also you can just do a shift. You can just add a constant, and it doesn't matter what that constant is, so long as it's a constant. And you can scale them, so but you want to scale positively, so it does matter what that constant is. Otherwise we can swap max and min by accident. RIght. But I think the difference between the third one and the first two is that the first two sort of don't change anything at all. They're just a kind of simple linear changing of the function. But the potential functions are set up to help you learn faster, not just learn the same thing. So have we learned anything else? Well, I think we learned something, or at least we, there was a subtle thing we should have learned, which is, well, potential functions can help speed things up. But we got the potential functions by just talking about shaping rewards, by just trying to change them in a way that captured the parts of the space that were better than others. This is kind of pre-potential functions. So I would say we learned about reward shaping in general, and that we learned that you can do it really wrong. And what happens when it goes wrong? Well when it goes wrong, you end up in these kind of sub-optimal positive feedback loops. And, in fact, the answer to that was potential functions and that keeps us from getting into these loops because we subtract off the things that we gained. Sure, okay, I'll buy that. Okay, so that's good. So we learned about reward shaping and all of its various forms and learned that it can hurt you and that potential functions are a way to avoid that. And potential functions are kind of magical. They do some really clever things. It's probably good if on the homework we look at some ways of building potential functions. Which things tend to speed up more than other things. So some nice empirical questions there. And we should probably dive into that. Yeah, I like that because I think one could walk away from this thinking well, potential functions are just magical, they work, and everything's okay. Except clearly then there must be better potential functions than others. Must be some that have no real impact on learning, well, have no impact on the speed of learning. In some sense, all we showed is that potential functions don't prevent you from finding the right answer, but presumably, you can come up with a potential function that actually hurts. Yes. Right. It could lengthen the amount of time before good behavior is learned. Right. Okay, cool. Luckily we have homework for that and the students can figure that out on their own. That seems great. [LAUGH] All right, cool. Well thanks so much for going through this with me. No, no, no. Thank you Michael. I enjoyed every moment of it. Hey, Charles. Hey, Michael. We're going to talk about exploration. Alright, I like exploration. Yeah, it turns out to be a really important property, or topic in reinforcement learning. I think there are some people who have said exploration is the topic that separates reinforcement learning from other kinds of machine learning. So it's kind the thing that's special to us. Yeah, I think I would argue it's part of a fundamental trade off of reinforcement learning. Okay, I'll tell you who said it. It was me. [LAUGH] Fair enough, Mike. Let's dive in and see what some of the subtopics are. So ultimately, we're going to work our way up to regular old stochastic MDPs and what it means to do exploration and learning in those settings. But by way of warm up, we're going to look at two other classes that illustrate other aspects of it. We're going to talk about bandits that don't have any state transitions at all. Okay. And we're going to talk about deterministic MDPs which have state transitions but don't have any stochasticity at all. Okay. But the bandits do have stochasticity? Bandits are all about the stochasticity, yeah. Okay. And so once we've kind of mastered those two, it's kind of combining the union of those that gives us the general ability to handle general MDPs. Combining the union? Taking the union of the topics and then integrating them into an algorithmic whole. Hm, sounds promising. So are you familiar with K-armed Bandits. Let's pretend I'm not. All right, so let me tell you a little bit about K-armed Bandits then. So here's the basic idea. You have K bandits and each of them has an arm. So wait, why isn't it K armed-bandits then? Why isn't the dash between armed and bandits? Oh, K armed-bandits. Yeah, because each one is an armed bandit and there's K of them. Right. Yeah. That works, but I think we're supposed to think of it, the important thing is that there's K different arms. [LAUGH] Okay. And so the bandit, this comes from terminology having to do with slot machines. They named this model back in the I'm going to say '50s. So, we don't have to necessarily understand it anymore. That was ancient history. But the fact of the matter is sometimes people will call a slot machine a one armed bandit. In a slot machine, you get to make a choice at each moment in time, which of the one arms to pull. Which, of course, is not a very interesting decision problem. So, the idea of a K-armed Bandit is, you've actually got, in this case, what K is 7? And each of these arms has some probability of paying off. So, you pull on it and you either get jackpot, or you don't get a jackpot. Now usually in a casino these would be rather low payoffs. In fact, I think it's not so uncommon in the casino for them to have not even necessarily different payoffs. But here for this, it's going to be interesting to have each of the arms have a different payoff. Or at least we don't know what the payoffs are, they could be different. Okay. So which arm do we want to pull? The best one? Yeah, which one is that? The one that has the best chance of giving you money. Here, I'll make it easier for you. Okay. I'm going to give each of them a name. All right, now which one do you pull? The letter that has the most money associated with it. Okay, all right, so I think you're making a valid point, though you're making it in a frustrating way. And that is that we don't know which arm to pull because we don't know what the payoffs are. So if I tell you the payoffs this problem becomes really easy right? Yeah. We just do argmax right? Whichever one has the highest payoff, we just choose it all the time. But, what makes the K-armed Bandits problem interesting is that we don't know the payoffs. Let's say instead what we know is that each one has some probability of giving a pay off. And the payouts are all the same, let's say they're all 0/1. And each bandit has a probability of paying off a one. But, we don't know what it is at first. And, in fact, if we pull arm A and it doesn't pay off, does that mean that A is not the best one? No, I mean, how would we know? We don't know what anything else is. No, right, right. So, therefore, we can prove that one arm pull is giving us some information, but not an awful lot. We actually have to combine information across a whole lot of pulls to start to really appreciate what the best thing to do would be. Okay, so we have to explore. Yes! Good. Way to connect it back to the topic. All right, so let's think about what that problem might actually look like. So, I wrote down the observed payoffs for each of these arms. Associated with each one of these arms is the number of times that we've pulled that arm: 10, 20, 40, 5, 10, 20, 40. And of those times that we've pulled it, the number of times it actually paid off for us. So, this arm, we pulled 10 times and we got a payoff of 1. This arm, we pulled 10 times, and we got a payoff twice, two payoffs. So between a and e, if we had to choose one, it seems pretty clear that e has been better so far? Yeah, so far. Yeah, and let's say between a and d they both paid off once, but d gave us the payoff after only five hits. Or in the context of five hits, five pulls. And a in the context of 10 pulls. So d is a bit better than a as well. So, what I'd like us to do is think about two different things. One is, given the data that we've gotten so far, which arm has the highest expected payoff? And, of all the different arms that we could pull, which of the estimates are we most confident in? All right, I reformatted things a little bit. Mostly I wanted these to be the two boxes, and wanted you to put the letter of the highest expected payoff arm in this box, and the arm for which your estimate for the expected payoff is most confident in the second box. Okay, I can do that. I actually think that's pretty straight forward. I don't mean anything super duper technical about this. I just want to intuitively what do we have the most confidence in our estimate on? Okay, you ready? Okay so one last just to verify, I'm suppose to be putting in each of those two boxes a letter. A letter. All right Charles, what do you think I was looking for on this one? Okay, so highest expected payoff? Well that's pretty easy in this case, because they all have the same payoff, maximum payoff of one, and minimum payoff of 0. So I really just want the one that has the highest probability of coming up with a 1. And you did me a really cool favor, of making each denominator easily divisible by its numerator. So I could rattle off the probabilities of each of those and they are one-tenth, one-tenth, one-tenth, one-fifth, one-fifth, one-fifth and one-fifth. So one-fifth is bigger than one-tenth so I could put anything of d, e, f, or g in the first box. Good, okay so that was in some sense not that interesting. So if you had put g just because 8 is bigger than all the other things, that would have been a right answer but not for the right reason. That's right. So you really do want to compute what those ratios are. Right and speaking of for the right reason, I think that's what your question most confident sort of comes from. And I'm just simply going to claim, without writing equations, that when it comes to understanding expectations, your confidence is a monotonically increasing function of the number of samples. So whichever has the biggest denominator should be the thing that I'm most confident in because that means I've seen the most samples. So that would be c and g. Either of them. Yeah, okay and in particular, I wanted you to think about the fact that a and c, even though they have the same expected payoff, scene 1 out of 10 are estimate of what the actual payoff here is going to be much less certain than if we did it 40 times and saw that it was 4 out of 40. This is going to be a more accurate estimate. And so as you say, the confidence is going to increase monotonically with the denominator. The more data that we've got the more we believe our estimate based on the data. Right. So if I were to combine those two things and I and I just now had to do a bandit, I guess I would pick g, because it has the highest expected payoff and I'm most confident in it. Well, that would be one thing to do. And let's talk about why that could be problematic. All right, so I took your comment to mean, and don't correct me yet, that when you have multiple bandit arms, you want to choose the one that has the highest confidence. That's not what I said, but okay. I said, don't correct me yet. So here we have two bandit arms, a and b. And we've pulled b twice and we've seen one pay out, and we pulled a once. And we didn't see it pay out. So, in terms of expected value, b is winning. Yes, B is winning. And in terms of confidence, b is winning. Right. So, what do you think the best thing to do is? Do you think at this point we should just always pick a? Always pick b? Or maybe something else. I'm going to go with something else. [LAUGH] All right, so if we didn't have that third option though, which would you choose? That depends, how many more times do I get to do this? Well wait, I'm forced to always choose a or always choose b? Yes. I guess I'd always choose B. Yeah. That actually is not a bad idea but the fact of the matter is that it's always this problematic and the reason you want to do something else is because well, let me first tell you this because I bothered to compute it so I might as well share it with you. I did the following calculation. I said, if we were to randomly choose the payoff rates for both a and b, and then we pull a once and we pull b twice. If we happen to see no payoffs for a, one payoff for b, how often is it the case that b is the better arm than a? Okay. And you were right, that from the data that we have at the moment b is a pretty good guess and in fact 70% of the time b is the better of the two arms, but a full 30% of the time A is the right arm. So you don't just want to go with b. because you're going to be wrong almost a third of the time. A better thing to do is something else. So what is, what's an idea for something else? What's a reasonable thing to do if we got an arbitrary number of polls left. So if we have an arbitrary number than we might as well talk about what happens after an infinite amount of time. If I had an infinite amount of time then I know my true expected values for a and b and so I know which one to pull. Right? So another way of saying that is I want to do the thing that gets me more confidence in my expectations. So based on your suggestion, Charles, I wrote down a couple different something elses that might be tried and I want you to see if you can figure out if any of these are actually any good. So the idea of maximum likelihood would be to say, okay, well we should figure out which arm has the highest expected pay off and choose that but keep revisiting the question. So after you've done the pull, recalculate your expected values again and again. Choose which one has the highest expected value. And just keep repeating that. That way, as new data comes in, we're factoring it into our decision. And so there'd be a similar argument for maximum confidence? No. Well, first of all, I wanted you to kind of say whether or not you thought that was a good idea. That's a bad idea. That's not a bad idea, but it's a bad idea. No, it's a bad idea. So in this case, if I look at a and b, right? Who has the maximum likelihood? B. Right. Well, the max. Let me say it this way. It's the maximum maximum likelihood estimate. The maximum maximum likelihood? Right. So the maximum likelihood estimate for this is 0.5. Mm-hm. And the maximum likelihood estimate for this is 0. Right. And so this one has the maximum maximum likelihood estimate. Right. Sure. Right. So but here's the thing. Since a has 0 and b has 0.5. I should choose b. But, no matter what happens from now on when I choose b, b's likelihood, b's expectation will always be greater than 0, right. Yeah. So, I mean it will begin to approach 0 after an infinite amount of time. But even if I never see a payoff again, it'll still be something greater than zero. Right. Which means I will never, ever look at a. So what does this strategy actually turn into in this case? Always b. Yeah, so that works 70% of the time. [LAUGH] That's true. But that means 30% of the time you're failing. Right. And we can do better than that, we truly can do better than that. All right, so maximum confidence says now the strategy is whichever one you have the best estimates for. You should choose that one. Well that's a bad idea too. All right. Why is that? Because whichever one I start out with having the most confidence in, I'm going to pull that, that means I'll get more data, which means I'll have more confidence. So what would it do in this case? This would be equivalent to what strategy? Always b. Yeah, all right. So this is, we now have two other ways of saying, always b. All right, what about minimum confidence? So you'll alternate between a and b? Yes. Well, you may not. You'll eventually alternate between a and b. I don't know. I guess you do a, and then you would immediately have the same amount of confidence. So we have to break ties. Yes, so if multiple things had the same minimum confidence, it doesn't matter which one you choose. Right. Although, I guess I would choose the thing with the maximum likelihood, minimum confidence. On minimum confidence of the maximum likelihood. Oh, if you break that tie, it doesn't matter. because what's going to happen is, these are all, all the counts are going to be equal. Then you're going to choose one. And then that one's going to have some additional confidence, and the other ones are going to be lower. So you choose one of the ones that are left. You're going to end up going round robin through all of them. So at the beginning of each round, you might break the ties in some interesting way. But then the rest of the round you're going to be going through and getting everybody up to the same denominator. That's true, but given that at any given time, you might take the bandits away from me, I should always pick the thing with the maximum likelihood to break the tie. Sure. But, your point is well made, Michael. All right, thanks, but the point in this case is that you could do this minimum competence thing and you're going to over time get better and better estimates of the payoffs of the two arms. But you're never really going to use that information. That's true. Right, so if you're always choosing minimum confidence you're really just going to be uniformly going through and choosing all the arms. It's not a strategy that's going to beat either of these always a or always b. Well, it's not going to be always b in particular. It'll be half the time always a and half the time always b, so to speak. So, it'll be 50% Something like that. Actually, it'll just be uniform over all the things that you got. Yeah, yeah, so it will never actually be picking the better arm. Or half the time. I guess half of the pulls in any infinite run will be on the better arm. Well, one out of K. Sure, if there's K arms. Right. In this case there's two arms. But that's right. Good. All right, so these are something else and these are not better than always a or always b. So we needed something else, even something or else. Okay. It's going to be some combination of these things. It does seem like that ought to help. So in particular maximum likelihood is not a bad idea. Minimum confidence is also not a terrible idea, because it is at least bringing you new data. But you want to somehow balance these two. And what we usually refer to this is is the exploration exploitation dilemma. Oh, yes. This sounds familiar. So the minimum confidence arm is going to get you better estimates, but the maximum likelihood arm is going to get you more reward, probably. Right, in particular, like we argued before, if I've only got one more thing to do, I should pick the thing with the highest expected value. Yeah. So okay, so. To actually make up a strategy that is going to do well in this game. We kind of have to decide what we think the right metric is. Oh. So it's not so clear yet that there's one obvious metric. I kind of suggested using these check boxes that what you want to do is eventually always pull the better arm. Well yeah, of course that's what you want to do. Eventually. Well, no. Eventually. No, no. Okay, well look, if you know what the better arm is, you should always pull it. Sure, but the point is that there's a lot of fun that happens on the way to that point. And so in fact, I think all the fun happens on the way to that point. So I think we need to kind of talk about what kind of metric makes sense before we figure out which strategy is the best. Okay. That seem reasonable. All right, so I've got a long list of these metrics. I'm not sure how many you would actually think of, but let me list some and then maybe you can riff with me. Okay. So one metric that we could use is, if we have an algorithm, does that algorithm identify the optimal arm in the limit? If you ran this infinitely long, would at the end of infinity, you know which arm has the highest payoff. Okay. So that's good. Yes. because it's good to know what the highest arm is, right? But it's kind of bad because this is, this metric is actually achieved quite well by an algorithm that just round robin chooses all the arms. Pull the first one, pull the second, pull the third one. So in the limit, we'll all the data that we need to estimate them as accurately as we want, but we'll actually not have taken advantage of it ever. Right. And so I think it's important to say then that this is like the normal reinforcement learning setting, so while you're learning and doing whatever you're doing, you're still accumulating reward. Yes. Or not. Right, so it would be nice to have a metric that actually paid attention to not just how much we're learning, but how much reward we're getting in the process. So we could just make that the goal. Sure, but is that the original problem? I mean, how do you do that? No, no, no. I mean we were phrasing the original problem as figuring out which arm has the highest payoff. Okay. So this is kind of different than that, right? This is maximizing the discounted expected reward, which is going to do that well. You should do some amount of exploration, but also some amount of exploitation. You're getting reward, but you also need opportunities to get high reward by identifying high scoring arms. Yeah. Sure. That makes sense. Okay. So this is totally doable, except it's not so doable in that it ends up being the computation for this ends up being somewhat intractable. Oh, of course. We could talk about how you can actually do this in practice. There's some special cases that actually work out beautifully. There's a thing called a Gittms index. So Gittms index is a way of saying, all right, here's an arm, and I've pulled it some number of times, in the denominator, and I've gotten some number of successes, these two numbers can be mapped to a value called the index. Such that, if you do that for all their arms, compute all their indices, and always take the maximum one, you actually maximize discounted expected reward. So it's this really clever mapping from the number of successes and the number of pulls, to a number, such that always greedily choosing with respect to that number is optimal. What is the Gittins index? What is the Gittins index? I think it comes down to if I gave you the choice between this arm and something with a known payoff, how high would that known payoff have to be for you to forego ever trying the uncertain arm? So if I could pay you off and say you're never allowed to choose this bandit again. How much money would you expect for me to do that? What expected value of a pay off would you need so that you don't even ever need to try this again? And so it ends up being greater than the expected value of the arm itself right? So the arm has paid off 0.2 on average, but we've only tried it 20 times then this index value is going to be bigger than 0.2. because I wouldn't be willing to just take a sure thing of 0.2 instead of this one because I could pull this a couple times, discover that it's actually higher paying and do better than the 0.2. So, somehow the index has to compute some kind of confidence on the probability that this is higher or lower than the expected value or something by some amount? Yeah, so it actually, ultimately combines this notion of the maximum likelihood and the low confidence, right? So if you have low confidence then you get an extra bonus for pulling this arm because it may reveal something really valuable. Right, okay, so there's magic you can do that computes this and stuff happens. Right, and so I want to caution you though is the Gittins index is extremely seductive. Yeah. In that it's just this beautiful idea that works incredibly well for bandit problems and feels like it should generalize. And people have not been able to generalize it, but they've wasted a lot of time trying to. So I would caution you don't go down this road. Unless you think you have a fundamentally new way of doing the analysis, the Gittins index thing seems to be pretty specific to bandits and therefore won't be helpful to us when we're doing more general reinforcement learning. Okay, I've gotta say that's very meta. Oh. I see what you did there. because I'm talking about thinking about different research topics as if they were bandit arms and trying to argue about well, this one has been tried an awful lot. And so the payoff seems high but once you know that it's been tried over and over again, the expected value is actually quite low. Uh-huh. And so, I'm just trying to help you, give you experience so you don't have to pull this arm. Yeah, very meta. That's cute, I wasn't thinking that way. I'm about helping others. All right. So we can think about maximizing reward. Or expected reward, or expected discounted reward. But over a finite horizon, instead of this kind of infinite horizon, like we talked about in number two. And that's similar. It's something that is actually computable, but can be very expensive to compute. Sure. So, how would you actually compute that? That's a fair question. Let's save that one, because some of these problems are actually going to end up being reducible to each other so we won't have to solve all of them independently. We just have to relate them to ones that are already solved. Okay, well I will point out, since I'm being meta, that if the finite horizon is one Then we know the answer. Mh-hm, right. So the finite horizon is one, then chose the one that has the maximum likelihood estimate. Mh-hm. That's fair. And you sort of kind of generalize that idea to do two Horizon of two, horizon of three, horizon of four. You could just kind of iterate it out. It's a little bit like value iteration. So now we're going to have a slightly twistier definition. So, what about a metric like this? We're going to try to identify a near optimal arm, and near optimal means, well, we can be wrong by choosing an arm that's epsilon close to the real arm. Somebody's going to give us an epsilon as a parameter, so we don't have to get the absolute best arm Mm-hm. because that's actually impossible to do in finite time, because we could always be really unlucky, and get just a strange sample where the highest payoff arm just happens to not have paid off for us. So, we're going to soften it to a near optimal arm, epsilon close to the actual optimal arm, and this doesn't even have to work all the time. It just has to work with high probability. Probability 1 minus delta. And the number of pulls before we can identify or before we have identified the near optimal arm needs to be polynomial in the number of arms. 1 over delta. 1 over epsilon. There's some polynomial. We'll call it T. Of K, epsilon, and delta that bounds the number of pulls before we can identify the near optimal arm with high probability. So that T is not the transition function. So you're not moving from state K to state delta through action epsilon. Oh, good point. Yeah, that was probably not the best use of T. T here is supposed to be time. Would you like tau better? Sure, let's call it tau. I like tau. Cool, I like the tau. All right, so I like four, because it looks like PAC learning and it looks like the kind of thing that machine learning people come up with. Yes! Yes, it is a lot like PAC learning. It is a very PACish sort of thing. I had forgotten that we knew about that. But that was in our other class, right? Probably approximately correct. Yeah probably approximately. So there was one paper that actually called this PAO. PAP. Probably approximately optimal. But that didn't end up sticking. They call this PAC even though it's not really a correctness thing, it's not a classification. But it, the rest of the flavor of it is very much like PAC so we just stick with that. Well it's a classification if the goal is to classify the best arm. Yeah. That's a good way to think about it, sure. So here's a metric that's kind of like number four. In that we're again going to have some function of epsilon, delta, and k, that is going to give us a number of time steps, and in this particular case, instead of saying, at the end of that, we're going to know what the near optimal arm is, instead we're going to say, we're going to have nearly maximized reward. Over that, that interval. Basically, you give me an epsilon, a delta, and a k, number of arms, and I will make a strategy and a polynomial tau prime, k, epsilon, delta, such that, if I run that long, I will get at least this close, I guess per step, to the maximum reward. With high probability. Okay, okay. So, that's like there was between one and two. Where, instead of the goal being to figure out what the best arm is, the goal is to pick up as much reward as possible on the way to figuring that out. Yeah, yeah, that's a good way to think about it. So, we're actually willing to take longer to find the optimal answer so long as we pick up more reward along the way. Could be, yeah, could be that there's a trade off there. It could be that these things are relatable to each other. At least, a little bit relatable to one another because certainly, if I can do five quickly, I would probably do four. Well, we'll work that through, but I wanted to put these up as separate things. And then it turns out we can, in fact, relate them to each other. Cool. But if I had to choose between one of them, I think I'd Prefer 5 to 4 just because it does matter how I get there, not just that I got there. Yeah. And certainly this is going to deal more directly with reward. Look, it seems like for number 4, we could actually potentially do a lot of really terrible stuff for tao, K,epsilon, delta steps. As long as at the end of it we go, And here's the right answer. We could have sort of popped up with our hand saying, this is what I should have been doing but haven't been doing. [LAUGH] Whereas in number 5, the money is where your arms are. [SOUND] That's not right, but we're actually getting nearly optimal payoff. Okay, that makes sense, cool. So I can't wait to see how you relate all these things together. There are other possible metrics, there's a very long list of possible metrics. But I just wanted to throw down one more, which is, let's say that we want to pull down a non-near optimal arm, epsilon, no more than tau double prime k, epsilon, delta times with high probability, 1-delta. So basically the number of times that you pull an arm that's not nearly optimal is small. And there's over no particular time window. It could be over a very very long time window, but the number of pulls, the number of mistaken pulls is really small. Okay, well that makes sense so basically I want to minimize my mistakes kind of. Yes, that's a good way to think about it or at least bound, this is mistake bound. This is number of reward and this is, you know, identifying your output more. And it turns out that all of these guys are actually equivalent in a sense that an algorithm for one gives us an algorithm for the other two. Oh, well that's good. All right, let me tell you about that then. So what I'd like to do is kind of talk you through the process by which we could show that these different metrics, at least these three metrics that I was claiming are equivalent, can be reduced to each other. That if we have a solution to one, we can use it to solve the others. So the first step of this that I want to show is that, if we have an algorithm that can find the best arm, or lets say the epsilon best arm, with high probability in some bounded number of pulls, then we can use that to build an algorithm, that will have low mistakes. And by low mistakes, I mean the number of trials at which it pulls a non epsilon prime close arm is small, is bounded. Okay, so by low mistakes, you mean few mistakes? Low mistakes, I mean few mistakes. I can change it to few mistakes. Yeah, that's your choice. That's just one of my many mistakes. So we want to construct an algorithm that if we give it epsilon prime and delta prime, that it can choose a tau prime K epsilon prime delta prime, that is going to choose an epsilon prime close arm on all pulls except for this bounded number, this tau prime number, with high probability. And all we've got to work with is this first algorithm that finds the epsilon best arm in tau K epsilon delta pulls. So what we have to do is to find, if we had that algorithm, what epsilon do we give it, what delta do we give it, and how do we use it so we have this guarantee? Wait, why can't you just use the original algorithm to find the epsilon best one, and now you can do an epsilon close arm? Okay, so we are going to set epsilon to epsilon prime. Delta to delta prime. Then we're going to run algorithm A and what do we do? So, after tau K, epsilon, delta, pulls, we know an epsilon or epsilon prime nearly optimal arm. And we just pull it then forever, right? Yeah. And how many mistakes do we make? From which point? Once we've chosen the epsilon prime arm. Well, no. In the whole run, right? because the idea is that we have the bound the total number of mistakes that are made. So the total number of mistakes, this tau prime is equal to what? Well whatever number of mistakes algorithm A would have made in finding it. We have a bound for the number of steps before we have identified, or before this algorithm is going to find a near optimal arm. Oh, I guess it's in fact tao. Yeah. So we just run the original algorithm until it finds a best arm and we pull it there after. The maximum number of mistakes it will make is exactly the number of steps that it took to identify that arm. It may be that on some of those trials it actually was taking the best arm, but this is just a nice upper bound. You don't have to worry about the mistakes that I'm going to make at that point on? No, because we're not going to make any more because with high probability, we found an epsilon close arm. So with high probability, we'll never make another mistake. Now there's some probability that it fails, but that probability will match across these two problems, so it's okay. Right. So the mistake is having pulled the wrong, a non-epsilon prime close arm. Yes. Not some other notion of the state. Exactly so, right. So we'll say that a mistake means we pulled an arm that wasn't very close to optimal. Right. And so we just whittled down the total number of mistakes that we make. Okay. Well yes, that was pretty straight forward. Good, that was an easy one. All right. So the next thing we want to show is if we have an algorithm that has a bounded number of mistakes in the sense that we just talked about. That that algorithm will eventually do well, in the sense that it will achieve a total reward that is arbitrarily close, epsilon prime close to the per step optimal reward. So let's imagine that we have an algorithm that pulls epsilon suboptimal arms at most this bounded number of times with high probability. So this is something that has very few mistakes. Oh. Except this wants to be tau. Right. Given that algorithm, we want to make a new algorithm, that with high probability, one minus delta prime, has a per step reward that is epsilon prime close to the optimal per step award. In other words, what we would've gotten for pulling the optimal arm on every step. And that bound starts to hold after some number of steps, tau prime, k epsilon prime delta prime. So does that mean we get to do the same thing that we did before? Let's try that. Okay. We run this mistake bound algorithm that makes very few mistakes. What's our tau prime? When is it that we're going to have achieved near optimality? Because algorithm b is the same as algorithm a, it means that by running algorithm b, we're going to end up with an arm that is epsilon optimal. Yes that's true, but I don't want to think about it that way. I don't want to bring algorithm A in to this at the moment. Lets stick with the algorithm on the slide, algorithm B. So, we have an algorithm that pulls epsilon suboptimal that many times. That implies the way it's written, that you could have pulled a bunch of epsilon optimal arms, many, many times during that. Yes. But not during that time period, like you could have pulled epsilon optimal a zillion times and then pulled epsilon sub optimal tau times. Yes. And then it would be tau plus a bezillion. Yes. Would be the total number of steps. Yes, so yeah, this is all true. So, okay, so we need to kind of think about how we can use that fact to build a bound. Hm. So ultimately we're going to run, this is going to be algorithm c by the way. We're going to run this algorithm c for some number of steps. We're going to call tau prime, k, epsilon prime, delta prime. And let's say that the algorithm that we actually run for this amount of time is algorithm b. Mm-hm. What do we know? We know that it could be that it made tau k, oh lets say epsilon prime, delta prime mistakes. And then all the rest have to be actual near optimal. In fact, well it's going to be epsilon optimal. So we might need a different epsilon than epsilon prime. But, so let's say that it's epsilon optimal for the rest of these steps. Mm-hm. What do we have to do to make it so that this average is close to optimal? Well from that point on, we just pull anything that's epsilon optimal. And it'll sort of all average out, right? No, because we don't know how to pull things that are epsilon optimal from that point on. All we know is that this algorithm b, is going to make it most tau k epsilon delta prime mistakes. Mm-hm. So, on those trials how close to optimal is it on the trials where it actually makes a mistake? Where it makes a mistake? Yeah. Something greater than epsilon. Sure. What's the largest it can be? One. Good, right. because remember, we're talking about a Bernoulli bandit, which is a bandit you pull the arm and it either pays off with probably 0 or 1, or something in between. So the worst we can do is, we pulled an arm, but it actually has a payoff of 0, but the right arm had a payoff of 1. Right. So the maximum error, the maximum first step reward cost that we could have in this case is going to be one. Times tau. For those trials. Right. That's times taro, that's right. In fact let's write that. And how close to optimal are we going to be in these other trials? Epsilon optimal. Epsilon, right. Alright so this is going to be our total suboptimality over the end steps. Mm-hm. And our average per step optimality is going to be that. Right. And we want this to be founded by epsilon prime. Yes. And we get to pick two things, right? We get to pick n and we get to pick epsilon. So, can we solve this? How do we do that? So I'm going to suggest that we just pick an epsilon. It's going to be problematic if we pick epsilon to be epsilon prime. Because then we're making these big mistakes here and we're making even biggest mistakes there. So, on average, it's not going to work out. Mm-hm. But, if we set epsilon to epsilon prime over 2, right. So, we want our average per step reward to be epsilon close. But, to do that, we're going to run this algorithm B, with an even tighter epsilon. Okay. Alright. So, that gives us this inequality. With only one free variable, which is n. So how big does n have to be, so that everything works out? So, that should be solvable. Yeah. Multiply both sides by n. You do some algebra. Okay, so the algebra got us to this point. Where we say that the n, the number of steps that we need to run before we have near optimal reward per step. Is at least m, the number of mistakes that the algorithm b is going to make, times 2 over epsilon prime minus 1. So epsilon prime, if you think of epsilon prime as being some teeny tiny number. So 2 over epsilon prime is going to be some fairly big number. We have to run that minus 1. But it's still a polynomial in the epsilon prime that we were originally given. Right. Or 1 over epsilon prime. So epsilon prime has to be, Less than 1. Right. So we're good. What's going to happen here, is we could get something like 2 over 1 minus 1 which is going to be 1. But as epsilon prime gets closer and closer to 0, this blows up, but again, it's polynomial in 1 over epsilon prime. So we're good. It just means that we have to run a bit longer than the number of mistakes that we are bounded by. So that on average, things work out, because we're going to be essentially optimal here. We're going to be epsilon over 2 optimal here. And so when we've actually worked that all the way through, we're going to get something that, averaged out over this very long time interval, it's nearly optimal per time step. So that the reward that we're going to get is actually close to optimal. So I'm going to claim this is the last relationship that we have to build to prove that there all equivalent. We'll pull that altogether at the end. But this is going to be the hardest one. This ones says that if we have an algorithm that does well on average per step, right, it's nearly optimal per step. Then we can use that to build an algorithm that actually identifies an arm that is nearly optimal. And so this is what we're imagining. We imagine we have an algorithm, we were calling it C I think. That gets within epsilon per step of optimal after some number of pulls with high probability. So after this tau k epsilon delta pulls, then the average reward that we've gotten is close to the average reward that you get if you were pulling the optimal arm on every step. What we want is algorithm A, which, after some number of pulls, returns an arm that is epsilon prime optimal. So someone has to tell us K, the number of arms. Epsilon prime and delta prime. Oh and this holds with probably one minus delta prime. Then what we get to do is set up this other algorithm C, however we want. And run it for how long it runs. And at the end of that, somehow use the information that was gathered to pick an arm that is nearly the best. Right, and I just realized something. I think was obvious, but I actually wasn't thinking much about it. So I should say it out loud, which is, we're going to take the epsilon prime, the delta prime, and we're going to create some epsilons and deltas. That has to be polynomial. Yeah, because if we did 2 to the 1 over epsilon prime or something like that, then we've actually made things really bad. Right, right. So it has to be polynomial. Okay, let's worth remembering. And that was true in the other two examples that we did, and it's going to be true in this one too. Right, sure, sure. It just occurred to me that you could cheat if you could just do any arbitrary epsilon, epsilon prime. [LAUGH] Yes. Just set epsilon to zero and. [LAUGH], Yeah. Okay, cool. All right, so I. I think I have half an answer here. So maybe half the answer will help towards the full answer. So here's the half the answer. If I've run algorithm c and I've got a per step, near optimal payoff, then one thing I know is that if I were to keep following whatever distribution of pulls, of sort of bandit arm pulls that I'd done before, I would still have the same per step average. So here's what we're going to do. We're going to introduce an additional epsilon looking thing. We're going to call it e sub i, and that is the sub optimality for the error of the plurality arm chosen by algorithm c. So there's going to be some arm that gets chosen more than the other arms. Or as much as the other arm, but no less than any of the other arms, and we will call that arm I and EI is the subabnormality of that arm. So, what do we know? We know that this arm is chosen 1 over K of the time, so the total per step error Epsilon has to be at least as big as ei times 1 over K. Right. Yeah? Right. Okay, so if we rewrite this in terms of a constraint on ei, ei has to be less than or equal to epsilon times K. But we want to set epsilon so that the suboptimality of the plurality arm is less than or equal to epsilon prime, right? So that's the constraint that we're given. We need to figure out a way of running things so that the plurality arm is epsilon prime optimal. So we want this plurality arm to be less than or equal to epsilon prime. So if we set epsilon equal to epsilon prime over k, then everything holds. Over here, epsilon over k greater than or equal to this implies that epsilon is greater than or equal to e sub I, which is exactly what we wanted. So this implies the rate thing. Right, and in fact, you could get the epsilon equals epsilon prime over k just by doing algebra, right? You got e sub i less than or equal to epsilon prime, and we know that e sub i is also less than epsilon times k, so we want epsilon times k to be less than or equal to epsilon prime. Therefore, epsilon is equal to epsilon prime divided by k. Good. So if we set that epsilon and then we run algorithm c for tau steps, then the arm that we get at the end of that run, that's chosen most often has to be nearly optimal. Right, because if it wasn't nearly optimal then we would have actually accrued a lot more per step error. So that actually does it. We actually can, just the fact we can do arbitrarily well means we can find something arbitrarily close to the best arm. Wow, so that's brilliant. So whoever came up with that proof is one smart, handsome guy. So now that we've gone through this exercise let's put these pieces together. We showed three separate results that being able to find the best, or nearly the best, let's you make few mistakes. Being able to make few mistakes let's you do well on average per step. Being able to do well on average per step gives you the ability to actually find a nearly best arm. So, all these things are actually equivalent. If you give me an algorithm for doing any one of these, I can derive something for doing the other two. Just kind of do it in a circle here. So, in a sense what this is telling us is, that we kind of don't have to pick among these three different optimality criteria because they're really all the same as each other. And you could sort of say, well and it must be that these are the right ones to think about because even if we don't think about this doing well on average per step, we would have to solve it anyway if we cared about any of these other things. So, Hm. I don't know. What I get out of this is the idea that whichever of these is most convenient for you to work with, you should just work with it because the other ones are going to fall out. And there's other optimality criteria that you could worry about, but I don't know, it seems like a lot of them come back down to these. Yeah, I like that a lot. You're really saying that they're kind of in an equivalence class of difficulty. Yes. Now interestingly, the thing that we haven't done yet, and that we need to do, is show that we can solve any of these problems. I thought we just did. We put a lot of words on some screens and there were some taus. [LAUGH] No, no, no. No, what we showed was that if you could solve one, you could solve the other. If you could solve that one, you could solve. But, we never said that any of these are actually solvable. So, we need one more result to kind of make this all doable. Okay, that's fair, but I was satisfied with proof by tau, but if you want to actually go and ground it out. Yeah, but the taus were all defined in terms of other taus. It never bottomed out. Right, what'd we'd really like is something that says, here is an algorithm that will in a polynomial number of trials find you a nearly optimal arm. I don't know, Michael, it seems to me this is pretty straight forward. It's taus all the way down. It can't be taus all the way down. Why not? It's turtles, they start with the same sound. That's true. I guess they're Greek turtles. Yes, it's Greek turtles all the way down, which is what we've always suspected. So at the bottom of this stack of turtles is a turtle named Hoeffding and Hoeffding gives us the solution to all of the rest of the turtles. There's actually a bunch of different names for this. I've heard churn off, I've heard other things, tail bounds, which you can see that the tale is bound, I kind of forgot to draw it. That all kind of boiled down to the same thing. So, I apologize to any person who may have derived one of these bounds who I didn't show proper appreciation to. But, this is the general flavor of it. Well, I think the most important thing is that you find someone, who did the derivation, who's Greek. Since all the turtles are greek. Except for Hoeffding, as it turns out. So, here's the form of the Hoeffding bound. So, Hoeffding bound says that if we have a set of n random variables, X1 through Xn. And let's say that they're all iid, which is independent and identically distributed. And each one has a mean of mu. And let's, so let's say that we observe these values. So it's bunch of zero ones. Mu hat could be a good estimate of mu, right? It's like add up all the variables that you saw, all the zeroes and ones and divide by the number of them that you saw. In other words, the maximum likelihood mean. Yes, which is exactly the maximum likelihood estimate. But so it's the maximum likelihood estimate. But the bigger n gets, the more confident we get as to how we are to it. So this gives us a way of quantifying this. So the following is a 100 times 1 minus delta percent confidence interval for mu. Right? So we don't know mu, but we know mu hat because we can get it just by averaging these values together. We're going to say that the real mu is highly likely to be between mu hat minus z delta over root n. And mu hat plus z delta over root n. Where z delta is the square root of 1/2 times log of two over delta. Oh right, that's obvious. Well, it's maybe not obvious but it does have the right kind of properties. Like I don't think I could justify this 2 without going through the detailed derivation of the bound. But why is delta where it is? Well, the more confident that we want to be. The smaller delta gets. And that means that this quantity gets bigger. The log of that is still very big. The square root of that is still very big. So this Z delta is very big. And so the width of this bound ends up being very big. If we want to be really, really sure, we need to give a big window, so that we're really sure that we're in that window. But on the other hand, if we have a lot of data, if we have a big n here, that tends to make the bounds smaller. They shrink with the square root of n. So if we have a billion for n, then it's going to be mu plus or minus some quantity, divided by something really big. So it's going to be just a small little interval around our estimate. So it has the right kind of properties. But the fact that it's a square root of a log and there's a 2 and a one half, that's maybe not so obvious. Well, it looks a lot like. In fact, we use Hoeffding bounds when we were doing pack learning. Yes, yes and again these learning results that we were just talking about have a very pack kind of feel to them. So, it's maybe not so surprising that the bottom of the stack of turtles is a turtle name Hoeffding. That's cool. So basically, everything really does all tie together. Yes. At least as long as you think about things this way. So, okay, but this isn't quite answering our question, but this is going to be the key tool for answering our question. So given that we have a way of knowing how close we are to estimating a hidden parameter from a sample. We can then use that to figure out how many times do we have to pull an arm before we know fairly accurately what its payoff is. And therefore, if we have K arms, how many times we have to pull each of those arms, so that we are very confident that we find something nearly optimal amongst them? Beautiful. I don't yet know how this is going to solve everything, but I can see how it is a step in the right direction. So the information about the Hufting bound gives us a way of thinking about how many samples we need to accurately learn the value of an arm. So we're going to have to actually pick a number for that. How many samples we're going to take of each arm to guide us in that decision. It's going to be helpful to think about how we're going to combine information across the arms. So this is going to be a strategy that will ultimately work, though we don't have all the details filled in yet, and it goes like this. What we're going to do is, we're going to take c samples of each arm for some value c. That's what I was going to do. The c is going to be some function of epsilon delta k, that sort of thing, but it's going to be some number, and it's going to be a number that, well, we're going to see. It's going to be some number that guarantees that we're some amount close with some certainty. And then after that point, we're going to choose the arm that has the highest estimate, the best estimate. All right. Ultimately, we want our final decision here to be epsilon close to the optimal decision with probability 1 minus delta. So I claim that to do that, for this to work, we need to learn each arm individually to an accuracy of epsilon over 2 with a certainly of 1 minus delta over k. All right. So instead of doing a super duper formal proof, here's how I like to think about this particular result. So here's a picture that I'm hoping is helpful. It's kind of a weird graph of the x axis is the arms. In this case, this is a five arm bandit. And the y axis is the value of that arm. And these rectangles represent our uncertainty integrals of that arm's value. Does that make sense? Okay. They're not the arms themselves, that's just the uncertainty. Yeah, if they were actually arms then I'd have to draw like [NOISE]. [LAUGH] Ooh, that's terrible. That was supposed to be a hand, so I apologize. That's a seven-fingered hand. That's okay. Yeah, I think that was the problem. You know what it is, it's to make up for the fact that, like, Mickey Mouse only has four. That's perfectly reasonable. So if we do this. If we make sure that each arm, we are 1 minus delta over k sure that its estimate is within epsilon over 2 of what r. Our sample to valuation is. So how big are these bars, exactly? Oh, okay, I see. So those are uncertainty bars or error bars. So that means your estimate is right smack in the middle of each of them. Right, so we have our estimates for each of the individual arms and that estimate is, we're pretty sure, epsilon over 2 close to the actual value. Right, so that would mean that you could be epsilon over 2, too high or epsilon over two, too low, which means the total height of those bars is 2 times epsilon over 2 or epsilon. And those estimates hold true with some probability. Sure. So each one is correct with probability, 1-delta, over k. So the first thing we need to do is basically make sure that all these estimates are correct because if one of them is wrong, then it casts doubt over the entire calculation. So what's the probability that all of these estimates are actually correct simultaneously. Well, if I pretend they're all independent of one another then isn't it just one minus delta over k to the k? Delta over k is the probability that a given arm is wrong. One minus delta over k is the probability that that arm is right. So if that probability is one, for example, then raise it to the k power, sure they're all going to be right because they all have certainty of being right. But if it's a little bit less, you know 0.95 or something like that. Then it's 0.95 and this one has to be right, it's another probability 0.95 and this one has to be right. So we ended up with the raised to the K power of that individual arm probability. Right. It turns out for various reason that this kind of quantity is hard to work with, and also sometimes, I think not in this case, but sometimes it's also a little risky because it assumes statistical independence of all the estimates. And sometimes it's safer to just say maybe they're not independent of each other. Maybe they actually all fail together or all fail separately. So we use a standard trick to get around this idea. And this is the idea that we can estimate the probability that at least one is wrong. If we know the probability that one is wrong, it can't be since there's k of them, it can't b any worse than k times delta over k. And this kind of follows from if you think of it as like a Venn diagram. Here's the probability that one thing is bad, here's the probability that the other thing is bad, here's a probability that a third thing is bad. What's the largest that this can be. The maximum badness probability is actually going to be the sum of these if their intersections are all zero. And so this is really like taking the union of all those sets. So this is referred to as the union bound. Which I don't know, to me sounds like kind of a fancy name for not such a fancy idea. We're just going to add the probabilities because we don't know how to account for the overlaps. Oh, but that just follows from probability, right? The probability of a or b is just the probability of a plus the probability of b minus the probability of a and b. And sort of the worst case in this situation is that a and b is zero. Right. So this is just how these probabilistic quantities are defined and what we're saying is if we want to maximize the probability of this or, that at least one of them is wrong, then if we set this intersection to zero that maximizes it. So it really is just, the sum is an upper bound. So what the union bound gives us, then, is this idea that if each of the arms individually has a certainty of one minus delta over k, then all k of them together have a certainty of at least one minus delta. So that was what we wanted. And now the last thing that we want is to know that the arm that we pick is within epsilon of optimal. So what gives us that? Well, I mean, the idea is that whatever true the optimal is in this case, it might be at the bottom of a bar's range, for example. Whatever's optimal cannot be at the bottom of a bar's range. Yes good point. But it could be at the top of the wrong bar. Right. Not the bar that we think is the highest. So here we think that this bar here, this third bar, is the highest, but its true value might actually be all the way down at its bottom. Whereas, some other bar that looks lower than it's true value might be, actually, much closer to the top. So the worst case is what happens is we have one bar that is infinitesimally above another bar, we just pick it. Because it's slightly better. But in fact, its actual value is down at the bottom. Whereas we failed to pick this other arm that has the optimal in it, and that optimal's at the top of its range. So the most we can be off, you know we picked this arm, the most that that arm can be off is the height of the bar, which is is epsilon. Right. So that makes sense, although I don't think it has to be infinitesimally lower. You're saying if it's just a dead tie and we picked arbitrarily. because you gotta pick one of them. That's true, and we could pick the wrong one. Right, and the wrong one could have been as far off as possible, and the one that we didn't pick could've been as far off as possible in the right direction. Cool. Okay. And then still, you'd be epsilon away, as opposed to epsilon minus epsilon. Epsilon minus epsilon, which I feel like is small. Okay. All right, excellent. So that gives us both pieces here. That means that if we can just figure out how many times to sample an arm, so that the guarantee that we have is that the value that we predict is within epsilon over two, with confidence one minus delta over k. Then putting all those estimates together is going to get us an arm that is epsilon optimal, with probability one minus delta. That's perfect. And we know how many to sample. We do. We're just going to do a little bit of algebra and we're there. Okay, cool. All right, so now we're going to figure out how many samples we need, and in particular we want to be epsilon over 2 accurate in our estimate. And we want to be 1 minus delta over k certain that our estimate is correct. And so just so substituting those values into the hoeffding bound that we had before gives us one half times natural log of 2k over delta, the search parameter. Divided by square root of c, the number of times we do the sample is going to be less than or equal to epsilon over 2. So now we just have to solve for c. Okay, that seems pretty straight forward. All right, so substituting these constraints. Epsilon over 2 and the 1 minus delta over k into our hufting bound from before gives us something that looks like this. The square of one half, natural log of 2k over delta in the numerator, squared of the number of samples, c, in the denominator. And that gives us an estimate that's within epsilon over 2. So now we should be good to go. All we need to know is how big does c have to be. So we just have to solve for c. Right, so we do some algebra and some other algebra and the very thing happens. Right, so I did the algebra, except I kind of left out some stuff. I left out two holes in here, that I want you to fill in. Because you hate me? Well, I want to make sure that you're kind of following along. All right, fair enough, I can do that. It should be straight forward, because algebra. So, Mr. Algebra, what do you get? That's Dr. Algebra. I did not spend 12 years in evil algebra school to be called Mr. Algebra. [LAUGH] Okay. My apologies. [LAUGH] All right, well I think there's actually only one answer here, and that answer is two. All right, which one is that? Both of them. So there's two twos. Two twos. How do you get that? Okay, so you squared both sides, and that causes the epsilon to be squared. Gets rid of all the square roots. Then, oh, but then because we squared both sides, we get a divided by 4 over here. Which ultimately is going to meet up with the half over here, and we're going to get a 2. Yeah, that's right, very good. And thus, more proof that every single thing in computer science gets generalized once you have two of them. Two is very important in computer science. So let's see what we actually get here. What we're saying is that if we sample an arm at least this many times, 2 times 1 over epsilon squared. Now, 1 over epsilon squared is polynomial. And 1 over epsilon, 1 over delta n k, times the logarithm of 2k over delta. So again, this quantity here is polynomial n k, and 1 over delta. And we're logging it. And that gives us a count, that if we do that enough times we're actually going to be sufficiently certain that we're sufficiently close, that chosing the maximum arm is going to be a good idea. And it's interesting to note here that the dependence on the certainty parameter is actually quite weak. Right, so it's 1 over delta, and it's stuffed into the log. So this is having not a huge impact. But the dependence on the epsilon parameter is actually quite profound. It's 1 over epsilon, so if we're talking about 0.0001, that's like, I don't know, 1,000 or 10,000 or something. And then we square that. And so, we could actually need a ton of samples if we want to be very, very sure that we're very, very close. In fact, even if we want to be moderately sure that we're very, very close, there's just this big dependence on the epsilon. So what you're saying is that the hard part is getting very, very close, not being certain how close you are? It looks that way, yeah. That's what this equation is telling us. So we've said this before, since it's all hufting here, and hufting there, and hufting all the way down, that you're going to get stuff that looks like PAC bounds. But this looks really similar to the PAC bound for agnostic learning right? Hmm. There was a 1 over epsilon squared, and a natural log of delta in there, 1 over delta or something in there. Yeah, and it uses a union bound too. It uses the union bound and the huffting bound, just like we just did. But I mean, it's not exactly the same thing, but you're right. It has a very similar flavor. I look at this and I just think, one of these looks a lot like this says something about hypothesis space, and the other is about the error. It just feels like there's a connection there. Hm, that there's a sense in which you can think about the payoffs of the arms as hypotheses, in some sense. Or each arm is a hypothesis. That we want to find the best hypothesis, it's like finding the best arm. It has the lowest error, or the lowest suboptimality. Yeah, maybe, maybe. Other than using one kind of math to solve the other kind of problem, I don't quite see what leverage we get from that, but that's a good observation. Well, in that case, k is the hypothesis. This is the size of the hypothesis space. Yes, like the number r is, exactly. Right, it just seems like there's something there. I don't know, I'm sure that if we thought about it long enough or gave someone a homework problem, they'd be able to connect the two. [LAUGH] So just to sum up here, what we've been figuring out is a kind of PAC-style bound for these kinds of bandit problems. That we want to say we can get near optimal payoff after a polynomial number of pulls, or we can get a near optimal arm after a polynomial number of pulls, or we can make only a polynomial number of mistakes in an infinite run. That all these things actually all boil down to essentially this result. We just have to pull each arm enough times that we have an accurate estimate. That's pretty good. Well, PAC-style is better than either monkey style or cream style. [NOISE] Well, monkey style is quite good, hm. I'm impressed with your PAC-style. [LAUGH] So I promised that we would do stochastic bandit arms, deterministic MDPs, and then ultimately combine ideas together and handle general stochastic MDPs. So I think we can do efficient exploration of deterministic MDPs relatively simply actually. Here's a little randomly generated MDP. Randomly generated in that I drew a bunch of arrows and I have absolutely no idea what the solution to this is. But if we were doing reinforcement learning in this MDP, all we would know at any moment in time is which state we were in. And which action we were choosing. And so, an action, in this case, would just have a label, like action one and action two. I made two actions for each of these states. We know how to solve a deterministic MDP to figure out the optimal policy for it. But we can't solve it in the beginning, because we don't actually know any of this stuff. Right, all these numbers. Where do the arrows go and what reward do we get on an arrow? We don't know any of that at the beginning. Right. So, two observations. One, you don't have two actions at every state. Oh, what do you mean? Well, the one that has the three coming out of it, there's only one action that comes out. You don't have a self action. This one? Yes. Dope. All right, there's just two things, then they both can have a plus three. But, you're right. I thought I got them all, but I missed that one. It doesn't really matter. So, that's one observation. One observation is one does not always equal to two. The second observation. Is that if we know it's deterministic, then all we have to do is, in every state, try every action once. Yeah, that's true. And then we know everything we know about which action is best or whatever, magic. Something like that. Shouldn't that work? [LAUGH] Okay, well you're kind of onto something. It's certainly the case that once we've tried each action in each state then we can actually build the entire MDP and solve it. Right. The question is what do we do along the way? What do we do when we don't have the whole MDP yet? Maybe it's not entirely clear. I mean if we could just teleport ourselves to any state that we wanted to be in and take the action at that state, then that would be fine. But that's not how the world is in reinforcement learning. Right? All we can do is choose actions. We can't choose which state to be in. Well, we could track every state that we've ever been in and every action ever taken. That's true. And then just do things that seem reasonable. Explore a little bit randomly so that we make certain we get to every state. And get a chance to take every action and then kind of go. All right. So you're thinking explore randomly so that we hit everything. That could work, but we can make MDPs where that's problematic because if we choose actions randomly we keep, for example, getting reset to some beginning state and actually not choosing things randomly so that we visit all of the states can be challenging. So, that's true, Michael, but if we don't know anything about the MDP then I can always construct an MDP that's going to mess with whatever exploration strategy you have, right. Because I'll just always there will be some state with some action that puts you into some state you can never get out of. So, unless you can do random restarts or be able to get to some state. Press a button and do a reboot. You could always get unlucky and never be able to explore the MDP. Okay, all right. Well, that is a fair point. We have to be very careful about what we think our criterion is. Now, we went through it pretty carefully in the case of bandits, but I kind of jumped to deterministic MDPs without revisiting this question. So I'm going to propose a particular optimization criteria, and then we'll talk about why it needs to be like that. Okay, that seems fair. We're going to talk about some possible MDP optimization criteria. And, well I guess this isn't really an optimization criterion. But we talked about exploring randomly being problematic because you can have MDPs that look like this. Where there's a chain and you have to keep taking some specific action to get to the high reward state. And if you're always choosing actions randomly, then the probability you would even ever reach that is exponentially small, so we have to be a little bit careful. For example, in this MDP, it's not really clear what to do from this state. From which state, that state? No, it's pretty clear what to do from that state. Go left. Oh, okay. Sure. No, I mean in the reinforcement learning setting, we have just two actions. If we choose wisely, as you just did, then we end up in this state and we get reward and that's yea. If we choose not wisely, then we get stuck in this trap state and we get bad reward forever and theres no way out. And so in the reinforcement learning setting, we don't know which of those, where the different arrows are pointing, until we try it. So, it's a little bit unfair to say what we should be doing is maximizing all possible reward values, right? Because it could be that we get ourselves into a position where we just can't any more. So here's what I advocate. Remember we talked in the bandit case about a mistake bound. The idea of the number of times you pull an arm that's not the right arm to pull, is bounded by some polynomial. It turns out that works really nicely in MDPs as well. So, the idea is that. Think about this state here in this trap example. It has only one action choice from the state. So, what's the right action to choose from here? Go to one action? Yeah. So, we're not wrong. True, true. Right? So, in fact. No matter what in this particular MDP, then total number of mistakes we could ever make is one. [LAUGH] We might make this wrong first action, and then we're making optimal actions thereafter. Again, optimal is relative to the state that we're in, but we're in the state that we're in. Oh, that's kind of deep. Okay. Thanks. So the criterion that I'm going to advocate for here is that what we're going to do is count the number of epsilon suboptimal action. That is to say the number of times that we're in a state, we choose an action and that action in that state was not epsilon close to the best action in that state. That we want that to be bounded by some polynomial, and let's say one over epsilon, one over delta, the number states n, and the number actions in each state k. Okay. [LAUGH] K so, oh, so yeah, the actions are like bandits. I like where this is going. A little bit, a little bit. We're going to bring it all together. And so this wasn't really an analysis of different criteria. This was basically me just trying to justify one criterion because it has some nice properties. It lets you deal with the fact that there can be trap states and that doesn't hurt us in a learning setting. And well, we're going to have to come up with an algorithm that's going to deal with this randomness, you can't explore randomly idea. Okay, so I think there should be a fourth criteria in here by the way, which was inspired by your listening to William Shatner saying. Which is the Marusha Criteria, which is we minimize the number of times we have to cheat. I do not remember recommending that you listen to William Shatner. But everyone should. Okay, but that's a separate issue. So minimize, in the Kobayashi Maru test, he reprogrammed it so that he could win, even though you couldn't win. So this is [LAUGH] kind of like that, in that we're defining winning. Well, it's not really like that. But it's defining winning to be doing the best thing given the state that you're in. And it's not actually penalizing you for having done something really stupid for having gotten into that state. As long as the total number of really stupid things is bounded. Huh, everything really does come back to Star Trek. Okay. I'll buy that. Three and four are the same. [LAUGH] Well, we can't cheat. You can't cheat in reinforcement learning. Oh, I beg to differ. Again, what we're trying to do now is explore a deterministic MDP and have low mistake bound. The number of actions that we take that are sub-optimal by more than epsilon is going to be, we need to bound it in terms of things. I don't know. Let's try a little experiment. You're going to be the reinforcement learner. Okay. And I'm going to be the environment, and tell you what's going on. So let's say we start of on this state here. Which action do you want to do, one or two? Do I get to cheat and look to see what the? [LAUGH] You could, but I think that somehow isn't going to be quite as pedagogical. Well, we'll learn a lesson for sure. I'll do one. I don't even know which one is one and which one is two anyway. I'll do one. Yeah, I get to pick that. That's the good news from my perspective. [SOUND] I'm sorry that cost you 8 points. No! Yes, but now we're here. So do you want to do one or two? Let's do one again. [SOUND] That's not bad. You got plus 2, and you didn't go anywhere. Just like grad school. Well, I've done one. I know what that is, so let's try two. Okay, [SOUND] plus 10, so that's really good, but now, you're some place new. Can I invent a time machine? Let's do one. Let's do one. Minus 6, ouch. Now you're here. Ugh. All right, let's do one. When I was in that other state and I got a plus 2, I could have just stayed there forever. In fact, that probably was the right thing to do, because I get infinite reward. Well it's discounted. I said by discount to be one. [LAUGH] No, you don't get to choose that. [LAUGH] That's not fair. Okay, so I'm sorry, I interrupted you. So you chose action one. So I'll give you one point for choosing action one. And I end up in another state. All right, well, let's do action one again. [SOUND] All right, so plus 1, and you get to be there again. If I just keep doing action one over and over again forever, I'll accumulate infinite reward. No, sorry. You'll get one over one minus gamma. Aah! Okay, I'll set gamma equal to one. [LAUGH] No, that's not one of the choices I was giving you. You could choose actions. I should have probably told you gamma. I guess that would only be fair, right? Well, I'm not sure it's going to matter just as long as gamma's less than one. All right, so, I now know something. You now know something. You know a lot of things. I do know a lot of things. I'm still in the same state. But I've got another action. And I'm going to think about it. Let's try action two. [SOUND] Plus 6, yay. And now you're at a state you've never been to before. This reminds me of Zork. Okay, I'll take action one. [LAUGH] It is like Zork. It's exactly like Zork. [SOUND] Hey, you're back to the state that you started in in the very beginning. I know minus 8 sucks. Actually, I now could basically do that cycle forever. The series of actions that you just did that take you around this loop. Yes? Mm-hm. Is that near optimal? Well, if it involves minus 8, probably not. Let's do action two. Good idea. [SOUND] All right, so you're back to another state you've been to before. And I got a minus 3 along the way which is better than the minus 8, but not better than the minus 8, plus 2 some number times, plus 10 except gamma by 0.9. Okay, so I know what action one gives me there, so let's do action two. [SOUND] Good job, you got a plus 4. And you're back here. Oh, I'm in a nice little state, too. I can do some other cycle. Yeah, and you tried action one from there. Oh, but I never tried action two from here, did I? Let's do action two. All right, so action two turns out to be identical to action one. Oh, okay. That's complicated. Cool, now you're back at the initial state, the state that you started in the beginning of time. And both actions that you have here, you've tried already. I can say something about what can happen after that point, right? I know, sort of, if I take action one, I get minus 8, plus 2 for some amount of time, plus 10 through the best I could. Minus 10. Oh, plus 10, you're right. Plus 10. Or I could do minus 3 and get immediately to this thing where I could go get plus 4s and plus 3s, which actually is better I think. Hm, so I should pick one of those. Well, okay, so here's an important point that we need to get to. So actually, you've explored the entire MDP, except for one state action pair, this one here. And so if this edge is like a self-loop with a reward of a million, then you have to find it. You have to get it, because otherwise you're not going to be near the optimal. Right, and I don't know that, so I have to get there. Right, but on the other hand, let's say that we know that the highest reward is 10, we have some bound, we'll call it Rmax, on the largest reward in any state, that could be really helpful, because it could be that I can get a value by acting in the MDP that I have now. Even if this were 10, even if it were its highest possible value with let's say a self-loop, we can still do better than that. This feels very minimax. Well, it's R and max. No, I mean, this feels like it's a tree and I could do pruning. What you're basically saying is I should prune things if I already know that I can't do any better than some other choice that I have. Yeah, that's right. So it has a boundish kind of feel to it. So what I'm suggesting is for any edge that we don't know, let's just be super optimistic and assume that it has value Rmax. Yeah, that makes a lot of sense. If I pretended that's true for everything that I don't know, then that would tell me what to do sort of in what the best worst case would be. And then, I obviously try to get to the place that I think is good because I'm optimistic. And if it turns out that I'm wrong, well as soon as I get there I will know that I was wrong. And I just keep doing that. All right, let's write down the steps, so that we don't have to hold them in our heads so long. All right, so 1, keep track of the MDP we have so far. We're saying that any unknown state action pair is an arm act self loop. We can get the maximum amount of reward and we can get back to that state and get it again and again. So clearly, there's no policy that's going to be better than that from that state. Right. So then we're going to solve the MDP, again, with the assumption that anything we don't know is Rmax. So solve the MDP assuming 1 and 2. And then we go back to 1, essentially, right? Well, you just execute the optimal policy. Like that. Yeah, and then as we take those action we may discover that some of the 8 action pairs were wrong, but we'll just fix that by doing 1. And we'll just solve the new MDP. Good, all right, so there's a couple of things we need to convince ourselves about this. One is that we get Epsilon optimal reward. Yeah. Another one is that the number of mistakes that it makes is going to be relatively small. I think the second one's probably pretty easy, right? Well, let's go through it. All right. So let's talk through the analysis of this algorithm in determine its MDPs. I'm going to call the algorithm RMAX because anything we don't know we assume is RMAX. It's not a great name, I guess, but it is the name that it was given by Brafman and Tennenholtz when they first proposed and analyzed it. I guess it had another name, too. RMIN? [LAUGH] No. because the algorithm actually is directly related to an algorithm that Andrew Moore proposed but didn't have an analysis for it. So it could've been RMOORE? [LAUGH] I see. RMAX. And even more than that is RMOORE. Mm-hm. All right. So let's just go with that. So here's the algorithm. I just copied it from the previous slide. And there's some things that we can figure out about this, like once we've actually visited all the state action pairs in a deterministic MDP. Then this algorithm is going to be perfect. It's never going to make any more mistakes after that. Right? Do you see that? Because in step three, it solves the MDP which is in this case now the actual optimal policy and then any action that it takes from that policy thereafter is going to be an optimal action not a mistake. So this is great. If it's the case that we eventually visit all of the edges in the MDP, then we will no more mistakes after that point. How do you count mistakes, though? Do you count mistakes on a per state basis? Yeah, a mistake in this case means I'm in a state, I take an action, and that action wasn't near optimal from that state. From that state, right. All right, but we need a couple more things here, right? So it could be that we never actually know the whole MDP. That's possible. For example, if you're in Windows trap states. What happens if you stop visiting unknown states, right? So what would that mean? In the case of a deterministic MDP, it would mean that we were following some policy that is taking us on edges that we know about. So nothing new ever happens. Nothing new is ever learned. But there could be other unknown edges elsewhere in the MDP that we are not visiting. So could we be making a mistake as we go around this loop over and over and over again. If we can get to that state from any of those states, then we would end up going to that state, because we assume its the best thing that we could do. Not necessarily. So here's a tiny little example, so let' say we're in this state here that we know has a self loop of plus five. And the discount factor is 0.5. Okay. So what's the value of actually going around this self loop over and over again? Five times 1 over 1 times 0.5, so 10. Good, and let's say the Rmax is 10. And we know that the only other action that we can take from this state goes to this state which has only one action. And it takes us to this state and we have no idea what happens from there. So what do we assume? Well, we assume that you can always stay there forever doing 10. And the value of that at this state would be 20. Right, for the same reasons. And the value of that at this state would be 10. And the value of this action At this state would be five. So from this state, from this bottom state here. Even if this has the maximum possible reward in the self loop, the most value we can get out of that, from this state, is five. So, double discounted, ten loop. So that means that we know that the optimal action from this state is this plus 5. It's going to be better than even the best that could possibly be out there, so we're good. It might be lower, it could be that this unknown thing is even lower, but that only strengthens our case, that we were doing the right thing. So the point is that we can actually be in a loop that uncovers no new unknown edges, it never visits something that we haven't seen. Mm-hm. But we still are not making mistakes and we know we're not making mistakes because, if we're choosing this path over something that uses these optimistic estimates or these optimistic assumptions. Then, we're doing at least as well as the best we could do. Sure, I'll buy that. I mean, although, that takes advantage of something that's always bothered me. What's that? You set gamma to be one half right? Yeah, it could be bigger, I just didn't want to do the math. No, I'm with you. So, you set gamma to be equal to one half. And so, with the gamma one, that's true. But in reality, when we keep track of whether an algorithm is doing well or not, we don't actually discount the rewards that we see. We just add them up. So we are not going to go down that philosophical rabbit hole. We are going to accept discount factors for now. I agree with you that discount factors are a little bit hard to swallow. If we don't though, I don't think we can make any progress. No I agree. It does make sense. It just seemed like it was worth mentioning. That you have to take the discount factors seriously. That the future really is going to be discounted from any given point where you are. Because otherwise, it actually makes sense to get to that 10. Yes. Yes, you're absolutely right. If we don't have a notion of a discount or the discount is really, really high, then it could be that the highest this could be is going to give us better than the repeated plus 5. So yes, we are definitely leaning hard on the notion of a discount factor. Right. And in fact, if the discount were 1 or essentially 1 and we know that our max is actually right. Whatever our max is, is right. You basically, unless you encounter the actual self-loop, plus ten or Rmax. Then you have to explore the entire MDP, you must. It'll just sort of force you to. Well, okay, we'll do that as step three of the analysis. So let me finish this step two first. So, what I'm claiming in step two here is that, if we end up choosing a policy that is not based on knowing all of the transitions that there are still some unknown transitions out there, but we end up taking a policy that puts us into a loop where we never visit any of those, then that still has to be an optimal loop, we're not making any mistakes. It could be that we could do as well by uncovering those other values but we can't do any better because we're assuming in step two that anything that we don't know has the largest possible value and so, if it has the largest possible value and we still don't want to go there, then we're doing at least as well as optimal. Yeah, that's kind of cute. All right, but now, we need step three. So now, step three says, okay, we solve the MDP, this sort of imagined MDP that has these optimistic estimates in it, and let's say that that actually takes us to a state action pair that we haven't been to before, and we learn its value. Mm-hm. How many mistakes might we make on route to that? So, when we actually get to that state, and traverse an edge that we didn't know about, that could have been a mistake. Right. So, that's definitely, one. And how many mistakes might we make seeking out the state that actually turned out to be a really bad state? From a particular state? I guess, the number of states. Yes, right. So, the number of transitions that we might take just to find out that we were wrong, could be as big as N. N minus one or something like that but yeah. Yeah, so, this isn't a deterministic M.B.P. under to any state our path is going to be either bounded by N, or if it's bigger than N, it's infinity because we're actually looping it without actually getting to that other place. Right. So the most it can be is in N right? Mm hm. All right. So, given that, we might make N mistakes to find out something new, to learn something new, to visit a state action pair that we've never seen before. How many times can that happen? How many times can it be that there's a policy that brings us to a state action pair that we haven't been to before? How many times can that be? Yeah, so each time. This is sort of step three. Maybe I should write in English. All right, so what this step three is, is that what our our max algorithm tells us to do is to execute a path that actually brings us to an unknown state action pair. Then, we go traverse that edge find out what it really is and generate a new policy. So, the number of steps that it might take to get to that new state action pair is N. Right. And the number of times that we might discover a new state action pair is what? N times the number of actions. n times the number of actions because that's the total number of unknown things in the graph to begin with. Right. Right, so we're exploring the MDP and each time we visit state and take an action we haven't taken before that's going to reduce one from the count. So, the largest that count is in times K and so, the total number of mistakes that this algorithm can make is bounded by N squared K. Hey, that's polynomial. N, N, and K. It is and it also doesn't depend on Epsilon, and it also doesn't depend on Delta. That's even better. So, wait, if it doesn't depend on Epsilon, and it doesn't depend on Delta, then it means it's polynomial in those things. Sure. Yes, it's a very simple polynomial in those things. So, then we're polynomial in everything. Yeah. Yes, so, this is an efficient exploration algorithm for deterministic MDP's, right. All it's saying is, whenever we don't know something, assume it's awesome and behave accordingly. If we learn something new. Make note of it. And so, the number of times that we take a step that might not be the best thing to do for that state, it can't be any bigger than N squared K. That's beautiful. Yeah, it's pretty cool, this kind of analysis comes from a paper that Koenig wrote with Reed Simmons where they actually analyzed Q learning in this kind of setting and they got a worse bound like N cubed times K. But this algorithm is sort of pretty simple, and gives us a nice bound and it sort of does what you want it to do. It's cool, I will point out that in cube case, also a polynomial. Yes, yes, but it's a slightly worse polynomial, you're absolutely right. I mean, he showed that Q learning has a polynomial bound in these kinds of environments. If you set up the rewards correctly, and if you set up the initial value functions correctly. All right, so, that gets us through stochastic MDPs that have no state, and state MDPs that have no stochastic, and the last thing we want to dive into is, what happens if we do something like our max except for on stochastic MDP's. Well, we still have another case, we have to worry about the case where we have no state and we have no stochastic's. Ahh. So, okay, let's do that one really fast. All right. So, you're talking about an environment has one state and all the actions are deterministic. They give you a deterministic reward and return you back to that state. Right. So, this is essentially deterministic bandit problem. Right. So, how many mistakes might, like what would be an algorithm for this and how many mistakes would it make? The dumbest one, or the simplest one would just be taking every action once, and then you know everything. Right, and each of those, so K minus 1 of those are mistakes possibly. And then, we only know that we just teach using the optimal action at their after. Yeah, you could do a little bit better view assume that everything is our max because as you come across our RMX, you ca stop. Very good, yes, right, if you end up finding along the way, an action that actually gives you the full value, the full RMX value, then we don't have to explore the rest of them because they're not going to be any better. Yeah, okay, so that gives us. It doesn't give us a better bound, but it does probably give us a better performance if we keep track of that. So, yes, so this is a silly case. [LAUGH] Well, but it does allow us to say we solved three of the four. Great. All right, so let's dive into number four. So that's pretty cool. So we have an upper bound, that says that our algorithm is not going to take any longer than n squared k steps to explore a deterministic MBP. And a reasonable question to ask is, is that good or bad? And so what I want to argue next is that there's a lower bound, saying that no algorithm can do better than n squared k, and therefore algorithms actually doing very well. No. Well, as well as one can do. As well as one can do, right. Which may help you well enough. Now when in fact, really when you're trying to learn in this kinds of domains, you don't really want to make worst case assumptions. So this is under a worst case assumptions, where there's really no structure in the MBP. How many mistakes might we make en route to finding the optimal policy. Okay. All right, so let me draw a little MBP. So here's an MBP that we can call a combination lock. So the basic idea of this is that we've got some kind of n state sequence and two actions, which I drew as purple and blue here. And each state has a purple action and a blue action. And one of them goes back to this reset state, this state number one, while the other one advances up the chain one step. I see. And so we need to explore this whole thing say, because there's some kind of giant reward over here, and every step that we're not making along this chain and then looping around here is a mistake. So the question is, how many mistakes might we make en route to finding this plus 100? So let's say we start off in this state number one. So what would our max do in this case, Charles? It would pick one of them, because they look the same. Right, so it's in this state, it doesn't know anything about the outgoing actions. And it knows that there is purple and blue, and it picks one. And I tried to be kind of random about the way which one would go forward and which one would go backwards so there's really nothing you can do other than guess. If you guess wrong, then you take this blue action and end up back in state one. At which point you would say, I know what that does, let me try the purple action. And now it advances us to the next state. Right. All right, so what happens from here? Well, you don't know what time it's going to happen next so you pick one of them. Right. So if we choose the purple one we get reset back to the beginning, at which point we can kind of march up the chain, and, you know, get back to where we have some unknown information and then try blue. But basically each time we're exploring one of these states it's taking us, well, if you think of it as state i. To explore state i we take one action, which resets us back to the beginning and it takes us i steps to get back to i. And then one more to get to the new state i +1. So the total number of steps that we're taking here is going to be 1 + 2 + 3 + 4, all the way up to n. Right, that looks very n squared to me. Which is exactly right, which is n squarish, as some people say. And so this total number of steps we take, it's not literally n squared, but it's some constant times n squared. And you can't really do better than that. Now in this particular case, we really wanted to prove a lower bound that was going to relate it to k times n squared, where k is the number of actions, here the number of actions was two. Do you think the same example actually kind of scales up for greater and greater k? How would you change this example so that it would basically force us to do k actions times n squared. Well, I can just make all but one of them send you back. Yeah, okay, so let's think about what that would do. So if we had k actions, one of them sent us forward and we have no idea which one and all the other ones send us back to the beginning. Then what's going to happen is we're going to have each time we explore state i, we're actually going to get reset and have to take n steps k times. Right. So for each of the n states, essentially, we're going to have to explore all the actions and for all but one of them it's going to be n steps to get back to where we are exploring again. And look at that! That looks like n squared k. Or Kn squared. Yeah, I just reordered it so that it looked more like the thing on the top. If you remember from our last machine learning class, everything got us back to k and n. And there you go. Oh! Nice. It's all the same. It's always k's nearest neighbors, but in this case, it's not. It's k's farthest neighbors. Well, yeah, I guess there's some truth in that. That each time we take an action that we don't know about, we're getting unlucky, and we're having to take a very long trail to get back to where we started. But no, I'm just saying k nearest Neighbors is a particular kind of algorithm. And here, n doesn't stand for neighbor here, right? It stands for number of states. I prefer to think that n stands for neighbor in this case. Great, okay, then I think we've got this one under control. So that's the argument that I wanted to make there is that we really do have to take kn squared in the worst case in a deterministic NBP, to figure out what's going on and stop making mistakes. Yeah, and there's no error on it, because this is like, well, it really is a combination lock. The way you draw it it's a random binary string that gets you to the end. Unless you get extremely lucky, there's no way to figure out what it is without trying all the combinations, sort of. Well, it's not all the combinations. So combination lock in some ways is actually not a great name here, so in particular, what makes it a combination lock is to make it across this whole chain of end things. You have to choose the right color and sequence. You have to choose, in this case, purple, blue, purple, purple, blue, blue, blue, blue, blue. So you have to kind of crack the lock. Right. In a real combination lock, if you get any of these wrong, at the end of it, you don't get to find out which ones were right and which ones were wrong. You get reset back to the beginning and you just have to start again. So you really end up taking like number of actions to the number of states. Right. Different combinations. Here we're lucky enough to actually know, hey, the first i parts of our combination are correct, and we're just guessing the ith one. So really, it's a sequential combination lock. Yeah, which is not very secure from a, I don't know, security standpoint. Well, in particular you can solve it in n squared time instead of k to the n time. Right, so n squared doesn't sound so bad if you're comparing it to k to the n. Right. But anyway, [LAUGH] the point is that we can't get around that. And so that's really what I wanted you to do. So the next thing I want to talk about is combining this idea with the bandit idea that we talked about earlier, and get an algorithm for general MDPs. Okay. So we want to do efficient exploration in general stochastic MDPs. And so we're going to need to combine the two ideas that we've been talking about. Wait, what were the two ideas we were talking about? Well in the context of bandits, we talked about stochastic processes. And the trick that we used was to use something like a Hoeffding bound. We just keep repeating the stochastic thing until we have enough data, so that we can make accurate estimates. Oh, okay. And then we talked about sequential domains that are deterministic. And the main trick that we used there was this r max trick that said, if there's a state action pair that we don't know about, assume it's really, really good. Okay. We can put these two ideas together and get a stochastic version of Rmax. Okay, so we'll have Hoeffding regions. Yes. Good, good, good. [LAUGH] We're still going to use Hoeffding bounds on the individual state action pairs. Okay. In fact, regions probably means state action pairs here. So I was right. Yes, you were right. Good. Well, then I think we're done here. All right, so let's write down what this algorithm looks like. We're going to make general Rmax algorithm and it's going to be really simple because it's going to be an awful lot like the Rmax algorithm we already looked at. Good. So I copied over the Rmax algorithm that we had before. And it went like this, we're going to keep track of the MDP. Which is to say we're going to have an estimate of the transitions and rewards for all the state action pairs. Any unknown state action pair, we're going to assume has the reward of Rmax and then a self loop, so we can just get like aahh for a long time. Then we actually build that MDP, we solve that MDP and we take an action from the optimal policy with respect to that MDP that we solve. The only difference is going to be this notion of unknown. And the idea is that we're going to have some parameter. Call it C, that state action pair is unknown if it's been visited or if we've tried it out fewer than C times. Okay, is that C different for every state action pair? That's a good question. So I want to first give the parametrized algorithm. Then what we have to do is, if we're going to say that this algorithm is efficient, we have to derive some value for C and show that it's not too big and not too small but- Just right. Just right, yeah. So the way that we're defining unknown here is that we have some parameter C. And we consider a state-action pair SA unknown if it was tried fewer than c times. After that point then, we switch over to the maximum likelihood estimate. So, we've tried it c times, and we saw we went from state s with action a to some state, S 53, so if we did that some number of times. If we only did that once, then we're going to estimate a probability of 1 over c for getting that transition. So this gives us a new estimate of the transitions and rewards based on the data that we've actually gathered. Okay. So I just want to point out that this is the Rmax algorithm we used in the deterministic case, and this very much is related to the hufting bound estimate we used in the stochastic non-sequential case in the bandit case. Which is grafting the two things together. All right. So there's going to be a couple key ideas to make this work. And one of them is the simulation lemma. The idea of the simulation lemma is that if we have a pretty good estimate of the real MDP, then optimizing our rewards in that estimate is going to be pretty good in the real MDP. This is a lemma about simulations, not a simulation about a lemma. That's right. It's not a simulated lemma. It's a lemma about simulation, right? And so the idea is that we've got transitions and rewards that are off by alpha or less, and we want to think about what happens if we adopt some policy pi. Any policy pi. We want to compare the value we get for following pi in MDP 1. That has transition function T1 and reward function R1, to the value that we get, return that we get for following policy pi in mdp V2 which has transition function T2 and reward function R2. Okay. And if those are going to be near each other given that T1 is near T2 and R1 is near R2, then that's going to give us the ability to have accurate simulations, right. We can use our model of the MVP to stimulate. What's going to happen in the real MVP. And just to be concrete, this is what I mean by the transition functions, reward functions not being too different, than we have this value alpha. And if we take the maximum over all state action x state triples, that the probability assigned by transition function T1 and the probability assigned by transaction function T2 are not different by more than alpha. And, same thing with the rewards. So that's a little weird. How so? Well, because the transition model is probability, so that can't ever be off by more than one. Sure. And the rewards can be gigantic numbers that can be off by billions. Sure. Billions and billions. So, it just seems a little weird. You feel like it should be two different alphas. Yeah, alpha T and alpha R, but maybe not. Yeah, I mean I didn't want to proliferate variables when we didn't really need to, but you're right. The scale of these could be very different. A lot of times in the proofs that people use for these kinds of MBPs. They first assume that rewards are all in the range zero to one, also. It doesn't change the fact that you still might want a different alpha for the two cases, but it does at least get the scale More approximately correctly. Okay, so then without loss of generality, assume that all your rewards are between minus one and one. Or zero and one. Or zero and one. Why not? Because it's all the same MDP. [LAUGH] Yeah, they look more like probabilities or something. Okay, cool. So, without loss of generality assume that your rewards are between zero and one and your transition probabilities which Would have to be here between zero and one so that we can use the same alpha and everything kind of works out. Yeah. Again, it still might be worth having different alphas. But yeah, you're right. Again, it puts us in the right ball park. Okay. So this is a ball park simulation. I'm perfectly fine with that. [LAUGH] Now we're out of the ballpark. So that's either a vacuum cleaner or a metal detector, or it's a gigantic baseball being knocked out of a park. It was supposed to be a ballpark, but it looks to me more like some kind of terrible Cruel alien. [LAUGH] So I With I don't know. Head issues. Anyway, the point is that, it's.it's alpha close to a reasonable pictures. And that's alpha. Okay. I actually just started going through the process of proving this and it was an awful lot like stuff that we've already done, in terms of taking bellman equations minus other bellman equations. So let me just jump to the punch line. So if you push the math all the way through, what you find is that all the transitions and rewards are alpha close to each other, then the difference in the values that you get for following the policy is going to be no more than G over one minus gamma, where G is that alpha. That error amount, plus a discounted version of that error amount, times the maximum reward that we get for forever. Right, so this is actually the maximum value that you can get. If you get the maximum reward over and over and over again. Okay. So, this just gives a quantity. And it has some properties that you'd expect, like the fact that if we estimate the parameters perfectly and alpha is zero, then this is zero plus zero. And then, this is zero over one minus gamma, which zero. So, the two value functions are actually identical. But they fall off from each other. So, if this alpha is something not zero, some slightly bigger than zero, then these value functions are going to be off by a bit more. So ultimately what we're going to want is, we want to know how accurately do we have to estimate the transition and rewards? What should alpha be so that the difference between the value that we get by running in our simulation of the model, and the value that we get running in the actual MDP, are really small, like epsilon close. So I think all we need to do, actually, is solve for alpha, in terms of epsilon, to be able to see how accurately we need to do our estimates to make sure that, ultimately, our values are going to be close. That makes sense to me. So this is me now solving for Alpha, and we end up with this expression here, [SOUND] which says that, that we need to take the epsilon that we want, how close we want the values to be, and then transform it in various ways. Actually make it a heck of a lot smaller, and that gives us how accurate we need the estimate to the transitions and rewards to be. So we know that we're going to be that close in terms of the value. Sure, that's intuitively obvious, even to the most casual observer. Well, okay, you're probably being sarcastic. [LAUGH] Possibly. So it's maybe not completely obvious but I, you know, I did algebra here so it's, it's not, it's not deep but it's not, it's not intuitively obvious why it should have this form. In fact, usually, I don't know. Epsilons are like one over. Oh, but I guess we're trying to define a quantity, alpha, which is like an epsilon quantity. So it wouldn't make sense for it to be a giant number. It should be a tiny, tiny number. But, yeah. No, no, you're right. It's not completely obvious. But the point. The point that I want to make with this is they're polynomially related to each other. Mm-hm. Right. So if we have RMAX, and one over one minus gamma, and one over epsilon, then the kind of alpha that we get out is going to be polynomially related to the other things. Okay. So that's reassuring. I mean, if you actually work out what these numbers are, they're pretty scary. But, you know, we're, we're trying to not let ourselves be scared of that. The important thing though is that alpha's not really a parameter of RMAX, the RMAX algorithm. Right? What was the parameter that we had when we talked about the algorithm. [FOREIGN] [FOREIGN] [LAUGH] [FOREIGN] So, that is to say, what we want is a parameter that says how many times do we have to try a state action pair, before we are confident that our estimate of that transition probability is better than alpha? So, how are we going to get that last piece? How are we going to turn alpha into C? More algebra? Yeah, but specifically, there's bounds that come into play here. So now, we've actually turned it into the same kind of problem as the bandit problems that we were talking about, right? That we need to know, how many times do we pull a bandit arm before your estimate in the mean is sufficiently close? And that is the Hoeffding bound. Right. You did say we were going to use Hoeffding bounds. Yeah, and the union bound. Oh, yeah. We want to simultaneously make this alpha close estimate hold across all state action pairs. So we need to make the c big enough, so that the probability that it fails in any one of them is sufficiently small. But it really is exactly the same argument that we did when we were talking about bandits. Okay. And, sure. So, every. Every state action pair is like a bandit, and you have to pull it a certain number of times. It's like an estimate of an arm, right, and we need all the arm estimates to be accurate if we want the arm that we eventually pick to be near the optimal. Right, okay. No, that makes That, you know? I can see the analogy. I can see the metaphor. I can see the simile. [LAUGH] All right, perfect. So, okay. There's one more piece to this story, and then I think we're doing with exploration. Yay, we'd be done exploring exploration. That sounded sort of sad. [SOUND] So the last little piece to talk about is the explore or exploit lemma. So the idea of the explore or exploit lemma is it gives us the peace of the argument that we did when we were doing deterministic Rmax, which basically said if we are in some kind of a loop going around and around and around, and we're getting optimal reward. If not, then we're going to quickly reach an unknown state, and we're going to learn something. Mm-hm. And so the corresponding version of this in a stochastic domain. Let's say we're running Rmax, ad all the transitions are either accurately estimated or we mark them as unknown, and we have it in our Rmax transition for those state action pairs. Then if we take that MDP and optimize it, that optimal policy is either near optimal. In other words, we're not making any mistakes. The reward that we're getting as we travel along this optimal policy is essentially optimal, or we're actually going to make it to some unknown state. So Rmax is marked state, relatively quickly in polynomial number of steps. We're going to reach this new state, and we're going to learn something from that. And so this property ends up being true throughout the execution of Rmax. I'm sorry, literally that property or that property like with high probability. Right, with high probability, right. So things can always fail. We set up our union bound in such a way to make sure that the probability of failure is sufficiently small. Given that it hasn't failed, then this will actually be true. And if it's true, then what's going to be happening is we're either going to be doing the right thing, or we're going to quickly bump into a new state and learn something new and continue. And the number of times that we can learn something new is going to be bounded by the number of states times the number of actions. Mm-hm. And the number of steps before we learn something new is bounded by a polynomial in the various quantities. And that's, I think, all we need, right? So we can't actually run for a very long time without either learning something new or being near optimal. Huh. I feel like there's some metaphor for life here. Hm. How would that go? I'm not sure something about grad school. Yeah you can't always just go there. I'm pretty sure you can and I proved it for many years. Well I don't know if this is related to grad school but it is sort of a key property of these algorithms that's just basically trying to handle the case that says by virtue of the fact that we make the unknown stuff optimistic. We do the right thing right? So we're either learning quickly or we're being near optimal. And so what would be bad is if there was some kind of hole in between these two cases that says, well we're kind of stuck doing something sub optimal for a long time, but we don't know it and we don't learn that we are doing something wrong. That's the bad case. And what this lemma is about is showing that, in fact, the bad case doesn't happen there. Either we're learning something or we're doing very well. Right. And that could only not happen if we were incredibly and extremely unlucky. But if we were incredibly and extremely unlucky, then you couldn't learn anything anyway. Yeah. Right, well, that's back to this sort of PAC notion that it's probably going to work. [LAUGH] And we can actually put- At least approximately. Yeah, that's right. And working is here related to approximately. Mm-hm. So that was really all I wanted to get at with exploring explorations. This idea that even in general MDP's we have a way of organizing our learning so that we can make guarantees on how close to optimal we are and how quickly we get there. But wait a minute, you didn't tell me what C was. Is C just quickly? Oh, this C is correct. No I meant the original C that was a parameter. Oh, well I did not write down a formula for that. But it follows the same analysis as what we did when we wrote down the value of C for bandits. Oh, okay. So, good enough? We could make that a homework problem. Sure. I mean that's what we usually do. [LAUGH] I want to write something down. [LAUGH] Okay. All right, done. So what did we learn about exploring exploration, Charles? Well, we actually learned a lot, but really I think you can summarize it in just a couple things. We learned about bandits, which is really about stochasticity and randomness. And in particular, we learned about how we can estimate what we know using Hoeffding bounds and maybe some union bounds. So, yeah. In particular we employed Hoeffding bound, u nion bound to convince ourselves that we have a sufficiently accurate estimate that we can get near optimal reward in these kind of non-sequential but stochastic decision problems. Right, and that's really important because a lot of the time you don't know what to do because you're not certain, or how certain you really are about what you know. And what the Hoefding and union bounds tell us is how certain we really are and so we can know when we're certain enough. Good, all right so we have a stochastic world, we want to learn how it works so that we can get near optimal reward. So we can optimize. Right, so that was the first thing. And then the second thing is we learned, well, we learned about Rmax, really, and how Rmax works specifically with deterministic MDPs. Right, and so that was about optimism in the face of uncertainty and how it would cause us to explore or discover new stuff about the world, kind of by planning ahead and trying to get to states where new information could be gained. Right, well what it really says, it's really, we believe the grass is always greener until we get there and learn otherwise. All right, great. This let us deal with sequential decision making, this let us deal with stochastic decision making. And then we combine them. Right, where we used the bandit idea to estimate the noisy parameters, the transition probabilities and so forth, and we used the Rmax idea to make sure that we visited things enough so that we could get those accurate estimates. Right, so Rmax basically remained Rmax, and the bandit stuff helped us with the transition probabilities, so that we knew when we actually believed them. So, they were kind of, this isn't really true, but they were kind of deterministic enough. So we knew enough to know what the transition model is, and once you know what the transition model is you can just kind of go from there, because then you can just estimate the rewards and do the right thing. Now this notion of, the way that learning happens in the bandit here where we actually kind of distinguish between known and unknown, can actually be generalized even beyond the kind of general MDPs that we're talking about here. And I don't think we're going to get a chance to talk about this, but the idea of KWIK learning is learning transition probabilities using methods that know what they know. And if it can distinguish between known and unknown, it can associate optimism with the unknown and then we can make guarantees on how uniquely and efficiently near optimal behavior can be learned. So this is kind of a learning framework that my students and I have been studying to try to generalize beyond learning in tabular MDPs to be able to generalize between transition probabilities and different parts of the MDP. Well that all makes sense to me. So then the only question is, how does this help us do practical reinforcement learning? All right, so that pretty much ends this. [LAUGH] Well, okay, so just briefly, I find that Rmax is actually a really effective algorithm to use in practice. We don't set C, we don't set that parameter of how often we need to see things to be what the theory says we should set it to. We usually set it to something much, much smaller than that. And we tried it by hand, strike a trade off between making it too small, in which case it might actually not learn near optimal behavior, and making it too big, in which case the learner actually has to sniff around and visit many, many, many, many state action pairs over and over again. So, Rmax itself does seems to be a pretty effective algorithm. It makes very good use of the data. It's just we can't quite use it in the theoretically specified way. Okay, well that's good enough for me. Seems pretty practical. Okay, so we're done. So, are we done with the class? What's next? We have more stuff! We have what's next, next. Oh cool, well I look forward to that. Hey, Charles. Hey, Michael. How's it going? How are you doing, Charles? I'm doing great. Guess who stopped by the studio this morning? Our T.A. from last year? Yes, Pushkar stopped by, and he brought me ice cream which I am eating. Good for you. Mm-hm, just like old times. All the times. [LAUGH] Not just the old times. And that's fair. So what are we going to talk about today. Well actually kind of in that same vain this sort of idea that every time that we record, push cart seems Fair, that's fair. So what are we going to talk about today? Well, actually kind of in that same vein. The sort of idea that every time that we record, push cart seems to bring you ice cream is an example of generalization. Well done. Thanks, I can make predictions about what might happen next time we record together based on generalizing from the past times that we've been together. Even though it's kind of a different state. And so, the idea of generalization is going to be to leveraged learning in states where we've been to try to make predictions about states where we haven't been or haven't been very often. Well that sounds interesting so this will allow us to teach Xs how to swim for example looking at your pictures. You don't like my pictures. The idea of the picture here is that we've got actual data points these five Xs. But then we use this notion of generalization to kind of make predictions at places where we don't have data. So it's machine learning. Yes, actually, very good point. Cool. All right. Well, I look forward to it. So this is the machine learning part, or the generalization part of reinforcement learning. Excellent. So first, to be able to talk about generalization. It helps to think about states as actually being collections of features instead of just unanalyzed blobs. So we're no longer to refer to states as state 17 and state 19? We can. But I think it's helpful actually to give them additional information like these features. So here's a concrete example to make this, try to make some sense. Do you remember our taxi problem that we talked about before? How could I forget. In this particular environment, there was something like, I don't know, 500 states and in standard q learning type approaches, we would just think of all those different states as being completely different from one another. But we can also imagine tagging each of the states with some particular value. Like whether or not well, we don't really have fuel when we talked about it, but whether we're low on fuel. What is the X coordinate of the taxi cab, this thing here. What's the Y coordinate the taxi cab? Is the taxi cab currently near a wall? Say to the you know to some direction, north, south, east, west. Each of these different features give us a little bit of a handle on the states and how they relate to each other right. So if we have this state that we've got right here and we say move the passenger to some other position, we're still in a related state. The taxi hasn't moved and it would be nice to be able to capture that and so this notion of features gives us that idea. Does that makes sense? So that's a lot like how we talked about machine learning and supervised learning in our other machine learning class. Exactly. Yeah, in fact we had words for these things. We talked about the, boy what were those words? It's sort of the notion that different machine learning representations cause different things to be similar to each other. Right. And there are lots of different ways that we could represent the state space other than whatever the different 500 states are. You have some of them up here, but we could have the coordinates of where the passenger is, not just the coordinates of where the taxi is. We could have things represented and sort of an egocentric way. What's the, I don't know, Manhattan distance between a passenger and where the passenger wants to end up. There are all kinds of things that we could the ways we could describe things and some of those things make some states look more similar than other representations would. I think I remember the term. I think it was inductive bias. Inductive bias was something that referred to the way algorithms preferred one solution over another. But I think you're right, though, because different features sort of create different kind of inductive biases by making some states look more similar. So, for example, if we had this egocentric notion of how far away a person is from a taxi, then so long as a person is say two steps away from a taxi, any state where a person is two steps away from a taxi looks the same or looks very similar. In fact, if you look at this one you have now if you just drew that right the red circle is close to the orange square. If you were to move both the red circle and the orange square up, those two states would still be very similar because they're still two away from one another or have two in between them. But even though I don't move the red circle, if I take the orange square and move it to the right, then that's less similar because they're farther apart. Okay, and they were ignoring walls the way he described it right? Right, I'm ignoring walls. We move both of these up by two we're so yeah we're two apart but it does feel kind of different to go around the walls. And if it's different or not different depends upon the state features that you use. Okay do we have a name for that or. Is inductive bias an okay stand in? It's a representation, right? It's a representation. Some representations are better for some things than others and you can learn differently depending upon which features you choose. Okay. For example, here's a great feature. For every state, here's the set of actions you should take to accomplish the goal. That's one representation that makes learning really easy. Yes, good point. So let's talk about what we're actually going to learn. Okay. Now that I've given some kind of motivation for this notion of generalization, it might be worth talking about what exactly we are trying to generalize between different learning opportunities. So what are the functions that we're actually making use of when we're doing reinforcement learning? What do you mean? When we're learning we're actually generally learning some mapping right. So this is true in reinforcement learning in general. There's some mapping from input to output. I see what you mean. So, well, the whole goal, of course, is to learn a policy, which is a mapping from states to action. Great. Okay, that seems like a really good place to start. So policy maps states to actions. But that's usually hard. So we usually learn some kind of intermediate thing, like a value function. Well, let's just stick with policy here for a second. So in terms of the policy. Generalization would mean that to the extent that similar states have similar actions. Then we could learn what the right action is for some states and then guess what the right action is for other states, using some kind of I don't know. Function approximation, some kind of supervised learning. Right, so if you're in a state where an anvil is about to hit you on the head, you should probably jump out of the way. That's probably true for every state where an anvil's about to hit you in the head. Even if you never seen the specific details of that state before. Yeah, I mean it could matter whether or not there also is a shark tank right in front of you. So when you jump out of the way of the anvil, you actually jump into the shark tank. But yeah, I think that's exactly right. That we want to be able to say across lots and lots and lots of related states, we could be using the same kind of action. But I think what you're pointing out is that we don't know how to learn the policy directly. Anyway, when we're talking about these kinds of reinforcement learning algorithms. So, we usually learn a kind of a stand in for it. And my favorite one is the value function, which is where you are now mapping states to some number to r. Right. So we can map, for example, states and actions to the estimated return. That's what the q function does and that's another function that we could be trying to learn and this is another function where generalization might be an option, right. So to the extent that similar state action pairs have similar returns, again, because if an anvil's about to hit you on the head. Probably that's a bad state, you're in a bad situation across all the different kinds of states where handles about to hit you in the head. There we go that you. You should draw a picture of me standing off to the side to your left. I have the world's tallest neck. Yeah. So say I'm in a state where enamels about to hit me on the head, there's lots of other details that are sort of not that important in predicting what the estimated returns going to be. So you even look kind of concerned. That's supposed to be me? Yeah you're tall enough. Okay, fair enough. So this is another place where we could be doing function approximation, is in the actual q function itself. And there's a third one that I think is actually really important and interesting, and that is the model. To the extent that you can actually do a good job of predicting next states, transitions for related states. Then you could actually be doing function approximation or generalization in the model as well. Right. In fact, that's probably very, very important often. Yes. Not only is it important but it's actually often really natural place to do it because when you're learning a model right so what is it about learning a model? You're in some state, you take some action, and you observe what net stage you're in. These are actually supervised examples of transitions. So unlike learning the policy or learning the value function, when you're trying to learn the model, you're actually getting standard supervised examples. So that being said using function approximation models it has been done in the field but it's not that well understood. In particular, you need models that can actually predict many many steps ahead to be able to be used effectively for planning and decision making. Mostly what researchers have focused on is function approximation in the value function, the generalization of the value function. So that's what we're going to look at in the rest of this lesson. Okay cool, look forward to it. So I think it will help if we think through what the basic update rule is when you're doing reinforcement learning,or cue learning. And then to try to adapt to this notion that are we're representing our value function using some kind of function approximater. All right. So here's a way to think about this, we've got Q function that we're going to try to represent as some function of a set of parameters, or weights. Maybe let's say one set of parameters for each action. Sure. And what F is doing is taking a state and translating it into a set of features. We're going to feed those features and the parameters of the function into something called F. And what will come out of that will be the Q value. So this is intended to be fairly general. And we'll talk about some specific examples of this in a moment. But let's just imagine that when Q is being represented by some kind of function approximater, and here's how Q learning works. We have an experienced couple that comes in, state, action, reward next, state and then we do an update. Now what's the update usually? We try to move the the value of that state action pair a little bit towards the reward plus the discounted value of the next state S prime. So, that's the bellman equation kind of hidden in here and the difference between that and the current Q value is the TD error. And what do we want to do with the TD error? The TD error tells us is our current prediction too high, too low, just right. You know does it like porridge? And what we want to do is change. [LAUGH] That's the three bears I guess. But the fact that matter is we actually need to know the kind of the direction that we should be going. Are we too high or too low and we want to move the parameters that represent the Q function, you know so a given weight say W super a sub i. We want to move it a little bit, this is a learning rate alpha, in the direction of that TD Error. But how much do we move the weight, depends on how that weight actually influences the Q function, right? So what's the partial derivative of the Q value for that state action pair as a function of that particular weight that we want to change? And we're going to change that weight proportion, we're going to change things proportionately to that, to that gradient. So this should look kind of familiar. Does this look familiar? Yeah, it looks exactly like the, you know, we drew up the update rule. The things that we did with perceptrons. Yeah, it's a normal kind of gradient update rule. Yeah, right. Exactly so. Right and so when you're representing a function in terms of a set of parameters we want to know how did the parameters impact the value that we're predicting and let's move those parameters in a direction that makes the prediction more accurate. So, this is a way that we can actually view Q learning as an update rule for underlying parameters of a function or proxy. And I guess the only difference is that, unlike the sort of y star minus y thing that we used to do, this is what we got on this particular step. So that's what makes a TD Error as opposed to actual error, if that makes any sense. Does that make sense? Yeah, I think that's exactly right. So when we were thinking about supervised learning, what was happening is we have some kind of output for our current input and we'll call that y. And then we have a target output, y*. And we want to change the parameters of the function so that instead of getting something like y we get something much more like y*. So I think think what your point is is that this Bellman equation kind of value and this kind of current prediction do play the rules of y* and y. So this I think is a really good analogy. But one way that it's different is that why star when we're doing supervised learning is actually the right value, right? It's coming from some kind of trusted source, and it is the label. Here, we're bootstrapping, which I think is a kind of wrapping. And what we're doing with bootstrapping is we're using our current predictions as a kind of way of making a target for what our other prediction should be moving toward. So, this this should look a little bit unnerving. But, it's not a bad idea but it's also not clearly of great idea because instead of using real label data we're actually making up our own labels using the current prediction from the Q function. Right. And we'll see sometimes this can really run us in a trouble and we have to be a little bit careful to make sure that it doesn't. Okay. Right? So this is in a very general form. I think it would be helpful though to make it make more sense if we dive in and think about some particular representations of the Q function. Okay. The particular case, I'd like us to talk about in more detail is linear value function approximation. So that is in particular the case where our Q function is represented as a weighted linear combination of the features that represent the state. And maybe the state in action, but we're going to keep the action separate for now. Okay. So this is how the Q function can be represented, if we're doing things linearly. So we have this set of weights, wai. And we have a set of features or at least this function f that maps states to particular feature values. And let's say we've got n different features. So if we're representing things linearly, what we're doing is we're saying let's sum over all the features for the current state, the value of that feature times the weight that is being used to represent the cube function for this action. And just yes, some of it, essentially take the dot product between this feature vector and the weight vector. And it's these parameters, these wa's, that are actually providing us the generalization, the ability to make a prediction to new states because there's parameters that are shared between all the different states. Sure, that makes sense. So it's like in their own network with nothing nonlinear going on. Yeah, in this particular case it is exactly a [LAUGH] linear function. [LAUGH] [LAUGH] Which you can think of as being under all that without any nonlinear and with a single layer. Yeah, and so in fact you have a whole set of them. You have a whole set of neural networks all kind of happening in parallel. I suppose you could claim that they're one neural network with a the non-linearity, which is kind of some kind of max function or something that figures out which action you should take based upon each of the Q(S,a)'s. And so you really are kind of learning a neural network. Yeah, I can agree with that. I mean, we're going to talk a little bit more about using more general neural network structures in this setting, instead of just linear weights. But the linear weight case is kind of easier for us to think about. And there's actually been a tremendous amount of research on this particular case because the hope has been that it's more well-behaved than what we get with non-linear function approximation. Usually is. Let's think a little bit about what these weights are actually representing. Okay. One way you can think about the weight for a feature is that it gives a kind of sense of importance for each of the features in how much they're contributing to the action's value. Sure, so, if the weight, ai, is 0, then it doesn't matter what the feature is. It doesn't contribute at all. That's right, and so, then, it completely ignores the feature for that prediction. And if the weight is a billion, then it means a lot, probably. And if it's negative a billion it also means a lot, just in the different direction. Right sort of anti-importance. Well, not necessarily, I mean, the feature depends upon how you want to interpret the xi produces a negative number. And you're actually, the weight is also negative. If you're saying that that feature is really important and the negativeness just it's kind of a function of the fact that you want you want Q values to be higher if you're going to use them. Okay, all right, I think that's a good way to think about it. All right, so this just kind of makes sense or you feel like you appreciate the notion of a linear value function approximator? Yeah, I think so, that makes sense. You just add up all your features and weight them appropriately. Cool, all right, so let's kind of make sure that we understand this. Okay. So, this is a good way to make sure that you're really on top of this is, let's do a quiz. So, in particular, we're going to put, I put back on the slide two things from the past slides, one is here is a linear representation of the Q function. So Q of Sa is represented by a weighted sum of the features for that state, and the weights depend on the action. And here is our weight update rule. This is in its very general form. The weight super a sub i is going to be updated according to the gradient of that weight with respect to the Q function. So let's make sure we get this and let's actually compute that gradient. So this is actually not super helpful. This is probably what we want. How does the Q function for Sa change as a function of some particular weight, say w a sub i? So I'm asking if you could do some, I'm going to say calculus. Ha, calculus! Yeah. Okay, sure, I can do that. You basically just want me to tell you what the derivative is in the case where you have a linear function. Right, because that's going to tell us how we actually do our update when we when we get a state action reward next state two bowl, we're going to want to figure out how to bump up the weight. We're going to take the T D error times the learning rate times that derivative. So what is that derivative? Okay I think I got it, so go. All right, what's your derivative? Wait. [LAUGH] Wait, wait. What? What? What? What? I need to remind myself what the answer is. Let's see. So, in this case, f of i is the constant and w is x, so it's just going to be f of i. c of x, the derivative c of x is c. I see, I see, I see. So you're saying, since we're taking this weighted sum, how does any particular weight depend on it? Well, for the weights that we're not actually talking about right now, their corresponding coefficients don't have any impact on how things are going to change. Right, they're all zero. And for the weight that we are actually asking about, the derivative here is really just the constant in front of that weight. So you're, let's see if I see what you're trying to say here is the Q function changes as a function of that weight is really just the feature with respect to the state that we're looking at the i feature. Right. Yeah I think that's right, [LAUGH] we're going to call that right. Some of season x is going to just try to remember how to take the derivative of a linear function so y equals 3 times x. The derivative of y is just c. So I was just thinking out loud about which was c and which was x. And the x is the w that's the thing that's going to change and the c is the future. Right and so it's helpful to kind of remind yourself that w are the variables in this case, there's the things that we're looking at changing and these fi's, even though they look like they could be variables, that is a constant. This is something that's just a fixed value given to us as a function of the state and which feature we're talking about. Exactly. Cool, all right so that's very simple. And I was right. Yes. Yeah. [LAUGH] You should do more of those simple quizzes. So it seems like a good idea and I think it's worth asking whether or not it actually works. Does it work? Well, it's complicated. Tell me more. There have been some examples where things worked out extremely well. So let me just quickly summarize some of them, so you have a sense. You mentioned neural networks, so neural networks, including with non-linear connections, have been used with backpropagation in this context. Folks like Gerry Tesauro and Tom Dietterich have gotten some very nice results creating learning systems that can actually do some fairly complicated things and some difficult tasks. Like what? Well Gerry Tesauro's work was in Backgammon, and he actually used the same kind of approach in Jeopardy in the Watson system, for figuring out how to do decision-making. What's the right strategy for handling different situations in jeopardy, and the value function was being predicted using a backprop net. In Wang/Dietterich's work, they were actually predicting the likelihood of success of various sort of shuttle scheduling strategies. They were actually doing for NASA shuttle scheduling strategies. Say that three times fast. She sells seashells to the shuttling scheduling system. Sure. So those worked out really well. That being said, there's not that many other people who've gotten that to work effectively. Certainly, whenever, I'd say to my students, look these things worked so well, why don't you try it? They don't seem to be nearly as successful as these folks have been. Hey, I got it to work for my master's thesis. Did you really? For what problem? For tic-tac-toe and then for a vision problem that, after I went through all this trouble, I realized was just [INAUDIBLE] around. [LAUGH], that's impressive, in quotes. Not impressive enough to give me a master's thesis, which I think was the important thing. So maybe the impressive thing, is that I got a master's thesis out of this. [LAUGH] At MIT, of all places. Of all places. So it does sometimes work. I find that it's kind of difficult to get it to work consistently. What happens often, when my students try this, is they get a system that actually tends to start to learn well and then it goes into some kind of a death spiral. Because, as I mentioned before, the bootstrapping aspect of this means that you're really dependent on making solid predictions based on your previous predictions. And when they start to go south, the whole thing can fall apart actually really quickly. That being said, there's some other function approximators that actually have been more successful that are variations of linear function approximators. So Rich Sutton, in particular, has gotten CMAC to work on a on a whole bunch of tough problems. CMACs are c-m-a-c, it's a particular kind of function representation that is like a neural net, except the first layer isn't really learned. It's kind of decided in advance how you're going to break up the input space. And so it really is just a generalization of straight up linear. These can be really effective, but they don't generalize very aggressively and that's part of what makes them a bit safer to use in this setting. Linear networks are like we mentioned before. The learning rule is really simple for those. So that's great. People [INAUDIBLE] Parr have gotten cool things to happen for things like cell phone, channel allocation, and so that's pretty cool. You have to be really, really careful about what features you use. Right? because I probably should have mentioned this before the important thing about the features. I'm going to say that it's important that we find features that are both computationally efficient and value informative. And what I mean by that is computational efficient. What would be an example of a feature that is really really great but is not particularly computationally efficient? The one that tells us what the proper action is I don't know. Or what the value is actually in this case. Exactly so. So if there was a feature that actually encoded the value function that would be great. That would be really good for learning. The thing is we don't know how to get that feature efficiently. If we did we wouldn't have to learn at all. So we have to somehow get features that we can extract efficiently from the state, but again that's really easy to do we can just return the state as a feature. It's really important that it actually also be value informative. So that one feature actually gives you lots of information And about what the predictive value ought to be so somehow balancing these two things is often very tricky to get to work. Right so going back to our taxi example if it turns out that knowing how far away the taxi is from some place or how far away a passenger is from some place. And you can compute that pretty easily then you should do it because it's helpful and in your learning are written as to figure it out so that is really good thing, right? Yes a distance to passenger could be something that's both computationally efficient. because you can just kind of pull it right out of the state representation. And probably value informative because it's actually very relevant to know, are we close to being able to pick up the passenger or is the passenger quite distant difficult to reach. So yeah I think that's probably the kind of things that we would want. We probably should try that before we tell people that we think that's a great idea. But in general these are the kind of properties that you're trying to balance. Okay. And I want to mention because this is very hot right now another example of using neural networks to learn value functions has been carried out recently by Google DeepMind and they were using deep neural networks. Do you know what deep neural networks are? Yeah, they're are networks that are very profound. Yes they're poetic neural networks. Mm-hm. No, they are neural networks that are neural networks except some of them have were instead of say three layers they could have ten, 15 they can actually be quite substantially more sophisticated and complicated. And they often involve using things like convolutions to work well on the visual kinds of inputs. So what Minnetal all did at Google DeepMind is they actually learned effective strategies for Atari video games mapping the pixels on the screen to joystick actions which is really neat. It is Right, so it's actually learning to play the game given very similar kind of input to what people get. And in fact they were able to get it to learn very effectively to the point where you could argue that it was comparable to people on average. At least on some games But did it learn how to play thermonuclear war? No, it learned that the winning move is not to play. Nice. No, that's not true. That was a WarGames reference, and in fact, it did not learn not to play. It was not given the option to learn not to play. It just had to play whatever it was given. Hm. Grad school. [LAUGH] And we're back. No, it is true that, on some of the games, it was superhuman, and in other of the games, it was way, way subhuman. And that was why I said, you could argue, on average, that was about human level. Mm, okay, but that's like arguing that, on average, if I have my head in an oven and my feet in ice water, on average, I'm comfortable. Yes. We're not going to get a chance to talk about a lot of the details on this though it's just a great example of reinforcement learning applied to a difficult problem and clever engineering tricks. The one thing that's worth noting is that in addition to using deep nets which are kind of in vogue at the moment, they also did a couple other things in changing the way that training happened. That I think were really, really important for getting the kinds of results that they got. And so it wasn't just a matter of hey, let's just use Deep Nets. It was like Deep Nets plus really careful training so that you don't get these kinds of catastrophic death spirals, like again what my students and I have tended to see when we try this stuff. So this kind of doesn't work successes story, is not really the end of the story. In the sense that, well, so we assign some some reading. I think these were reading associated with this current lesson. Three papers, Tesauro, Boyan and Moore and Sutton which kind of tell an interesting story which is, I think, a really nice summary of what the state of the art is here. So, Gerry Tesauro got TD-Gammon to work really well. So that's a three layer back prop net for learning to choose moves in Backgammon and it worked great. So everybody thought, hey everything works great. So then Boyan and Moore wrote a paper where they showed that there's some very simple examples where things ought to work well but they need not, they can actually work extremely poorly. Using this general approach of substituting in a function approximator for a Q function can lead to terrible results. So, it need not work. Then Sutton had a reply to that where he published a paper that said, yeah, but you can, you can do it if you do it right. So, he actually replicated the same results that Boyan and Moore showed, not working and showed that if you change the way the training happens you can get those particular things to work, but this really is just, the argument is it need not work, but it need not not work. [LAUGH] Which is not really a very strong kind of guarantee. It's not like it has to work or it has to not work. So this is kind of where we are now in terms of our understanding. So what's the Pollack thing at the bottom? Pollack? Sorry, so yes. I just wanted to point out that why did backgammon work so well? It might not have just been the fact that it was reinforcement learning, it might have been, well backgammon is kind of a good thing to learn this way. So, Jordan Pollack had a paper where he showed that genetic algorithms can also do particularly well in backgammon in particular. So, it might not have been that Jerry Tesauro discovered that TD is really great, he might have discovered that backgammon is really great as a test bed for TD. Yeah, I remember that. In fact the argument that Jordan made was that because it's so random it forces you to do all kinds of exploration and keep you from falling into certain traps. That made generalization powerful and in fact if we compared to Isabelle's master thesis work on something that you thought was simpler, like tic tac toe. It actually doesn't do terribly well on tic tac toe, and that's because it's so deterministic there's huge chunks of the state space, you kind of never see even when you're trying to do this generalization. Yeah. Okay. And I would summarize that by saying that nearby states in tic tac toe don't necessarily have nearby values. You just move one x to a different place on the grid. And the whole gameplay is very very different. Backgammon, because it has such a tremendously influential random component. If you move one piece, it can change things but it's not going to change things you know night and day, it's going to change things gradually and so that kind of gradualness means that function approximation is probably going to, well could at least work out. Maybe, it's certainly a good argument. So what I'd like to do is actually step through a concrete counterexample that really does show that a particular way of doing this kind of training can fail in the worst case and this is an example due to Lehman Baird and it's devious and clever. Hm, I like devious and clever. So here's a really simple MDP. You ready? It's got six states and then a seventh absorbing state. Okay. For each state, there's exactly one action. There's no stochasticity. There's no rewards of any kind. They're all zero. So this is a really simple example, right? Sure, that seems pretty straightforward. All right, so then what is the optimal value function and policy for this example? The optimal value function or optimal policy? I asked for both. There is no optimal policy, it doesn't matter what you do. You just have to do one thing. There's only one policy and it is optimal by definition of optimal. In the value function, the proper value function would be zero everywhere. Zero's everywhere, right, because there's no Rewards anywhere. So right so this is super duper simple and if we had to learn this using Q learning or something like that presumably this would actually learn quite quickly. What we're going to do though to make life difficult is we're going to do this in a function approximation setting, linear function approximation with this following set of features. So, there's there's the seven states in the picture and associated with each state is a feature vector and there's eight features that are part of that. Feature zero one, two, three, four, five, six, seven, feature zero is one for all the states and seven for this for the repeating state. And interestingly, we have it set up so that these other features are actually completely indicative of which states you're in, so state one has feature one being true. State two has feature two being true. State three has feature three being true. Or at least. [LAUGH] And by true I mean two. I guess. Two. Yeah. And so that's kind of the representation here. So state six looks like one. And then zeros is everywhere. Except for feature six which has a 2 and then a 0, right? So this feature representation is actually really close to being just a table. Yeah. So I'm going to say near tabular in the sense that if we get rid of this first feature there's going to be one weight that represents the value for state one and one weight that represents a value for state two and so forth and one weight that represent the value for state seven and so it's exactly just a table. One weight or one table entry per state. All right. And all we've done is kind of made it a little bit more by having this extra feature that we ought to be able to just ignore, right. And we should be able to represent things pretty well. So in fact, I think that it's worth asking. Can we represent the optimal value function using this set of weights right? So if our parameters are W0 through W7, is there a way to set them so that the value function that we get is the actual value function for this MDP? Sure there is one easy one. And what's that? Set them all to zero. Right. So we're taking this weight vector of ws and taking the dot product with the feature weights. If the weights are all zero, it's always going to predict zero which is actually the correct answer for this NDP. So this it's really easy to set the weights so that that happens. Is that the only way to set the weights to make this happen? No because in fact for each one of those states, there's only two features that are turned on and there are different two features for every state except for state zero so you could be pretty clever about it. So I think if you make the weight for zero minus 1 maybe and then make features one through six one-half, that would get you 0 for everything except state seven and then state seven just needed to be 7. y or a seventh? 7. 7, good. So in particular, this weight vector times this feature vector is going to give us, well minus 7 plus 0 plus 0 plus 0 plus 7, so 0. For this other one, it's going to give us minus 1, plus 0, plus 0, plus 0, plus 0, plus 0, plus 1, which is 0. So, yeah, so this is a weight vector. So, not only can we represent the value function, but we can represent it in multiple different ways. It is multiply representable, which I spelled wrong. Multiply, which is spelled the same as multiply, but it's representable in at least two ways. Actually an infinite number of ways because you take that vector minus 1, one-half, 7, the multiplier by any constant including zero and then it will work. Nice. Multiplied by 0 and you get that first weight vector that we talked about anything else that we get were multiplying 0 times a constant. So in fact there's an infinite set of weight vectors that all capture the optimal value function, which is also 0s. And so, we have plenty of rope. We should be able to build a rope ladder. Yeah, paying ourselves. Ultimately that's actually what's going to happen in this case. But it seems quite friendly, right? It's also tiny, right? Yeah. These are all things that would seem like it should make it so that it's really easy to get this right. So then that's get it right. Yeah, well so it turns out that what Baird showed is that, if you give it the right sequence of updates, it will actually spiral out of control. So let's take a look at that. Okay. Here is a bad sequence of updates for this particular example. So I rewrote the value function for all the different states, all seven states. They're all remember dot products of the weights with the feature vectors. So V(1) can be written as W0 + 2W1. V(2) can be written as W0 + 2W2. And so on. State seven is the weird one, right, that has 7W0 + W7 and we have to change the w values so that we actually are representing the value function correctly. So let's consider the case we start off. We're going to get this process going with the weights positive and the value of state seven much bigger than the others. And we're going to update things round robin. We're going to say after a transition from state one to state seven using the only action that we've got and getting no reward, because there's no reward. We update when we go from state two to state seven, state three to state seven, state four to state seven, state five state to state seven, state six to state seven, and seven back to itself. So these are all the possible transitions in this MVP. So we'll just update all of them one at a time. And then do it again, repeat. And just for concreteness, let's say that the discount factor's point nine, and the learning rate is point one. And let's see what happens. Okay. So I think this kind of puts us in a good position if we want to try to understand how these updates happen very stealth quiz. No, you tricked me! Yeah so here we are in quiz land. Here's what we need to do. We need to say how the weights change as we do these various updates. So in particular let's start the weights off at one. So all the weights are 1. So the value that we assigned to state 1. Is going to be, what, 3. Right? And these are all be 3 actually except for this one which will be 8 and I said that the value for states haven't had to be much bigger than the others. Let's say that 8 is much bigger than 3. It's not generally that much bigger. Well it depends. Certainly 8 year olds are much bigger than 3 year olds. [LAUGH] Right, so assumption validated. So now what we'd like to do is say what happens to weight 0, when we make any of these transitions on the right here. Except for 7 comas, 7,0,7. Let's say, let's look at this particular transition from state 1. State 7, with a reward of 0. Okay. So the weight is currently 1. What is the weight after this update takes place? For just the 1,0,7? Yeah, that's right. After the update just for the transition from state 1 to state 7 with a reward of 0. All right, so, let's see if the students can do it first. Students go. Okay, so, what's the answer? I don't know the answer, but I know how to compute the answer, I think. That's just as good. I agree, you just substitute all the values that you have for that equation right there and see what happens. Let's do that. So let's see, alpha is 0.1 times the reward, which is 0, plus 0.9 times the value of where we're headed up which is 7, which was 8. Mm-hm, state 7, which has a value of 8, right. Right, minus the value of the state that we came from, which was 3, times the derivative, the value function with respect to that weight, which as we recall from earlier is just the weight, which was 1. Good. Right, so that would be 0.42? [LAUGH] Wow, can you really do that? That's impressive. Yeah, 9 x 8 = 72- 3, so it's 42 x 0.1. All right, nice. So yeah, 0.42, very good. That means, after that first update, weight 0 is actually going to get bigger, maybe not a lot bigger, but a little bigger. By exactly the 100th of the answer to life, the universe, and everything. Nice, nice. Little Hitchhiker's Guide reference, good. And that's what happens to weight 0. What happens to weight 1 after this particular transition from 1 to 7? I don't know, I guess I didn't stick everything in, I guess. No, no, no, you wouldn't. It's actually very simple because the derivative of the value function with respect to that other weight, weight 2, it's just the feature value of that associated with that weight. It used to be 1 but now it's 2. It's just multiplying everything here that we have here by 2. That's true, that's true, very good, 0.84. So, weight 0 is going to go up, weight 1 is going to go up. What's going to happen after we do these other transitions, 2 to 7, 3 to 7, 4 to 7, 5 to 7 and 6 to 7. The very same thing, a very similar thing. Yeah, ish, right, because weight 0 is going to be a little bit bigger than it was, right. It's going to get bumped up each of these times, but essentially everything is growing and the weight 0, in particular, is growing a lot, right, because it's actually getting updated six times and each of those times is by 0.42-ish. Plus a little bit more. Right, plus a little bit more because it's actually accruing weight gain. Aw, I feel bad for it, I can relate to that. So that's what happens to the weight, they go up and up and up. So, that's these first six updates, the updates of states 1 through 6 to state 7. What happens when we go from state 7 to state 7? Let's look at that. Yeah, we could do that. What I hope happens is things start going down because otherwise things are going to keep going up, like housing prices. Right, which seems problematic. Yes, and then you have a bubble and then the machine learning economy collapses. No, okay, let's see what happens. All right, so based on what we just talked through, weight 0's going to keep going up after each of these updates. And in fact, if you just plunk it through, it's not particularly complicated that after these first six updates at this point weight 0 is about 10.4. And now, we're going to finally do this this last update of state 7 going back to itself. So let's work through that update here. Okay, well, that should be pretty easy. We just do what we did before, we substitute in everything. Okay. So let's see, alpha's still 0.1. Yeah. R is still 0. Yeah. Gamma is still 0.9. Yeah. The value of 7 is, 7 times 10.4 plus, I guess, still 1- Yeah. Which is 2.8, so 73.8. I'll take that. Minus the value of S, where we just came from, which we just said was 73.8. Yeah, so we're making a transition from state 7 back to state 7. And so the value of S and the value of S prime are the same. Right, and then it's just the feature. Which is? Which is seven. Seven. And that's equal to some kind of math thing. That's not that hard. It's minus 0.738 times 7. Negative 5.166. If you say so. All right, so that's kind of good news, right? because this weight 0, which had been getting pumped up and up and up and up is going to come back down a bit. And, in fact, now- False. That's not good because w0 went up a lot more than you're about to take it back down. Well, sure, yeah. So weight 0, after all these updates have happened, the weight is 5.234. It started off at 1, recall, and now it has actually blown up quite substantially. So this last update does help, but it doesn't help enough. Is that what you're getting at? Yeah, and in fact, that's going to keep happening because it didn't matter that we started at one input. But there's nothing magical about one. The point is, it's going to keep getting bigger and bigger and bigger and bigger, and that's actually true for w1, w2, w3, w4, w5, w6, and w7. Right, so if we finish this sequence of updates and we go and do it again, this weight 0 is going to get pumped up even more. Weights 1 through 6 are also going to get pumped up. Weight 7 is going down, but not enough to make up for the rest of everybody. So yeah, so this is bad, right? So we just keep doing these updates over and over again, and we'd want them to converge. We want them to kind of get to the right numbers, and instead, we're just spiraling out of control. So, they're going to converge to infinity. I think that's kind of the definition of not converging. Well, it's one kind of not converging, right? It's diverging. Well, that's depressing. Yeah, so even in this, what would otherwise be a very well behaved example, doing what is clearly the right thing in terms of doing these weight updates actually is not the right thing. So yeah, so bad things happen. So I think I can convince myself just by staring at it that if we happen to set all the weights to start off as 0, they won't ever move, and the right thing will happen. Should we check that? Sure, let's give it a quick check. Bang, it's a quiz. Wow, that was also quick, Michael. Again, the magic of video editing. So what we want to know is that after all these updates, what happens to waits zero through seven? They start off at zero, what are they after all these updates? Okay, let's find out. Let's find out Go. Let's figure it out. Okay, well, we should figure it out by doing the same thing that we did before which is substitute everything. Uh-huh. But I think we can do that pretty easily. So let's see, alpha's .1. Uh-huh. So that's .1x, let's see, zero because that's the reward. Plus .9x Zero. Whoo, because all the rewards are zero minus zero. Mm-Hm So that's going to be some number times zero, times some other number so it's going to be zero. Right. So, double your zero is going to be zero after the after the after the first update. First one, but if you recall from last time we just knew that W 1 was just going to be twice as big and two time zero is zero still, yep. So that means that after the first update they'll both be zero which means by induction or something that'll also be true for states two through six. Good, all right, so after we've done the first six everything's still zero. Right. What about the seventh one? It just It has to be the same right so yeah so 0.1 time zero plus zero minus zero so it's still zero. And W7 would be well the same thing. It's still zero. It's all zeros. Yeah. So we can repeat this update this whole round robin sequence of updates over and over and over again. And we're going to do is go from all zeros to all zeroes to all zero to all zero to all zero. So we're stuck at the right answer. Right. So if we did that an infinite number of times, it would be infinity times 0. So you're telling me infinity times 0 is 0 sometimes. I think it's undefined, but in this particular case it would actually be 0. Excellent. So if we start out at least with all zeroes, we won't leave the wrong answer at least in a case we're all the way through zero in this case. Yeah, and I think that's actually going to be true as long as all the transitions are deterministic. So for this case it holds, but I think it holds in general if the things are deterministic. Because we're going to get the same kind of issue happening here. Which is the next state and the state are going to have their correct values. This temporal difference is going to be zero and then it doesn't matter. Nothing else matters. The temporal differences are just going to keep being zero. So that would be true even if we had our other infinite number of weights. Right? Remember we said that we could, there's an infinite number of ways to get the right value function. Everything to be zero, because the value function will be zero everywhere. That means that nothing will ever get updated. So if we have exactly everything set up so that all the values start out being zero then nothing will ever change. So the exact answer is sticky if determinants. That's what I would say. But wait, so that happens to be good in this case. But I think it's the case that if we have any set of weights where the value function everywhere is 0, you can never escape even if it's the wrong answer. No. No? No. It's true. It's sticky if the TD error is zero. Not if the value function is. Right. So in particular in this MDP. The reward is always zero. Right. Right. You're right. So let me amend my statement. It's the case that if the value functions all are at any given time all equal to zero. Then the only way you're going to escape is if your reward is not equal to zero. Yeah. And if the reward is equal to zero, then being at the value function zero is actually a solution to the element equation. Because there's no error. So, there's no error. Okay, good. So, this is why the exact answer is sticky this depends on a being deterministic, though. So if it is the case that, for example, some of the transitions we observe Are probabilistic right so the next state is chosen from among other possible next states. Then one of these updates can actually move us away from the right answer. We hope that on average the different updates would actually move us away from the right answer in ways that all canceled out. I think that's true, but yeah the details are right, so it really depends very much on exactly how the learning rate is set and other issues like that. Yeah. So if it's not deterministic then we can wander away from the right answer even if we've gotten it. And then, as far as this example is concerned it seems like we could potentially wander very very far away. Or we could wander towards the right answer if we are extremely lucky Instead of having things divergent explode. I guess the lesson here is that if you're doing this generalization where everything kind of depends upon everything else because you're having these shared weights in your value function. Then even in a simple linear case you could not always converge you can diverge bad things can happen it's just not guaranteed. Which is sad because usually we stick with linear functions so that things are easier. Right, so at the end of the day we can create examples where it truly does not converge. So we have no way of assuring ourselves that it will converge, unless we deviate from some properties of this particular example. And so, what we're going to look at next is actually a value function approximation scheme that is well behaved. That is actually going to do the right thing and converge. Nice by the way can I just say that from a pedantic English point of view I really appreciate the lessons here because cannot is supposed to be one word. But if you would one word here you would be saying this thing could not converge. But what you're saying is that it can be the case that it not converge and that space matters. Right. That is pedantic. Yes. Thank you, Michael, thank you so much. All right so we're going to talk about a particular kind of function approximator here that we refer to as an averager and the whole class of averager function approximators. Okay. So this is a kind of function approximator that was actually or at least in this context introduced by Jeff Gordon and when he did it there was this sort of collective head slapping that happened over the entire community of reinforcement learning researchers. Because it was just there. It was so beautiful and nobody saw it except for him. And so it really was at that time, the age of Gordon. [LAUGH] That's fantastic. So moving on. So we're going to introduce this averages concept in the context of a simple example. So imagine that we're trying to do is represent a function over the line. So over the over this one dimensional space. So it's going to be some kind of curve [SOUND] And the parameters that we're going to get to set are the actual value of that function at some particular point. So at X1, at X2, and at X3 we can, you know, move these up and down, these points. And what we'd like is that we define a function approximator so that once we've defined these points, the rest of them will get kind of filled in in a natural way. Let's say. And for this story to make sense they're going to have to somehow be related to averaging Sure. How do we define the value of points in between here between x1 and x to say we want to get going to get the value of some point Just plain old x as a function of the value at x1 and the value of x2. Yeah. So so what would be a natural way to do that? I know you can say more than one if you can think of it. Sure there's lots ways of doing it but I guess the simplest one would be just to take our weighted average based on distance. Yes which is why these things are called Averagers. So what we're going to do is we're going to say okay the value of x I don't know what it is, but we're always going to make sure it's, I don't know, 0.8 times the value of x1, which is a parameter, plus 0.2 times the value at x2, which is another one of our parameters. And that's because of the relative distance between Vx1 and x2. Yeah, so it turns out it doesn't matter but that is a very natural thing to do. To kind of do a linear interpolation. And so what that means is once you nail down where x1, x2, and x3 are. The way that it generalizes, the way that it fills in the rest of the function is like little line segments. There's a bunch of other things you could do and they're all okay as long as what we do is any point that we want to define, we define it as a convex combination of the anchor points. So the anchor points are the ones that we get to set as free parameters and the value at any other state needs to be expressible as a convex combination of the anchor points. So this doesn't really tell us what should we do for beyond x3 for things that are out here in the state space. Well that's funny. That is just what I was going to ask you. What we do for extrapolation instead of interpolation? Yeah, so this is very interpolative. [LAUGH] Is that a word? It is now. Interpolative, it's sort of interpolation related. So the constraint that we've got. For it to be an averager, it needs to be that the value of any point is the convex combination of a set of anchor points. So, we get to define how things extrapolate but it has to be the case that for example for some point X prime out here, its value needs to be defined in terms of these black points, in terms of the these other anchor points whose values are going to get to be set. So, one natural thing would be to say okay,we're out to the right of X1. Let's just set anything out here to be the value of X3. Sure. So we don't really extrapolate so much as just you know flick flatten it out on the two ends. So we don't get too kind of crazy up or down from from where we were but there's other things that we could do as well you could also say that if you're far enough away from the points then it's going to be the average the just straight up average of these guys. And as you get closer it's going to be more influenced by the near ones. By virtue of the fact that it's a convex combination. What does that tell us about the value out here? Well, it has to be in between the highest and lowest value. That's for sure. Exactly. That's right. So we are extrapolating, but we're not really extrapolating outside of the data that we've got. Okay. And this is, this is going to be a really important property. These averages to make sure that they're going to be well behaved. Right? So that, that makes sense so far? Yeah. Do I have a good definition of an average or? I think you do So here's a general way of writing these averagers. So what we need, is there's a set of states. And we're going to need to define the value at those states. And there's a second set of states. What I was calling the anchor points here, or the basis set. Mm-hm. I love basis sets. To define the value of sum state s, we need to know what the convex combination of the basis values we're going to take. So we need to have specified what the basis set is. It's going to be, part of our function approximator's going to get to pick the value at those basis points. And we have to pre-define the averaging weight, the convex combination weight, between any given state that we want to do an approximation, and any given basis point, any anchor point that we want to adopt some of its value for. So, these are the parameters that the function approximator gets to alter. These are all predefined. Things that define how the value of other states are going to be defined in terms of these parameters. So it looks a lot like the linear case, right? Except, it's not. Well, in particular, aren't you're missing something? I mean, I was going to say it looks just like the linear case, but there's nothing in there that forces it to be a convex combination, the way you've written it. Right. So that goes without saying, but I should probably say it. [LAUGH] Yeah. These particular weights that we're defining, need to make it so that the value of any state, is a convex combination of these parameters. So I wrote that in math, too. So for all states and basis points, or anchor points, the weight between them is not negative, and they sum up to 1. Okay. This let's us express things, like these kind of linear interpolation that we mentioned before. And in fact, even these kinds of smoothed, or maybe even kernalized representations work as well. As long as at any point where you need to make a prediction, there is a set of these weights w, that define how we're going to define the value at that point, in terms of the value of the set of anchor points. Sounds good to me. So hearkening back to when we talked about supervised learning in that other class. Can you think of any supervised learning algorithms that you can actually express as an averager? Probably. Okay, then I think you need to fill that in. [LAUGH] Okay, fine, well maybe I'll get lucky and the students will do it for me. Go! All right when he-- did they-- were they able to help you? [LAUGHS] No, unfortunately because I'm doing this stupid linear time. I could just start naming all of them and you can tell me which one is right. [LAUGHS] Well, I could just say that the answer to every single other question I ever asked was k and n. K nearest neighbors. I am glad you got that answer and that was it was a really good answer to give. I think it might have been a guess though and so we should at least convince ourselves that this is a reasonable thing. Well let's see how does k and n work? You find the things that are closest to you and you let them vote, if it's a number or even if it's not you end up basically taking an average. S, if we know the where the anchor points are which the k, you know defined points are and we want to know what happens at some other point. All we have to do is say okay, well it's the average of the k nearest neighbors in other words it's one over k. I think I'm overloading K. Okay, then we have a whole bunch of points where the values are defined. But to figure out for some other point, some point that we want to extrapolate to or interpolate to, we take the k nearest neighbors. K is 3 here and we give whatever the values are at those points a weight of one-third. And so that's a convex combination of anchor points. Right. K-NN is totally an averager. And even the extensions to it where you take, Averages based upon distance and such and such and still, it's still going to be convex combination. Yes, that's exactly right because they are in fact, exactly averaging values together. In fact lots of, you know, kernel versions of this have that that same property. So yeah. And friends. Really is the answer to everything. [LAUGH] So there are natural function approximeters that have this form there's other new things that we can define and all were constrained by for an Averager is that the value at a point whose value we don't know needs to be representable as a fixed convex combination of some values that we do know. Okay? Okay. It sounds like a reasonable thing. It sounds like a fairly general thing, and not a particularly weird and scary thing, but it's just what you can do with it is just brilliant. Show me, show me the brilliant. Let's think about what these things imply. So the value of a state, well under the Bellman equation is this and nothing scary here right this is all stuff that you know. So now, the next thing we're going to do is we're going to say, well the value of the state that we end up in since we're in average land. We can write that as the sum over all the anchor points of how related this state s prime is or what the wait is between s prime and that basis point. And then whatever value we set for the basis point. Sure All right so here it is in the context of the whole Bellman equation. So this is cool to write I just changed the order of summation and kind of regrouped a little bit. So now, we've got max over actions the reward of that action for the state that we started. And now, we're summing now instead of over all next states, we're stepping over just the basis points. Sure. We're multiplying some quantity time's the value of the basis point. What can we say about this value here. If we write it let's say like this. So we can write this as some function T of the state, the action and the basis point. And the sum over next states is kind of already built in here, right? Yes, yes, yes. [LAUGH] I gave this a name. I called it T prime, because i wanted you to see a connection between this and transition functions. So, how is this like a transition function? Well, choose a state, you take an action. What basis functions will you go to next? Right which you don't. The actual MDP, you don't go to the basis function. But the point is that this acts like a transition function in that it's nonnegative, right? And, when you sum up over all SBs, over all the anchor points, it sums up to one. RIght. Why does it sum up to one? Because this is a convex combination, and this is a convex combination, and so, when we combine them, we get a convex combination. We could do the algebra for that, but i don't think we need to. And so it acts just like a transition function. So we can actually fold the function approximate to itself. Into the transitions. And all we get at the end of it is an MVP. So why does it actually do the right thing because MVP's have a well defined unique value function and that you can find it using things like Q learning in value iteration. That's beautiful. Isn't it, though? So, what we do is, we end up with something that is actually a reasonable function approximator. It's taking the value of some state, S prime, and writing it as a combination of other states. And then we just we turn it into, over series of steps, into a new Markov decision process over a smaller set of states, specifically, over just the basis states. And at the end of the day We know that this is going to be well behaved, we know that it's going to approximate things. Well, we don't know that it's going to approximate things reasonably but at least without blowing up in a convergent sort of way. Because we know a lot of stuff about MDPs. That's kind of insane. It's very clever. No no, it's very very clever. So if I were going to make too fine of a point on it, what would I end up with aA kind of interpretation here might think the MDP are moving over is sort of basis state to basis state? Yeah, I think that's a good question so it's as if we're going from a state to some next state which is S prime. And then, from there we're making an additional mini transition to the basis states the SBs And the weights on these put these probabilities are the combination weights, are the convex combination weights. Convex combination weights look a lot like probabilities. Right, they do. So it's as if each time we make a transition to a next state s prime we make an additional little transition to each of the basis states, and from there we continue. And so, we can actually solve out What the value function is over everything by just solving the MDP over the basis states. And then, we can extrapolate that or interpolate that to anything that we want. I love it. And it makes some sense because the basis states sort of represent a set of states. So really what you're talking about is you're averaging over the set of states that kind of get aliased through the basis functions. I think that's beautiful. Yeah, exactly. So yeah, we fold our function approximater into the MDP itself. Done. Very cool. So, the good news is, here's a value function approximator that is very well behaved. The bad news is, it's hard to do some things with it, because value functions aren't always very well represented. As average or type things there's often kind of weird cliffs and so forth in the value function that are not well captured by just smoothing everything out. But still this is a really nice idea. Beautiful. I think that just about covers it as an introduction to generalization, so I think maybe it's a good time to reflect on what we've learned. I'm willing to do that, especially if it means I don't have to do another quiz. Okay, well, but I, eh, okay, no, I'm not going to do another quiz, but I am going to ask you what we learned. Okay, I think I can remember some of that, so I think the big thing we learned is that there is both a need for and a way to do generalization. In reinforcement learning, yeah. And the need part comes from the fact that for a lot of problems we care about, there are zillions and zillions of states. You could be our generation's Carl Sagan. Yes, zillion and zillions of states. And that's a problem because of course, we'll visit states that we've never seen before, and the chances of seeing every state an infinite number of times and all that other stuff that the theory requires is just kind of unreasonable. Or at least slow, and so it would be nice to overcome that. Good, okay, and methods for? We talked about a few methods, but I think at the end of the day, it really boiled down to let's just take all that stuff we learned in supervised learning and apply it. Cool, but we did talk about some specific ones. Do you remember any particular supervised learning methods that we looked at in the context of generalization in reinforcement learning? Sure, we looked at linear function approximation, which makes sense because linear is nice and well behaved. Or at least we hoped it was. And it does seem like the simplest most straightforward thing to do based on what we know. But I think it's worth mentioning that we were function approximating something in particular, right? So we were deciding to do the value functions and to try to learn those. That's how we were doing our generalization. Right, that's a good point. We actually looked at different functions that we might approximate, value functions, policies and models, and we focused primarily on value functions. Models, there's interesting things to say about that, but it's not nearly as well studied or as well understood. The value function case, we just focused on representing like the V function or the Q function, and that could be done in a number of ways. And we looked at kind of a general gradient form, where we turned the Bellman equation itself into a kind of an error measure. And then we looked specifically at linear value function approximation, how you could, if you have a set of features, you can map them to values using a linear function and how we might learn that linear function. Right, and that's pretty much what has been almost done from the start. I don't think we did hardly anything with the policy function approximation. Nope, and that's okay. [LAUGH] [LAUGH] And one of the reasons it's okay is because you're able to talk about lots of successes that people have had doing function approximation with value functions. Yeah, and there are some successes on doing function approximation with policies as well. In fact a lot of the coolest stuff in robotics in the reinforcement learning setting has happened where there's an actual gradient taken with respect to representation of the policy. It's not exactly clear why it seems to be the more effective way to use feedback in the robotic setting, but it's worth noting at least that policy gradient methods do seem to work well in the robotics setting. That's a good point, but it doesn't really matter. It's not like robotics is the future or anything. Robotics is totally the future. Mm, we'll see. We'll have this conversation in 2050 and see what we think. In 2015, it is 2015. No, 2050. 2050, phew. I'm like, my gosh, it's the future and I wasn't even paying attention. [LAUGH] Well that's okay. I, for one, welcome our new robotic overlords. Good. In the meantime, we had all these successes, and you named a few. Right, the one I remember the most was TD gammon, but there were others, which if I went back and looked at what we've recorded, I'm sure I could find out what they were. [LAUGH] Or maybe you'll just remember. Well, I mentioned several, but Atari is one that actually came up relatively recently, and hopefully will actually change the way people think about this whole topic of generalization, but we'll see. It's still too new to know. Very good, prince. Okay, so now those successes of course were followed by the downer of problems and sort of problem cases, where things that seem like they would obviously work do not obviously and easily work, even in the case of linear function of arguments. Yes, that's exactly right. The good news is in the last few years, there's a new set of methods that are based on TD and a gradient. But it's a different kind of gradient that actually uses the function approximator as part of the error metric that's being resolved and that these can actually these provably converge. For the case of linear, in particular, the paper that talks about GTD2 actually shows what happens when you apply this particular problem case, the Baird counter example that we looked at. And you can actually correctly learn it if you use this other update rule instead of kind of the classic Bellman residual minimization kind of thing. What they showed in some of the recent work is that you run into problems if you try to minimize the loss function that we talked about which is trying to minimize the kind of GTD error directly. And instead, you actually get something that's much more well behaved if you incorporate the function approximation itself into the last function. So take advantage of the fact that we know that we're changing parameters that are alternately going to influence a linear representation of the function. And then through that linear representation the function that that's where we're going to actually measure the loss. And so the projection with respect to the function approximate or ends up showing up in the last function and that makes things happier. Nice, well done. So we had successes we had problems and then we have successes again it's pretty good. Yeah and one of the thing to note on the successes side is that the current We can talk about this so much but the go to algorithm these Is called fitted Q iteration, which is a way of mixing function approximation and Q value learning together. Again, if [LAUGH] it has the same kinds of problems, it need not work, it need not converge, it can behave really badly, but this is something that a lot of people have gotten to work decently. So if you're you know if you're trying to solve these kinds of problems this is this is the go to. GTD is not quite there yet it's more of theoretical interest but the hope is that these will actually come more together in the future. And there's one other topic that we that we spent time on that I think is worth mentioning which is the notion of Averagers. Yes. And the cool thing about an Averager was not only where things well behaved you got convergence with a function approximater, but it actually could be viewed as a kind of MDP itself. The function approximater or could be viewed as kind of MDP itself which, lets us get all kinds of nice properties like that there's a unique. Value function that it needs to converge to. One thing we didn't talk about that's also really related to this this topic is in the Averagers case we have a set of anchor points for that? Yes I do. As the number of anchor points increases, the error in the value function approximation that you get actually goes down. And so you can actually converge over time on not only an answer but actually the right answer, the right value function in the limit. Right because this is just K and N, or was a lot like K and N. And that would be true for canon as well. Yes. Alright if you had an infinite number of points for example you could actually represent the point. So in optional reading we'll actually put a link to it to the first paper that really goes through that argument and shows that. Yeah in fact as the amount of data increases things like averages kernel methods converge to the optimal value function the true value function. That's pretty cool, so that everything we learn including stuff we did learn? I think that's everything we learned and a bunch of things that we didn't learn, and then I probably should also just say LSPI and just leave it at that. [LAUGH] I think that's right we have to leave them to go learn something on their own or those with a point B. Yes. Least squares policy iteration, which is which is a kind of linear function approximation where policy improvement is actually incorporated into the system itself, and it has some super nice properties and people have gotten it to do good things. The tricky thing is always the feature set, getting the right features so that linear function approximation does an okay job. Well it's always true, isn't it? Exactly. If we could always have the right features, then the problem becomes easy. So that's a lot to learn. Yeah, and to have learned. What are we going to next time, Michael? Here, I'll give you a hint. I'm hiding the slide. We're going to talk about partially observable models. Hm, okay, that makes sense. That means that I got lots of opportunities for puns. [LAUGH] All right, we'll see you there then. Wouldn't want to miss that. [LAUGH] See it has already begun. All right well by, Michael. See you, Charles. Hi Charles, welcome to the next lesson. Hi Michael thank you for welcoming me to the next lesson. What is the next lesson? So I don't know, can you read it? I can read some of it I can't quite see all of it. [LAUGH] I get it. So the other topic is partially observable MDPs but I made MDPs partially observable. Brilliant. Kind of fell off the bottom of the screen. So yeah so this is the idea of talking about Markov decision processes like we've been talking about, but ones where the agent has to make decisions based on what it can see of the Markov decision process, not necessarily the true and current state. You know we talked a little bit about this in the old machine learning class, didn't we? But just a little. Okay, all right, well so then we're going to get a little bit more into it, maybe, I don't know. What did we talk about last time? I don't know. I don't remember, just that there was-- we mentioned I think we mentioned that they exist. Okay. So we're going to talk about how they work. So here are the quantities that make up a POMDP. We have a set of quantities that are actually just an MDP. There's a set of actions and states and transitions and rewards. But the thing is that that MDP that is inside the POMDP isn't directly observable to the agent. And instead, the agent has to make its decisions based on the observations that it makes. So this Z stands for surveysions Or observables. So it's a set, like S and A. And then we need some kind of function that connects the states and the actions and the observables, or actually, just the states and the observables, and we're going to write that as O, which is the observation function. What O consists of is a mapping from state and observation to the probability of actually seeing that observation, given that the agent's currently in that state S. Okay, that makes sense. So there's a true MDP, and what makes the MDP really, I don't know, an MDP or, is really the state. The key thing has always been what state are we in right now, and what you're saying is that with a POMDP, you never actually know what state you're in. And so you get these observables or these observations, and that's what you see that gives you a hint about what state you're in. But doesn't necessarily tell you everything, so what's a concrete example of that in the real world? Well, in the real world, I mean, in the real world, the real world is like this, right? So when I'm talking to you and some aspects of the state of the world in Atlanta are actually relevant and important to me, like whether or not you have ice cream. But I can't directly observe that. I have to infer the presence of ice cream indirectly from whether or not you're taunting me. [LAUGH] But I'm always taunting you. But I think I get your point. Okay, that's fair, that makes sense. Yeah, I mean, you could taunt me about ice cream, and that's not a perfect indicator that you actually have ice cream. That's true, sometimes I just taunt you for the fun of it. Exactly, so what I need is some kind of observation function. I need to be able to predict, given whether or not you have ice cream, so that's given you have ice cream, what's the probability you taunt me? It's very close to 1. And given that you don't have ice cream, what's the probability that you taunt me, and that's much lower, but not 0. But still close to 1. [LAUGH] Well, you won't taunt me about ice cream when you don't have ice cream so much. That's true, also, when I have ice cream, you can hear me smacking. Smacking? [SOUND] And making mouth sounds. Well, again, that's a [LAUGH] kind of observation. So smacking is a kind of observation that, again, is indicative of, but not a perfect indicator of, whether or not you've got ice cream in your mouth. Okay, fair enough, fair enough. All right, so I think I get that, okay. So, POMDP basically has an MDP inside of it, and you can't see all of it. And in particular, the part you can't see is S and maybe T. The T relates S and A, right? The T says how given that you're in a state and you take some action, what state you go to next. But of course, you don't really know what state you were in. And you may not know what state you ended up in. Because, again, you, as the agent, have to make decisions based on what you can see. Okay, fair enough. Great. Now would be a good time for a transition to the next slide, if we knew what it was. Well, do we even know what slide we're in? [LAUGH] [LAUGH] Our Charles let's do a little quiz just to to solidify this idea of PomDPs in your mind and this notation that we're using. Yay. So, what we're going to show is a useful fact which is that PomDPs are generalization of MDPs and in particular anything that you can represent as a MDP, you can also represent as a PomDP. You can build an equivalent PomDP out of the same pieces. So, let's do that. So, imagine that we've got an MDP consisting of state, action, transition, reward and we need to explain how we're going to create POMDP out of it. And for POMDP you need to specify the state, action, observables, transition, reward and observation function. And I'll give you a hint the states of the POMDP are just going to be the same as the states the MDP. The actions of the POMDP are just going to be the actions of the MDP. The transitions are going to be the same. And the rewards are going to be the same. And have to define what are the set of observables and what is the observation function that makes the POMDP act just like the corresponding MDP. Okay, I think I can do that. Cool, just because you can observe things partially that means in principle you might be able to observe things completely and so that's what you need to show me your ready? I am ready, so let's go. All right, just talk me through it. Okay, so I think this is fairly straightforward. So, when you ask the question does a, when you suggest that a POMDPs generalize MDPs, what you're really saying is that, if I have a true MDP, the POMDP can represent it, basically without any painful extra effort. And so that means there has to be a case where there's really a direct and simple connection between the parameters of the POMDP and parameters of the MDP. And you've already taken us a pretty far way there by pointing out the states as states to actions and actions to transition the transitions and the rewards of the rewards. So what would the observables be? Well if there is no uncertainty, there is no confusion about what the underlying state is, and one way to capture that would be simply to point out that Z is S. There's a one-to-one correspondence from every state to every observable. Okay, yeah that's, that's what I was looking for. I would say that a slightly different way which is that the notion that the vocabulary of things you can see, we might as well just reuse the vocabulary of the states, because we're going to get to see them directly. Right, so they're state one, two, three, four, five, and six. There's observables one, two, three, four, five, and six, and they match up in the obvious way. Sure. In other words, you can observe the states. You can observe the states. Well, so far, all we said is that the set of things you can observe Is equal to the set of states. Now we need to define the observation function to say that what we're really observing is the state. Well wait right so that's so those two things together will sort of give us what we want. So here's what I'm going to say for any particular state. There is a particular Z, and no other Z matches that state. And no other state goes with any other Z. So I'm going to say that the probability of you seeing Z when you're in state S is 1 if S = Z. Which we can just shorten as 1 if and only if S = Z. Right, and that works because probabilities have to add up to one. Yeah but there's additional constraints on this observation function that make it an observation function. We get a probability distribution. Yeah, so we could write that out as like for all I's, Z of I = S of I or something. Excellent all right so now we get that idea, let's try to use it a little bit. Okay well wait is it, I mean I think it's obvious but is it obvious from here that MDPs do not generalize POMDPs? Well, we would need to find some way of figuring out how to smoosh the observation function and the observed set of observables into the MDP itself. It doesn't seem so obvious that you can do that. Alternately we are going to do something a lot like that by expanding the states be significantly of the MDP but I don't know, I mean do you have something in particular in mind for that? Actually, probably just that. I was just thinking that I could, if I could tag all the states with the observables and update the transition function accordingly I could probably do it but I guess that's not the same thing as saying an MDP generalizes a POMDP. Agreed, yeah you can try to make an MDP out of the observations themselves but it need not be mark off, right, it could be that you can see the same observation twice in a row when in fact you've actually changed the state and you changed the distribution over what futures are going to be like from there. So I guess that's a slightly subtle point. But the fact of the matter is it doesn't really go the other way unless we expand our notion of state. Okay, cool. So here's a little, tiny POMDP that I've thought about in the past that I think makes makes for good illustration. So we've got four underlying states, these four here, the four circles, and we've got two actions. The actions essentially move the agent left and right along this kind of hallway type thing, with some exceptions. So from the green state, any action just resets you randomly to one of the blue states. And otherwise, things move left and right. At the end of the hallways here, these two end states, if you try to go right when there's no more space to go, you just stick where you were. And if you try to go left when there's no more space to go, you stick where you were. And just to ground it the rest of the way out, actions that take you to the green state give you a reward of one. So does that makes some sense to you? Do you do follow how this would go? I do, and I just realize, if I don't know which blue state I'm in, this is a scary, scary little problem to try to solve. Good, so that was the thing I guess I neglected to point out. I'm showing the observations in this diagram with the colors. So let me just number the states just for ease of referring to them. We've got three states, 1, 2, 3, and 4. State 1 is blue, state 2 is blue, state 4 is blue. Based on the immediate observation alone, we don't know really where we are. And we could be a place that's to the right of the goal, so we should go left to get reward. Or we could be a place that's to the left of the goal, which means we're going to go right to get reward. And so we're kind of in a tricky spot. Yeah, this is unfortunate. You're trapped like a rat in a maze. I see, I see. Yeah, you're pointing out that, [LAUGH] to you, this looks like a rat drawing, but it really is not supposed to be a rat. This is not the nose. This is not the tail. It really is just transitions. I think you doth protest too much. [LAUGH] So given that we think maybe we understand what the transitions and stuff do here, let's try o answer a question, and that'll at least kind of force us to think through how observations and actions, and the states, actually interact. So let's imagine once upon time, we took a left action and we saw blue. Then we took a right action and we saw blue. Then we took a left action. And now the question is, what's the probability that the color that we see next is blue versus green? OK. That sounds excruciatingly painful, but I probably can figure something out. Yeah, it starts to feel kind of like one of these liar paradoxes, right? Where you have to think about all the different ways that this might have been true. And then figure out what's contradictory to what. But nonetheless, I think that you can sort of intuitively work through this. And so, I think we should try. And then we'll develop some kind of math, in a systematic way of doing this next. Wait, wait, wait, wait, wait! I have one last question, I think, before I can do this. Okay, sure. So, let me be. Are you talking about left and right here? What about if I'm in state three? Can I still go either left or right? Okay sorry, yes. Both left and right from the state three, do the same thing. Which is it resets us to one, two or four. With equal probability. With equal probability. Okay. So now I think I've got it. So I go left or right in every state. It just so happens that in state three, it doesn't quite do what you want it to do. All right. I think I'm ready to go. So, go. We're going to work this through in front of everybody, but let's now work this through as the answer to the quiz. So the claim here is that, let's say that for whatever reason we know that we've started off in states 1, 2, or 3 with equal probability, a third, a third, a third. And we're definitely not starting off in state 4. Then, we go left, see blue, go right, see blue, go left. What's the probability of what we see after that? This time, this actually does have an answer. And the question is whether or not it matches the answer that you gave or if it's something else. Okay. So how can we how can we figure this out? Well, instead of keeping track of the states that we could be in we could keep track of the probability that we were in each of those states because you kind of representing start as probabilities. Great. Now, the reason I thought we'd end up with three quarters and one quarter, I thought that by writing down every time we were in some state, you go down the next state that we were likely to be in to. And you kept track of all the different ways we could get to one, and to two, and to three, and to four. We were actually keeping track of probabilities. But maybe I'm wrong. It's possible. All right. So, let's think that through. So, after we've gone left and observed blue, what's the probability that we're now in state one, two, three or four? From here? Yeah, we started off a third, a third, a third, zero. Okay. And now we want to know what happens after this first, we go left, we see blue. So, here's a question for you. So, can we just assume we start out in state one and figure out where we would end up with the probabilities, and then multiply that by a third and do the same thing with two, and the same thing with three? Yes. Okay, good. So we were in state one and we went left we would definitely see blue. Good. And be in state one of that point. Okay. So, maybe what you're saying here is that we don't know where we are. We're kind of split. There's little ghosts of us. A third of us here, a third of us here, a third of us here. But the third of us that is here, when that one goes left and sees blue, it ends up in state one. Right. All right. What about the ghost of us that's starting in state two here? It also sees blue and will end up in state one. Okay. And what about the state three here? Well, state three, no matter what it does, will end up seeing blue for sure. But it will end up in one, two, or four with equal probability. All right. So, let's let's figure out what now our probability is after this has all happened. What's the probability that we're in state, well, so let's do state three first. State three is zero. Not possible to get there. That's right. All right. What about state two? What is the probability that we're in state two? If you just count these things, there's one out of five. If we just look at it this way in terms of counting arrows, then we'd see one of the arrows goes to two out of these five. But it's not clear why that's justified. So, so- Well, wait. It's completely clear why it's not justified. It's not justified because- Okay, that's better. [LAUGH] because you have one-third outside each of those arrows and you can't ignore that. So, there is a probability we end up in two. Well, if we were in state three, then we had a one-third chance of ending up in two. But we only had a one-third chance of being in state three. So it's one-ninth. Excellent. And the same argument works through for four, right? And so now, since the probabilities all have to add up to one, we can deduce that it's seven-ninths. But it is a more direct way of getting that seven ninths? Yeah, it would be one-third times one plus one third times one plus one-third times one-third. Good, right. So a third of the time we started in state one, and all of those we'd end up in state one. A third of the time we start in state two, and all those we end up at state one. So, that's two-thirds right there. And then, a third of the time we start in state three. And a third of those brought us to state one. And if we add up all those pieces together, we get seven-ninths. Let's say six-ninths and one-ninth. That's seven nights. Right. Great. This is weird, right? But this is kind of where we are after that first step. Seven-ninths of us are in state one. One-ninth of us is in state two. None of us is a state three. And a ninth of us is in state four. And now, what happens next? Well, next I realize that we're almost certainly not going to end up in three-quarters one quarter. But who knows? Maybe we will. All right. So, if I was in state one and I went right, then not only do I definitely see blue, but I would be in state two. Okay. And there's no other option. If I was in state two, and I went right, then I would see, I would be in state three. And by the way, I would see green. So, something bad happens here, right? So, this can't, we couldn't have been in state two. Right. So does that mean I get to re-calculate what state I must have been in? That is a good question. So, yes. So, it seems like in principle we should do that. Well, let's finish seeing where else we could end up before we do that. So, if I were in state three, which I'm not, it doesn't matter. So, I couldn't end up anywhere. Okay. if I were in state four, and I went right, then I would stay in state four. All right. So, what's now our probability of being each of the possible states? And some of these are really easy. We know we're not in state one, that just can't happen. And we know we're not in state three, it just can't happen. But now it's sort of weird. It looks like we're in state two seven-ninths of the time, and state four one-ninth of the time. But that can't be, because it's not a probability. Okay. So, then what I would want to do, so, okay. So, using the reasoning I used before, I said, well, you should just take the probability you're in the state, and the probability that you ended up in the state, right? So that's how I got that one-third times one plus one-third times one plus one-third times one-third to figure out seven-ninths. Well, I want to do the same thing here, but I realize that the reason that worked before is because I had all the numbers adding up to one. And here, I don't actually have the numbers adding up to one anymore. So, I'm going to say it's still seven-ninth times one versus one-ninth times one. But now I need to re-normalize, and get that to be an actual probability. Okay. All right. So, that is certainly one way to make things add up to one again, is re-normalize them so that they add up to one. And let's just do that, that we have seven-ninths and one-ninth. That adds up to eight-ninths. So we need to divide everything by eight-ninths, which is like multiplying by nine-eighths. So, the nines cancel and you get seven-eights and one-eights. Which is what I wrote. Right. So the only question is, is that right? I mean, so, I think it's right, but that assumes, so, why wouldn't that be right? Because, well, what if by learning that there's no way I could have been in state two, that actually changes my original probabilities of what states I could have been? Yeah. Yeah, so we should work this out more formally. But let's, if, assuming this is right, we should at least be able to write down what the probabilities are now. Right. So, after step two, we go right, we see blue. And now the question is if we were to go left, what would we see? That's pretty easy. So, assuming our numbers are right, w e know that we're not in state one. So it doesn't matter. We could be in state two. If we are in state two and we go left, then we're going to end up in state one. We know we're not in state three. And we could be in state four. And if we go left, we will see green. We will be in state three. So, those add up to one neatly. So, that suggests that we will see green with probability one-eighth. Green with probably of an eighth. Yeah. And blue with probability seven-eighths. It's more likely that we'll see blue, but it's not impossible that we'll see green. Yeah, so that's the answer I was going for. And it's not, I mean, basically, the weird step was we just at some point normalized because things didn't add up to one. And that was the right thing to do, but it's not a great way of reasoning it out. We need to be a little bit more systematic and careful about that. But, yeah. That's, we didn't quite get to three-quarters, one-quarter. We got seven-eighths, one-eighth. Yeah, but that's the same thing. For sufficiently large values of one-eighth. Yeah, well said. Thanks. So the next from that we need to worry about is, the problem of state estimation. And as we were just talking about a couple moments ago. We can make a POM D.P. into a M.D.P., by expanding out the state space. What we're going to do is, consider what we call belief states. So, the state that the decision maker is going to use in the context of a POM DP is, going to be a belief state b. And what a belief state b gives you is, that if you tell me a state like s. Belief state b(s) tells us the probability that we're actually in state s at the current moment in time. So b is a sort of like a state, well it's a belief state. So it's like what we did in the last example. It's kind of a vector. It's a distribution over states. Yes, exactly. It's a probability distribution over states. Yes, and so and that's going to that's going to encode for us. It's going to retain for us the information about the history that might be important for decision making. Okay, so gets us all the way back to the beginning. Okay, so b(s) then, so b is just a vector and it's indexed by s and s, instead of being an integer is just a state. Right. Okay, that works. It's a probability distribution. All right, that makes sense. Okay. So now what we need to do is, figure out how this belief state gets updated as we take actions in the world. Okay. So the way we're going to think about that is, we're in some belief state, we choose an action. The world gives us back an observation and we need now, a new belief state to be formed from that. Okay, so that's exactly parallel to the, we're in a state we took an action, then we got into another state. Right, so what we really are doing here is, we're building up a notion of a belief MDP. So, we're going to turn the POMDP into a kind of MDP, specifically the belief MDP. Where the states of the belief MDP are probability distributions over states of the underlying MDP in the POMDP. Okay, sounds good. So, an aside Michael, is if reward is a function of state. Which it sometimes is, so you should be able to use that to update your belief state. Right, so in what we're going to be talking about, the reward is essentially not observed. But of course in reality, if you have a learner that's in some environment and it's making observations. And getting some reward back, it can actually, and if it knows something about how that reward is being generated. It can use that as a kind of observation and so, we can essentially assume that the observation has whatever information might be in the reward in it. Right, so in fact, the observation is whatever sub teachers you get to see plus the reward. Yeah, I've seen some people write it this way, where you say that the reward is actually something about the observable. You know, all the reward you get, is extracted from what you observe and so, anything that you model in the POMDP. As being reward relevant, has to actually show up in the observation. Right, that seems reasonable. Right, otherwise you can't learn if you can't observe it. Right, that makes perfect sense. All right, so we need to do is, derive how we get this probability distribution over states b prime from the old belief state. The action, the observation, and whatever quantities in the POMDP that we have. So, what we need to figure out is, in this new belief state, after we've taken action a. And seen observation z, what is the probability that we're in state S prime? And so, we're going to have to do some kind of probabilistic, I guess inference or derivation to work that out. Sure, we ought to be able to do that right, because we have all the quantities that we need in the POMDP, I think. Well that's the hope, so we want the probability of the next state, given that we're in a belief state. Took an action and made an observation. So, can we manipulate this expression using the laws of probability to make sense out of it? Sure. The good answer. So what would be a piece of information that if we knew it, would make it relatively straightforward to figure out what new state we were in? Well if we knew what state we were in before, since we have a transition model we know which states we are likely to be in next. Yeah okay, let's just write that. That's a that's a really good start. So what we're saying is, let's break things down by the possible state that we started in. And what we want to know and we're going to weight things by, what's the probability that that's the state we started in? Given B A Z. Then given that, what's the probability of the next state given that state? So this is going to help us break things down, successfully. So let's see if we can do that. So what do we know about the probability of being in a state, given a belief state that we took an action and we made some observation? So I think the thing to point out here is that the observation that we saw is generated by the state that we ended up in. And so we're actually trying to figure out what state we are in, basing observation we got, when, in fact, the generative model goes the other way. So this is a prime situation to try to use Bayes' Rule. Prime. [LAUGH] It's an s prime situation for using Bayes' Rule. So that gives us the probability of making an observation, given the state that we're in, or that we arrived in s prime and carrying over the a and the s. And the probability that that's the state we ended in, given a and s, divided by the normalization factor. The probability of the observation, given a and s. Right. All right. So now we're getting really close to quantities that we recognize. In particular, the probability of an observation given the state that we just landed in is independent of the action and the state that we just talked. So this is going to be the observation function. This right here is the transition function the probability of landing in some state as prime given that we were in state S and took action A. And then this is just going to be a normalization factor at the bottom. Right. So substituting those quantities in and rewriting out this normalization factors is basically the numerator divided by the sum of all the possible next eight s prime. We get one component of the new police state be prime if we apply the same idea over all possible states as prime. Then we get a probability distribution over all the states that represents the likelihood of being in those states given that we In belief state b, took action a, and made observation z. So that's intuitively obvious, even to the most casual observer. But more importantly, it just uses all the quantities we already had. [LAUGH] It does, it seems like sort of when a TV series is over and they bring all the characters back for one last show. It's like we've got to B back, and the O back, and the Z back. Everybody is friends again. And it spells bOT. [LAUGH] And it spells bOT. So my point is that these quantities can be easily calculated from. Or sorry, this quantity, this b prime s prime can be easily calculated from quantities that we have lying around because we have the model, the POMDP. And we know what the previous belief state is and we know what action we just took and what observation we made. And so all this can be updated. And so we can keep track of this notion of where we are. This belief state notion of the belief MDP. I like it. Cool. All right so this doesn't actually tell us how to do decision making because all we done actually it's we turned a POMDP into an MDP. That by the way has an infinite number of states. So we can you know we have a number of algorithms that we can run like linear programming or policy iteration or value iteration. And [LAUGH] they all at best grow polynomial in the number of states. So it's polynomial in infinity, so infinity. So it's a constant. Yeah, no. [LAUGH] It's bigger than any constant. So this is problematic. We can't just Take this MDP now that we've defined it and just solve it using our existing algorithms, we're going to have to be a little bit more careful. And so I want to step through the algorithmic process for how you can actually do that. It's kind of nice I think. Okay, I mean, I can't wait to see how you make infinity finity, so let's do it. So the trick for doing algorithms like Value Iteration in infinite state space like what you get from the belief MDP is going to come only by looking really, really, really carefully at the infinite case and then showing that we can be represented in a finite way. So what we're going to do now is actually step through what Value Iteration would look like in POMDPS. And then piece by piece, we're going to convert that to something that actually can be computed. So at first, it's going to be math and then it's going to be an algorithm. So here's Value Iteration written out and it's in POMDPS but sort of doesn't really matter yet. We're going to define the value function at step zero, as a function of the belief, to just be zero everywhere. And then we'll say, for t greater than zero, the value function for time stamp t as a function of belief state b, is going to be the max overall actions, the reward for taking that action in the context of that belief state. Plus the discounted expected value of where we end up right so that we're going to observe is an observation z. And so we're going to let sum over all the possible observations the probability that we make that observation times the value function at the previous time step for the resulting belief state b prime, which is the what we get by doing what we talked about on the previous slide which we call the state estimation for b, a, z. All right, that seems reasonable. I mean other than the fact that r is not a function of b and a that just it means the reward you actually got. Right so let's, what is this, what is this like, what is the reward for being being in the belief state b given that we take action a, how can we represent that in terms of quantities that we're more familiar with? By unrolling probabilities or something the same way we did last time. Yeah and in particular all we have to do is say well you know we would get the reward for state s we were in state s. So let's just sum overall states the probability we're actually in that state the reward that we would get from that state and of course this is just the belief state for for state s. So this is really just the dot product of the belief state with kind of a vector that you could make out of the rewards for a given action. So, it really is just the the average award you expect given the belief state. Exactly. Okay. And so again now that we've got that out of the way, the scary thing here is this function v this value function is defined over an infinite set of belief states, these vectors of probabilities over states. So again we're not going to let that scare us quite yet, but it is something we have to keep in mind. Otherwise, we could just implement this directly. All we would need is a loop that would say for all belief states do this update. It's just that there's an infinite number of them so that would not terminate Yeah but you could write it in really just a few lines of code. [LAUGH] Yes, you could write an infinite loop in very few lines of code. Beautiful really. This is how we're going to jump from the infinite to the finite is we have, we're going to make a claim. And the claim goes like this, that for all t, the value function times steps t, over all belief states b, right so this is over the infinite set of possible belief states, can be written in a finite way. In particular, the maximum over some set of vectors gamma sub t that we're going to have to deal with later. But the point is that it's a finite set of vectors. The maximum of the dot product of the vectors in that set with the belief state. Okay. Where a dot product is really just the sum over all the states, the weighted probability being that state times this alpha of s. So this kind of function is sometimes called piecewise linear and convex and the reason is, that if we actually, if we think about belief space as being well it's just a really simple one, like the space of probability distributions over just the two possible states. So we have some probability between zero and one and if probability one represents we're totally definitely in state S1 otherwise if the probably zero, were totally definitely in state S0 and in between where in between there some probability of S series some probably of S1, In the halfway in between would be half of each. Okay? So what functions of this kind have in common is that each of these alpha times b is a linear function of the belief state. All right? So each of that alphas in the set capital gamma sub t is some linear function of the belief state and what we're going to do is we have a whole bunch of them at any given belief state b what we're doing is we're taking the maximum value of all these possible linear functions. So at the end of the day the actual function that we've represented is this upper surface of these linear functions each of which is defined in the set capital gamma. Does that make some sense? Sure, makes me think of something you did in the last class. What was that? Well, when we did the game theory stuff. Remember that? Yes, so right, so maximums over linear functions comes up in game theory as well. Yeah, it's a good point. And there they end up looking very piecewise linear. Yep, and convex, right? They open upwards to the top. And that's because of the max and the piece has linears because it's a finite set of linear functions. So it's probably not obvious why this would be true, but this is the thing that we're claiming, and I want to at least make it clear what we're claiming. The claim is that any value function at any given step of value iteration can be represented finitely as just a maximum over a bunch of linear functions. Each linear function can handle an infinite number of inputs. Right? And that is not so surprising if a linear functions can do that. But what is cool is we don't actually have to represent the value for each possible belief state b. Instead, we just throw all these linear functions into a bag, gamma sub t. And then when we want to know the value for a given belief state b, we compute all the linear function values and just take whichever one is the highest at that point. Okay. I mean I guess that that makes sense in the abstract. I guess the only thing that I don't yet get is exactly where gamma comes from. Yes, that's what we have to step through. And that is not at all obvious, I think, but it is very elegant that it all works out. And this was originally shown by a fellow named Sondik and once you see it, it's maybe not so hard to see, but I don't know how he got here in the first place. It's, to me, not all readily apparent, but once you see things this way, it gives you a very powerful way to work with these infinite-sized state spaces. You just have to take operations on bags of vectors and so it ends up all being finite. So, what we're going to do actually is a proof by induction we're going to show that the base case is that we can represent the value function at times stamp 0 using a finite set of vectors. And that if we can represent the value function for some state T minus 1 with a finite set of vectors then we can construct a finite set of vectors for representing the value function at T. So, let's start off with the base case, so that's this is going to be a quiz because it's really often base cases are not so hard this base cases not so hard. . Here's what we need to do we need to show that there exists a set of vectors Gamma sub-zero such that we can actually represent the zero with step of value in a ration function over all possible blue states as the max over Alpha in the set they were about to define as the dot product between that vector Alpha and the blue state b. So, what you need to do is figure out what does this bag of vectors look like that has this wonderful property of representing the zero with value function. All right, I got it. So, let me let me give a hint. Okay. Can you remind everybody what is V0 of b? 0? Yes. So what we're trying to do is figure out a set of vectors such that you get that. Okay, I can do that. Let's just kind of work this through. Okay, well actually, I think there is an easy answer. Okay I guess we'll discover whether it is the answer. As you pointed out before, there's actually a big hint. We know that the value function is supposed to be 0 everywhere. Right. And we know that the values for B, of course, are whatever, they all sum to 1 and they're between 0 and 1, the usual thing they're probably. Yeah, it's like a vector and the dimensionality of that vector is the number of states in the MVP. Right. And the sum of all that's to be 1 and blah blah their non negative. So the easiest way to get something times something to equal to 0 is to have one of those somethings be 0. Good, okay. So, I would say that the gamma is well, 0, the 0 vector. 0 vector where the dimensionality of this vector again is the size of the state space. Right, so I would just write 0, capital zero. [LAUGH] Right, this is how I usually write those things. How do you make a 0 capital? You make it bold or you put a little arrow, a little vector symbol on top of it. You're drawing the Green Lantern symbol there but okay. [LAUGH] It sort of happened, I didn't mean for it to happen. Right. So the answer is 0. Because 0 times something is 0. Right. We only need one vector in that set. Whatever the least state we take the dot product with that 0 vector with is going to come out with 0. The maximum over that one vector is going to be that 0 value. This is just going to spit out 0 everywhere. So do you agree that that was kind of easy? That's pretty straightforward, once you remember that we set the value to be equal to 0. Did we do that on purpose? Yes, there's another thing that you could do that actually makes things work nicely also, is to say, that the value of times subzero is whatever the immediate rewards are. That turns out to be okay as well. And then what would the gamma need to be 1? No, it ends up being the reward vector. Right. Because then you add them up. Yeah, yeah, that makes sense. Okay, fair enough. And as you might expect, in fact as you told me offline that you did expect, what we're going to do is Vt is defined using the Bellman update, right, the Bellman equation in terms of Vt-1. So what we're going to do is structure things around that Bellman equation. So what I've done here is broken up the Bellman equation into kind of kind of a series of steps. Vt(b) if the max over actions of something, what is that something? That something is the sum over all observations of something else. And what is that something else? That turns out to be the sum over all states of the reward function for that state of given that action times the belief state. And then normalized by the number of observations plus the discounted expected value of the next state. The expectation comes over the observation z. And then the value the next state is Vt-1 of the state estimation of b, a, z, all right? So this is really just the Bellman equation. But I kind of unrolled it, I guess, or kind of un-telescoped it so that each of these operations' going to be easier to build out of vectors. Okay, so the sum of the s is just for the thing on the left side of the plus. Yes. Right, because it's just the reward. Right, and what's going to happen is this side is also going to have a sum over s in it, weighted by b(s). Mm-hm. And that's going to allow us to actually think about this whole thing as a dot product of the belief state with some kind of messy vector. Okay, I like messy vectors. So here's what we're going to do. First thing we're going to note is that if each of these things could be represented by a bag of vectors max over a bag of vectors then all the other ones can disappear represented as max over a bag of vectors. So the easiest one is the very first one the first one says that the value function at time sub t is the max over actions of some other kind of thing. If this is represented as the max of a bunch of dot products over belief states then think about what happens if you just take all the vectors for all the different actions and throw them into one big bag. So imagine that we've got one set of vectors that represents a value function And then we have another set of vectors this is for one particular action then we have a set of vectors that represent the value function for different action called a2. So the thing to note is that the max over the bag of vectors this upper surface Is actually the right thing. That if a1 gives the highest answer in one case then we get that. If a2 gives a higher answer in another case we get that. We can just pull all the vectors together so this is just a union over all actions of the bag of vectors for each of those actions. This is a way of writing max as at the operation over sets of vectors. Sure I believe that, that all makes sense. The next piece is that the value function for an action a timestamp t for a belief state b we said that it was the sum over observations of some particular vector that we get for that observation. If these can be represented using bags of vectors then this sum, over observations, can also be represented using operations over bags of vectors. And in particular what we wound up having to do, is take those bags of vectors and compute what I like to call across sum. It's basically, the sum where we choose for each of the observations, we choose a vector and then we add those factors together. It ends up producing the same effect as this sum. So what we're doing here is now given a set of vectors one set for each action observation pair. We're going to create a bag of vectors just for an action by taking this sort of cross sum, summing out all the different observations. Because that's really what we're doing, we're summing out over the observations. So that's just really advance what we're writing which is you wrote. Yeah but again it's just letting us go from this function v which is over an infinite state space b to just a finite set of vectors. We have a set of vectors and we generate from that a new finite set of vectors. So in this case the number of vectors this first step, the number of vectors is the sum of the size of the sum number of the vectors In all the sets gamma at. Here were actually multiplying, so it's the product of the sizes of all the sets which is finite but it could get actually quite big. Given what you say that we should say out loud that you're making assumptions that everything else is finite to like the number of actions. Sure. That was already built into the definition of MDP's that we were talking about them. Fair enough. But yeah it's probably worth repeating that certainly if the number of actions is infinite then this is problematic. This union is problematic. And if the number of observations is infinite then this cross sum is problematic. So all but, yeah. Finite state space, finite action space, finite observations space. Okay, so I'm getting the idea. Basically if you can start out with something finite at every step of the way you might grow but you're never going to grow an infinite amount in a single step. And therefore, so long as the number of steps is finite, or bounded or somewhere like that you will maintain a finite number things in your bag. That's kind of what you're doing that by step. Okay. That's what we're going and and yeah and if it feels like there could be a lot of vectors this is true it actually can grow doubly exponentially with t. The number of iterations of value duration we're doing. But for now we're pretending that's not a problem. Yeah, I mean look exponential is bad unless you're comparing it to infinite. [LAUGH] That's right, that's right. We set things up so that exponential's actually awesome. Yeah, because it's finite. So what the last one? So we're going to do in the last one. I probably shouldn't go through all the grungy details. Though all the all the pieces that you need for this have been stuff that I've said but what we're trying to do now is represent this value function vaz of t. Applied to belief state B and we said that was going to be basically a dot product of the belief state with some kind of reward vector for the current action plus a weighted version of the value function for the previous time step. So it's going to be helpful to actually take the value function of the previous time step and write it out more explicitly. So this is basically just expanding on the definition of the state estimator that we did in the previous slide. Mm-hm. We're going to say that the value according to the t minus one time step value function. For the belief state that we get from taking action a, and making observation z from belief state b. We can write this out as the maximum. Overall vectors in that set of the dot product of that vector with the resulting belief state which we had previously derived to have this form. Okay, the observation function times the transition function times the belief state and then normalized and this normalization factor is the probability of making the observation z given that we are in belief state band action a. This is a highly non-linear function but this is the awesome part ready, since we're going to ultimately multiply this value function times the probability that z is the actual observation. In other words probability of z given b comma a. This and this end up canceling and we're left with is an actual linear transformation. So this this weird divide by thing that actually makes things really yucky cancels itself out. So convenient. [LAUGH] Just like life. Not usually, but math sometimes does this for us and so at the end of the day we end up with a quantity that is a dot product with the belief state and another quantity that is a dot product with the belief state. We can factor out the belief state and what it leaves us with is a giant vector one for each combination of observation, action, and which gamma is going to be the maximum at that observation. So we actually have to take the product of the number of vectors that was in the t minus one set raised to the number of observations times the number of actions. And that's going to be the maximum size of the set that we get for representing the t step value function. So like you said, could could blow up. But still finite. Can't block to be infinite if it started out as finite. Nice, so that feels like a prove with induction to me. Great, okay and so you wouldn't want to actually do this. There is code in burlap for doing a lot of these processes, there's also my collaborator Tony Cassandra created a program called POMDP-solve that you can download that does all these calculations carefully and, you know, on your behalf. So I didn't really want to get this to the point where you could actually necessarily code all this up, just to believe that it's code upable and you can use an existing implementation If you want to actually run. It very mathematical of you. So there's one more step before we can get something that I mean you can actually implemented this way but this is guaranteed to be exponential because you're actually combining all these things together. There's one step that we can do that if it's the case that the Q functions have a small representation. Then the algorithm will run quickly. All right. Here, it could be that the Q functions are actually very very big. In case it's not obvious, the Q functions are represented right there. Yeah. I called it va sub t. But that's really the Q function. I like it. I like it. To explain how we can actually keep this from getting out of hand if it doesn't need to be, we're going to look at one special operation. And let me draw a picture of it first and then we'll talk about it more mathematically. All right, so do you [LAUGH] do you see what I drew here? A web. [LAUGH] Yeah, it doesn't look so great, does it? So this is intended to be a piecewise linear convex function, so basically a value function. And remember that what we end up doing is over belief space, that's the black line here, we're going to be taking the max over all these linear functions. So we get this upper surface. Wait, I see, you can get rid of any of the vectors that don't ever participate in the max because they don't matter. Yeah, and so that's going to keep our vector sets potentially from blowing up if they don't need to. So in this particular case I drew, what, one, two, three, four, five vectors. But only three of them are really needed to represent the function. This one's not needed and this one is not needed. So we can define a computational operation, that we call purge, that takes a bag of vectors and shrinks it down, purges out any vectors that aren't really needed to represent the upper surface, the piecewise linear convex function. Wait, how computationally complicated is it to do that? That's a good question. Thank you. [LAUGH] So how computationally complicated is it? It's at least linear. It's at least linear, yeah, and so there's some quick tests that you can do. So let's look at this vector first, this vector number one here. If you look, what you can see is that over belief space, this 01 zone here, there's another vector, specifically this one, call it vector 2 that always dominates it. It is always the case that you can do better by taking vector 2 instead of vector 1. So if there's a vector that is dominated by another vector, you could totally throw it out. But you ought to be able to figure out by doing something cute, like multiplying them together or taking their cosine or their angle, there's something you ought to be able to do to figure that out. Actually all you have to do is check the corners. If it's bigger on both 0 and 1, that must be bigger the whole way through. Well, in particular, let's say we have two vectors like this, with components alpha 1, alpha 2, alpha 3, and this is this vector. And the other vector's represented by alpha 1 prime, alpha 2 prime, alpha 3 prime. If it's the case that alpha 1 is bigger than alpha 1 prime, and alpha 2 is bigger than alpha 2 prime, and alpha 3 is bigger than alpha 3 prime, then this vector is not needed, right. No matter what belief state you stick in, you're going to get a higher value out by using this vector than that one. Does it have to be strictly dominated, or can it just be dominated? So, if one is greater than or equal to the other, you can get rid of the one that is less than or equal to, right? Yes. All right, that's neat. Yeah, there's some subtleties to doing this consistently. In my dissertation, I talk about how you can use a lexicographic ordering of the vectors to make sure that you only keep things that really are going to be needed someplace. But that is a subtlety that I don't think is very important for this. This dominated check is actually really easy to do and it's very fast, but unfortunately, it's not sufficient. So let's look at this vector, we'll call it number 3. This one is better than each of the other vectors, at some point along the way, but the union of the other vector, the union of these three vectors dominates it. So showing that is actually considerably more difficult. Knowing that 3 doesn't matter, isn't needed, because over here it's dominated by this vector, over here it's dominated by this vector, and over here it's dominated by that vector. Wait, what you said isn't true. 3 is dominated by the one that first does the max all the way through. You mean this one, 4? Yeah. Yes, that is a good point. [LAUGH] All right, that sounds like a quiz. Okay. All right, so what we've got now is a piece-wise linear convex function represented, in this case, by a vector that goes from p1 to q4 and p4 to q1. And now, what I'd like you to do is add a vector to this set that has two properties. One, it is purgable in the sense that it actually isn't above the maximum at any point, but it isn't dominated, one or the other of these vectors isn't greater than or equal to it everywhere. So, it needs to be a vector that's not dominated by either of these vectors but doesn't actually come up above the max of these vectors. Okay, I can, I think I can do that. Cool. This holds, can I help you thinking about these kinds of linear functions? Really, they end up being used in extremely high dimensions which can be hard to work with, but for this small dimension one, low dimension one, I think it's not so bad. I think this one is actually pretty straightforward. Okay, so I think the students will be able to get this. So, go. Okay, so I think this one's actually fairly straight forward, because of the way you've drawn it. This is ultimately a multiple choice, and there's only 16 possible things. So we could just go through all of them. We could, and let's start with just coincidentally. How about p3 to q3? [LAUGH] That doesn't seem like much of a coincidence. All right, so if we if we draw this factor from p3 to q3, */then what property does that vector have? Does it participate in the max? It never once participates in the max and yet it crosses both of the lines and therefore is not dominated by line. I mean, we got really lucky there. I guess that's one way to think about it but I mean it's sort of worth considering You know like why not Well, do all the parallel ones, do P2 to q2. P2 to q2. Well, P2 to q2 ends up being part of the max. So that's not good. That's right. As would P1 to q1. P1 to q1. Yes, definitely. In fact, it is the max. Yeah. It makes everybody else irrelevant. And P4 to q4, they were participates in the max-+ .+3 but is dominated by both of them depending upon whether equal two counts is dominated. Yes, so let's, let's just draw, let's draw this one in. The P, Q four to Q two. So this vector is underneath this vector always right so we could just Just check this one vector to know that this was not needed. Right. And so that's not what we're looking forward, it is dominated by this other vector. And they would ask me to repeat 2 to q4 with the other vector and then anything that goes down from there. So P4 to q2 is dominated by P4 to q1 and so with P4 to q3 and P4 to q4. All right. So this is the unique answer to this question. Yes. And so, one thing that I think is actually worth pointing out while we have this particular picture is when you're dealing with these kinds of Alpha vectors. These, these these linear functions over belief space this p1, p2, p3, p4 Q1, Q2, Q you can think of those as actually being the components of the vector because the corners of the lease space are the places where one component is one. And all the other ones are zero. So the actual vector itself has exactly this form. That this vector P1 to Q4 actually has a value of P1 in the first position and a value of Q4 in the second position, and so that makes it more straightforward to kind of do the managing or whatever. So what is it wouldn't actually give me of a final answer of what the computational purging would be. Yes, so this was just an aside so to actually do this kind of purging involves solving a linear program. So it's big O of a linear program. Yeah, that's right. So it's basically polynomial with respect to the time it takes to solve linear programs, which is polynomial, but it can be expensive. But what we have found is that it still does help to do all this purging, to run all these linear programs to get the redundant vectors out. Because if you leave them around, they just cause things to blow up. Right. And since you keep purging things even though you're going to run a linear program again, you're running a linear program over fewer things. Right. And so, this sort of interspersing purges among all the different vector operations actually tends to keep things. If it's going to be small, it tends to keep things small. Okay, this makes perfect sense to me. Do you actually see the linear program? Sure. [LAUGH] I did not expect you to say yes to that. I didn't. I said sure. Right. We'll do that next time. So now that we have an idea of what POMDPs are about. Maybe we should talk a little bit about what it might mean to do reinforcement learning in them because we basically talked about planning. If we have a model of the POMDP we can run some calculations we can run value iteration to get a way of deciding what to do actually did I say that? I think I did not say that. I think what I said is how you get a value function. Is it clear how you would use a value function to get a policy? In the same way you would for an MDP right? Yes? No? That's not true? Yeah that's right. If you have the model, then you can use one step look ahead with the value function to figure out what the optimal action is in any given believe state. Once we've run valuation and gotten an approximation of the optimal value function, and what about that valuation doesn't actually necessarily converge. It doesn't? You didn't tell me that. I did actually when we do in less than triple a. Lesson triple a? Yeah the advanced algorithm analysis. Well we said there is that value iteration converges in the limit to the right value function but after any finite number of steps it need not have the optimal value function. But it will after some finite number of steps have the optimal policy that's not going to be true in the POMDP case because there's an infinite number of states. I see. But it is going to be the case that we get an arbitrarily good approximation after some finite number of iterations. Wait arbitrarily good approximation of what? Of the optimal value function. Wait but not necessarily optimal policy? Yeah if you do one step back ups or one step look ahead with a near optimal value function you get a near optimal policy? Okay so that's nearly good. [LAUGH] Now that we've talked about planning in POMPDs we should talk about reinforcement learning in POMPDs. Charles, what's the difference between planning and reinforcement learning? Well, in planning you know everything in a reinforcement learning you don't know everything and you have to explore and exploit and do all that other stuff in order to find things out like you don't know the model necessarily and that makes the problem harder so what you're telling me is reinforcement learning is harder then planning in the way you're using the words And POMDPs are more difficult to deal with than MDPs. So reinforcement learning in POMDPs should be like, is that additive, multiplicative? So reinforcement learning in POMDPs is hard. Hm. Like, super hard. Actually planning in POMDPs is formally undecidable in the sense that [LAUGH] Undecidable? Yeah, [LAUGH] if I give you a POMDP and ask, is this the optimal first action to take from this belief state? If you could solve that problem you could solve the halting problem. Holy cow! Wait, wait a minute, wait a minute, wait a minute, wait a minute. That has profound implications. No, it doesn't. Yes it does, because we're human beings running around in the world we're living in a POMDP because we can't know everything. So you telling me that even if I could relive life an infinite number of times I still don't know what the right thing to do is you're saying that life is unknowable. It's undecidable you can never know what to do. That's profound. Wow I need a moment. So basically planning is hard. So I assume reinforcement learning is also hard. So the results that we have for reinforcement learning in POMDPs are more empirical and algorithmic results, they're not really formal results. But still something that people try to do sometimes. If you have some kind of robotic system or agent system that's trying to figure out what to do, and the world that it's in is partially observable, then you have to do something like this. Yes, solve the halting problem. Well, no, no, getting the exact optimal answer is undecidable Well, then is it decidable that you can get it near optimal? Yes Okay, well, I feel better. Why don't you stop depressing me and tell me what to do, so that I can sort of feel better about this whole approach? Sure, no problem, I'm going to to ask you what to do. Okay, so in particular, do you remember we had kind of two main flavors of reinforcement learning algorithms for MDPs. Do you remember what those were? There's value iteration and policy iteration, right? Yeah, those are planning algorithms, but for reinforcement learning in an MDP, the two main branches, model-based RL and model-free RL. Yeah, sure, right. Well, that was obvious. Uh-huh, and do you remember [LAUGH] the distinction between them? Well, one used the model and one didn't. One learned a model and one didn't. Okay, well, same thing. You can't use a model if you don't learn it if you didn't know it. [LAUGH] That's true, that's very well said. So you learn a model and then you use it. Versus don't bother to learn the model and just do it. I think the little quip was, the world is your model. Whose quip is that? Probably Rob Brooks. All right, so we can actually use this same kind of distinction, this model-based RL and model-free based RL, or model-free RL, in the POMDP setting. Where in model-based RL you actually try to learn the POMDP and then you plan in it, and in model-free RL we try to map observations to actions. And we do that iteratively over time, so we don't actually build the model, but we do try to figure out, okay, when I see this, this is a good thing to do. Well can I ask you a question while where here before we jump in? So, one of the things that we know we learned in AI class 150 years ago is that if you don't know what's going on, you can often figure out what's going on by taking specific actions that guarantee you end up in some state that you actually know. Sure. So even if you're blind and you're trying to get to a particular place in the room, you could do stuff like, well, I'm just going to go left for 15 minutes and then I know no matter what happens, I'm going to be against the left wall, then I know where I am. Yes. And then I can do things. Do either of these methods do the equivalent of that or an analogue of that? Either could, in fact. Well, okay, well, that's good. Again, the guarantees on whether things actually work in this space are nonexistent, so it's not the case that we can always say, yeah, it's always going to do the right thing. It's going to figure out the simplest way of getting to a known state and then behave from there. But yeah, I mean I've seen both these kinds of algorithms do that kind of thing. Okay, good, good, well, that's promising. For learning a POMDP itself, it actually is really helpful to think about the relationship between POMDPs and other kinds of Markov models. So I've made a little two by two chart that I like to think about sometimes, and we're going to fill it in. The two by two, or you can have a Markov chain or Markov structure that is controlled or uncontrolled. And you can have the controller be able to observe the state or only partially observe the state. And for each of these four things, we get four different models that we've talked about not necessarily in this class. [LAUGH] Okay But it's helpful to think about the relationships between them. So do you think maybe we could fill this in? Well, sure, we can give it a try. Okay. So I guess what I'll suggest to the students. Is to go and they spend a couple of months reading everything. So they can answer this question. And then come back. All right? Can you do that? All right go. [LAUGH] Wait, wait, wait, wait. [LAUGH] I mean you know. Not to give away the farm or anything. But couldn't they just listen to the answer video, and then go back? It seems a lot faster. Well but you can't get to the enter video without first like clicking stuff and trying things. All right sorry. It's fine, it's okay. Well in any case, I think my previous advice stands. So go, go forth. You ready to give this a shot? Let's start with the easiest one. Well, which one is that? Let's say, what do you call a model that's partially observed and controlled? Well, we've been talking about that. That's a POMDP or a POMDP. PODP For those who are rich. Rich MDP, yeah. [LAUGH] If you actually can observe the state at all times then it's still controlled- Then it's a phone B.P. or fully observable mark opposite in process which we sometimes we just call MDP. Yeah, which is just an MDP. Right, it's just the difference between an assistant professor and social professor, and professor. You start dropping prefixes as you become fully observable if you go. Okay. All right. Like full professor, you can just say professor. Fully Observable Markov Decision Process you can just call Markov Decision- I guess it's the word full really that you can always get rid of. Right. You can always get rid of full. Because that went without saying. All right, so we know what MDP and POMDP is. Now the uncontrolled version is what we've haven't really been talking about in this particular course. But I kind of gave it away in the question. If you have something that's observed and uncontrolled, you basically just have, it's jumping around between a bunch of discrete states. Right, and we had talked about that a little bit we talked about passive RL. But this is really just a Markov chain, which are words you actually used earlier. Yeah, so MC. Yeah, MC MDP, my favorite. What is this? [LAUGH] All right, so then the last one. This is kind of the payoff. So you can go with POMC. Yeah, it could be a POMC or POM key. But I actually think we talked about this in our last class. Yes. Yes, and in particular in our last class we said we weren't going to talk about it. Really? I thought we talked about it. Well, we talked about not talking about it, because it's a hidden Markov model or HMM. HMM, right and so we so we really didn't talk about them? Do we know how to learn in HMM because here's here's where I was going with this. My goal was if we could remember the relationship between HMM and a POMDP, and we can remember how to learn and HMM, we can use that same idea to create an algorithm for learning. Yeah, I think that's fine. But do we know how to learn HMMs? Somebody does. I mean, but I'm pretty sure we didn't talk about it as a class. [LAUGH] But that's okay. We'll just have Chris or somebody like, do a whole thing on HMMs. And then you can just pretend that people are learning about it. Well, okay, no but I'm pretty sure we did talk about this idea. Yes, we did talk about Expectation Maximization and I believe you did mention that it could be used to solve HMMs. To learn HMMs Yes, to learning to HMMs which is like solving them. Okay, just a quick refresher on Then. So in We actually go back and forth between two quantities that we don't know. We don't know the model, the hypothesis, the actual thing we're trying to build up, and we don't know the expected value of the hidden variables given the data that we're seeing. But we can actually use each of those to improve the other. So we start off with a guess of the model, we use that to estimate the expected value of the hidden variables given that guess. Then once we have those expected values, we can actually use that to re-estimate the model find the maximum likelihood model. And that gives us a new model that we can and use to compute expected value in variables that we just go to tick tock back and forth like that. Right and so my recollection from the last class is that we did this with Gaussians and so we had a bunch of Gaussians and a bunch of data and we didn't know which of the Gaussian each data point came from So we started out with a model of what the Gaussians, which was there mean. So we had a bunch of means of the Gaussians. That was our model and then given the model, that is the means of the Gaussians. We figured out the probability of each point belonging to each of those Gaussians and that was the hidden variable. The hidden variable was, which Gaussian that each of the data points belong to. So we computed expectation over those, and then once we had the sort of probability of each point belonging to each Gaussian we were able to use that to reconfigure the means of the Gaussian. And back and forth and forth and back and back and forth and forth back and eventually you converge to proper for your question. That was the use of That we talked about. You can do a similar thing with HMMs where you're actually the hidden variables in this case or what state was the system in at a given moment in time in the observed data and it turns out you can do essentially the same thing with POMDPs. You imagine you've got some kind of hidden state model. You used your observations, of I took this action I made this observation I took this action needs observation to estimate the hidden variables, which state was the POMDP in, which are the underlying MDP in at each moment in time. And then we use that to reactivate the model of the POMDP. So that's actually all we need to say about that or all we can say about that. Lonnie Chrisman was the first person to point this out in the 90s that, hey we can learn POMDP the same way we learn MDPs. And is that fast or not slow? Yes that is fast and not slow. No that is fast. No that is slow. No well so the fact the matter is there's some interesting results that seem to indicate that there's some POMDPs that are really easy to learn and doesn't matter which algorithm you use. And there's other POMDPs that seem to be really hard to learn and it doesn't seem to matter which algorithm you use. There is no guarantee that we will be able to learn a good POMDP. It turns out that learning a good HMM is already known to be very hard so adding actions to it doesn't actually make it any easier in the worst case. That all we're going to say about Model Based Reinforcement Learning in POMDPs and now we're going to talk a little bit about Model Free Reinforcement Learning in POMDPS. Okay. And particular we're going to focus on memory loss policies and what I'd like to show actually is not so much about learning, but more about planning. So here's a little POMDP. This is a POMDP that has four underlying states. Two observations there's this sort of clear blue thing and there's a green thing. Two actions, black and red. Black takes you to the right and red takes you to the left back around the other way. And the way that this is set up, I think of it as a one dimensional hallway. And we don't know exactly where we are in the hallway. But we're trying to get to you know say my office along the hallway. If we're to the left of my hallway, we should go right to go to my office. But if we just keep going left and left and left and left and left we're just going to go and get stuck and It's infinitely long hallway, it feels like an infinitely long. We just going to get stuck at the end. Whereas if we're to the right of my office and we go to the right, we also kind of get stuck, we need to go to the left to get where we're going. Once we actually get to the office, we have a reset action, the black action resets us with equal probability to one of those three states. because I don't know we're drunk and then we get to go again try to get back to the office. Okay, good. First, I want to point out. I think we talked about an example just like this before and we said that probably the right thing to do is to go like left, right, right, that sort of thing. Because each time we reset we're in one of these states which we go left hoping that it's going to take us out of here, but that doesn't seem to work so we go right to take us from this side back out. Actually right, right, left is probably even better. Just keep doing this over and over and over again. All right. But now what we're talking about are memoryless policies. So memoryless policy has the form. When you're in the green state. Well, there's only one action choice so you take it. When you're in the clear blue state. You have to go either left or right. Or maybe you can choose probabilistically among left and right. But you can't do these sequences because it's memoryless. [SOUND] You need memory to do a sequence. Right, that's a problem. Okay I see, I see why this is a problem or you could just invent memory. POMPDP's do invent memory. If you're going to learn the POMDP model of this, it captures the memory that you need to actually make optimal decisions. All right, fair enough. So let's do this is a quiz. Okay. Here's what I want us to do. So one kind of memoryless policy that we could use in this example, that actually kind of works okay, is that each time we are in the clear blue state we go left 50% of the time, we go right 50% of time. So we actually just randomize, we flip a coin, and whichever, If it comes up heads, we go left, if it comes up tails, we go right. And so what we'll just do actually if we're in this environment what will happen is, we're in one of the clear blue states and let's see where in this state, there's some probability to actually get to the goal and some probability will actually go away from the goal. >From here, there's some probability that we'll get stuck, not get stuck here but we'll stay here for another step and some probably that we'll get close to the goal. And from here again, there's probablity that we'll stay where we are or we'll go to the goal, and so it doesn't take that long, the average reward that we get the average number of steps that it takes before we actually experience a reward and get reset, Isn't that long, right, certainly not infinite and the expected value is some particular value. What I would like us to do is figure out, okay can we do better than just a coin flip 50, 50. What would be a way to actually get a better score. How can we randomize between left and right to get a better score. Wait a minute before we say go I just realize I had a question. Okay. This is a normal MDP right so what's your gamma? Yeah I think it actually would be helpful to to give a specific value for gamma to set gamma equal to half to make the calculations easy. And what we're talking about is, you know we were just in a green state, we got reset and we're going to follow up a policy which is some fixed probability going left, some fixed probability of going right. What gives us the highest average discounted reward. And just once you get to the green just stop at that point, just consider the reward upon reaching the green state. Okay. So it's really very closely related to the number of steps. Right, that's what I was thinking. You want to get the fewest number of steps. So for any value of gamma less than 1, greater than or equal to 0, it's really the number of steps. Yeah. Okay, cool. All right, okay I think I got it. Okay, so I reassert my go. [LAUGH] So go. All right, so how do you go about solving this? Well, I think there is the right way of doing it, but then there's the way that I want to do it. And the way that I want to do it is I want to just ask myself, given that I just left green and I just saw blue, what should I do? Here's my reasoning. We want to do it in the fewest number of steps, so we always want to take an action that is likely to move us towards the green. So if I have an equal probability of ending up in any of the clear blue states, then in one of them I want to go left and in two of them I want to go right. And so I think a probability distribution of one-third, two-thirds will get me there better than going one-half, one-half. Yeah, so that's correct. I guess we can accept that. Yay! [LAUGH] Maybe, we should actually go through and convince ourselves that we really do get a better number. Yeah, I think that's fair. I mean, I'm pretty sure you get a better number. I'm not sure that that's the optimal thing to do. Actually, I don't know that. I worked this again in my thesis, I worked this through in the case of average reward where there's no discounting. It's just, we divide by the number of steps, the long-term sum of rewards and that actually ends up being irrational. There's a square root of two in the probability that you need. This is going to be a little different, so maybe we should figure out what this one is. Okay, sure. So let's see if we can work this through. In the case of gamma equals 1 and probability of left is a half and probability of right is a half. Turns out it will be nice to solve for the value function, so let's have three variables that represent the value function, one from each of those states. >From the z state, what happens? Well, with probability a half, we go to the right, which ends up giving us an immediate reward of 0 and then we get a discounted value of being back in z. Or with a half probability, we go to the left and we get a reward of 1 and we're done. So z is two-thirds. Yeah, so just boring old algebra. I see, you magically put one-half in where gamma used to be because gamma is one-half. That makes sense. Okay, fair, fair enough. I thought that would make things easier. All right, so z is two-thirds, and let's work out x and y. Okay, so I guess you will do a similar thing. Yeah, so the value at x from x with probability a half, we go to the left which gets us a 0 reward, and then a discounted value of being back in x. Or with probability a half, we get no reward, and then we have a discounted value being in y. Meanwhile y, the value from y is, well, with probability a half, we get the discounted value of being in x. And with probability a half, we get a value of 1, and then we're done. That's two equations and two unknowns. Yes, linear equations. That's even better. So convenient. Cool. And some normal algebra-ish kind of things later, I have x is two-elevenths and y is six-elevenths, better to be in y. So when we average our discounted expected reward from each of these possible starting states, we get a third of this and a third of that and a third or 0.4646 as our value for taking this stochastic policy. That's not even 50/50, ha. [LAUGH] Okay, and we said that we could do better. So, we said we could do better by choosing probability of left to be a third and probability of right being two-thirds, right? Yeah. All right, so let's see if that actually gives us a better number. Okay. So we need to work out what the value of z is in this case where we're going two-thirds, one-third. So once we solve through for z we find out that z is a half. Before, z was two-thirds and so we actually are doing worse in this state because we're spending more of our time bashing against the wall to the right. Yeah, that's cool. And we're going to write down what happens from these other two states. So the value at x which simplifies to 5x = 2y, and the value of y, because again we're going to go to the left one-third of the time, getting us the value at x. And we're going to go to the right two-thirds of the time getting us our value of 1. All right, so once we solve all that through, we get y is five-sevenths and x is two-sevenths. And so when we averaged these three values out, we actually get a half which is a little surprising to me that those 7s kind of cancel out, but a half is bigger than 0.464646, so we actually have a better policy. Right, and coincidentally it does give us 50/50. What do you mean it gives us 50\50? Well, you said you wanted to do better than 50/50, 0.5 that's 50/50. No, no, no. The 50/50 policy which gets us 0.464646, we did better than that policy by trying a different policy that actually gets 0.5. I know Michael, I was using word play. Brilliant. Thank you, so I suppose using all of that algebra, we could actually figure out what the optimal numbers are if they aren't one-third, two-thirds. Maybe they are, just by replacing all the one-thirds and the two-thirds with P and 1- P. Then we'd figure out what P is. Yeah. And I'm sure that would be a wonderful exercise for the students to do. All right so, we'll leave it for that then. Excellent. Now that we talked about doing reinforcement learning in a POMDP setting, we're actually now going to talk about a much weirder combination. We're going to talk about how we can think of reinforcement learning as actually being itself a POMDP. Okay. This is an area that is often referred to as Bayesian Reinforcement Learning. The idea is that we're going to keep a a Bayesian posterior over possible MDPs that we're in. And we're going to use that to actually optimally balance exploration and exploitations. We're actually going to behave optimally and reinforcement learning becomes planning. Okay, so does it mean we're going to use Bayes rule? Well I guess technically in that whenever your compute a posterior, you're actually doing something like Bayes rule. Yeah, but maybe this is not so clear. Let me try to draw a picture. So I'm hoping this example might make things a little bit clearer. So let's consider a really simple reinforcement learning problem. We've got two states, state 1 and state 2, two actions a black one and a, I don't know, purplish kind of one. And if we were doing reinforcement learning in this environment, and you were say the reinforcement learner, what would you do? Let's say we're in state 1, what would you do? Would it be better to do purple or black? I don't know. Okay so then what would you do? I'd pick one of them. All right, so let's say you pick black and then we would observe a transition. And ultimately we can sort of build out a model or use queue learning or something like that to work out the values. So what I'd like to do is actually try to have you think about this like a POMDP. And I think a simpler way to show you that would be instead of imagining all possible MDPs this could be, let's just pretend it comes from of a small finite set. Okay, are there any small sets that aren't finite? No. Good, then I think we're on the same page. All right, then I agree with you. So here's the set that I had in mind, we've got three possible worlds that we're in A, B and C. So these are three possible MDPs, right? Okay and they all look exactly the same to me except where you get reward. Yeah, so it just so happens that the actions all do the same thing in all three of these and we know that. We just don't know where the reward is. Is the reward something you get for purple when you go from state 2? Is it something that you get for purple from state 1? Or is it something that you get from taking the black action from state 1? And we don't know. Now if you had to solve this reinforcement learning problem, it's kind of a reinforcement learning a problem, right? You're in some state you get to see what state you're in, 1 or 1. You can try actions black or purple. And what you're trying to do is actually figure out how to maximize reward. But now that it's just a finite set of possibilities, we can think about this like a POMDP. Sure and in fact, I think this one's pretty straightforward. If I'm in state 2, I know what I should do. And what is that? I should take the purple action. And why is that? Explain. Because I immediately either get reward for it or I know that I get reward for being in state 1. And then when I'm in state 1, well actually it's not clear which one I should do. All right, so let's well let's just do two things. Let's start off by just tracking belief states in this because I think that's already kind of an interesting problem in and of itself. What we're thinking about here is that we're in POMDP with six states. Often state 1, but we don't know which MDP were in, then the belief state is going to look something like this. Agreed? Agreed, if this is where we're starting, we know nothing else, and we haven't done anything yet, then we can be in any of those states, presumably with equal probability. Good, right, so now the question is, if you were to do the purple action from here, what would your expected reward be? My expected the reward would be one-third. Right, so if we happen to have been in MDP B, which we don't know if we are and we took the action purple, we'd get a one. If not, then we get zero. But what also would happen if we get the one? What's our new belief state? If we get the one? Yeah. Well then we know that we are in B1. Good, B1, right. So we're in this state, so the belief state becomes basically fully observable at this point. We know which MDP, we're in. And we know what state of that MDP we're in. And we can just execute the optimal policy for that MDP, which in this case, as you might guess, is just purple all the time. Purple all the time, purple all the time. Yeah, I agree. Excellent. So let's make sure you get this, we'll do a little quizzy kind of thing. Okay. So here's the question, what happens if we start off in state one, we know we're in state one, we don't know which MBP we're actually in. We take action black and we observe no reward. How does actually change the belief state? Where do we think we are at that point? I don't know but I think I can figure it out Awesome excellent. Okay, so is there anything else you need to tell me? Do I know everything I need to know? I feel like you do, I have the discount factor is a half but I don't think you need that for this question and this is the current belief state, so you know that. So I feel like there's enough information. Okay, well we'll find out and if not we'll fix it in post. All right, ready? Ready. Okay, go. All right, so how should we think about this? Okay, I think this one's fairly straightforward because determinism. So at least for state 1, no matter what you do if you take the black action, you will end up in state 2. That means that I know right away that my belief for being in state 1 will be 0. So I can just go ahead and put those zeros in. But you also told me I got a reward of 0. Well, if I have a reward of 1, then I would end up in state C2. Since I didn't get a reward of 1, I cannot be in state C2, that would have zero. And since there's nothing else to distinguish state A2 and B2, I have a probability of one-half and one-half. So why is it not a third of third. It's a third of third but they're normalized. Right, because of? Because of the fact that you have zero probability of being in state C. No, no, no but what if we were actually deriving that probabilistically, we would use. Probability. I want to say Bayesian RL. Well, sure. Yeah, and so could you ask before, if it has anything to do with Bayesian RL. All right. So Bayesian RL's that does have to do with Bayesian RL in the sense that we’re tracking the likelihood of different possibilities. And the way that we do that is we make a transition, observe what we observe and then we normalize. We use the Bayesian RL trick. Right, but that was basically what I was saying. [SOUND] Thank you very much, I'm here all week. So at this point in time, we would actually be sure that we're not an empty PC, but we could be an A or B. And they're equally likely, what action do you suppose we should take from here? Actually from here we should take purple. Yes. In fact, we have to take purple because we know that we either get a reward immediately. Right. Or we end up in a state, we know what state we're in. Right. Actually no matter what we do after we take purple, we'll know whether we're in MDP A or B. Right. And if we're in MDP A, great, we got a reward of 1 and we should just go back and forth. If we did not get a reward, then we know we're in the MDP B and we should just keep taking purple from that point. Yes. So that points something out to me. We had everything we needed for this quiz, right? But if we had started out in the purple state and you told me that we took a black action. And got a reward of zero. And got a reward of zero. I need to know what state I'm ending up in. That's right. I think. Yeah, so I really should write state 1, action black and then we observe reward 0 and we observe that we're in state 1. Or state 2 in this. Right, in this particular case, state 2. Okay. By state 1, I mean state 2. Okay, good, I get it. So right, because even though we're turning this into a POMDP, there is an MDP. Our observables are actual states and so you need to know what the observable is to update your belief state in general. Right, and so when we represent this as a POMDP, we're actually saying the states of the POMDP are the six states, as we talked about. The observations are the rewards, the possible rewards, zeros and ones in this case. And which state we end up in, blue or purple, 1 or 2. And we don't know which state of the POMDP we end up in, but we know which state of the underlying MDP. Whichever one it is. Whichever state it is. So what does an optimal policy mean in this setting? What does it do? Well, I think that probably depends on the actual things, but what I want to say is that it's the thing that allows you to figure out which MDP you're in, so that you can execute the optimal policy. So it tries to figure out which MDP it's in, because that might matter for optimizing reward. Right. But it's balancing that against, well, maybe along the way of figuring that out, we have opportunities to get higher reward. Right. Which is to say exploit. So we usually or at least so the way we've talked about it so far, we make this distinction between explore and exploit. And what's neat about this POMDP or this Bayesian way of thinking about reinforcement learning. Is this goes away, this dilemma goes away. It's no longer, I'm trying to explore, trying to exploit, it's I'm always trying to maximize reward. And maximizing reward involves possibly taking actions to gain information. Which is to say exploring, but it's mixed in with actually getting the rewards. And in fact, the optimal POMDP policy for this gives you the optimal reward. It just so happens that some of the time that involves some exploration, some it involves exploitation. But that the algorithm doesn't make that distinction, it's just maximizing. Right, because it's not exploration of the POMDP so to speak, it's exploration of the underlying MDPs you might be in. And so you're always exploiting in some sense. You're always exploiting, but you're exploiting in belief space. Which causes you to actually sometimes gain information or make your belief state get more pointy. Right, more specific to where you actually are. Right, I like that. How would you figure this out? How would you figure what the right, so you asked me what the [INAUDIBLE] action would be. Right. And I figured it out using proof by staring at the screen, but it seemed to me that that's not necessarily an algorithm. Okay, so if we needed to find out the optimal behavior, let's say from this belief state. The one-third zero, one-third zero, one-third zero. What would you do with your computer to figure it out? You don't have to tell me the exact answer though I think I know it. Well, I would just search. Search but like it's a POMDP, so we just need to solve the POMDP. Right, sure. And so we talked already in this lesson about various kinds of algorithms for solving POMDPs. In particular, doing value iteration with piecewise linear convex functions, that sort of thing. Okay, I guess that works out to be the same thing. Fair enough. Yeah, and so this works in general as long as the space of possible MDPs is finite, but it's not generally finite. So then does something else work in general? Yes and no. So what I'm basically arguing here is that we can think of reinforcement learning as a kind of planning. It's not learning at all. It's actually planning in a kind of continuous space in POMDP, where the hidden state is the set of parameters of the MDP that we're trying to learn. Now there's an infinite number of those, which makes things a little bit awkward. And so we don't really get the piecewise linear and convex property anymore. But it can be shown that you can actually get a result that shows that the value function in this continuous space POMDP is actually piecewise polynomial and convex. Okay. It's not as nice as linear, but it's still representable. How big is the degree of that polynomial? It grows with iterations of value iteration. I see. Yeah, so it's not like piecewise cubic and convex, it's like piecewise, it could be a lot of degree [LAUGH] depending on how many iterations you do. So this is still pretty awkward to work with. There's an algorithm that actually works fairly nicely called BEETLE that tries to approximate this piecewise polynomial and convex function. And can sometimes actually end up learning very good approximations of the optimal way to do reinforcement learning for those spaces of problems. Does BEETLE stand for something? Bayesian Exploration Exploitation Tradeoff in learning. Which is cheating, I think we agree. I think we agree, yeah, but otherwise it would be like BEETLE, which would be just way too hard to say. Bayesian Exploration Exploitation Tradeoff In Learning Everything. [LAUGH] The E for everything. Yeah, and then that would be better. I think I could forgive that. [LAUGH] Learning excellently. No, it's too many, that's just absurd. The point that I'd like to make though or the point of the slide is that there are Bayesian RL algorithms, BEETLE is one of them, it is not by any means the only one. There's lots of different ways that people have looked at actually keeping Bayesian posteriors over the MDPs that are being learned and then trying to use that Bayesian posterior to make better decisions than you would in just reinforcement learning, where you don't have any kind of prior, any kind of structure on what the possible underlying MDP is. So a lot of people really like this stuff. It seems to, at the moment, be on the side of just kind of too expensive to be practically useful. Q learning seems to tend to win. But it's very elegant, and it's a useful way of sort of realizing that planning and learning actually are two sides of the same coin. We've used it before in some work that I've done. Okay. But what we were taking advantage of is the fact that it gives you probability distributions over value functions or rather over Q functions. How does it do that? You're in a state, and it just tells you basically the probability of an action being optimal. And that turns out to be very nice because you can then compare various sources of information together because they're all, if you can make the model look like probabilities. That sounds like that really cool policy sort of merging paper that I saw. Yeah, yeah, that one. Is that you? Yeah, it was by Isbell and his colleagues. Woo. All right. So, the last topic I'd like to cover in palm DP land is a line of work that that I am somewhat fond of, that is called predictive state representation. So let me let me say a little bit about the motivation behind studying this, and we'll talk a little bit about the representation itself. But we're not going to talk much about algorithms that actually make use of this. There is a literature if you want to dive in. All right. So, here's the basic premise of this line of work. So, we talk about POMDPs. So, POMDPs, when we're moving around in a POMDP we're tracking the belief state, which is a distribution over states, right? Right. So, here's the thing. The states are not observable, right? Right. So, how can we ever thought hope to learn in a PomDP, when we never actually get to see these underlying states? They're never observed. So, you could ask yourself do they even exist, or are they just kind of useful fictions of our imagination? There's nothing that we can ever do to test hey, was this the state we were actually in? The answer is I don't know. I mean, if we're learning in a POMDP, we're just creating this notion of states to provide some structure to the observations that we've seen. Is this like a tree in the forest thing? Yeah, yeah. If a POMDP is solved in a forest, does anybody know whether or not they exist? And the answer is me. No, I think you're right. I mean, we're making this stuff up anyway, right? Yeah. And in the case that somebody gives us a POMDP and says would you please plan in this, it's a reasonable representation to work with. But if you're in a reinforcement learning setting, you never get to see these underlying states. You can never actually collect statistics about them. In fact, most of the good work that I've seen that uses POMDPs, they don't learn the POMDP. They actually send, I want to say grad students out into the field to measure underlying states, and then measure the observation function. They actually kind of instrument the world to build a POMDP, and then they can act in the POMDP successfully later. But it's just there's really no good way to learn about the states. Or even that they're real even. Eve that they're real. Okay. So, now that you've blown my mind, what does it have to do with? Right. So, so, the idea of a predictive state representation, or P.S.R., is that the thing that we're going to use for representation isn't going to be probability distributions over these possibly fictional states. But instead, they're going to be probabilities of the outcomes of future predictions. So, I'm in a state where it's about 80% chance it's going to rain tomorrow. Okay. Which doesn't require an actual state. Well, and that is maybe not a great example. So, let me give a more concrete example to kind of ground this idea out. So here's an example that we're going to use to kind of motivate this idea. So here's a representation of a POMDP. It's got how many states? Four. Good. It's got how many observations? Three! Right, blue, green, and red. And so these two states are indistinguishable by their immediate observations. And let's say there's four actions, north, south, east, west. So north from this red takes you to this green, north from this red takes you to this blue. East and west from this blue leave you where you were, etc. Does that make some sense? Yes. So we've got left, right, up, down as our actions and these four states and these three possible observations. And so we can represent this as a POMDP. If we know these dynamics, we know this system, we know this model, we can represent what's the probability we're in each of these states, one, two, three, four, the states. And so for example, if I say okay, I'm going to start off uniformly at random. And by the way, the state you're in is currently red. So then our belief state would be what zero, zero, a half, a half. If we then go left, then our belief state would be zero, zero, a half, a half still. Right. If we went up and observed green, then our probability distribution would be zero, one, zero, zero. We'd definitely be in this state. And actually, given that there's no stochasticity in this case, we would know where we were forever after, right, and we would continue to act and we'll know where we are. We're going to to try to re-represent the same idea, the same system, the same model, but not using a POMDP but instead by using a set of predictions. So I'm going to claim that the tests that we're going to use for this representation, well, there's lots of possibilities, but we're going to do is say, hey, what would happen if you were to go up? Would you see blue? So what's the probability that you'd see blue after going up? And if you were to go left, would you see red? What's the probability of seeing red if we were to go left from the current state? And so again, the state is going to be predictions. It's going to be probabilities on these two quantities, and we're going to try to keep that updated. We're going to keep these predictions updated as we move around in the environment. Okay, so we're replacing a distribution over states with a kind of distribution over the outcomes of tests that we might take. Yeah, I think that's a good way to think about it. Probabilities that we get here don't necessarily have to add up to one, right? That's sort of independent, the question of how likely is it to see blue when we go up? How likely is it to see red after we go left? But other than that, it is a sort of probabilistic quantity. But we kind of like this notion that these tests are actions and observations because in any given time, we could say, hey, this seems like the kind of situation where it's two-thirds probable that I go up and see blue. Let's just try that just to make sure. So we can go up, see whether or not we see blue, and see how often that actually seems to happen. It's still not trivial to learn in this case, but at least it's grounded out in observables, actions, and observations. Right, and I guess sort of philosophically, if I might wax philosophical for a moment, there's really no difference between this and states, right, because states are kind of made up things, which represent states of the world, dare I say it. Then these tests also kind of represent, at least implicitly, other states of the world and are just as fine a reasonable representation as these made up nominal circles. Yeah, they're not certainly no worse, but I'm going to argue that in principle, they might even be better. Because again, you can actually execute these tests and see what the outcomes were. And then that will tell you where you were and possibly where you are. Yeah, or at least would let you validate your current predictions. Whereas with states, if I think, okay, I'm in the belief state zero, a half, a half to zero, there's nothing that you can validate. There's no test that you can run. There's no experiment you can do in the world that would tell you what the states actually were because states are not observable. But these tests can tell you sort of whether you are, I'm going to try to ground this out an example. So, am I sick, do I have a particular disease? Well, there are tests that I could run that would tell me, and then I might be able to do other more interesting things. And that's kind of a practical thing to know, rather than asking the question, I mean, it's one way of sort of answering an underlying question of am I in the state of being sick or in the state of, I don't know, having blue hay fever. [LAUGH] I guess that's this upper left state. Yeah, so I'm intrigued, so what's next? What do we do with these tests? Well, so in principle, we should be able to learn PSR representations. And because again, if we have a test, we should be able to validate it and make our predictions better over time. And in principle, we could plan in this kind of space as well. We're not going to talk about either of those in the class, but I do want to do a slightly more concrete example of these test probabilities with this little grid that we've got going here, just to make it more concrete. Okay. What I want to show is that we can actually kind of move, potentially move, back and forth between belief state type representations and predictive state type representations. So let's imagine that we have this environment here that we've been talking about. And the belief state is, well, if 0.1 probability of being here, 0.2 probability of being here, 0.4 probability being here and 0.3 probability being here. Could we use this information to actually work out the probabilities for these predictions, what these predictions ought to be? So, from this belief state, how likely is it that I'm going to see blue if I take action up? And how likely is it that I'm going to see red if I take action left? So do you think you can work out what those numbers are just from the information we have here? Yeah, I'm pretty sure I can, actually. You want to give it a try? If you insist. I'm going to insist. All right. Well, let's go. All right, so how we going to do this? Well, I'm going to do it in the most methodical way I can think of. I'm just going to say, well, for each of those states, what's the probability of test 1 coming out such and such and test 2 coming out such and such? And then, given that, I can just multiply that effectively by the belief state, and then get the predictive states. Yes, very good. All right, so what's the probability that if we were to go up, we would see blue from each of the four states? Okay, so it would be 1, 0, 1, 0. Good, all right, and so if we were now to combine these. I guess it's just a dot product right? Yeah. It's the dot product between the probability of the different outcomes times how likely it is that that actually is the state that we're in. So that should be really easy, that should be like 0.5. Yes. For the first prediction. And it seems like we should be able to do something pretty similar for the second one. Yes, so what's the probability that I go left that I will see red? It is 0, 0, 1, 1, 0.7. Cool, all right, so that wasn't so hard, right? And it doesn't add up to 1, to your point. Which is what I was saying before, that's right. Yeah, so because each of these predictions are kind of an independent thing. So it's not like a distribution over which prediction we're going to do. It's more as you said it, which I thought was really good, the distribution over the outcomes of the test, right. So in this case, we'll either see blue, red, or green after we go up. So the probability of up blue, up green and up red, that'll add up to 1. But it turns out we didn't need those other tests to represent things in this particular example. So I just noticed something while I wasn't listening to what you were saying. [LAUGH] Which is that I can go from belief state to predictive state, but I can't go from predictive state to belief state. So why don't you say what you said again? Let's look at the problem the other way around. Let's say I know the predictive state is 0.7, 0.6. So we know that if we were to go up, the probability we'd see blue is 0.7. If we were to go left, the probably we would see is 0.6. Can we figure out the belief state using that information? I'm going to claim that we can come up with a belief state that is consistent with that, but there is not a unique belief state that is consistent with it. So we just looked at an example where we had a belief state that gave this as the predictive state. Can you give us another one? Can you give us a belief state where we get the 0.7, 0.6 but it's different from the one that we just looked at? Well, I don't even think I have to do that. Let's just pick the one for 0.6, right? So with 0.6, what's the value of a test in each of those states? So let's just take test 2, and we know that it is 0, 0, 1, 1, right? So we know, as you pointed out before, we just have to do a dot product with the belief state and those 0, 0, 1, 1 up there and we add it up together to get our predictive state. Okay. So we got a value of 0.6, right? Yep. Well, since the two reds are 1, then that means any belief state in the bottom row that adds together to get me 0.6 will be correct, right? Well, it'll give us that prediction for sure. It will give us that prediction for sure. So let's- So let's pick a belief state. Sure, so- How about 0.3 and 0.3? I was thinking that, too. So we agree that would give us 0.6? That would give us as a prediction for test number 2, 0.6. Right, now what if it weren't 0.3, 0.3, but instead 0.2 and 0.4? That would also give us 0.6. Right, and I'm going to claim that since all we're doing is taking two numbers and adding them together to get a third number, there's an infinite number of those. But we're not done yet, because we didn't do test number 1. So our current predictive state says we're in a situation now where both test 1 has a probability of 0.7, and test 2 has a probability of 0.6. So there's additional constraint there. Okay, cool, so we have 0.3 and 0.3- Yeah. Which leaves us with 0.4. Yes, which would have to be there. No, we have a total of 0.4 that we can play with, right? So the top row has to add up to 0.4. No, the side row, the side column, has to add up to 0.7, right? Because we're saying that if we were to go up we would see blue and the probability of that is 0.7, which means the sum of the probability of being here and here is 0.7. So we know what the probability of the upper left state is. It's what's left over after we pick the 0.3. And then that means the 1 in the upper right has to be- 0. 0, which is what I was saying. What I was saying is that the total, all four of those numbers, have to add up to 1. So you get 0.3, 0.3, 0.4, 0, great. You're right, so now we have one belief state. Now what if I made it 0.2 and 0.4? Yeah, good, so if I made 0.2 and 0.4, then we know that that has to be 0.5, and that has to be whatever is left. Yeah, negative 0.1. So that's not even possible. So what if I made it 0.4 and 0.2 at the bottom? So that has to add up to 0.7, that has to be 0.3, and that gives you 0.9. So I have 0.9. I see, I see what you did there. So now we actually have two different belief states that would be consistent with that predictive state. So what do you suppose we do in a predictive state representation to make it more complete, make it more equivalent? I don't know, but I think you're going to tell me. I am going to tell you. So we need more tests. So it turns out that if we have enough tests, and the right tests, then knowing the outcomes of those tests gives us a bijection with the belief states. We can always go, any belief state's going to give us one of the predictor states. Any actually achievable predictor state is going to give us a unique belief state. The two tests wasn't enough to do that. In this case we definitely don't need more than four. So would that be generality true, you never need any more tests than there are states? Is that true? So funny you should say that because we have a thing that I'm going to call the fundamental theorem of PSRs, but we can just call it the PSR theorem, that says specifically, any end state POMDP can be represented by a predictive state representation. Or PSR. With no more than n tests, each of which is no longer than n steps long. So that's good. But also you just introduced something, the number of steps. I guess I didn't think about that before, since both of those tests were sort of ones- They were very short, right, yeah. But it's not hard to come up with an example like that, right. I'm writing it as a POMDP because I am a limited beast and it's hard for me to think about PSRs. So what might a test be that you'd need to represent POMDP written like this? So we've got white state, white state, black state, black state. So one step test would be things like if I go, well, there's only that one action, and I see white, what's the probability of that? That's not enough information to know where we are, right, and in fact, seeing back is also not enough in this case. We actually have to think about what happens in two steps from now. Right, so yeah, yeah, so you could say if your test is two steps away then I could, for example, figure out If I see a white and then I see a black, then I know I must have been in the bottom state. Right. If I see black after making one move, then I had to be in one of the two middle states. Right. And I guess if I took two steps and I saw white twice, then I know I had to be in the top state, for example. So are you saying that you think we need another test to make this go? Well, I don't know. It just occurs to me that I can figure out that I'm in the top state if I see two white ones, that's all. I'm just pointing that out. Wouldn't it be enough to just know, okay, if I go up, will I see white, and then go up and see black? If the probability of that is 0 and going up and eeying black, the probability of that is 0. So if going up I see black, the probability is 0. Then I have to be in either the top state or the bottom state. And if it's also the case if I will not be able to see white and then black then that tells me what state I must be in. Yeah, that was the thought. Right, yeah, yeah, sure, sure, sure, sure. But these longer tests are really important here. Just having the single step doesn't give you enough information, doesn't give you enough constraint. With those two tests, can I alone figure out everything? Can I completely- So I think it's like the other example that we looked at, which is if we know that we're in one of these states, top, second, third, fourth, then I think these two tests are enough to tell us which of those four things are in. But in general, a belief state might be something like well, 0.6 of the top one and 0.2 of the second one and 0.1 and point something else. So in that case, we've got a four dimensional vector, actually a three dimensional vector, because it has to add up to 1, that we're trying to specify in term by way of two numbers. And we get that same kind of indeterminacy that you worked us through in the other example. I see, okay. So this kind of makes me want to ask a question. Okay. Why does this help? Why should I bother going to PSR, other than the mathematical niceties? There are some examples where it helps, and part of it is a philosophical thing. It's got to be better for us in the long run to represent our world in terms of things that we can actually observe instead of things that we can never, ever observe. So that's kind of the philosophical point that this answers. There is, as far as I'm aware, the literature that I'm familiar with in terms of learning a POMDP, you can learn a PSR representation. It may be easier than learning a POMDP representation. But the thing that I find interesting is that there's POMDPs that are easy to learn, and there's POMDPs that are hard to learn. And the POMDPs that are easy to learn are easy to learn as PSRs and the POMDP that are hard to learn are hard to learn as PSRs. So there's almost sort of this underlying toughness of the POMDP itself or the environment itself that switching to PSRs doesn't seem to exactly help. There is a case, though, when certain of the attributes of the problem are continuous, where in fact, it can be shown that a PSR representation can actually be learned efficiently when the POMDP representation's not clearly learned efficiently. I mean, I suppose one nice thing about the test model, at least in a particular case where there's some 1 or 0 probability of a test coming out under certain circumstances, it might be easier to think about what you ought to do in order to figure out exactly where you are. Like the set of tests you might run so that you can completely determine where it is you are in the real, sort of underlying world. Just knowing that might be a useful thing. Yeah. Yeah, for control and yeah, discovery. Right. Yeah, I mean, I'm not familiar with papers that do that exactly. Yes, you are. Do you have a paper that does that exactly? Holmes and Isabel. I guess I wouldn't think of it that way. I always thought of it that way. Or at least so Tinder told me that I should think about it, though. [LAUGH] I didn't think about it that way until he told me I should. And in particular, that's Tinder Singh who, his group did a lot of the pioneering work on PSRs and had some very nice analyses for how they work. So yeah, okay, but I just wanted to give kind of a sense of this. There is some more recent work using least squares methods instead of probabilistic methods that actually can lean on this PSR idea to learn efficiently actually. If you have enough data, it can actually turn it into a model that predicts well, and the model has elements in common with PSR. So it has gotten some leverage. I think the jury's still out as to whether or not this is really the way we want to represent our environments, but it's a useful tool for the toolbox. Yeah, I can see that. So that actually brings us to the end of the PomDP topic. How do you know at the end of the PomDP topic? We have high probability of believing that we're at the end of the pond E.P. topic because the observation that we're making is the what have we learned slide. [LAUGH] So what have we learned? Well, we learned about PomDPs. Partially Observable Markov Decision Processes? Well we had been exposed to these things before but we really kind of dove into sort of some detail about what they are and how to solve them. And in particular, we've talked about belief states as kind of a representation, which represents the states that we might be in and how much we believe that were in them. We talked about PomDPs as a strict generalization of MDPs which is sort of important. And that if we can solve a PomDP that happens to be an MDP, we will solve the underlying MDP. One of the unfortunate consequences of all that is that dealing with PomDPs is hard they're hard to solve they're to sort of fundamentally difficult. I can't remember the exact result now but there was something in there about undecidability. Yeah. And that just that was difficult. That was hard for me deal with. [LAUGH] It was hard for you to accept hard to accept. [LAUGH] Right, despite the fact that it's hard, we did come up with some algorithms for solving MDPs like a value iteration. I remember something about piecewise linear and convex. Right, and representing value functions as piecewise linear and convex functions. Or pulc and one of the things that was nice about that is that you show that you could build up these sets of kind of linear functions. And because you only cared about the max in a given point, you could throw away a bunch of them that might be unnecessary and sort of keep the hard problem possibly manageable. Good. That was really nice. Then we went from the solving of PomDPs to reinforcement learning with PomDPs, and if solving PomDPs is hard then RL for PomDPs definitely has to be hard, and that turns out to be true but, we tried to do it anyway. Let's see, we did memoryless. First we talked about a model-based method. Right, model-based method. Where you try to actually learn the PomDP from experience wandering around in it. And do you remember how we how we did that learning? With great difficulty? Well sure, we talked about explicit expectation maximization as a way of trying to organize that computation. Sure, that's a favorite of my expectations. Your maximally favorite? [LAUGH] Yeah, my maximally favorite one. We've got to go all the back and try to remind ourselves how Worked. Yes. That was fun. [LAUGH] And then we did memoryless? Yeah, and that we needed in the memoryless case we might need to be random. Again, just like grad school. [LAUGH] I don't think that's true. You're saying that if you're in grad school but you don't remember where you are in grad school, then it helps to act randomly. Well it certainly explains the behaviors on my grad. Okay, then we had, we probably did more stuff there but I can't remember it. [LAUGH] And so I do remember though that we started talking about Bayesian RL. Yes. And the cool thing about Bayesian RL being that it blurs the line between planning and learning so. And Bayesian stuff does that in general, this sort of notion that, well all learning really is is estimating probabilities of things, it's not actually learning, so just changing your posterior. And so, decision making in the Bayesian RL setting ends up becoming just a kind of version of planning in a continuous ugly kind of PomDP. Speaking of which, if we have Bayesian RL, do we have frequentist RL? Well sure, I mean Bayesian and frequentist are kind of rivals in a sense. So anything that's not Bayesian maybe is frequentist. Okay. So all the other reinforcement learning that we did like hue learning and whatnot. As it certainly has a frequentist flavor in that what is frequentist statistics is about, it's about you know counting up how many times something happens and divided by the number of times you asked. And that is definitely, queue learning has that flavor model based RL where you're trying to estimate the parameters of the model by from experience they tend to have that flavor. Okay, so good. So then we have been learning about frequentist RL. And now that we have Bayesian RL, it reveals the frequentist things that we'd been doing in the past. So I'm okay with that. Yeah and the Bayesian RL is very nice because it doesn't make an exploration exploitation distinction anymore. You're always acting given your knowledge of what the truth is and you're acting in a way that maximizes your award given that knowledge. Right and so exploration is exploitation. [SOUND] Planning is learning. Exploration is exploitation. War is peace. [LAUGH] And we have always been at war with supervised learning. Okay, then the last thing is PSRs, or Predictive State Representation. Yes, indeed and the upshot of the PSR idea was what? Well, that rather than thinking about states which might not even exist, and even if they do exist you never know where you're in them or not, so really they don't exist in some very important insane potato sort of way. What does make sense to think about is whether there are specific tests that you could take that would then predict what you are likely to observe. And that in some sense that's a very satisfying way of thinking about the world that I need to kind of keep track of and well if I were to do certain things now what would happen and that was kind of the push for sort of PSRs. And in so far as you prefer that kind of way of thinking about the world, the nice thing is that PSRs can represent any POMDP, and so they're equivalent. Right, except maybe more palatable philosophically or so forth. Certainly Rich Sutton was pushing people in the direction of this representation for that reason. There's a kind of philosophical stance that says. Am I sick? What does it mean to be sick? It means that I [LAUGH] could be dead tomorrow, I could be stuck in bed tomorrow, I could be running a fever. It's like there's a bunch of observable things that are a function of what the current state is, and so it might be useful to talk about things in terms of what those predictions are as opposed to positing some kind of magical internal state. Well, this makes sense right because the only usefulness. If you go back to that and we talked about this on our lesson management course in some sense the only usefulness of a state is what it allows you to predict about what's going to happen next, little made of objects. So, the only reason it's worthwhile even recognizing something as a state is it allows me to distinguish between something else that might be important later. So right, so another one of our universal contradictory truths is that states and predictors are kind of the same thing. Yeah, all things are the same. Excellent, all right, so I think that's the last lesson that I was doing by myself. I think I'm going to turn it over to you for the next one. Well, I will point out that you none of this by yourself like we've done this together. That's fair. Alright, well with that I guess we should say goodbye. Alright, see you next time. Bye. [LAUGH] Oh, hey Charles! Hi Michael! How are you doing today? I'm doing well, thank you. How are you? I'm doing just fine, thank you so much for asking. But I do have a question. And what is that? Why are we here? Why are we here? Because we're teaching a class on reinforcement learning. But we've already taught a class on reinforcement learning. [SOUND] We taught a class on machine learning, and there was a section of that class on reinforcement learning, so it was kind of a mini course. Okay, but we've done this already, so why would we be doing this again? The idea was that when we were doing the mini course last time, there was all sorts of topics that we didn't get to go into depth on. And it's a topic that's really interesting to us, so we thought we could just kind of expand it out and do a whole class. Okay, that seems reasonable, but you haven't convinced me yet that this is really a whole class, and that we're doing something different. So what are we doing that's different? So, well mostly we're just going to be going more deeply into the theory and practice of reinforcement learning. Hm, okay well, half of that sounds interesting. Oh, good. So on the theory side, we're going to be studying things like the convergence of algorithms. How long it takes them to converge. Whether you can put bounds on how many mistakes it's going to make before it converges. Situations in which you don't have convergence. These are all sorts of really, you don't want to hear so much about this. Oh, were you talking? I'm sorry. What about the practice side? All right, so on the practice side we're going to be doing a lot more in terms of implementations of reinforcement learning algorithms. And we're going to do, for each lesson, some kind of exercise where you use a system that we built called Burlap. The sorts of topics we're going to talk about include things like temporal difference learning, like the TD lambda algorithm. Oh, I like TD lambda. I actually did a thesis on that once. Oh nice, okay good, so you should- Yes, exactly once. Okay, [LAUGH] so you should be interested in hearing about that or maybe even talking about that. We're also going to be talking about how you can express things using reward functions. Which is a really important topic in reinforcement learning because all of the way that we communicate with the algorithms is in terms of their incentives in the form of rewards. I like that, that makes a lot of sense. So that's sort of the practical things that matter about making reinforcement learning actually work in the, wow, let's do that. I like that. What else? We should be talking about generalization and scaling to some degree. Oh, I like that. I want to talk about generalization and scaling, and I want to tie it back in to abstraction. Oh, that's a really important topic, great. Okay, cool. Can I do that? Sure, why don't you do that piece. And I'll talk about partially observable Markov decision processes, which is decision-making when you don't have complete knowledge about the current state of the environment. Okay, that's seems reasonable. I'll trade you. You can do POMDPs, if you let me do options. Okay, all right. And, Monte Carlo methods. Okay, it's a deal. But we're also going to talk about game theory which is awesome, because it has theory right there in the name. Okay that's true, but it also has game right there in the name, and that's what makes it cool. It should be fun for everyone. Yes, fun for the whole family. So, in the game theory section, we're actually going to hearken back to things that we did in the previous mini course. But then we're going to update it by giving more details about different solution concepts. Oh right, that's right, last time we did this class we talked about all these things besides Nash equilibria that were really interesting. And so we're going to have a chance to talk about that now? Yep. Good, so can we do correlated equilibrium? You can do correlated equilibrium. I'll do correlated equilibrium, sounds good. All right, so are you ready to go? I think I'm ready to go, so let's go. Hi, Michael. Hey Charles, well that was fun. That was fun, we should do it again. I don't think we need to do it again. I think we've now covered everything we wanted to say about reinforcement learning. And I'm completely sure that we haven't. Well, what haven't we covered? Well, what haven't we haven't we covered? Well, we haven't, we haven't we covered TD-Lambda. Sure. So, we talked about TD lambda. We talked about a few algorithms here or there, but we haven't really touched on the big questions. We haven't covered everything that we should. Like? BURLAP! We have definitely covered BURLAP. Okay, sure, there was a tutorial, and there were assignments about it, but you haven't answered the big question about BURLAP. Which is what? Why BURLAP? Why do we need anything like BURLAP? Why don't you just use the UCI database and Weka we're done. Right so the UCI data set their depository and Weka are really great for supervised learning but they're not actually well situated for reinforcement learning. Weka doesn't include reinforcement learning algorithms it the UCI repository doesn't include reinforcement learning problems in it. You really need a different set up for dealing with reinforcement learning problems. You need a different, okay, I don't understand that. So we have algorithms, we've talked about many of them in class. That's right. So algorithms, are algorithms, are algorithms, are algorithms, and this is all machine learning and so all we need is data. And that's what repositories are for, data. So, well yes, BURLAP is trying to answer both of those issues. One of the things that BURLAP is trying to be is a collection of those algorithms or algorithms or algorithms or algorithms and it's also providing a replacement for data. So data in the reinforcement learning setting is really hard to work with. Instead, what BURLAP has is a set of simulators, the ability to Act like the environment so that the learning algorithm can interact with it and learn from them. [CROSSTALK] I see so the keyword there is in Iraq, so the notion of supervised learning as you just got data you got him put it up with pairs input and output pairs, but as we pointed out multiple times reinforcement learning is not just about input output you and back Get to see states and rewards you take actions. Those are the kinds of thing that you do and that requires interaction. Right and the data that you actually see depends on how you're exploring the environment so just capturing a big static set of data and presenting that to the learner isn't really the reinforcement learning process. Okay, so I like that. So I heard two things there. One thing I like, and one thing I don't like. The thing that I like is that you distinguished reinforcement learning from supervised learning. You talked about the need for interaction and how that's sort of crucial, and you said your solution was having simulators. Right, so, I agree that that's not a great thing. So simulators are good in the sense that you can actually interact with them, and so it's a way of transferring and an environment or a test problem from one research group to another research Group. [CROSSTALK] Sounds good. But on the bad side they're not really data, it's not actual data coming from a problem. It's just fit. [CROSSTALK] So then you're suggesting this is the second thing that I didn't thing I like you're suggesting that we don't have any way to make reinforcement learning work in the real world. It is more challenging, but it is not the case that reinforcement learning hasn't been used to solve some real problems. Like what? So, it's worth probably mentioning a few of them. We talked about backgammon as part of the class, the TD-Gammon algorithm that actually learn to play backgammon at a level comparable to, if not better than, human beings. There's other sorts of problems that are probably worth talking about as well. Elevator control is an interesting problem to think about. >From a reinforcement learning perspective. How so? How is an elevator controller reinforcement learning about? Elevator control if you think about the elevators in a building they're trying to work together to move people to where they want to go. And so we have actions, we have states and we have rewards. The actions are where the elevators are trying to go, whether or not they're going to open their doors at a given floor. The states are what is the arrangement of all the different elevators at the moment. Which floors have elevators on them, which don't. Which buttons have been pressed. Which buttons have been pressed. So who's waiting on which floors. And from the reward perspective you can try to get the system to try to minimize wait time for people, right? So that's something that you can measure, how long is somebody waiting on the third floor for the elevator to come, and then eventually getting to the destination floor and that's something you can measure and something that you can try to find ways of behaving to improve. Okay cool, so that's a reinforcement learning problem. I like that, anything else? Yeah, so that one actually was done in simulation. Okay. But there has been some good work in reinforcement learning in robotics. So one that's maybe worth mentioning is the the helicopter work. There's a work out of Stanford and UC Berkeley on using reinforcement learning to actually fly model helicopter tricks. Like what? So it turns out that if you, you're flying a model helicopter, I've tried to do this. Turns out to be really hard to do. Part of it is you have to coordinate the blades in the blades in the Blades. Yeah, and part of it is that you can do crazy things with helicopters, if you know how to do it. You can make them fly upside down, you can make them swing back and forth, you can make them kind of roll over and over and over. You can do all sorts of really, well I don't know, you can do all sorts of amazing things, but professional stunt flying helicopter people can do amazing things. What the reinforcement learning work was about is trying to get a system to learn to do those same kinds of tricks. And they were able to do this with real helicopters or real model helicopters? A real model helicopter, not a simulator, but an actual Mini helicopter. Okay, so that's cool. So you sort of convinced me that burlap and repository, that's sort of a good. Okay, I buy that. But I do want to point out something that you said, that I thought was interesting. You talked about simulators and that maybe what we really want to do is work in the real world. But it occurs to me that at least one of those examples, the simulator is the real world. So, games, right? So if you have games like Backgammon, if you have simulations that people actually interact with then that is actually the real problem itself, that is the real world. But if I'm playing Pac-Man and I've got a simulator for Pac-Man, that's the same thing as having Pac-Man. And if I can learn to play Pac-Man, I've learn to play Pac-Man. There's not really a simulator involved. I would make a distinction between grid world type simulators where they were developed specifically for reinforcement learning researchers to work with. Versus, as you say, the kinds of simulators that people actually interact with when they're doing things like playing video games. And so, video games actually is a really great intermediate step between real physical world stuff and things that can be run in simulation. And actually there's been some terrific work in that space. Really, like what? Well, one of the things that I'm very excited about recently is a group called Deep Mind, which they're very good at naming things. Have a system for actually doing what they call deep reinforcement learning, deep q learning. Where they are actually trying to learn behavior in Atari video games. Video games from the 80s, where they actually are using Atari emulators, the actual games as people actually play them. Taking the information that's available on the screen as input and sending joystick commands as output, and learning to play actually at the level of human players. Wait, they're as good as human players? They are as good on average as human players. In some of the games, they're actually much better than people. Games like Breakout where you have to bounce a ball all around. Okay. And then there's other games where they're actually much much worse than people. Games like Frost Bite where you have to jump around on a bunch of ice floes. It seems very difficult for the system to actually learn what to do in that game. Okay, so their heads in an oven, their feet is in ice water. But on average, they're comfortable. I see what you're saying. So, your point is that it's not playing just like a person. It's playing better and worse than people, so on average you can say that it's kind of playing like people. Mm-hm, but still that's kind of cool. I think it's really amazing and I think it's that particular data set is itself really interesting, because there's 40 some odd Atari video games that are all built in the same system. You can use the same inputs, and the same outputs, and the same learning algorithm, but you can put in different games to see what it will do. Okay, so it is just like Weka and it's just like UCI, except here, Weka is the burlap algorithms or whatever, reinforcement algorithms. And UCI, instead of being input output pairs are sort of the real world. Yeah, and there's some work that's being done in collecting data from the real world. For example, medical diagnosis or online education. I like that, we should do something like that. [LAUGH] For trying to figure out what the right way of interacting with people online is. People have collected some of that data and there's reinforcement learning algorithms that are being run at that data. It's not the full reinforcement learning problem, because you don't actually, the learning system doesn't actually have to decide how to collect the data, it's already been collected for it. But it is really using real data and making decisions based on that and I think that's a really promising direction for reinforcement learning to go in the future. Okay, I like it. So I got two things out of that. The first thing is reinforcement learning really can be used in the real world, it has been, it can be and there's a nice bright future for it that's good. And the second thing is I was right, there was a bunch of stuff we hadn't talked about. Yeah, okay, all right, that's a valid point. Okay, was there anything else you want to wrap up with anything? Well, I feel like we should say maybe a little bit about the current direction in the future of reinforcement learning. So, one of the things I think is really neat is that it's having an impact not just on engineering and computer science, but also on the behavioral sciences and the neurosciences. How so? Well so, the sorts of things that we think about reinforcement learning being good for, are an agent it's interacting with some environment, it's making decisions. It's trying to figure out what to do, it's trying to be happy. And so you could argue that animals and people are trying to do exactly those sorts of things, that we are reinforcement learners. Sure animals do it, people do it. Yeah. Birds do it. Educated fleas have been known to do reinforcement learning. Right, okay. And so, the interesting thing about that is the Nora scientists when they're trying to think about, okay. Well, how can we understand the algorithms that essentially that people are running. They're turning to the reinforcement learning field as a source of inspiration for the structure of the algorithm, how they can be set up. What sorts of things seem to work and not work and they're actually looking for evidence of these in real brain. I like that. I like that a lot. And you know what I really like about that? That it takes us back to the beginning of the machine learning class. It takes us back to the beginning of the reinforcement learning class, where we tried to emphasize the point that reinforcement learning is not just a set of algorithms, it's not just an approach. It's actually a way of thinking about problems. And so really this whole class has been about solving problems. And I feel like we have solved some problems. Great, I think that's a wonderful note to end things on, what do you think? No, I think this is really valuable, thanks so much. All right well, thank you very much, Michael. I appreciate it and I will see you in the next class. Bye. Hi, I'm Michael Littman. You may remember me from such courses as Machine Learning, Intro to Algorithms, and well, this course. Hi, I'm Chris Pryby. I'm a course developer with Udacity, and I worked with Charles and Michael to build this course. Where is Charles anyway? You know what, let's just add him in post. The main goal of this course is that you become prepared to engage with the reinforcement learning community. One aspect of being engaged with the community is becoming familiar with software built specifically for solving reinforcement learning problems. To help you with this goal, you will use a java framework called the Brown UMBC Reinforcement Learning and Planning library, or BURLAP. In this tutorial lesson I will introduce you to BURLAP and help you write your first programs using the library. We'll talk about how to set up and solve a basic MDP using BURLAP. As you work through the course, we encourage you to think about the planning and learning algorithms presented, and try to implement them using BURLAP. A great resource to consult is the BURLAP documentation, available at the URL listed in the instructor notes. All right, Michael, ready to get started? I'm ready. Awesome, yeah! Let's consider the following decision problem. >From an initial state, we have a choice of three actions, a, b, and c. The immediate reward associated with each of these actions is zero. But these three actions will take us to different states with different possible rewards available from these states. Action a takes us to state S1, from which our only action is to loop at each step back into state S1, accruing a reward of P1, which is a parameter, each time. If we take action b at the initial state, we will spend the rest of the game alternating between states S2 and S4. We will gain a reward of P2 each time we move from state S2 to S4. And we will receive a reward of P3 on the return trip. And if we take action c initially, we will move into state S3, from which our only next action is to move into state S5, attaining no reward. After that, we repeatedly loop in state S5, attaining a reward of P4 at each step. Suppose the game has an independent probability, of ending after each step? And, let's call this probability, 1 minus gamma, so that the effective discount factor for the scenario is gamma. Given the parameters P1, P2, P3 and P4, our problem is to determine for which values of gamma we prefer the initial action a, the initial action b, and for which gamma we prefer the initial action c. We will solve this problem by setting up a Markov decision process encapsulating this state diagram in Burlap. We will then run value iteration procedures on the MDP for different values of gamma in order to answer this question. Before jumping into the code let's quickly review the components of a Markov decision process, or MDP, and see how they are implemented using the BURLAP library. The first component of an MDP is a set of states S that the system can be in. Associated to each state S, in S, is a set of actions A(s), that can be taken at the state, little s. Additionally, there is a transition function T(s,a,s'), that gives the probability that if we are in state s, and take action a, that we end up in state s prime. Finally, there is a reward function. R(s,a,s'), which gives the reward we attain by taking action a at state s and ending up in state s prime. Recall that it is sometimes more natural to express the reward as a function of just s and a, or even as a function of just the initial state, s. We will use the most general form, R(s,a,s') in this tutorial since reward functions are implemented in BURLAP in this manner. A terminal state is a state in which no further agent action is possible. After entering the terminal state an agent receives no more reward for the rest of time. In lecture, we did not define terminal states as a component of an MDP, because they can be considered states without any associated actions. However, in BURLAP, we will have the capability to specially denote terminal states. So it is good to keep this concept in mind as we build our MDP. BURLAP uses the object oriented paradigm to represent Markov decision processes. Here is a brief overview of how MDPs can be represented and created using the library. An MDP as a whole is represented in BURLAP by a Domain Object. Domains can be created directly. But it is often useful to use one of several classes in the BURLAP library, specifically to generate certain types of MDPs. These classes are implementations of the DomainGenerator interface. In the JAVA documentation for BURLAP, located at the URL here, and also included in the instructor notes, you can explore the classes available through the BURLAP library. In the left hand pane of the documentation you'll see a list of all the classes available in BURLAP. Scroll down until you find DomainGenerator. Notice that DomainGenerator is italicized. This means that the class is an interface. If you click the link on the left side for DomainGenerator, you will see information about the interface in the main pane. Notice that at the top of the page, there is a list of classes that implement the DomainGenerator Interface. If you're not familiar with Java Docs, you can click each of these classes to read a brief description of the type of domain each class generates. Now considering the type of MDP we are interested in building, which of these classes would be most appropriate for us to use to generate a Domain Object. Use the documentation for each of these implementing classes, and to the state diagram to help inform your answer. If you need a link to the BURLAP documentation, a URL is provided in the instructor notes. You'll find the state diagram there too. Looking again at our state diagram, we can see that the diagram is a directed graph. That is, it's a set of nodes with arrows leading between them. Notice that BURLAP has a class called GraphDefinedDomain, which implements the DomainGenerator interface. Looking at the description for GraphDefinedDomain, we see that it indeed generates domain objects that are represented as graphs. This is the type of domain generator that we will use. Let's begin the process of building our MDP to represent this state diagram. Our first step will be to define the set of states for our MDP. Let's take a look at a short program I've written to accomplish this task. Notice that I am importing several packages at the beginning of the program. If you'd like to run this code locally, you'll need to make sure you've included the burlap jar file in your Java build path. The jar file is available at the URL listed in the instructor notes. The class I built, FirstMDP, currently has two fields, a Domain, and a DomainGenerator. in my constructor for FirstMDP I'm instantiating a new GraphDefinedDomain object, and assigning it to the FirstMDP's domain generator field, dg. I then use a method from the GraphDefinedDomain object to generate an appropriate domain object, which is then assigned to the FirstMDP's domain field. I've left two blanks in this code which I'd like you to now fill in. Using the documentation for the GraphDefinedDomain class, please fill in the appropriate argument or arguments for the constructor I called. And also fill in the appropriate method that needs to be called in order to create a domain object using the GraphDefinedDomain. Again, a link to the documentation can be found in the instructor notes. In the documentation for the GraphDefinedDomain class, we can see that it has only one constructor, which takes as an argument a single int. This int is the number of states that the domain should have. Since we have already defined and initialized the variable numStates to be 6, we could either pass the variable num_States or the number 6 to the constructor. Though it would probably be better to pass numStates, so we don't have a magic number in our code. Also, looking through the list of methods available to a GraphDefinedDomain object, we can find the method, generateDomain, which takes no arguments and outputs a Domain object. This Domain object will have six states since we specified this in creating the generator object. I'll go ahead and fill in the second blank with this method. Note that we could create a second domain by calling the generateDomain function a second time. This second domain would be functionally separate from the original domain object, but they would have the same initial structure as defined by the specifications given to the GraphDefinedDomain object. Essentially a GraphDefinedDomain acts like a blueprint for the domain objects it generates. However, since we won't be needing tow different domain objects for this project, I'll go ahead and delete the object I just added. Now that we've created the set of states, let's designate one of the states in our domain to be the initial state for the MDP. At the moment, we haven't defined any transitions between states. So our choice of an initial state is arbitrary. But for grading purposes, let's call State 0 the initial state. Here I added in initState; field which has the type of the Burlap object state. Now let's initialize this field in our constructor. Take a look at the Burlap documentation for graph defined domain again. Using this documentation I'd like you to fill in the blank in this new line of code. So that it initializes this.initState to the 0th state in the domain we created earlier. Using this.dg.generateDomain(); In the documentation for GraphDefinedDomain, we see that there is a static message, returning a state object called getState, which takes a domain object as an argument and an int representing the state object from the domain to retrieve. So we can call GraphDefinedDomain.getState with arguments this.domain, and 0. In order to retrieve the zeroth state for the domain we generated in line 14. One thing to keep in mind is that there isn't an intrinsic notion of an initial state built in to the domain object itself. We are specifying the initial state outside of the domain. This allows us more flexibility in building our MDP. For example, we can specify different initial states for different runs of a planning algorithm. We finished encoding our states in the BURLAP domain, so now let's add actions to the domain as well. To add actions to domain objects we generate, we can use a method built into the GraphDefinedDomain class, setTransition. This method is used to define both the set of actions and the transition function for the MDP. The first argument is an int giving the ID of the state S we are transitioning from. The second argument is also an int, giving the ID the action we are defining. I'll talk about that more in just a second. The third argument is an int giving the ID of the state T we are transitioning into. And the fourth argument is a double, giving the probability of transitioning to state T upon taking this action. Note that many different actions coming out of the same state can have the same ID. But the probabilities of the actions with the same ID leaving a given state must add up to one. Looking at our state diagram again, if we want to define the action a, taking us from state 0 to state 1, we can call setTransition with the arguments 0 for the ID of the state we are leaving. 0, since this is the first action we're defining from state 0. 1, which is the ID of the state we are transitioning into. And 1, and I'm adding a decimal point to denote that this is a double. Since this is a deterministic action, with probability 1 we will always enter state 1 by taking action 0 from state 0. To define action b, we call setTransition with the arguments, 0, again because we are leaving state 0. 1, since this is going to be action 1 from state 0. 2, which is the ID of the state we are entering. And 1., since again, this is a deterministic action. With probability 1, this action will always take us from S0 to S2. Similarly, action c will be defined by calling setTransition with arguments 0, 2, since this is action 2 coming out from state 0. 3 for the ID of the state we're entering, and 1, since this is a deterministic action. Now let's put this into our code. Notice that I am casting this.dg, which is our DomainGenerator object, to a GraphDefinedDomain before calling setTransition. This is needed because setTransition is not defined for the DomainGenerator class. Another point to notice is that we are defining all of these actions before we call this.dg.generateDomain. We're calling the setTransition functions first because it tells the GraphDefinedDomain object to include these transitions in its blueprint when it generates a new domain object. Calling generateDomain first would create a domain with six states but no transitions, as I found out myself as I was learning to use BURLAP. Now I'd like you to define the remaining transitions according to our state diagram. This will be a programming exercise. Please add the appropriate mythic calls in this area of the code. Make sure you use action ID 0 for each of the actions you define. Since each of the remaining states besides the initial state has only a single action. You may also notice I've added a getDomain method to the class. This is workaround to allow our greater to see the domain object inside the first MDP class, for the purposes of our grading. This sort of method is discouraged in practice because it violates encapsulation by exposing this private member. For these burlap programming exercises, clicking test run will compile and run your code as it is in the editor. Clicking submit will run the grading script. For this exercise, the grading script will call the constructor of your first MDP class and check that all the required transitions are present and are deterministic. You can write test code in the main method to check that your solution is working. We started you off by calling the first MDP constructor, so you can make sure no runtime errors are thrown. If you want, you can dive into the Burlap API on your own to learn how to examine your domain object more closely. One place you might consider starting, is in the documentation for the domain class. You can use the getAttribute method to find the number of nodes in your domain object, checking the validity of the actions is a bit more complicated. It involves the getActions method of the domain class. As well as the getTransitions method of the action class. We'll provide the code used by our grader in the solution video after the quiz if you're interested in seeing how to examine these objects yourself. Here are the transitions I used. Let's take another look at our state diagram to see the correspondence between these transitions and the arrows in the diagram. For the set of transitions following the initial action a, we only have one possible action, going from state 1 to itself. There are two transitions that come out of initial action b. Going from state 2 to state 4 and then vice versa from state 4 to state 2. Finally following initial action c we have a single transition from state 3 to state 5 captured here and then another transition from state 5 to itself captured here. Notice that all of these transitions are deterministic. So we have probability one for all of them. Also notice I used action id 0 because it's the first action coming out of each of these states. For state 0 we had three actions which is why we had action id 0, 1, and 2. If you're interested in seeing how we checked your code, please take a look at the first MDP grader ex4.java file in the downloadable section on this page. The next element of the MDP that we need to include is the RewardFunction. Burlap comes with an interface which we can use to implement a custom RewardFunction in our code. First, we'll need to import the RewardFunction and then we'll add a RewardFunction member to our first MDP class. Now within the first MDP class, let's make a static class that will implement the RewardFunction interface. I'm going to call it FourParamRF, since the rewards will be based on the four parameters P1, P2, P3 and P4 we have in our state diagram. P1 through P4 will be member doubles of this FourParamRF. And the constructor for the FourParamRF will just initialize these members based on the inputs. We can then go ahead and add a line to the end of the constructor for the first MDP. It assigns a new FourParamRF object to the first MDP's new .rf member. Now in order to implement the RewardFunction interface, we need to override the reward method. This should match the reward scheme from our state diagram. This will be your test for the next exercise. Make sure to look at the documentation for the RewardFunction interface for the appropriate signature for the method. You might also find the getNodeId method from Graph Defined Domain helpful in writing your method. Here's how I implemented the Reward Method. According to the RewardFunction documentation, the method takes a state, a grounded action, and another state, representing the state before the action, the action taken, and the state entered after the action. And the method need to return a double In our state diagram, notice that we only have one action leaving each state, except for the initial state. Also, each action is deterministic. Each will lead to exactly one possible state. Finally, all the actions coming out of the initial state have the same reward, zero. So the reward function in this particular MDP will only depend on the state the agent is in before its action. Therefore, it's fine to just perform a check on the argument s and to disregard the arguments a and sprime. Here I use the static getNodeId method of the GraphDefinedDomain class to retrieve the ID of the state the agent is in before acting, then based on the state ID, I return the appropriate reward. In States 0 and 3, the reward is 0. In State 1, the reward is p1. In State 2, the reward is p2. In State 4, the reward is p3, and in State 5, the reward is p4. Finally, I include a run time exception after my check in the case that a state ID outside of the range zero to five occurs. This isn't a requirement for the method to work correctly, though it can be useful for debugging purposes. Finally, to complete the definition of our MDP, we need to specify the states in which the agent's action terminates. For this particular problem, the MDP we're building won't have any terminal states. So this will be relatively quick to do. Take a look at the TerminalFunction Interface documentation. One of the implementations of this interface provides precisely the functionality we need for this problem. Which one is it? Type your answer in the blank on the following screen. The NullTermination class implements the terminal function interface in such a way that no state is considered terminal. So let's use the NullTermination class. TerminalFunction is already imported with the core package. To import NullTermination, we need to add this import statement here. Then we add a TerminalFunction member to the FirstMDP class and then set it to be a new NullTermination object in the FirstMDP constructor. Now that we've finished defining the MDP in burlap, we can use a planning or reinforcement algorithm, provided by burlap, to solve this MDP. For this tutorial, we'll use a value iteration to find the values of states S1, S2, and S3. Whichever of these states has the highest value, will tell us which action, a, b, or c, we most prefer for our initial action. Value iteration is a class in burlap which implements the value iteration algorithm from lecture. It does all the work of iterative bellman equation updates for us. To instantiate it, we need to pass the domain reward function and terminal function we previously defined. We also need to pass a few more parameters. A double gamma, which is the discount factor between zero and one, a state hash factory object, which I'll talk about in a moment, a double maxDelta and an int maxIterations. Which are two stopping conditions for the value iteration procedure. The procedure will stop if no state's value changes by more than maxDelta in a single iteration. And the procedure will also stop after running max iterations times. The state hash factory is an interface providing the method hash state, which takes a state object and produces a state hash tuple. This allows for fast retrieval of state objects during the value iteration procedure. Since our MDP consists of a finite number of discrete states, we will pass a DiscreteStateHashFactory to the value iteration constructor. Let's go ahead and add a DiscreteStateHashFactory to our first MDP class. And later we'll pass this object to the value iteration constructor. Note that we'll need to add these import statements to use the discrete hash factory and the value iteration objects. The packages you need to import can be found by browsing the documentation. Or if you are using an IDE, the appropriate import statements can likely be added automatically as needed. Next, lets add the member field hashfactory, and then in the first MDB constructor, initialize hashFactory to a new DiscreteStateHashFactory object. Now that we've created our hashFactory, we're in the position to perform value iteration. Let's write a private method, computeValue that will be used to return a ValueIteration object, available internally to the class. Let's make the method dependent on just the single parameter gamma. For our purposes, we'll just set the maxDelta and maxIterations parameters by hand to 0.0001 and 1000. Then we'll pass all the required parameters to a new ValueIteration object. Now I'd like you to fill in this blank in the code to perform the value iteration procedure on our MDP before returning the object vi. Feel free to refer to the value iteration documentation as needed. The value iteration class, provides the method, planfromstate, which takes the initial state object of a domain,. This is the method we need in order to execute the value iteration procedure. Therefore, we can fill in the blank with vi.planfromState(this.initial state) Great, we're almost done. The last step is to write a method that will use the result of the computeValue method to determine which action, a, b, or c, is the best first action for the MDP. Here is the stub for the method bestFirstAction. I'll leave it up to you to fill it in. As always, refer to the Java docs for help finding any methods or objects that will help in solving the problem. Also, feel free to add more fields or helper methods if you need to. Good luck. Here's my solution. Yours might not be exactly the same, but that's okay as long as it outputs the same initial action based on the parameters given. I started by adding a new field, the int numStates, to my FirstMDP class. Then in the constructor, I initialized this.numStates to 6. Then, down in the bestFirstAction method, I create a ValueIteration object using the computeValue method we defined earlier. To extract the value of each state, I start by using the static getState method from the GraphDefinedDomain class to convert from integers to state objects. Then I use the value method of the object vi to obtain the value of each state object in our MDP. These values are then stored in the array V for easier access later. Finally, I check to see whether V[1], V[2], or V[3] is the largest. If V[1] is the largest, then I return the string action a. If V[2] is the largest, then I return the string action b, and if V[3] is largest, then I return the string action c. Now that we've walked through solving our original problem together, here's a new problem for you to solve on your own. This will let you practice a bit more with building MDPs in Burlap and let you self-assess your understanding. In this MDP we will again begin in state S0. >From state S0 we will have a choice of two actions, action a and action b. Action a will have two possible outcomes. With probability p1, we will loop back into State S0 and obtain a reward of -1. With probability 1-p1, we will enter state S1 and obtain a reward of +3. If we take action b from S0, we will deterministically enter state S2 and we will obtain the reward +1. S2 has a single action that deterministically takes us to state S1 with zero reward. At state S1 we have another choice of actions. Action c will be stochastic and action d will be deterministic. With probability p2, action c from S1 will take us to the terminal state S5, with zero reward. And with probability 1-p2, action c will take us into state S3 and we'll obtain a reward of +1. >From S3 our only option is to deterministically reenter S1 with zero reward. If we take action d, we enter S4 with probability 1, obtaining the reward +2. The only action available at state S4 is to deterministically enter the terminal state S5 with zero reward. Your problem will be to determine, based on the parameters p1 and p2 and the discount factor gamma, the optimal actions to take at states S0 and S1. Using the provided class SecondMDP, fill in the constructor method and the method bestActions. Please feel free to add whatever fields and helper methods or classes you need to solve the problem. And as always, the Burlap javadocs will be your friend. Good luck. Here's how I implemented my solution using burlap. Let's take a look at our constructor. As before, we have six states. We only have two parameters now instead of four, not counting the discount factor. And these parameters are now probabilities of transitions instead of rewards. I use these probabilities in the last parameter In the stochastic transitions denoted in our state diagram. Then, as before, we can conclude by generating the domain, setting the initial state to 0, setting the reward and terminal functions, and then instantiating a DiscreteStateHashFactory. You'll notice that I'm using the classes SpecificRF, and SingleStateTF here. These are custom classes I wrote to implement the reward and terminal function interfaces. We'll look at these next. For the reward functions, since we now have non deterministic transitions, we need to look at both the state s each transitions leaves, and the state t each transition enters. Like before, I get integer IDs for the nodes by calling the static method getNodeId. Then using our state diagram, I assign the appropriate value to the variable r based on the state the transition leaves, and then based on the state the transition enters. I also have a few cases that throw exceptions for debugging purposes. For the terminal function, my constructor takes a single integer as an argument, the id of the terminal state, and then stores it in the member field, terminal sid. Then, whenever the isTerminal method is called, it compares the id of the argument with the terminal sid. If they are the same, then the method returns true, and we know that the state is terminal. Otherwise, the method returns false. As before, I have a private computeValue method that takes the discount factor gamma and runs a value iteration from the initial state that we find in the constructor. It then returns the ValueIteration object with the values of each state stored in it. In the bestActions method, I pass this ValueIteration object as an argument to a GreedyQPolicy constructor. This object allows me to access the best action to take at each state based on the values in the ValueIteration object. I do this by calling p.getAction, followed by the state whose action I wish to query. Since the output actions are either action0 or action1 for these two states, and since their output needs to be in the form of letters a, b, c, and d, I call some string replacement methods before returning the answer. If you were able to come up with a solution for this problem, great. You should be well on your way to being able to tackle further reinforcement learning problems using the burlap library.