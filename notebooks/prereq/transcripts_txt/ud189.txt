Hello, everyone. It's my pleasure to introduce to you Doctor Yousef Khalidi. Yousef, as I call him graduated from Georgia Tech in 1989. And and then after he graduated from Georgia Tech with a PhD in computer science, and I'm very proud to say that he was my first PhD student. He went to work for Sun Microsystems where he was developing multiprocessor operating systems and then at some point of time Microsoft found him. And he moved to Microsoft and while he was at Sun he was the CTO of the Solaris MC product. And some of which we saw in our own discussion of structure of operating systems early on. And and then he moved to Microsoft and in Microsoft he developed the current he developed the cloud platform which is called the Microsoft Azure cloud platform, and that's the one that he'd been leading. He was a distinguished engineer at Sun Microsystems and then, now he is a distinguished engineer at Microsoft. And so, it's a great pleasure to have you Thank you Kishore for having me, and I just want to say that Kishore was a great professor. But a tough professor. [LAUGH] And I want him to tell, tell that to my students even now. [LAUGH] I'm sorry, but that's the truth. So Yousef, it's a pleasure always to meet you and, and see you. And, and, and the thing that is really exciting for me is being able to bring you back to this interview because this is, interview's going to be part of the course that I'm developing in advanced operating systems, and and, and you contributed a lot to that. And so what I want to do is first sort of rewind. 'Kay. Back to the the mid to late 80s. When you were a PhD student and and you developed, as part of your dissertation work, the cloud's distributed operating system. And then fast forward to 2013. You're heading the cloud Azure platform at Microsoft. Talk to us about your journey and, and if you can, connect the dots from what you did back then, to what you're doing now. You know, in many way, things have changed. In many other ways things have not changed at all. So the fundamentals are the same. The structure of an OS, the way you layer the system, the basic concepts of security, isolation. All these high separation policy of mechanisms, simplicity of design, recovery from failures and the like, all these concepts still apply. But at the same time, things have changed quite a bit. Give you an example. The product you mentioned that you and I worked on, clouds and, for 85, 86, to 89, it was a distributed system, true, but we had exactly three machines. Growing, it grew to four or five machines later on. That was it. I mean we had to manage three, four, five machines. Lots of algorithms that are very similar today but the SK wasn't the same. Fast forward to today. I can't even tell you how many machines, how many VMs run in a public lot. Millions, millions of VMs run all over the place. So the scale have been going up by orders and orders of magnitude. So the fundamentals are the same, but the problem at hand has changed, and of course, the physics and the how the industry moved. before, a given machine was what? A vax 11 750. Mm-hm. This is an old machine now of course. Mm-hm. Well, I mean, there many, many students may not even know about vax 11 750. You find them in the, in the history museum now. It used to be as big as a And the company doesn't exist. That we. Company's gone. The machine is gone but it was the best machine there was for awhile. As big as this. And probably my watch or my phone has order of magnitude more power than this. So, you know, again this is just how the industry moved. So again, the, the concepts are the same, the fundamentals are the same. Some of the papers that we now teach and learn from, are the same ones that were written back then, even before. But their application now has been applied to a much, much bigger scale. So, so my short answer here. And this scale, of course, you see everywhere. All of us have some devices, some smart phone, or some machine, or a PC and the like. And all these services out there are using the the, the, the connectivity, the computing, the power of the collective if you will. That's new as well. Before we used to apply all these concepts in the small. To a given machine. To a server. Now we apply them to a collective. So, is it fair to say that some of the techniques that were invented in the 80s, and the 90s are still applicable today? It is just that the scale has gone up. Very much so. I mean, the fundamentals of distributed computing, replication algorithms, consistence, leader election, synchronization, all these are of course the bread and butter of what we do. It's a given. But at the same time since things have changed, when we scale changes by one, two, three orders of magnitude some other things break. So we have to think in terms of maybe loose consistency instead of very absolute consistency. You would need to think of handling failures in a different way. Optimized for mean time to recovery versus making it perfect the first time through. So, as you know, in computer science, when change by this much, some things have to move, but the fundamentals are the same. That's a very comforting piece of news for the students because some of the papers that we read are from the 90s and the 80s and the classic papers and I tell them why they are. The classic even goes before that as you know. Right. Exactly. It, somebody said that was, wasn't me that nothing has been invented in our field for the last 20 years or so. Mm-hm. And, again, they're partially right. Mm-hm. The fundamentals are the same. Mm-hm. So you have to start with the basics. Mm-hm. And you build on top of them. Right, right. So, I, I wanted to ask specifically with respect to Microsoft Azure it uses the platform as a service model for the cloud. Now discuss why that model is the one that you chose. And what are the pros and cons of that, that model, vis-a-vis other kinds of models? Sure. So, I just want to tell you, in Azure we have the whole spectrum now. Platform as a service, infrastructure as a service, all combined to build, basically, services, so-called software as a service. First I want to start by telling you I'm not a fan of these terms. Okay. They are not technical at all. The industry at times wants to put things into separate segments. Separate segments. Mm-hm. But by, you know what the definition of them are. Platform services you typically have APIs and other services out of which you compose other services. Whereas infrastructure service is typically just comp, built VMs, virtual machines, and instantiate them. Real life, of course, is more complex than that. It's a combination. And Azure, like other systems support both. The long-term direct, direction, though, many applications are not written from scratch anymore. You compose existing components. There was a time, before I was in this field, you know? People used to write machine language, then assembly language. You wrote things like maybe in C and so forth. And you started with a program out of scratch from nothing. Now fast forward to today. Everything's composed. So you may have some storage services, some database services, some caching services, some identity services. And you compose your application out of that. Mm-hm. And that's really the way to go from now on. Right. And you compose things out those compositions. Mm-hm. To restful APIs and the like. Mm-hm. So in this spectrum I, that's why I am not a very fan of those terms. You know, yes they have a definition. Yes, you can look at them and apply them. But what's more interesting is to have a very large, scalable, reliable platform with rich services that allows you to run, if you want to, VMs, all the way to compose apps at a very high level. So in, in that sense what I'm hearing you say is that these distinctions are, are not that important. Architecturally, scientifically, whatever. There is no. I don't think so. There is more of a marketing. Marketing or, or a way to simplify concepts. Okay. Remember cloud computing is still early on. And we feel early sometimes people have to define terms just to be able to reason about the concepts. To me speaking for myself I really think it's far more important to look at the actual services APIs have and what can you do with them to make your life easier to develop applications and to run reliably at scale. Now students get to know about these giant scale services and how cloud computing is enabling the giant scale services. What do you see as the future evolution of system structure for providing distributed services? Is it going to be the same, more of the same, or is it going to be drastically different in the future? To be honest if I knew exactly I'd be probably lying. We, nobody knows, exactly. But we can look at the trend. Mm-hm. And the trend, as I alluded to, shifted from the old days where we used to have three machines in our data center. Mm-hm. You and I used to have three machines. Yeah. We used to have mainframes bigger than this room we're in at the moment. Is shifting to more of a scale out model. Where it's built out of smaller servers, off the shelf components, and many, many of them. That's dictating a trend in application structure and format that's different than before. You are no longer looking at big SAP boxes and an app that optimizes a big SAP box. Looking at an application that may have multiple cores in a box but has to live across boxes, across servers, and has to deal with failures, and has to deal with the network, as as a, as a reality. Latency is there and so forth. So the shift will be more and more towards scale out, horizontal scaling, versus scale up. And more trend towards simplicity where failures will happen and you have to optimize for recovery from failures and to make forward progress. Said differently all our apps today are what? You know, back end services to your phone, to your PC, to your laptop, to your pad, et cetera. They expect the network to be always there. They expect the servers to be always there. Of course, this is not true, but you have to design it as it, to make sure it can handle failures, for it to appear to be always there. So this disconnected operation of the network is important. Caching and, and, the, and the host stack is important. And when a component fails, you need to have to state somewhere else. Mm-hm. The truth has to be in the network, not anywhere else. Mm-hm. Mm-hm. So those, I think, trends will only accelerate. Mm-hm. Mm-hm. Mm-hm. Exactly where they're going to be in ten years from now, I'm not sure. Mm-hm. Five years, probably like I described. Mm-hm. Now, so I think you started touching on this already in your answer. So what do you see changing in the data centers for catering to these kinds of future evolution of distributed services? Uh,a number of things are changing in a data center. First given the accounts of scale you have by having the horizontal model, you have a, an increasing concentration of computing power in a, effectively in a flat network. So, when you said horizontal model. Yes. Perhaps you may want to elaborate it for the students. Okay, so on a horizontal model if you want extra capacity you end up adding more servers to a network. So, you may start with one server with maybe one CPU with multiple cores and a fixed amount of memory. If you want more capacity, like extra memory or extra cores, you don't open up the box and add to it. There's nothing to add to it, the box is closed. You add more boxes. I'm being figurative here, but you know what I mean. So, this is as, to contrast how it used to be in the past where you bought one big mainframe, one big chassis, and you added more stuff inside it to make it bigger. So, you have to know networks. You have to know distributed systems. It's a fact of life, basically. So the trend is basically to go that way. If, if you go to a modern data center now, a truly modern one, you're going to find a network which is fairly flat network where it, it caters both to south, east traffic, and west, east, west traffic. Which in different words, it can do well to take requests in and send them back out, or to do traffic within the network. Meaning computations that are going inside. Computation and data replication. Okay. So that's a trend that's definitely happening, so, scale out model of many, many servers. Typically commodity service, which is fairly small. They increase of course with Moore's law, but still are fairly small. A network which is fairly flat, so you you can do a lot of traffic between them. And applications that are basically laid over that network. And with the app itself is mostly, mostly structured in way it can withstand any piece going down. And the state, replicated in different data tiers. So you already touched on this aspect also. The fact that more and more we are seeing end users using mobile platforms. Yeah. And mobile platforms are entirely in the cloud. And so as this future revolution happens in the data center things are changing. What do you expect as changes that are happening at the consumer end of these services? To be honest, the consumer end is driving most of this. Mm-hm. And the classical enterprise IT is following. Mm-hm. So the, the consumer revolution in the mobile And small devices of the like, are driving much of this criteria. And the benefit then as accruing over time to the usage of the systems. So, you want examples. Right. With more and more collaboration needs, or sharing of state. You're going to put more state in the back end. To be able to share them with multiple users. Or they need to do things beyond just messaging, you know, sharing, sharing data sharing state and the like. You'll find out that data will flow quite a bit from the devices [INAUDIBLE]. And there are physics however, the device cannot be very big, the battery or device cannot be very big, so the device can only do so much. So the back end has to do it for you. Right. And finding the balance between them will change over time as well, of course. But there's also brings an interesting, concern, that is, as you said, there's going to be more that's happening in the back end, right? Because the mobile devices may not be able to do it all. And you also mentioned that, there may be collaboration among the mobile devices. And a lot of the things that happen in mobile devices today are streaming kinds of things, and how ready is the cloud for dealing with that kind of traffic. And in particular you also mentioned that cloud is architect ed today as throughput oriented not so much latency oriented. So what do you see as changes that might be coming [UNKNOWN]? [CROSSTALK] The different type of streaming order. Okay, so for the, the bulk of inter traffic [UNKNOWN] steaming of pre recorded material. And the way that Internet can handle it at large as you know its [INAUDIBLE]. So you have circle cs networks. Content solution networks. Where the content is told that they edge closer to the user and once you ask for a movie and you'll be able to ask for it the second time is almost free and by and large that's all working practically. I mean um,uh, depending where you are in the world, Netflix for example in the US is very big and so forth. To do more real time, it becomes more interesting, and more conversant to be honest. And I think, I think things will change as well. But I have to tell you, I am myself amazed how it works are already in that sense. The latency aspects are, have to do with Ultimately with physics. So there will be cases where if your application requires a lot of chatty, lots of protocol between a component and another component, you have to think seriously about where they should land. If it's very chatty and physics, as the same, we cannot change the laws of physics, They have to be closer to each other. So decomposing the app, whether it be, be to [INAUDIBLE] time live streaming of data or video, whatever, is very important to put chatty stuff and where they should land, which is where they can withstand the long distance. Right, but that also brings up the question of data centers as they are architecture today. Yes They are, as you mentioned, they're in some darkroom somewhere. But if my mobile device is latency sensitive, shouldn't it be closer to where the mobile device is? True. And there will be multiple levels. So you want put some competitional on device. That's, if it, if the latency between two components is so critical, there's no way you can cross any wire So some will live in the device itself. Some will live on the edge of the network before we get to the actual DC. And in, increasingly, there are mega services on the internet. Don't directly hit, you don't hit the DC directly. You go to a net somewhere. And you do also when optimizations. TCPIP optimizations. For example, You may land to an edge closer to you. And there the connection between that base and the actual DC yard, I get to create over common [INAUDIBLE] connections. So tricks are definitely important. Latency hiding tricks are important. So this next question I'm going to ask you is more related to. From the students' perspectives. You know, the students that are taking this course, this is an advanced operating systems course, and and these students, they may be aspiring to work in leading software companies such as yours. What would be your advice to students that are taking this course? Specifically can you talk to the students about, the opportunities that exists, for O.S. designers today in the industry space. And how the students should prepare themselves for such opportunities. Sure, I'll be honest with you, you may not design new O.S. from scratch We have a bunch of them already. But every concept you're learning here will be needed, more than beneficial, needed, when you move up and down the stack. If you're doing an application, like much of the discussion we had right now, a high scale application, it's, it's no longer writing an application through on a machine by itself, displaying to a screen. All the issues we touch on, you have to worry about, you have to know about them. You have to know about caching and scale, you have to know about synchronization, data replication, handling failures, so all these concepts you have in a classic of OS sense, are directly applicable regardless where you live up and down the [UNKNOWN] So I'm convinced that with out a fund, a strong foundation operating systems and a few other topics, programming languages and the like. One cannot handle or build those big applications. You could do some simple work programming, do some java script and a nice animated screen, don't get me wrong, this all is important. But if you want to do the real scalable services where the entity's moving you have to have the foundations. More generically, you have to also use the information you learned here and be ready, ready to learn more. And to apply it in different domains. So the classical papers that you taught me way back when are still relevant. But as we discussed, you may have to apply them differently to morph them, evolve them, into newer [INAUDIBLE] areas. So I want to make sure that the students get an idea about specific things that you think that they should get when they're in school. What are the things if you can name, specify, I mean, specifically can you name a few things that you think are important for students to get while they are still a student here. Mm-hm. College time. The time you are taking right now to learn all this stuff is a most exciting time to be honest. Because you can keep an open mind. And go white. So learn as why I think that's possible. You want specific things in computer science, let's be specific here. Definitely operating systems. Distribute systems and definitely programming language as a background. I think they're very important. The foundations for any platform level type of work if you [UNKNOWN]... Know the classical information, but also know how to apply them and what you actually have in every day and the like. What else would I want to do? Frankly beyond CS, I would also try to get other back ground that complement what I do. Mm-hm. Even the things like humanities and the like, that are actually important because when you move up and you want to work in industry and the like. Much what we do, we actually write code. We design code. We make software. But we also, no longer build things by ourselves. Now the systems you have at your disposal aren't built by one person, they are built by teams. So learn how to work in teams. Learn how to deal with other people like you who have to collaborate. And, build a software collage. Some districts can only, we often, look, you may only learn the basics in college. But, as you go forward, you know, apply the large software. And also, don't shy away from the tough assignments, you know. Push the envelope while you can right now. Then, come back and you can really apply them as you go forth. Lastly a good foundation in just knowing the basics of computer science, basic you know, algorithm, algorithm design analysis is actually very important. So the more advanced topics, you never did ask you in industry what np is equal to p, and go solve this one or solve that. But you will have to reason almost every day about the basic complexity of an algorithm, or to basically to apply a certain technique on the like. And the foundation algorithms are really important as well. So outside of school, though they are in school, there's a chance for them to, to hone their skills overall. What do you think of things that they should be absorbing outside of school? I would start the habit now, outside of school, to keep track what's happening out there. If you join [UNKNOWN] tomorrow, you will, in a sense, have stopped learning what the [UNKNOWN] stuff is and you start going and learning what's happening out there. And things move very quickly out there as you know. So start right now understanding what's happening in general. So know the fundamentals but also know how people are applying them. Be curious. I mean I sometimes bring up a browser look at a pic and say how they done this thing and oh it's Ajax doing dress for APIs and so forth. So learn to be curious from right now and learn to link what you do with what you see around you as well. And that habit has to start early and don't stop it. Keep it going. The worst thing that can happen is for you to go to industry and to spend five years doing the same thing and nothing but that. You have to be able to be curious and see around you as well. And that habit has to be started right now. So the next question is very specific to you and your own journey. >From a student in the 80's to becoming a leader in your chosen field, what are some of the skill sets you wish you had acquired when you're still a student. If you reflect back, what are the things that you would have done differently or learned differently as a student that might have helped you? Honestly, I mean, I'll be quite honest with you. I took the least amount of humanities I could get, and I loaded my schedule with CS and E,classes and math because that was the fun stuff, the heavy stuff. And not go any wrong this always important too, but take a balance view to this. The humanity is all important knowing what's really around you beyond just the actual chord and the bits are very important. And the reason is very simple really, which is a part of my last answer. When you go out there and do things, you'll be expected to lead. To work with others and to tell them where to go, if you will. So you can lead them or you can follow. It's better to lead, to be honest. And to lead, you need to have soft skills and an ability to understand people. So learning about the humanities, learning about critical thinking and writing. Had actually very important, and sometimes we in Academia don't pay enough attention to these things, to be honest. We'll be great to make sure you hone your skills in that space as well. Mm-hm. I mean a critical for example, in the US, a critical English writing class may sound, you know, too much work, and why do I need this. But it can really hone your skills for critical thinking, critical analysis. Of the language. So, fast forward 20 years, or in my case more than that, and you have to send an email explaining why something is going right, is going wrong, et cetera. Conveying the message in two paragraphs or to some busy people, that's actually a skill that has little to do with writing code, to be honest. But more of, can I put my thoughts down to paper quickly and clearly and concisely? With softer scales or in ethical scales are important too. So, this is more of an advice from me of professors like me from your experience. In your opinion, what you, what. How do you think we can better prepare our students? To be ready for the work place. There's a couple of dimensions. One is, just to bring the last thought, and then we'll switch a different point. One dimension is that in Academia, you, you, you're going through a class, you're going to give a grade to every student and the grade is his or her grade. So in academia we basically look at the individual performance, did I pass or not? Don't get me wrong, the same happens in industry, but to get through anything in industry, you have to collaborate. So what's usually missing in, in, in, in the more academic environment is how to build software, for example, in a, and how to do the soft skills for building software in a group. And those things are hard to get, I know, in academia, but it is something I wish we can get more at the graduate, with that background, if you want. You typically have something in you when you, when you're, you're a good leader, but then when you go join a company and the like, you have to go nurture and do more of that. Otherwise, that really would happen. Beyond that, there are some things which would be great if you make sure that the students can push the envelope in ways that otherwise we in industry cannot do. So make sure they have in them the, ability to try things which otherwise some of us will try it and say it doesn't work. You know, I'm not jaded by that. So it's important to instill in the students that, that, and I think you do a good job of that, but it's important to remember that, that, that to push the envelope. Because once you hit industry and the like, you're going to find constraints. You have to shift quickly. There's a deadline et cetera, and you may forget about what's possible. Never forget what's really possible. So, do you have any final thoughts that you want to share? Again, thank you for the opportunity. This is great. I hope you enjoy his class. I'm going to actually plan to take it. I'm not going to take it for grades ok but I'm going to go I want to see it. I'm looking forward to it. I think, sir, it's a great thing you're doing. And I wish you real luck. Well thank you so much, Yousef, for coming and joining us. Thank you, thank you, sure. And let's thank Youself for being here and sharing his thoughts with you all. Thank you. If you walk up to typical computer today, you're likely to find several applications open. One for listening to music. One for playing your video game. A web browser displaying information on some ancient culture from some remote corner of the world. And a messaging application for connecting with friends, and so on. How is it possible for a single device to do all these things, and moreover, do them all at the same time? How can all these diverse applications, written by different programmers who don't even know each other, coordinate? Who's in charge here? The answer of course is the operating system. Hello and welcome to CS6210 Advanced Operating Systems. My name is Kishore Ramachandran and I'll be taking you on a journey to understand the role played by the operating system in providing the rich user experience afforded to modern applications, the modern computers. And along the way, I'll highlight the symbiotic relationship between hardware and software that makes it possible for the computer and the operating system to provide the rich user experience to please you. Operating systems have evolved over time. In this course we will see all the advances that have been made in the operating system that has led to the state of the art in operating systems today. Covering a variety of platforms, from cell phones to multi-core, to parallel systems, to distributive computing and cloud computing. In part one of this introductory lesson, my goal is to primarily talk about the power of abstraction. That will lay the foundation for the rest of the introductory lesson, which will include an overview of the hardware resources managed by the operating system. The operating system functionality that is expected, and the abstractions in an operating system for managing the hardware resources. By the end of the lesson, we will be ready to discuss the structure of operating systems. Much of the background that you'll need to know, is covered in this introductory lesson and the review lessons taught by Charlie Brubaker. However, to get the full benefit of this course, you should have a strong undergraduate background in computer systems and networks. With that in mind, let's get started with our review and load some of the operating systems basics into our collective memories. I've intentionally structured this lesson to be quiz-heavy, since most of the background material should be already familiar to you. And hopefully that format that I'm going to use, we'll find this to be a more fun way to jog our collective memories. Having promised you that, let's start with a quiz. Now this quiz is a fun quiz just to get you primed. Which of the following is an operating system? I'm sure that you've been using a lot of these things and heard these terminologies. Firefox? MacOS? Android? Which of the following is an operating system? The names are a gagetary, doesn't it? Well, we all use Firefox. It is a browser. A browser is not an operating system. It is an application that sits on the operating system. You may have heard of Android. It's a system software stack that provides a lot of services but sits on top of the operating system. So in this. Set of choices that I've given you MacOS is really the only one that can be categorized as an operating system. [BLANK_AUDIO] As computer scientists, one of the most powerful tools that's available to us, is the concept of an abstraction. And by an abstraction, we mean a well understood interface, that hides all the details within a subsystem. Let's see how, in a computer system, the hierarchy of abstractions works, and where operating system fits, within this hierarchy. Again, I'll want to use a fun quiz to get you situated with respect to the concept of an abstraction. Now remember what I told you about an abstraction, that it's an interface that allows us to hide all the details. With that in mind, what I want you to do Is to think about the objects and the terms that I've laid out on this panel for you. And you may have come across many of them and from this set tell me which of these is not an abstraction. There may be more than one item in this set that I've shown you on this panel that may not be an abstraction and I want you to identify all of the items that you think is not an abstraction. The first choice I have is a door knob. The second choice I have for you is instruction set architecture. The third choice I have is the number of pins coming out of a chip, like a processor chip. And the next choice I have for you is a logic gate such as and gate that does the and function. And the next item I have for you is a transistor. And the next item I have for you is the INT data type in a programming language like C. And the last item I have for you is exact location of base pads in a baseball field. So these are the choices that I'm giving you and what I want you to do is think about these different items. Which you may have come across for sure in your every day experience, and tell me which one of these does not qualify as an abstraction. And as I've said, there could be more than one that does not qualify as an abstraction. Let's walk through all the choices I gave you, and try to understand which can be categorized as an abstraction, and which is not an abstraction. Door knob is something that we use all the time to enter a room, when you open a door. And that's an abstraction because we don't know how the door knob may actually be implemented, nor do we care. So that's an abstraction. Instruction set architecture of a processor is once again an abstraction, because it tells you what the processor's capable of doing, not how it is doing it. Now the third choice I'm giving you here, is the number of pins coming out of a chip. Now, this is being very specific as to, what exactly is going on here? That these are exactly the number of pins coming out of a chip, and therefore it's not an abstraction. The next choice is an and logic gate, again here it's only saying what the function of the logic gate is, not how it is doing it. And therefore, this is an abstraction. The next choice is transistor, and this again, is an abstraction, because we don't know how exactly the transistor is implemented. All we know is a functionality of the transistor. For instance, if a transistor is used as a switching device, you know that using the transistor you can turn it on and off, but you don't know what exactly is going on under the covers in order to make that work in the solid state physics. The next choice is INT data type. Now, again it's only saying that it is a data type and the name is INT. And therefore you can use this as a data type that holds integers in your program, but it doesn't tell you how the data type is actually implemented by the compiler and therefore INT datatype is also an abstraction. The last choice is saying the exact location of base pads in baseball field. Again, this one is being very specific about some particular feature, namely, base pads and how they're laid out in a baseball field and therefore, this is not an abstraction. Now, let's dig a little deeper into the power of abstraction as it applies to computer systems. Now, here I'm showing you a screenshot of the Google Earth application. Deep down in the silicon, you have electrons and holes. You may have heard about these terminologies or worked with them in a physics 101 course at some point of time. Let's say that in an application such as Google Earth, you move your mouse from Atlanta, Georgia to Chennai, India. When you do that, magic happens on the screen. Now, between a user action, such as moving the mouse from Atlanta, Georgia to Chennai, India, and the solid state electronics that is enshrined in your computer system, there is a whole series of abstractions. Now, I want you to help me think about all the abstractions that sit between these two extremes, Google Earth application that makes magic happen on the screen and the solid-state physics the electrons and holes enshrine in your computer system. What are all the abstractions that sit between these two extremes? What I want you to do in this quiz is to name as many abstractions as you can between these two extremes that I am showing you. To make room for you, let me clear some space. I put a box now for your answer. To give you an idea of what is expected in this question, as an answer, let me start you off with naming the abstraction that sits right on top of the electrons and holes. This is of course your solid state physics. And you rain in all the randomness in the movement of the electrons and holes in the solid-state physics, with this abstraction called transistor. The transistor is akin to an on-off switch. Now, what I want you to do is fill the rest of the box with all the other abstractions that sits above transistors, all the way up to the application. You may have gotten a lot of these abstractions yourself, but let's walk through and think about all the abstractions that sit between the electrons and holes, and applications at the top. We've already talked about the transistor, we know at the lowest level of the hierarchy, the movement of the electrons in the holes. Are governed by the laws of physics. And as I mentioned earlier, transistor reigns in the randomness in the movement of electrons and holes, and gives us an abstraction of a switching device, which is the transistor. Using the switching device which is a transistor, we can implement. The next level up in the hierarchy which are logic gates. Here is where the and- gates and the or-gates and the not-gates come in, where we are implementing Boolean logic using transistor as a switching device. Sequential and combination logic elements are implemented using logic gates such as and, not and or. Logic elements. Sequential and combination logic elements are then organized into a data pack, depending on what hardware circuitry, or functionality we want to implement. The data pack establishes the communication paths we need between these combinational, and sequential logic elements. To realize whatever is the hardware device that we're trying to design, and the control part of it is a finite shaped machine that controls the data path, and implements the repertoire of the hardware device that we are trying to realize. And for instance, if. The intent is to implement a processor, then the instructions in the processor have to be implemented using the data pad, and controlled by the control logic. Next level up in this hierarchy is the instruction set architecture. This is the abstraction that is defined by a processor. When you hear the promo commercial, for instance, that says Intel Inside, it is the ISA, or the Instruction Set Architecture that is being talked about by the promo. Buried within. This term ISA, or the abstraction ISA, is all the details of how the ISA is actually implemented by the data path and the control logic. Now we come to software. Sitting on top of the Instruction Set Architecture, is the system software. Operating systems, compilers, run-time systems and so on, that exercise the instructions provided by the processor architecture. Finally, we have apps on top. This is where applications like Google Earth come in. Notice that the abstraction that is used by the applications themselves is the high level language in which these applications are written. The system software component such as the compiler, translate the high level language used. In writing these applications into the instructions that can be executed on the processor. Similarly, the operating system provides interface for the applications to request services of the operating system. For instance, to access specific devices. During the execution of the application, or for getting resources such as memory. On demand, those are the functions provided by the operating system as an abstraction. Thus, the instruction set is the meeting point between software. And hardware. It serves as the abstraction needed by the system software, for example, the compiler, to generate the code that corresponds to the high-level language constructs that are being used in coding up an application. The software, whether it is. At the system software level or at the application level does not care and does not know how this instruction set may actually be implemented down below. Meaning the implementation of the instruction set itself in terms of data path and control. The sequential logic, elements that sit below that, are the logic gates of the transistor, or finally the electrons and [INAUDIBLE], they don't care what programs are running on top of the processor. So the instruction said architecture is the meeting point between software and hardware. The hardware implementation simply fulfils the contract. Of realizing the instructions of architecture of the processor. Operation system is the abstraction level we will be focusing on in this course, which is a course on advanced operating systems. You will have the opportunity to learn about other levels. In this hierarchy, from other courses that are offered through this program. Now an application, such as Google Earth, sits at the top of the hierarchy. And it's code for tracking the mouse movement on the screen, written in whatever high level language is compiled into the instruction set and traverses the rest of the hierarchy and magic happens on the screen. Now within this hierarchy, the operating system acts as a broker between the application at the top. And the physical resources such as CPU, memory, and IO devices, can provide an app, with all it needs, for its successful and safe execution. Most of what we'll be learning in this introductory lesson plan should feel like review to you. If it has been challenging so far, or becomes challenging as you go through the introductory lesson, it might mean that you don't have the necessary background for taking an advanced operating systems course. After that fun review of the power of abstractions, next we will take a look at the hardware resources being managed by the operating system. As we will see, one of the operating system's role is to control access of the applications to physical hardware. So a basic understanding of hardware components and how they interact is essential to the course. Let's do a quick review. What better way to start the review than by a fun quiz. Now in this quiz, I'm showing you the hardware continuum that exists today in terms of the devices that you and I are using on an everyday basis. Smartphones all the way to the cloud and a lot of data points inbetween. There are tablets, and laptops. There are desktop computers. There are servers. And of course, the cloud computing itself that you may have heard of, that is being enabled by data centers. So this is a hardware continuum that you're looking at. And the question that I'm posing to you is, is the internal organization of computer system in this continuum, going all the way from smartphone to the cloud. Is this vastly different? That's the question. And it's a binary answer. Yes or no. The right answer is no, and since this might seem a bit counterintuitive given this huge range of the hardware continuum, let's reason it out. Basically, the hardware inside a computer system consists of the processor, memory. And implode the devices. And the organization of these hardware elements within the computer system, whether we are talking about smart phone or cloud, is not going to change tremendously. So we have this hardware continuum from cell phones and PDAs to laptops, to tablets, to desktop computers, all the way to nodes in a data center that is providing cloud computing services. Regardless of these different manifestations and sizes, the organization of the hardware inside the computer system is pretty much the same. You've got a CPU. And the CPU is connected to a conduit, which we call the bus. And you've got memory that has the instructions and data that is needed for the CPU to execute. And the CPU can use it's conduit, namely the bus, to access the memory. And you need storage for persistance of files and other data that your produce during the computation. And, once again the CPU can access stuff on the student's device through a controller that is also connected to this conduit. And you may have other peripheral devices, like a microphone and a camera. All of these devices, of course, are accessible for the CPU through this conduit, which is the bus. And of course in this day, and age a device has to be able to communicate with the outside world. You have a network, and once again the network is interfaced through a controller to the CPU via this conduit. So this conduit for communication Which is the BUS, serves the purpose of connecting the CPU to the memory as well as all the iode devices that it may need to interact with in order to do whatever the intent is of the user and the application that they want to run on the computer. And depending on the sophistication of the device that we're talking about, and the speed of the device that we're talking about. These controllers that I'm showing you may have different capabilities. So for instance a network controller may have the ability to move the data directly from the memory into the network, or from the network into the memory and this As you may have learned in a computer organization course, is called direct memory access for the controller to move data swiftly between memory and the IO device. And similar DMA facility may exist in the controllers of other high speed devices, this is the high speed device. So that's the capability that you got for some of these controllers. Some of these controllers may be slow enough, for instance a keyboard or a mouse, that the CPU can directly query new data that is coming from the controller that is associated with slow speed devices, and move it into memory or use it in any fashion that it wants. Here are the more elaborate organization of the hardware resources inside a computer system with an I/O Bus and a system Bus, and usually the system Bus, is a synchronous communication device between the CPU and the memory. The I/O bus is primarily intended for the devices to communicate with the CPU. And the intent is that the individual needs of each of these devices, in terms of the communication bandwidth that they may need, is less than the cumulative bandwidth that may be available for the CPU to communicate with the memory. Or in other words, the system bus has a communication bandwidth that is much more than the communication bandwidth that's available in the I/O bus. So the system bus is a high speed bus and it connects via a bridge to the I/O bus. And this bridge itself, could be a specialized IO processor for scheduling the devices that need to communicate with the memory for instance, if it is a DMA device or need to communicate with the CPU if it's a slow speed device. So the role of the bridge is like a processor in itself. Controlling who has access to this I/O bus among the set of devices that maybe competing at the same time for attention by the CPU and communicating the intent of these I/O devices either directly with the memory or via the CPU. The I/O bus is typically lower speed than the higher speed system bus. And the cumulative bandwidth that is needed in the system bus is usually much higher because it has to cater to all the clients that may want to access the memory, either from the CPU or from any of these devices coming through this bridge. There may be other high-speed devices, for example, a frame buffer, of a graphics display that may also be hanging off of the system bus due to the need for refreshing the screen in a rapid manner from the memory. So in a nut shell, if you look at the internal organization of a computer system that are going to be one or more CPUs, whether it is a single-core machine or multi-core machine or parallel machine and so on. You're going to have one or more CPUs and youre going to have a bunch of memory that the CPUs can access and there's going to be a whole number of input-output devices. And device controllers that allow these devices to communicate with the CPU, or directly with the memory. And there are conduits, system bus and I/O bus, for connecting these controllers to the CPU as well as to the memory. Basically, these conduits allow ferrying data from the CPU to the devices, or from the devices to the CPU, and between CPU and the memory, as well as from the devices to the memory. This is why I mentioned earlier that there is no difference in the organization, regardless of the platform specifics, the form factor may be different, or the intended use may be different of a particular platform. But the internal organization is pretty much going to look as I've shown you here in terms of the hardware elements that are going to be there as well as the connectivity that you might see among these hardware elements. The specifics, such as the computational power, the memory capacity, and the number and types of input, output of devices, may vary from one manifestation of a computer system, to the next. For example, commensurate with it's intended use, a cell phone or a pda may have very limited input/output capabilities such as a graphics display, speakers and microphones. But on the other hand, a high end supercomputer used for running large scale scientific applications may employ thousands of CPUs, incorporate several terabytes of memory and be connected an array of disks with storage capacities on the order of several parabytes. In summary then, the organization of a computer system is consistent enough that many of the key operating system concepts apply to all of them regardless of size and capacity. On the other hand, these differences should not be ignored and advances in hardware have helped drive innovation in operating systems, so that we can get the most out of their capabilities. Now, having reviewed the conceptual tool of an abstraction and the physical resources that is contained in a computer system, we turn our attention now to the major subject, the operating system. What exactly is the operating system? Come help me describe one. So we want to understand the functionality of an operating system. What better way than ask you what you think are the functionalities that should be provided by an Operating System. So here are some choices I'm giving you in this question, and what I want you to mark off or select are choices that you think describe the functionalities that should be provided by an Operating System. And, more than one of these choices may apply to the functionality that you expect from an Operating System. So go ahead and choose these correct choices. The first choice is, Operating System is a resource manager. The second choice says, the Operating System provides a consistent interface for the hardware resources such as CPU, memory and so on. The third choice says, the Operating System schedules applications on the CPU. The fourth choice says, the Operating System stores personal information such as credit card numbers, social security numbers, e-mail addresses and so on. So, pick the things that you think are functionalities that you expect an operating system to provide. Now lets talk through the choices I've given you. yes, the operating system is a resource manager. It is the boss in control of the hardware resources. So that's a functionality that you do expect from the operating system. Yes, the operating system - Is the entity that provides a consistent interface to the hardware resource. Remember earlier lecture on obstructions, I mentioned that operating system is a level in the obstruction hierarchy sitting between the application and the processor and the other resources and so in this sense, the operating system should be the entity that provides a consistent interface to the hardware resources such as CPU, memory, and io devices. So yes, the second choice is a good one in terms of functionality, that you would expect from an operating system. The third choice says the operating systems schedules applications on the CPU. This also seems like a think that the operating system should do because there could be multiple applications that require these resources and therefore their has to be an arbitor that is going to provide a way of scheduling. Those requests on the hardware devices and therefore yes. The operating system is an entity that schedules applications on top of the CPU. The last choice says the operating system stores personal information such as credit card numbers, social security number and email addresses. Now this looks like something that you wouldn't have want the operating system to do. And these are, these are personal information and you would not want the operating system to store such information, and therefore this is not a functionality that you expect of the operating system. So the first three choices are the right choices. The operating system is not an entity that should be storing your personal information such as credit card numbers and so on. Most concisely, an operating system contains code to access the physical resources, that is contained in a computer system, and arbitrate among competing requests for those hardware resources, coming from multiple applications that may be running simultaneously on the processing platform. So in other words, the operating system is just a program, like any other program you may have written. Okay, slightly more complex than a hello world program, I'm being facetious here, intentionally. But you get the point. It's a matter of climbing the latter of programming, just like, a ball player may start from t-ball to little league to the minor league and then to majors, you can also do it. If Microsoft can do it, you can do it. If you can write a hello, world program today, you can eventually write an entire operating system. So what the operating system provides are well-defined APIs for accessing the hardware resources that are managed by the operating system. And these resources are provided as operating system services through this well defined interfaces, and an application may make a request to the operating system for such hardware resources through this well defined api interface. And the application, via the response from the operation system gets the services from the operating system via these API codes. Let's start by giving a few examples of how hardware and software interact. An application asks for and obtains services from the operating system. For example, reading a file that is stored on the hard drive. Or clicking the mouse on Google Earth to take you virtually to some place. Now let's understand, how the hardware architecture and the system software work together to serve an app running on the computer system. Here is another fun quiz to get you thinking about hardware, software interaction. And in this question, I am asking you, what happens when you click the mouse? And I'm giving you three choices. One of those is the right choice. And the first choice is, the mouse click corresponds to specific CPU instructions. The second says, the mouse click starts up a specific program to read the spatial coordinates of the mouse. And the third choice says, the mouse click results in a CPU interrupt. As I said, there's exactly one right choice, and I want you to pick the right choice. As you may have guessed, the right choice is, the most clicked results in a CPU interrupt. That's the first thing that happens, and once the interrupt has been delivered to the CPU, there are domino effects that may eventually result in some program running to read the spacial coordinates. But the thing that happens when you click the mouse is it results in a CPU interrupt. Let's understand what happens when you click the mouse on your computer. Let's say you're running the Google Earth application and you select the point that you want to visit on the globe by clicking the mouse. When you click the mouse, first the hardware controller that is interfacing the mouse, which is a hardware device. To the system using the conduit of the bus is going to raise an interrupt on the interupt line which is one of the dedicated lines on the bus. This conduit, which I call the bus, contains, as you know, data lines and address lines and one of the things it also contains is an interrupt line or multiple interrupt lines. And the controller asserts this interrupt line to indicate to the CPU that it wants attention. It's sort of like, if I'm teaching a class and you all were a live audience. If you have a question, you might raise your hand and that's exactly what happens when you click the mouse. The equivalent of raising your hand in a classroom is what the controller is doing, but asserting the interrupt line on the bus. Asserting the interrupt line on the bus results in an interrupt to the CPU, now the processor at this point of time, is running some program. Perhaps your Google Earth application is being run on the CPU at this point of time. An interrupt is a hardware mechanism for alerting the processor that something external, in this case the mouse click, requires the attention of the processor. It's sort of like a doorbell in a house. Someone is ringing the doorbell, someone has got to pay attention to who is at the door. Remember that the CPU is a dumb animal. All it can do is execute instructions. Right now, it's executing some application. Now, an interrupt comes in, some other program has to run on it, in order to field that interrupt, answer the doorbell. Who's that entity? That is the operating system. The operating system, which is also a collection of programs, schedules itself to run on the processor so that it can answer the doorbell. So the operating system is the one that fields this interrupt, finds out, who it is intended for, and passes it to the program for appropriate action for this particular interrupt. This example of a mouse click and what happens between the hardware and the software is a good way to segue into the abstractions provided by an operating system for managing the hardware resources. Think back to the beginning of this lesson. Where we considered a computer system that was displaying graphics for a video game. Playing music or browsing the web. Seemingly all of them at the same time. All of these applications need access to the physical hardware And they have to be made to share nicely. No hogging the CPU, no overwriting each other's coder data. This becomes then the job of the operating system, to protect these applications from one another, and to protect an application from itself. While getting out of the way as quickly as possible so that these applications can do what they need to do in order to get their jobs done. Now we will discuss the abstractions in an operating system. From managing the hardware safely and efficiently. Particularly we'll focus on the processor and the memory. Before we talk about the abstractions in an operating system for managing the processor and memory, I want to throw another fun quiz at you. Now in this quiz, I'm going to ask you the following question. The computer, regardless of which platform you're using, seemingly runs several programs in parallel. You may have an email program running, a browser you may have music playing, maybe you're watching a video clip and so on, but supposed it is, only one CPU inside. Let's assume, that your computer system has exactly one CPU inside. How is it possible that it is running several programs in parallel? I'm going to give you multiple choices. The first choice is, even though there is only one CPU, there are multiple cores within the CPU, and there's one core for each of the applications that you're running, and that's how we're able to run multiple applications in parallel. So that's your first choice. The second choice is, I'm trying to trick you, that this is a trick question, actually there's only one app that is running at a time and the premise of the question itself is wrong. So that's your second choice. The third choice says that there's only one CPU, there are multiple applications, but these applications share the CPU through the operating system so that for different time units, different applications are running on the CPU. So, for instance, in time unit t1 application one is running, time unit t2 application two is running, and so on. And this is what is making it appear as though there are several programs running on the CPU simultaneously. What is going on is that the operating system is multiplexing the CPU among these applications. So these are the choices I'm giving you, and I want you to pick what you think is the right choice. The right choice is the third one, as you may have guessed. The operating system is multiplexing the CPU among the competing applications. You may have multiple cores, but you don't have one core for every application that is currently running on the CPU. It may be that you've only have one core. Or, exactly one CPU. In which case, it is the operating system that is multiplexing these applications to run, at different points of time on the CPU. And that's what is giving you this appearance as though, there are multiple programs running concurrently on the processor. But they're not running concurrently. It is just that they're being multiplexed, so that its appearance as though, they're all running in parallel. The resource needs of the applications include time on the CPU, the memory that it needs for holding its instructions and data, and peripheral devices that it may have to access during the course of its execution and so on. Now, are the resource requirements of a program known to the operating system before you launch it? Well, yes and no. The operating system knows enough about the program, at the time of launch, so that from the disk, it can create a memory footprint for this application. So for instance, on your favorite platform, when you click on an icon, what is going on is, a piece of the operating system called the operating system loader is reading in the disk resident image off that application, and creating a memory resident image of that application. So this is what is called the memory footprint. And the memory footprint of the program contains the code that needs to get executed on the processor, global data that it might be accessing, the stack that is needed when the program is making procedure calls and the heap which is the dynamic memory that it might be needing during the course of its execution. So this is what is called the memory footprint of the program. Then that's what is created by the operating system loader at the point where you click on an icon. Once a program starts running, can the application ask for additional resources at run time? Of course. This is exactly the service that is provided by the operating system. For example, if the application needs more memory, it can make an operating system call and similarly if it needs to make a connection to access a web server it makes an operating system call. The operating system then performs the service on behalf of the application and the application can then continue with whatever it needs to get done. That's how an operating system caters to the resource requirements of applications. So in other words, in addition to catering to the initial requirements of an application at the point of launching it, the operating system is also the broker through which a running application can request and get additional resources during its execution. That brings up a question. If the operating system is a broker, is it taking precious resources, for example, CPU cycles or available physical memory, away from an application? And I'm giving you three choices here. Yes it is, taking resources away. No it's not. Maybe. So these are the three choices I'm giving you, and I want you think about what would be the right choice, and I think you will know what the right choice is. As you may have guessed, the right choice is no. The operating system should not be taking precious resources away from an application. For example, if I'm running a program such as, computing the prime number up to a billion, it's going to need a lot of resources, CPU cycles in particular, and during that time I don't want the operating system stealing cycles away from me. So operating system is also a program and has to run on the CPU as we saw when we talked about how an operating system deals with external interrupts. So it is going to need some resources, CPU and memory cycles, to do it's work. But a good operating system will take the minimal amount of time and minimal amount of resources to do it's thing. So the right answer is no, as otherwise you will not use an operating system. It's sort of like when give to a charity. The first question you ask is, what percentage of the collection is used by the charity as administrative overhead? You don't want to give to a charity that spends more than a few percentage points of the collections on administrative overhead. Same thing with an operating system. Most of the time, the resources, CPU, memory and so on, is being used for running the applications. The operating system gets in the way as a broker, only for arbitrating and providing the resources needed by an application safely and securely, and then get's out of the way as quickly as possible. So the right answer is no. A modern operating system is a complex piece of software and lots going on, some of which may be completely outside the user's control. For example, a network message comes in, and your anti virus software goes into high gear checking for attacks. The normal behavior of a good operating system should provide an application with the resources is it asking for. For example, more memory or access to a file, and so on. And get out of the way, as quickly and quietly as possible. Now, let's get familiar with some of the abstractions provided by an operating system. I'm sure that you may have heard several different terms used in the context of an operating system such as program or a process or a thread and task and so on. Let's understand these terms. A program is the memory footprint that is created when you click on an icon to launch an application on your favorite platform. So it is a static image of the program loaded into the memory. A process, on the other hand, is a program in execution. That is, the operating system, breathes life into the program, which is a static entity, by running the program on the CPU. That is, by scheduling the program to run on the processor, the operating system gives life to the program and the process is the program in execution. Therefore a process is the program plus the state of the running program. And yes, the state of the running program is not static. It is continuously evolving as the program executes. So these are the two important abstractions related to the process and program. Which is a static entity created by the operating system when you launch a program. And process is the program in execution. Now you've heard the term thread. Well, it's also used in the context of an operating system. What is the difference between a process and a thread? An analogy will help here. Let's say here is the morning newspaper. This morning newspaper lying on the dining table is like a program in memory. No life. I come to the dining table, pick up the newspaper, and particularly the sports section of the newspaper and start reading it. My starting to ready the sports section of the newspaper is akin to the operating system, giving life to the program by starting to execute it. So, now there is one life in the program. That is one line of control, that is coursing through the core and data structures of the program. This is what is called the thread of execution through the program. So we have one thread of control that is coursing through the program just as I am reading. A section of the newspaper. Now, my wife comes along, and being the more responsible one in the family picks up the business section and starts reading it. That's perfectly fine, depending on our interests, I'm reading the sports section. While wife is reading the business section, each is reading a different section of the same newspaper. similarly, we can have multiple lives, coursing through the program. Each blazing a completely different trail through the code and data structures of the program. Now each of this is a thread of control. Now could there be a conflict between these different threads of control? Sure. Both my wife and I may want to read the same section of the newspaper. That's a conflict. Similarly. The threads that are executig whitin the same program may wana read or update the same data structres. These are the issues that the operating system has to deal with, and this is what I meant when I said that te operating system is the orbiter for copleteing reqests for rescorces. Now generalizing it. A program can have several threads of control, and each thread of control maybe coursing through different sections of the program. And it could also be competing for the same section of the program as well as the same data structure in order to manipulate. Thus, a process. Is a program plus the state of all the threads that are executing within the program. Just as a single newspaper could be shared by me, my wife, and possibly my children. In a similar manner, a program may have multiple lives. That are coursing through it. And each is a bit of control, and the process is the program in execution, meaning it is the program plus the state of all the threads that are currently executing within this program. For example, if this program. Is a web browser. One thread in this program, could be fetching a page that I've requested from the remote server. And another thread could be painting the screen for me. How is one program, let's say an email, protected from the misbehavior of another program, say the web browser. This is where memory related operating system abstractions come into play. In particular, the operating system provides address space as an abstraction for each process that is distinct from one another. So the data and the code that corresponds to a particular program is contained in a container which is called the address space. That's the abstraction provided by the operating system. And this address space abstraction of the operating system is implemented by whatever hardware capabilities that the underlying processor architecture provides you. Processor and memory are the most precious resources. And, what we've done is a quick review to understand the abstractions in the operating system for managing these resources. Until now, what we have seen in this lecture is a quick review of the concepts that you are most likely already familiar with. The next lesson will launch into the evolution of the operating system's structure. Before we get into that lesson, if you feel you could benefit from reviewing the basic concepts of an operating system. I strongly recommend that you review the basic subsystems of an operating system. CPU scheduling, memory management and the network protocol stack. To help you navigate this background material, my tall friend here, Charlie Brubaker has produced lecture materials that are available as part of this course offering. Hello and welcome back. As you've seen previously, an operating system has to protect the integrity of the hardware resources it manages while providing the services to the applications. Thus it has many responsibilities and a variety of functional components that provide these services. But how should all these pieces fit together? At least some of the components of the operating system will have to run in a privileged mode of the processor architecture that allows them access to hardware. But must the whole operating system have this privilege? Also, if an application would benefit from having certain services, for example, memory management handled in a particular way. Can we personalize the services to suit the needs of the application? In other words, can we make the operating system flexible in terms of the policies it implements for the services offered by it? Does this flexibility have to come at the price of performance and or safety of the operating system? These are some of the questions we will try to answer in this course module. In this course module, we will learn the basics of operating system structuring issues. We will use SPIN and Exokernel as case studies of two closely-related approaches to providing extensibility of operating system services. We will then study a microkernel-based approach as well, using L3 microkernel. This is a free-form quiz, and what I want you to do is, name as many system services as you can, that you expect from an operating system. You may have hit many of the services that I've identified here, and plus even more. And even if you did not get some of the things that I've listed here, that's okay. It's just, is sort of refresh your memory as to what system services, one can expect from an operating system. What do we mean by operating system structure? What we mean by this term is the way the operating system software is organized with respect to the applications that it serves and the underlying hardware that it manages. It's sort of like the burger between the buns. There is application at the top and technology or hardware at the bottom and system software of the operating system is what connects the applications to the underlying hardware. Since this lesson is all about operating system structure, I would like to get your thoughts on why you may think that the structure of an operating system is important, and from that point of view I'm going to give you a quiz again. The question that I'm going to pose to you is, why you think the structure of the operating system is important, and I'm giving you several choices and you can choose as many of these choices as you'd think is appropriate in terms of your own perspective on why you think the structure of the operating system is important. The first one is protection, second one is performance, third one is flexibility, the fourth one is scalability and the fifth one is agility, and the final one is responsiveness. If you checked off all the boxes you're right on. All of these issues are important issues to worry about in the structure of an Operating System. Let's now elaborate on the goals of operating systems structure. The first goal is protection. By protection what we mean is protecting the user from the system and the system from the user and also users from one another. And also protecting an individual user from his or her own mistakes. That's what we mean by protection. And this is an important goal of operating system structuring. An operating system, of course, provides services and one of the key determinants of a good operating system structure is how good is the performance of the operating system. That is, what is the time taken to perform services on behalf of the application. I have, you've heard me say this even before in the previous lecture. A good operating system is one that provides the service that is needed by the application very quickly and gets out of the way. And that's the key determinant of operating system structure as well. Another goal, of operating system structure, and in fact, one of the goals that we will be focusing a lot on in this course module, is flexibility. Sometimes also called extensibility, meaning that a service that is provided by the operating system is not one size fits all, but the service is something that is adaptable to the requirements of the application. Another important goal of structuring an operating system is to ensure that the performance of the operating system goes up as you add more hardware resources to the system. This is sort of an intuitive understanding, but you want to make sure that the operating system developers on this intuitive understanding that when you increase the hardware resources, the performance also goes up, and that's what is meant by scalability. It turns out that both the needs of the application may change over the lifetime of an application and also the resources that are available for the operating system to manage and give to the application may change over time. Agility of the operating system refers to how quickly the operating system adapts itself to changes either in the application needs or the resource availability from the underlying hardware. Another worthwhile goal of operating system structure would be responsiveness. That is, how quickly the operating system reacts to external events, and this is particularly important for applications that are interactive in nature. Imagine you are playing a video game. In that case, what you want to see is when you do something like clicking the mouse to shoot at a target, you want to see action immediately on the screen. So that is responsiveness, how quickly the operating system is reacting to external events. Are all the goals simeltaneously achieviable in a given operating system? At first glance it would seem that some of the goals conflict with one another. For example, it might seem that to achieve performance, we may have to sacrifice protection and/or flexibility. Let's explore how researchers have used their ingenuity to have the cake and eat it too. You're probably wondering how the commercial operating systems that you and I use on an every day basis Meet many of these goals that I identified. The short answer to that question is they don't meet all the goals. We will return to the influence of research leading up to the current state of the art in operating system structure towards the end of this course module. Now let's talk about different approaches to operating system structuring. The first structure that I will introduce to you is what we will call as a monolithic structure. You have the hardware at the bottom which is managed by the operating system and hardware includes, of course, the CPU, memory, peripheral devices such as the network and storage and so on. And there are applications at the top. And each of these applications is in its own hardware address space. What that means is that every application is protected from one another because the hardware ensures that the address space occupied by one application is different from the other applications and that is the first level of protection that you get between the applications themselves. And all the services that applications expect from the operating system are contained in this blob and that might include file system and network access, scheduling these applications on the available CPU, virtual memory management, and access to other peripheral devices. The OS itself of course is a program providing entry points for the applications for the services that are expected by the applications. And code and the data structure of the operating system is contained in its own hardware address space. What that means is that the operating system is protected from the applications and vise versa. So even if an application were to do anything in terms of misbehavior, either maliciously or unintentionally because they are in there own address spaces and the operating system is in its own hardware address space. Malfunctioning of an application does not affect the integrity of the operating system services. That is, when an application needs any system service, we switch from the hardware address space that is representing this particular application, into the hardware address space of the operating system. And execute the system code that provides the service that that is expected by the application. For example, accessing the file from the hard disk, or dynamic allocation of more memory that an application may want, or sending a message on the network. All of these things are done within the confines of the address space of the operating system itself. Note that all of the services expected of the operating system, file system, memory management, CPU scheduling, network and so on, are all contained in this one big blob. And that is the reason it's also sometimes referred to as the monolithic structure of an operating system. Some of you may remember Microsoft's first entry in the world of PCs, with their operating system called DOS, or disc operating system, and the structure of DOS looks as shown here. And at first glance, at least visually, You might think that this structure is very similar to what I showed you as a monolithic structure before. What is the difference you see in this structure? First I would like you to think about it yourself before we go any further, so here is a question for you. What is gained with the DOS-like structure that I showed you? And what is lost with the DOS-like structure that I showed you? You have noticed visually that the key difference was, the red line was replaced by a dotted line, separating the application from the operating system. And what you get out of that is performance. Access to system services are going to be like a procedure call, and what is lost in the DOS-like structure is the fact that you don't have protection. Of the operating system from the application. An errant application can corrupt the operating system. We'll elaborate on this in the next few panels. So in the DOS-like structure, the main difference from the monolithic structure that I showed you earlier is that the red line separating the application from the operating system is now replaced by a dotted line. What that means, the main difference is there is no hard separation between the address space of the application And the address space of the operating system. The good news is an application can access all the operating system services very quickly. As they would any procedures that they may execute within their own application with the same speed. At memory speeds, an application can make calls into the operating system and get system services. That's the good news. But the bad news is that there is no protection of the operating system from inerrant application. So, the integrity of the operating system can be compromised by a runaway application, either maliciously or unintentionally corrupting the data structures that are in the operating system. Now, you may wonder why DOS Chose this particular structure. Well, at least in the early days of PC, it was thought that a personal computer, as the name suggests, is a platform for a single user and, more importantly, the vision was, there will be exactly one app that is running at a time. Not even multitasking. So performance and simplicity was the key and protection was not primary concern in the DOS-like structure. And that you can get good performance comes from the simple observation that there is No hard separation between the application and the operating system. The operating system is not living in its own address space. The application and the operating system are in the same address space. And therefore, making a system call by an application is going to happen as quickly as the application would call a procedure which the application developer wrote himself or herself. But this loss of protection with the Dos-like structure is simply unacceptable for a general purpose operating system today. On the other hand, the monolithic structure gives the protection that is so important. At the same time, what it strives to do is also (no period) It reduces the potential for performance loss by consolidating all the services in one big monolithic structure. That is, even tough and application has to go from its address space, into the operating system's address space in order to get some service, It is usually the case that the operating system has several components and they have to talk to one another in order to provide the service that an application wants. Think about the file system, for instance. You make a call to the file system to open a file and the file system then may have to call the storage module in order to find out where exactly a file is residing. And it may have to contact The memory manager module to see where it wants to bring in the file that you want to open and see the content of. So in this sense there's infraction that's going to go on under the cover. Inside the operating system between components of the operating system. In order to satisfy a single service call from an application. So this monolithic structure insures that even though... We have to go from an application into the operating system's address space. Once you're inside the operating system's address space, then potential for performance loss is avoided by the consolidation of all the components that comprise the operating system. But what is lost in the monolithic structure (no period) That is the ability to customize the operating system service for different applications. This model of one size fits all, so far the system service is concerned with the monolithic structure shutsoed the opportunity for customizing the operating service for the needs of different applications. Now, you may wonder why do we need to customize the operating system service for different applications? Why not one size fits all? Why is there an issue? If you look at a couple of examples, the need for customization will become fairly obvious. For example, Interactive video games. The requirement of applications, that are providing a video game experience for the user. Or consider another application, which is computing, All the prime numbers. You can immediately see that the operating system needs for these two classes of applications are perhaps very different. On the one hand, for the little kid who is playing a video game, the key determinant of a good operating system would be responsiveness. How quickly the operating system is responding to his nifty moves when he plays his video game. On the other hand, for the programmer that wrote this prime number computing application, the key determinant of performance is going to be sustained CPU time that's available for crunching his application. Let's explore the opportunities for customization with a very specific example and the example I'm going to choose is memory management, in particular how an operating system handles page faults. Let's say that this thread executing on the processor incurs a page fault. The first thing that the operating system has to do in order to service this page fault will be to find a free page frame to host the missing page for this particular thread. And once it allocates a free page frame, then the operating system is going to initiate the disc IO to move the page from virtual memory into the free page frame that has been identified for hosting the missing page from this particular thread. Once the IO is complete and the missing page for this thread has been brought from storage into the free page frame, the operating system is going to update the page table for this thread or process, establishing the mapping between the missing virtual page and the page frame that had been allocated for hosting that missing page. Once the page table has been updated then we can resume the process so that it can continue where it left off. At the point of the page fault. One thing that I haven't told you the sequence of actions that the operating system takes, is that every so often, the operating system runs a page replacement algorithm to free up some page frames. And readiness for allocating the frame to a page fault that a process may incur. Just as an airline overbooks its seats in the hope that some passengers won't show up, the operating system is also overcommitting its available physical memory hoping that not all of the pages of a particular process which is in the memory footprint of the process will actually be needed by the process during its execution. But how does the operating system know what the memory access pattern of a particular process is going to be in making this decision? The short answer is it does not. So whatever the operating system chooses as an algorithm to implement page replacement. It may not always be the most appropriate one for some class of applications. So here is an opportunity for customization depending on the nature of the application. Knowing some details about the nature of the application, it might be possible to customize the way page replacement algorithm is handled by the operating system. Similar opportunities for customization exist in the way the operating system schedules processes on the processor and reacts to external events such as interrupts and so on. There is a need for customization and the opportunity for customization is what spurred operating systems designers to think of a structure of the operating system that would allow customization of the services and gave birth to the idea of microkernel-based operating system. As before, each of the applications is in its own hardware address space, the microkernel runs in a privileged mode of the architecture, and provides simple abstractions such as threads, address space, and inter-process communication. In other words, small number of mechanisms are supported by the microkernel. The keyword is mechanisms, there are no policies ingrained in the microkernel, only mechanisms for accessing hardware resources. The operating system services, such as virtual memory management, CPU scheduling, file system, and so on that implemented as servers on top of the microkernel. So in other words, these system services execute with a same privilege as the applications themselves. Each of the system service is in its own address space and it is protected from one another and protected from the application and the microkernel, being below this red line, is running in privileged mode, it is protected from all of the applications as well as the system services. So in other words, we no longer have that monolithic structure that we had before. Instead each operating system service is in its own hardware address space. In principle, there is no distinction between regular applications and the system services that are executing a server processes on top of the microkernel. Thus, we have very strong protection among the applications, between the applications and system services, among the system services and between application system services and the microkernel. Now, the structure what it entails is that you need the microkernel to provide inter-process communication so that the applications can request system services by contacting the servers and the servers need to talk to one another as well. And in order for them to talk to one another they need inter-process communication as well. So what have we gained by the structure? What we have gained by the structure is extensibility. Because these OS services are implemented as service processes, we can have replicated server processes with different characteristics. For instance, this application may choose to use this particular file system. Another application may choose a different file system. No longer do we have that one size fits all characterization of the monolithic kernel. And this is the biggest draw for the microkernel based design that it is easy to extend the services that are provided with the operating system to customize the services depending on the needs of the application. This all sounds good, but is there a catch? Is there a downside to the microkernel based approach? Well, there is. There is a potential for performance loss. Now consider this monolithic structure. Let's say this application makes a call to the file system to open up a file. The application slips through this red line into the hardware address space of the operating system. It runs in privileged mode because the operating system may have to do certain things that are privileged, and therefore, the hardware architecture of the CPU usually allows a privileged mode for execution of the operating system code. So now the app is now inside the operating system in a privileged mode with one instruction usually, called a trap instruction. For example, a system call results in a trap into the operating system. And once inside the operating system, all the work that needs to be done in order to satisfy the file system call that the app made. For instance, contacting the storage manager, contacting the memory manager and so on. All off that, are available as components within this blob. Which means that those components can be accessed at the speed of normal procedure call in order to handle the original request from this application. On the other hand, if you look at a microkernel based structure, the application has to make an IPC call in order to contact the service, which is, in this case, a file system service let's say. Which means that the application has to go through the microkernel, making the IPC call. Going up to the file system and the file system does the work, makes another IPC call in order to deliver the results of that system service back up to the application. So the minimum traversal so that you can see is going from the application of the microkernel, microkernel to the file system, and back into the microkernel and back up to the application. Potentially, there may be many more calls that may happen among servers that are sitting above the microkernel. Because the file system may have to contact the storage manager and the file system may have to contact the memory manager. All of those are server processes living above the microkernel and all of them require IPC for talking to one another. So what that means is that with this structure, there is a potential that we may have to switch between the address spaces of the application and many services that are living on top of the microkernel. Whereas in the case of the monolithic structure that I showed you here, there is only two address space switches, one to go from the application into the operating system, and the other to return back to the application. Whereas in a microkernel based design, there could potentially be several address space switches depending on the number of servers that need to be contacted in order to satisfy one system call that may be emanating from the application. Why do we have this potential for performance loss, with the microkernel based design? Mainly because of the border crossings. That is, going across hardware address spaces, can be quite expensive. First there is this explicit cost of switching the address space, from one hardware address space to another hardware address space. That is the explicit cost. And in addition to the explicit cost of going across address spaces, there are implicit costs involved in this border crossing. And that comes about because of change in locality. We're going from one hardware address space to a different address space, and that changes the locality of execution of the processor. And that means that me memory hierarchy, the caches in particular, close to the processor, may not have the contents that are needed for executing the code and accessing the data structures of a particular server, different from the application. A change in locality is another important determinant of performance and it can adversely effect the performance. And also, when we are going across address spaces to ensure the integrity of the system, either the micro kernel, or the server that is living on top of the microkernel. There may be a need to copy from user space to system space. And those kind of copying of data from the application's memory space into the microkernel and back out to a server process. All of those can result in affecting the performance of the operating system. Whether they're in a monolithic structure, since all the components are contained within the same address space, it is much easier for sharing data without copying. And that's one of the biggest potential sources of performance loss when we have this microkernel based structure. Now it's time for a question. Based on the discussion we've had thus far, what I would like you to think about is, coming up to the scorecard for the three different structures that I have identified. One is the monolithic operating system, the DOS-like structure, and the microkernel-based operating system. And what I would like you to do is think about the features that are important in the structure of the operating system. Extensibility, protection, performance. And try to fill out this score card as to which of these features are adequately met by each one of these structures. So for each of these operating system structures, monolithic, think about whether it meets all of the features that I've identified here. Do the same thing for Dos-like structure. Do the same thing for the microkernel based structure. A Monolithic structure definitely gives you protection, no questions about that. And we also argued that it's performant because of the fact that border crossings are minimized and loss of locality is minimized. And, and sources of copying overhead are minimized. All of that add up to giving good performance for the Monolithic structure. On the other hand It's not easily extensible. Any change to the operating system would require rebuilding the monolithic structure with the changed characteristic of the system service. So, one size fits all is what you get with a monolithic structure. A DOS-like structure is performant because there is no separation between the application and the operating system and, therefore, an application can execute system services at the same speed as it would execute a procedure call that is part of that application itself. And it's also easily extensible because you can build new versions of system service. To cater to the needs of specific applications. But on the other hand, it fails on the safety attribute. Because there is no boundary separating the kernel from the user space. A micro-kernel based operating system also. Pays attention to protection because it makes sure that the applications and the servers are in distinct hardware address spaces separated from the microkernel itself and it is also easily extensible because you can have different servers that provide the same service. But differently to cater to the needs of the application but it may have performance flaws because of the need for so many border crossing that might be needed to go between applications and the server processes. Having said that I want to give a note of caution, on the surface it may appear That the microkernel based approach may not be performant because of the potential for frequent border crossings. I'll have a surprise for you on this aspect when we discuss the L3 microkernel later on in this course module where it is shown that a microkernel. Can be made performant by careful implementation, that's the key. I'll leave you with that thought, but we'll come back to a micro kernal base design using L3 later on. Here's another way to visualize the relationship between these different attributes that I mentioned of performance, extensibility and protection or safety. A DOS-like structure that does not pay attention to safety or protection, needs the two attributes of performance and extensibility. A micro kernel based approach achieves protection and extensibility, but may have issues with respect to performance. A monolithic structure may yield good performance, has protection. But, it is not easily extensible. Now what do we want? Of course we want all three of these characteristics in an operating system structure. But can we have all of these three characteristics in an operating system? In other words, what we would like the operating system structure to be such. That, we get to this, center of the triangle that caters to all three attributes. Performance, Extensibility, and Protection. And the research ideas that we will study in this course module. Is, looking at ways to get to the center of the triangle so that all three attributes can be present in the structure of the operating system. We will resume the course module with research approaches that have been proposed than that we will cover in this course module that help us get to the middle of the triangle. So now we set the stage for discussing the spin and the exokernel approaches to achieving extensibility of the operating system without losing out on protection or performance. Both these approaches start with two premises. The first premise, is that micro-kernel based design compromises on performance due to frequent border crossings. And the second premise is that, monolithic design does not lend itself to extensibility. Because of the starting premises of spin and exokernal. Both these approaches have certain commonality in what they strive to do, although the path taken by these two approaches are very different. So, let's revisit what we are shooting for in the structure of an operating system. We want the operating system strucure to be thin. That is, like a microkernel. That is only mechanisms should be in the kernel, and no policies should be ingrained in the kernel itself. The structure should allow fine-grained access to system resources without border crossing, as much as possible. That is, it should be like the DOS-like structure. That is, it should have a structure similar to what we saw in DOS. And it should be flexible, meaning resource management should be easily morphed to suit the needs of the application without sacrificing protection and performance. So, the flexibility part of it should be similar to what we can get from microkernel based approach, but at the same time we want the protection and the performance we can get with the monolithic approach. So in other words, in a nutshell what we want in the operating structure is performance, protection, and flexibility. We'll now turn our attention to the issue of extensibility and some of the approaches that have been taken to achieve this goal. Historically I should mention that there was interest in extensibility at least as far back as 1981. With a system that was developed at CMU called the Hydra operating system. The Hydra operating system provided kernel mechanisms for resource allocation. The key word is mechanisms, not policies, just mechanisms for resource allocation in the kernel. And it had a way of providing access to resources, using a capability based approach. The notion of a capability has a special conotation in the operating system, literature. And that is, it is, an entity that can be passed from one to the other. Cannot be forged. And can verified. All of the things that you want in order to make sure that. The system integrity is not compromised, as enshrined in this abstract notion of capability. And as originally envisioned, capability was a heavyweight mechanism in terms of implementing it efficiently in an operating system. And because capability is a heavyweight mechanism. The hydra operating system, resource managers were built as coarse-grained objects, in order to reduce the border crossing overhead, because border crossing in the hydra system would mean that you have to pass capability from one object to another. And validate the capability for entering a particular object, and so on, and for that reason, hydra used coarse-grained objects to implement resource managers. That way, they can reduce the border crossing overhead. And implementing resource managers as coarse-grained objects also means that it limits the opportunities for customization and extensibility. In other word, the closer you make these object, the less opportunity you have for customizing the services. Which is exactly the strike against, monolithic kernel as well. So, while in principle, hydra had all the right ideas of providing minimal mechanisms in the kernel. And having the resource managers implement policies because the fundamental mechanism for accessing the resources was through this capability, which is a heavy weight abstract notion to implement efficiently. In practice, hydra did not fully achieve its goal of extensibility. One of the most well-known extensible operating system of the early 90s was the Mach operating system from CMU. It was microkernel-based, providing very limited mechanisms in the microkernel. And implementing all the services that you expect from an operating system as server processes that run as normal, user level processes above the kernel. And clearly, with this micro kernel based approach, Mach achieved its goal of extensibility. So it focused on extensiblity. And portability. The keyword is portability. And therein lies the rub. Performance took a backseat, because Mach was very much focused on making the operating system portable across different architectures, in addition to paying attention to extensibility. And this, unfortunately, gave a bad press for microkernel based design. Because it focused on these twin goals of portability and extensibility, allowing performance to take a backseat. And since operating systems are generally so focused on performance. This design choice in Mach of supporting portability gave microkernel-based design a bad press. But later on, when we look at L3 approach to microkernel-based design, we will revisit the right way to build a microkernel-based design. But in this lesson, let's focus on Spin Approach to Extensibility. So the key idea in spin is to co-locate a minimal kernel with its extension in the same hardware address space and avoid the border crossing between the components of. The kernel and the extensions of the kernels that are containing the specific services that the applications need. And this core location, also means that we avoid the border crossing which he said is one of the biggest potentials for losing out on performance. But, if you're going to co-locate the kernel and extensions in the same hardware address space, isn't that compromising on protection? Wasn't that the strike against the DOS-like structure that we talked about earlier? Well, the approach that Spin took was to rely on the characteristics of a strongly typed programming language, so that the compiler can enforce the modularity that we need in order to give guarantees about the protection. So, by using a strongly typed language, in the case of Spin they used modular three more on that in a minute. The Colonel is able to provide. Well defined interfaces, all of you may be quite familiar with declaring function prototypes in a hydra file and having the actual implementation of the procedures in other files. In a large software project, this is the same idea that is now taken to the design of the operating system itself. After all, operating system is also a piece of software, a complex piece of software, and why not use a strongly typed language as the basis for building the operating system. That's the idea in the spin approach. Now, what you get when you use a strongly typed language, is that you cannot cheat. For instance, in a language like C, you can type cast pointers so that a given data structure can be viewed completely differently, depending on what you need to get done. At the moment. That's not possible with a strongly typed language. Data abstractions provided by the programming language such as an object serve as containers for logical protection domains. That is, we are no longer reliant on hardware address spaces to provide the protection between different services. And the kernel. As I mentioned the kernel provides only the interfaces and these logical protection domains actually implement the functionality that is enshrined in those interface functions. And there can be several implementations of the interface functions. And that's where the flexability comes in. Applications can dynamically bind different implementations of the same interfaith functions. And that's how we get different instanciations of specific system components. Getting you the flexibility that you want in constructing an operating system. Because we have co-located the kernel and the extension in the same hardware address space, we are making the extensions as cheap as a procedure call. So in a nutshell, what we've accomplished with a Spin approach to extensibility as we are writing on the characteristics of a strongly typed programming language, that enforces strong typing and therefore allows the operating system designer to implement logical protection domains instead of relying on hardware address spaces. And consequently we're making extensions as cheap as procedure calls. Modula-3 is a strongly typed language with built-in safety and encapsulation mechanisms. It does automatic management of memory. That is, since it does automatic storage management, there are no memory leaks. Modula-3 supports a data abstraction called an object with well defined entry points. Only the entry points are known outside the object, not the implementation of the code for that entry point, or the data structures that are contained within an object. And therefore there's no cheating possible as you can do with a language like C. And modula-3 allows exposing the externally visible methods inside an object using generic interfaces. And it also supports the notion of threads that execute in the context of the object, and it allows raising exceptions, for example, when there is a memory access violation. All of the features that I mentioned here in a nutshell allows implementing system services as an object with well defined entry points. This modula-3 allows the creation of logical protection domains. What you can do from outside the object is what the entry point methods that are inside the object will let you do and no more. In other words, we are getting the safety property of a monolithic kernel without having to put system code in a separate hardware address space. So in other words the logical protection domains give you both protection and performance, the two things that we strive for. Now, what about flexibility? Well, the genetic interface mechanism allows you to have multiple instances of the same service. And a given application may be able to exploit the different instances of services that are available, that cater to the same generic interface, and that's the way you can get flexibility as well. And objects that implement specific services can be the desired granularity of the system designer. It can be fine-grained, or it can be a collection. You can think of individual hardware resources as fine-grained object. For example, a page frame and what you can do with a particular page frame. You can have interfaces that provide a certain functionality. That can be what an object is. For example a page allocation module can be on object. And it can also make a collection of interfaces into an object. For example, an entire virtual memory subsystem can be an object that is hierarchically composed of page allocation module, and within that, you may have hardware resources defined as objects as well. And all of these objects, whether it is at the course level of a collection of interfaces, or individual interface that is a component of this collection, or specific hardware resources, all of those are accessible via capabilities. Now the word capability may give you jitters, because I just now said that capabilities traditionally in the operating system parlance signifies a heavyweight mechanism. But because we are dealing with a strongly typed language, capabilities to objects can be supported as pointers. Or in other words, the programming language supported pointers can serve as capabilities to the objects. So now, with this idea access to the resources, that is entry point functions within an object that is representing a specific resource, is provided via capabilities that are simply language supported pointers. And because they are language supported pointers, these capabilities that we are talking about here, are much cheaper compared to real capabilities as was used in the hydra operating system. The question is asking you to differentiate between pointers in in Modula-3 and pointers in C. And the choices I have for you are, there are no differences between pointers in a language like Modula-3 and C, C pointers are more restricted, or Modula-3 pointers are type-specific. So these are the three choices. And one right choice. Good luck. The right answer is modula -3 pointers are type-specific. That is, pointers in modula-3 cannot be forged, there's no way to subvert the protection mechanism that is built into the language. So if I have a data structure defined In Modula-3. And if I have pointer to the data structure, the only way you can use that pointer, is as a pointer to that type of data structure. You cannot take a data structure and cast it to appear like something else, this is something that we as C programmers, maybe very used to doing, but that's not something that is possible in Modula-3. And that is what allows us to implement logical protection domains in Modula-3 Using objects and capabilities to objects as pointers supported by the programming language. There are three mechanisms in SPIN to create protection domains, and use them. The first one is, of course, the create call that allows creating a logical protection domain. And this mechanism in SPIN allows initiating an object file with the contents and export the names that are contained as entry point methods inside the object to be visible outside. That's what this create call, supported by SPIN, provides to a service creator. For example, if I'm creating a memory management service. I can write the entry point functions in my memory management service and export the names using this create mechanism that's available in SPIN. The second mechanism in SPIN is resolving names. If one protection domain wants to use the names that is there in another protection domain. The way we can accomplish that is by using this resolve primitive that's available in SPIN. Resolve is very similar to linking two separately compiled files together that a compiler does routinely. So, you may be very familiar with the compilation process where you may separately compile files and once you have done the separate compilation of the files, then you go through a link phase of the compiler where the linker resolves the names that are being used by one object file with the names that are defined in another object file. That's the same thing that the resolve mechanism of SPIN does is, it resolves the names that are being used in source, which is a logical protection domain, and the target, which is another logical protection domain. As the result of this resolve step, the source logical protection domain and the target logical prediction domain are dynamically linked or bound together. And once bound together, accessing methods that are inside this target protection domain happens at memory speeds, meaning it is as efficient as a procedure call, once this resolve step has happened. As I mention before, to reduce the proliferation of small logical protection domains you may want to combine protection domains to create an aggregate larger protection domain and SPIN provides a mechanism for that, which is the combined mechanism. Once the names in a source and target protection domain have been resolved, they can be combined to create an aggregate domain. And the aggregate logical protection domain will have entry points, which is the union of the entry points that were exported as names from the source and the target or any number of such domains that have been combined together to create an aggregate domain. So this combined primitive in SPIN is mainly useful as a software engineering management tool to combat the proliferation of many small domains. So, once again, the road map for creating services is, write your code as a Modula-3 program with well defined entry points. And using the SPIN mechanism of create, you can instantiate a service and export the names that are available in that service. And if another logical protection domain wants to use the names that are exported, it can do so by using the SPIN mechanism resolve that causes the dynamic binding of the source and target logical protection domains. And finally, the combined primitive allows aggregation of logical protection domains to create an aggregate domain, that's the union of all the entry points that are available in the component logical protection domains. This is it. This is the secret sauce in SPIN to get protection and performance while allowing flexibility. Everything hinges on the strongly-typed nature of the programming language that is being used for implementing the operating system. That is, the language allows compile time checking, and run time enforcement of the logical protection domains. That's the key to the success of this approach to providing flexibility, protection, and performance, all in one bag. So the upshot of the logical protection domain is the ability to extend SPIN to include operating system services and make that all part of the same hardware address space, so no border crossing between the services or the mechanisms provided by SPIN. So here is one example where all these system services are implemented as protection domain and using create, resolve and combine. We've created all these services as logical extensions of SPIN. Here is another extension living on top of the same hardware, concurrently with the first extension. And as you see, each of these mounds represent a completely different operating system. And each of these mounds may have their own subsystems for the same functionality. For instance, this process uses memory manager two. This process uses memory manager one. Both of them implement the same functionality. But very differently, hopefully, to cater to the needs of the applications that need those services. But they may also have common subsystems. For example, the network protocol stack, may be shared by both extensions that live on top the same hardware framework. Here is a concrete example of an extension. It's a fairly standard implementation, let's say of Unix operating system, but it is implemented as an extension on top of the spin. Here is a more fun example. A client server application that is implemented directly on top of spin as an extension. In other words, there is no operating system. A display client uses an extension interface to implement the functionality for displaying video that is going to be sent by a video server. So both of these are extensions on top of basic spin, and provide exactly the functionality that is needed for the video server application. And the bounding box here is showing spin and the extensions thereof. And similarly the bounding box here is showing spin and the extension thereof. In this case it is an entire operating system, in this case it is just the client of the video server. And in this case it is just the video server itself as an extension on top of spin. Now it's time for a question. I'm showing you three different structures here for the operating system. Here are the monolithic structure and here is the micro-kernel base structure, and here is the spin structure with extensions. And the question to you is, which of the above structures will result in the least number of border crossings? Is it the monolithic structure, is it the micro-kernel structure, or the spin structure, or either spin or monolithic structure? So, these are the four choices available to you, for you to pick one correct choice. The right answer, either SPIN or monolithic, will result in the least number of border crossings. Why? In the microkernel based structure, we're assuming that each one of these services are available as server processes. In their own hardware address space and therefore any system service that an application needs may have to go through multiple border crossings. And by border crossing we, of course, mean going across different address spaces and the incumbent loss of locality that it entails. Whereas, in the case of mono, you have only two border crossings. One to get to the monolithic kernel and the other to come out back into the application. And by construction, in SPIN also, because we are taking SPIN and extending it with the services. They're all contained in the same hardware address space. So, even though we may be going through several different protection domains in satisfying the system call emanating from an application. Those prediction domains are all logical prediction domains. It does not involve border crossing that entails change of locality and loss of performance. An operating system has to feel external events. For example, external interrupts that may come in when a process is executing. Or, the process itself may incur some exceptions such as a page fault. Or it may make system calls. All of these are events that needs to be fielded by the operating system and SPIN has to support such external events. And SPIN supports such external events using an event based communication model. Services can register what are called event handlers with the SPIN event dispatcher, and SPIN supports several types of mapping. It support a one to one mapping between an event and a handler. It also support one to many mapping between an event and the handlers. And it also supports many to one mapping, many events being mapped to the same handler. And let's look at concrete examples to illustrate the power of this mapping and how it might help in building system services. And what this picture is showing you is a typical protocol stack. And you may have several different interfaces available on your machine. And therefore, a network packet may arrive through one of several interfaces. Let's say you have an Ethernet interface and you have an ATM interface. A network packet may arrive on the Ethernet port or an ATM port. Those are events, and both of those may be IP packets, in which case there is an IP handler that needs to see the events. And so, here is an example of many events mapping to the same handler. Ethernet packet arrival is an event, ATM packet arrival is an event, different events but they map the same handler, which is the IP handler. That's an example of many-to-one mapping. The processing of the packet by this IP handler results in an IP packet arrival event. And there will be several clients of the IP layer of the protocol stack. There will be UDP transport, there will be TCP transport, there will be an ICMP layer. And they are all sitting on top of this IP network layer. So when an IP packet arrival event is figured by this handler, there are multiple clients for that, and this is an example of a one to many mapping, one event, to multiple handlers, that need to get triggered. And the SPIN dispatcher allows any number of handlers to be registered as handling a particular event type. And when that even type is reached, then all the handlers associated with that event type will get scheduled by the SPIN dispatcher. The order in which they get scheduled is not something that the designer can count on, because SPIN has freedom in the ordering which these event handlers may get scheduled when a particular event arise. But all the handlers that are associated with an event will get triggered when that event is released. That's an example of one to many mapping. And finally, here is an example of a one to one mapping. If it's an ICMP packet, the ICMP handler processes the IP packet, and if it is an ICMP packet, then it raises an event than an ICMP packet has arrived, and maybe there is only one client for that particular event, and that may be the ping program, so that's one, two on mapping. Even handlers may also be specified with guards of finer grain handler execution. For example, this handler could specify that only when IP packets arrive it should be executed. So that's a guard that the IP packet handler could specify so that even though different kinds of packets may arrive on these interfaces, this handler will only get triggered when the packet that arrived on the different interfaces than IP packet. So now, we know the toolbox provided by SPIN for building an operating system. Now, one can build each of the services we talked about early on, that an operating system should provide. Such as memory management, CPU scheduling, threads, file system, network protocol stack, and so on, from scratch, as extensions to SPIN. As we know, memory management and CPU scheduling are core services that any operating system should provide. However, an extensible operating system should not dictate how these services should be implemented. SPIN provides interface procedures for implementing these services in the operating system. Physical memory is a precious resource. We know that native operating systems such as Linux or Windows manages the physical memory that is available from the hardware. SPIN wants to allow extensions to manage physical memory allocated to them in whatever fashion they choose to. The macro-allocation of a bunch of physical memory to an extension, it's outside the scope of this discussion. But, assume that the allocation of a bunch of physical memory happens when an extension's charged up. The discussion in this tablet frame is to do with the management of the pre-allocated physical memory by the extension. The interface functions that I'm showing you here from memory management are simply header files provided by SPIN. For example, allocating a page frame. Deallocating a page frame. Reclaiming a page frame. Similarly, allocating a virtual page or deallocating a virtual page which might be used for dynamic memory allocation. Translating, has to do with creating and destroying address spaces, adding or removing mapping between virtual pages, and physical frames. All of those are interface functions, that are provided as header files by SPIN. And memory management, because of what we said earlier about overcommitment of memory. Not all of a processor's address space is going to be fitting in physical memory. So, there are event handlers, that are provided as part of the core service of SPIN for handling page fault, access fault, meaning, if you had a page that is write protected and if a process tries to write to it, that's an access violation. Or, if a process is trying to access a region of memory that it doesn't have access to, generating a bad address exception. All of these are interface functions that are defined as core services of memory management in the SPIN operation system. It is not saying anything about how these services are implemented, but it is giving you just a header file. The implementer of an extension has to write the actual code for these header functions and create a logical protection domain that corresponds to physical address management, virtual address management, translation management, and the handler functions for dealing with these different types of events. Once the logical protection domain is dynamically instantiated, it becomes an extension of SPIN, and after that there's no border crossing between a particular service that has been so instantiated and SPIN itself. And all of these functions are invoked automatically when the hardware events occur, corresponding to a page fault or access violation fault and so on. SPIN arbitrates another precious resource, which is another core service, namely the CPU. SPIN only decides at a macro level, the amount of time, that is given to a particular extension. That's done through the SPIN global scheduler. The global scheduler interacts with the application threads package. And application is a loose term here. It is the extension that is living on top of SPIN. Which may be an entire operating system or maybe just an application. For example, let's say, we are running Linux and Vista as two extensions on top of SPIN. Each maybe given a particular time slice, say of x milliseconds. How each extension uses the time that has been given to it for scheduling user-level processes running inside the operating system is entirely up to those extensions. And to support the concept of threads in the operating system and management of time, SPIN provides an abstraction called strand. The actual operating systems that extend SPIN will have the threads map to strands. So strand is the unit of scheduling that SPIN's global scheduler uses, but the semantics of the strand is entirely decided by the extension. If, for instance, I'm implementing p-threads, I will define the semantics of the strand to be the semantics of the p-thread's scheduler. And there are event handlers that help in the scheduling that needs to happen in the extensions. And the kind of events that SPIN provides for this core service of CPU scheduling are block, unblock, checkpoint, and resume. And the extensions event handlers have to give the semantic meaning of what needs to happen when these event handlers are called, because these are only interface functions. What needs to happen when this interface function is called is up to the extension. For example, a disk interrupt handler may result in an unblock event being raised for a particular strand that was waiting for the disk IO completion. Similarly, if an application were to make a system call that is a blocking system call, then the service that provides that facility to the application will raise this block event, which will result in the extension taking the appropriate action of saving the state of the currently running process. And putting it in the appropriate queues that it has, to wait for that system call completion. So in a nutshell, what SPIN provides are exactly the kind of primitives that may be needed by an extension that wants to provide the service of CPU scheduling. So SPIN only provides the interface function definitions. The semantics are all up to the extension on how exactly the scheduling is affected. And all that SPIN does is to ensure that the extension gets time on the CPU through this global scheduler that SPIN has for allocating time to different extensions that may be concurrently living on top of SPIN. There are some deep implications that may not be readily obvious. Core services are trusted services, since they provide access to hardware mechanisms. Why? The services may need to step outside the language-enforced protection model to control the hardware resources. In other words, the applications that run on top of an extension have to trust the extension. Extensions to core services affect only the applications that use that extension. That is, it is not catastrophic and does not affect other applications that do not rely on this particular extension. Having seen SPIN's approach to extensibility, now we will look at Exokernel's approach to operating system extensibility. The name, Exokernel, itself comes from the fact that the kernel exposes hardware explicitly to the operating system extensions living above it. The basic idea in Exokernel, is to decouple authorization of the hardware from its actual use. Lets say you want to do research in my lab. I may interview you, and once we're on the same page, I'll give you a key to the lab. And the resources you need in order to do work in the lab. Mm, example, laptop, servers and so on. Then I get out of the way. When you actually use the resources, that's exactly the same idea in Exokernel. Library operating system asks for a resource. Exokernel will validate the request for the resource from the library. And bind the request to the specific hardware resource. In other words, Exokernel exposes the hardware that was requested by the Library OS through creating a secure binding between the ask and the actual hardware resource. Once Exokernel has established this binding, it creates an encrypted key for the resource, and gives it to the requesting library operating system. Similar to the analogy that I gave you, of a student using my lab resources, the semantics of how the resource is going to be used by the library is entirely up to the library. Of course, within the norm of accepted use, similar to what I may impose as certain restrictions or rules of behavior that students have to observe in the lab. Same way, there are certain accepted norms for the user's resource that Exokernel may have imposed. And so long as the library operating system is staying within those norms, then the semantics of how a particular hardware resource is used is entirely up to the library operating system. Once a library operating system has asked for a resource and Exokernel has created the binding for that resource to the requesting library operating system, then, the operating system is now ready to use the resource. Now, how does it use the resource? Basically, what the library operating system will do is present the encrypted key that is received, authenticating that use of the resource for this library to the Exokernel. In other words, Exokernel will be able to validate whether the key presented to it, is the key that was presented for this particular libraries operating system. So in other words, the key cannot be forged, cannot be passed around. If I gave a key to this library operating system, that key, if it is presented to the Exokernel by this library operating system, it's a valid key. Even if it's a valid key, but it is not the operating system to which Exokernel gave the key, then that request would be denied. So with a valid key, any time the library operating system can present the key to the Exokernel, Exokernel will validate it, and then, the library operating system is free to go in using that resource for which it has this valid key. This is sort of like a doorman in an apartment building, checking when a resident comes in, whether the resident is a bona fide occupant of the residence. Once inside his apartment, what the resident does is not something that the doorman cares about. Exactly the same thing is being done by Exokernel as a doorman for using the hardware resource for which a valid key exists with a library operating system. So, establishing the secure binding is a heavy duty operation. That's where Exokernel comes in the middle of saying, well, this particular library operating system wants access to a specific resource, can I give it? And it makes that decision. Once such a secure binding has been established, the actual use of the hardware is going to be much cheaper. You are thinking, wow this sounds tedious and not performance conscious if exokernel has to validate the key every time for the library to use it. Well, it depends on what we mean by a resource. Let's look at some examples. Here is an example of a candidate resource, a TLB entry. TLB entry is going to establish a mapping between a virtual page to a physical page. That mapping of the virtual page to the physical page is done by the library. Now, once the mapping has been done by the library, it presents the mapping to the exokernel along with capability of the key, the encrypted key that it has for a particular TLB entry. Exokernel validates it and puts this mapping into the specific TLB entry of the hardware TLB. Now this is a privileged operation. Putting an entry into the hardware TLB, is a privileged operation. The library operating system cannot do it by itself, because it doesn't have the same privilege as exokernel. And therefore, once that capability in the form of the encrypted key for this TLB entry is presented to exokernel, then exokernel, on behalf of that operating system, is putting that mapping that has been established by the library operating system into the specific TLB entry of the hardware TLB. Once this entry has been put into the TLB, the process that is going to be using that virtual page, when it is running, can use this multiple times without exokernel intervention. So even though putting it into the hardware TLB require the intervention of exokernel because we are messing with hardware. Once that entry has been put in, that entry is on behalf of this library operating system. And processes of that library operating system, when they are running on the CPU, can access the TLB. And do the translation any number of times because all of that is happening under hardware control and exokernel, of course, is not in the middle of any of that. So that gives you an idea of how, even though we are seeing that in order to do certain things in the hardware, you need exokernel help for the library operating system. The normal use of a hardware resource is not going to be in any way affected by the fact that exokernel is in the middle between the hardware and the library operating systems. Here is another example of a candidate resource. Let's say that the operating system wants to install a packet filter that needs to be executed every time a network packet arrives on behalf of a library operating system. Predicates for looking at this incoming packet are loaded into the kernel by the library operating system. Now, this is a heavy-duty operation, because you're doing it with the help of exokernel. But once those predicates have been loaded into exokernel by the library operating system, on every packet arrival exokernel will automatically check it using those predicates. So those are examples of candidate resources that tell you that establishing the binding may be expensive but using the binding, once established, does not incur the intervention by exokernal and therefore it can happen at hardware speeds. Now let's talk about the mechanisms that are there in Exokernel for implementing these secure bindings. There are three methods. The first method is Hardware mechanisms. And I gave you this example of a TLB entry. Other examples of hardware mechanisms include. Getting a physical page frame from exokernel or a portion of the frame buffer that is being used by the display. These are all examples of specific hardware resources that can be requested by the library operating system and can be bound to that library operating system by exokernel. And exported to the library operating system as an encrypted key, and once the library operating system has the encrypted key for that resource, it can use that any time it wants. The second mechanism that exokernel has is software caching on behalf of each library operating system, specifically the shadow TLB. Or caching the hardware TLB in a software cache for each library operating system is to avoid the context switch penalty when exokernel switches from one library operating system to another. Basically, what will happen is that at the point of context switch, exokernel will dump the hardware TLB into a software TLB native structure that is associated with that specific library operating system. And similarly load the software TLB of the library operating system to which it is switching to into the hardware TLB. We will talk about these mechanism in much more detail shortly but at this point I wanted to mention that this is second mechanism that exists in exokernel for establishing a secure binding between a library operating system and the hardware. The third mechanism that exokernel has for establishing a secure binding on behalf of an operating system is downloading code into the kernel. This is simply to avoid border crossing by inserting specific code. That an operating system once executed on behalf of it. I gave you the example of the packet filter earlier. So that's an example of downloading according to the kernel that needs to be executed on behalf of a particular guest operating system. And if you think about it, this idea of downloading code into the kernel is very similar to the spin idea of extending the kernel with logical protection domains that I created and dynamically linked in. Similarly, in the exokernel world, a library operating system can, if allowed by exokernel. Securely download code into the kernel that will get executed under specific conditions that are laid down by the library operating system. Time for a question. In exokernal we have this mechanism of downloading code into the kernel. Spin has a similar functionality, which is to extend logical protection domains. The question to you is, which one of these two mechanisms compromises protection more. Does spin compromise protection more? Or exokernal compromise protection more by this idea of either downloading code in into the kernel as is done in exokernal or extending spin using the logic and protection domain idea? The right answer is exokernel. Now so long as SPIN's logical protection remains, are entirely following modular three language enforced, compile time checking, and run time verification, there is no violation of protection In SPIN. But we cannot say the same about Exokernel because it is arbitrary code that is being downloaded into the kernel by a library operating system and that's the reason that Exokernel may end up compromising protection more than the SPIN mechanism. But having said that, I should mention that it's not always possible to live within modular three enforced protection domains, even in SPIN. Because we've seen that even in SPIN, in order to do certain things in the hardware, SPIN may have to step outside the protection boundaries of modular three. In other words, a reality that exists with real hardware is that it's not always possible to do this within the confines of language-enforced protection domains. But if you just think in terms of the logical protection domains as defined by SPIN as modular three objects. Those have strong guarantees of protection compared to arbitrary code that we can download into Exokernel. When we discussed spin, I mentioned that memory management and CPU management are core services that any operating system has to provide; and we discussed how spin had its own way of dealing with those core services. We will do the same analysis for exokernel as to how it does memory management and CPU scheduling, first memory management. Specifically let's see how exokernel will handle a page fault incurred by a library operating system. So in this picture that I'm showing you, here is an application thread running, and maybe this application thread, belongs to a specific library operating system and so long as this application thread is doing normal memory accesses, where all its virtual addresses have been mapped to physical page frames. The thread is executing at hardware speeds on the CPU. Life is good. But life may not be always good. Because this thread my incur a page fault. And when it incurs a page fault, the page fault is first fielded by Exokernel. Exokernel knows which library operating system is currently executing on the CPU, it has no knowledge of processes within a library operating system. All it knows is that this library operating system is doing something on the CPU... And it knows that there is a page fault incurred, and it can kick it up to the library operating system through a registered handler and we will talk about how these handlers are known to Exokernel later on. Right now, I just want to give you road map of how a page fault is handled in Exokernel. Because the library operating system knows about processes, whereas Exokernel has no knowledge about that, and it services the page fault. And servicing the page fault may involve requesting Exokernel for a page frame to host the specific page that is missing. And if it does that as we detailed before, that will involve the library asking Exokernel for a page frame and Exokernel creating a binding for a page frame. And returning an encrypted key for the page frame. Assume for the moment that the library operating system has page frames already with it and all that it is doing at this point is in servicing the page fault, it establishes a mapping between the virtual piece that was missing and the page frame that contains the contents of the virtual page. Once it does that, that mapping between the virtual page and the frame that it corresponds to has to be presented to the Exokernel. So the library presents that mapping to the Exokernel along with the TLB entry, where it wants this mapping to be placed in the hardware TLB. Remember that, when the process runs the CPU is consulting the hardware TLB, to see if there's a valid mapping between the virtual page number and a physical frame, so that the CPU can go and access that page. And we got the page fault because the mapping did not exist, and the whole point of this exercise is for the library operating system to reestablish a mapping. But it cannot do that directly into the TLB, so it presents a mapping to Exokernel along with the encrypted key that represents the capability that the library operating system has to a specific TLB entry where it wants this to be placed. Exokernel will validate the encrypted key presented by the library operating system, and assuming all is good, it will go ahead and install the mapping in the hardware TLB, and this is a, a privileged operation, meaning that it can be done only in the kernel mode of the processor. That's the reason that you have the red line between the library operating system that runs at the non-privileged level, and Exokernel that runs at the privileged level to do certain operations such as, installing an entry into the TLB. And once the entry had be installed in the TLB, if the library operating system is once again scheduled on the processor and if the same process is run by the library operating system when it generates the same virtual address we're going to find a valid mapping and life will be good. Downloading code into the kernel and secure binding, how is this secure? This is a bit dicey. Here, the library operating system is given an ability to drop code into the kernel. The rationale is purely a performance one. Namely, avoid border crossings. But obviously it can be a serious security loophole. Even in spin we've noticed that a core service may have to step outside the language enforce protection mechanism. In order to control hardware resources. Bottom line is, while both SPIN and Exokernel start out with the idea of allowing extensibility, they may have to necessarily restrict who will be allowed to do such extensions. Not any arbitrary user. It has to be a trusted set of users. I mentioned software caching as a mechanism that's available in exokernel for establishing secure binding. And software TLB is one specific example of using the software caching idea. As we know, when we have a context switch one of the biggest sources of performance loss is the fact that you're losing locality for the newly scheduled process. And since the address base occupied by this library operating system and this library operating system are necessarily completely different, when we switch from one library operating system to another, we have to flush out the entire TLB. And if we do that, when we run this other library operating system, it's not going to find any of its virtual addresses in the TLB. And that's a huge source of overhead and in order to mitigate that overhead, exokernel has this mechanism called software TLB. The idea is quite simple, the software TLB is sort of a snapshot of the hardware TLB for each of the operating systems. So this software TLB, if a data structure in the exokernel that represents the mappings for operating system one. Similarly, this data structure represents the mapping for library operating system number two. So let's say currently we're running library operating system one. So the TLB entries correspond to valid mappings for library operating systems one. And let's say that exokernel decides to switch from this library operating system to this one. At that point, what exokernel will do is dump the TLB into the software TLB data structure it has on behalf of OS1. Actually not all of the TLB, but we'll get to that later on, some subset of the TLB mappings will be dumped into this data structure. And let's say that we are switching from this library operating system to this library operating system. In that case, what exokernel is going to do is, it's going to pre-load the TLB with the software TLB data set that is associated with this library operating system. Essentially, what we are accomplishing is that when the library operating system starts running on the CPU it will find some of its mappings already present in the hardware TLB. That's the idea in exokernel of having the STLB data structure associated with every library operating system to mitigate the loss of locality that happens when you do context switch, in terms of address translations. Of course, when the library operating system starts running, and it does not find a mapping for a virtual address in the TLB, at that point, exokernel is going to kick up that missing virtual page translation in the TLB as a page fault up into the library operating system and you'll get result exactly as we detailed earlier. The second core service of course is CPU scheduling. And exokernel in order to facilitate this core service, maintains a linear vector of time slots. So, time is divided into these epochs T1, T2, T3 and so on, and every time quantum has a begin and an end. And these time quantums represent the time that is allocated to the library operating systems that live on top of exokernel. And the time quantum is bound by the begin end markers for each library operating system. Each library operating system gets to mark its time quantum at startup in this linear vector of time slots. So, for instance, OS1 may say that I get this time slot, I get this time slot, maybe some of the time slot and so on. And similarly, OS2 marks its spots in the linear vector time slots. So CPU scheduling in exokernel is essentially looking at this linear vector of time slots and asking the question, in this time quantum, which is the library operating system that should be running on the processor? And there is a start time for the time quantum. There's an end time for the time quantum. And Let's say, OS1 is now running on the CPU. When the timer interrupt goes off, at this endpoint, control is transferred by exokernal to the library operating system to do any saving that it has to do of the context. And the time that is allowed for a library operating system to do the saving and restoring of context at the point of a context switch is bounded. And if an operating system misbehaves, let's say that OS1, when exokernel says it's time for you to save your context and give back the processor to me so that I can schedule it, for some of the operating systems. And let's say OS1 takes more time than is allowed to at the point of this context, which, in that case, what will happen is exokernel will remember that OS1 misbehaved And it will take time off of OS1 the next time it is scheduled. For there's a penalty associated with exceeding the time quantum. The time quantum is bound. During this time quantum, OS1 has complete control of the processor, and exokernel is not going to get in the middle of it, unless a process that is running on behalf of OS1 incurs a page fault. In that case, exokernel has to come in the middle in order to field that page fault and pass it up to the operating system. And otherwise, during this time quantum, the operating system is entirely at liberty to use a processor for running whatever processes it wants to. At the end of the time quantum, the time integer goes off. Exokernel feels it and kicks it up to the operating system and tells the operating system to clean up its act. Save any context it wants so that the CPU can be reallocated to the next library operating system. And that's where the time is bounded, as to how much time the library operating system can take in order to do that saving of the context. Notice so far that Exokernel supports no extractions. It only has mechanisms for securely giving resources to the Library Operating System. And the resources may be a space resource, memory, time resource, or specific hardware resource like area of the graphic display, and so on. Therefore, exokernel needs a way of revoking or reclaiming resources that have been allocated to a library operating system. Of course, exokernel keeps the scoreboard as to what resources have been allocated to different library operating systems. And therefore, at any point of time, it can revoke the resources that had been given to a library operating system. Sort of similar to a student working in my lab, if he or she graduates, I might say, well, it's time for you to return the key to the lab, and I have a way of revoking that resource that I may have given to a student for use in my lab. Similarly, exokernel has mechanisms for revoking resources from a library operating system. So specifically, for instance, if you think about the CPU, exokernel has no idea what the library operating system is using the CPU for. As opposed to SPIN for instance, which has an abstraction of strand that represents the user level that represents the library's notion of a thread. So exokernel has a revoke mechanism for exactly this purpose of revoking resources that have been allocated to a library operating system. And the revoke call, which is an up call into the library operating system, is going to give this library operating system a repossession vector, saying these are the resources I'm taking away from you. For instance, it could say, you know, remember I gave you these page frames? Well, I'm going to reclaim those page frames. And when it gives those repossession vector to the library operating system, it is a responsibility of the library operating system to do what it needs to do in order to clean up. In other words, the library takes corrective action commensurate with the repossession vector that has been presented by exokernel to it. For example, if the exokernel tells this library that I'm going to take away page frame number 20 and page frame number 25 from you, then the library operating system will say, oh, in that case I have to stash away the contents of those page frames into the disk. So that's the corrective action that the library operating system will have to take when it is informed by exokernel that some hardware resources that have been given to it have been taken away by exokernel. To make the life of the library operating system easier, exokernel also allows a library to seed it with autosave options for resources that exokernel wants to revoke. So, in other words, if exokernel decides to revoke, let's say, some page frames from the library operating system, the library could have seeded the exokernel ahead of time. That, any time you want to take away these page frames, dump it into the disk. And that kind of seeding allows the exokernel to do the work on behalf of the library operating system. So that, at the point of revocation, the amount of work that the library has to do, in order to do corrective action for the repossession, is minimal. Time for a question for you. In this question I'm asking you, to give a couple of examples, of how downloading code, mechanism may be used by a library operating system. I already, explained to you the concept of this mechanism. The concept of this mechanism is of course the fact that a library operating system can say well here there's a piece of code. That I want you to run on my behalf, and present it to exokernel, so that it is now part of exokernel. Sort of increasing the code base of exokernel by the downloaded code on behalf of that particular guest operating system. So this question is asking you to give a couple of examples. Of how you envision a library operating system may use this core downloading facility, provided by Exokernel. Here are a couple of examples. I'm sure that you may have thought of other examples as well. But packet filter is one thing that I mentioned already to you. And this is something that may be a critical component of performance for any operating system. And therefore, it might install a packet filter for demultiplexing incoming network packets so that exokernel can hand packets intended for a particular library operating system by running this code on behalf of the library operating system. A second example would be things that a library would like exokernel to do on its behalf, even when it is not currently scheduled. For instance, a garbage collection mechanism for an application is something that a library operating system may want to be run on behalf of it. And that's something that can be installed as a code that is downloaded into exokernel and executed on behalf of that library operating system. So these are all examples, you may have thought of other examples as well. So let's put it all together the mechanisms exokernel offers, and how library operating systems can live on top of this red line, that is the protection boundary for exokernel. And, meaningfuly execute applications that belong to it on the hardware resources, and not interfere with one another. That is, achieve both extensibility, protection, and performance. So one of the hooks for getting good performance is what I mentioned earlier, and that is called that a performance critical for library operating system is something that can downloaded securely into the exokernel, so that piece of code becomes sort of the extension of exokernel. For a particular library operating system. This may be for OS one, this maybe for OS two and so on and so forth. So, now with this setup at any point of time some application process of some library operating system is running on the CPU. Now remember that Exokernel has no idea about processes within any of these library operating systems. All it knows is existence of these library operating systems and the fact that Exokernel has been the broker in giving some hardware resources. Capabilities for some hardware resources, I should say, to specific library operating systems, and it has also been the broker for downloading some code specific to library operating systems. Into the exokernel code base itself. Let's say that this particular thread belongs to this library operating system as long as this thread is well behaved by which I mean it's not doing anything funky. Doing normal program execution. Accessing memory. For which, mapping exists in TLB and so on. Life is good. Address translation happens on every memory access entirely in the CPU, and the process is making forward progress at memory speeds without intervention from exokernel or any of the library operating system. But, there could be this discontinuities, in the execution of this process. For example, let's say that this processor thread makes a system call, like opening a file. When it does that, that is a discontinuity in the normal execution of this process. Or worse yet, it may incur a page fault, meaning that not all of the pages for this particular process is currently In the mapping available to the hardware in the TLB, and therefore, there's a page fault. Even worse, this thread could do something stupid, such as divide by 0, or something like that, that causes an exception. And lastly, the thread is not doing anything to cause the discontinuity, but there is an external interrupt that came in And that is going to cause a discontinuity to this execution of this process on the CPU. All such discontinuities essentially result in the CPU incurring a fault or a trap, and the trap is fielded by exokernel. When such discontinuities occur, exokernel has to pass the program discontinuity to the appropriate library operating system that is living on top of it. Now, exokernel knows, based on the linear vector of time slots that I mentioned earlier Which library operating system is currently running on the CPU. And therefore, it knows the right library operating system to which it has to pass the discontinuity that occurred right now for the currently executing process. To facilitate a finer-grain association between these different kinds of discontinuities. And the specific functionality in the library operating system for dealing with those discontinuities exokernel maintain state for each currently existing library operating system and we will discuss the state maintained by exokernel. On behalf of every library operating system next. To facilitate the bookkeeping needed for the different types of program discontinuities that I mentioned, exokernel maintains the PE data structure on behalf of each library operating systems. So for example, this PE data structure corresponds to this library operating system. This B data structure corresponds to this library operating system. And the PE data structure contains the entry points in the library operating system for dealing with the different kinds of program discontinuities. For example, exceptions that are thrown by the currently executing process has a specific entry point in the library operating system. And similarly, external interrupts are going to be handled by interrupt handlers that have specific entry points in the library operating system. And a process may make system calls and if it makes system calls, there is a protected entry context, which is the entry point in a library operating system for system calls that are made by the currently running process that belongs to that specific library operating system. This last entry may correspond to the addressing context that is the location of the handler for page fault service the, in the library operating system. So in a nutshell, this PE data structure that is unique for every library operating system, contains the handler entry points for different types of events that are fielded by exokernel. In this sense, the PE and the associated exokernel action of calling the appropriated handler entry point, when a particular event is triggered, is very similar to the event handler mechanism that we discussed in the spin operating system. In addition to the PE data structure, we already mentioned the software TLB, that exokernel maintains on behalf of every operating system. So this is in software TLB for OS1, software TLB for OS2 and these are what are called a guaranteed mappings. In other words, I mentioned that on a context switch exokernel when it goes from one operating system to another, it saves the current TLB in the hardware, the hardware TLB in the software TLB that is associated with this particular operating system. And here is where this entry point is important. This actually is specifying to exokernel the set of guaranteed mappings that a particular library operating system wants exokernel to maintain on its behalf, every time it is scheduled. And that's the set of TLB entries that are dumped into the software TLB data structure at the point of a context switch. Not all of the hardware TLB entries, but only the entries that have been guaranteed to be kept on behalf of a particular operating system are dumped into a software TLB data structure. And those are the ones that will be repopulated into the hardware TLB when the same operating system is scheduled on the hardware. I mentioned an external interrupt is another source of discontinuity for the currently executing process. Well, note that an external interrupt may not always be for the currently scheduled operating system. It may be for some other operating system, maybe this operating system scheduled a disk IO. And the disk IO got complete. And that interrupt that came into exokernel is on behalf of this operating system and not the process that is currently executing on the CPU for this operating system. Exokernel has to have a way of associating the interrupt that is coming in with a correct operating system which is expecting it. Downloading code into the kernel, which I mentioned as one of the mechanism that exokernel provides, allows first level interrupt handling on behalf of a library operating system. Systems research is 1% inspiration and 99% perspiration. The only way to convince someone of your ideas is to build a system and evaluate it. Now, how do you go about evaluating a system such as Spin or ExoKernel? You can qualitatively argue that you're achieving extensibility without sacrificing safety. Simply by the way the system is constructed. But you also have to show that you're not losing out on performance, due to the extensability hooks. There's an interesting dilemma when you're reading research papers. How do you make sense out of the quantitative results from a data research paper? When you research papers, remember that absolute numbers are meaningless. It is the trends that are meaningful. While doing any performance study of a new system that you're proposing, you have to identify what the competition is. Remember that spin and exokernel were done in the early to mid 90s. For spin and exokernel, the competition is a monolithic operating system and a microkernel based operating system. And the competition at that time for both Spin and exokernel was UNIX as a monolithic example, and Mach [INAUDIBLE] as a micro kernel example. And the performance questions always center around space and time. For example, how much better timewise, is the extended kernel, whether it is a spin approach or exokernel approach, compared to a microkernel-based approach in terms of performance. And since we know an extended kernel may have to incur loss of locality and border crossing overhead and so on compared to a monolithic kernel, another question that we may want to ask is, is the extended kernel at least as good as a monolithic kernel? What is the code size of implementing a standard operating system, say Unix, as a monolithic operating system, or a micro kernel based operating system or an extended kernel operating system. So that's a space question. And so, we can have time questions, as well as space questions when we propose a new way of doing any particular system component. I encourage you to read the performacne results in the papers. Key take away that you will see when you read the performance results reported by both SPIN and exokernel, is that they do much better than Mach, microkernel. For protected procedure call. That is, when you go from one protection domain to another, how well are you doing? For that, both SPIN and exokernel exceed the performance of Mach. And you also see that both SPIN and exokernel do as well for dealing with system calls as a monolithic kernel does. Both Spin and Exokernel started out with the assumption that microkernel-based operating systems structure is inherently poised for poor performance. Why did they start with such an assumption? Well they used a popular microkernel of the time called Mach which was developed at CMU as an exemplar for microkernel-based operating system structure. But Mach had portability as an important goal. If we keep performance as the primary goal, can we achieve that with a microkernal based approach? In this lesson, we will look at L3, a microkernal based operating system design that provides a contrarian viewpoint to the spin and exokernal assumption. Just to refresh your memory about micro kernel based operating system structure, the idea is micro kernel the micro kernel is providing a small number of simple abstractions. Such as address based and inter process communication and all the system services that you normally expect from an operating system. Such as the file system Memory manager, CPU scheduling, and so on are implemented as processes above microkernal. In other words, these operating system services run at the same privilege level as user-level applications. All of them in their own individual address spaces. And only the microkernel runs at a different level of privilege provided by the processor architecture. Since all the operating system services are implemented as server's processes on top of the microkernel, they may have to cooperate with one another in order to satisfy a particular user's request. And in that case, they may have to talk to one another, and in order to do that, they need IPC that is provided by the microkernel in order to accomplish what is needed by a particular system call emanating from an application. What are the potentials for performance loss when you have a microkernel based operating system design? Well, the main source of potential performance loss would occur at border crossings as we have seen before, and border crossings have both an explicit cost associated with it as well as an implicit cost associated with it. The explicit cost is the fact that from an application which is at a particular protection level, namely the user level protection level of the processor architecture, you are slipping into the microkernel, and, which is at a different privilege level. That is the explicit cost in border crossing. And in order to accomplish a particular service that an application has requested, the service has actually provided by several processes above the Micro Kernal, and therefore, there are boarder crossings involved going from the application to the Micro Kernal to the particular service you're talking about Let's say a file system service. And on top of that, a system service like file system may have to consult other services such as a storage module or a memory management module, in order to complete the requested service of the application. In which case there are going to be protected procedure calls that are going to be executed. Between services that are part of the operating system. And these protected the procedure calls, because they are going across address spaces, is going to be more expensive than simple or normal procedure calls. And typically, protected procedure calls can be as expensive as 100 x, 100 times. Normal procedure calls and this is coming up because of the fact that each of these services in a micro-kernel based design is assumed to be implemented in its own address base to protect the integrity of each of these system services. Now why are protector procedure calls that much more expensive than normal procedure calls. This is where the implicit Cost of border crossings comes in. Whether the border crossing happens when we go from the user address piece into the kernel address piece or between one hardware address piece representing a paticular system service. To the hardware address space of another system service. And that implicit cost is a fact that we're losing locality, both in terms of address translations contained in the TLB, as well as the contents of the cache that the processor uses in order to access memory. All of those add up. In making protective procedure calls or going between user space and kernel space, that much more expensive. The keyword when I describe the, the performance loss in micro kernel-based operating system structure is the potential for performance loss. What L3 micro kernel does is by proof of construction, they show that they can debunk the myths about micro kernel-based operating system structure. Now, L3 micro-kernel, being a micro-kernel, has a minimal set of abstractions. Has address space, threads, inter-process communication, and a service for providing unique IDs for subsystems that live on top of the micro-kernel. The exact details of these mechanisms provided by L3 micro-kernel is not that critical, but the important thing is the micro-kernel provides the minimal set of abstractions as we've been always talking about. Namely address space, threads, inter-process communication, and UID. And L3 argues that these are fundamental abstractions that any subsystem that lives on top of the micro-kernel, or any subsystem that you want to implement in the general purpose operating system requires these facilities. And therefore, L3 argues that micro-kernels should provide this as the minimal set of abstractions. How they actually provide it may differ from one micro-kernel to the other. But the important takeaway is that, this is the minimal set of abstractions that a micro-kernel should provide. Now let's talk about the system services. Now, as I mentioned in the previous slide, the system services have to be distinct protection domains. Protected from one another and protected from the applications that live on top of the operating system. And of course, we have this hard line between the applications and the servers providing the services and the micro-kernel. This is the structure of a micro-kernel based operating system. So what is different about every micro-kernel. The key distinction is that each of these services of the operating system. They have to be in their own protection domain, not necessarily distinct hardware address spaces. The point L3 establishes by proof of construction, is that there are ways to construct a micro-kernel based operating system providing these services. Efficiently, knowing the features of the hardware platform. In other words, L3's argument is, it's all about efficient implementation of the micro-kernel and not the principle of a micro-kernel based operating system structure. So to fully understand how L3 micro-kernel goes about systematically debunking the myths about micro-kernel based operating system structure, we have to understand, the strikes against the micro-kernel. What are the strikes against a microkernel based design? The first strike is the border crossing cost going between kernel and the user and vice versa. And this you have to do every time a user-level process makes a system call. You have to go through the kernel. That's the border crossing cost. So that is the first explicit cost that can be a strike against microkernel if this were to happen too often. The second strike against a microkernel based design is address space switches. With the assumption that each system service is living in its own hardware address space, whenever an application needs any system service, that may involve the servers living above the microkernel having to talk to one another in order to provide that particular service that was requested by the application. And I mentioned that protected procedure call is the basis for cross protection domain calls. So here's the protection domain, a file system. Here is another protection domain, the storage module. And, if the file system has to get some service out of the storage module in order to serve the original request from the application process, that communication is implemented as a protected procedure call. And going across hardware address spaces minimally involves flushing the TLB of the processor in order to make room for the TLB entries of the domain that we're entering. And we'll talk about that in a little bit more detail shortly. But the key point is, there's an explicit cost involved in going from one address space to another address space, and that is the second strike against a microkernel based design. The third strike against microkernel-based design is the cost for doing thread switches. These thread switches have to be mediated by the kernel. If the file system needs to make a protected procedure call to the storage module, in order to complete an application level request for an operating system service, that involves the file system having to be mediated through the microkernel in order to go and execute some functionality in the storage module. So that involves a thread switch, and IPC, in order to do that. So in other words, the basis for protected procedure call is thread switches and interprocess communication, which has to be mediated through the kernel. And this kernel mediation can be expensive, that is, thread switches and interprocess communication can be expensive. That's the third strike against a microkernel. So all of these three strikes that I mentioned are explicit costs associated with providing an application level service in a microkernel based operating system, because the application has to first make a request to the micro kernel. Microkernel may have to pass that request on to server processes that are living above the microkernel, and the server processes may have to talk to one another. And this protected procedure call itself has to be mediated by the microkernel via thread switching and interprocess communication. And in addition to all of these explicit costs, there could be a fourth cost, which is the implicit cost. And this is due to the memory subsystem and the loss of locality that can happen when we are going across address spaces. And when the file system makes a protected procedure call into the storage module, at that point, we are changing locality from the address space of the file system to the address space of the storage module. And therefore, the thread that starts executing inside the storage module may not find the cache warm, meaning, the contents of the cache may not be reflecting the working set of the storage module that needs to get executed right now in order to satisfy the request coming from the file server. And this loss of locality can be both for address translation in the TLB, for this new address space that we are in, as well as for data and instructions that are contained in the cache. Both of those may take a hit because of the fact that we're going from one address space to another address space. That's the implicit cost of the locality loss that would be the fourth strike against a microkernel-based design. All of these strikes against a microkernel-based design, seems pretty solid. Now how does L3 go about debunking, the myths about microkernel-based design. Let's see that, systematically. As I said, L3, by proof of construction, systematically debunks all of the myths I detailed in the previous slide. The first myth is the border crossing myth. That is, the user space to kernel space switching. Now L 3 accomplishes this boarder crossing in 123 processor cycles. And this includes the TLB misses that may be incurred because of the fact that we're going from user space to kernel space. As well as cache misses that may be incurred because we are starting to execute some code in the micro kernel. All of that put together, L3 shows, by proof of construction, that you can do that border crossing in 123 processor cycles. And L3 goes one step further, it actually cones through a back of the envelope calculation, what will be the minimum cost involved in doing this border crossing? Or, in other words, how many cycles will the instructions that are needed, in order to do the border crossing, take on a particular processor architecture? And they count the number of processor cycles, and they show that the number of processor cycles needed is about 107 and L3 microkernel accomplishes that in 123 processor cycles. Or, in other words, this is debunking the myth that border crossing is going to be expensive in microkernel based design. It's as close to what it would take minimally in the processor, even if you hand code through machine instructions, what needs to happen to go from the user space to the kernel space? So that is proof that microkernel-based design is not going to be incurring any more overhead for border crossing. It turns out that CMU's Mach operating system, on the same hardware, takes 900 cycles, as opposed to the 123 cycles taken by L3 for border crossing. Spin and exokernel that we have looked at in fairly great detail, used Mach as the basis for decrying microkernel based design, saying that, well, border crossing in microkernel based design is expensive because it takes this much time. But what L3 has shown is that it need not take this much time, it can be done in much shorter amount of time. Which is close to what the hardware can actually do. This is a good time to bring up a quiz for you. This question is asking you, why did Mach take roughly 800 or more cycles than L3 microkernel for doing this border crossing between user and kernel? Is it because L3 uses faster processor? Is it because Liedtke is smarter? Liedtke, as you know, is the author of the L3 microkernel. Is it because Mach's design priorities are different from L3 microkernels? Or is it because microkernels are slow by definition? Those are the four choices. You had to pick the right choice. The right choice is the design priorities of Mach. First of all, I already mentioned that the number that I showed you for Mach's performance for user to kernel switch, is on the same processing hardware as L3 uses, and therefore there is no cheating here, and the Liedtke may be smaller but that's not the reason either. And it is not because microkernels are slow by definition, but it is because of the fact that the design priorities of Mach was different. And in particular, Mach's design priority was not just extensibility of an operating system, but also portability. And we will talk more about how that portability design consideration comes into play. In the performance of an operating system later on. But at this point I just want you to understand that the reason why Mach took significantly more time for this border crossing compared to L3 micro kernel. Is nothing to do with the structure of the micro kernel. It is only the design priorities. The second myth concerns going across protection domains and espeiclaly if the protection domains are impmlemented as distinct hardware address spaces, then the myth is that crossing protection domains implemented using hardware address space switch is expensive. And we are now talking explicit cost for crossing protection domains implemented as distinct hardware address spaces. Not the implicit cost, we're only talking about the explicit cost of crossing protection domain. Now, let's understand what is involved in an address space switch. This is a refresher to show you what exactly is happening in terms of translating a virtual address to a physical address. You all know about the concept of a TLB and the virtual address consists of two parts; an index part and a tag. The index is used to look up the TLB and the tag that is contained in that particular entry of the TLB is matched against the tag coming from the virtual address. If they match, then we got a hit, then this particular virtual address is contained in the TLB entry and you've got the physical frame number that corresponds to the virtual page number that you started with, so that is your, very simple, TLB, matching hardware that does the address translation from VPN to PFN. Now what happens on an address space switch? That is a context switch going from one address space to another address space. On a context switch, the virtual address to physical address mapping will change for the new process that is going to be scheduled. So in other words, if this TLB contains the translations for a particular process that is currently executing on the CPU, if we do a context switch, do we have to flush the TLB because the virtual address to physical address mapping is going to be different for the new process or new thread that is going to run on the processor now? The answer to the question do we have to throw away the mappings that exist in the TLB, when we do a context switch, is, it depends. It depends on whether the TLB has a way of recognizing that the virtual to physical address translation that is contained in it, is flagged by the distinct process for which those translations have been put into the TLB. In other words, if the TLB has address space tags in addition to the tag for disambiguating one virtual address from another virtual address then you don't have to flush the TLB. So these are what are called address space tag TLBs which contain the process ID for which a particular TLB entry is present in the TLB. MIPS Architecture for instance uses address space tagged TLBs. But on the other hand, an architecture may not use an address space tagged TLB. Intel 486, and Intel Pentium are examples of architectures that do not have address space tags associated in the TLB. So, in an Intel architecture, at the point of the context switch, you have to throw away all the entries that are there in the TLB on behalf of the process. In the Intel architecture, actually the TLB is split into two parts, a user part and a kernel part. And the kernel part is common regardless of which process is running. And therefore, you don't have to flush that portion of the TLB, the kernel portion of the TLB, but the user portion of the TLB has to be flushed on a context switch because the virtual address, the physical address mapping, is going to be different for the new process that starts to run on the processor. Let's look at how address translations work. An, an architecture that supports address space tags in the TLB. Recall that the virtual addresses are being generated on behalf of a particular process, and the process has a process ID that is uniquely assigned by the operating system for that process. So when we make an entry into the TLB, what we are storing in the TLB is not only the tag that is associated with a particular virtual address, but also the PID of the process for which we are creating this entry in the TLB. So in other words, every entry in the TLB is flagged with the process ID that, that particular entry corresponds to. So how does address translation work in this address space tagged TLB? Well, similar to a normal TLB, we're going to take this virtual address and split it in two parts, the index part and the tag part. The index part is what lets us look up a particular entry in the TLB. And from the TLB we get two tags. One tag is the address space tag. This is signifying which process created this particluar entry. So what we want to do is, we want to compare the PID of the process that is currently generating this virtual address against the tag that is contained at that entry. And this matching hardware, is going to say yes or no, if it says no then we are done, then this entry does not correspond to the virtual address that we're trying to translate here. On the other hand, if it does match, this entry does correspond to this process, then we want to ask the question, whether the tag that is associated with this entry, is the same as the tag that is contained in the virtual address. So that's a second level of matching that goes on. And only if both the process ID matches and the tag matches do we have a hit in the TLB. So in other words, there is two level of matching going on. And therefore, when we do a context switch from one process to another process, there is no need to flush the TLB on the context switch, because the TLB may contain some translations on behalf of the previously executing process. And it may contain some translations that correspond to the new process that has been scheduled on it. And the hardware disambiguates these entries by doing this second level of matching of process ID against the address space stack that is contained in the TLB. But, if the memory management hardware does not support address space tag, then what do we do? And this is a case for instance, in the Intel architecture, that the TLB does not have address space tags. Liedtke, the author of the L3 microkernal, suggests tricks for exploiting the hardware and avoiding TLB flushes, even if the TLB is not address space tagged. And in particular, Liedtke's suggestion is that take advantage of whatever the architecture offers you. For example, the architecture may offer segment registers in x86 and PowerPC, both of them offer segment registers. What the segment registers do is give an opportunity for the operating system to specify the range of virtual addresses that can be legally accessed by the currently running process. What that means is, even though you have a linear address space, that is, a linear virtual address space. You can carve out that linear virtual address space among several different protection domains by using segment registers. So, here is the linear address space provided by the hardware, starts from zero to a max and that max is, of course, decided by the number of bits you have for addressing in the hardware architecture. If it's a 32 bit architecture, you have 2 to the 32 as the maximum addressing capability of that particular processor. If you have 64 bits, you have 2 to the 64 as a maximum address space that's available in that particular architecture. So that's the linear address space that is provided by the hardware. If the architecture, such as the PowerPC, offers segment registers to bound the range of virtual addresses that can be legally generated by a running process. Then use segment registers to define a protection domain. So here is one protection domain, S1, and it uses segment registers to say that this particular protection domain can generate virtual addresses starting from here to here. Any other virtual address generated by this guy is illegal, and the hardware will check that, because the segment registers are hardware-provided facility for bonding the range of legal virtual addresses that can be generated by this protection domain. Similarly, another protection domain, S2, can use the segment registers to carve out a different portion of the linear hardware address space. So, for S2, the bounds for legal, virtual addresses that can be generated by, a thread that is running, in this protect domain, starts from here. Ends here. And so on. So, in other words, we can take the hardware address space that's available. And using segment registers, provided by the hardware, you can carve out the hardware address space into these regions. And once we do that, there is no need for flushing the TLB on a context switch. Because, even before we consult the TLB to see if there is a match for a particular virtual address in the TLB, segment register will act as a first line of defense. And say that, oh, this address is kosher. This address is not kosher because it is not within the bounds of legal addresses that can be generated by this protection domain. In other words, the segment bounds that we're talking about are hardware enforced, so this works really well. If these protection domains are fairly small, meaning that the amount of space, memory space, that is needed by any given protection domain, is not the entire hardware address space. But it is a portion of the hardware address space. So we're able to carve out the available hardware address space among multiple co-resident, protection domains, in the hardware address space, using these concept of segment registers. You may ask, what if the protection domain is so large that it needs all of the hardware address space? Maybe the file system code base is so big that it needs the entire hardware address space. Cannot share it with anybody else. Similarly, the code base for the storage module is so big that it may occupy the entire hardware address space. Now, if this were the situation, that is, if the protection domains are so large that they pretty much occupy the entire hardware address space, and if the TLB does not support address space tagging, then you have to do a TLB flush on a context switch. If you go from this service to this service, you need to do a TLB flush because the memory footprint of each of these service is as big as the hardware address space that's available on the processor. Or in other words, the segments overlap and therefore you have to do a TLB flush when you do a context switch. The cost that we've been talking about so far is the explicit cost, that is, the cost that is incurred for flushing the TLB at the point of a context switch. And for small protection domains that we want to go across, we want to avoid that cost and we can do that by packing them in the linear address space provided by the hardware. But if the memory footprint of the service is so big that it occupies the hardware address space of the architecture completely, the explicit cost we're going to incur, because we have to do a TLB flush, if the TLB is not address space tagged. But that explicit cost is very insignificant compared to the implicit cost that is going to be incurred. Now what do we mean by implicit cost? We mean cache effects, that is, the loss of locality going from this service to this service, that the cache is not going to have the working set of this service, when we go from here to here. That impact is much more that the explicit cost. For example, LeKay shows that on the specific architecture in which they implemented L3, which is a Pentium architecture. It had 32 entries for kernel translations and 64 entries for user translations. Even if you want to flush the entire TLB, all the entries in the TLB, it takes 864 cycles to do that. But, the loss of locality, when a service goes from this to this, in terms of cache effects is going to be much more significant because, with a large address space you expect that you're doing more work in the subsystem. And the implicit costs are going to dominate. So the upshot of address space switching is you have to really ask the question, are we switching between small protection domains or large protection domains? If you are switching between small protection domains, then by taking advantage of whatever the hardware gives you, you can make. The switching between these small protection domains efficient by careful construction of the services, on the other hand if the switch is from one large protection domain to another large protection domain. The switching cost, the explicit cost of switching from one hardware address space, to a different hardware address space Is not that significant, because the loss of locality when you go from a large protection domain to another large protection domain, both in terms of TLB misses that are going to happen, when you start executing the new large protection domain. For translations as well as the cache effects, that the fact that your cache is now going to be warm with the data and the instructions for the new protection domain. That cost is much more significant than just the switching cost going from one large protection domain to another large protection domain. So this is the way, the address-based switching myth, is debunked by the L3 microkernel, by construction. The third myth about microkernel based design is that the thread switches and inter process communication can be very expensive. And by thread switch we mean, if you are executing one thread in a particular protection domain and going to execute another thread in a different protection domain, how much time does it take for this thread switch to be effected by the microkernel, that's the question that we're asking here. Here again, we're only talking about the explicit cost of doing the thread switching. And the explicit cost that is involved in thread switching, is saving all the volatile state of the processor, the T1 thread that is running on the processor. It's modified the registers of the CPU. All of that has to be stashed away in the thread context block before we can schedule the second thread to run on the processor. The cost that we're talking about is saving all the volatile state of the processor. And what L3 does is once again, by construction, it shows that the thread switch time in L3 microkernel is as competitive as SPIN and exokernel. So, once again by construction, L3 debunks that myth that microkernel based OL structure is going to be more expensive for thread switching compared to SPIN or exokernel or even a monolithic operating system. The next myth is regarding memory effects. And that myth concerns the assertion that the lost of locality in a micro-kernel base design, is much more than in a monolithic structure. Or the structure that is advocated in spin and exo-kernel. But before we talk about the memory effects. There's a primer on the memory hierarchy. You know that, in the process of architecture, you have the CPU, you have the TLB. And you have several levels of caches, main memory. And the virtual memory that resides on the disk. And these caches are typically physically tagged. Now, what do we mean by memory effects? What we mean by that is, if you have this hardware address space, and this of course is much bigger than the amount of space that's available in these caches. And in fact, we know that the entire hardware address space may not even be in physical memory. Because In a demand-based system, when a process is executing, the pages that it needs will be demand paged from the virtual memory that's on the disk, and brought into physical memory. And when the processor is executing instructions, then the instructions and data contained in physical memory move into the memory hierarchy close to. The CPU, so that the CPU can access the working set of the currently running thread by getting it from the cache that is closest to it. That's the hope in this memory hierarchy. What we mean by memory effects is when we context switch between protection domains, how warm are the caches? Now recall that if we have small protection domains. P1, P2, P3, P4. Let's say they are all small protection domains. In that case, led case suggestion is, don't put each of these in its own hardware address space. Pack them together in the same hardware address space and then force protection, for these processes from one another, through segment registers. So when we have small protection domains, then, the caches are going to be pretty warm. Even when you do a context switch from one process to another process. There's a good chance, that the cache hierarchy is going to contain the working set of the newly scheduled small protection domain. So in other words, the memory effects can be mitigated by carefully structuring the protection domains, in the hardware address space. So debunking the second myth with respect to address space switching, also helps in reducing the ill effects of implicit costs, associated with address space switching, because these small protection domains. Occupy only a small memory footprint, and therefore occupy only a small memory footprint in the caches as well. And therefore, when we go across small protection domains, there's a good chance that, the locality for the newly scheduled small protection domain is going to be contained in the cache hierarchy. So the caches are probably going to be warm, when we do context switches, so long as the protection domains are small. We already mentioned that if the protection domains are large, you cannot avoid that. Whether it is a monolithic kernel or. The exokernel, or the spin type of extensibility. If the memory footprint of the system service is so big, it is going to pollute the cache when we visit that particular system service. So even if we have a monolithic kernel, and the monolithic kernel has subsystems, that occupy a significant portion of the hardware address space. Even though we are not doing any context for it, the ill effects of implicit costs in the memory hierarchy is going to be felt. Because the cache is after all a physically tagged. And therefore, when we go from large protection domains, or large subsystems in the context of a monolithic kernel. You have to incur the cache pollution. So that ill effect is unavoidable for large protection domains. So the only place where a monolithic kernel can win, or an exokernel can win, or a spin can win, is in small protection domains. And a microkernel can also win for a small protection domain, by packing multiple small protection domains in the same hardware address space. So this begs the question. Why was it so bad in Mac? Recall, I mentioned that, in Mac, on the same hardware, border crossing cost 800 more cycles, compared to the border crossing in the L3 microkernel. So we can ask the question, why was it so bad in the Mac microkernel? The reason for Mach's expensive border crossing, is it's focused on portability. What that means is that, the Mach microkernel is architecture independent to allow that Mach microkernel to run on several different processor architectures. What that means is that there's going to be a code bloat in the Mach microkernel because it has to have the personality for any architecture that it may need to run on. In particular, in the Mach microkernel, there is an architecture independent part of the microkernel, an architecture specific part of the microkernel. The two together results in a significant code bloat, which means it has a large memory footprint. Which means, it has lesser locality. Which means, you're going to have more cache misses and pollution of the cache. And that is the reason why you have a longer latency for border crossing in Mach, as opposed to the theoretically smallest number of processor cycles in order to go from user space to kernel space. As I mentioned earlier, LeKay did back-of-the-envelop calculation on the same processor hardware and said that it should take about 107 cycles to go from user space to kernel space. And then he implemented L3 microkernel and showed that it took only 123 processor cycles to do that. And the reason for Mach's expensive border crossing is all due to the fact that there is code bloat, resulting in more cache misses and therefore incurring longer latency for border crossing. So in other words, Mach kernel's memory footprint is the culprit for the expensive border crossing. It is not the philosophy, or the principle, of microkernel based operating system design, because by proof of construction, L3 has shown that you can have very minimal border crossing overhead by careful construction. Another way of saying it is that portability, which was one of the main concerns in Mach design, and performance, are at loggerheads with each other. So if you focus on portability, you may sacrifice performance, and that's what we saw when we look at the difference between Mach's performance and L3's performance on the same processing hardware. So, L3 Microkernel shares to debunk the myth about Microkernel-based operating system structure. It goes beyond that, and it has a thesis for how operating systems should be structured. The first principle advocated by L3 is that the Microkernel should have minimal abstractions that includes support for address spaces, threads, interprocess communication, and generating unique IDs. Why do we need these abstractions in the microkernel? The argument is that these four abstractions that I mentioned, address space, threads, IPC, and UID, Our extractions that are needed by any subsystem that provides a functionality for end users in an operating system and therefore, the principle of optimizing the common case suggests that these abstractions should be part of any microkernel. The second thesis coming out of L3 microkernel experience is that microkernels are process specific in implementation. In other words, if you want an efficient implementation of microkernel, you have to take advantage of whatever the hardware is offering you. Which suggests that micro kernels by the very nature are non-portable if you want to focus on performance. Making a micro kernel processor independent is essentially sacrificing performance. So if you put these two principles together, what L3 is advocating is the right set of kernel abstractions and processor-specific implementation. If you do that, then you can build efficient processor-independent abstractions at the upper layers. All of the services that we associate in a monolithic kernel like a UNIX operating system such as, file system, network protocols, scheduling, memory management. They all can be implemented in a processor independent way on top of a microkernel that provides the right set of abstractions and exploits whatever the hardware gives in terms of capabilities to get an efficient processor specific implementation. That's the thesis of L3 microkernel, processor specific kernel and process the independent abstractions at higher layers of the operating system stack. Research on the structure of operating systems in the mid' 90s, as exemplified by the three research papers we studied in this course module, led to fascinating innovations in the operating system structure. It is important to note that all three systems, Spen, exokernel, and the LT microkernel were done contemporaneously. In this sense, they mutually informed each other and laid the basis for several innovations in operating system structuring. For example, many modern operating systems have internally adopted a microkernel based design. Similarly, technologies such as dynamic loading of device drivers into the kernel, have come out of the thoughts that went into extensibility of operating system services. The next lesson we're going to look at is a logical progression of the idea of extensibility, namely virtualization. Welcome back. As we mentioned at the conclusion of the previous lesson module, the drive for extensibility in operating system services led to innovations in the internal structure of operating systems and dynamic loading of operating system modules in modern operating systems. And do not stop there. In this lesson, we will see how the concept of virtualization has taken the vision of extensibility to a whole new level. Namely, allowing the simultaneous co-existence of entire operating systems on top of the same hardware platform. Strap your seatbelts and get ready for an exciting ride. Before we look deep into the nuts and bolts of virtualization, let's cover some basics. Let's start with a fun quiz to get you warmed up. You may have heard the term virtualization in different contexts. Memory systems, data centers, JVM, virtual box, IBM VM/370, Google Glass, cloud computing, dalvik, VMware workstation And even the movie Inception. What I want you to do select all the context in which you've heard the term. There is no right answer for this and in fact there are several that might fit into this. Go ahead and pick the choices that you think jogs your memory when you hear the term virtualization. You know what, if you picked all of them you're right on. We've heard the term virtualization in the context of virtual memory systems. Data centers today catering to cloud computing use virtualization technology. Java virtual machine. Virtual Box is something that you may be using in your laptop for instance. An IBM VM/370, was the mother of all virtualization in some sense, because way back in the 60s and the 70s, they pioneered this concept of virtualization. Google Glass, of course many of you may have heard the hype behind it, as well as the opportunities for marketing that this provides. Cloud computing, Dalvik Virtual Machine in the context of Android, VMware Workstation as an alternative to Virtual Box, and of course the movie, Inception, has a lot of virtualization themes in it. The words virtual and virtualization are popular these days. From virtual worlds and virtual reality to applications level virtual machines like Dalvik or Java's virtual machine. What we are concerned with here are virtual platforms. And by platform we mean an operating system running on top of some hardware. Let's say Alice has her own company, Alice Inc. And she has some cool applications to run for productivity. And let's say that they're running on a Windows machine, on some server that the company maintains. Now, if cost were not an issue, then this will be the ideal situation for Alice Inc. The hope of virtualization is that we will be able to give a company perhaps not as well endured as Alice Inc., lets's say Bala Inc., almost the same experience that Alice Inc., gets at a fraction of the cost. So, instead of a real platform that Alice Inc., has, Bala Inc., gets a virtual platform. Now, in this diagram, what I've shown you is a black box to represent the virtual platform. Because, so far as Bala Inc., is concerned, they don't really care what goes on inside this black box? All they want is to make sure that the apps that they want to run can run on top of this virtual platform. in other words, as long as the platform affords the same capabilities and the same abstractions for running the applications that Bala Inc., wants he is happy. Now, we however as aspiring operating system designers are very much interested in what goes on inside this black box. We want to know how we can give Bala Inc., the illusion of having his own platform without actually giving him one. And including all of the cost, the implementation and maintenance associated with implementing such a virtual platform. Now, if we peak inside the black box however, we find that it is not just Bala who is using the resources in the virtual platform, but there is also Piero and there is also Kim, and possibly others who are also running their own applications. Their own operating system on the same shared hardware resources. Why would we want to do this? Well, unless we are quite unlucky, the hope is that sharing hardware resources across several different user communities is going to result in making. The cost of ownership and maintenance of the shared resources, much cheaper. The fundamental intusion that makes sharing hardware resources across the diverse set of users is the fact that resource usage is typically very burst. If we were to draw. Let's take one particular resource usage, for instance, memory. If we were to draw Bala's memory usage over time, it might look like this. And maybe Piero's need for memory over time may look like this. And Kim's like this, and so on. Now, adding all of these Dynamic needs of different user communities, we may see a cumulative usage pattern that might look like this. Now, let's consider Bala's cost. If he were to buy his own server, then he would have to buy. As much as this, because that is the peak usage that he has and probably he'll want to do more than that just to be on the safe side. The virtual machine actually has a total available memory that's much more than the individual needs of any one of these guys and each of these guys. Get to share the cost of the total resources among themselves. On a big enough scale, what this would mean is that each of these guys will have potentially access to a lot more resources. Then they can individually afford to pay for at a fraction of the cost, because both the cost of acquiring the resource as well as maintaining it and upgrading it and so on is borne collectively. And that in a nutshell is the whole behind utility computing. That is promoted by data centers world wide and this is how Amazon, Webservice, Microsoft, and so on they all provide resource on a shared basis to a wide clientele. Some of you may have already seen the connection to what we have seen in the previous lecture. Yes. Virtualization is the logical extension to the idea of extensibility or specialization of services that we've seen in the previous lesson, through the spin and exokernel papers. Only now, it is applied at a much larger granularity. Namely an entire operating system. In other words virtualization is extensibility applied at the granularity of an entire operating system as opposed to individual services within an operating system. Now returning to our look inside the black box, notice how we have multiple operating systems running on the same shared hardware resources. Now how is this possible? How are the operating systems protected from one another, and who decides who gets the resource and at what time? What we need then is an operating system of operating systems. Something called a virtual machine manager or hypervisor. The terminology that you will often encounter is either VMM, which stands for virtual machine monitor or hypervisor. And these operating systems that are running on top of the shared hardware resources are often referred to as virtual machines, or VM for short. We've always been using the term VM to mean virtual memory. You have to do a, a mind shift for this lesson module, VM means virtual machine. And each of these operating systems that run on top of the shared hardware resource, I'll often refer to them as either guest operating systems or virtual machines or VM for short. So watch out. Now, I should point out that there are two types of hypervisors. The first type is what is called a native hypervisor or bare metal, meaning that the hypervisor is running on top of the bare hardware. And that's why it's called a bare-metal hypervisor or a native hypervisor. And all of the operating systems that I'm showing you inside of the black box are running on top of this hypervisor. They're called the guest operating systems because they're the guest of the hypervisor running on the shared resource. The second type of hypervisor, what is called the hosted hypervisor, the hosted ones run on top of a host operating system and allows the users to emulate the functionality of other operating systems. So the hosted hypervisor is not running on top of the bare metal, but it is running as an application process on top of the host operating system. And the guest operating systems are [UNKNOWN] clients of this hosted hypervisor. Some examples of hosted hypervisors include VMware, Workstation, and Virtual Box. Both of these terminologies you may have heard of. If you don't have access to a computer that's running Linux operating system in this course you're likely to be doing your course projects on a virtual box or a VMWare workstation that's available to run on Windows platform. For the purpose of this lesson today, however, we will be focusing on the bare metal hypervisors. These bare metal hypervisors interfere minimally with the normal operation of these guest operating systems on the shared resources in a spirit which is very similar to the extensible operating systems that we studied earlier, like the spin and exokernel and therefore the bare metal hypervisors, offer the best performance for the guest operating system on the shared resource. If you're a student of history you probably know that ideas come at some point of time and the practical use may come at a much later point of time. An excellent example of that is Boolean algebra, which was invented at the turn of the century as a pure mathematical exercise by George Boole. Now, Boolean algebra is the basis for pretty much anything and everything we do with computers. The concept of virtualization also had its beginning way back in time. It started with IBM VM 370 in the 60s and the early 70s. And the intent. And IBM VM370 was to give the illusion to every user on the computer as though the computer is theirs. That was the vision and it was also a vehicle for binary support for legacy applications that may run on older versions of IBM platforms. And then of course we had the microkernels that we have discussed in the earlier course module which surfaced in the 80s and early 90s. That in turn gave way to extensibility of operating systems in the 90s. The Stanford project SimOS in the late 90s, laid the basis for the modern resurgence of virtualization technology at the operating system level and in fact, was the basis for VMware. Even the specific ideas we're going to explore in this course module. Through Xen and VMware, are papers that date back to the early 2000s. They were proposed from the point of view of supporting application mobility, server consolidation, collocating, hosting facilities, distributed web services. These are all sort of the candidate applications for which Xen and VMware while positioning this virtualization technology. And now today, virtualization has taken off like anything. Why the big resurgence today? Well, companies want a share of everybody's pie. One of the things that has become very obvious, is the margin for device making companies in terms of profits, is very small. And everybody wants to get into providing services for end users. This is pioneered by IBM and others are following suit as well. So the attraction with virtualization technology is that companies can now provide resources with complete performance isolation and bill each individual user separately. And companies like Microsoft, Amazon, HP, you name it, everybody's in this game wanting to provide computing facilities through their data centers to a wide diversity of user communities. So that it's a win-win situation for both the users that do not want to maintain their own computing infrastructure and constantly upgrading them. And the companies like IBM that has a way of providing these resources on a rental basis, on a utility basis, to the user community. You can see the dots connecting up from the extensibility studies of the 90s like the spin and exokernel, to virtualization today, that is providing computational resources, much like we depend on utility companies to provide electricity and water. In other words, virtualization technology has made computing very much like the other utilities that we use to such as electricity and water and so on. And that's the reason why there's a huge resurgence in the user virtualization technology in all the data centers across the world. One idea for this virtualization framework is what is called full virtualization, and in full virtualization the idea is to leave the operating system pretty much untouched. So you can run the unchanged binary of the operating system on top of the hypervisor. This is called full virtualization because the operating system is completely untouched. Nothing has been changed. Not even a single line of code is modified in these operating systems in order to run on the hypervisor simultaneously. But we have to be a little bit clever to get this to work, however. Operating systems running on top of the hypervisor are run as user-level processes. They're not running at the same level of privilege as a Linux operating system that is running on bare metal. But if the operating system code is unchanged, it doesn't know that it does not have the privilege for doing certain things that it would do normally on bare metal hardware. In other words, when the operating system executes some privileged instructions, meaning they have to be, in a privileged mode or kernel mode to run on bare metal in order to execute those instructions. Those instructions will create a trap that goes into the hypervisor and the hypervisor will then emulate the intended functionality of the operating system. And this is what is called the trap and emulate strategy. Essentially, each operating system thinks it is running on bare metal. And therefore, it does exactly what it would have done on a bare-metal processor, meaning that it'll try to execute certain privileged instructions thinking it has the right privilege. But it does not have the right privilege, because it's run as a user-level process on top of the hypervisor. And therefore, when they try to do something that requires. A high level of privilege than the user level, it will result in a trap into the hypervisor, and the hypervisor will then emulate the intended functionality of the particular operating system. There are some thorny issues with this trap and emulate strategy of full virtualization, and that is. In some architectures, some privilege instructions may fail silently. What that means is, you would think that the instruction actually succeeded, but it did not. And you may never know about it. And in order to get around this problem, in fully virtualized systems, the hypervisor will resort to a binary translation strategy, meaning. It knows what are the things that might fail silently in the architecture. Look for those gotchas in each of these individual binaries of the unmodified guest operating systems. And through binary editing strategy. They will ensure that those instructions are dealt with careful, so that if those instructions fail silently, the hypervisor can catch it and take the appropriate action. And this was a problem in early instances of Intel architecture. Both Intel and AMD have since started adding virtualization support to the hardware, so that such problems don't exist any more. But in the early going, when virtualization technology was experimented with, in the late 90's and the early 2000s. This was a problem that virtualization technology had to overcome in order to make sure that you can run operating systems as unchanged binaries on a fully virtualized hypervisor. Full virtualization is the technology that is employed in the vmware system. Another approach to virtualization is to modify the source code of the guest operating system. If we can do that, not only can we avoid problematic instructions, as I mentioned earlier with full virtualization, but we can also include optimizations. For instance. Letting the guest operating system, see real hardware resources, underneath the hypervisor, access to real hardware resources and also being able to employ tricks such as page coloring. Exploiting the characteristics of the underlaying hardware. It is important to note, however that so far as the applications are concerned, nothing is changed about the operating system because the interfaces that the applications see is exactly the interfaces provided by. The operating system, if there is an application that is running on window, it sees the same API. If the application is running on top of Linux, it sees exactly the same API as it would if this Linux operating system was running on native hardware. In this sense, there's no change to the application's themselves. But, the operating system has to be modified in order to account for the fact that it is not running on bare metal, but it is running as a guest of the hypervisor. And this is why this technology is often referred to as. Para virtualization, meaning it is not fully virtualized, but a part of it is modified to account for being a guest of the hypervisor. The Zen product family uses this para virtualization approach. Now this brings up an interesting question. I said that, in order to do this paravisualization we have to modify the operating system, but how big is this modification? Someone poses a question to you. What percentage of the guest operating system code may need modification with para virtualization? I just want you to give a guesstimate as to what you think may be needed in order to make a guest operating system run on top of a hypervisor in a para virtualized environment. You think 10% of the code base has to be changed, 50% of the code base has to be changed, 30% of the code base of the native operating system has to be changed, or is it less than 2%? The right answer is that, the percentage of the guest operating system code that would need to be modified with the para virtualization technology is minuscule, less than 2%. And this is shown By proof of construction and then, what they did, was to implement multiple operating system on top of Xen, with para virtualized hyperviser. . This table is showing you the lines of code that the designers of Xen hypervisor had to change in the native operating systems. The two native operating systems that they implemented on top of Xen hypervisor are Linux and Windows XP. And you see that, in the case of Linux, for instance, the total amount of the original code base that had to be changed is just about 1%, 1.36%. And in the case of XP, it is miniscule, almost an annoyance. So in other words, even though, in para virtualization we have to modify the operating system to run on top of the hypervisor, the amount of code change that has to be done in the operating system in order to make it run on top of the hypervisor. Can be bound to a very small percentage of the total code base of the original operating system. Which is good news. So, what is the big picture with virtualization? In either of the two approaches that I mentioned, whether it is full virtualization or parel virtualization, we have to virtualize, the hardware resources and make them available safely to the operating systems that are running on top of the hypervisor. And when we talk about hardware resources, we're talking about the memory hierarchy, the CPU, and the devices that are there in the hardware platform. How to virtualize them and make them available in a transparent manner for use by the operating systems that live above the hypervisor? And how do we affect? Data and control transfer between the guest operating systems and the hyper-visor. So these are all the questions that we will be digging deeper into in this course module. That wraps up the basic introduction to virtualization technology And now it is time to roll up our sleeves and look deeper into the nuts and bolts of virtualizing the different hardware elements in the hypervisor. . We will now dig deeper into what needs to be done in the systems software stack to support virtualization of the hardware resources for use by the many operating systems living on top of the hardware simultaneously. As you've seen in the earlier course module, on operating system extensibility. The question boils down to how to be flexible while being performance conscious and safe. You'll see that a bulk of our discussion on virtualization centers around memory systems. The reason is quite simple, namely, memory hierarchy is crucial to performance. Even efficient handling of device virtualization is heavily reliant on how memory system is virtualized and made available to the operating systems running on top of the hypervisor. So let's start with techniques for virtualizing the memory management in a performance-conscious manner. I'm sure by now this picture is very familiar to you. Showing you the memory hierarchy going all the way from the CPU to the virtual memory on the disk. You have several levels of caches, of course, the TLB that holds address translations for you, virtual to physical, and you have the main memory and then the virtual memory on the disk. Now, caches are physically tagged and so you don't need to do anything special about them from the point of your virtualizing the memory hierarchy. The really thorny issue is handling virtual memory, namely the virtual address to the physical memory mapping, which is the key functionality of the memory management subsystem in any operating system. Recall that in any modern operating system, each process is in it's own protection domain. And usually, a separate hardware address space. And the operating system maintains a page table on behalf of each of these processes. The page table is the operating system's data structure that holds the mapping between the virtual page numbers and the physical pages where those virtual pages are contained in the main memory of the hardware. The physical memory of course is contiguous starting from zero to whatever is the maximum limit of the hardware capabilities are. But the virtual address pace of a given processor course is not contiguous in physical memory, but it is scattered all over the physical memory. And that's in some sense the advantage that you get with page based memory management. That a process notion of which virtual address being contiguous is not necessarily reflected in the physical mapping of those virtual pages to the physical pages in the main memory. In the virtualized set up the hypervisor sits between the guest operating system and the hardware. So the picture gets a little bit complicated. Inside each one of these operating systems of course they use a level processes and each process is in its own protection domain. What that means is that in each of these operating systems, there is a distinct page table that the operating system maintains on behalf of the processes that are running in that operating system, similarly in this case. Does the hypervisor know about the page tables maintained on behalf of the processes that are running in each one of these individual operating systems? The answer is no. Windows and Linux are simply protection domains distinct from one another so far as the hypervisor is concerned. The fact that each of Windows and Linux within them contain application of a processes is something that the hypervisor doesn't know about at all. So far as each one of these operating systems is concerned, in their native form, when they think about physical memory, they think of physical memory as contiguous. But, unfortunately, the real physical memory, we're going to call it machine memory from now on, is in the control of the hypervisor. Not in the control of any one operating system. So, the physical memory of each of these operating systems is itself an illusion. It's no longer contiguous in terms of the real machine memory that the hypervisor controls. So for instance, if you look at the physical memory that windows has, I've broken that down into 2 regions, R1 and R2. And R1 has let's say a number of pages 0 through Q, and R2 has a number of pages starting from Q plus 1, through N. So this is a physical memory. But if you look at this region R1, it's occupying a space in the machine memory, the real physical memory, which we call the machine memory, controlled by the hypervisor over here. R2 is not contiguous with respect to R1 in the machine memory. It's been put over here. And come over to this guy, Lennox, and it has its own physical memory, and we'll call that, region 1 and region 2 again. And it has a total capacity of. M + 1 physical page frames, all of which, zero through L are contiguous in the machine memory. L+1 through M are contiguous in machine memory, but, they're not contiguous with respect to the other region R1. Why would this happen? The hyperwaves is not being nasty to the operating system. After all. Even if all of the N plus 1 pages of Windows, and N plus 1 pages of Linux would be contiguous in the machine memory, they cannot all start at 0 because the physical memory, the real physical memory, which we're calling machine memory, is the only thing that is contiguous. And that has to be partitioned between these two guys, and therefore, the starting point for the physical memory, the illusion of the physical memory, so far as a particular operating system, cannot be 0. Also, the memory requirements of operating systems are dynamic, and bursting. When Windows started out initially with Q plus 1 pages, and later on it needed additional memory, then it's going to request the hypervisor and at that point, it may not be possible for the hypervisor to give another region, which is contiguous with the previous region that Windows already has. So these are the reasons why the physical memory of an operating system, it still becomes an illusion, in terms of how it is actually contained in the machine memory. Zooming back in to what is going on within a given operating system, we already know that the process address space for an application is an illusion in the sense that the virtual memory space of this process is contiguous, but in terms of physical memory, they are not contiguous. And the page table data structure that the operating system maintains, on behalf of the processes. Is the one that supports this illusion so far as this process is concerned. By mapping the virtual page number of the process, using the page table for this particular process into the physical page number, where a particular virtual page may be contained in physical memory. This is a setting in a non-virtualized operating system. So the page table serves as the broker to convert a virtual page number to a physical page number. In a virtualized setting, we have another level of indirection, that is this physical page number of the operating system has to be mapped to the machine memory or the machine page numbers. We'll call the pages in the machine memory, which is the real thing, as MPN, short for Machine Page Numbers. And this goes from zero through some max, which is the total memory capacity that you have in your hardware. This data structure, the page table, is a traditional page table, that gives a mapping between virtual page number and the physical page number. And this is a traditional page table. The mapping between the physical page number and the machine page number, that is PPN to MPN mapping, is kept in another page table, which is called shadow page table, S-PT. So now, in a virtualized setting, there's a two-step translation process to go from VPN to MPN. The page table maintained by the guest operating system is the one that translates VPN to PPN and then there is this additional page table called a shadow page table that converts the PPN to MPN. That brings up an interesting question. It's clear that the vpn to ppn mapping is in the guest operating system. Now the question is, where should this ppn to mpn mapping be kept. In a fully virtualized setting. Should it be in the guest operating system. Or should it be in the hypervisor. And simirerly. In the context of a para virtualized system, where should the mapping be between PPN and MPN? Should it be in the guest operating system or in the hypervisor? In the case of a fully virtualized hypoviser, the guest operating system has no knowledge of machine pages. It thinks that it's physical memory is contiguous because it is thinking that it is running on bare metal, nothing has been changed in the operating system to run on top of a hypervisor in a fully virtualized setting. And therefore, it is the responsibility of the hypervisor to maintain the PPN to MPN mapping. In a para virtualized setting, on the other hand, the guest operating system knows that it is not running on bare metal. It knows that there's a hypervisor in between it, and the real hardware. And it knows, therefore, that its physical memory is going to be fragmented in the machine memory. So the mapping PPN to MPN can be kept in either the guest OS or the Hypervisor. But, usually, it is kept in the guest operating system. We'll talk more about that later on. Let's understand what exactly this shadow page table is and what it is. In many architectures, for example, Intel's X86 family, the CPU uses the page table for address translation. What that means is presented with the virtual address, The CPU gets to what? It first looks up the TLB to see if there is a match for the virtual page number contained in this virtual address. If there is a match, it's a hit and it can translate this virtual address to the physical address. If it is a miss, CPU knows word in memory the page table data structure is kept by the operating system. And therefore what it does, it goes to the page table, which is in main memory, and retrieves the specific entry which will give it the translation from the virtual page number to the physical page number. And once it gets that, It'll stash it in the TLB, as well, and be able to generate the physical address that is specified by this particular virtual address. So that's the way the CPU does the translation in many architectures. So in other words, both the TLB and the page table are data structures that the architecture uses for address translation. Page table is also a data structure that is set by the operating system for enabling the processor to do this translation. So in other words, the hardware page table is really the Shadow page table in the virtualized setting, if the architecture is going to use the page table for address translation. As I mentioned in a fully virtualized setting, the guest operating system has no idea about machine pages. It thinks that the physical page number that it is generating, is the real thing. But it is not. And therefore, there is two levels of indirection, one level of indirection going from virtual page to physical page, that's an illusion. Then the hypervisor has to take this physical page and using the shadow page table, convert it into the machine page. And as I said, the shadow page table may be the real hardware page table that the CPU uses as well. So this is the data structure that is maintained by the hypervisor to translate the PPN to MPN. So, how to make this efficient? Because on every memory access of a process of the guest operating system, the virtual address has to be converted to a machine address. So in other words, we want to avoid the one level of indirection that's happening because the virtual page number has to be converted to a physical page number. By the guest operating system and then it has to be looked up in the shadow table by the hypervisor to generate the machine page number. So, we would like to make this process more efficient because it's happening on every memory access by getting rid of one level of indirection that is, this translation by the guest operating system. Remember that, the guest operating system is the one that establishes this mapping in the first place between a virtual page number and a physical page number. By creating an entry in the page table for the process that is generating this virtual address in the first place. And updating the page table, is a privileged instruction. So, when the guest operating system tries to update the page table or what it thinks is the page table, it'll result in a trap. Any time the guest operating system tries to update this page table to establish a mapping between VPN and PPN, there'll be a trap and this trap is called by the hypervisor. And what the hypervisor is going to do is, it's basically going to say, oh this particular VPN corresponds to a specific entry in the shadow page table. So the update that guest OS is making to it's page table data structure, which it thinks is the real thing. It's not the real thing. The hypervisor is updating the same mapping by saying well, this particular VPN is going to this machine page number, that's the one that we're going to put as the entry here. So as a result, what we have done is, even though there is a solution, the real translations, that is a mapping between VPN and MPN, is remembered in the shadow page table. Which may be the hardware page table if the processor is using the page table for its address translation or it could be the TLB. In either case, the hypervisor is going to install the translation from VPN to MPN into the TLB and the hardware page table. So now we basically bypassed the guess operating system's page table in order to to the translation because every time a process generates a virtual address. We are not going through the guest operating system to do the translation, but so long as the translation has already been installed in the TLB and the hardware page table. The hypervisor without the intervention of the guest operating system can translate the virtual page number of a user level process running on top of the guest operating system. Lyrically to the machine based number. Using the TLB and the hardware page table. So this is a trick to make atis translation efficient because it's extremely crucial that on every memory access, we don't go through the guest top rating system to do the address translation, it's just not acceptable. And this is the trick that is used in VMware ESX server that is implemented on top of Intel hardware. In a para virtualized setting on the other hand the operating system knows that its physical memory is not contiguous. And there for this burden of efficient mapping can be shifted into the guest operating system itself. So now the guest operating system is going to maintain. Contiguous physical memory makes it simpler in terms of all the other subsystems to do that. But it is also going to know that its notion of physical memory, is not the same as machine memory, and so it will map the discontiguous physical memory to real hardware pages. So that burden of doing the PPN to MPN mapping can be pushed into the guest operating system in a very virtualized setting. So on an architecture like Intel, where the page table is a data structure of the operating system. And it is also used by the hardware to do the address translation. The responsibility of allocating and managing the hardware page table data structure can be shifted into the guest operating system. In a fully virtualized setting, it's not possible to do that because the operating system in a fully virtualized setting is unaware of the fact that it is not running on bare metal. But in a paravirtualized setting, since it is possible to do that, it is more efficient to push this efficient mapping handling into the guest operating system. For example, in Xen, which is a paravirtualized hypervisor it provides a set of Hypercalls for the guest operating system to tell the Hypervisor about changes to the hardware page table. So, for instance, there is a call that says create page table and this allows a guest operating system to allocate and initialize a page frame that it has previously acquired a real page frame that it is previously acquired from the hypervisor as a hardware resource. It can target that physical page frame as a page table data structure. Recall that each guest operating system, would have gotten a bunch of physical memory from the hypervisor at the beginning of establishing its foot print on the hypervisor. And so it can use one of those real physical memories to host a pay stable data structure, on behalf of a new process that it launches now. So anytime a new process starts up in the guest operating system, the guest operating system will make a hypercall to xen saying. Please create a page table for me, and this is the page frame that I'm giving you to use as the page table. So, when the guest operating system has to operate this particular process which got launched, then it can make another hypercall to the hypervisor saying please switch to page table, and here is the location of the page table. The hypervisor doesn't know about all these processes. All it understands is that there's a hypercall that says change the page table from whatever it used to be to this new page table. And that essentially results in this guest operating system, switching the address space. Of the currently running process on the the bare hardware on the bare metal to P1 by this switch page table hypercall. Xen will do that appropriate thing, of setting the hardware register of the processor to point to this page table data structure. In response to this hypercall from the guest operating system. If the process P1 one were to page fault at some point of time, the page fault would be handled by the guest operating system. We'll talk about how it does that later on. But once it handles that page fault for P1 and says, oh, this particular virtual page of this process is now going to correspond to a physical frame that I own. I'm going to tell the hypervisor that the mapping in the page table, has to be set for this translation that I just established for the faulted page for this process. So there's another hypercall that's available for updating a given page table data structure. And using this the guest operating system can deal with modifications to the base table data structure. All the things that an operating system would have to do in a normal setting on bare metal. You have to be able to do in the setting where you have the hypervisor sitting between the real hardware and the guest operating system. And the three things that are required to be done in the conflicts of memory management, in a para virtualized setting, is being able to create a brand new hardware address space for a newly launched process which involves creating a page table. That's a hypercall that's available. When you do a conflict switch, you want to switch the page table. That's a hypercall that's available when you do a conflict switch in the guest operating system from P1 to P2, the guest can tell the hypervisor that the page table to use from now on is such and so. That's the way the guest can do a contact switch from one process to another. And, thirdly, since not all of the address space or the memory footprint of a process would be physical memory, if the currently running process were to incur at page four, that has to be handled by the guest operating system. In handling that, it establishes a mapping between the messy virtual page for this process and the physical frame in which the contents of the page is now contained. That mapping has been put into the page table for this particular process. Again that's something that only the hypervisor can do, because it is a privileged operation happening inside the hardware. And for that purpose, the hypervisor provides a hyper call that allows a guest operating system to update the base table data structure. So at the outset I said that handling virtual memory is a thorny issue. Doing the mapping from virtual to physical on every memory access, with all the intervention of the guest operating system is the key to good performance. And it can be done both in fully virtualized and paravirtualized setting by the tricks that we talked about just now. The next thing we are going to talk about is how can we dynamically increase the amount of physical memory, that's available to a particular operating system running on top of the hypervisor? As I mentioned, memory requirements tend to be bursty, and therefore. The hypervisor has to be able to allocate real physical memory or machine memory on demand to the requesting guest operating systems on top of it. Let's look at this picture here. Let's assume that this is the total amount of machine memory that's available to the hypervisor. And the hypervisor has divvied up the available machine memory among these two operating systems. So the house, meaning the hypervisor has no spare memory at all. It is completely divided up and given to these two guys. What if the windows operating system experiences a burst in memory usage, and therefore its Requiring more memory from the hypervizor. It may happen because of some resource hungry application that had been started up in windows, maybe a video streaming application, that is gobbling up a lot of physical memory, and therefore windows needs more memory and comes to the hypervizor asking for additional hardware resources. But unfortunately, the bank is empty. What the hyper-visor could do is recognize that. Well maybe, this operating system doesn't need all of the physical resources I allocated it. So I'm going to grab back, a portion of the physical memory that Linux has. And once I get back this portion of physical memory that I previously allocated to Linux, I can then give it to Windows to satisfy its sudden hunger for more memory. Well, this principle of robbing Peter to pay Paul. Can lead to unexpected and anomalous behavior, of applications running on the guest operating system. A standard approach of course would be to coach one of the guest operating system in this case perhaps Linux, to give up some of its physical memory voluntarily, To satsify the needs of a peer that is currently experiencing memory pressure. That's the idea behind a technique that I'm going to describe to you called ballooning. The idea is to have a special device driver installed in every guest operating system. So even if it is a fully virtualized setting, since device drivers can be installed on the fly, with the cooperation of the guest operating system. The hypervisor can install the device driver, which is called a balloon, in the operating system. And this balloon device driver is the key for managing memory pressures that maybe experienced by a virtual machine or a desktop operating system. In a virtualized setting. Let's say that the house needs more memory suddenly. And this may be a result of another guest operating system, saying that it needs more memory. So what the hypervisor would do, is It'll contact one of the guest operating system that currently is not actively using all of its memory, let's say. And talk to this balloon driver that it has installed through a private channel that exists between the hypervisor and the balloon driver. So this is something that only the hypervisor knows about because this is a special device driver that the hypervisor has installed. It knows how to get to this device driver and tell it to do something. And in this case, what the hypervisor is going to do is tell this balloon device driver to inflate. What that means is that this balloon device driver is going to make requests to the guest operating system saying I need more memory. I need more memory, I need more memory. And the guest operating system will of course, give this balloon driver the memory that it is requesting. And since the amount of physical memory that's available to the guest operating system is finite. If one process, in this case, the balloon driver, is making requests from the guest saying give me more memory. Guest has to necessarily make room for that by paging out to the disk unwanted pages from the total memory footprint of all the processes that are currently running in this guest operating system. And once it is done the swapping out of pages, and perhaps even entire processes out onto the disc. That's the way it can make room for the request that is coming from this balloon driver. Now, once the balloon driver has gotten all this extra memory out of this guest operating system. It can return those physical memories. The real physical memory, or the machine memory, back to the hypervisor. So we started with this house needing more machine memory. And the way, the house got that more machine memory is by contacting its balloon driver installed in one of the guest operating systems that is not actively using all of its resources. Asking the balloon to inflate and inflating the balloon is essentially meaning that we are acquiring more memory from the guest operating system. So, you can see visually that the footprint of the balloon goes up because of the inflation. That means, it's got a bigger memory footprint. And all of this memory footprint has extra resources that it can simply return to the hypervisor. It's not going to use it. It just wants to get it so that it can return it to the hypervisor. So that's this path here. So the opposite of what I've just described is a situation where the house needs less memory. Or, in other words, it has more memory to give away to guest operating system. So maybe it is this guest that requested more memory in the first place, And in that case, it wants the guest to get to the memory that it wanted. And the way it can do that is as follows. Once again the hypovisor through it's private channel will contact the balloon driver and tell the balloon driver to deflate the balloon. By deflate what is being instructed to the balloon driver is to contract its memory footprint. So, if it contracts its memory footprint by deflating, it is actually releasing memory into the guest operations. So, available physical memory. In the guest operating system is going to increase, because of the fact that the balloon is deflating and giving out the memory that is currently sitting on. So now the guest operating system has more memory to play with. That's the effect of the balloon deflating is that the guest operating system has more memory. Which means that it can page in from the disk, the working set of processes that are executing on this guest operating system. So that those processes can have more memory resources than they've been able to because of the balloon occupying a lot of the memory. So, this technique of ballooning assumes cooperation with the guest operating system. So that based on the need of the hour the hypervisor can work with the guest operating system implicitly. Without the guest really knowing about it because it is all happening through the balloon driver that has been installed. By the hypervisor in each of the guest operating systems. And this technique of ballooning is applicable to both fully virtualized and para-virtualized environments to ease the over commitment of memory by the hypervisor to the guests. I mean, it's sort of line airline reservations. You always Notice that in airline reservation, the airlines tend to sell more seats than what they have with the hope that someone is not going to show up. That's the same sort of thing that is being done here that there is a finite amount of physical resources available with the hypervisor. And it is doling it out to all the guest operating systems. And what it wants to do is, it wants to be able to reacquire some of those resources to satisfy the needs of another guest operating system that is experiencing a memory pressure at any point of time. Memory is a precious resource. You don't want to waste it. You want protection for the virtual machines from one another. But at the same time if there's an opportunity to share memory so that the available physical resource can be maximally utilized you want to be able to do that. So the question is can we share memory across virtual machines without affecting the integrity of these machines? Because protection is important, but without affecting the protection, can we actually enhance the utility of the available physical memory in the hyperviser? And the answer is yes, think about it. You may have one instance of Linux contained in a virtual machine, maybe hosted by me. And maybe you have the same Linux hosted in another virtual machine, virtual machine number two. And in both of our virtual machines, the memory footprint of the operating system is exactly the same. And even the applications that run on it are going to be exactly the same, in terms of the contents. Because of the same operating system, it is just that we have two different instances. And let's say that we have Firefox running on this VM, and similarly this Firefox running on this VM. This copy of Linux is going to have a page table, unique for this particular Firefox instance. Similarly, this instance of Linux is going to have a piece-table deal structure unique for this instance of Firefox and if all things are equal, in terms of versions and so on, then a particular virtual page of, the Firefox instance running here and the Firefox instance running here is going to have the same content, whether we are talking about this VM or this VM. And so there is an opportunity to make both of those page table entires to point to the same machine page. If we can do that, then we are avoiding duplication. We're not comprimising safety, but we're avoiding duplication. This is particularly true for the core pages. The core pages are immutable. So, the core pages for this Firefox instance and this Firefox instance could actually share the same page in physical memory. And if it does that you're using the resources taht much more effectively in a virtualized setting. One way of doing the sharing is by a cooperation between the virtual machines and the hypervisor. In other words the guest OS has hooks that allows the hypervisor to mark pages, copy on write, and have the PPNs point to exactly the same machine page. And this way if a page is going to be written into by the operating system, it'll result in a fault and we can make a copy of the page that is currently being shared across two different virtual machines. That is one way of doing it. That is, with the cooperation between the hypervisor, and the virtual machines. So that we can, with their connivance, share machine pages across virtual machines, but mark the entries in the individual page tables as copy on write, meaning that, so long as they reach here, perfectly fine. The minute any one of these guys wants to write into a particular page, at that point, you make a copy of it and make these two instances point to different machine pages, so that is one way of doing it. An alternative is to achieve the same effect, but completely oblivious to the guest operating system, and this is used in VMware ESX server. The idea is to use content-based sharing. And in order to support that, VM Ware has a data structure which is a hash table kept in the hypervisor. And this hash table data structure contains a content hash of the machine pages. So, for instance, this entry is saying that. For virtual machine number three. For it's physical page which is at this address 43f8 there is a machine page that hosts this physical page number of VM3 and that's contained in machine page number 123b and the content hash that is... If you hash the contents of this memory page, you get a signature. That signature is the content hash. That content hash is stored in this data structure. Now let's see how this data structure is used for doing VM-oblivious page sharing in the ESX Server. We want to know if there is some page in VM2 which is content wise the same as this page contained in VM3. In particular we want to know if this physical page number.; PPN 2868 of VM 2, which is mapped to this machine page,1096 is content-wise the same as this guy. So how do we find that out? What we do is we take the contents of this machine page 1096 Create a content hash. So that's the content hash that you're going to generate, a particular algorithm that the hypervisor is going to run to create a content hash. So we create a content hash for this page 1096, that corresponds to PPN 2868 of VM 2. Now we take this content hash and look through hypervisor's data structure to see if there is a match between this content hash that I created for this page and any page currently in the machine memory. Well we have a match. We have a match between the content hash for this page and the content hash (no period) Of the page comtained in VM 3, 43f8, which is mapped to MPN 123b. So now we've got this match. Can we know that this page and this page are exactly the same? Well, we cannot. It's only a hint at this point that this pages content hash is the same as this because this content hash for 123b was taken at some point of time. And we created this data structure to represent this as a hint frame, which has a particular content hash. Now, VM3 could have been using this page actively and modified it, and if it has modified it, then this content hash that we have in this data structure may no longer be valid. And therefore, even though we've got a match, it's only a hint, not an absolute. It's only a hint. So once we have a match, then we want to do a full comparison between these two guys. Make sure that these two guys are exactly the same, full comparison upon match. If the content hash of 1096 and 123b are exactly the same, then we can modify the PPN to MPN mapping in VM2 for the page 2868, which used to point to 1096, we can now make it, point to 123b because they both are exactly the same content. And once we have done that, then we increment the reference count to this hash table entry to 2, indicating that there are 2 different virtual machines that map to the same machine page, 123b. And, we're also going to remember that these two mappings. Between PPN 2868 to 123b, 43f8 to 123b are copy on write entries, indicating that they can share this page, so long as these 2 virtual machines are reading it. But if any one of them tries to write it, at that point For the integrity of the system, you have to make a copy of this page and change the mappings for those PPNs to go to different MPNs, that's the reason that we want to do this copy on write. And now, we can free-up page number 1096. So there is one more page frame that's available for the house, in terms of allocation. because all of these things that I mentioned just now, are fairly labor-intensive operations. So you don't want to do this when there is active usage of the system. So, scanning the pages, that is. Going through all of a virtual machine's pages to see if pages that are contained in a virtual machine may already be present in the machine memory reflecting the contents of some of the virtual machine. That kind of scanning you want to do it as a background activity of the server, when it is lightly loaded. Looking for such matches and mapping the virtual machines to share the same machine page and freeing up machine memory for allocation by the hypervisor. And the important thing that you have to notice is that, as opposed to the earlier mechanism that I mentioned where I said that with the coninements of the guest operating system, the hypervisor can get into the page table data structures inside the guest operating systems. No such thing here. It is completely done oblivious to the guest operating systems and therefore, there is no change that needs to be made to the guest operating systems. In order to do the sharing in an oblivious way. And this technique is applicable to both fully virtualized, as well as the paravirtualized environments. Because basically, all that we are saying is that, let's go through the memory contents of a virtual machine, and see if. The memory contents of the virtual machine, any particular page frames can be shared with other virtual machines, and if so, let's do that. And free up some memory. That's the idea, can be applied to both fully virtualized and para virtualized settings. Up to now, we talked about mechanisms for virtualizing the memory resource. In particular for dealing with dynamic memory pressure and sharing machine pages across VM's. A higher level issue is the policies we have to use for allocating and reclaiming memory from the domains to which we've allocated them in the first place. Ultimately, the goal of virtualization is maximizing the utilization of the resources. Memory is a precious resource. Virtualized environments may use different policies for memory allocation. One can be a pure share based policy. The idea here is, you pay less, you get less. That's the idea. So if you have a service level agreement with the data center, then the data center gives you a certain amount of resources based on, the more of dollars you put on a table. So that is a pure share based approach. The problem with a share based approach, is of course the fact that it could lead to hoarding. If a virtual machine gets a bunch of resources and it's not really using it, it's just wasting it. Now, the desired behavior is if the working-set of a virtual machine goes up, you give it more memory. If it's working-set shrinks, get back the memory so that you can give it to somebody else. So working-set-based approach would be the saner approach. But at the same time, if I paid money, I need my resources. So, one thing that can be done is sort of put these two ideas together in implementing a dynamic idle-adjusted shares approach. In other words, you're going to tax the guys that are hoarders, so you tax the idle pages more than active pages. If I've given you a bunch of resources, if you're actively using it, more power to you. But if you're hoarding it, I'm going to tax you. I'm going to take away the resources that I gave you. And you may not even notice it because you're not using it anyway. So that's the idea in this dynamic idle-adjusted shares approach. And now what is this tax? Well, we could make the tax rate 0%, that is plutocracy, meaning you paid for it, you got it, you can sit on it, I'm not going to tax you, that's one approach. Or, I could make the tax 100%, meaning that if you've got some resources and you're not using it, I'm going to take all of it away. [LAUGH] So that's the wealth redistribution. Sort of a socialistic policy, use it or lose it. And in other words, if you make the tax 100%, we are ignoring shares altogether. Now something in between is probably the best way to do it, so for instance if you use a tax rate of 50% or 75% saying if you have idle pages, then the tax rate is 50%, there's a 50% chance I'll take it away. And that's what is being done in the VMware ESX server today in terms of how to allocate memory to the domains that need it. You are going to use share based approach, but if you're not actively using it, we're going to take it away from you. And of course we'll give it back to you if you start using it, and that's the reason why you don't want to make the tax 100%, but have it somewhat smaller. So by having a tax rate that is not quite 100% but maybe 50% or 75%, we can reclaim most of the idle memory from the VM's that are not actively using it. But at the same time, since we're not taxing idle pages 100%, it allows for sudden working set increases that might be experienced by a particular domain. Suddenly, a domain starts needing more memory, at that point it may still have some reserves in terms of the idle pages that I did not take away from that particular domain. So the key point is that, you don't want to tax at 100% because this allows for sudden working set increases that may be there, in a virtual machine that happens to be idle for some time, but suddenly work picks up. As we said at the outset of this course module, virtualizing the memory subsystem in a safe and performance conscious manner is the key to the success of virtualization. What remains to be vertualized are the CPU and the devices. That's what we'll talk about next. Memory virtualization is sort of under the covers. When it comes to the CPU and the devices, the interference among the virtual machines living on top of the hypervisor becomes much more explicit. The challenge in virtualizing the CPU and the devices is giving the illusion to the guest operating systems living on top, that they own the resources. Protected and handed to them by the hypervisor, on a need basis. There are two parts to CPU virtualization, we want to give the illusion to each guest operating system, that it owns the CPU, that is; it does not even know the existence of other guests on top the same CPU If you think about it, this is not very far removed from the concerns of a time shared operating system, which has to give the illusion to each running process that that process is the only one running on the processor. The main difference in the virtual setting is that this illusion is being provided by the hypervisor at the granularity of an entire operating system. That's the first part. The second part is we want the hyperviser to field events arising due to the execution of a process that belongs to a parent guest operating system, in particular during the execution of a process on the processor. There are going to be discontinuities that occur. And those program discontinuities, have to be fielded by the hypoviser, and passed to right guest operating system. To keep things simple, let's assume that there's a single CPU. Each guest operating system is already multiplexing the processes that it is currently hosting on the CPU in a non-virtualized setting also. So each operating system. Has a ready que of processes, that can be scheduled on the CPU, but there is this hypervisor that is sitting in between a guest operating system. It's already qued and the CPU. So the first part of what the hypervisor has to do is to give an illusion of ownership of the CPU for each of the virtual machines. So that each virtual machine can schedule the processes that it currently has in it's very queue on the CPU. So, if you look at it from the hypervisor's point of view, it has to have a precise way of accounting the time that a particular VM uses on the CPU. From the point of view of billing the different customers, that is the key thing that hypervisor is worried about that it gave the CPU for a certain amount of time to this VM, certain amount of time to this VM and so on. Now for the time that has been allocated for instance this virtual machine. How this virtual machine is using the CPU time, what processes is scheduling on the CPU? The hypervisor doesn't care about that. So commensurate with borderware is a scheduling policy enforced in a particular guest operating system. That guest operating system. Is free to schedule its pool of processes on the CPU for the time that has been given to it by the hypervisor. Now, similar to the policy that we discussed for memory allocation. One straightforward way to share the CPU among the guest operating systems, is to give a share. Of the CPU. A proportional share of the CPU for each guest operating system, demonstrates with the service agreement, that this virtual machine has with the hypervisor. This is called a proportional share scheduler and it is used in the VM-ware ESX server. Another approach is to use a fair share scheduler. Which is going to give a fair share of the CPU for each of the guest operating systems. An equal share to each of the guest operating systems. Running on top of the hypervisor. Most of these strategies, proportion, share scheduler, or a fair share scheduler are straight-forward conceptual mechanisms. And you can learn more about them from the assigned readings for this course. In either of these cases, the hypervisor has to account for the time used on the CPU on behalf of a different guest during the allocated time for a particular guest. And this can happen for instance if there is an external interrupt that is feeded by the CPU that is intended for this VM while a process that belongs to this VM is going to be executing on the CPU. So this is accounting that the hypervisor would keep to make sure that. Any time that was stolen away from a particular VM in order to service an external interrupt that did not belong to this VM, it'll get rewarded for later on by the accounting procedure in the Hypervisor. We have discussed this issue already in the concept of operating system extensibility, specifically when we discussed the [UNKNOWN] approach to extensibility. So, it's not new problem. It is just that this problem occurs in the hypervisor setting also. The second part of CPU virtualization, which is common to both full and para-virtualized environments, is being able to deliver events to the parent guest operating system. And these events are events that are generated by a process that belongs to the guest operating system, currently executing On the CPU. Let's see what is happening to this process, when it is executing on the CPU. Once this process has been scheduled on the CPU, during its normal program execution, everything should be happening on hardware speech, what is that mean? Well. The process is going to generate virtual addresses that has to be translated to machine page addresses. And we talked at length about how the hypervisor can ensure that the virtual address translation to the machine page address. Can be done at hardware speeds by clever tricks in the hypervisor and or the guest operating system. In a fully virtualized environment, the hypervisor is responsible for ensuring that the virtual address gets translated directly to the machine address. With all the intervention of the guest operating system. And para-virtualized environment once again with cooperation between the guest and the hypervisor, we can ensure that the page table that is used for translating virtual address to physical addresses is something that had been installed. On behalf of the guest operating system, by the hypervisor so that the translation can happen at hardware speeds. And is the most crucial part of ensuring good performance for the currently executing, process in a virtualized environment. All address translations. I dealt with. That's why I kept harping on the fact that virtual memory, managing the virtual memory, is the thorny issue in a virtualized setting. So we know that this is in good hands, we know how to handle that. Let's look at other things that can happen to this process during the course of its execution. One thing that this process may do. Is execute a system call. For instance, it might want to open a file. And that's really a call into the guest operating system. That's something that this process may do. Another thing that can happen to this process is that it can incur a page fault. Not all of. The virtual address piece of the process is going to be in the machine memory. And therefore if some virtual page cannot be translated, then it's going to result in a page fault. Or the process may throw an exception. For instance, it might do something silly like divide by zero which can result in an exception And lastly, though no fault of this process, there could be an external interrupt when this process is executing. So these are the kind of discontinuities, these are called program discontinuities, that is, affecting the normal execution of this process. And the first three things that I mentioned, syscall, page fault, and exceptions, are due to this process in some shape or form. But the fourth one, the external interrupt is something that is happening asynchronously and unbeknownst to what this process intended to do while running on the processor. And all of these continuities have to be passed up, by the hypervisor to the guest operating system. So the common issue. To both full and para virtualized environment, is that all such program disk continuities for the currently running process have to be passed up to the parent guest operating system by the hypervisor. Whether it is a para virtualized or a fully virtualized operating system. Nothing special needs to be done in the guest operating system for fielding these events from the hypervisor. because all of these events are going to be packaged as software interrupts by the hypervisor and delivered to the guest operating system. Any operating system knows how to handle interrupts. So all the hypervisor has to do. Is to package these events as software interrupts, and pass it up to the guest operating system. There are some quirks of the architecture that may get in the way and the hypervisor may have to deal with that. Recall that syscalls and page faults and exceptions. Or all things that need to be handled by the guest operating system. So it probably has entry points in it, for leading with all these kinds of program list continuities. Now some of the things that the guest operating system may have to do to deal with these. Program discontinuities may require the guest operating system to have privileged access to the CPU. That is, certain instructions of the processor can only be executed in the kernel mode, or the privileged mode. But recall that the guest operating system itself Is not running in the privileged mode. It is above the red line, which means it has no more privilege than a normal user-level process. This is a problem, especially in a fully virtualized environment. Because the fully virtualized environment, the guest operating system has no knowledge that it does not have the privileges. So when it tries to execute some instruction, that can be executed only in publish mode, the expectation is that. It'll trap, get in the hypervisor, and the hypervisor will then do the work that the fully virtualized guest operating system was intending to do. But here is where the quirks of the architecture come into play. Because unfortunately, some privileged instructions fail silently in some architectures when they're executed at the user level. And this is a problem for the fully virtualized environment. Where the binary is unchanged. In the paravirtualized environment, because we know that the paravirtualized guest is not running on bare metal, we know that it does not have the same privilege as the hypervisor. And therefore, we can collaboratively make sure that anything that the guest has to do in privileged mode. It can take the help of the hypervisor to do it. But in a fully virtualized setting, the guest has no knowledge. So the only mechanism that will save the day, is if the guest tries to execute a privileged instruction, it'll trap into the hypervisor, and the hypervisor can do the necessary thing. However, because some instructions when executed at the user level. Even though they are privileged instructions. They don't trap, but fail silently and that can be a problem. This happens in older versions of Access Architecture for privilege instructions, executed in user mode. Therefore, the hypervisor has to be intimately aware of the quirks of the hardware, and ensure that there is workaround such quirks. Specifically, in a fully virtualized setting, the hypervisor has to look at the unchanged binary of the operating system, and look for Places where these quarks may surface and do binary rewriting in order to catch those instructions when and if they're executed on the CPU. Having said that, I should mention that. Newer versions of Intel's architecture and AMD architecture for the x86 constructions have included virtualization support so that these kinds of problems don't occur. As a result of servicing the events that are delivered with a hypervisor, the guest may have to do certain things. Which may result in communication back to the hypervisor. In the case of a fully virtualized environment, communication from the guest to the hypervisor is always implicit via traps. For example, as a result of page four servicing, the guest may try to install a new entry into the page table. When it tries to do that, that's a trap that will come in to the hypervisor and the hypervisor will take the appropriate action. In a pair of a virtualized environment the communication from the guest to the hypervisor is explicit, so that API's that the hypervisor supports for the guest OS to communicate back to the hypervisor. What maybe reasons for that? Well. We talked about memory management done by the guest operating system in a pair of virtualized environment such as [UNKNOWN] where the guest operating system may have to tell the hypervisor, here is a page table entry. Please install it in the page table for me. So that is the kind of communication that may have to come back from the guest operating system down to the hypevisor. That completes discussion of issues that the hypervisor has to deal with for virtualizing the CPU. The next issue is virtualizing devices. Here again, we want to give the illusion to the guest that they own the I/O devices. In the case of full virtualization, the operating system thinks that it owns all the devices already. And the way devices are virtualized, is the familiar trap and emulate technique. That is for the devices that the operating system thinks it owns when it tries to make any access. To those devices, it's going to result in a trap into the hyperviser and the hyperviser will emulate the functionality that the Operating System intends for that particular device. In this sense, there is not much scope for innovation in the way devices are virtualized in a fully virtualized environment. A lots of details of course, that the hyperviser has to worry about, once the guest. Trans into the hypervisor to ensure the legality of the I/O operation and also whether the guest is allowed to make those I/O operations and so on, but nothing fundamental conceptually. Is there in terms of device virtualization in a fully virtualized environment. Para virtualized setting is much more interesting. The IO devices seen by the guest operating system are exactly the ones that are available to the hypervisor. That is, the set of hardware devices That are available in the platform are exactly the one that the para-virtualized guest operating system is going to be able to manipulate. This gives an opportunity for innovating the interaction between the guest operating system and the hypervisor, in particular ways in which we can make Device virtualization more efficient when we have this para-virtualized environment. So for one thing, it is possible for the hypervisor to come up with clean and simple device abstractions that can be used by the para-virtualized operating system. Further, through APIs, it becomes possible For the hypervisor to expose shared buffers to the guest operating system. So that efficiently data can be passed between the guest operating system and the hypervisor and to the devices without incurring the overhead of copying multiple times data from one address space into another. And similarly, there can be innovations in the way event delivery happens between the hypervisor and the guest operating system. So, in order to do device virtualization, we have to worry about two things. One is how to transfer control back and forth between the hypervisor and the guest. Because, devices being hardware entities They need manipulation by the hypervisor in a privileged state. And there is a data transfer that needs to be done because the hypervisor is in a different protection domain compared to the guest operating system. So these are two things that one has to worry about in device virtualization and we'll see how both control transfer. And data transfer at accomplished in both the fully virtualized and the para virtualized settings. So control transfer in a fully virtualized setting happens implicitly from the guest to the hypervisor. How? When the guest operating system executes any privileged instruction. Because it thinks it can do it, it'll result in a trap and hypervisor will catch it. And then do the appropriate thing. That's how control is transferred from the guest to the hypervisor implicitly. And in the other direction, control transfer happens as we already mentioned, via software interrupts or events, from the hypervisor to the guest. In a para virtualized setting, the control transfer. Happen explicitly via hypercalls from the guest into the hypervisor. I gave the you the example of page table updates that the guest operating system may want to communicate to the hypervisor through the API calls. When it executes the API call corresponding to that, that results in control transfer from the guest into the hypervisor. And similar to the full virtualization case In para virtualization, in the other direction, that is, going from the hypervisor to the guest, it is done through software interrupts. So that's how control transfer is handled in both the fully virtualized and paravirtualized environments. The additional facility that you have in a paravirtualized environment Is the fact that, the guest has control, via hypercalls on when event notifications need to be delivered, in the case of full virtualization, since the operating system is unaware of the existence of the hypervisor, events are going to be delivered as and when they occur, by the hypervisor to the guest. But in a para virtualization [INAUDIBLE]. The guest, via hypercalls, can indicate to the hypervisor that leave me alone don't send me any event notifications now or it can say now is a good time to send me event notifications. So that level of control exists in a paravirtualized environment, which doesn't exist in a full virtualized environment. So this is sort of similar to an operating system disabling interrupts. That's exactly the same facility that's available in a para virtualized environment at the granularity of the operating system. The operating system can say that I don't want any event notifications. I'll come ask you when I need some event notifications. How about data transfer. Well once again, when you think about full virtualization, data transfer is implicit. Any data movement that has to happen between they hypovisor and the fully virtualized outputting system happens implicitly. In a para virtual setting, for example in xen, there's an opportunity again, to innovate because you can be explicit about the data movement from the guest operating system into the hypervisor, and vice-versa. There are two aspects to resource management and accountability ,when it comes to data transfer. The first is, the CPU time. So when ,an interrupt comes in, for instance, from a device, hypervisor has to demultiplex the data That is coming from the device to the domains very quickly upon an interrupt. And that is CPU time involved, in making such copies. And, the hypervisor has to account for the computation time from managing the buffers on behalf of the virtualized operating system above it. The CPU time accountability is crucial from the point of your billing and data centers, and therefore hypervisors pay a lot of attention to, how CPU time is accounted for and charged to the appropriate virtual machines. The second aspect of a data transfer is how the memory buffers are managed. There is a space issue. The first is a time issue. CPU time issue. The second is a space issue. Meaning, how are the memory buffers allocated? And how are they managed, either by the guest operating system or by the hypovisor? As I said in the context, the full virtualization that is available to scope for innovation. But in the context of paravirtualization, there's a lot of scope for innovation in the way memory buffers are handled. Between the guest operating system and the hypervisor. Specifically, looking at Xen as an instance of para virtualization, let's look at the opportunities for innovation in the guest OS to Xen communication. Xen provides asynchronous IO rings, which is basically a data structure that is shared Between the guest and the Xen for communication. Any number of these, data structures, that is any number of these I/O rings can be allocated for handling all the device I/O needs of a particular guest domain. The I/O ring itself. Is just a set of descriptors. What you see here, they're all descriptors that are available in this data structure. And it's a ring data structure, that's why it's called IO ring. And we'll talk about how it is used in a minute. The idea is, requests from the guest can be placed in this IO ring by populating these descriptors. Every descriptor is a unique I/O request coming from the guest operating system. So every request, is going to have a unique ID associated with that particular request coming from the guest operating system. And recall, what I said earlier that is the I/O ring is specific to a guest. So every guest can declare a set of I/O rings As data structures for its communication with xen. Now, in responding to the request from the guest, xen, after it completes processing the request, it'll place a response back in the same I/O ring in one of these descriptors, and that. This pointer's going to have the same unique ID that was used to communicate the request in the first place. So ,it's sort of a symmetric relationship between the guest and Xen, in terms of request and response for things that the guest wants Xen to get done on its behalf. And for Xen to communicate back ,to the guest that has completed what was requested of them. So the guest is the producer of requests. So it has a pointer, into this data structure that says ,where it has to place the next request. So, this is a pointer that is manipulated, meaning modified, by the guest. But it is readable by Xen. So there's only one guy that can modify, that is a guest. But Xen can look at this pointer. It's a shared pointer in that sense. For example, the guest operating system may have placed new requests, and that's why it has moved it's pointer to here, indicating that it has placed new requests. This is the next empty spot, where it can place new requests, after these two requests. The consumer for the request produced by the guest operating system, is of course Xen. And it is processing requests in the order in which, they've been produced by the guest operating system. And therefore, it has the pointer into this IO ring data structure, saying Where exactly it is presently servicing requests from this guest operating system. So right now ,the pointer is pointing here indicating that Xen is yet to process these two requests that have been queued by the producer. Meaning, the guest operating system, into this I/O ring data structure. So this pointer that the request consumer has is private to Xen. It is just telling Xen ,where it is in processing requests from the producer. And the difference between these two pointers is telling how many requests are outstanding so far as Xen is concerned, in terms of processing. Request emanating from the guest. Similar to request production, Xen is going to be the guy that is offering responses back to the guest operating system. So Xen, is the response producer. So it's going to have a pointer, where it can place new responses. So once again, this pointer is a shared pointer updated by Xen, but can be read by the guest. So these are two new responses that Xen has placed in the I/O Ring in response to request that it has already processed, but these responses have not yet been picked up. By the producer. The guest operating system has a pointer that says where it is in this I/O ring data structure, in terms of picking up the responses coming from Xen. Xen has produced these two new responses as a result of processing some prior requests. From the guest operating system, but the guest operating system is yet to process this request. So, this pointer is private to the guest operating system that is telling it, where it is in processing responses coming back from Xen. The difference between this pointer and this pointer is the number of Responses that are yet to be picked-up by the guest operating system. Just to recap, the difference between these two pointers is the number of requests that are yet to be processed by Xen. And the difference between these two pointers is the number of responses that are yet to be picked-up by the guest. And these slots ,are the empty slots. Into which ,new requests can be placed by the request producers. The request producer moves like this. And the response ,taker moves like this. And, the response producer moves like this. And the request consumer moves like this. And these are the empty slots into which. Xen can place additional responses as it processes more requests from the guest operating system. That's the idea behind this higher ring data structure. Very powerful extraction that allows the guest to place requests [UNKNOWN], and for Xen to place responses [UNKNOWN], into the data structure. As I mentioned already, the guests can identify the request for which a particular response is intended, because the id that is associated. With the response is going to be exactly the same, as the ID that was associated with the request in the first place. The other thing that, we should remember is that these are just descriptors of the requests and responses. Any data, that has to be passed from the guest to Xen will just be a pointer from these descriptors to a machine page that the guest owns, so that Xen can pick up the data directly, from that machine page without any copy. Similarly, if the responses given by Xen is going to have data associated with it, that will again be A pointer from the descriptive data structure to the machine page that contains the data that Xen wants to pass up to the guest. In other words, this asynchronous I/O rings is a powerful mechanism, both for efficient communication between the guest and Xen. And for also, avoiding. Copying overhead between the para-virtualized guest operating system and Xen, which is the virtualization layer. We will look at two specific examples, on how these I/O rings are used for guest and Xen communication. The first example that we'll look at is how control and data transfer is affected in zen for a network virtualization. Each guest has two I/O rings, one for transmission and one for reception. If the guest wants to transmit packets, it enqueues descriptors in the transmit I/O ring. And placing these descriptors in this IO ring is via hyper [UNKNOWN] that then provide for desktop systems. The packets that need to be transmitted are not copies into zen, but the buffers that contain these packets are in the guest operating system buffers. And what the guest does is it embeds pointers to these buffers in the descriptors that have been enqueued for transmission by hypervisor. So in other words, there's no copying of the package themselves From the guest operating system buffers into [INAUDIBLE]. Because the pointers to the guest operating system buffers have been embedded in these descriptors. And for the duration of the transmission, the pages associated with these network packets are pinned So that Xen can complete the transmission of the packets. Now, this is showing interaction of one guest operating system with Xen. Of course, more than one VM may want to transmit. And what Xen does is it adopts its own draw bin packet scheduler. In order to transmit packets from different virtual machines. Receiving packets from the network and passing it to the appropriate domain works exactly similar to what I described for transmission except in the opposite direction. What xen does when it receives a network packet intended for a particular guest, it exchanges the received packet for one of the guest operating system page that has been already provided to xen as the holding place for incoming packets. In other words, in order to make things efficient What a guest operating system will do is pre-allocate network buffers which are pages owned by the guest operating system. So when a package comes in, Xen can directly put the network package into the buffer that is owned by the guest operating system. And in queue, a descriptor for that particular guest operating system. So once again, we can avoid copying in the direction of going from xen to the guest operating system. So one of the cute tricks that xen also plays, is that when it receives a packet into a machine page. Then it can simply exchange that machine page with some other page that belongs to the guest operating system. And there again, is another trick to avoid copying. Either the guest can pre-allocate a buffer, in which case, xen can directly put that packet into the pre-allocated buffer. Or if xen receives a packet into a machine page, it can simply swap that machine page. For a page that the guest already owns so those are two different techinques for avoiding copying altogether in the reverse direction as well. Disk I/O virtualization works quite similarly. Every VM has an I/O ring which is dedicated for disk I/O. This is an I/O ring associated with VM1. This is an I/O ring associated with VM2. Similar to network virtualization. Here also, the communication between the guest operating system and then strives to avoid copying altogether. No copying into xen because what we're doing is we are entering descriptors for the disk I/O that we want to get done with pointers. To the guest operating system buffers, where the data is already contained and the transfer has to go into the disk. Or a placeholder for the data to come into from the disk. And the philosophy being asynchronous I/O. Enqueuing these descriptors into these I/O ring by the guest operating system happens asynchronously with respect to xen enqueuing responses back for prior requests coming from this VM. Since xen is in charge of the actual devices, in particular in this case, the disk subsystem. It may reorder requests from competing domains in order to make the I/O throughput efficient. But there may be situations where such a request reordering may be inappropriate from the semantics of the I/O operation. And therefore, xen also provides a reorder barrier for guest operating systems to enforce that do this operations in exactly the order in which I've given them. Such a reorder barrier for instance may be necessary for higher level semantics such as right ahead, logging, and things like that. So that completes discussion of all the subsystems that need to be virtualized, whether it is by a fully virtualized environment or a para virtualized environment. Next, we will talk about usage and billing. The whole tenant of utility computing is that, resources are being shared by several potential clients. And we've gotta have a mechanism for billing them, so it's extremely important that we have good ways of measuring the usage of every one of these resources. Whether the CPU, memory, storage, or network. And virtualized environments have mechanisms for recording both the time and the space usage of the resources that it possesses, so that it can accurately bill the users of the resources. I want to conclude this course module with a couple of pictures. One showing Xen and guests supported on top of it. And this is from the original paper which shows what are all the resources available. And how they are virtualized and how guest operating systems may sit on top of it. With a similar picture for VMware. As you know, Xen is a paravirtualized environment. VMware is a fully virtualized environment. And there's a picture showing how VMware allows different guests to sit on top of its virtualization layer. The main difference of virtualization technology from extensible operating system that we have seen in the earlier course module is, in the case of virtualization technology whether we are talking about para virtualization a la Xen. A full virtualization a la VMware. The focus is on protection and flexibility. Performance is important, but we are focusing on protection and flexibility and making sure that we are sharing the resources not at the granularity of individual applications that may run on top of the hardware. But, at the granularity of entire operating systems that is running on top of the virtualization layer. In closing, we covered a lot of ground on operating system structuring. We saw how the vision of extensible services, which had its roots in the HYDRA operating system, and the visualization technology for hardware, that started with IBM VM/370. How all culminated in the virtualization technology of today that has now become mainstream. Data centers around the world are powered by this technology. All the major IT giants of the day from chip makers, to box makers, to service providers are all in this game. Processor chips from both Intel and AMD have started incorporating virtualization support and hardware that makes it easier to implement the hypervisor, especially for overcoming architectural quarks that we discussed in the context of VM were like full virtualization. Even Xen, which started out with paravirtualization, has newer version that exploit such architectural features of the processor architectures to run unmodified operating systems on top of Xen, a technique that they call hardware-assisted virtualization. In this day and age, parallelism has become fundamental to computer systems. Any general purpose CPU chip, has multiple cores in it. Every general purpose operating system is designed to take advantage of such hardware parallelism. In this unit, we will study the basic algorithms for synchronization, communication, and scheduling, in a shared memory multiprocessor. Leading up to the structure of the operating system for parallel machines. We'll start today's lecture with a discussion of the model of a parallel machine. A shared memory multi-processor, or a shared memory machine we can think of three different structures for this shared memory machine. The first structure is what we call a dance hall architecture and a dance hall architecture is one in which you have CPUs on one side and the memory on the other side of an interconnection network and let me say something that is common to all the three structures that I'm going to describe to you. The common things are that in every one of these structures, there's going to be CPUs and there going to be memory, and there's going to be an interconnection network. And the key thing is it's a shared memory machine. What that means is that the entire address space defined by the memories is accessible from any of the CPUs. So that's one common thing that you see in all the three styles that I'm going to talk to you about. And in addition to that, you'll see that there is a cache that is associated with each of these CPUs. So there's a Dance Hall Architecture. And the next style is what is called an SMP architecture, or a Symmetric multiprocessor. Here what you see is the interconnection network hat I showed you from the dance hall architecture. I simplified it considerably, showing a simple bus. A system bus that connects all the CPU's to talk to the main memory. And and it is symmetric because the access time from any of the CPUs to memory is the same. And that's the idea of the system bus that allows all of these CPUs to talk to the main memory. The other thing that you'll notice that I already mentioned is that every CPU comes equipped with a cache and we'll talk more about the shared memory machine, the caches in a minute. So the third style of architecture is what is called a distributor shared memory architecture. So in this distributor shared memory architecture what you have, or DSM for short, is that. You have memory and a piece of memory that is associated with each CPU. At the same time each CPU is able to access all of the memories through the interconnection network. It is just that the access to memory that is close to this guy is going to be obviously faster than trying to access memory that is farther from here that has to be accessed from the interconnection network. Now let's start discussing shared memory and private caches. And in order to simplify the discussion what I've done is, I'm using the simplest form of the shared memory machine that I told you about. That is an SMP where there's a single system bus that connects all these processes to talk to the main memory. Now cache that is associated with the CPU Serves exactly the same purpose in a multiprocessor like this, as it does in a uniprocessor. And that is, the CPU, when it wants to access some memor, memory location, of course, it is going to go to the cache and see if it is there. If it is there, life is good, it can get it from the c, from the cache. If it is not in the cache, then it has to go to the main memory. And fetch it from main memory, and put it into the cache so it can reuse it later, and that's the purpose that cache performs in a uniprocessor. In a multiprocessor, performs exactly the same function as in a uniprocessor. By cacheing data that is pulled in or instructions that are pulled in from memory into the cache so that CPU can re-use it later. When cache in a multiprocessor, associated with each of these CPUs performs exactly the same role. As it does in [UNKNOWN] process. And that is, if the CPU goes to main memory, and pulls in some data, it's going to come and sit in the cache. So obviously when the CPU is looking for something, first it is going to come and look at the cache. If it is not there, it's going to go to main memory. And fetch the data and put it into the cache so that in the future the CPU doesn't have to go to the main memory, but get it from the cache itself. That's the purpose of the cache in a uni-processor. Exactly the same purpose a cache performs in a multiprocessor as well. However there's a unique problem with a multiprocessor. The fact that there are private caches associated with each one of these CPUs, and the memory itself is shared across all of these processors. Let me explain that. Let's say that there's a memory location y that is currently in the private caches of all the processors. Well maybe y is a hot memory location so all the processes happen to fetch it and therefore it is sitting in the private caches of all the processes. Let's say that process P1 decides to write to this memory location y now y is changed to y prime. Now what should happen to the value of y that is sitting in all the p caches. And clearly, you know, in a multiprocessor, a multi-threaded program, there could be a shared data structure that is being shared with all the processors. And therefore if this guy writes to a particular memory location it is possible that that same memory location is in the private caches of the peers. So this is referred to as the cache coherence problem. Now someone has to ensure that, if at a little point of time, if process of p two or p three or any of these processes that happen to have this memory location y, in the private caches decide to access it. They should get y prime and not y. Now who should ensure this consistency? Here again there's a partnership between hardware and software. In other words, the hardware and software have to agree as to what is called the memory consistency model. That is, this memory consistency model, is a contract between hardware and software as to what behavior a programmer can expect, writing a multi threaded application running on this multiprocessor. An analogy that you may be familiar with is a contract between hardware and software. If you just think about a uniprocessor, if you think about a uniprocessor. There's a compiler writer that knows about the instruction set provided by the CPU. And the architect goes and builds a CPU, and he has no idea how this instruction set is going to be used, but there is an expectation that the instruction set, the semantics of that instruction set. Is going to be adhered to in the implementation of the processor. So that, the compiler writer can use that instruction set in order to compile high level language programs. Similarly, when you're writing a multithreaded application, you need a contract between the software and the hardware, as to the behavior of the hardware when. Processors are accessing the same memory location. And that is what is called the memory consistency model. And what we're going to do now, is in order to get your creative juices flowing, I'm going to ask you a question. Let's say that initially, you've got through, two memory locations, a and b, and both these memory locations have been initialized to zero. And let's say that there are two processes executing on two different processes, P1 and P2, and so a and b, c and d, they're all shared memory locations. And let's say that process P1 is executing this piece of code. It's assigning a + 1 to a, b + 1 to b and there is another process P2 that is using the values b and a and assigning them to variables d and c. Now one thing you have to recognize is that the process of P1 and process of P2 are working completely independently, right? They're completely independent of each other. It is just that these memory locations a, b, c, and d are all shared memory locations. And, but, we have no way of knowing the relative ordering of all of these instructions that we're seeing here. When you think about a uniprocessor program there is an expectation that instructions are textually the way they're ordered, is the order in which the processor is executing. But what we're looking at is instructions that are happening on different processes. There's a set of instructions here. Set of instructions here. All manipulating the same shared memory. So the question to you is what possible values do you expect to see for d and c? Do you expect that, when, let's say that you know, process P1 completes, process P2 completes. We don't know what order they executed. Both have completed executions. At the end of that, would you expect to see c and d to be equal to 0, c and d to be both equal to 1, c to be equal to 1 and d to be equal to 0, c to be equal to be 0 and d to be equal to 1, okay? Basically I've given you all the permutation combinations of the four possibilities for the values that c and d can assume. Given this set of instructions that is happening, partially on P1, partially on P2. Now, let's talk through what possible values d and c can have. You may have picked several of these choices, but it is okay, you know, whatever you picked, it's okay. Let's talk through these different choices, to see what are possible given this set of instructions and the fact that. Processing speed one and speed two are executing, independently on two different processors and, we have no way of knowing, what is going on with the shared memory. Now the first possibility, is that, these two instructions, assignment of, a B to D and C to A, they happen. In time order, before any of these instructions executed. That's possible, because if these shared memory accesses happen before these guys, they're responsible that both of these instructions are executed before any of these instructions executed. In that case, what you would get into DNC are the old values of a and b, namely zero. The second possibility is that the both these instructions that is executed on P2 they're executed after both the instructions on P1 have completed execution. So in this case, both a has gotten new value, b has gotten a new value, and so when we go here and make the assignments. Then both d and c are going to have new values that are in b and a respectively. And so, this is a possibility, right? There is a possibility that both c and d have a value of one in them. Let's see if the third possibility can happen. The third possibility for it to happen, it is conceivable that we insert these two instructions in the middle of this. Or in other words Process P1 executed this instruction and, and in time order, it so happens that these two instructions got executed, and then this, this instruction got executed. And therefore, once you get into d is the old value of b, that is zero. And once you get into c is the old value of a. I'm sorry, the new value of a. Because this instruction is executed. And therefore, you get one into c. And that's why this possibility is also, is also perfectly valid. Now, let's look at this last choice that I have. C gets zero and D gets one. Can this happen? C getting a zero and d getting a one. And the reason I ask you this question is because, if you look at this piece of code and this piece of code here. If d were to get one, what that means is that, this assignment of b gets b plus one has already happened on P1. That's how the new value of b has gotten into d. But yet, we're saying when this process completes, c has a value of zero. What does that mean? It means that the new value of a hasn't come into the processor P2. How can this happen? It can happen if messages go out of order. You have to remember that, if you recall the picture of the shared memory machine, you've got an interconnection network that is connecting all these processors. And a write that happens on this processor has to go through the interconnection network and get to this other processor. Now it is conceivable that if message go out of order. It is possible that when this process executes this statement. This new value of b has arrived, the message that contains a new value you b has arived and there for this assignment gets a new value of, of b, but when the process execute this statement. The new value of a hasn't arrived yet and it can happen if the messages go out of order and in that case, you can end up with this particular choice of c having a value of zero and d having a value of one when this process completes execution. Do you want it to happen? Now intuitively, you would see that this is not something you expect to happen. As a programmer, you don't want surprises, right? And if you don't want surprises, perhaps if it is a non-intuitive result, that's something that should not be allowed by the model. So, when we talk about memory consistency model, we're saying what is the contract between the programmer and system? And what we are seeing through this example is that, this particular outcome is counter-intuitive and therefore the model should not allow this particular outcome to be possible in the execution. And this is the reason why you have memory consistency model. So here I'm showing you a set of accesses, memory accesses down on processor P1 read access, and write access and so on, and these are the memory location being touched by these accesses on P1. And on P2 I'm showing some real set of accesses to shared memory locations, and we know that Processor P1 accessing memory and processor P2 accessing memory, are completely independent of one another And therefore it is possible that in one execution of P1 and P2 this particular access of of writing it to memory location A Happens after reading a memory location, a happens on P1 in one execution. And if you run the same program again, P2 and P1 constituting the program run it again. It's possible that another execution of the same program the write of a happens before the read of a. It's perfectly feasible for this to happen because there is no guarantee on the ordering of these axises going to main memory. And if you think about it, both of these executions, whether it is earlier execution where write happened after this read, or this execution in which the write is happening before the read. Both these executions are reasonable and correct and something that the programmer can live with. It's acceptable to the programmer. Now in other words, what the programmer needs to know is what to expect from the system in terms of the behaviour of shared memory reads and writes that can be emanating from several different processors. And this is what is called the memory consistency model. So the expectation of the programmer is, is, is what is engrained in this memory consistency model. As a programmer, you don't want any surprises. And there's a purpose of the memory consistency model to sort of satisfy the expectation of the programmer. So I'm going to talk to you about one particular memory consistency model, which is called a sequential consistency memory model. And you consider the axises from P1 and P2. Well. One expectation that you have of the programmer is that the accesses that you have on a particular processor, is going to be exactly in the order in which your radiant or in other words, if you look at these sequences of accesses, you have the right of b here and the need of b here. You know that your one expect to see when you do this V, whatever you wrote here is what you expect to see. That's what's called a program order. What you expect is the program order to be maintained, namely the order in which you've generated memory axises should be maintained by the execution on that processor. That's program order. And in addition to that, there is this interleaving of memory accesses between P1 and P2. And this is where we said, we have no way of controlling the, the order in which these accesses are going to be satisfied by the memory. Because it depends on the execution of P1 on processor P1. And the execution on P2 and how that each memory and so on. And so this interleving can be arbatrary. That is, interleaving between axises that you see here and the axises that you see here can be arbitrary. So, that's the sequencal consistency memory model, which has two parts to it. One is the program order. That is the order that you see, textually, in every individual processes. I' m showing you two here, but you can have any of these processes. But in each one of these processes, the textual order in which memory axises are generated, they're going to be satisfied. That's the program order. On the other hand, the interleaving of these memory access has occurred all of the processes is going to be obituary. So those are the two properties of the sequential memory consistency model. In order an analogy that will drive home the point about the sequential consistency and what you might see In a casino and if you, if you watch a, a casino card shark shuffle cards. He might take a card deck and split it into two halves, and then he'll do a merge shuffle of two splits, and, and, and create a complete deck. Exactly what's going on with sequential consistency. You have Splits of memory axis's on serveral different processors, and they're getting interlinked in some fashion. Just like card shuffler is interweaving the cards from two decks and creating one card deck. All of it. By the way, this particular memory consistency model's sequential consistency Was proposed by Leslie Lamport and, this is a popular guy. You're going to see him again later on when we talk about distributor systems. But he came up with this idea of sequential consisting memory model back in 1977. And since then there have been a lot of different consistency models that have been proposed. And in future lessons on distributed systems, we will see other forms of memory consistency models such as release consistency and lazy release consistency and eventual consistency. But hold on. We will come back to that later. on. So now having seen the sequential memory consistency model, what we can do is go back to our original example, and ask the question, what are all the possible outcomes for this particular set of memory accesses performed on p1 and p2? Now what possible values can d and c get? Well obviously, you can get the first choice, no problem with that. Can get the second choice, it can get the third choice, and as we illustrated earlier, all of these are just interleaving of these memory accesses on P1 and P2. But the fourth one Is not possible with sequential consistency, because there's no interleaving of these memory axes and these memory axes that'll result in this particular outcome. That's comforting, that's exactly what we thought would be a useful thing to have in a memory-consistency model that gives only intuitive results and, and makes sure that non-intuitive results don't happen. Memory consistency model is what the application programmer needs to be aware of, to develop his code and know that it will execute correctly on the shared memory machine. As operating system designers, however, we need to help make sure that this code runs quickly. To do that, we need to understand how to implement the model efficiently. And also the relationship between hardware and software that makes it possible to achieve this goal. So now, we understand the memory consistency model. What is the model that is presented to the programmer? That's what memory consistency model is. On the other hand, cache coherence is how is the system implementing the model in the presence of private caches? So this is a handshake, a partnership between hardware and software, between the application programmer and the system, in order to make sure that the consistency model is actually implemented correctly by the cache coherence mechanism that is ingrained in the system. And the system implementation of cache coherence. Is itself a hardware-software trade off. Now for instance one possibility, is that the hardware is only giving shared address space. It's not giving you any way of making sure that the caches are coherent, but it is giving you the shared address space. And it is letting the software, the system software ensure that this contract is somewhat satisfied. And the working of the cache coherence maintained in software by the system. That's one possibility, and that is what is called a non-cache coherent shared address multiprocessor. Meaning that There is shared address space, that's available for all the processors, there is private caches for holding data that you bring from memory, but, if you modify data, it is a problem of the system software to make sure that the caches remain coherent. So it's non-cache coherent. That is called NCC shared memory multi-processor. The other possibility of course is that, the hardware does everything. It provides the shared address space, but it also maintains cache coherence in hardware. And that's what is called a cache coherent multi-processor, or a CC multi-processor. Now let's focus on the hardware implementing cache coherence entirely in addition to giving the shared address space. There are two possibilities if the hardware is going to maintain the cache coherence. One possibility is what is called write invalidate scheme. And here the idea is, if a particular memory location is contained in all the caches, all these processes have fetched this particular memory location Y, and it's been sitting in the private caches of all these processes. And if now, process of P one, decides to write to this particular memory location it changes to from y to y prime. When that happens, what we're going to do is, the hardware is going to ensure that all of these caches are invalidated. So, the way it's done is that the hardware, as soon as this change happens, is going to broadcast a signal on the bus. Called invaldidate memeory location Y. So that's something that propagates on the system bus, and all these process of caches, are observing the caches, and this is sometimes referred to as snoopy caches, in a, in a lighter vein, these caches are snooping on the bus to see if there's any change to memory locations that are cached locally. And in this case, if an invalidation signal goes out for a particular memory location y, then each of these caches are going to say do I have that particular memory location? If I do, let me invalidate it. So, that particular memory location gets invalidated. So the idea is if you have that particular memory location, invalidate it. If you don't have that memory location, ignore it. Right? So if you don't have it, you don't have to bother, but if you particularly happen to have this memory location cached in your private cache, and if you observe an invalidation for that particular memory location, you go ahead and invalidate it. That's what is called write invalidate cache coherence scheme. You may already be one step ahead of me, and you may be thinking what would be an alternative to doing this invalidation? And, and you may be, you may be right. You thought of perhaps updating the caches. That's what is called write update scheme. The idea here is, if this guy is going to write to this particular memory location, modify to y prime, what we do is, instead of invalidating it on the bus, if there is a capability in hardware to send an update for this particular memory location on, on the bus. You send it out saying that I modified this particular memory location, this is a new value, and if these caches happen to have the same memory location, they all modify it from y to y prime. And now, all of these caches have the new value of y and the old values disappear from the system. So in this case, what we are doing is, if you have it, update it. Once again, you're snooping on the bus. Each of these process of caches is snooping on the bus and if they see an update for a particular memory location, they're saying, well, let me modify it so that future accesses by my CPU will get the most recent value that had been written into this particular cache line. That's the idea behind write update scheme. Now whether we're talking about write update scheme or the earlier write invalidate scheme, one thing should become very clear in your mind and that is there is work to be done whenever you change some memory location that could conceivably be cached in the other private caches of the CPUs. And the invalidate scheme has sent out an invalidate message. If it's an update scheme, it sends out an update message. And that kind of transaction that's going on is, is an overhead. And as, as a system designer, one of the thing that we've been emphasizing all along is that we want to keep the overhead to a minimum. But you can also see immediately that the overhead is going to be something that grows as you increase the number of processors. As you change this inter-connection network from a simple bus to a more exotic network. And also depending on the amount of sharing that is happening for a particular shared memory location. Now as a programmer, you have a certain expectation as you add more processors to the system. Your expectation is natural if you think that if you add, if you add more processors your performance should go up. So this is expectation. This is what is called scalability. That the performance of a parallel machine is going to scale up as you increase the number of processes. Reasonable. Reasonable to expect that. However, I mentioned just now that the overhead associated with increasing the number of processes in terms of maintaining cache coherence when you have sharing that is happening for shared data. And so therefore, the pro in adding more processors is the fact that you can exploit parallelism. That's the reason why you're able to get this expectation of increased performance with processors. But unfortunately, as you increase the number of processors, there is increased overhead. And, and the increased overhead also grows. As you increase the number of processors more, overhead is going to be incurred by the system. If we have an eight processor SMP the overhead for cash coherence is less than if we have a 16 processor SMP or a 32 processor or a 64 processor, so the overhead is going to grow. As a result, you can see that you have the pro of exploiting parallelism but you have the con of increased overhead and you end up with an actual performance that's somewhere in the middle between your expectation and the overhead. So, in some sense this is a difference between what your expectation is and what the overhead you're paying. And that becomes the actual delivered performance of a parallel machine. And this is very important to remember, that your delivered performance may not necessarily be linear in the number of processors that you add to the system. So what should we do to get good performance? Don't share memory across threads as much as possible, If you want good performance from the parallel machine. A quote that is attributed to a famous computer scientist Chuck Thacker comes to mind, shared memory machines scale well when you don't share memory. Of course as operating system designers, we have no control over what the application programmer does. All we can do is to ensure that the users share data structures is kept to a minimum and the implementation of the oppressing system caught itself. You will see how relevant Chuck Thackers quote is as we visit operating system synchronization, communication and scheduling algotithums and more generally. The structure of the operating system itself in this lesson. See if you can remind yourself of this quote, and how often it permeates our discussion as we go through this lesson. In the previous lecture, we got done with discussing the model of a parallel machine. And in this lesson, what we're going to start doing is talking about synchronization algorithms that goes into the guts of any parallel operating systems that is supporting multi-threaded applications. And as we discuss the synchronization algorithms watch out for attacker's quote that I mentioned in the previous lesson on sharing, in shared memory multiprocessors that is going to be very, very key in terms of understanding the scalability of the synchronization algorithms. Synchronization primitives are a key for parallel programming. In your first project, you implemented a threads library, which provides the mutual exclusion lock. Let's talk about locks. What exactly is a lock? Well, you know, in the metaphor that you know about in real life. Lock is something that protects something that is precious. And in the context of parallel programming, if you have multiple threads executing and they share some data structure, it is important that the threads don't mess up each other's work. And a lock is something that allows a thread to make sure that when it is accessing some particular piece of shared data It is not inter being, being interfered with by some other thread. That's the purpose of a lock. So the idea would be that, a thread would acquire a lock, and once it acquires a lock, it knows that It can access this data that it shares with potentially other threads. I'm showing only two threads here, but potentially in a multi-threaded program you can have a lot more threads that are sharing a data structure. And once T1 knows that it has access to this data structure, then it can do whatever it wants with it. And then once it is done with whatever it wants to do with this data it can release the lock. So that's sort of the idea behind a lock. And locks come in two flavors, one is what we'll call an exclusive lock, or a mutual exclusion lock. And this is exactly the one that you implemented in your first project. And the idea is, as the name suggests, a mutually exclusive lock means that it can be used by a thread, one thread at a time. That's the idea. And here's a silly example of two children playing, and you know, they have to take turns in order to hit this ball and obviously, you don't want both of them hitting the ball at the same time. Not good for the game and not good for the safety of the children either. That same, same thing that applies to the mutual exclusion lock that we use in parallel programming. The idea is that a thread that wants to modify data It has to make sure that when it is modifying the data, nobody else is going to be accessing that particular data structure. And therefore it is going to get a mutual exclusion lock, it knows that nobody else is going to be messing with it. Then it can modify the data and then release the lock. And similarly if another thread wants to read that data and wants the assurance that nobody is going to be modifying this data while it is reading it, it can get a, an exclusive lock, access the data, read it and then release it. That's the idea behind mutually exclusive lock. You can also have a shared lock. Now, what that means is that this lock is something that allows multiple threats to access the data at the same time. Well, under what conditions would that me meaningful? Well, here is a, an analogy again. If there is a newspaper, and multiple people want to read the newspaper at the same time, perfectly fine to do that, right? That's the same sort of thing that happens often in parallel programming. That you have a database, and, and there are records in the database that multiple threads want to inspect. But they want to make sure that while they're inspecting it the, the data itself is not going to be changed so a shared lock is something that allows multiple readers to access some data with the assurance that nobody else is going to be modifying the data. So these are two different types of locks that you might have that might be useful in developing multi-threaded shared memory programs. Another kind of synchronization perimeters that is very popular in multithread apparel programs, and extremely useful in the developing applications, especially in the scientific domain, is what is called a barrier synchronization. The idea here is that, there are multiple threads and they are doing some computation And they want to get to a point where they want to know where everybody else is at that, at that point of time. And they want that insurance that everybody has reached a particular point in the respective computations so that then they can all go forward from, from this barrier to the next phase of the computation. Now I'm sure that you've gone to dinner with your friends and one of the experiences that you may have had is that, and you may have a party of four or five people that are going for dinner. Two or three of you are showing up at the dinner restaurant. And the usher say wait, you know, do you have the entire members of your party here? If they're not here wait til the other members of the party show up, so that I can seat you all at the same time. And that's sort of the same thing that's happening with barrier condition. It is possible that, you know thread t1 and t2 arrive at the barrier, meaning they completed their portion of the work. They've gotten to this barrier but the other threads that are lagging behind and those shirkers are going to eventually show up but they're not here yet and until everybody shows up nobody can advance to the next phase of the computation. So that's the idea, behind barrier synchronization, exactly similar to the analogy that I mentioned here. So we looked at two types of synchronization primitives. One is the lock, and the other is the barrier synchronization. Now, these are concepts I am expecting that you know already. If you find that these two concepts are either new to you, or you would Like some refresher for that, I strongly advise you to go and, and take a look at the review lesson that we have on multithreaded programming. Now that we understand the basic synchronization primitives that are needed for developing multithreaded applications on a shared memory machine. It's time now to look at how to implement them. But before we do that, let's do a quiz to get you in the right frame of mind. To get you primed up to answer this question, let's first discuss a little bit about the instructions as architecture of a processor. In the instruction set architecture of a processor, instructions are atomic by definition, or in other words if you think about Reads and writes to memory which are usually implemented at loads and stores, and the instructions have architecture for processor. Those are atomic instructions, and what that means is, during the execution of either a load instruction or a store instruction or also, as you might think about them, read or write instruction, the memory. The processor cannot be interrupted. That's important, that that's the, the definition of an, of an atomic instruction that the processor is not going to be in-, interrupted during the execution of an instruction. Now the question that I'm going to ask you to think about is, if I have a multi-threaded program And in that program, there is a process of P 1, which is modifying a data structure A, and there is a process of P 2. That is waiting for the modification by P 1 to be done, and after the modification is done, it wants to use that structure. Very natural, to think about situations in which you have this kind of a producer-consumer relationship. This guy is the producer of data, this guy is the consumer of data. And the consumer wants to make sure that the producer is done producing it before he starts using it. Quite natural. Now, given that the instructions of architecture is only read and write atomic instructions, The question that I'm going to pose to you is, is it possible to achieve the programmer's intent that I have embodied in this code snippet here? And, you know, the the answer is a binary answer, yes or no. And and if you, if you answer yes, I would like to see a code snippet that you think would make this particular code snipet work correctly on a multiprocessor. If you answered yes, then you and I are on the same wavelength. And in the next few panels, I'm going to show you how this particular programming construct that a multithreaded program may execute in terms of producer and a consumer, can actually accom, accomplished with simple read/write atomic operations available in the instruction set of a processor. The solution, it turns out is surprisingly very simple. The idea is that between p1 and p2, I'm going to introduce a new variable, a shared variable, and that variable, I'll call it a flag. And I'll initialize this flag to be zero to start with. And the agreement between these two. Producers in consumer is that the producer will modify the data structure that he wants to modify and once he's done with the modification he will set this flag to be a one. And that's the signal to p2 that this guy is done with the modification. Now, what is p2 doing? Well, p2 is basically waiting. For this flag which initial, initially the flag was initially zero. And, and basically the processor P2 is waiting for the flag to change from a zero to a one. Now once p1 is done with its modification, it's going to set this flag to a, to a one. And that's the signal that this guy's waiting for. And as soon as this flag changes to a one. Then he'll break out of this spin loop here,and he is now ready to use the real structure. And once he is done using the real structure, he can flip it back to zero, to indicate that he is, that he is done using it. So that the next time it the producer wants to modify it again he can do that. So that's sort of the solution. Now, let's analyze the solution and see why it works. It will just atomic reads and writes. Now the first thing to notice is that all of these are read and write accesses. There's nothing special about them. This is, this is going to be modifying data using loads and stores, and this is storing a value into it, and this is reading a value and using a value. So all of these are normal read write accesses, but there is a difference between. The way the program uses this flag variable versus this data structure. The flag variable is being used as a synchronization variable. And that's a, a secret that only this P1 and P2 know about. That this, even though innocuously it looks like a simple Integer variable that is used in a program where there is special semantic for this particular variable so far as this, this program is concerned. P1 and P2 know that this is the way by which their signalling each other, that something that this guy waiting on is available from P1. Right? And so its a synchronization variable. On the other hand, the - The data structure A is a normal data. But, both accessing the synchronization variable and normal data is being accomplished by simple read write accesses that's available in the processor. And that's how we're able to get the solution for this particular question. It's comforting to know that atomic read and write operations are good for doing simple co-ordination among processes as we illustrate it through this question. And in fact, when we look at certain implementation of barrier algorithms later on You'll find that this is all that is needed from the architecture in order to implement some of them. But now, how about implementing a synchronization primitive like a mutual-exclusion lock? Are atomic reads and writes sufficient to implement a synchronization primitive like a mutual-exclusion lock? Let's investigate that. Let's look at a very simple implementation of a mutual exclusion lock. In terms of the instructions that the processor will execute in order to get this lock, will be to come in and check if the lock is currently available and that is done by this check. And if it is available then we're going to set it to one to indicate that, I've got the lock, nobody can get it. All right? That's the idea behind this check and then setting this to one. On the other hand, if somebody already has the lock L is going to be one and therefore I'm going to wait here until some the the lock is released. And once the lock is released, then I can go back and check again, to make sure that the lock is available and set it to one. So this is the basic idea. Very simple implementation of this lock. And, and how will I know that the lock has been released? Unlocking this is a very simple operation again. All that you have to do is reset this L to zero, and that'll indicate that the lock has been released. So, if I am waiting here, and somebody else has got the lock, they going to come and unlock it by setting it to zero. And that way, I will know that it has been released. I can go back. I double check to make sure it is still zero, because somebody else could have gotten in the middle. If nobody else has gotten it, then I can set it to one. So this is the idea of a simple, very simple minor implementation of, a lock algorithm. Is it possible to implement the simple minded implementation of the lock using atomic reads and writes alone? Let's talk thought this implementation here. Now, if you look at this set of instructions that the processor has to execute in order to acquire the lock. It has to first read L from memory, and then check if it is 0. And store that new value which is 1, into this memory location. That's a group of three instructions that the processor has to execute and the key thing is these three instructions have to be executed atomically in order to make sure that I got the lock and nobody else is going to interfere with my getting the lock. And as we know reads and writes instructions by themselves are atomic, but a group of reads and writes are not atomic and therefore. What we have here is a group of tree instructions and we need them to be atomic. What that means is we cannot have just reads and writes as the only atomic operations if we want to implement this lock algorithm. And we need a new semantic for an atomic instruction, and the semantic is what I call the read modify write operation. Meaning that I'm going to read from memory Modify the value and write it back to memory. So that's the kind of instruction that is needed in order to ensure that we can implement a lock algorithm. Now several flavors of read-modify-write instructions have been. Proposed, and or have been implemented in processor architectures. And we will look at a couple of them. The first one is what is called a test and set instruction. The idea here is, the test and set instruction takes on a memory location as an argument. And, what it does is, it returns the current value that is in this particular memory location. And also sets the memory location to a one. So, these two things that are being done. That is, getting the current value from memory and setting it to one, is being done atomically. That's the key thing. That it is testing the old value and setting it to this new value, atomically. Another atomic Read Modify Write instruction that has been imposed and/or implemented is what is called a fetch and increment instruction. And this takes on again, a memory location of an argument, and what it is gong to do is, it is going to fetch the old value of what was in the memory... And then incriment the current value that is in the emmory by one or whatever value. So it could be that this may take on an extra argument to indicate how much it is going to change it by. But in the simple version it might simply imple, increment, in the simple version it might simply incriment the current value that is in the memory location by one. As I said before, there have been several flavors of read modify write instructions that have been proposed in the literature. And often generically these read modify instructions are called fetch and phi instructions. Meaning that it is going to fetch an old value from memory. And do some operation on that fetched value and write it back to memory. So, for instance, fetch an increment is one flavor of that. There are other flavours like fetch and store, fetch and decrement compare and swap and so on. And you can read about that in the papers that we've identified for you. Okay, now that we have an atomic read modify write instruction available from the architecture, we can start looking at how to implement the mutual exclusion lock algorithems. Now, I gave you, of course, a very simple version of it, we'll talk more about that in a minute. And, and I'm sure that in the first project, when you implemented the mutual exclusion lock, you did not care too much about the scalability of your locking implementation. Now if you are implementing your mutual exclusion algorithm on a large scale shared memory multi-processor, let's say with 1000's of processes. You'll be very worried about making sure that, that your synchronization algorithm scale and scalability issues fundamental to the implementation of synchronization algorithms. Now let's discuss some of the issues with scalability of the synchronization primitives in a shared memory multiprocessor. Now we already saw that locks, both mutual exclusion as well as shared lock is one type of a synchronization operation. And we also saw that barrier algorithms is another type of synchronization operations. And when you look at both of these guys Both of these types of synchronization perimeters that a parallel operating system is going to provide for application programmer developing multi-threaded applications. The sources of inefficiencies that come aboard is first of all latency. What do we mean by that? Well, If the thread wants to acquire this lock, it has to do some operation. Has to go to memory, get this lock, and make sure that nobody else is competing with it. And, so that's the the latency that is inherently what is the time that is spent by a thread in acquiring the lock. That's what we mean by latency. Well to be more precise what we mean is that latency is saying, lock is currently not being used. How long does it take for me to go and get it? That's really the key question that latency is, is trying to, look at. The second source of scalabilty with synchronization, is the waiting time, and that is if I want to go and get the lock, how long do I wait in order to get that lock? Well clearly this is not something, that you and I as the oldest designer has complete control over, because it really depends on what these threads are doing with this lock. So for instance, if this thread acquires this lock and, and then it is modifying the data for a long time before releasing it, and if another thread comes along and wants the team lock, it's going to wait for a long time. So the waiting time is really in the purview of the application. And there's not much you can do, as an OS designer, in reducing the waiting time. The third source of unscalability of locks is contention. What we mean by that is. If currently some guy is using the lock, and he releases it, when the lock is released, it's now up for grabs. Maybe there's no, I've shown you only one thread here, but maybe there's a bunch of threads waiting here to access this particular lock. If they're all waiting to access this lock, they're all contending for this lock. And how long does it take in the presence of contention for one of them to become the winner of the Lock and the others to go away? So that's the contention part of im, implementing a synchronization primitive. And all of these things, latency, waiting time and contention even though I mentioned it in the context of a mutual exclusion lock appears when you're talking about barrier synchronization, algorithms, or shared locks. So latency and contention are two things as all designers, we have to be always worried about, and implementing scalelable versions of synchronization primitives. Let's start our discussion with the world's most simplest and naive implementation of the lock, and we're calling it spinlock because, as you're going to see a processor that is waiting for lock has to spin, in order to spin, meaning, doing no useful work, but it is waiting for the lock to be released. And the first one that we're going to look at, is what is called spin on testing set. The idea is very simple and straightforward. There's a shared memory location, L and it can have one of two possible values. Either unlocked or locked. And let's say that at the. at the beginning, we've initialized unlocked, so nobody has the lock. And the way to implement this naive spinlock algorithm is the following. What you do is, you go in and check, using test and set primitive, the memory location, L. So when you call this lock primitive the lock primitive executes this Instruction testant set of L, and what that is going to do is, its going to return the old value from L, and set it to the new value which is locked, that's going to be done automatically, we that from the architecture, that it is going to provide you that, that is a primitive, and so now, if we find that this test and and set, instruction execution returns the value locked, it means that somebody else has bought the lock. And therefore, I cannot use it and i'm going to basically spin here. That's why its called spin on test and set, so you're basically spinning waiting for this test and set instruction to return to me. A value that says, the old value is unlocked. If I, if it gives me, the old value is unlocked, then I know I won. But if I don't, then I, basically, I'm going to wait here. That's why it's called spinning on test and set. So let's put up some threads here that are trying to get this lock. And, so let's say that T 1 is the first one to make a test and set call on this lock, and it finds it unlocked, and therefore, it locks it. And once it locks it, T1 knows that it's got the lock. So, it's got the lock, it can go off to messing with the data structure that it wants to mess with, and that is good. So far as T1 is concerned. In the meanwhile, T2 and T3 may come along and say well, we also want the lock, and they also execute the same lock algorithm. And when they execute the lock algorithm, they're going to do the Test-and-Set and you know that the Test-and-Set when they do that, the old value that is going to be returned for T2 or T3 is going to be that the value L is locked and therefore these two guys, both T2 and T3 are stuck here. How long are they going to be stuck? Until this guy releases the lock, and the way to do that is very simple. So he comes along and calls an unlock function, and what the unlock function does, is it basically goes in, and clears this lock. Meaning it resets this lock to the unlocked state. And, and so once it does that, then this lock becomes available. So, it becomes unlocked and at this point, T2 and T3 were stuck here. When they tried to do this testing set again, they're going to find, at least one of them hopefully exactly one of them, is going to find that, that the lock is unlocked and therefore they're going to get it. And one of them will get it, and, will, will go on to executing whatever code they want to do, and the protection of the lock, and so only exactly one of T2 or T3 will win, because that's a semantic of test and set. So that's the world's simplest lock algorithm, spinning on test and set. I'm going to pause here and let you think about, what are the problems with this naive implementation of this spinlock. Do you think there is going to be too much contention? Or do you think that it, it's not good enough because it does not exploit caches? Or do you think that it is going to disrupt useful work? Think about that. If you checked all three of them, you're exactly on the right track. Let's talk about it. First of all, you know that with this, with the naive implementation there is going to be too much contention for the lock when the lock is released. Because everybody, both t2 and t3 in the previous example, jumped in and started looking at the test and set instruction, trying to acquire the lock. And there are thousands of processes, everybody is going to be executing this test and set instruction, so there is going to be plenty of contention on the network, in order to get to that shared variable, that's the first problem. Now, let's talk abolut why the second answer is also the right answer. You know from the previous lesson that a shared memory multiprocessor has private caches associated with every one of the processors. And it is often the case that the caches may be kept coherent by the hardware. Now if the private caches associated with every processor and if a value from memory can be cached in that, there is an issue with test and set instruction. And that is, test and set instruction cannot use the cached value, because it has to make sure that the memory value is modified atomically when, when it is inspecting the memory. And therefore. By definition, a test and set of instruction, is not going to exploit caches, it is going to bypass the cache and go to memory, in order to do the test and set operation. And therefore yes, this is also true, that the spin algorithm that I gave you, spin on test and set, is not going to be able to exploit the caches. The third problem is, is the fact that it might disrupt useful work. And it's also a good answer and the reason is because, when a processor releases the lock. After releasing the lock, that processor wants to go on and do some useful work. And similarly. If, let's say there are four processors trying to acquire the lock. Only one of them is going to get it, and the others are going to have to back off, because they're not going to have the lock. Now the one, one guy that did get the lock has useful work to do. But, because of the fact that there's a lot of contention, the guy that can actually do useful work is being impeded from doing useful work because the contention that is there. In all the other processors trying to go and get the lock when it is not available. So, this is really the problem, that the test and set instructions, because it is bypassing the caches, it's first of all causing a lot of contention on the network and it is also impeding some of the useful processor from carrying on with its work. Which may advance the cause of the parallel program. So all of these are good answers in terms of the problems with this, with this naive spinlock. Now let's look at how we can exploit the caches available. Now, it is a fact that a tested set instruction has to necessarily go to memory when we want to acquire the lock, we have to execute a tested set instructions so that we can atomically make sure that exactly one processor gets the lock. But on the other hand, the guys that don't have the lock. Could, in fact, exploit the caches in order to wait for the lock. And that's why this particular algorithm that I'm going to describe to you is what is called spin on read, and the assumption here is that you have a shared memory machine in which the architecture is providing cache coherence, or in other words, through the system bus or interconnection network, The hardware is ensuring that the caches are kept coherent. Well that gives us an idea as to how we can exploit the caches. The waiters, instead of executive a test [INAUDIBLE] instruction that has to go to memory, they can spin locally on the cached value of the lock. because when you are spinning on the local cached value of the lock. If that value changes in memory, these guys are going to notice that. That's the principle behind the cache coherence that is implemented in hardware. And so we can exploit that fact in implementing a more efficient way of spinning. Which is called spin on read. The idea is that the lock algorithm, the first thing it's going to do is go and do a check on the, the memory location to see if it is locked. So this is a, a normal atomic read operation that is being done, not a test and spin operation, so if it is not in the cache, you're going to go to memory and bring it in, and once you bring it in, so long as this value doesn't change, we're going to basically looking at the value that is in that cache in order to do the checking And I'm not going to go to the bus and therefore, I'm not producing any contention on the network. And there could be any number of processes waiting on the lock simultaneously. No problem with that because all of them are going to be spinning on the local value of L in their respective caches. And so if there is one processor that's actually doing useful work and it has to go to memory, it's not going to find that to be a problem. No contention on the network from the waiting processors because of this. Now, if the one processor that was having the lock eventually releases it, they go, everybody's going to notice that. And so if I'm waiting for the lock, and I've been spinning here locally in my cache, when the, the lock is released, I'll notice that through the cache coherence mechanism as I'll, I'll break out of this spin loop. But immediately, I want to check, I want check if the lock is available by doing this test and set and get it uniquely for myself. So multiple processors are trying to execute this testing set simultaneously. It's possible somebody else is going to beat me to the punch and if that happens, I simply go back and, and, and do the grouping on my private copy of L and wait for the guy who beat me to the punch to release a lock eventually. So that I can get it. So that's the idea. The idea is you spin locally. When you notice that the lock has been released you try and do a test and set. If you get lucky you win, if you lose you go back and spin again locally. So that's the idea behind spinning on read. The unlock operation of course is pretty straightforward. The guy that wants to unlock is simply going to change the memory location to indicate that L is no longer locked. So that's all it has to do. And then, and then the other processes can observe it through the cache coherence mechanism, and be able to acquire the lock. But note what happens when the lock is released. When the lock is released, all the processes that are stuck here in the spin loop, they are going to go and try to do this test and operation at the same time, and we know that test and set has to bypass the cache, everyone is hitting on the bus, right? Everybody is hitting on the bus, trying to go to memory, in order to do this test and operation. And so that essentially means that, in a right invalidate this cache coherence mechanism is going to result in order of n squared bus transactions. For all of these guys to stop chattering on the bus, because everyone of these test and set instructions is going to result in invalidating the caches, and as a result, you have an order of n squared operation that is going to result when a lock is released, where n is the number of processors that are simultaneously trying to get the lock. And, obviously, this is impeding that one guy that got the lock and can actually get some useful work done. And this is clearly disruptive. And earlier one of the things that we said is that we want to avoid or limit the amount of disruption to useful work that can be done by the process that acquired the lock. Now, in order to limit the amount of contention on the network when a lock is released, we're going to do something that we often do in real life. procrastination. So basically, the idea is the following. Each processor is going to delay asking for the lock, even though they observe that the lock is released. Then I will immediately go and try to get the lock. They're going to wait for a little bit of time. It's sort of like what happens at rush hour. If you find that the this traffic is too much, you might decide that I don't want to get on the highway right now. I'm going to delay a little bit so that I don't have to spend as much time on the highway. So that's sort of the same thing that is being proposed here, and this is what is called spinlocks with delay. Let's discuss two different delay alternatives. In the first one, you're here. You found that you did not get the lock and therefore, you're here locally spinning in your cache, waiting for the lock to be released. Normally what you would have done, when the lock is released, is go back out, break out of this loop and go back and check if you can get the lock again. But what we're going to do is, instead of doing that, when we break out of this loop, meaning that the lock has been release, I'm not going to immediately go and check to see if I can get the lock. I'm doing to delay myself by a certain amount of time. And you notice that the delay is conditioned by what processor id I have. So every processor is waiting for a different amount of delay in order to contend for the lock. So since the delay is being chosen differently for each processor, even though all of them notice that the lock has been released simultaneously, only one of them will go and check it. And so we are sort of sequentializing the order in which the processors that are waiting for the lock are going to check whether the lock is available. So that is one possible scheme for delaying. Now the problem with this is it's a static delay, right? So every processor has been preassigned a certain amount of delay, which means that even if the lock is available, I may not immediately go and check because my delay may be very high compared to some other processor. And that's always an issue when you have static decision making. What we can do is instead make the decision dynamically, and what we're going to do is, when we notice that we don't have the lock, we're going to delay ourselves by a certain amount of time before we try for the lock again. You notice that if you're going to delay checking for whether I have the lock or not. It's not super critical that, that you spin locally or go to memory. But in this example, I'm making it very simple by saying that if you don't get the lock, just delay a little bit before you try for this lock again. And the idea here is this delay is, is some small number to start with. But suppose I go and check and I find it again to be locked. Now, what I'm going to do is the next time around, I'm going to increase the delay. That's why it's called exponential backoff. So I'm increasing the delay, doubling the amount of delay that I'm going to do. So that the next time, if I don't find the lock to be available, I delay by twice the amount from the previous time. And this is essentially saying that when the lock is not highly contented for, I'm not going to delay myself too much. I'm going to immediately go and get it. But on the other hand, if I go back again and again, and every time I go and check, I find it is locked, I'm going to increase the amount of delay. Because that's saying that lot of people are contending for the lock at the same time. And therefore, in order to make sure that we are being sensitive to the contention that is there for the lock, we increase the amount of delay that we're experiencing. Now one nice thing about this simple algorithm that I've shown you is that I'm not using the caches at all. And, if the, if the processor happens to be a non-cache coherent multi-processor, this algorithm will still work. Because we're always using test and set, and not using just loading from the memory. Because if it is not a cache-coherent multiprocessor, your private cache is now going to be coherent with respect to memory. And so you have to execute test and set. But you don't want to do it all the time. And this delay makes sure that you can reduce the amount of contention on the network. Generally speaking. If there's a lot of contention, then static assignment of delay may be better than the dynamic exponential backoff. But in, in general any kind of delay, any kind of procrastination, will help a lock algorithm better than the naive spin lock that we talked about. Up to now, what we've talked about is how to reduce the latency for requiring the lock and the contention when the lock is released. So far we've not talked about fairness. What do we mean by fairness? Well, if multiple people are waiting for the lock, should we not be giving the lock to the guy that made the lock request or tried to acquire the lock first. Unfortunately, in Spinlock, there is no way to distinguish who came first. Because, as soon as the lock is released, they are going to try and gab the lock. And, it's entirely up for grabs, as to, who may be the winner. So next, we're going to do is, we going to look at a way by which we can, we can ensure fairness in the lockout position. Now many shops and restaurants, busy ones, that is, often use a ticketing system to ensure fairness for those who are waiting to get served. So for instance, in this example here let's say, I walk in the deli shop. And my ticket is 25, and I notice that currently they're serving 16. So I know that I have to wait for a little bit of time. And you know, once my number comes up, I can get served. So this is actually, and if I know that there at least nine people ahead of me who need to be served before my turn comes up. And by similar argument. If people come after me, I know that they're not going to served before me. That's the basic idea that we're going to use in this ticket lock algorithm. The ticket lock algorithm is basically implementing what I described to you as to what happens in a deli shop. The lock data structure has two fields to it, a next-ticket field, and a now-serving field. And the lock algorithm, in order to acquire a lock, what I'm going to do is I'm going to mark my position. And the way I do that is I'm going to get a ticket just like when I walk in a deli shop. I get a unique ticket, I get a unique ticket by doing a fetch and increment on the next ticket field of the log data structure, and when I do the structure increment, I get a unique number and this number is also advanced, exactly like how it would happen in a deli shop. And once I have my position marked, as to when I can get my lock. I can then wait by procrastination. And what I'm doing here is pausing to see if I've won my. Lock by an amount that is proportionate to the difference between my ticket value and who is being served currently. And after there's amount of dealing, I'm going to go and check if the now serving value equals my ticket value. And if, if it is, then I'm done, I can return. Otherwise I go back to looping. So basically I'm looping, waiting for, waiting for my number to be up so that I can assume that I've got the lock. And how am I going to get, get this information that, that my ticket is up for serving? That is going to be done with the current holder of the lock. He's going to come and release the lock, and when he releases the lock, he's going to increment the now_serving value in the lock data structure, and that's all, eventually, the now_serving will advance to be equal to my_ticket, and I'll get the ticket, and then I can, I can return from the acquire lock. Now this algorithm is good, that it preserves fairness, but you notice that, everytime the lock is released, there is now serving value that is in my local cache is going to be updated with a cache coherence mechanism, and that's going to cause contention on the network so On the one hand frener is achieved and on the other hand, we have not really completely gotten rid of the contention that can happen on the network when the lock is released. So, to summarize the Spinlock algorithm that we've seen so far, we saw that the read spin on read and spin on test and set and spin on test and set with a delay. All of these spin algorithms, there's no fairness associated with them. And if you think about the ticket lock algorithm, it is fair but it is noisy. So, all of them are not quite there yet in terms of our twin objectives of reducing latency and reducing contention and if you think about it, let's say that you know, that currently this T1 has got this lock. And all of these guys are waiting for this lock to get released. You know when T1 releases the lock, exactly one of them is going to get it. Why should all of them be attempting to see if they, they've got the lock. Ideally, what we would want is that when T1 releases a lock. Exactly one guy, one of these white reading guys is, is a signal to indicate that you've got the lock. Because exactly one guys can, can get the lock to start with. And therefore, ideally T1 should signal exactly on the next thread and not all of them. Now, this is the idea. Behind, queueing locks that you're going to see next. We will discuss two different variants of the queueing lock the first one we'll talk about is the array-based queueing lock, and this is due to Anderson. And I'll refer to it as Anderson's lock later on as well. Associated with each lock L is an array of flags. And the size of this array is equal to the number of processes in the SMP. So if you have a an N-way multiprocessor, then you have N elements in the circular flags array. And this flags array serves as a circular queue. For N-queuing the requesters that are requesting this particular lock L. So every lock has associated with it, this flags array and it's really intuitive that since we have utmost we have N processors in this multiprocessor. We can have utmost N requests simultaneously waiting for this particular lock so the size of the data structure, the flags data structure is equal to N where nN is the number of processors in the multiprocessor. Now each element in this flags array can be one of two states. One state is the has-locks state. And the other state is a must-wait state. Has-lock says that whoever is waiting on a particular slot has the lock. So this particular entity let's say, is hl. And that means that whichever processor happens to be waiting on this particular slot is a current winner of the lock and is using the lock. On the other hand, must-wait is indicating that if a processor has must-wait as the. Entry in this particular element of the array, and is waiting on this particular slot, that means that the processor has to wait. You guessed it. There can be exactly one processor that can be in the hl happy state because it's a mutually exclusive lock. And therefore. Utmost one processor can have a lock at a time, and all the others should be waiting. And, so what we do is, in order to, when we get this lock. To initialize the lock, what we do is we initialize the lock data structure, this array data structure. The flags of array data structure which represents a circular queue by marking one slot as hl. And all the others as must-wait. An important point I want you all to notice, is that the slots are not statically associated with any particular processor. As requesters come in, they're going to line up in this flags array at the spot that they get in the next available slot. The key point is that there is the unique spot that is available for every waiting processor. But it is not statically assigned and we'll see. How requests get formed using this circular queue in a minute. Since we've initialized this array with hl in the first spot and mw in all of the other spots of this array, to enable the queuing what we will do is associate with each lock another variable, which is called a queuelast variable. And this queuelast variable is initialized to zero. And so these two are the two data structures associated with every lock. So every lock that you have in your program, the operating system is going to assign two data structures for you. One, which is the circular queue, represented by the flags array. And the other is the queuelast variable, which is saying, what is this part that is available for you to queue yourself in this, in this particular array? So as you can see, since there is no lock request yet, we just initialized the queue, the first guy that comes around to ask for the lock will get it, and, and he will queue himself here and he will get the lock as well. So let's say some processor came along, and, and made a lock request. It's going to get it immediately because there's no locks request currently pending. And so it's got this position and it's got the lock and what will happen is that the queuelast variable will advance to the next spot to indicate that future requesters have to start queuing up from here. And now this current lock holder has got the lock and he can go off in the critical section and do whatever he wants in terms of managing or messing up with the data structure that is governed by this particular lock. Let's say that at some point of time, I come along and request the same lock. Now depending on who else got ahead of me at the point that I made that lock request, there may be some number of people that are lined up ahead of me and where ever queuelast is pointing is my place. And, and so this is where I'm going to queue myself, waiting for that lock, and of course queuelast will advance to the next open spot for future requesters that come after me. Now the important point that I want you to notice is that since that the array size is N and the number of processes is N, nobody will be denied. [LAUGH] Everybody can come and queue up waiting for this lock. Because since there are N processes at most N simultaneous requests can be there for the lock and everybody will get their unique spot to wait for if in fact the lock is currently in use. Given the timing of my lock request and the position of the current lock holder, you can see that I have some waiting to do, because there are a quite a few requests that are ahead of me, and so I have some waiting to do before I get my turn in in acquiring this particular lock. So now I can tell you how the lock algorithm is going to look like, pretty simple. When I make a lock request what I'm going to do is mark my place in this flags array and the way I do that is by calling fetch and increment on the queuelast variable. And that ensures that I get my unique spot due to the fetch operation and I increment the queuelast to point to the next spot which is available to the next spot for future requesters. And since fetch-and-increment is an atomic operation, remember that we have read modify write operations, fetch-and-increment is one of those. And it's an atomic operation and therefore, even though it's a multiprocessor there could be multiple guys trying to get the same block at the same time. They're all good to be sequenced through this fetch-and-increment atomic operation, and so there is no issue of any risk condition in that sense So, I will get my spot and I'll increment queuelast. And, of course, if the architecture does not support this fancy fetch and increment read modify write operation, then, you know, you have to simulate that operation using, using test and increment instructions. So once I've marked my position in this flags array, then I'm going to basically wait for my turn. So what I do in order to wait is I'm basically waiting for this spot that I've marked myself, it is right now must wait, it has to change to hl. Once it changes to hl, I know I have the lock, and therefore I'm going to do a spin on this particular location. and, and I'm going to wait for this, this location changing its value from mw to hl, so that's the spin loop that you see here. So basically once I have marked my position, I'm going to wait on my position becoming hl to know that I have acquired the lock. And, I will get it eventually, because that's the way this algorithm is supposed to work. So let's see what happens when the current lock-holder comes around to unlocking the lock. What he's going to do is, he's going to execute the unlock algorithm. And the unlock algorithm, the first thing that it does, is it sets this position that the lockholder had from HL to MW. And the reason for that is, is that this is a circular queue and since it's a circular queue even though queue last is here future requesters can, can come around and then eventually somebody may come here and may want to occupy this particular slot and they have to know that they have to wait. And that's the reason, the first thing that the current lock holder does, is to mark this spot that he used to be at, as hl. The next thing that the current lock holder is, is going to do is signal the next guy in the circular queue. So, the current lock holder was here, so you'd mark it as mw for future requesters that may come and wait on his spot. And the next request in the circular queue is the guy next to him. And therefore what he is doing is, he is saying you know, current plus one mode N, is going to be set to hl. And so, that guy would have been waiting in this position and so he'll get the signal. And therefore he will be getting ready to go. And he can get into the critical section and do whatever he wants to do with the Data structure that is protected by this particular lock. Now this will go on, and eventually, my predecessor will become the current lock holder. And when my predecessor is done using the lock, he'll come around to do an unlock and when the current lock holder who's my predecessor does the unlock operation, that's going to be resulting in a signal for me, because basically. He's going to set the flags array, the next spot in the flags array, as hl. And that's the spot I'm waiting on. So good news for me. I've got my position marked as hl, and what that means is that now I've got the lock. And now I can go off into the critical section do what I need to do in order to do the code that is associated with the critical section protected by, this particular lock out. Now that we understand that the lock and the unlock algorithm works with this array-based queuing, let's talk about some of the virtues of this algorithm. The first thing that you notice is that there is exactly one atomic operation that you have to carry out, put, put critical sections so,every time you want to acquire a lock you come in and do a fetch and increment and that is all that you do in order to get the lock. And so there's one atomic operation that you do per critical section, that's good news. And the other thing that you also notice is that, the processes are all sequenced in other words there is fairness uh, so whoever comes first. Gets into the queue ahead of me and when I come in if people are going to come after me they're going to get queued up after me. So that's good news also. And the spin variable we're going to mark my position in this array my spin variable is distinct from the spin variable of all the other guys that may be waiting for the same lock. That's another good thing. In other words, I'm completely unaffected by all the signaling that it will happen when the guys that are ahead of me were getting the lock and, and signaling the next guy and so on. I'm completely impervious to that because I'm spinning on my own private variable. Waiting for the lock. And of course correlating to what I just said is that whenever a lock is erased, exactly one guy is signaled to indicate that they've got the lock. And, and that's another important virtue of this particular algorithm. So, it is fair. And it is also not nice, so these are two things that very good things about this algorithm. And those we saw were you know the deficiency of the ticket log algorithm was exactly that where it is fair, but it is noisy when the lock is released. So that problem has gotten away with this queuing lock. Now you might be wondering, are there any downside to this array based queuing lock. I assure there is. The first thing I'm sure that you've noticed already is the size of the data structure is as big as the number of processors in the multiprocessor. So the space complexity [COUGH] for this algorithm is order of N for every lock that you have in the multiprogram. So if you have a large scale multiprocessor with dozens of processors, that can start eating into the memory space. So that's something that you have to watch out for. So the space can be a big overhead. And the reason I'm emphasizing that is because in any well-structure multi-threaded program even though we may have lots of threads executing in, in all the processors. At any point of time for a particular lock, they might not be in contention but all the processors, only a subset of them may be requesting the lock. But still, this particular algorithm has to worry about the worst case. Contention for a lock, and therefore it creates a data structure that is as big as a number of processes that you have in the multiprocessors. And that's the only downside to this, but all the other things are good stuff about this algorithm. And of course the reason why you have that downside with the, with this particular Anderson's queueing lock is the fact that the queue is being simulated by a static data structure, an array. And since it is a static data structure and you have to worry about the worst case contention among requesters for a lock we have to make this static array as big as the number of processors. So that's really the the catch in this particular algorithm. Next we will look at another algorithm, a lock algorithm that is also based on queuing, but it doesn't have the space complexity of Anderson's queuing lock. So to avoid the space complexity in the Anderson's array based queueing lock, we're going to use a linked list representation for the queue. So the size of the queue is going to be exactly equal to the dynamic sharing of the lock. And this particular linked list based queueing lock algorithm is due to the authors of the paper that I've prescribed for you in the reading list. Namely Mellor-Crummey and Scott. And so, sometimes this particular queueing lock is also referred to as the MCS lock. So the lock data structure. The head of the queue is a dummy node. It is associated with every lock so every lock is going to have this dummy node associated with it and will initialize this dummy node to indicate there is no lock requesters presently for this particular lock. So, this pointer is pointing to nil. Nobody's got the lock. And there are two fields for every q node for a requester. So every new requester is going to get this q node. And in this q node there are two fields. One field is the guarded field. And guarded is basically a bullion that, which, and, and it says whether I have the lock or not. If it is true, I've got it. If I don't have, if, if it is false I don't have it yet. And the next field in the queue note is pointing to my successor in the queue. So if I came in and I requested the log, I get into the queue. And if a, if a successor comes along and requests a log, he gets queued up behind me. So that's this basic data structure, every queue note Is associated with a requester. The dummy node that we start with is representing the lock itself. And since we are implementing a queue, fairness is automatically assured. The requesters get queued up in the order in which they make the request, and so we have fairness built into this algorithm, just like the Anderson's array-based queue lock. The lock to, to nil indicating there are no requests yet. And let's say that I come along and request a lock. I don't have to wait because currently, there's nobody in the queue and therefore I get the lock right away. And, and I can go off into the critical section and start executing the critical section code, that is associated with this particular lock. So what I would have done, when I came in to make this lock request. Is to get this q node. And make the lock data structure point to me. And I'd also set the next pointer to null, to indicate there's nobody after me. And once I've done that, I know that I've got the lock. And I can go off in the critical section, and do whatever I need to do. I was lucky this time that there was nobody in the queue when I, when I first came and requested the lock. But another time, I may not be that lucky. There may be somebody else using the lock already, and if that is the case, then what I would have to do is to queue myself in this data structure. And the way to do that, is to indicate by setting the last pointer, in this list to point to me. This pointer is always pointing to the last requestor. In this case, the original case that I showed you, I was the only requestor that was also last requestor. But now, the queue has somebody using that particular lock, and so when I come in, what I'm going to do is, I'm going to set this field of the lock data structure, the dummy load, the head node, of the lock data structure to point to me and the last requester. And I'm also going to fix up the link list so that the current guy is going to point to me. Why am i doing this? Well, the reason i do this is because when he is done using the lock, he needs to reach out and signal me. What am I going to be doing? I'm going to be spinning. And what am I spinning on? I'm spinning on the got-it flag. So this is a data structure that is associated with me, and one of the fields, you know, is the got-it field in the data structure. So I'm going to spin on this got-it field in the data structure, waiting for this guy to set it to two. So, I initialized it to false when I came in, and form this request. When I form this request, what I did was to set myself as the last requester, I'll clear out this field to indicate that I don't have the lock, and I'll set up the link list so that the current lock holder points to me through his next field. And my next field of course, is null because there is no requester after me. So once I fixed up this, link list and in this fashion, then I basically can spend on my got it a boolean variable. So now we can describe to you the lock algorithm. Basically the lock algorithm takes to arguments. One is, this name domino that is associated with this particular lock. And, and it's also taking my queue node, the one that I am providing, to say that this is my queue node, please queue me into this lock request queue. And when I make this call it could be that I'm in this happy state, in which case, I don't have any lock requesters ahead of me. But if it turns out that, when I come in there is somebody is using this lock, then I'm going to join this que. And has to be done atomically. There are two things going on here in joining this queue atomically. What I do is, I set the last pointer. This list is always pointing to the last requester. So, it used to point to this guy, he was the only requester. I came along, so we had to fix up this list so that this, and pointer is going to point to me, the last requester. And I also had to fix up, the current requestor point to me. And once I have done that, then I can await the predecessor, namely this guy, to signal me, by spinning on the got-it variable that is associated with my data structure. And the other thing that I would do as part of joining this queue is to set my next point at null, because there is nobody after me, I just made the lock call. Notice that when I'm joining this queue, I'm doing two things simultaneously. One is, I'm taking the pointer that was pointing to him and making it point to me. And I also need the coordinates of the previous guy so that I can set his next pointer to point to me. So I have to do this double act. So this has to be done atomically as well. So joining the queue, essentially, is a double act of breaking a link that used to exist here, make it point to me, and get the coordinates of this guy, so that I can fix him up. And remember that this is happening simultaneously. Perhaps with other guys trying to do the same thing, joining this queue. And therefore, this operation of breaking the queue and getting the coordinate of my predecessor has to be done atomically. And in order to facilitate that, we will propose having a primitive operation called fetch and store. And atomic operation, and the semantics of this fetch and store operation is that when you make this call and give it two arguments, L and Me. What this fetch and store is going to do is, it's going to return to me what used to be contained in L, so what used to be contained in L is my predecessor. So I'll get that, and I'll get the coordinates of this guy. And at the same time it's also storing into L a new node that is the pointer to the new node that is me. And so that is what is being accomplished by this. The double act that I mentioned of getting my predecessors coordinates and setting this guy to point to me is accomplished using this fetch-and-store operation. It's an atomic operation. And clearly the architecture is not having this fetch and store instruction you have to simulate that with a test and set instruction. So once I've done the double act, then I can set up the current node's next pointer to point to me. And then I'll be done with joining the cube and then I can await the predecessor to signal me. So, I'm spinning on the got-it variable. And how will I know that I've got the lock? Well, my predecessor who is currently using the lock will eventually come around And call this unlocked function. And the unlocked function is basically taking, again, two arguments. One argument being the name of the lock, and, and the other argument is the guy that's making the unlock call, in this case, the current node that's making the unlock call. And what it does is to remove current from. On the list and it is going to signal the successor. And the way the successor is going to be signalled is because the current node has an x pointer and the x pointer says he's the next guy waiting in line for getting this particular lock. And he's pinning on the got it variable. So he's just going to signal the successor. By setting the guarded variable for the successor to be true, and that will get me out of my spin loop, and I'll have the lock. And I'm now running inside the critical section having obtained the lock that's protecting the data structure associated with that critical section. So now I'm in my critical section. And eventually I'll get done with my critical section. When I get done with my critical section I have to unlock and I call the unlock function. Normally the unlock function involves me removing myself from this link list and then signaling the successor. So these are the two things I have to do. Remove myself from the list, and signal any successor. The special case occurs. When there is no successor to me. The special case when that occurs what I have to do is I have to set the headnode, the dummy node, of the link, link list, namely l to null to indicate that there is no request... Waiting for this lock. So that's a special case. And so if I look at this picture here, what I have to do is I have to set this L to null, and then I'll be done. I don't have a successor signal. But wait, there could be a new request that is forming. And if a new request is forming, now this guy what you would have done is To do a fetch and store. And, and if you did a fetch and store on this linked list, what would have happened is that he would've gotten my coordinates, and you'd have set the list to point to him. So the new request is forming, but it will not form completely yet. In other words, the next pointer in me is not pointing to this new request yet. And this is the classic race condition that can occur in parallel programs, and in this particular case, the race condition is between the unlocker, that is me, and the new requester that is coming to put himself on the queue. And such race conditions are the bane of parallel programs. And one has to be very, very watchful for such [INAUDIBLE] conditions. And being an operating system designer, you have to be ultra careful to ensure that your synchronization algorithm is implemented correctly. You don't want to give the user the experience of the blue screen of death. You have to think through any corner case that can happen In this kind of scenario and design the software in such a way, operating system in particular, to make sure that all sets of these conditions are completely avoided. Now, let's return to this particular case and see how we can take care of this situation. So if there was a new request that is forming, you know that the new request would have called the lock algorithm. And if you call this lock algorithm, and it actually executed this fetch and store operation, then you know that this link is no longer going to be pointing to me. But is going to be pointing to him, right? And that's what this fetch and store would have done. It is to give this new guy my coordinates, and it'll also set the linked list to point to him as the last requester. So that would have been accomplished through this fetch-and-store. So what I have to do, when I come in and try to unlock, that is, removing me from the queue. Even though my next pointer is nil, I cannot trust it entirely because it could be a successor that is forming, it's just that it's not that the formation of the list is not complete yet. So what should I do? Well, remember when I told you if I was the only guy, what I wanted to do was to set this guy to nil to indicate that there's no requesters after me. the, the list is empty. But before I do that, I have to double check if there is a request that is in the information. And, in other words, I want to have an atomic ray of setting this guy to nil, if in fact he's pointing to me. And the invariant in this case, is that. If he's pointing to me, I can set him to nil. If he's not pointing to me, I cannot set him to nil because he's pointing to somebody else. That's the invariant that I should be looking for, so I need an atomic way of checking for the that invariant. And the invariant is in the form of a conditional, store operation. The conditional store being. Do this store only if some condition is satisfied. Now in this particular case, I'm going to tell you a primitive that will be useful for this purpose. And that primitive is what is called compare and swap. It takes three arguements. The first two arguments is saying, here is L and this is me. Check if these two are the same. If these two are the same, then you set L to the third argument. The third argument is what L has to be set to if these two are the same. That's where it's called compare and swap. You are comparing the first two arguments, and if the first two arguments happen to be equal, then we are saying set the first argument to be equal to the third argument. So that's the idea behind compare and swap. So, essentially when I execute the compare and swap operation, on L, me, and nil. What I'm telling is to, to set this guy to nil if he's pointing to me. If he is not pointing to me, don't do that. So that's the idea behind compare and sway. So this compare and swap instruction is going to return true if it found that L and me, that first two arguments, are the same and therefore it set L to the third argument, in that case, it's a success and success is indicated by a true being returned by the operation. But on the other hand, if the comparison failed, it won't do the swap. It'll simply return false. So it won't do the swap, but it'll return false. So that's the semantic of this particular instruction. Again, this is an atomic instruction. And this atomic instruction maybe available in the architecture. But if it isn't, then you have to simulate it using test and set instruction. So in this particular example that I am showing you, when I try to do this unlock operation because this new guy has come in and he's executing, he's halfway through executing his lock algorithm. So he has done the fetch and store and, and he's going to set up the list so that my next pointer will point to him. So that's the process that he's in right now. So at that point, I'm coming in, I'm saying, well, I want to do the unlock operation, and that's when I found that my next pointer is nil. And so what I have to do is, do this compare and swap, and at the compare and swap, now it's going to return to me false, indicating that this particular operation failed. So once I know that this operation has failed, then I'm going to spin. And so the semantic of the unlock call is, I come in, remove myself from L. And in order to do that, I'm going to do this compare and swap on the linked list. And if I find that the compare and swap instruction fails, I'm going to spin. Now what am I spinning on? When will it become not nil? So basically what I'm going to do is I'm going to spin on my next pointer, being not nil. So right now it's nil. That's the reason that I think that there's nobody after me. I was going to set this guy to nil. But I know that compare and swap fail and therefore I know that there's a request information and I'm going to spin waiting for my next pointer to become not nil. Now when will my next pointer become not nill? Remember that this guy the new guy that is doing this lock operations doing exactly what I did earlier. Right? And, and what he's doing is he's gotten my coordinates and he is in the process of setting it up, so that my next pointer's going to point to him. So, eventually, he'll complete that operation. So my spinning is on this becoming not nil and it'll become not nil because of this new guy completing what he needs to do as part of this, lock operation. And, so, eventually the next pointer in, in my note will point to him and at that point I can come out of my spin loop. Now, I'm ready to signal the successor that hey, you got the lock. So, that's how I can make sure that when we unlock the corner case that occurs during unlock and that is there is no requesters after me, I can take care of that by doing this atomic and ensuring that there's no race condition between me the unlocker and a new requester that is in the process of forming through this lock call. So once this lock data structure has been fixed up nicely by this new requester, so far as I'm concerned, everything is good. I can, the list is good, and therefore I can go ahead and signal the next guy that he's got the lock and be done with it. I strongly advise you to look through the paper and understand both the link list version as well as the previous Anderson's array based lock version of the queuing locks. Because there are lots of subtleties in implementing these kinds of algorithms in the kernel and in the parallel operating system kernel. And therefore, it is important that you understand the subtleties by looking at the code. I've given you, of course, a description at a semantic level of what happens, but looking at the code will actually make it very clear what is going on in terms of writing a synchronization algorithm on a multiprocessor. And one of the things that I mentioned is that both the the link list based queuing lock as well as the earlier array based queuing lock required fancier re-modified write instruction. So for instance, in this case, we need a fetch and store, and in this case and also a compare and swap to fancier re-modified write instruct, instructions. And similarly the array based queuing log required a fetch and increment. Now it is possible that the architecture doesn't have that. If that is the case then you have to simulate these fancier read, modify, write instructions using a simpler test and sentence structure. So now let's talk about the virtues of this link list based queuing lock. Some of virtues are exactly similar to the Anderson's queuing lock, and that is it is fair. And so Anderson's lock was also fair, ticket lock was also fair. The linked list queuing lock is also fair. And again, the spin location is unique for every spinner, right? Every spinner has a unique spin location to wait on and so that is similar to the Anderson's queue lock as well. And that's good because you're not causing contention on the network when the lock is released. When one guy releases the lock, others if they're waiting, they don't, they don't get bothered by by the, by the signal. And exactly one processor gets signaled when the lock is released. That's also good. And usually, there's only one atomic operation per critical section. And the only thing that happens is this corner case. In order to implement this corner case, you have to use a second atomic operation. But if the link list has several members in this, in these examples. I'm just showing only two requesters at a time. But if the link list has a number of requesters, then if I am middle of the gang, have, using the lock, I simply signal the successor. I don't have to do anything fancy in terms of compare and swap. So this is something that needs to be done only for the corner case, not as a, a routine for doing the unlock operation. And the other good thing that we already mentioned is that the space complexity of this data structure is proportional to the number of requesters to the lock at any point of time. So it is dynamic. It's not statically defined as in the array-based queueing lock. And so that's one of the biggest virtues of this particular algorithm that the space complexity is bound by the number of dynamic requests to a particular lock, and not the size of the multi-processor itself. Now the downside to this link list based queuing lock of course is the fact that there is link list maintenance overhead that is associated with making a lock request or unlock request. And Anderson's [INAUDIBLE] queue lock because it is in a irregular structure can be slightly faster than this, link list based algorithm. And one of the things that I should mention to that is that both Anderson's [UNKNOWN] queue lock as well as the NCS link list based, queue lock. May result in poorer performance if the architecture does not support fancy instructions like this, because they have to be simulated using test and set, so that can be a little detriment to to this particular algorithm as well. We have discussed different algorithms for implementing locks in a shared memory multi processor. If the processor has some form of affection free operation, then the two flavors of queue based locks, both due to Anderson and MCS, they are good bet for scalability. If on the other hand, the processor only has test and set, then an exponential backoff algorithm would be a good bet for scalability. So we've completed discussing the lock-based synchronization algorithms for a multiprocessor. So how about we close this section of the lesson with a quiz? What I want you to do is, we've talked about several different algorithms, spin on test and set, spin on read, spin with delay. TIcket lock, Anderson's queue lock, array based, MCS link based queue lock. So these are the different algorithms that we've looked at. And along the way, I mentioned some of the attributes that we look for latency for getting the lock, contention when locks are released, fairness whether the spin is on a. Private variable or a shared variable. How many read modify write operations are required for acquiring a lock? And what are the space overhead associated with the lock? And when the lock is released, are we signaling one guy or are we signaling everybody? So all of these are different attributes that you can associate with these different. Block algorithms, and what I would like you to do is take your time and read the algorithms by filling this table, and of course, you should go back and look at these algorithms to get a clearer understanding if you find yourself not ready yet to fill out this table, so take your time. So I'm going to give you the solution for this particular question by filling out this table. And as I said, take your time thinking about it. And, and verifying your own intuition against what I'm presenting to you here. Now what you'll find is that MCS Link-based queue lock and Anderson's array-based queue lock are the two things, two algorithms that done, do quite well on most of the different categories of attributes that I have mentioned to you. But I should tell you that if you, if you have fancy instructions. Fancy read, modify, write instructions. Then Anderson's and MCS lock give you the best performance on all these attributes. But on the other hand, if the architecture does not support fancy read modified op operations and it only has testing set operation available, then some sort of a, a delay base, is a, in a exponential delay base or. Starting delay based, spin lock algorithm, may turn out to be the best performer. And in fact, when the amount of contention for lock is, is, fairly low, it's best to use a spin lock with exponential delay, start out a small delay and keep increasing it. On the other hand, if it is a highly contended lock Then it is good to use a Spin Lock that has categorically assigned various spots for every processor. And one of the things that I also want you to notice is that the number of re modify right operations that, you need to do for the different lock algorithms really depends on the amount of contention that is there for the lock in the case of spin algorithms. In the case of Anderson's and MCS the number of Atomic operation is always one, regardless of how much contention there is. And of course, in MCS, this is the quanta keys that you have to worry about during, during unlocking that might result in an extra remodified item operation. But in the case of the Spin algorithms the amount of contention is really dependent on the number of re modified item operations that you have to perform per critical section. Really depends on the, mode of, mode of contention that is there for the lock. In the previous lesson we looked at efficient implementation of mutual exclusion lock algorithms. In this lesson we're going to look at barrier synchronization how, how to implement that efficiently in the operating system. And just to refresh your memory about the barrier the barrier synchronization works like this, that you have a bunch of processors and they all need to know where they are with respect to each other. Where they want to, reach a barrier. And they want to wait here until everybody has arrived at this barrier. So if T1 arrives at the barrier, it's going to wait until everybody else has come. So one of the guys, maybe a straggler is going to come a little later, and in that case, everybody has to wait until all the threads that are part of this application have arrived at the barrier, then they can move on. And, and I mentioned to you that this kind of synchronization is very popular in scientific applications and they go through these phases where they execute code for a while, reach a barrier, and then execute code for a while, reach another barrier, execute four code for a while, reach a barrier and so on and so forth. And, and I mentioned also that in real life this happens quite often. When we go to a dinner with with a bunch of our friends and some of us show up early and others come late. The usher is going to hold us all, ransom. Wait 'til everyone is here. Until, until then I cannot seat you. So that same sort of this that's happening, with the barrier that all of the threads have to arrive at the barrier, only then they can proceed on. So that's the semantic of the Barrier Synchronization. And I'm going to describe to you a very simple implementation of this barrier. The first algorithm I'm going to describe to you as what is called a centralized barrier or also sometimes called a counting barrier. So centralized barrier, counting barrier, that's a name that, that's given to this. The idea is very, very simple. You have a counter, that's why it's called a counting barrier. You have a counter. And the counter is initialized to N, where N is the number of threads that need to synchronize at the barrier. And what is going to happen is that, when a thread arrives at the barrier, it's going to atomically decrement the count. A key thing is it has to be done atomically. So once is it atomically decremented and the count then, it's going to wait for the count to become zero. So long as the count is not zero, it's going to wait. So if the count is zero, we're going to do something else, but if the count is not zero that means that, I've arrived at the barrier, but I don't know where the others are yet. So I'm going to wait. So they're, they're going to spin and the spin is basically saying while count is greater then zero, spin. And all the processors except the last one are going to be doing this spinning on count becoming zero. Now the last processor, the straggler may be the T2's sta-, straggler. And the straggler arrives eventually. And when he arrives, then what he's going to do is he's going to decrement also. And when he decrements the count, he'll see that the count has become zero. And so what he will do is he'll reset the count back up to N. And that is indication that everybody, so, so all of these guys are waiting on count being greater than zero. So as soon as the count becomes zero, then they can, they can be released from the barrier. And the last processor to arrive is going to reset the count to N to indicate that when these guys go off, before they come to the next barrier, the count has to be N. So that's the idea behind that. So very simple algorithm. Decrement the count atomically when you come to the barrier. If the count is greater than zero, then you know that everybody has not arrived, spin. And everybody except the last guy will do the spin. And the last guy that comes around decrements the counter for, and, and the counter becomes zero. And once the counter becomes zero, all the guys that are stuck here, they're going to be released. And then the last processor will reset this count to N so that you know, all these guys are now on their way to the next barrier. So, it is resetting it to N so that the barrier can be executed again when all these guys get to the next barrier. And that's the idea behind the centralized barrier. Now, I'm going to ask you a question. Given this very simple, implementation of the barrier decrementing count and count becoming zero resetting it to N by the last processor and all the other guys waiting on the count not being not yet being zero, do you see any problem with this algorithm? And this is an open-ended question. So I want you to think about it and see, could this lead to any raise condition. And, and I mentioned to you when we talked about mutual exclusion algorithm itself that raise conditions are the bane of parallel programming. So, when you're implementing synchronization algorithms you better be absolutely certain that there are no raise conditions. The answer is yes there is a problem. And, and the problem is that the before the last processor, the last processor guy comes and sets the cone back up to N. And remember what the last processor is doing, decrementing the count. And if the count is zero, as soon as there is decrement of the count and the count is bigger than zero the other guys are sitting here. They're going to go off on their merry way, executing code towards the next barrier. And the last processor is, in the meanwhile, fitting the count backup to N. But before the last processor sets the count back up to N, the other processors may race to the next barrier. And they may go through, because they may find that this count has not been set to N, yet. And they will find that the count is zero, and then they'll fall through. And that can be a, another happy situation. Right? So there is a problem with the centralized barrier. That is that when, the count has become 0, if these guys immediately are allow-, allowed to go on executing before the count has been reset to N, then they can all reach the next barrier and then they fall through. And that is a problem. So the key thing to do To avoid this problem, or to overcome this problem, is to make sure that the threads that are waiting here, they don't leave the barrier before the count has been reset to N. Right? So they're all waiting here for the count to become zero, and once the count has become zero they are ready to go, but, we don't want to let the go yet. We want to let them go only after the count has been reset to N. So what we're going to do is, we're going to add another spin loop here. And that is, after they recognize that the count has become 0, they're going to wait till the count is not N yet. And so this ordering of these two statements is very important, obviously. So, we want to wait till the count has become 0. At that point we know that the value is over, but we want to make sure that the counter has been reset to N by the last guy, and once that has been done, then we are ready to go on executing the code that we need to execute til, til we get to the next barrier. So we solve the problem with the first version of the centralized barrier, and that is the counting barrier. By having a second spindle. That's the problem, right? There are two spin loops for every barrier in the counting algorithm, and ideally, we would like to have a single spindle. And and that's the reason that we have this particular algorithm, which is called sense reversing barrier. If you recall in the counting barrier, we needed two spinning episodes. The first spinning episode was When you arrive at the barrier, decrement the count, and wait for the count to become 0. That's the first spinning episode. And the second spinning episode to leave the barrier, what you need to do was to make sure that the count has become N, right? Those were the two spinning episodes that were there in the counting barrier. And in the sense reversal, Barrier, we're going to get rid of one of those spinning episodes. The arrival one. We'll get rid of it. So we don't have to spin on count becoming zero. And we'll see how that is done. So what you notice is that in addition to the count, there is a sense variable, in the shared variables that we have, we included a new variable called sense variable that's also shared by all the processes that want to accomplish a barrier synchronization. And the idea behind the sense variable is that the sense variable is going to be true for One barrier episode, and it's going to be false for the next barrier. So because we, at most you have one barrier at a time, and therefore, if you call this barrier the true barrier, the next barrier is going to be the false barrier So that's the way we can identify which barrier we are in at any particular point of time so far as a given thread is concerned by looking at the sense variable. So the barrier algorithm is going to work like this. When a thread arrives at a barrier, what it is going to do is, it is decrement the count exactly like, like in the counting barrier. It's going to decrement the count. But after it, its decrements the count, what it is going to do is, it's going to Spin on Sense reversal. Remember that, you know the sense flag is going to be True for. This barrier and once everybody has progressed to the next barrier the, the sense flag will become false. And therefore, let's say that we are executing the, the true barrier. In other words all the threads are executing some right here. The sense flag is true, and so if T1 comes along it decrements the count and it's not going to worry about whether the count has become zero or no. All that it is going to read on, is for the sense to reverse. So it's, it's saying well my sense is we are on the true value here I'll stay here until the sense becomes false. I'll know then that, that we've moved on to the next value point. That's the idea behind behind what all the processors will do except the last one. What did the last one do? Well, you guessed it. The last one, in addition to resetting the count to N, which was happening in the counting barrier, it was also going to reverse the sense flag. So, last processor comes along and finds that the count has become zero, it'll reset it to N. No problem with that. And then it is going to reverse the sense flag. It used to be True here, and it's going to reset it to False. And all the other guys are waiting on the sense reversal. So decrementing the count itself by chaining the, the count value, that doesn't do anything to these threads. Only when the sense flag is reversed, all these guys come out of the spindle and they can go on. So you can see now that we ha, have only one spinning episode per critical section or one spinning episode per Barrier. What we're doing is we decrement the count and spin on sense reversal, last guy decrements the count. When count goes to zero, resets it to N. And then it is going to reverse the sense. And that is the signal for all the reading processes to say well we can now go on to the next phase of the computation. So we've gotten rid of one of the spinning episodes that used to be there in the pure counting version of the centralized barrier. One of the centralized barrier is simple and intuitive as to what's going on and of course with the sense reversing barrier we got rid of two spinning episodes and got it down to to one. All of these are good things. But the problem is, that you have a si, shared variable for all the processors. And so if you have a large scale. multi-processor. And if you're running large-scale scientific applications with lots of parallel threads and they have to do a barrier, causes a lot of contention on the interconnection network. Because of this hot spot for this shared variable. And remember what our good friend Chuck Thacker said, less sharing means the multi-processor is more scalable. And that is something that we want to carry forward in thinking about how to get rid of this sharing that is happening among the large number of processes in order to build a more scalable version of a various synchronization algorithm. So I'm going to first describe to you a more scalable version of the sense reversal algorithm. And the basic idea is to use divide and conquer. I have a hierarchical solution. That is, limit the amount of sharing to a small number of processes. Let's say a small number K of processes and, and in this example, k is equal to 2. So essentially, you know, what we are saying is, if you have n processors that the condition, break them up into small groups of k processors. And so we build the hierarchical solution and the hierarchical solution obviously leads to a tree solution. And so, since we have K processors competing and accomplishing a variable among themselves. If you have N processors, then you have a log, and the base K as a number of levels in the tree, in order to achieve the value. And in this case, what we have done is K is equal to 2. And so, the number of levels and with the eight processors The number of levels in the tree is going to be three. So let's talk about what happens when we [UNKNOWN] a value. So, at a micro level algorithm works exactly like a sense reversing algorithm. And that is, these two processes if they're sharing this data structure at count Variable and a locksense variable and you see that for every k processes and in this case k being two, every two processes you have issued two shared variables: account variable and a locksense variable. Count variable locksense variable, count and locksense. And, so what's going to happen and you'll see that you have this count and locksense variable. Replicated in every level of the tree, and we'll talk about how these going to, are, variables are going to be used in the progression of this algorithm. So let's first talk about arriving at a barrier. So let's say that P1 has arrived at the barrier. What it is going to do is, it's going to go and decrement this counter. Now, what is this counter going to be set to? Well, This counter is, is just for the key processes that are value syncing here and keeping two this counter is going to be two. And so, this guy is going to decrement the count and if the count is not zero it's going to basically wait for the sense to reverse. Just like the sense reversal of algorithms. The same thing is going to happen that P1 comes here decrements the count and it waits for sense to reverse by spinning on this flag. Sometime later, P0 comes to the barrier and it decrements the count, count goes to zero, but you're not done with the barrier yet, because the barrier is for all of the processes. So what P0 is going to say is okay, between the two of us I know that we both have reached the value because the count is zero. But I have to go up, and go to the next level up and here I'm going to decrement the count here, to indicate that I've arrived at the value. So P0's the one that arrive up the tree, P1 is stuck here waiting for sense to diverse, P0 moves up. So remember that even though P0's come here decremented the count and made it zero, that doesn't flip the sense flag yet. Right? Because the value will be done only when everybody has arrived, and therefore all that P0 is going to do now is decrement the count, see that it is 0, then it is going to move up in the tree and go to the next level of the tree. And this data structure, which is now shared among this half of the tree this half of the tree is sharing this data structure, so P0 decrements this count. And what'll this count be resized to? Again, 2, right? Because at every level, you have k processors, k being 2 in this case, arriving at a barrier. So P0 arrives here, decrements the count, count is not 0 yet, and so it waits. So P0 is going to wait on locksense to reverse here. P1 is waiting on locksense deliveries here P0 is not waiting on locksense deliveries here because it has arrived at the barrier but his partners are still stragglers, they have not arrived at the barrier yet. Of course, multiple processors can arrive at the barrier at the same time and all of them are going to work with their local data structure. So, like, this guy will work with this local data structure. This guy with this local data structure. With this local data structure. And each of them is waiting for his partner to arrive so that he can move up the tree. So that's what going on and so eventually, P3 is going to arrive and so when P3 arrives, he decrements the count, sees it as zero so he can move up the tree. When it comes here it says, oh the count is already one so I decrement it and the count becomes zero and remember P0 decremented the count and it is waiting on locksense. So P3, when it comes here, finds that the count is one, decrements it, becomes zero and it moves up the tree because the barrier is still not done until we know that everybody has arrived at the barrier. So in the meanwhile, on this half of the tree, what's going on is that P4 has arrived, P5 is not there yet, P6 and P7 have arrived. And it turns out that P6 was the last guy to come to the barrier here, and therefore, he is the guy that has moved up. And he has decremented count. And he's waiting for this half of the tree to arrive at the barrier. And you can guess which one is going to come up, right? Because P4 has already arrived here, and so if P4 has already arrived here, he's decremented the count, and he's waiting on locksense to flip. So the straggler in this whole seam, scheme of things, is this guy right here. He's the guy who is, is still not arrived, but eventually he'll also arrive. When he arrives, he will decrement the count, find that the count has become zero, move up the tree, and he'll find that this count is already decremented also, and when he comes up here, he will decrement it to zero, and then he'll say, oh, if we're all done, so we can move up here. So, that's what is going to happen. So we come here, P5 comes here and goes all the way up. And then when it comes up here, it sees that P3 has already decremented the count to one. And so when he comes up, he decrements it, and it becomes zero. And at this point, everybody has arrived at the barrier. So let's understand what each processor does. When a processor arrives at a barrier it is going to decrement the count. If the count is not zero, it's going to spin on this locksense flag. If a processor arrives at a barrier, decrements the count, finds that the count is zero, then what it's going to do is one of two things. The first thing it's going to do is, he's going to say, do I have a parent? If I have a parent, what I have to do is, I have to recurse. Do the same thing to the next level. Right? So, so the algorithm is, decrement count and see if the count becomes zero. If the count has become zero, then you recurse. If end of the parent is there, you recurse. If the count does not become zero, then spin on the local locksense flag. And you continue this. So you continue this P0, that this came up here and informed this is another parent. So so this, you know, it, it is, it is, it is stuck here. But P3, when it came later on, it moves up. And when it came up here, this is the last part. So there's no more recursing here. So when P5 finally arrives here, it finds that there is no more parent. This is the root of the tree. And since we reached the root of the tree, you know that if the count is zero now at the root of the tree, then everybody has arrived at the barrier. So count at the root of the tree becoming zero is indicative to the last arriving processor, P5 in this case, that everybody has arrived at the barrier, so it's time now to wake up everyone. So the last processor to arrive at the root of the tree, in this case P5. He's the guy who is going to start the waking up process for everyone, and the way the wake up process works is that P5, having realized that he has reached the root of the tree and having realized that he's the last one to arrive, because count is already zero after you decremented it, he's going to flip this locksense flag. So, when he flips this locksense flag, what's going to happen? Two things, one is this guy, P3, he's waiting on this locksense flipping. So he's going to be released from the spin he's on. Of course, P5 has reset the count back up to n to prepare for the next barrier and it has flipped the locksense. So freeing up P3 and it is now ready to go down the tree as well to tell his buddies that the barrier is done and wake up everyone along the way. So the wakeup starts from the root. And, so in this case P5 and P3 having been released from the root, they go, come down to the next level. And they're going to wake up their buddies that are waiting at this level of the tree. Remember I told you that this can be a kauri tree. K happens to be two in this case. But for any general K, basically at every level of the tree, there's going to be on K minus 1 buddies waiting here, K minus 1 buddies waiting here. So what we're going to do is we're going to release that many number of prisoners from every level of the tree. So this is the zeroth level of the tree. There's the first level of the tree. There's the second level of the tree. And at the zeroth level, there is k minus 1 buddies. At the first level, there is k times k minus 1 buddies waiting. And similarly as you go down the different levels of the tree, there're more and more buddies waiting to be released. So for this simple example, with the K equal to two, so when he comes up here, comes down to this level, P3 is going to release P0 and P5 is going to release P6. And so now we have more helpers, to go down the tree and wake up more people. So at this level only P5 was there to wake up P3, and at this level both P3 and P, P5 are there to wake up the respective buddies, P0 in this case, and P6 in this case. So once P0 and P6 have been woken up, there are four of them now available that can go down to the next level of the tree. And they can go down to the next level of the tree. P0 can wake up, his buddies at this level of the tree, P3 his buddies at this level, P5, and P6. And so now all the others, so P1 in this case P2 in this case, P4 in this case, and P7 in this case, were all been waiting at this level of the tree, they will all get awakened because of these guys marching down from the root. And basically what each of these guys are doing on the way down is to flip this locksense flag. So the first thing that P5 did was to flip the locksense flag over here. That released this guy. And when, when P3 and P5 come to this level of the tree, each of them respectively flip the locksense flag that is associated with this data structure and when they do that, P5 release P6, P3 release P0 and now both P0, P3, P5 and P6 on this side. They all can go down to the next level. And P0 can flip locksense over here, P3 can flip locksense over here, P5 over here, P6 over here. That is going to release the rest of the buddies, P1, P2, P4, and P7. And everybody has now been released from the barrier, and that signals that the spin is done for all the ba, the processes that I've been waiting, and, the barrier completion is complete. So once, these locksense flags have been flipped, then all of the processors that have been waiting on these locksense as respective nodes, they're going to be released and everybody is now awake. So the tree barrier is a, a fairly intuitive algorithm that builds on the simple centralized sense reversal barrier except that it breaks up this And processes into K-Sized groups, so that they can all do spinning on a less contentious set of shared variables. So that's good, that is a, it's a recursive algorithm that builds on the centralized sensor [UNKNOWN] algorithm, and allows scaling up to a large number of processes. Because the amount of shading is limited to k, and so long as the k is small, like two or four, then the amount of contention for shared variables is limited to, to that number. So that's, those are all good things about that, but there are lots of problems as well. The first problem that I want you to notice is that the spin location is not statically determined for each processor. So for instance, if you take this particular execution that I've shown you in this picture, P0 happens to arrive later than P1. So P1 is the first to arrive here and so when P1 arrived here, it decremented count and it realized that oh, the count is not zero, I'm going to spend here. And P0 arrived later. And that's why it went up to the next level. And, and it is spinning on this locksense variable over here. So, in another execution of the same program, it is possible that P0 arrives first. If P0 arrives first, then it'll spin on its locksense variable that is in this data structure. And p one will be the second guy to arrive, and therefore, he'll be the guy that will move up. And he'll be the guy that will be spinning on this locksense flag. So, the locksense flag that a particular processor is going to spin on, is not statically detemined. But it is dynamically determined depending on the arrival pattern of all these processes at a barrier. And the arrival pattern, obviously, is going to be different for different runs of the program. Since it depends on the amount of code that is getting executed on each one of these processors. And and other variables such as how busy the processor is and so on. And the second source of problem, is that the airiness of the tree determines the amount of contention for shared variables. I've mentioned that, you know, here it is show, shown with two, two processors. But if you increase the airiness of the tree to be key to be something more than to maybe four or eight or something like that. And if you have a large-scale multiprocessor with with 1000 processors the editors of the tree may be much more than 2, and in that case, the mode of contention for said data structures is, is going be significant and that can result in more contention on the network as well. The other issue with this Tree Barrier is that it depends on whether our multiprocessor that we are executing this algorithm on is cache coherent or not cache coherent. If it is cache coherent multiprocessor, then, you know, the spin, even though it's on a particular variable, it could be encached in a private cache, and therefore, the cache coherent hardware will indicate when the spin variable changes value. But if it's a non cache coherent, multiprocessor, the fact the spin variable that we have to associate with a particular processor is not static, but dynamic. Means that the spin may actually be happening for P0 on a remote memory. Remember I mentioned to you that one of the styles of architecture is a distributive shared memory architecture? Sometimes the distributive shared memory architecture is also called a non-uniform memory access architecture, or NUMA And the reason it is called numer architecture is because the access to local memory for a particular processor is going to be faster than the processor's access to a remote memory. And if you don't have cache coherence, then the spinning that has to be done, has to be done on a remote memory, and that goes through the network. And so static association of the spin location of the processor is very, very crucial if it's a non-cache-coherent shared memory machine. So the next algorithm that I'm going to describe to you is due to the authors of the paper that we are reviewing in this lesson, which is [UNKNOWN] and Scott, and for this reason, that algorithm is going to be called MCS barrier. It's also a tree barrier but you'll see that in the MCS algorithm, the spin location is statically determined as opposed to the dynamic situation that you have in the hierarchy of the tree barrier here. So the MCS tree barrier is, it's also a tree barrier. It's a modified tree barrier, and what you'll notice, and once again, to make life simple, I'm showing you an arrangement of the MCS tree barrier with with 8 nodes. And it's a 4 ary arrival tree. So the arrival tree and the wake-up tree are different in the MCS algorithm. The arrival tree is a 4 ary tree, and I'm showing the arrangement for N equal to 8. There are 2 data structures that are associated with every parent, and and this one data structure is what is called have children, and the other data structure is what is called child not ready. And I'll describe to you what each one of these things is. Have children is a data structure that is associated with every node. And this data structure is going to have meaning only when a node is also a parent. So for example, if you look at this arrangement, node P0 has 4 children, P1, P2, P3 and P4. And if you look at node P1, it has 3 children. And so, P5, P6 and P7, has 3 children. And so we have a total of 8 processes, so we've got all 8 processes accounted for here. And therefore, these guys, P2, P3, P4, all the way up to P7, they're not as lucky as P0 and P1. They don't have children. So P2 through P7, they do not have children. And therefore, their HaveChild vector is, is, is false. So what you see here is a HaveChild vector and the HaveChild vector is, is true for P0 in all the big positions. And indicating that it has because it's a 4 ary tree, it can potentially have up to four children. And yes, P0 has 4 children. And the have child vector is true all the way, whereas, for P1, the have child vector is true for the first 3 children and false for the fourth because it has only 3 children. And these guys don't have any children. And similarly, these guys don't have any children. So, the HaveChild vector is completely false for P2 through P7. Now what about this Child Not Ready data structure? The Child Not Ready data structure is a way by which each of these processes has a unique spot in the parent to signal when they are arriving at a barrier. So what I'm showing you here, the arrows here are showing you the specific spot in this data structure, the child not ready data structure associated with his parent, for each of the child, there is a unique spot for this guy to indicate that they've arrived at the barrier. And similarly, for this set of children, the parent is P0 and each child has a unique spot in the parent's child not ready vector to indicate that they've arrived at the barrier. So the black arrows in this structure that I'm showing you is just showing the arrangement of the tree. And in terms of the, the parent child relationship for the 4-ary arrival tree. And the red arrows are the ones that are showing you the specific spot where a particular child is going to indicate to the parent that they have arrived at the barrier. And as you can see that since P1 has 3children, the fourth spot is empty indicating it has to wait only on 3 children to know that the value is completed on the tree and so it can move up. So, the algorithm for barrier arrival is going to work like this. When each of these processors arrive at a barrier, what they going to do is they going to reach into the parent data structure, very specific spots, statically determined. That's important, right? So it's statically determined that this is a spot that P5 is going to indicate to the parent that it has arrived. This is the spot that P6 is going to indicate that it has arrived. P7, and similarly, once all these guys have arrived at the barrier, P1 can check, and the way P1 checks is, just sees whether this CN vector has ones in all these pods. If there is ones in all these pods, it can spin on this, and therefore, it knows that its children have arrived at the barrier. Once its children have arrived at the barrier, then it can move up the tree similar to what we saw in the vanilla tree barrier before. P1 is going to move up, and it's going to inform its parent. And the way it does is by going to a specific spot in the parent's child not ready vector. And there is specific spot assigned for P1. It's going to set this to indicate that it has arrived at the barrier. So what P0 is doing is waiting on everybody arrive. If P0 is the first let's say to arrive at the barrier. It's basically waiting on everybody else to arrive at the barrier. Could be P0 is the first one or the last one, it doesn't really matter. When P0 arrives at the barrier, it is going to wait on this child not ready all the bits being set by the children. And so, when each of these nodes arrive at a barrier, they know because of the arrangement of this data structure, they know their position in the data structure relative to other processes arriving at the barrier. And therefore P2, when it arrives at a barrier, It knows that all it has to do, given the structure, it has to go to this part on the parent vector and set it to 1. P3 has to go to this part set it to 1 and so on, okay? And so once it is done, P0 will know that everybody has arrived at the barrier. So, that's the arrival at the barrier. So once again, the recap. The arrival tree is a 4-ary tree. And the reason why they chose to use a 4 ary tree is because there is a periodic result backing the use of 4 ary tree leading to the best performance, and that's the reason that they, they chose this particular arrangement. And the second thing that I want you to notice is that each processor is assigned to a unique spot by construction, a unique spot in this 4 ary tree. And because of its unique spot, a particular process on may have children, or may not have children and in this case, I showed you that P0 and P1 are have children, and the rest are not as lucky, because N is equal to 8. The other nice thing about this particular arrangement is that in a cash coherent multiprocessor, it is possible to arrange so that all the specific spots that children have to signal the parent can be packed into one word of a processor and therefore, a parent has to simply spend on one memory location in order to know the arrival of everybody, so it doesn't have to individually spend on memory locations for different processes, they can all be packed into one word, and the cash coherence mechanism will ensure that P0 is alerted every time any of these guys modify this shared memory location. So the wakeup tree for the MCS barrier is a binary wakeup tree. Once again here, there's a theoretical result that backs this particular choice that the shortest critical path from the root to the last awakened child, is shortest when you have a binary wakeup tree, and that's the reason that... They chosen to have this construction. Even though the arrival tree's a [INAUDIBLE] tree. The construction for the wake up tree is a binary tree. And let me explain the construction of this binary wake up tree. Every processor is assigned a unique spot again. So P 0 the root And uh,P1, P2 over here, P3, P4, P5, P6, and P7 So that completes the eight processes for this binary tree set-up for wakeup. And the latest structure that is used in the wakeup tree is as a child pointer data structure. And the ChildPointer data structure is essentially a way by which a parent can reach down to the children and indicate that that it is time to wake up. So, that's the purpose of this ChildPointer data structure. And, once again, as you can see, depending on the particular location in this wakeup tree, they may have children, they may not have children. So, P0 has two children, P1 has two children, P3 and P4. P2 has two children, P5 and P6. P3 had one child, P7, and that is it. Because you have a processors, and these guys. Don't have any children P4, P5, and P6. So in terms of wake up, what is going to happen is that when everybody arrives at the barrier P0 is going to be noticing it, and through the arrival tree. And so now it says oh, it's time now to wake up everybody, and the way it does that, it has a specific pointer To reach into P1 and signal to P1 that it's time to wake up. And similarly it has a specific pointer in, in, in, in P2 to wake up. So a particular memory location, which is a pointer to a location that this guy's waiting on to wake up. So it's going to do that. And so what is going on is that agian, this is another important point that in order to, to know that it is time to wake up, each one of these processes is standing on a statically determined location. P2 is standing on a particular location here, and, and P1 is standing on a particular location here. And so when P0 signals P1 it is exactly sending a signal to P1 and it is not affecting any of the other processes. And similarly, when it signals P2 it signals exactly P2 using this pointer. And similarly, once P1 and P2 are woken up. They can march down the tree and signal P3 and P4, and signal P5 and P6 by using the, the statically assigned spots that the children are spinning on to indicate that it is time to wake up. So, the key point I want to stress again is the fact that. In this construction of the tree, by design. We make sure that we know a position in the tree and we know exactly the, the memory location that we have to spin on, in order to know that it is time to wake up. So these red arrows show the specific location that is associated with each one of these processors In the wakeup tree. So once the parents signal the children and they marched down and signal all the other children, then at that point, everybody's awake, and the barrier has been reached. So the key take, take away points with the MCS tree barrier is that the wakeup tree is binary. The arrival tree is forwarding and the static locations associated with each processor, both in the arrival tree that we saw earlier, and the /? tree. And through the specific statically assigned spot that each processor can spin on, we are making sure that the amount of contention on the network is limited. And also by packing the variables into a single data structure we can make sure that the contention for shared locations is minimized as neat as possible. Okay, the next value algorithm we're going to look at is what is called the Tournament Barrier. The barrier is organized in the form of the tournament with N players and since it's a tournament with N players and two players playing as, against each other. In every match there are going to be log N rounds, log N with a base 2. So here is the setup for with [INAUDIBLE], they're going to be, they're going to be, three rounds corresponding to login. And being eight we get three rounds. The first round, second round and the third round. So in the first round, they're going to be four matches. P0 and P1 is one match. P2, P3. P4, P5. P6, P7. And the only catch is that we're going to rig this tournament. In other words what's going to happen is that we're going to predetermine who is going to be the winner in this round. And, and particularly, we're going to say P0 is the winner for this match, P2 for this one, P4 for this one, and P6 for this one. So in other words, the matches are rigged. In this day and age, when we hear about international scandals about match fixing. I guess this is not too far fetched. But what is the rational for match fixing? The key rational is the fact, that if the processes are executing on a shared memory machine. Then the winner can basically sit on his bumper and wait for a process of P1 to come over and let him know that he has won the match, P2 can wait until P3 comes over and so on and so forth. And what that means in a shared memory multiprocessor, is that the spin location where P0 is waiting for P1 to come and inform him that he's lost the match is fixed. The static. And, and so this is the idea behind match fixing, that the, the spin location for each of these processes, P0, P2, and P4, and P6, the winners in the first round, is predetermined. And that is very, very useful, especially if you don't have a cache coherent multiprocessor. If you have NCC [UNKNOWN] machine, In that case it is possible to locate the spin location, in the memory that is very to P0 P2 P4 and P si, P 6 respectively. That's the idea behind this this match fixing. So the result of matches. Of course P0 will advance to the next round. P2 will advance to the, next round. P4 and P6. And once again, in the second drawing we're going to fix the matches. And the winner is going to be P0 for round 2. P4 for in this bracket for round 2. And so essentially what that means again, is that P0 and P4 can spin on a statically determined location. In various processors and P2 and P6 respectively will come over and let the other guy know that when the match [UNKNOWN] for this round. So that is the end of the second round. And, and, and, of course, if you have you know, with N equal to eight, there are only three rounds but, if, for arbitrary N, we're going to have more levels in the tree and the and the, and, and, and [INAUDIBLE] level. We're going to fix the the winners and, so it'll propagate up this tree in this fashion, in terms of determining statically, who are going to be the winners for each round of the tournament. And this will go on, all the way up to determining who the tournament champion is. So in this case, P0 is our luck guy, who wins the tournament and so he's the champion. And so P0's going to be waiting on a statically determined location, where P4 can come and signal that P0 has one determinant. So again, the important thing that I want you to get out of this this particular arrangement that I've mentioned is the fact that the spin location for each of the processors that are waiting on the other guy are statically determined. At every level. So this the first round, the second round, and finally the championship, the championship round. So at this point, when p0 is declared the champion of the tournament, what we know is that everybody has arrived at the barrier. And this knowledge is available with p0 but not with anybody else. So everybody, everybody has arrived at the barrier, but P0 is the only one who knows because he's a champion, he knows that, that everybody has arrived at the barrier. So clearly, the next thing that has to happen is of course free up all the processors to indicate to them that you know, its time to move on to the next phase of your computation. So let's talk about the wake up. So what p0 is going to do is is going to tell p4 that it's time to wake up. And you know, if you want to use the tournament analogy again, in any tournament the winner walks over to the loser and shakes hands, right? So, you can sort of think of the same thing happening over here, P4 with the [INAUDIBLE] face is waiting for P0 to come over, and let him know that okay, its a good match and shake hands with you. And so, P0 is going to come over and let him know, shake hands. So that's the first thing that happens. So in other words, at this point P0 is awake of course, and he is also waking up P4 saying that well barrier is done. And now one of these guys can go to the next level and do the honours at every level so, just as I said about P0 coming in and shaking hands with P4, what P0 is going to do is, go to the next round and shake hands with P2, P4 go to the next round and shake hands with P6 and, and so on. And of course, if you think about the analogy of a tournament, as soon as the match is over, the winner is going to shake hands with the loser. But in this case, the winner shakes hands with the loser after the tournament is all done. So at every level, we're going to have that. So, its essentially, P0 and P4 come down to the next level and they shake hands with the respective losers of that level. And as I said, if we have for some arbitrary N, where N is a binary power, you're going to have this kind of propagation of wake-up signals going from the winner to the loser at every round. And all of them wake-up and go to the next level. Because all of these guys are winners from the previous level. So, all of these winners will go down to the next level and wake up the losers at that level. So that's what is going to happen. Again, what that means from the point of view of a shared memory multiprocessor is that the spin location for P4, P2, and P6, it's all fixed, right? Statically determined. If P4 knows that P0 is going to come over and shake hands, and so that he can spin on a local variable that is close to it's processor, and so again this is important for NCC NUMA machines in which there is no cache coherence and therefore it is convenient if P4 can be spinning on a memory location that is close to the processor. Same thing with P2 and P6 at the next level. So this process of waking up the losers at every level goes on till we reach round 1. And when at round 1, all the winners have congratulated. Well, not congratulated, [LAUGH] but shook hands with the respective losers at the first round. At that point, the wake up is complete. Everybody's awake now. And, and, the, the barrier is done. So all are awake, and the barrier is done, and they can move on, the next phase of the computation. And once again, in order to make sure that there is sense reversal, everybody knows that this barrier is done, and they're going to go to the next phase of the computation where they will wait on the different [INAUDIBLE] of the barrier. . So, that's Tournament Barrier Algorithm. So the 2 things that I want you to take away is, the arrival moves up the tree like this, with match fixing. And all the respective winners at every round, waiting on a statically determined spin location. And similarly, when the wake up happens, the losers are all waiting on statically determined spin location in their respective processors and the winner comes over at every level at every round of the tournament, the winner comes over and tells the loser that it's time to wake up. So that's how this whole thing works. So now that we understand this tournament algorithm let's talk about the virtues of this algorithm. You will immediately notice that there's a lot of similarity between the Tournament algorithm and the the sensor verse, interversing tree algorithm and also similarity to the [UNKNOWN] algorithm. So lets talk about the difference between the tree barrier and the tournament barrier first. So the, the, the main difference is that in the tournament barrier, the spin locations are statically determined, whereas in the tree barrier we saw that the spin locations are dynamically determined based on who arrives at a particular node in the barrier, in the tree in that algorithm. And what that means in the tournament barrier is that we can statically assign. The spin location for the processes at every round of the tournament. Another important difference between the tournament barrier and the the tree barrier, is that there is no need for a fetch and free operation. Because all that's happening at every level. At every round of the tournament, there is spinning happening. And what is spinning? Basically reading. And there is the signaling happening, what is this? This is just writing. So, so long as we have atomic read and write operations in the multi-processor, that's all we need in order to implement the tournament barrier. Whereas uh, if you recall in the tree barrier we need fetch and free operation in order to atomically decrement the count variable. So that doesn't exist in the tournament barrier. That's, that's another good use. Now what about the total amount of communication that is needed. Well, it's exactly similar because of the tree arrangement. As you go up the tree the amount of communication that happens is going to decrease. Because the tree is getting pruned as you go towards the root of the tree and so the amount of communication in the tournament barrier in terms of all the notation is exactly similar to the tree barrier it is older, login. That's the amount of communication that is needed. Now the other important thing that that I should mention is that at every round of the tournament you can see that there, there's quite a bit of communication happening. In the first round going up the tree, P1 is communicating with P0, P3 with P2 and so on. All of these red arrows. Are parallel communications that potentially take advantage of any inherent parallelism that exists in the interconnection network. So that's good news. That, that all of this communication can happen in parallel if the interconnection network allows that kind of parallelism. That can be exploited. And the, the other important point that I want you notice is that the tournament barrier works even if the processor is not a shared-memory machine. Because all that we're showing here is a message communication. So P1, P0 is waiting for a message from P1, and so on. So all of these arrows you can think of them as messages. And so even if the processor the multiprocessor is a cluster, well by a cluster what I mean is a set of processes in which the only way they can communicate with one another is through message passing. There is no shared memory, no physical shared memory. And even in that situation the, the tournament barrier will work perfectly fine to implement the barrier algorithm. Now let's make a comparison of tournament to to NCS. Now because this tournament is arranged as a tournament there are only two processes involved in this communication at any point of time in the parallel. So it means that it cannot exploit the spatial locality that may be there in the caches. If you recall, one of the virtues of the NCS algorithm is that it could exploit spatial locality. And that is, multiple spin variables could be located in the same cache line and, and the, the parent for instance could spin on a spin location to which multiple children are going to come and indicate that they are done. That's not possible in the, the tournament barrier because it is arranged as a tournament where there are two players playing against each other in every match. Similar to MCS, Tournament Barrier does not need a fetch and fee operation, so that's good. A common good property of both MCS and Tournament. The other important thing what tournament has an edge over MCS is the fact that tournament barrier works even if the processors are in a cluster. Meaning, it's not a shared memory machine and is only a cluster machine where only message passing is really good communicator to one another. Even in [INAUDIBLE] that situation, you can implement the tournament barrier. So that's, another good thing. Now is a good time for me to mention to you. I've been using the word cluster what that means is that the set of nodes in the multiprocessor they don't physically share memory and the only way they can communicate with one another is through message passing. And is important for you to know this particular terminology cluster because clusters become the work horses for data intensive computing today. The data centers and content distribution networks we're going to see a lot of that when we talk about giant skin services later on in this course, and those environments, they all use this kind of a, a, a computation cluster. And these computation clusters employ on the order of thousands or 10000 nodes connected together through an interconnected network and they can operate as a parallel machine with only message passing as the vehicle for communication among the processes. The last barrier algorithm I'm going to describe to you is what is called a Dissemination Barrier. And it works by information diffusion in an ordered manner among the set of participating processes. And, what you will see is that it is not pairwise communication as you saw in the tree barriers and the NCS barrier or the tournament barrier. But it is through information diffusion. The other nice thing about this particular barrier the dissemination barrier, is that it is since it is based on ordered communication among participating nodes. It's all like a, a well-orchestrated gossip protocol. And therefore, N need not be a power of 2. And you will see why this condition need not be satisfied as I start describing the algorithm to you. So what's going to happen is that, there's going to be information diffusion that's going to happen among these processors in several different routes. And in each round what's going to happen is that a processor is going to send a message to another ordained processor. And the particular processor that it's going to choose to send it is dependent on the round which we're in. So, the idea is that processor Pi will send a message to processor Pi plus 2 to the k mod N. This is the peer to which Pi is going to send a message to. And of course you know an example is always more illustrative. So since we have five processors here, we can then figure what's going to happen in every round. And Round 0 k is going to be zero. So what's going to happen, is that since k is zero, Round 0, P0 is going to be sending a message to Pi plus 2k, k being zero, is going to send it to P1. So, P0 sends a message to P1. Similarly P1 sends a message to P2, P2 sends to P2 to P3 and P3 to P4. And the arrangement is that this is cyclically arranged. That if before the neighbor for him is going to be in the cyclic order whoever is the next neighbor. So, in this case since there is mod function that we using before is going to be sending Its message to processor P5 mod. N, N being, N, N being 5, it will be sending the message to P0. So this is Round 0 of the communication. So the key thing that I want you to get is that in every round, we're going to see more rounds in a minute in every round a processor is sending a message to an ordained processor based on their own number. But depending on their own numbers, their own number zero, P0 sending to P1 and so on and so forth. And that is what's going on. So this completes one round of gossip. And what you, what you want to see it that. All of these communications that I'm showing you are parallel communications. They're not waiting on each other. So P1, whenever it's ready to arrive at a barrier it's going to tell the next gay, P2 is going to tell the next guy when hes ready and so on. Now how will these guys know that Round 0 is done well, if you take any particular process here let's say P2, as soon as it gets a message from P1 and it has sent a message to P3. It knows that Round 0 has done so far is as P2 is concerned, it can progress to the next level, or next round of the dissemination. So, each of these processes are independently making a decision that the round is over based on two things. One is, they have sent a message to the peer and they want to receive the message from the ordain neighbor that they're supposed to get it from. At the end of that, they can move on to the next round. Now how many communication events are happening in in every round? Well, it's order of N communication events per every round, because every processor is sending a message to another processor in, in every round. And therefore, the amount of communication that's happening is order of N, where N is the number of processors that are participating in this barrier. So now you can quickly see what's going to happen in the next round, and the next round, k is going to be equal to one and therefore. I'm going to choose, each processor is going to choose a neighbor to send the message to based on this formula that I've, that I have here. So in round zero, for instance, what we did was, P0 is sending a message a neighbor that is one distant from it because k was zero. And now. In round one case one and therefore P0 is going to be sending a message to a neighbor that is two distant from it. So P0 will send to P2 and similarly P1 two distant from it P3, P2 two distant P4. P4. Too distant from it, in cyclic order, is going to be P1. So it's sending a message to P1. So that's the round one of communication with k equal to one, round one of communication. Where once again, order of N messages are being exchanged among these processes to indicate that this round is complete. Just as I said about Round zero, every processor will know that this round is complete when it receives a message from its ordained neighbor. So in this case, P2 is going to expect to receive a message from P0, and it has also sent its message to P4, its ordained neighbor to which it is supposed to send the message in this round. Once it is done, P2 knows that round one is over and it can progress to the next round. So the independent decision is being made by each one of these processes in terms of knowing that this particular round is over and they can progress to the next round of the dissemination barrier. And just as I mentioned in the previous round. All of these communications happen in parallel, so if the interconnection network has a redundant parallel path these parallel, these parallel paths can be exploited by the dissemination barrier. In order to do this communication very effectively. So the next round, meaning round number two. K is equal to two, and therefore, what we're going to do is, every one of these processors is going to be choosing a neighbor that is four distant from itself. So in other words, P0 is going to send a message to its neighbor that is four distant, that is, P4. P1 is going to send it to four distant. Which means if you wrap it around, it's going to be P0 and so on. So this is the communication that's happening in round two where every processor is sending a message to its neighbor who is four distant because gave with the two four distant from itself. So just sort of biz, belaboring the point the gossip in round two. Is over, so far as P3 is concerned, when it has received a gossip message from its four distant neighbor, which happens to be P4. And it has also sent a message to its four distant neighbor, P2 in this case. At this point, every one of these processes knows that round two of gossip is compelte. And, similar to what I've been emphasizing all along, parallel communication pads in the interconnection network, can be fully exploited, by the dissemination barrier algorithm. So given that there are N processors participating in a barrier algorithm, particularly dissemination barrier algorithm, how many rounds do you think the dissemination barrier algorithm needs to complete in order to know that the barrier is done? So how many rounds for the barrier completion? So the choices I'm giving you: it's N log N, a log N to the base 2, Or feeling of log and base, to the base 2, or N where N is of course a number of processors that are participating in this barrier. A hint for you [LAUGH] I told you already that N in a dissemination barrier need not be a power of 2. So that's a hint for you to, to answer this question. The right answer is log n to the base two ceiling of log n to the base two and of course, from the example that we just saw, with n equal to five. We saw that at the end of three rounds, everybody has gotten a message from every other node in the entire system. And therefore it is common knowledge at that point that that has been reached. So the right answer is ceiling of log n To the base two, and if you chose log N to the base two, you're not far off from the right answer but, you know, the reason why it is ceiling is because of the fact that N need not be a power of two. So, with any row of five, at the end of round two, every processor has heard from every other processor in the entire system. Right? So you can eyeball this figure and see that every processor has gotten the message from every other processor, and so it's common knowledge that for every processor that every one else has also arrived. Add the barrier. So, how many rounds does it take to know that everybody has arrived at the barrier? Well its ceiling of log N to the base two. You add the ceiling because it's N need not be a part of two. So at this point, everybody is awake. So, barrier completion here there is no distinction between arrival and wake up as you, as you saw. In the case of the tree barrier or the MCS barrier or the tournament barrier. In the dissemination barrier, because it is happening by information diffusion, at the end of a ceiling of log n rounds, everybody has heard from everyone else in the entire system So everyone knows that that barrier has been breached. So in other words, in every zone of communication in the dissemination barrier, every processor, you eyeball any particular processor. Every processor is receiving exactly one message, exactly one message in every round of the dissemination barrier. So overall During the entire dissemination barrier information diffusion that's going on, every processor is receiving a total of a ceiling of log n to the base two messages. Every round one message, and so they are ceiling of log n rounds and so a ceiling of log n to the base two is a number of messages that any given processor is receiving and once. Every processor has received this ceil N log to the base 2 number of messages. It knows that the barrier is complete. It can move on. And I've been using the word message in describing this dissemination barrier. It's convenient to do, to use that word because it is information diffusion but if you think about a shared memory machine, a message is basically a spin locatoin. And, once again because I know an ordained processor is going to talk to me in every round, the spin location for this guy is statically determined. Spin location, statically determined, and so on. So every round of the tournament, we can statically determine the spin location that a particular processor has to wait on, in order to receive a message. Which is really a signal from its ordained peer, for that particular round of the dissemination barrier. So, again static determination of spin location becomes extremely important if the multiprocessor happens to be an NCC NUMA machine. In that case, what you want to do is to locate the spin location. In the memory that is closest to the particular processor. So that becomes more efficient. And as always, in every one of these barried algorithms, you have to do sense reversal. That once this barrier is complete, everybody is going on to, to the next phase of the competition. And when they go to the next phase of the competition. They have to make sure that the barrier that they arrive is the next barrier. So you need sense reversal then, it needs to happen at the end of. Everybody algorithm. So now let's talk about some of the virtues of the dissemination barrier. The first thing that you'll notice is in the structure, there is no hierarchy. In the tree algorithms, the root of the tree automatically gives you a hierarchy in terms of The organization of the tree in the dissemination barrier, there's no such thing. And I already mentioned that this algorithm works for both NCC NUMA machine, as well as clusters. That's also a good thing. And there is no waiting on anybody else. So every processor is independently making a decision to send a message. As soon as it arrives at the barrier. Is ready to send a message to its peer for that particular round. And of course, every processor can move to the next round only after it has received a corresponding message from its peer for this particular round. So as soon as that happens, it can move on to the next round of the dissemination barrier. And all the processes will realize that the barrier is complete when they received Ceil (Log N) messages in the entire structure of this organism. So if you think about the total amount of communication, because the communication in every round is fixed, it's N messages in every round and since there are Ceil (Log N) rounds, the communication complexity. Of this algorithm is order and log in. Compare that to the communication complexity of the tournament, or the the Tree barrier. In both of those cases, the communication complexity was only log in, because of the hierarchy, as you go toward the root of the tree, the amount of communication shrinks, so. The amount of communication in those algorithms is only order of log N. Where as, in this, this simulation down here, since there is no hierarchy, the total amount of communication in the, algorathim is order of analog N. We covered a lot of ground discussing different synchronization algorithms for parallel machines, both mutual exclusion lock and [INAUDIBLE] barriers, but now it's time to talk a little about performance evaluation. As always designers, of course, they're always concerned about the performance of these algorithms, because all the applications that sit on top of the processor. Is going to be using the algorithms that you've designed. [INAUDIBLE] Designer. And so the performance of these algorithms are very, very critical in determining how good the applications are going to be performing. So we looked at a whole lot of spin algorithms from, a very simple spin on test and set to spin with delay and, spin. Algorithms that respect the order of arrival of fairness if you will. Starting from ticket lock and the queue-based locks, all of these are different kinds of spin algorithms that we looked at. And we also looked at a whole number of barrier synchronization algorithms, starting from a simple shared counter to a tree algorithm, to an MCS tree. A tournament and dissemination. And I also introduced you to several different kinds of padlock architectures. Shared memory multiprocessor that is cache coherent. Which may be a symmetric multiprocessor or it could be a non-uniform memory access multiprocessor. And you can also have non-cache coherence shared memory multiprocessor. And of course, the last thing that I mentioned to you, is the mul, message passing clusters. So these are the different flavors of architectures that parallel machines can be built today. And the question you want to ask is, if you implement the different types of. Spin algorithms that I've been discussing with you. Which would be the winner on these machine? Well the answer is not so obvious. It depends really on the kind of architecture. So as OS designers, it is extremely important for us to take these different spin algorithms and implement them on these different flavors of architectures. To ask the question, which one is the winner? It may not always be the same. Algorithm that may be the winner on these different types of machine. And the same thing you should do for the barrier algorithms as well. So the barrier algorithms, all the way from the counter to the dissemination barrier, all the different flavors of algorithm. And you have to ask the question, which would be most appropriate to implement on these different flavors of architectures? As always, I've been emphasizing that when you look at performance evaluation that is reported in any research paper, you have to always look at the trends. The trends are more important than the absolute numbers because the dated nature of the architecture on which a particular evaluation may have been done. Make the absolute numbers not that relevant, but what is important is trends. Because these kinds of architectures that I mentioned to you, they're still relevant to this day. And therefore what you want to ask is the question, if different types of spin algorithms and barrier algorithms. When you implement it on different kinds of architectures, which one of those algorithms are going to be the winners? That completes the discussion of synchronization algorithms for parallel machines. I encourage you to think about the characteristics of the different spin lock algorithms and the barrier synchronization algorithms that you studied in this lesson. And we also looked at two different types of architectures. One was a symmetric multiprocessor, the other was a non-uniform memory access architecture. Given the nature of the two architectures, try to form an intuition on your own on which one will win in each of these styles of architectures. Verify whether the results that are reported in the MCS paper matches your intuition. Such an analysis will help you very much in doing the second project. The next topic we'll start discussing is Efficient Communication across address spaces. The client-server paradigm is used in structuring system services in a distributed system. If we're using a file server in a local area network every day, we are using a client-server system when we are accessing a remote file server. And remote procedure call. Is the mechanism that is used in building this kind of a client server relationship in a distributed system. What if the client and the server are on the same machine? Would it also not be a good way to structure the relationship between client and servers using RPC. Even if the clients and the servers happen to be on the same machine. It seems logical to structure clients of a systems even on the same machine using this RPC paradigm. But the main concern is performance. And the relationship between performance and safety. Now for reasons of safety, which we have, talked a lot about when we talked about operating system structures, you want to make sure that servers. And clients are in different address spaces, or different protection domains, as you've been calling them. Even if they are on the same machine uh,they will be running on different processors of an SMP, but they're still on the same machine. You, what you want to do is, you want to give a separate protection domain for each one of these servers from the point of view of safety. But, what that also means, because we are providing safety, there's going to be a hit on performance. Because of the fact that an RPC has to go across the outer spaces. A client on a particular outer space, server on a different outer space. So that is going to be a performance penalty that you pay. Now as operating system designers, what we would like to be able to do. Is to make RPC calls across protection domains as efficient as a normal procedure call that is happening inside a given process. If you could make the RPC across protection domains as efficient as a a normal procedure call, it would actually Encourage system designers to use RPC as a vehicle, for structuring services, even within a same machine. Why is that a good idea? Well, what that means is that you know, we've talked about the fact that in structuring operating systems in microkernel, you want. You want to be able to provide every service having its own protection domain. What that means is that to go across these protection domains, you're making a, a protected procedure call or a RPC call. Going from one from one protection domain to another protection domain. And that is going to be more expensive than. Simple procedure call. It won't encourage system designers to use these separate protection domains to provide the services independently. So, in some sense again is the same question of wanting to have the cake and eat it too. So you want the protection and you also want the performance. All of you know how a simple procedure call works. There is a caller you have a process in which all the functions are being compiled together and linked together, made an make an executable. And so when a caller makes a call to the callee, it makes a call passing the arguments on the stack. The callee can execute the procedure. And then a return to the caller. So this is your simple procedure call. And the important thing is that all of the interactions that I'm showing you here is happening at compile time. All of these things are being done at compile time. Now let's see what happens with remote procedure call. You know in principle a remote procedure call looks exactly like this picture. That you have a caller and a callee. SO the caller is making a call Executing a procedure, and returning. So that's what is going on in, in a remote procedure call. But under the cover, let's see what's going on when you're using remote procedure call. When the caller makes its call, it's really is a trap into the kernel. A caller trap into the kernel. And what the kernel does is, it validates the call. And it copies the arguments of the call into kernel buffers from the client idle space. The kernel then locates the server procedure that needs to be executed, copies the arguments that it has buffered in the kernel buffer into the idle space of the server. And, once it has done that, it schedules the server to run the particular procedure. So that's what's going on in this, in this direction. At this point, the server procedure actually starts executing using the arguments of the call, and performs a function that was requested by the client. When the server procedure is done with execution of the procedure. It needs to return the results of this procedure execution back to the client. And, in order to do that, it's going to tap into the kernel, there's the return trap that the server is experiencing in order to return the results back to the client. And, what the Kernel does at this point. Is to copy the results from the address space of the server into the kernel buffers and then it copies out the results from the kernel buffer into the client's address space and now at this point, we have completed sending the results back to the client. So the kernel can then reschedule the client who can then receive the results. And go on its merry way of executing whatever it was doing. So that's essentially what's going on under the cover. So even though the picture is so clean up here, that a client is making a call and you get the results and it can continue with whatever it was doing. In reality, what is going on under the cover is fairly complex. And more importantly, all of these actions are happening at runtime as opposed to What I mentioned about a procedure call, where everything is happening in compile time, all of these actions are happening at run time, and that is one of the fundamental sources of performance hit that an RPC system is going to take in the fact that everything is being done at the time of the call. In particular, if you want to analyze all the overheads that... Or the works that needs to get done at run time. There are two traps. The first trap is a call trap. The other trap is a return trap. There are two traps, and there are two context switches. So, the first context switch is when the kernel switches from the client to the server to the run the server procedure. And when the server procedure is done with its execution of the server procedure, it has to reschedule the client to run again. So, two traps, two context switches, and one procedure execution. That's the work that is being done by the runtime system in order to execute this remote procedure call. So what are all the sources of overhead now? Well, first of all, when this call trap happens, the kernel has to validate the access, whether this client is allowed to make this procedure call or not the validation has to happen. And then it has to copy the arguments from the client's address space into kernel buffers. And potentially, if you look at this picture, there could multiple copies that are going to happen in order to do this exchange between the client and the server, and then there is the scheduling of the server in order to run the server code and then there is the context which overhead, we talked about. The explicit and implicit costs of doing context switches, there is a context which overhead that is associated between but, when we go from the client to the server and back again to the client from the server, and of course dispatching a thread on the processor itself is also time, which is explicit cost of scheduling. So, before we discuss how we can reduce the overheads in this remote procedure call, when the clients and the servers happen to be on the same machine, let me prime the pump with a, with a quiz. So the question that I'm going to pose to you is the following, in an RPC, there is a client call, followed by the server procedure execution, and then the returning the results to the client. How many times does the kernel copy stuff from the user address spaces into the kernel, and vice versa? And I want you to focus on the question a little bit more carefully. I said, the entire interaction, going from the client call, to server execution, and returning results back to the client, the whole package in order to execute an RPC. How many times does the kernel copy stuff from user address spaces into the kernel buffers, and vice versa? Meaning, from the kernel buffers, back out to the user address spaces. Is it done once? Is it done twice? Is it done three times? Or four times? The right answer is four times. And I sort of walked through that for you hopefully you got that. Basically, the kernel has to copy from the client address space into the kernel buffer. That's the first copy. The second copy is, the kernel has to copy from the kernel buffer into the server. And then the third time when the procedure is completed, the server procedure is completed, the kernel has to copy it from the server address using the kernel, and then the fourth time, it's going to be copied from the kernel buffer into the client. So it's tough being moved from the user address space... Through the kernel and back out happens four times. This copying overhead that we're talking about in this client server interaction in RPC call is serious concern in RPC design. Why? Because this copying happens every time you have a call return between the client and the server. And so if there is a place where we want to focus on shaving overheads, it'll be on avoiding copying multiple times between the client and the server in order to make the RPC calls efficient. And if you go back to this analogy of a procedure call, the nice thing about this is that this, the arguments are set up in the stack. And and that might involve some data movement, but there is not kernel involvement in the data movement. And that's what we would like to be able to accomplish in the RPC world as well. And, in fact let's analyze how many times copying happens in the RPC system. Recall that in a RPC system the kernel has no idea of the syntax and semantics of the arguments that are passed between the client and the server. But yet, the kernel has to be the intermediary in arranging the rendezvous between the client and the server. And therefore what happens in the RPC system is that when a client makes a call, there's an entity, that is called the client stub. And what the client stub is going to do is, the client's thinking that it's making a normal procedure call, but it is a remote procedure call. And the client stub knows that. And what it does is it takes the arguments that is in the client call, which is living on the stack of the client, and makes an RPC packet out of it. This RPC packet is essentially serializing the data structures that are being passed as arguments by the client into a sequence of bytes. It's sort of like herding cats into an enclosed space. So that's what is happening by the client stack taking the arguments that are on the stack of the client and creating a packet of contiguous bytes, which is the RPC message. Because that is the only way the client can actually communicate this information to the kernel. So this is the first copy that's happening from the client stack into creating the RPC message is the first copy that's happening. Even before, the kernel is involved in this client server interchange. The next thing that happens, the client traps into the kernel and the kernel says well, you know there is a message, which is the RPC message that has to be communicated to the server. And that's sitting in the user address space. I better copy it into my kernel buffer so that's a second copy that's happening. From the address piece of the client is the RPC message is copied into the kernel buffer. So that's the second copy. Next the kernel schedules the server in the server domain because the server has to execute this procedure. So once that server has been scheduled the kernel copies the buffer. It, it, it has all the arguments packaged in, into the server domain. So that it the third copy that's happening. So this, so we went from the client stack to the RPC message first copy. From the RPC message to the kernel buffer, second copy. And now the kernel buffer is passed out to the service domain, that's a third copy. But unfortunately even though we've reached the address space of the server, the server procedure cannot access this because from the point of view of the procedure call semantics, the client of the server think that they are just doing procedure call. So the server procedure is expecting all of the arguments in the original form on the stack of the server, and that's where the server stub comes in. So what the server stub is, just like the client stub, the server stub is a piece of code that is part of the RPC infrastructure that understands the syntax and semantics of the client server communication for this particular RPC call. And therefore it can take this information that has now been passed into the server's address space by the kernel and structure it into the set of actual parameters that the procedure, the server procedure is expecting. So this, from the server domain, wherever the kernel put it, into the stack of the server for the server procedure to execute that procedure, that's the fourth copy. So you can see that just going from the client to the server there are four copies involved. These two copies are at the user level. And these two copies are what the kernel is doing in order to protect itself and the address spaces from one another by buffering the address space contents into a kernel buffer, and passing that to the server domain before the server domain can start using it in the form of actual parameters on the stack. So at this point, the server can start executing, the server procedure can start executing, do its job. And when it is done, it has to do exactly the same thing in order to pass the results back to the client. So it is going to go through four copies except that we're going to reverse it. We're going to start from the server stack and go all the way down to getting the information to the client stack in order for that exchange to happen. So, in other words, with the client server RPC call on the same machine with the kernel involvement in this process, there's going to be four copies each way. Going from the client to the server, there's four copies. Going from the server back to the client, there's going to be four copies. Two copies are happening in the user space and two copies are happening in the kernel space. Are orchestrated by the kernel, and two copies orchestrated on the user level. Now as you can see this is a huge, huge overhead compared to a simple procedure call that I showed you early on. So once the kernel gets all the information from the server, kernel gets to work. First of all, it creates this data structure on behalf of the server, and holds it internally for itself. So there's a data structure that is entirely in the kernel, and nobody else has to see it, [LAUGH] it is only for the kernel to know all the information that is needed, in order to make this upcall into the entry point procedure. It also establishes a buffer, and this is what is called the A-stack, and this A-stack sizes as A-stack was just specified by the server as part of this grand communication to indicate how big this A-stack is got to be. Because the you kernel has no idea what the relationship is, is between the, the client and the server. And so the server is saying, telling the kernel that look, in order for us to communicate, I need a buffer, and the size of the buffer is this much. So, the kernel allocates shared memory, and takes the shared memory that is allocated, and maps it into the address space of both the client and the server. So there's the client's address space. There's the server's address space. So, in some part of the client address space and the server address space, need not be exactly matching parts of the virtual memory space of the client and the server. But somewhere in the address space of the client and the server, it maps this A-stack. So essentially, what we have now. Is shared memory for communication directly between the client and the server, without mediation of the kernel, because once this has been setup as shared memory and mapped through the address space of the server and the client then the client can write into it, the server can write into it, client can read from it, server can read from it. No mediation by the kernel, or in other words, what we have accomplished is, we are getting the kernel out of the loop in terms of copying. Client and the server can directly communicate the arguments and the results back and forth, using this A-Stack. And that's the reason it's called A-Stack, it stands for argument stack. It's available for communication between the client and the server. So now the cla, the kernel is done with all the work that it has to do in order to set up this remote procedure call mechanism between the caller, the client and the callee, which is the server. And what the kernel is going to do is, it's going to authenticate the client that you're good to go. You can make calls on uh,this procedure foo that is being exported through the main server by the server, so I let you make calls on this in the future, and what you need to do every time you want to make a call to S.foo you have to give me a descriptive which i'm going to call the binding object B.O stands for the binding object In the Western world, BO has a different colloquial connotation. I won't go there. But here, BO stands for, Binding Object and it's basically a capability for the client. To present to the kernel that I am authenticated in order to make this call into the service domain to this particular procedure called s.foo. So that's the idea. So all the work that I have described to you up until now, is the kernel mediation that happens in terms of entry point setup, on the first call from the client. On the first call from the client, all of this magic happens in order to set up the communication buffer between the client and the server and authenticate client that you can make future calls on this particular entry point procedure, by providing or presenting to the kernel this capability which is called the BO, the binding object. And of course the important point is that the kernel knows that this binding object and this procedure descriptor are related. Or in other words, if the client is going to present a binding object, the kernel knows from the binding object What is the proceeded descriptor that corresponds to the binding object so that it can find the entry point to call into the server. So once again, what I want to stress is the fact that this kernel mediation happens only one time. On the first call by the client. Now let's see what is involved in making the actual calls between the client and the server. And you will see that all the kernel copying overheads are eliminated in the actual calls. What the client stub does on the client side is when the client makes the call is that through, the clients tab is going to take the arguments and put those arguments into the a stack, ignore this result for a minute, so that the stub is going to, the client stub is going to prepare the a stack, with the arguments of the call, and then in the a stack, you can only pass arguments by value, not by reference. And the reason is that this A stack, I mentioned to you is mapped into the client address space and shortly, it's going to be mapped into the, it is, it is mapped into the server address space as well by the, by the kernel and since only the A stack is mapped into the address space of both the client and the server. If this has pointers pointing to the other parts of the client address space, so it is not going to be a, able to access that. So, it is important that the arguments are passed by value and not by by reference. And the work done by the stub in, in preparing the array stack is much simpler than what I told you earlier about. The general RPC mechanism of creating an RPC packet. Where it has to serialize the data structures that are being passed as arguments. In this case, it is simply copying the arguments from the stack of the client thread into this A stack. That's what is being done by this stub. Then the client traps into the kernel, making a procedure called s.foo that is also in the trap. And, at this point, the the client's stop is presenting through the kernel the binding object associated with s.foo. So the binding object, I told you, is the capability that this client is authorized to make calls on s.foo. So once the BO is validated by the kernel, it can then see what the procedure descriptor associated with the BO is. And this procedure descriptor is, as I told you, the information that is needed by the kernel in order to pass the control to the server, to start executing the server procedure corresponding to this particular RPC call being made by the client. Now recall that the semantics of RPC is that the client, once it makes this RPC call, it's basically blocked. It's waiting for the call to be complete before it gets started resuming its execution. Therefore the optimization, what the kernel could do is. Borrow because all of this is happening on the same machine the kernel can borrow the client thread and doctor the client thread to run on the server address place. Now what do I mean by doctoring the client thread? What I mean is. Basically what you want to do is you want to make sure that the client's thread starts executing in the address space of the server, and the PC that the client thread is going to start executing in is the entry point procedure that is pointed to by the procedure descriptor. So you have the fix of the PC. The address space descriptor, and the stack that is being used by the server in order to execute this entry-point procedure. And for this purpose, what the kernel does is it allocates a special stack, which is called the execution stack, I'm not showing you this picture. An execution stack, or E-Stack, and that is a stack that the server procedure is going to use. In order to do its own thing, because server procedure may be making it's own procedure calls and so on, so it's going to do all of that action on the E-stack. So the A-stack is only for the purpose of passing the arguments, and the E-stack is what the server is going to use in order to make, do its own work. So at this point, once the kernel has doctored this client thread to start executing the server procedure, it can transfer control to the server. So it transfers the control to the server, and so now, now we're starting to execute the server procedure in the server's address space. And in the server's address space because A-stack has been mapped in, this is also available to the server domain. And the first thing that's going to happen in the server domain is our server stub is going to get into action and take the arguments that are sitting in the A-stack, and copy them into the stack that the server procedure's going to use. Remember I told you the kernel provides a special stack for the purpose an E- stack, execution stack and that is a stack into which the client, the server stub is going to copy the A-stack argument into that E-stack and then at that point the procedure foo is ready start executing. So at this point, procedure foo is like any normal procedure, it finds the information it wants on the stack, it does its job. Once it is done with executing this procedure, it has to pass back the results to the client and what is going to happen is that the server stub is going to take the results of this procedure execution and copy them into the A-stack. And of course, all of this action is happening in the server domain without any mediation by the kernel. So once the server stub has copied the results into the A-stack, at that point it can trap into the kernel, and this is the vehicle by which the kernel can transfer control back to the client so it, it does a return trap. Now, when this return trap happens, there is no need for the kernel to validate this trap as opposed to the call trap, because the up call was made by the kernel in the very first place, and therefore it is expecting this return trap to happen, and so the kernel doesn't have to do any special validation for this. And at this point, what the kernel is going to do, is it is basically going to re-doctor the thread to start executing the client address space. So basically it knows the return address where it has to go back in order to start executing the client code, and it knows the client's address space so it's going to redoctor the thread to start executing in the client address space. So when the client thread is rescheduled to execute, at that point, the client stub gets back into action, copies the results that are sitting in the A-stack into the stack of the client, and once it has done that, the client thread can continue on with its normal execution. So that's what is going on. And the important point that you notice is that the copying through the kernel that used to happen is now completely eliminated, because your arguments are passed through the A-stack into the server. And similarly the result is passed through the A-stack into the client. So let's analyze what we've accomplished in terms of reducing the cost of the RPC in the actual calls that are being made between the client and the server. Recall that we had four copies in doing the client call, just transferring the arguments from the client to the server's domain. That was the original cost. And the four copies were first creating an RPC packet, copying that RP-, RPC packet into the kernel buffer. Copying the kernel buffer out into the server domain. And in the server domain, the server stub getting into action. Taking this information that had been passed up to it by the kernel, and putting it on the server stack to start executing the server code. So this was the original costs that we incurred in terms of copying. Now, life is much simpler. All that is happening is on the client side, the client's stub is copying the parameters into the A-stack. And I want to emphasize the word copying the parameters. That is very different from what was happening over here. Here the client stub was doing a lot more work. It actually had to serialize the data structures that are being passed as, as actually arguments into a sequence of bytes in this RPC message. Whereas here, it is simply copying it, because the client and the server know exactly what the semantics and syntax of the arguments that are being passed back and forth and therefore there is no need to serialize the data structure. It just has to create a copy of the parameters into the A-stack. And this A-stack is, of course, shared between the client and the server. So what the server stub is going to do is basically going to take the arguments that are now sitting in the A-stack and copy it into the E- stack. Remember, the execution stack provided by the kernel for executing the server procedure? That is a, the special server stack that we're going to use. So the arguments are copied by the server stub into the E-stack, and once it is done the server procedure is now ready to be executed in the server domain. So what we accomplished is that the entire client server interaction requires only two copies. One for copying the arguments from the client stack into the A-stack, which is usually called the marshal link of the arguments. And the second copy is taking the A-stack arguments that are sitting in the A-stack and copying it into the server's stack, that is the unmarshal link. So, these are the two copies involved. One on the client side and one on the server side, and both these copies are happening above the kernel. It's in the user space, right? It is in the space of the client that the client stub is making this copy of the arguments into the A-stack. And similarly, it is in the space of the server domain that the unmarshaling is happening. And, of course, this is the work done. So we're basically taking the original four copies and gotten rid of the two copies that were being done inside the kernel. One into the kernel and one out of the kernel. These two copies, which is done by the kernel, we got rid of them. And instead, we have only two copies. These copies, even though you're calling it copies, it is, it is really not as tedious as creating an RPC message. It is, it is a more efficient way of creating the information that needs to be passed back and forth between the client and the server using this A-stack. And needless to say, the same thing is going to happen in the reverse direction for returning the results. So it is just that, it is, the server stack that is going to have the result and the server stub is going to put it in the A-stack and the client stub is going to take it from the A-stack and give it to the client so that the client can state resuming its execution. So there's two copies involved in going from the client to the server, and two copies involved in going back to the client from the server. So, to summarize what goes on in the new way we are doing the RPC between the client and the server. During the actual call, copies through the kernel is completely eliminated. Right? It's completely eliminated because all of the argument-result passing between the client and the serving is happening through this A-stack which is mapped into the outer space of the client and the server. And so the actual overheads that are incurred in making this RPC call is this client trap and validation by the kernel that this call can be allowed to go through. And switching the domains I told you about this trick of doctoring the client thread to start executing in the server procedure. That is really switching the protection domain from the client address space into the server address space so that you can start executing the procedure that's visible only in this address space. So that is the switching domain in the second overhead. And finally, when the server procedure is done executing, the return trap. That's the third explicit cost. So three explicit costs associated with the actual call. The first explicit is the client trap and, and validating this BO. And the second explicit cost is switching the protection domain from the client to the server so that you can start executing the server procedure. And the third explicit cost is when we have this return track to go back to the client address space. So those are the explicit costs. But we know, having done a lot of work on the operating system structure early on, that there are implicit overheads that are associated with switching protection domains. The implicit overhead is the loss of locality due to the domain switching that's happening. When we go from the client address space to the server address space, we are touching, of course we are touching some part of the address space, are going to be in physical memory and therefore in the caches of the processor. But, there's a lot of stuff that may not be in the caches of the processor. So, there is going to be a loss of locality due to the domain switch that, that may happen. In the, in the sense that caches and the processor may not have all the stuff that the server needs in order to do its execution. This is where multiprocessor comes in. If you're implementing this RPC package on a shared memory multiprocessor, then we can exploit multiprocessors that are are available in the SMP. What we can do is, we can preload the server domains. In a particular processor. And what we mean by that is, if we preload a server domain in a processor and, and don't let anything else run on this processor. This particular server is loaded on CPU 2. We're not going to let any other thing disturb what's going on in this CPU. What, what that would mean is that the caches associated with this CPU. Will be warm with the stuff that this particular domain needs. So, in other words, the server's address space is pre-loaded in a particular processor. If you have multiple processors then you can exploit the fact that you have multiple processors in the SMP. So, if a client comes along and wants to make an RPC call. Then what we want to do is use the server that has been preloaded in a particular CPU as a recipient of this particular RPC call. So when this client makes that call, that call is going to be directed to the server that has been preloaded. In a particular CPU and so the VP loaded in the CPU, the caches will be warm and therefore we can avoid or reduce, mitigate the impact on loss of locality. That I mention to you that goes on when you go from one protection domain to another protection domain. So this is the happy state of the world where what we have done is, we've first of all eliminated kernel intervention in making the actual call and return between the client and the server by providing an argument stack in shared memory that is shared in the address space of the client. And the address space of the server. And this way, the client can pass the actual arguments of the call to the A-stack, and the server can retrieve it from the A-stack without kernel intervention. And when the server is ready to return the results back to the client, once again it can do the same thing. Put it in the A-stack so that it is available for the client. So, without any kernel intervention, you can actually do the. Call and return, and of course the mediation happens only in the fact that the kernel has to validate the call. Every time the client makes a call it has to validate that call. But the loss of locality you can avoid by making sure that the server domain is pre-loaded in one of the CPUs. And the other thing that the kernel can do is look at the popularity of a particular server. If a server is serving lots of different clients than in a multiprocessor, then it can potentially based on monitoring the site that we may want to have multiple. CPUs very catered to, the servers and that way you have several different domains of the same server preloaded in several CPUs to, cater to the needs of several simultaneous requests that may be coming in for a particular service. So, in summary what we have done is we have taken a mechanism, that is typically used in distributed systems, namely RPC, and we ask the question suppose we want to use RPC as a structuring mechanism in a multiprocessor. How to make that efficient so that the designers of services will in fact use RPC as a vehicle. For building these services. And the reason why you want to promote that, is because when you put every service in its own protection domain you are building safety into the system. And that is very important for the integrity of an operating system. As an operating system designer, we worry about the integrity of services and we can provide the integrity by putting every service in its own protection domain. And we're making RPC cheap enough that you would use as a structuring mechanism we are promiting a, a software engineering practice of building services in sep, in sepearate protection domains. With that we conclude discussion of synchronization and communication issues in parallel systems. The next part of the lesson will cover scheduling issues in parallel systems. You'll notice once again when we discuss scheduling issues that the mantra is always the same. Namely, pay attention to keeping the caches warm to ensure good performance. We're going to look at scheduling issues in parallel systems. Fundamentally, the typical behavior of any thread or process running on a processor is to do the following: compute for a while and then make a blocking IO system call or it might want to snychronize with other threads that it is part of in the application or it might be that it is a compute bond thread, in which case it might just run out of the time quantum that it has been given by the scheduler on the processor. But fundamentally what that means is this is a point at which the operating system, in particular the scheduler piece of the operating system, can schedule some other thread or process to run on the CPU. So how should the scheduler go about picking the next thread or process to run in the processors, given that it had the choice of other threads that in can run at any point of time? Now I'll turn that into a question for you. How should the scheduler choose the next thread to run on the CPU? I'm going to give you four different choices. The first is, first come first served. That is if I have a bunch of threads, the scheduler says, well, which was the earliest one that was ready to run on the processor? That's going to be the one that I'm going to schedule. That is first come first served. The second possibility is, it's going to assign static priority to all the threads so it's going to pick the threads that have the highest static priority to run on the processor. The third possibility is that the priority is not static, but it is dynamic or in other words it changes over time. And so what the scheduler is going to do is pick the one that has the highest dynamic priority. And the fourth choice is, its going to pick a thread whose memory contents are likely to be in the cache of the CPU. So these are the four choices, and I want you think about it and come up with some thoughts as to what might be the right thing that the scheduler might do, in picking the next processor to run. If you picked any or more or all of the choices that I gave you, you're not completely off base. Let me just talk through each of these choices and why it may be a perfectly valid choice for the scheduler in picking the next thread to run on the processor. First come first search says well, you know there is an, an order of arrival into the processor, there's a fairness issue, I'm going to pick the one that became runnable at the earliest, so there is a first come first serve policy, [INAUDIBLE] in that. The second is, well somebody paid a lot of money to run the program, and so I'm going to give it a priority that it statically assigned with every process or thread. And I'm going to pick the one that has the highest priority, so that's also a valid choice. The third possibility is, a thread's priority is not static. It may be born with a certain priority, but over time, it might change. Why might it, might the thread's priority change over time? Well, for one thing, operating systems typically tend to give priority to jobs or processes, or threads what do we even want to call them that tend to be interactive. That tend to take a short amount of time on the CPU and then go off and IO or synchronization. Those kinds of threads are shortest amount of time that it takes on the processor, the schedule may want to boost up the priority of the process and give it a higher priority, even if it was born with a smaller static priority. And that maybe a reason why it might choose a higher dynamic priority. That's a third choice. And the fourth choice is pick the one whose memory contents in the CPU cache is likely to be the highest. What that means is the that thread that has the cache warn for its working set is likely to do really well when it gets scheduled on a processor. And so it makes sense to suggest that this might be a good choice as well. So, all of these four choices, one can't argue for and, for and against. But in this particular lecture, what we're going to think about is particularly look at this last choice and that is picking the thread whose memory contents are likely to be in the CPU cache. Picking that as a choice, why that makes a lot sense especially in a multiprocessor, where there's going to be several levels of caches and, and cohesivenesses and so on and so forth. We'll discuss more about that in the rest of this lecture, but I wanted to warm you up with this particular quiz In which we have all these different choices. And one can, as I said, argue for or against every one of these choices, and there are valid arguments both for and against. But, this is the choice that we're going to focus on for the rest of this lecture. Here's a quick refresher on the memory hierarchy of a processor. As you know, between the CPU and the main memory there are several levels of caches. And typically these days, you may have it up to three levels of caches between the CPU and the memory. And the, the nature of the the memory hierarchy is that, you can have something that is really fast, a small amount of, or really slow, and big amount of. And, and so all of these choices that are in between are really in between the two extreme choices of an L1 cache and the main memory. The disparity in the CPU cycle time and the main memory if you take the disparity between the CPU and the main memory, it's more than two orders of magnitude today, and it's only growing. So any hiccup that the CPU has in not finding data or instructions that it needs to execute the currently running thread in the caches and it has to go all the way to the memory, is bad news in terms of performance. So what this suggests is that in picking the, the next thread to run on the CPU, it'll probably be a very good idea if the scheduler picks a thread whose memory contents are likely to be in the caches. If not in the L1 cache, at least in the L2 cache. If not in the L2 cache, at least in the L3 cache. So that it doesn't have to go all the way to the memory in order to get the instructions and data for the currently running thread. So that's an important point to think about. So that brings us to this concept of cache affinity scheduling. Basically, the idea is very, very simple. And, and that is, if let's say, that a particular process at P1. I had this thread T1 running for a while and it got descheduled at some point of time becuase it made an I/O call, it tried to synchronizes another thread. Whatever it is. Or time quantum. Expired for T1, any of those situations will result in T1 one getting descheduled, and then the schedule is going to use the process of for, perhaps running some of the thread, but finally, at some point of time, if T1 gets ready to be scheduled again It makes a lot of sense for T1 to be scheduled on the same processor. Why? Because it used to run on, this processor P1 and therefore the, the memory contents of T1, T1 that needed to have its execution. We're in the cache of P1, and therefore, when T1 gets ready to run again, if I schedule T1 on the same processor, it is likely that T1 will find its working set in the caches of P1. That's the reason why it makes sense to say well, look, let's look at the affinity Of a particular threat to a processor. Cache affinity of a particular threat to a processor. So, the cache affinity for this thread is likely to be higher for P1 because, it ran on P1 before, got descheduled and is rescheduled on, when it is time to reschedule it if you reschedule it on the same processor, good chance that T1 will find its working set. In the memory hierarchy, the caches of processor P1. But can something go wrong? Well, what can go wrong is the following. When T1 was descheduled, the scheduler may have decided that, okay, P1 is now available for doing business for some of the thread, so it scheduled T2 and it scheduled T3 And eventually, when T1 gets ready to run again, it's ready to run again, but in between, it's running on the processor here and running on the processor again here, along this timeline. Two other intervening threads ran on P1. So watch out. The cache may be polluted by the contents of threads T2 and T3 So far as T1 is concerned. So, when T1 runs again, it's quite possible that it may not find a lot of its memory contents in the Cache because these two guys that got in the middle of its running on the process at T1 may have polluted the cache and gotten rid of a lot of this stuff that used to belong to T1, and therefore even though we, we made this choice that, well, when T1 is ready to run, let's put it in on P1. But it used to run before. And that way, we can ensure that T1's working set, is probably in the cache of, process of B1. But unfortunately, these intervening threads may have polluted the cache. So that's something that you have to watch out for. So the moral of the story is that you want to exploit cache affinity. In scheduling threads on processors. But also, you have to be worried about any intervening threads that may have run on the same processor and may have polluted the cache as a result. So, that's something that you have to watch out for. So, now that I've introduced the idea of cache affinity for a processor,uh, we'll just pick to a particular thread, we are now ready to discuss different scheduling policies, that an operating system may choose to employ The first scheduling policy is a very simple one, first come, first serve. And what this is saying is that you look at the order of arrival of threads into the scheduling queue of the scheduler and pick the one that is the earliest to become runnable again. And that's the one that you're going to schedule. So what this is saying is, well basically we will give importance to fairness for threads as opposed to affinity. So it is ignoring affinity altogether and simply saying let's just be fair. We'll pick the thread that became runnable at the earliest, that's the one that we're going to pick as the next one to run on the processor. That's first come, first served. The second scheduling policy is called fixed processor, or in other words, for every thread, when I schedule the thread the first time, I'm going to pick a particular processor. And I'm always going to stick to that. So the processor on which Ti will run will always be a particular fixed processor. And the, the, the, way we choose the initial processor on which to schedule Ti may depend on the, load balance. Making sure that all the processors in the multiprocessor are equally stressed in terms of, using the resources for running the available threads that are there in the system. And that's how I pick a particular processor but you, for the life of this thread, the processor on which Ti is going to run is always fixed. So that's fixed processor scheduling. The third scheduling policy is what is called a last proccessor scheduling policy. The idea here is the processor is going to pick among the set of threads that are available to be run at any point of time. It is going to pick a thread that used to run on it. In other words, if TI the last time it had any cycles from the system was on a particular processor. Then, when this processor is coming around looking for work, it'll see oh, Ti is there, he used to run on me. I'm going to pick that guy to run on me again. And as you can imagine, this is giving preference to the fact that there could be affinity for Ti to this processor, because it used to run on this. So that is what is called last processor scheduling, and of course when a processor is looking for work and it looks at the run queue, does not find any thread that used to run on it, and of course it has to pick some thread, right? So the, the inclination is to pick the thread that had run on this processor before. And, and that's the one that I want to schedule on P last. But if such a thread is not available, then you're going to pick something else. So, the idea behind this is that, you want to make sure that if this processor is going to pick a thread to run on it, the likelihood of this thread finding its memory contents in this processor is high. That's, that's what we're trying to shoot for in this last processor. The next couple of scheduling policies I'm going to tell you about. It requires more sophistication in terms of the information that the scheduler needs to keep on behalf of every thread. You know in order to make a scheduling decision. The next scheduling policy is what is called minimum Intervening policy. MI for short. And in MI, what we're going to do is the following. We're going to keep, for every thread, it's affinity with respect to a particular processor, and pick the processor for running this thread in which this thread has the highest affinity. I want to explain this a little bit more detail. So, let's do this. If you look at the timeline for a particular process of Pj, it might look like this. That Ti was running here, got de-scheduled, and then there were a couple of other threads that ran on, on Pj, Tx and Ty. So now if I want to think about the affinity for Ti with respect to this processor Pj. That affinity, if we're going to schedule Ti now on Pj, the affinity number that I want to compute for this guy is two, indi-, indicating the number of intervening threads that ran on Pj between the last time Ti ran on it, and if I schedule Ti now at this point of time. And so clearly, this number that I'm talking about, the affinity number, the smaller the number the higher the affinity. So when we say the affinity number is two, it means there are two intervening threads that ran on Pj between the time Ti got dibs on Pj now at this point of time and at this point of time. 'Kay that's the idea behind this affinity index. So what we want to do is in a minimum intervening scheduling policy, you want to keep this information about the affinity for Ti to run on every processor. If I have a multiprocessor with 16 processors, then there is an affinity index associated with every one of those processors for Ti. It might be that on processor one Ti has an affinity index of two, on processor two it has an affinity index of four and so on and so forth. And what we want to do is when it comes time to scheduling Ti, I want to pick a processor on which the affinity index is the minimum. So the minimum affinity index indicates that there is the minimum number of intervening threads on this particular processor. That's the processor on which I want to run Ti. That is amplifying the chance that Ti is going to find its memory contents, the working set in the caches. That's the idea behind minimum intervening scheduling policy. So that's your minimum intervening scheduling policy that is ensuring that the processor that is picked for T i to run on has the highest affinity for T i. That's the minimum intervening. And there's a variant of minimum intervening, which is called limited min, minimum intervening, which is essentially saying that if I have lets say, 1,000 processes in the multiprocessor then the amount of information that I want to keep for every one of these threads is huge, right? For every processor that is available in the multiprocessor, I need to keep this affinity index for this thread. At maybe too much meta data that this scheduler has to maintain on behalf of every thread. And therefore, that means there's a variant of minimum intervening which is called limited minimum intervening. Which is saying, don't keep this infinity index for all the processors. Just keep it for the top few processors. So if the infinity index, if it is two or three, those are the ones that I care about. If the infinity index is 20 or 30 I'm not going to pick that, so why bother keeping all of the affinity index for a particular thread? Just keep the top candidates. That's the idea behind limited minimum intervening scheduling policy. The last policy I'm going to introduce you to. It's called Minimum Intervening Plus Queueing. The idea is still the same that I want to look at whether Intervening Threads that ran on a particular processor with respect to this thread that I am trying to schedule at this point of time. But when I make a scheduling decision that Ti is going to run on a particular processor It may be that this particular processor Pj may already have some other threads that are going to run on it, and that's the idea behind minimum intervening plus queue. Again, I want to explain this in a little bit more detail. So in minimum intervening scheduling plus queuing wouldn't be the same as It's not only the affinity index of Ti with respect to a particular processor I'm going to look at, but I'm also going to look at the queue for this particular processor. Why do we need to know do that? Well, if Ti is going to be scheduled on, on this particular processor PJ. Maybe there's a scheduling queue associated with PJ, which already has some number of threads to be run. And therefore, even though I'm picking the process of PJ based on cash affinity. By the time TI gets to actually run. Two other threads are going to run before it, so this was when Ti ran last, and I might find the definitive for Ti with respect to Pj, is two, just like in this previous example that I gave you, the affinity is two, so it looks like a good choice to put ti on, on pj, if this turns out to be There are the minimum, but when I made that decision, what I'm going to do is I'm going to stick this thread Ti in the scheduling queue of Pj and if the scheduling queue of Pj has Tm and Tn already populated, then what's going to happen Time is now, but by the time Ti gets to run on the process of Pj, Tm and Tn would also have run on the processor. Right? So even though the affinity index that I computed at the point of the scheduling decision, the scheduling decision, at the scheduling decision I made the decision to put Ti on Pj based on its affinity with respect to processor Pj. But unfortunately, the reality is that Ti is not going to run immediately, but it is going to run much later in time, and by the time it gets to run, two other threads that are already sitting in the Q of Pj, they're going to run. And therefore, the cache will be more polluted [LAUGH] than what we thought it was going to be at this point of time. So that's the reason that this scheduling policy's called minimum intervening plus Q, saying that. Not only should you ta, take into account the affinity index of a thread with respect to a particular processor, but you should also look at the Q of the processor. And ask the question, is the Q already populated? In that case, the, the processor that I want to pick Ti to run on is the min of i plus q where i if the affinity index and q is the size of the scheduling q associated with this particular processor pj. So that's the last scheduling policy. So basically have introduced five different scheduling policies, first come first serve. Fixed processor, last processor, minimum intervening, and minimum intervening plus queuing and as I mentioned, these two scheduling policies will really not be having the information for a thread with respect to all the processors in the system, because in a large scale process it may be invisible to do that. So what you do is, you limit the amount of information that you keep for every one of these threads. Remember one of the attributes of a good operating system is to make a decision really quickly and get out of the way, and from that point of view the less information it has to sift through in order to make a scheduling decision, the faster it can do it's work. So to summarize the scheduling policies, I already mentioned that first come first serve simply ignores affinity, and pays attention only to fairness. And these next two policies that I introduced to you, fixed processor and last processor, the focus is on cache affinity of a thread with respect to a particular processor. That's what we're focusing on. And Fixed Processor was the last processor. The next two policies, they focus not only on cache affinity, but also cache pollution. In particular it asks the question, how polluted is a cache going to be by the time Ti gets to run on the processor. That's the question we're asking (no period) In both minimum intervening, as well as minimum intervening plus queuing, in terms of making a scheduling decision. And that should be clear, from the discussion up until now, the amount of information that the scheduler has to keep. Grows as you grow, go down this list. The order of arrival is all that you care about, you put them in priority order in the queue and you're done with it. And you have, the schedule has to do a little bit more work, in each one of these cases, and corresponding with the amount of information that this schedule has to keep for every one of these scheduling policies is going to be more. But the result of scheduling decision is likely to be better when you have more information to make the scheduling decision. Another way to think about the scheduling policy is that, the fixed processor and the last processor is thread-centric in saying what is the best decision for a particular thread with respect to it's execution. Where does this MI and minimum intervening plus queuing? Both of these are processor-centric, saying that, what thread should a particular processor choose in order to maximize the chance that the amount of cache contents is going to be relevant for the currently scheduled thread? So that's what we're looking at. Now that I've introduced to you these scheduling policies, it's time for a quiz. Information that we have available is that on some processor pu, the queue contains a task tx. On another processor, pv, the queue contains four threads. Tm, tq, tr, and tn, so these are these are the threads on pv's queue. And there's a particular thread, Ty, the affinity of Ty with respect to Pu, is 2. This is the intervening thread index, and I mentioned to you that the smaller the index, the higher the affinity. So PuI for Ty is two, and similarly Pu, PvI For T y is one. There's a number of intervening tasks that have run on the process of P u and P v respectively since the last time T y had a chance to run on these processes. And the scheduling policy we're going to pick is the minimum intervening plus q. Minimum intervening plus q, that's a scheduling policy that we're going to pick. So, given that this is a scheduling policy, when we decide at the point that Qi gets to run again, when it is ready to be put on a Q, which Q will I put Ty on, if the scheduling policy is minimum intervening plus Q? Is it Pu's Q or is Pv's Q those are the two choices available to you. Let's walk through and pick the Q on which to place Ty, based on the scheduling policy. Now if you ask the question, what is the minimum I plus Q for PU for this particular thread Ty, that's going to be the infinity that Ty has on PU. The affinity that T, Ty has on Pu is 2. But also we have to look at the queue size. And the queue size of Pu is 1. There's only one thread sitting there. So the overall min of I plus Q for Ty with respect to Pu is 3. Let's do the same thing for Ty on Pv. In the case of Pv, its affinity apparently is higher because there's only one intervening task that ran since the last time Ty ran on it. That's good news, but we also have to look at the Q of Pv. When Ty gets put on Pv's Q, it has to sit behind whatever they may be, and in the case of Pv, the Q already has four threads to run. So when Ty is put on Pv's Q, it's going to be stuck at the end of this Q. Which means that, in a sense, the amount of intervention that is going to happen for Ty on Pv, by the time it gets to run, is actually the size of the, the affinity index for Py with respect to Pv currently as well as the intervention that's going to happen by the time it actually gets to that. So that's four, so the overall I plus Q is five for this guy, and three for this guy. Which means that the choice I am going to make is to put Ty on Pu, because, that's the one that will result in the least amount of intervention for polluting the cache of Pu with respect to this particular thread Ty Now that we looked different scheduling policies, let's discuss the implementation issues of these scheduling policies in an operating system. One possibility is, the operating system can maintain a global queue of all the threads that are runnable in the system. And, what these processes might do is. When they're ready for work, they'll go to this global queue and pick the next available thread from this queue, and run that on itself. And the way we organize the queues is orthogonal to the scheduling policy itself. But if the policy is something like FCFS, it makes sort of logical sense to have. A global queue and let the processes pick from the queue the head of the queue is the earliest arriving thread and therefore first come first serve policy we use this global queue policy. This global queue becomes very infeasible as an implementation vehicle. When the size of the multiprocessor is really big. Because then, it's a huge data structure that all these guys have to access centrally and so on. So typically, what is done is to keep local queues with every processor. And these local queues are going to be based on affinity. And the particular organization of the queues. In each of these processes. These local queues for each of these processes is going to depend on the specific policy that you're going to use. So, if it is last processor, or is it, is it fixed processor, is it a minimum intervening, or is it minimum intervening plus queuing? All of those things will decide how these local queues are going to be maintained. But important point I want to get across is that. In implementing the scheduling policies, you have to have a ready queue of threads from which the processor will pick the next piece of work to do. And the organization of these queues will be based on the specific scheduling policy that you might choose to employ for the scheduler. And it might be that processor p2 runs out of work completely, nothing in its local queue. In that case it might pull its peers' queues in order to get some work from other guys and run it in that processor. Now that's something that might be done and that is what is called work stealing in the scheduling literature. So that might be something that is commonly employed in a multiprocessor scheduler. So I mentioned that the. Way these queues are organized is based on policies that scheduler picks which might be affinity-based or might be fairness based and so on. But in addition to the policy specific attribute, it might also use additional information in order to organize its queue In particular, a priority of a thread is determined by three components. Now one component is the affinity component assuming it's an affinity based scheduling policy. But in addition to that, it might also use additional information. So for instance every thread may be born with a certain priority, so that is the base priority that. A particular thread has when it is started up, and as I mentioned, it might depend on whether you know they usually give a huge amount of money you know, to run this particular thread. So that is the base priority that you associate with the thread, and, and, of course, then you take the affinity into a, into account. And in addition to that, there is age coming in. And this is sort of like a senior citizen discount. If a, if a thread Ti has been in the system for a long, long time, you want to get it out of the system as quickly as possible. So what you do is equivalent to giving a senior citizen discount. You boost the priority of the thread by At certain amount, so that it gets to, to be at the head of the queue, and it will get scheduled on the process of p2. So basically, the priority attribute is what determines the position in the queue in the particular thread. And as I said, three attributes that go with it is a base priority that you may associate with a thread, then it is. First created, the affinity it has for a particular processor, and also the senior citizen discount that it might give to a particular thread depending on how long it's been on the system. So having discussed several different scheduling policies, we have to talk about performance. Now the figures of merit that is associated with the scheduling policy are threefold. The first scheduling policy figures of merit is what is called throughput. And as the name suggests, what this is saying is how many threads get executed or completed in per unit time. So that is. You know, how many threads are being pushed through the system per unit type, so that's what you're asking the super, and as the name suggests, it's a system centric metric. It doesn't say anything about the performance of individual threads, how soon they are performing their work and getting out of the system, but it is asking the question. What is the throughput of the system with respect to the threads that need to run on it? And the next two metrics are user-centric metrics. The response time is saying, if I start up a thread, how long does it take for that thread to complete execution? And that's what response time is saying and variance of responsing's time is saying. Well, does the time that it takes for me to run my particular thread vary depending on when I run it on the system. Why will it vary, well for instance, if you think about a first come first serve policy if I have a very small job to run, and if it gets the processor immediately it's going to quickly complete its execution. But suppose when I start up my particular thread, there are other threads in the system ahead of me that are going to take a long time to execute, then I'm going to see a very poor response time. So depending on from run to run, the same program may experience different response times depending on the load that is currently on that system. And that's where the variance of response time comes in and so clearly from a user's perspective I want response time to be very good and variance to be very small as well. Now when you think about first come, first serve scheduling the figure of measure that is really good about it, is the fact it is fair. But, it doesn't pay attention to infinity at all. And it doesn't give importance to small jobs vs big jobs. It's just doing it first come first serve, and therefore, there's going to be a high variance, especially if it is small jobs that need attention of the processor, and there are long-running jobs on the processor connecting... Now if you look at the memory footprint of a process. And the amount of time it takes to load all of its working sect into the cache. The, the bigger the memory footprint, the more time it's going to take for the processor to get the working set of a particular thread into the cache. So that the cache is warm enough, and the process of the thread can do its work. Without having to have those hiccups where it has to go to the memory in order to fetch the contents in the cache. So what this suggests is that it's important to pay attention to cache affinity in scheduling. And so the variance of cache affinity scheduling that we talked about are all excellent candidates to run on a multiprocessor. And in fact, the minimum intervention, or minimum intervening scheduling policy, and the minimum intervening scheduling with queuing, both of those are very good policies to use when you have a fairly light to medium load on a multiprocessor. Because that is the time when it is likely that a thread, when it is run on a processor, if it has an affinity for that particular processor, the contents of the cache are going to contain the memory contents for that particular thread. But on the other hand, if you have a very heavy load in the system, then it is likely that by the time a thread is run on a processor, on which it supposedly having an affinity, all of the cache may have been polluted because the load is very heavy. So, in between the time that a thread got run on a particular processor, next time it runs on the same processor, maybe it's cache contents have been highly polluted by other threads. And therefore, if the load is very heavy then maybe a fixed processor scheduling may work out to be much better than the variants of minimum intervening scheduling policies. So, the moral of the story is that you really have to pay attention to both how heavily loaded your processor is, or system is and also what is the kind of workload that you are catering to, both those things play a part in deciding what what we will be best scheduling policy and it may not always be the case that the same scheduling policy applies in all circumstances. So, a real agile operating system may choose to vary the scheduling policy based on the load as well as the current set of threads that need to run on the system. Another interesting wrinkle to taking a scheduling policy is the idea of procrastination. And that is, normally we think of the scheduler when the processor is looking for work, it's going to the, to the run queue and saying well I need to, to do something.and let me pick the next thread to run on myself. That's what a processor is going to do. Perhaps procrastination may help. Why would procrastination help? First of all, how do we implement procrastination? Well, what the processor can do is, it is actually ready to do, do some work. Now, what it does is it, it's going to insert an idle loop. Why would it insert an idle loop? It'll insert an idle loop because it has, it's looking in the, the scheduling queue and it sees that there is no thread in the scheduling queue that has run on it before. And therefore, if it schedules any one of those threads though all of those threads are not going to find any of their working set in the cache of the processor. And therefore the processor says okay, now let me just spin my wheels for a while, not schedule anything. It is likely that a thread that has its cache content in that processor becomes runable again. And then you can schedule that, and that might be a win, in terms of performance. So in other words, procrastination may help boost performance. We saw that already in the synchronization algorithms where we talked about inserting delays in the scheduling algorithm in order to reduce the amount of contention in the connection of the network. It's the same sort of principle. Often times you'll see in system design, procrastination actually helps in boosting performance. It helps in the synchronizational rhythms, it helps in scheduling and later on when we talk about file systems you'll see that it helps in the design of file systems also. So lets talk about cache affinity and modern multicore processors, in modern multicore processors you have multiple cores on a single processor, and in addition to the multiple cores that are in a single processor, the processors the,selves are also hardware multithreaded. What hardware multithreading means is that. If a, if a thread that is currently running on a processor, on a core C1, is experiencing a long latency operation, for instance it misses in the cache and therefore has to go out in order to fetch the contents from memory, that's a long latency operation. In that case. The hardware may switch to one of the other threads and run those. So, in other words, it wants to keep this core busy. There's only one execution engine in this core, but it has four threads that it can run on this core. Depending on what these threads are doing, if they are involved in long latency operations, meaning they are going out, they're not switched out of the processor, in terms of operating system scheduling. It is just that these are the, the threads that have been scheduled to run on this core and the hardware is switching among these threads by itself without the intervention of the operating system. It is automatically switching among these threads depending on what these threads are doing. If this thread, does the memory access which is going outside the processor, then the hardware is going to say, well, you know this guy is going to be. Waiting for a while, so let me switch to this guy, and let him do its, do its work because it's possible that what he needs is available in the cache. And if this guy also makes a long latency operation, like a memory access, then the hardware can switch to this guy. And to this guy. So if all of these guys are waiting on memory, then of course the core is not able to going to be, going to be able to do anything useful until at least one of these memory accesses are complete. So that's the idea behind. Hardware multithreading. So it is not very unusual for modern multicore processors to employ hardware multithreading. So in this example I'm showing you, there are four cores and in each core I have four hardware threads. So it is a four way hardware multithreaded core. And I'm showing you two levels of caches, L1 and L2 cache. L1 cache is specific to this particular core, C1, shared by these threads. Similarly, L1 cache here is. Is specific to this core C2, shared by the threads that are on it. On the other hand this L2 cache, is common for all the cores. So [INAUDIBLE] and any, anyone of these L1 caches, the hope is that we were able to find it in the L2 cache. If the processor. Has only these two levels of caches, L1 cache and L2 cache. This thing in L2 cache is really bad news [LAUGH] because then, you're going all to, all to the chip. It's a long latency memory operation. And modern multiprocessors may in fact even employ even more levels of caching. In addition to L1 and L2, there may be an L3 cache. It's normal to have, modern processors having at least three levels of caches on the chip. And L1 cache associated with core, and a shared L2 cache, and a shared L3 cache. So that's the structure that you might see in modern mulitprocessors. So what we have to think about now is. Eh, thinking about this cache affinity and the modern multi core processes and how the operating system should make its scheduling decisions. So here again, there's a partnership between the operating system and the hardware. The hardware is providing these hardware threads inside each core. And what the operating system is doing is picking which threads that it has in its pool of vulnerable threads and map them on to the threads that are available in the hardware. And clearly, the scheduling decision, what it tries to do is to make sure that. Most of the threads that are scheduled a particular core may find their working set in the L1 cache if possible, and similarly the threads that are scheduled on this may find its working set of [UNKNOWN] cache of C2 if possible, and so on. And also the other thing that the operating system may try to do is, if you just take the universal, all the threads That are currently scheduled by the operating system to run on all these four cores. You want to make sure that working set of all these threads are likely to be found in the L2 cache, because if you're missing the L2 cache bad news because then you're going outside the chip and that's a very long latency memory operation. And of course you can extend this idea if, if there is a third level of cache but to make things concrete let's just stick to two levels of caches L1 cache and L2 cache and. The criterion for operating system is to make sure that the threads that are currently scheduled on the processors that's available, all the cores that are available, what it wants to try to do is make sure that all the threads will find the contents in the L2 cache. Because nothing in the L2 cache is going to be elongating the memory operation. So let me briefly introduce here the idea of cache aware scheduling, when you have these multithreaded multi-core processors. And to make things concrete, let's assume that you have a pool of ready threads, and in this case I'm going to tell you that the pool of ready threads I have is 32. So I have 32 ready threads and I have a four way multi-core per CPU. Meaning that there are four cores in the CPU, and each core is four way hardware multithreaded. Or in other words, at any point of time the operating system can choose from this pool of ready threads, 16 threads to be run on the processor. Because that's the number of hardware multi-threads that are available if you pool together all the four cores. So each core has four multi, hardware multi-threads, together they have 1,600 threads that can be run on the CPU at any point of time. And the, the job of the operating scheduler is to pick from the available pool of ready threads, 16 candidates to be scheduled on the CPU. So how does the operating system choose the 16 threads to be run on the CPU at any point of time? What the operating system should try to do, is it should call schedule some number of cache frugal threads, and some number of cache hungry threads on the different course so that together the sum of all the cache hungriness of the 16 threads that are executing at any point of time in the CPU is less than the total size of the L2 cache. And as I said, L2 cache in this simple example, I gave you two levels of caching, a caching that is associated with each of these cores, and an L2 cache that is sitting outside of these cores, but it is, it is common to all the four cores. But of course, you can generalize this and say it is the last level cache, or in other words, you want to make sure that this, the universe of threads that are scheduled at any point of time, on the CPU, the sum total of the cache requirements of the universe of thread schedule on the processor, is less than the total capacity of the last level cache in the CPU. Because if it's missing the last level cache on the CPU, you're going outside the chip, out of memory, long latency operation, bad news. That's the thing that you're trying to do. So we're going to categorize threads as either cache frugal threads, or cache hungry threads. So cache frugal threads are one in, ones that require only a small portion of the cache to keep them happy. On the other hand, a cache hungry thread is one that requires a huge amount of cache space in order to keep it happy, meaning that the working set of cache hungry threads is much bigger than the working set of cache frugal threads. Now how do we know which threads are cache frugal and which threads are cache hungry? Well that's something that we can know only by profiling the execution of the threads overtime. So the assumption is that many of these threads get to run on the CPU over and over again, so overtime you can profile these threads and figure out whether a particular thread belongs to this category of cache frugal thread or this category of cache hungry thread. And the criterion that you want to use in picking the set of threads to be populated in the CPU at any point of time from the pool of available threads, is to make sure that the sum of the cache requirement of all the cache frugal threads is that there are N cache frugal threads and there are M cache hungry threads, then the cumulative cache requirement of all the threads put together is less than the total size of the L2 cache. And then I told you, we can generalize this L2 cache to the last level cache, that is the cache that is sitting at the last level inside the CPU beyond which you had to go out of the chip, go out to memory, and so that last level cache becomes the determinant in saying whether the size of that last level cache is within bounds of the cache requirements of all the threads that I want to schedule. So this is the set of threads that I want to pick, where, in this particular case, since the total number of hardware threads that I have available to me is 16, I want to make sure that M, the cache hungry threads, N, the cache frugal threads, is 16 and this inequality is satisfied as well. So that's what we want to shoot for in picking the set of threads to run on the processor at any point of time. I mentioned that we have to profile these threads, or monitor these threads as they're executing in order to figure out their cache occupancy over time so that we can categorize these threads as cache frugal or cache hungry, and the more information the scheduler has, the better decision it can take in terms of scheduling. Be we have to be careful about that. In order for the system to do this monitoring and profiling, clearly, the operating system has to lose some work in the middle of these threads doing useful work. And I always maintain that a good operating system gives you the resources that you need and gets out of the way very quickly. And so, you have to be very careful about the amount of time that the operating system takes in terms of doing this kind of monitoring and profiling, and this information is useful in scheduling decisions, but it should not be disrupting useful work that these guys have to do in the first place. Or in other words, the overhead for information gathering has to be kept minimal so that the OS does not consume too many cycles in doing this kind of overhead work accounting for making better decisions in, in terms of scheduling. Since it is well known that processes scheduling is np complete we have to resort to heuristics to come up with good scheduling algorithms. the literature is ripe with such heuristics, as the workload changes And the details of the parallel system, namely how many processors does it have, how many cores does it have, how many levels of caches does it have, and how are the caches organized. There's always a need for coming up with better heuristics. In other words, we've not seen the last word yet on scheduling algorithms for parallel systems. Thus far, we've seen how to design and implement scalable algorithms that go into the guts of an operating system for a parallel machine. Now it is time to look at a case study of an operating system that has been built for a shared memory multiprocessor. This operating system is called Tornado. And the purpose of this case study is to understand the principles that go into the structuring of an operating system for a shared memory multi-processor. Thus far, we have covered a lot of ground on parallel systems. And as a reminder, I want to tell you that you should be reading and understanding the papers by Merigram and Scard on synchronization. Anderson and others on communication issues in parallel systems, Skilante and others on scheduling federal loans, scheduling. To get the full benefit of all the lectures you've seen already, you definitely should read and understand all those papers. And all these papers are listed in the reading list for the course anyhow. And what we're going to do now is look at how some of the techniques that we've discussed thus far gets into a parallel operating system. So, I'm going to look at one or two examples of parallel operating system case studies, so that we can understand these issues somewhat in in more detail. Modern parallel machines offer a lot of challenges in converting the algorithms and the techniques that we have learned so far into scalable implementations. Now what are some of these challenges? Well first of all, there's a size bloat of the operating system. And the size bloat comes because of additional features that we have to add to the operating system and so on. And that results in, in system software bottlenecks, especially for global data structures. And then, of course, we already have been discussing this quite a bit, that the memory latency to go from the processor to memory is, is huge. All the cores of the processor are on a chip, and if you go outside the chip to the memory, that latency is huge. 100 to one ratio is what we've been talking about and that latency is only growing. The other thing that happens in parallel machines is the fact that, this is a single node. And we're talking about the memory latency going from the processor to the memory. But in a parallel machine, it's typically constructed as a non-uniform memory access machine. And that is, you take individual nodes like this that contains a processor and memory and put all them together and connect them through an interconnection network. And what happens with this NUMA architecture is that access, there's differential access to memory whether this processor is accessing memory that is local to it, or it has to reach out into the network and access some memory that is farther away from where it is. In addition to the NUMA effect, there is also the memory hierarchy itself is very deep. We already talked about the fact a single processor these days contains multiple levels of caches before it goes to the memory. And this deep memory hierarchy is another thing that, that you have to worry about in building the operating system for a parallel machine. And there is the issue of false sharing. And false sharing is essentially saying that even though programmatically there is no connection between a piece of memory that is being touched by a particular thread executing on this core, another thread that is executing on, on this core. The cache hierarchy may make the block that contains the individual memory touched by different threads on different cores to be on the same cache block. So, programmatically, there's no sharing, but because of the fact that the memory that is being touched by a thread on this core, and a memory that is being touched by a thread on this core happen to be on the same cache line, they appear to be shared. That's what is false sharing. False sharing is essentially saying that there is no programmatic sharing But, because of the way the cache coherence mechanism operates, they appear shared. And this is happening more and more in modern processors, because modern processors tend to employ larger cache blocks. Why is that? Well, the analogy I'm gong to give you is that of a handyman. If you're good at doing chores around the house, then you might relate to this analogy quite well. You probably have a tool box if you're a handyman. And if you want to do some work, let's say a leaky faucet that you want to fix, what you do is you put the tools that you need into a, a tool tray and bring it from the tool box to the site where you're doing, doing the work. And basically, what you're doing there is, you know, collecting the set of tools that you need for the project so that you don't have to go running back to the tool tray all the time. That's the same sort of thing that's happening with caching and memory. Memory contains all this stuff but what I need, I want to bring it in. And the more I bring in from the memory, the less time that I have to go out to memory in order to fetch it. That means that I want to keep increasing the block size of the cache, in order to make sure that I take advantage of spatial locality in the cache design. And that increases the chances that false sharing is going to happen. The larger the cache line, the more chances are that memory that is being touched by different threads happen to be on the same cache block, and that results in false shading. So all of these effects, the NUMA effect, the deep memory hierarchy, and increasing block size leading to false sharing, all of these are things that the operating system designer has to worry about in making sure that the algorithms and the techniques that we have learned, when it is translated to a large scale parallel machine, it remains scalable. So that's really the challenge that the operating system designer faces. So some of the things that the OS designer would have to do is work hard to avoid false sharing, work hard to reduce write sharing the same cache line. Because if you write share the same cache line, then it is going to result among different cores of the same processor, then it's going to result in the cache line migrating from one processor to another. And even within the same core and even within the same processor, multiple cores, and across processors that are on different nodes of parallel machine, connected by the interconnection network. So, we can think about some general principles that one, has to keep in mind as an OS designer in designing operating systems for earlier machines. The first principle is of course cashe conscious decisions. What that means is, you want to pay attention to locality. Exploit affinity to caches in scheduling decisions for instance. And you want to reduce the amount of sharing of data structures. If you reduce the amount of sharing of data structures, you're reducing contention. So, limit the amount of sharing to system data structures. We've seen this when we talked about different synchronization algorithms. We talked about how we can reduce the amount of sharing of the system data structure, so that we can limit the amount of contention, that's important to do, and the other thing that you want to do is, you want to keep the memory accesses local to every node in the multiprocessor as possible, and basically what that means is you're reducing the distance between the accessing processor and the memory. Already, the distance is pretty big when you go outside the chip, and access the memory over here. But, the distance is even more if you have a traverse interconnection network. And reach into a memory that is on a different node of the multiprocessor. So, keeping memory access local is another important principle that you want to adhere to in designing operating system for, multiprocessors. So, let's understand exactly what happens during a page fault service. So when a thread is executing on the CPU, it generates a virtual address and the hardware takes that virtual page number and looks up the TLB to see if it can translate that virtual page to a physical page frame that contains the contents of that page. Now the TLB look up fails, that's a miss in the TLB. At that point, the hardware, if the hardware is doing the page table lookup, it'll go to the page table and look up the page table to see if the mapping between the virtual page and the physical page is in the page table. And this would have been there if the operating system has already put the contents of the page in physical memory. But if the operating system has not brought in that page from the disk into physical memory then when the hardware goes and looks into, into the page table, it may not find the mapping between the virtual page and the physical frame. And so that will deserve a page table miss. And that miss is the point at which you have a page fold. So you have a page fold now that says that I don't have the page in physical memory. And so what the operating system at that point in the handler, what it has to do is to locate where on the disk that particular page, which were pages residing on the disk, and as part of the page fold service, the operating system has to allocate a physical page frame, because it's now missing in physical memory. And do the I/O to move the virtual page from the disk into the page frame that is allocated. And once it has done the I/O, the I/O is complete. Then at that point the operating system can update the page table to indicate now it has a mapping between that virtual page and the physical frame number, which was missing in the original scheme of things, and that's the reason that we have this fault. And we handle the fault by bringing in the missing page from the disc into physical memory. And we update the page table to indicate that the mapping is now established between the virtual page and the physical frame number. And then we can update the TLB to indicate that now we have the mapping between VPN and PFN, and once the TLB is also been updated, the page fault service is complete, and life is good. So that's the whole workflow in taking a virtual page and mapping it to a physical frame when there's a miss. Now let's analyze this picture and ask the question, where are potential points of bottlenecks? Now what I'm showing you here is thread specific. A thread is executing on the CPU. And looking up the virtual page, advance leading that to physical frame. It's entirely local to a particular thread and local to the processor on which that thread is executing. No problem with that. No serialization at that point. Now, moving over here, once the page fault has been serviced, updating the TLB to indicate that there is a mapping now, a valid mapping between the virtual page number and the physical page number, that is done on the TLB that is local to a particular processor and therefore it processes a specific action that's going on in terms of updating this TLB. Now, let's come to the middle structure here. This is where all the problem is. So what we have here, is the situation where we have to first allocate a physical page frame. That's an operating system function, in order to allocate a physical page frame. You have to update the page table to indicate now, that the IO has been complete and now we can have a mapping between virtual plane and physical frame. And I told you that the page table data structure is a common data structure that might be shared by the threads in which case all of these things, what I've shown you here can lead to serialization. So this is what we want to avoid. We want to avoid the serialization that is possible in allocating data structures, allocating physical resources in order to serve as a page fault. So what we are seeing here is entirely lookup, and that can be done in parallel. No problem with that. Reading is something that you can do in parallel. And similarly what is happening over here is we are updating the tlb but it is local to a processor. There's no serialization that's going to happen here. But here we can have serialization if you're not careful. So, as an OS designer and designing this particular service, page fault service, this is what you have to focus on to make sure that you avoid serialization. So if we look at the parallel operating system and page fault service the easy scenario for the parallel operating system is what I call as a multiprocess workload. And here what we're seeing is, yes you have threads executing on all the nodes of the multiprocessor, but these threads are completely independent of one another. Think of this as a separate process, this as an independent process. Maybe you have a web browser here, a word processor here, and so on. So they are completely independent processes. And if that is the case, if there's a page fault that has incurred on, on this node, simultaneously a page fault on another node, they can be handled completely independently. Why? Because the threads are independent. The page tables are distinct. And therefore, you don't have to serialize the page fault service, as I told you, the parallel operating system is going to have a page fault handler that's available in each one of these nodes. So the work can be done in parallel, so long as there is no data structures that are shared among these different units of work that the operating system has to do. And so long as page tables are distinct, which is the case in a multi-process workload, there is no stabilization. And life will be good. The hard scenario for a parallel operating system is a multi-threaded workload. Now what I mean by a multi-threaded workload is that you have a process that as multiple threads, so there is opportunity for exploiting the concurrency that's available in the multiprocessor by scheduling these threads on the different nodes of the multiprocessor. And to make it concrete, what I'm going to show you is two notes, N1 and N2, and let's assume that there are two cores available in each one of these nodes. In that case, what I can do is, the operating system may have chosen to put T1 and T3 on node N1, and T2 and T4 on node N2. So you have a multithreaded workload now executing on different nodes of the multiprocessor. And there is hardware concurrency, because there are multiple cores available. So in principle, all of these threads can work in parallel, and if they incur a page fault it is in incumbent on the operating system to see how it can ensure that there is no serialization of the work that needs to be done to service the page faults. So if we want to naiively think about what the parallel operating system would be doing in this scenario, the address space is shared and therefore, the page table is shared. And since the threads are executing on different processors, The TLBs will have shared entries, in the process of TLBs, because they are accessing the same address space. So that'll be the scenario. Now if you think about it, what we would want is to limit the amount of sharing in the operating system data structures when they are executing on different processors. In particular, for this particular mapping that I've shown you, that T1 and T3 are executing on N1 and T2 and T4 are executing on, on N2, what we would want is the operating system data structures, that they have to mess with, T1 and T3 have to mess with, should be distinct from the operating system data structures that T2 and T4 may have to mess with. And that will ensure that you can have scalability. So popping a level, what we can learn from the example that I just gave you with page fault service is in order to design a scalable operating system service in a parallel operating system. You have to think about what is the right recipe. For every subsystem that you want to design, first determine functionally what needs to be done for that service. Now you've got parallel hardware and therefore the functional part of that service can be executed in parallel in the different processors that are available. That's, that's the easy part but in order to ensure concurrent execution of the service, you have to minimize the shared data structures. Only if you minimize the shared data structures will you really be able to execute the functional part of that service concurrently on the available processors. So, less sharing will result in more scalable implementation of the service. Now the problem is, it is easy to say avoid sharing data structures, but it is hard to practice. Because it is always not very clear how in designing the subsystem, we can limit the amount of sharing of shared data structures. Now coming back to the example of the page fault service, the page table data structure that the operating system maintains on behalf of the process, it is a logically shared data structure. But if you want true concurrency for updating this data structure, it is inappropriate to have a single data structure that represents a page table for a process. Because if you have a single data structure that represents a page table for a process, in order to do the function of page fault service, you have to lock the data structure. That leads to a serial bottleneck. But at the same time if we say, well, you know, let's take this page table data structure and replicate it on all the nodes of the multiprocessor, that probably is not also a very good idea. Because then the operating system has to worry about the consistency of the shared data structure copies that are existing on all the processors, and making them up to date all the time and so on. So we can now quickly see what the dilemma is of the operating system designer. So as an operating system designer, we want the freedom to think logically about shared data structures. But later, depending on the usage of the data structure, we want to replicate or partition the data structure so that we can have less locking and more concurrency. That's the real trick. The trick is, you want to think logically. Yes, it's a shared data structure, but based on the usage, we'll replicate or partition the system data structures so that you have more concurrency and less locking for those shared data structures. So we'll keep this recipe and the principles we talked about in mind, and talk about one particular service, namely the memory management subsystem, and how we can avoid serial bottlenecks using the techniques that are proposed in one of the papers that I've assigned you for reading, which is called the Tornado System. The key property is less sharing leads to more scalable design. The secret sauce in Tornado for achieving the scalability is the concept called clustered object. The idea is that from the point of view of all the pieces of the operating system, executing on the different nodes, there's a single object reference. The object reference is the same. But the object reference under the covers may have multiple representations. So for instance, n0 may have a represenatation that it is lookating at, different from n1, different from n2 but the object reference is the same. So there is an illusion of a single object. So, that's what I meant when I said logically the operating system designer. I think of a shared data structure's logically the same thing. But physically, it may be replicated under the covers. Course, who decides to replicate it, that's the decision of the operating system as well. We'll see that in a minute. This is where the idea of clustering comes about. The na, the name clustered object. The degree of clustering, that is, the replication of a particular object, it's an implementation choice of the service, so as a designer of the service you make a decision whether a particular object is going to have a singleton representation, or is going to be one per core in the machine or one per cpu meaning it is shared by all the cores that may be there on a single cpu. Or maybe one representation of an object for a group of processes. So these are all design decisions that is left up to the implementor of the service. But when designing the service, you can think abstractly about the components of the service containing objects and each object is giving you the illusion that it is single object reference. But under the covers you might choose to implement the object with different level up replication, and of course if we are talking about replicated objects you have to worry about the consistency of the replicated objects, and this is were the, suggestion in the tornado system is to maintain the consistency of the objects. Through protective procedure call, that is implemented under the colors in the operating system. So in other words, as a designer of the service, you are going to orchestrate the sharing of the data structures that are replicated and you orchestrate maintenance of the consistency of the share data structures. Through protective procedure call that you execute across these replicas, and don't use the hardware coherence mechanism in order to maintain the consistency, and the reason for that is the hardware cache coherence it can be indiscriminate about how it does the hardware cache coherence whenever you touch a shared memory location. If it is present elsewhere, it is going to update that. And that's the reason we don't want to incur the overhead of the hardware cache coherence and replicate it. But if you replicate it then the hardware is normal. And, therefore, you have to worry about keeping these copies consistent with one another. But, of course, when in doubt, use a single representation. And that way, you have the hardware cache coherence as a security blanket when you're not sure yet about the level of clustering that you want in order to reduce the amount of contention for shared data structures. All of these may seem a little bit abstract at this point of time, but I'll make it very concrete, when we talk about a simple example, namely the memory management subsystem. Just to put our discussion in perspective, let's look at a traditional structure of an operating system. In the traditional structure of the operating system there is something called a page cache, which is in DRAM, and this page cache is supporting both the file system and the virtual memory subsystem. And the file system has opened files explicitly from the storage and they live in the page cache that is in the physical memory. And similarly, processes are executing in the virtual memory and the virtual memory of every process has to be backed by physical memory. Therefore, the page cache in DRAM contains the contents of the virtual pages. And of course, all these virtual pages are in the storage subsystem. So let's, for the purpose of our discussion, we will focus only on the virtual memory subsystem. And in the virtual memory subsystem the data structures, that are kept per process in a traditional structure, is there is a PCB, there is a process context block, or process control block, that contains information specific to that particular process in terms of, you know, in terms of memory management, the memory footprint of that process. And a page table that describes the mapping between the virtual pages that is occupied by the process and the physical memory that has been allocated in the DRAM by the operating system for backing the virtual pages of that process. And if the operating system is also managing the TLB and software then there will be a global data structure that describes the current occupancy of the TLB for that particular process. So, these are the things that it has per process and of course all the virtual pages for the process are resident on the storage subsystem so that if there is a page fault, the missing virtual page can be brought from the storage subsystem into the page cache for future access by the process. So, this is your traditional structure. And what we want to do is, for scalability, we want to eliminate as much of the centralized data structures as possible. That's the key thing that we're going to look at. How we can do that so that the operating system service will be scalable. Now using object as a structuring mechanism, let's talk about objectization of the memory management function. We first start with the address space of the process. The address space of the process is shared by all the threads, and there's gotta be representation for the address space, and that is your process object. And it is shared by all the threads that are executing on the CPU. So we can think of this process object as somewhat equivalent to the process control block in a tradition setting. Now what we wanted to do is, we're going to take this address space. Remember that I mentioned I don't want a centralized data structure that describes the address space. Because, intuitively, if you think about the multi-traded application, the different trade of the application maybe accessing different portions of the address space and therefore, there is no reason to have a centralized data structure in the operating system to describe the entire address space of that process. So what we're going to do is we're going to take the address space. And break it into regions. So there's a green region here and a purple region here. So, the green region is a portion of the address space. The, the purple region is another portion of the address space. Logically, they are all part of the operating system, data structure of the page table, but what we have done is we have sort of detonated the page table data structure, essential data structure, and said that well there is a portion of this page table data structure The green region, another portion is the purple region and of course, these regions have to be backed by files on the storage sub system so, we will call them, these objects as File Cache Managers, so similar to breaking up the address space into regions, we are going to carve up the backing store also into what we call File Cache Manager that backs each one of these regions, so, for instance, This FCM1 is a piece of the storage subsystem that backs this region R1 and similarly FCM2 backs this region R2. Of course for any of these threads to do their work the regions that they're executing in They have to be in physical memory, so that they can actually get to the instructions and data corresponding to that portion of the address space, and therefore we need a page frame manager, and the page frame manager is also going to be implemented as an object, a DRAM object, and this DRAM object is the one that serves page frames, so when You, when the page fault service needs to get a page frame, it contacts a page frame DRAM object in order to get a physical page frame so that it can then move the contents of this backing file the file cache manager for that particular region and bring that from the storage subsystem Into the DRAM, for future use by a particular thread. So that is another object, and of course, you have to do the input output in order to move the page from the, back in store into DRAM, and so we going to declare that there'll be another object which we'll call the cached object representation COR, and this is the one that is going to be responsible for knowing The location of the object that your looking for on the backing store and do the actual page IO. So we end up with an objectized structure of the virtual memory manager that looks like this. That you have a process object that is equivalent to a PCB in the traditional setting. And of course there's a TLB on the processor that's going to be maintained even in hardware or software depending on the architecture. Because architectures do it in hardware, some architectures leave it up to the software to manage the TLB. And the region object Is, as they said, a portion of the address piece. So essentially, the page table data structure is split into these region objects. And the regionobjects, there is a file cache manager that knows the location of the files on the backing store that corresponds to the a particular region. So the file cash manager is responsible for backing this region. And this file cache manager interacts with the DRAM manager in order to get a physical frame because when there is a page fault in a particular region, the file cache manager has to contact the DRAM object in order to get a physical page frame. And, once it gets the physical page frame, it kicks off this COR, which we said is the cached object representation. Of a page. It [UNKNOWN] this COR object to say, well, here is a page frame for you and here is the page on the disk. Go do it. And it is the responsibility of the cached object representation to populate the physical page frame by doing I/O with the disk in order to move this page from the disk representation into a memory representation. So, this is sort of the structure of the objectized virtual memory manager and depending on the region of the virtual memory space that you're accessing the path that a particular page fault may take will be different. if you're accessing a page that is in the green region then this is a path that is going to be taken by the page fault handler and similarly... If the page fault happens to be in the purple region then this is the path that's going to be taken by the page fault handler. So, logically given the structure, let's think about what is the work flow in handling a page fault with this objectized structure of the virtual menu manager. The third T1 is executing poor guy, and it incurs a page fault. And when it incurs a page fault, it goes to a process object. And the process object is able to say, say given the virtual page number, what region is that particular page fault falling into? So, that's the region that we want to go to in order to service the page fault. So that region object is then going to contact the file cache manager. That corresponds to this region object and to the file and the file cache manager is going to do two things. One, it's going to see what exactly is the backing file for that particular virtual that is missing. So it may be that it is a file that contains multiple pages. And so it's going to say file and offset. And that is going to be the information that has to be passed on to the COR object. Saying that, here is a file, and here is the offset in the file. And that's where the faulty page content can be found on the storage device. And of course FCM has to get a physical frame. So it contacts the DRAM object in order to get a physical frame. And so once it has the physical frame and it has the actual location of the file then the COR object can preform the IO and pull the data from the disc into the DRAM. And so now the p frame, the page frame that has been allocated for backing this particular virtual page, has now populated because of the I/O being complete. So the green arrows is showing you the completion of the I/O. As a result of that, you've got the page frame containing the contents of the virtual page that was missing in the first place. And once that is available, then the FCM can indicate to the region that your page fault service is complete. And at that point, the region can go through the process object in order to update the TLB, in order to indicate that now there is a mapping between the virtual page and the physical frame that is being populated in physical memory And now the process can be resumed. So this is the, the flow of information in order to make the process runnable again, which faltered on the first place on a, on a virtual page that is missing in physical memory. So, now that we have this flow, and we also mentioned that the cluster object has a single representation. When it is a region, it's a region. Now how do we replicate a region object? Should this be a singleton object, should we replicate it, should this region object be a singleton object, should it be replicated? If you're going to replicate it, should it be replicated for every core or a set of processors of, of a group of processors and so on? These are all the design decisions. That the operating system designer has to make. So let's look at the process object. The process object is mostly read-only and you can replicate it one per CPU. It's like a process control block, and you can make it one per CPU. And all the cores on the CPU can share this process object because ultimately, The TLB is a common entity for the entire processor and since the processed object is updating the TLB, we can have a single processed object that manages the TLB. What about the region object? Well, let's think about this. Now region represents a portion of the address space. Now a portion of the address space Maybe traversed by more than one thread. So, a set of threads that are running on a group of processors may actually access a portion of this address space. And we don't know a priori, how many threads may actually access a particular region. It's something that may have to evolve. Over time, but it is definitely a candidate for partial replication. That is, it is in the critical path of a page four, so let's partial replicate the region, not one per processor, but maybe for a group of processors, because a group of processors may be running threads that are accessing the same portion of the address space, and so we will replicate this region object one... For every group of processors. And the granularity of replication decides the exploitable concurrency from parallel page fault handling. Now, the interesting thing to notice is that the degree of replication and the design decision that we take for how we cluster, the degree of clustering that we choose for every one of these objects is independent of one another. So when we talk about the process object, we said that well, the process object can be one per CPU. And I said for region object could be applicated for a group of processes. Now what about the FCM object, FCM object is backing A region. There may be multiple replicas of this region, but all of those regions are backed by the same FCM. And therefore, what we can do is, the portion of the address space that is being backed by a particular FCM can be partioned. So, we can go for a partioned representation of this FCM. Where competition represents the portion of the agro space that is managed by this particular FCM. So, you can see that there is a degree of freedom in how we choose to replicate process object, how you we choose to the region objects. Of course we're partitioning the region objects, but once we've partitioned it, how we have replications for each, each of these partitioned regions is something that is up for grabs as an OS designer. And similarly, for the file cache manager, because it's backing a specific region, we can go for a partitioned representation of the FCM. And what about the COR, the Cached Object Representation now? Now, this object is the one that is really dealing with physically entities. It is actually doing the IO from the disk into the physical memory. And since we are dealing with physical entities, it may be appropriate to have a true shared object for cached object representation. And the, all the I/O is going to be managed by this cached object representation. Even though I'm showing you two different boxes here, in principle it could be a singleton object that is managing all of the I/O activity that corresponds to the virtual memory management. And what about the DRAM object? Now, the DRAM object you can have several representations for the DRAM object depending on how the physical memory is managed. For example, we may have at least one representation of the DRAM object for every DSM piece that you have in a single node's portion of the physical memory. So in other words, We can break up the entire physical memory that's available in the entire system into the portions that are managed individually by each processor. And there could be a DRAM object that corresponds to the physical mapped memory that is managed by each of those. processors, but you can go even finer than that if it is appropriate, but it is a design decision that is up to the designer. So we come up with an replicated process object, a partial replication for the region object, a partitioned representation for the FCM object, and maybe a trued shared object for COR, and several representations - For the DM object. So this is one way of thinking about it, but the nice thing about this objectized structure is that when we designed the objectized structure, we did not have to think about how we could replicate it when we actually populate these objects. That is a level of design decision that could be... [INAUDIBLE], because of the secret source, that's available in tornadoes is a cluster object. Clustered object offers several advantages. First of all, the object reference is the same on all the nodes, so that's very very important. Regardless of which node a particular server is executing, they all have the same object reference. But under the covers, you can now have incremental optimization of the implementation of the object. According on the usage pattern, you can have different levels of replication of the same object, so it allows for incremental optimization. And you can also have different implementations of the same object, depending on the usage pattern. And it also allows for the potential for dynamic adaptations of the representations. The advantage of course of the replication is that, the object references can access the respective replicas independently. And this means that you have less locking of shared data structures. Let's think about the process object as a concrete example. So if you think about the process object, it's one per CPU and with mostly read only. And, and therefore page fault handling can start on all of these different processors, independent of one another. And if they touch different regions of the address space, then the path taken by the page fault handling for all these different threads can be different. So what that means is that, page fault handling for instance, will scale in this case using this as a concrete service with the number of processes. It'll scale to the number of processes. And this is important because, page fault handling is something that is going to happen often, and so you want to make sure it scales with the number of processes. On the other hand, if we want to get rid of a region, then the destruction of region may take more time because the region may have multiple representations, and all of the representations of that particular region has to be gotten rid of. And so destruction of a region may take more time but that's okay because you don't expect region destructions to happen as often as handling page faults to service the ongoing needs of the thread. So, the principle again is to make sure that the common case is optimized, the common case is page fault handling. Region creation, region destruction. All of those things happen more rarely and it is okay if those functionalities take a little bit more time. Let's now talk about the implementation of clustered objects. Given an object reference, there is a data structure in the operating system called the translation table and the translation table maps an object reference to a representation in memory. So, when you have an object reference presented to the operating system that can pointed to the particular representation. Remember that the reference itself is common, the same object reference may be pointing to this replica on a particular processor, a different replica on a different processor, and so on. That's a function of the translation table, so on each CPU, this is what happens. When an object reference is presented, the operating system converts it to a representation. And this is a normal operation. Now, you present an object reference but that object reference may not be in the translation table yet, because this object has not been referenced so far. In that case, you'll have a miss in looking up the translation table. And if a miss happens, then there is another data structure in the operating system called the miss handling table. And the miss handling table is a mapping between the object reference that you are presenting and the handler that the operating system has for dealing with this missing object reference. Because if an object reference is missing, then the operating system has to find some way to make this reference point to a representation. So that's the focus of this object miss handler. What this object miss handler does is it knows the particular representation for this reference. And it is also going to make a decision. Should this object reference point to a representation that is already existing or should it create a new representation for it? All of those decisions are going to be taken by this object miss handler. Once it takes the decision, it creates a representation for this object reference and it installs the mapping between this object reference and this representation in the translation table. So that subsequently, when you present the object reference, it'll go to the particular representation for that particular object reference. So that's the work done by the object miss handler. And, so this happens on every translation miss. And the object reference is locally resolved in this case because the object miss handler is locally available and it can handle that. But it can happen that the object miss handler is not available locally. Now how will that happen? Well, the idea is that the miss handling table itself is not a replicated data structure. It's a partitioned data structure. Remember that all of these are things that are being done under the cover to implement the idea of a clustered object. So, if you think about the region object that we talked about. The region object is something that is not going to be accessed on every processor because, depending on the threads that are executing in a particular region, those are the threads that need to access the region object. And therefore, this miss handling table is a partition data structure that contains the mapping between object references and the miss handlers that correspond to those object references. So in this particular example that I give you, the miss handling table happens to contain the miss handler for this particular object reference. It is possible that when an object referenced is presented in a particular processor, the object miss handler is not local, because the miss handling table is a partitioned data structure. What happens in that case? Well, that's why you have a notion of a global miss handler, and the idea here is if the miss handling table does not have the miss handler for that particular object reference, then you go to a global miss handler. This is something that is replicated on every node. Every node has its global miss handler. And this global miss handler knows exactly the partitioning of the miss handling table. So it knows, how this miss handler table has been partitioned and distributed on all the nodes of the multi-processer. And so, if an object reference is presented on a node, the translation table will say, well, you know, this particular object reference, we don't know how to resolve it because the object miss handler doesn't exist here. And therefore, we're going to go to this global miss handler. And the global miss handler, because it is replicated on every node, it says, oh, I know exactly which node has the miss handling table that corresponds to this object reference. And so it can go to that node. And from that node it can obtain a replica, and once it obtains a replica, it can populate it in the translation table for this particular node. And once it populates it, then we are back in business again, as in this case. So, the function of the global miss handler is to resolve the location of the object miss handler for a particular object reference i. So given an object reference i, if you have no way of resolving it locally, then the global miss handler that is present on every node can tell you the location of the object miss handler for this particular object, so that he can resolve that reference, get the replica for it, install it locally, populate the translation table, then you're back in business again. So what this workflow is telling you is how incrementally the Tornado system can optimize the implementation of the objects. So depending on the usage pattern, it can make a determination that I used to have a single replica, it is now accessed on multiple nodes. Maybe I should really replicate it on multiple nodes. So that's a decision that can be taken during the running of the system on the usage pattern of the different objects. So let's return to our memory management subsystem. And of course the whole idea of objectization of the memory management subsystem, or any subsystem for that matter, is to increase the concurrency for system services that we're going to offer for the threads that are executing on the processors. So in the memory management subsystem, the main service that we're offering is the page fault service. And lets say that in this example, there are two threads T1 and T2. Let's assume that they've been mapped to the same processor, which means with the objectization that I described to you they are sharing a process object. So if T1 incurs a page fault it's going to through the process object to the region that corresponds to this particular page fault. And now let's think about what needs to happen in order to service this page fault. We might do hierarchical lockings or for instance if I want to do some modifications to the region object to indicate that I'm modifying the data structure that corresponds to this portion of the address piece, I might say that well, let's lock the process object. Let's lock the region object that it corresponds to. Let, let's lock the FCM object that is backed by this region. And let's lock the COR object that are actually going to do the I/O for me. If I do this, and now let's say the operating system is incurring a page fault for this second thread P T2. And let's say that this page fault because it is happening on the same processor, it shares the same process object, but maybe this page fault is not for this region, but it is for a different region, let's say region 2. But if we have locked the process object in servicing this page fault, then we cannot get past this process object, because the lock is held on behalf of servicing this particular page fault. And therefore, the operating system cannot process this page fault, even though you may have multiple cores on the processor. And these threads are executing on different cores of the same processor. You don't have the concurrency that you wanted. So this hierarchical locking kills concurrency and that's a bad idea. So, what you don't want to do is do this hierarchical locking. But it seems like, in order to service this page fault, if I want integrity for these objects, I want to be able to lock the critical data structures. But if the path that is taken by this page fault is different from the path that is taken by this page fault, why lock this object in the first place? So we don't have to lock this object because the path taken by the page fault service is different from this, and so hierarchical locking is a bad idea. It kills concurrency. But you do need integrity of this process object, and in particular if the reason why we locked this process object is in some sense to ensure that this process object doesn't go away. How can it go away? Well, one of the things that can happen under the covers while page fault service is happening, there could be a decision to migrate a process from one processor to another processor. And if that happens, then the process object may be migrated. And that's the reason that you have to worry about the integrity of this process object. When something is happening that is going to modify something in this process object, you don't want this process object to go away. That's actually to do with existence guarantee. So, what we're going to do is, we're going to associate a reference count with the object, and rather than do hierarchical locking, what we're going to do is put an existence guarantee. Every time this object is being used, there's a reference count that is associated with that, and the reference count is a way of guaranteeing the existence of this object. So, let's come back to this example again. So if T1 has a page fault, it first go to this, goes to this process object before it goes to the region object, because this particular page fault is going to be serviced through this region object path, but it is not going to hold lock on this object. What it is going to do is, it is going to increment a reference count for this object, saying that this object is in use, please don't get rid of it. And, subsequently, if this page fault happens, accesses the same process object, it's also going to increment the reference count. It is not going to lock this object because its path is different. It is going through this path in order to service its page fault, which is for a completely different page. The whole point of having a reference count is now, if let's say some other entity in the operating system, such as a process migration facility that says, I need to balance the load on this multiprocessor by moving some process from this processor to a different one. If it looks at this process object and it will say well, I cannot touch this process object, because its reference count is not zero. Which means that this process object is currently servicing some requests locally. And that is the way we can, we can ensure the existence guarantee for this objects and integrity of this, of this object. And that can allow us to get rid of hierarchical locking and promote concurrency for service activities that can be provided by the operating system for the same service, but where there is concurrency that is possible. In this case page fault service that can be happening in parallel for independent regions of the address space touched by the threads that are running on the same processor, but executing on different cores perhaps of the same processor. So essentially, what the reference count and the existence guarantee is giving you is, it is giving you the same facility, without doing the hierarchical locking, that was what we really wanted. What we really wanted in this hierarchical locking is the existence guarantee of this process object to guarantee the integrity of this object. We're getting that. We're getting that by associating a reference count and making sure that this particular object is not gotten rid of until the reference count goes to zero. So we're achieving the effect of hierarchical locking without losing concurrency for operations that can go on in parallel. Of course if these page faults for T1 and T2 are accessing the same region of memory, you have no choice except to go to the same region object. But there again, this is something that the operating system can monitor over time and see if, even though it is the same region, maybe this region itself can be carved up into sub regions and promote concurrency. And the limit, you can have a region for every virtual page, but that might be too much, right? And that's the reason that you don't want to have a detonation of a page table into such a fine grain partition. But you might want to think about what is the right granularity to promote concurrency for services like this to go on in the multiprocessor. So coming back again to the hierarchical locking. The key to avoiding hierarchical locking in Tornado is to make the locking encapsulated in individual objects. There's no hierarchical locking. Locking is encapsulated in the individual object and you're reducing the scope of the lock to that particular object. So if there's a replica of this region, then a lock for a particular replica is only limited to that replica. And not across all the replicas of a particular region. That's important, it reduces the scope of the lock. And therefore it limits the contention for the lock. But of course it is l incumbent on the service provider to make sure that if a particular region is replicated, then the integrity of that replication is guaranteed by the operating system through a protective procedure called mechanism that keeps these regions consistent with one another because you made a replica of that. Even if the hardware provides cache coherence, there's no way to guarantee that these replicas will be consistent. Because they are dealing with different physical memories, and therefore, it is the responsibility of the operating system to make sure that these regions are kept consistent with one another. Dynamic memory allocation is another important service that is part of memory management. It's important, once again, to make sure that memory allocation scales with the size of the system. And in order to do that, one possibility is to take the heap space of the process and break it up. So this is the logical address space of a multi-threaded application. And in the logical address space, everything is shared. But what we're going to do is we're going to take this heap portion of the address space and break it up into the portion of physical memories that are associated with the nodes on which these threads are executing. Suppose the mapping of the threads of this particular application is such that T1 and T2 are executing on N1. And T3 and T4 are executing N2, and it's a [INAUDIBLE] machine, so there's a physical memory that is local to this node N1. And therefore what I'm going to do is dynamic memory allocation requests. If it is centralized, it'll be a huge bottleneck. Instead, we're going to break up the heap and say that this portion of the heap fits in the physical memory that is close to N1. This portion of the heap fits in the physical memory that is close to N2, so dynamic memory allocation requests from these threads, satisfied from here. From these threads, satisfied from here. That allows for scale-able implementation of dynamic memory allocation. The other side benefit that you get by breaking up the heap space into these distinct physical memories that it can avoid full shading across nodes of the paddling machine. So, similar to microkernel-based operating system design that we have discussed before, functionalities in the Tornado operating system are contained in these clustered objects. And these clustered objects have to communicate with one another in order to implement the services. Because it's not a monolithic kernel anymore, it's a micro kernel where the functionalities contained in these objects. And so we need efficient inter process communication via object calls that go between an object that can be a client, can, can be thought of as a client and an object that can be thought of as a server. For instance The FCM object may need to contact the DRAM object in order to get a page frame. So, in that case, the FCM object is a client and the DRAM object is a server that is serving the request. And the way the request is satisfied is through the IPC realized by a protective procedure call mechanism. And if the calling object and the called object, the client and the server, they are on the same processor then Tornado use this handle scheduling between the calling object and the called object. It's very similar to what we discussed in the LRPC paper on how we can have efficient communication without a context switch. So local protected procedure call, you don't have to have a context switch, because you can implement this by handoff scheduling. Between the calling object and the called object. On the other hand, if the called obgject is on a remorte processor then you have to have a full context switch in order to go across to the other processor and execute the protective procedure call. And this ICP mechanism is fundamental to the tornado system. Both for implementing any service as a collection of cluster objects, and even for managing the replicas of objects. So for instance, I mentioned that you might decide based on usage pattern that I want to have replicas of the region object which represents a particular portion of the address space. If you have a region object that is replicated it's equivalent to a page table, has mappings between virtual pages and physical pages. If I replicate it, then I have to make sure that the replicas remain consistent. Whose job is it? It is a job of the clustered object implementation to make sure that replicas are kept consistent. So, when you modify one replica, you have to make a particular procedure called the other replicas to deflect the other changes that you made In the first replica. So all of these are things that are happening under the collar, so the key thing that you'll notice is that all of the management of replicas and so on is managed in software, we're not relying on the hardware cache coherence because the hardware cache coherence only works on physical memory. Now if it replicated. The physical memory is not the same anymore. But is a replica that is known only to the software. The system software. So, to the management of the replica, that is, that has to be managed by the operating system. So to summarize the Tornado features, it's an object oriented design, which promotes scalability. The idea of cluster objects in the proce, protected procedure call is mainly with a view to preserving locality, while ensuring concurrency. And we also saw how reference counting is used in the implementation of the objects so that, you don't have to have hierarchical locking of objects. And the locus of locks held by an object is confined to itself. And doesn't span across objects, or its replicas. That's very important, because that's what promotes concurrency, and that also means that careful management is needed of the reference count mechanism to provide existence guarantee and garbage collection of objects based on reference counts. And multiple implementation are possible for the same operating system object. Now for instance, you may have a low-overhead version when scalability is not important. And then you might decide to know this particular operating system object I am experiencing a lot of contention for this. I want to go for a more scalable implementation of this particular operating system object. So, this is where incremental optimization and dynamic adaptation of the implementation of objects comes into play and the other important principle that is used in Tornado is optimizing the common case. I mentioned that when we talked about page-fall handling, that is something that happens quite often. On the other hand, destroying a portion of the address based because the application does not need it any more, that is called region destruction. That happens fairly infrequently, so if it takes more time, that's okay So that's where the principle of optimizing the common case comes in. And no hierarchical locking through the reference counting mechanism. And limiting the sharing of operating system data structures by replicating critical data structures and managing the replicas under the covers is a creep up property in Tornado to promote scalability and concurrency. The main principle in structuring an operating system for a shared memory multiprocessor, is to limit sharing kernel data structures. Which both limits concurrency and increases contention. This principle finds additional corroboration in the Corey operating systems research that was done at MIT. Which wants to involve the applications that run on top of the operating system to give hints to the kernel. So let's talk about some of the ideas in the Corey System that is built at MIT, the ideas are similar to what we saw in Tornado, namely you want to reduce the amount of sharing. That's the key thing. If you reduce the amount of sharing it allows for scaling. And one of the things that is in, Corey System is his idea of address ranges in an application. And basically this is similar to the region concept in Tornado. The region concept in Tornado is under the covers. The application doesn't know anything about it. It knows that An application has an address space and the operating system decides that, well, this application has its address space, but the threads of this application are accessing different regions, and therefore I'm going to partition this data structure, the global data structure called. The page table into regions, and that way, I can ensure that there is concurrency among the page fault service handling for the different regions that, that the operating system has to deal with. Similar idea, except here the address ranges are exposed to the application. So in other words, a thread says that this is a region in which I'm going to operate and if the kernel knows the address range in which a particular thread is going to operate in, then it can actually use that as a hint in saying, well, where do I want to run this thread. If a bunch of threads are touching the same address range maybe you want to put it on the same processor. And these are the kinds of optimizations that the operating system can do. If this hint is provided by the application. Similarly, shares is another concept, and the idea here is that an application thread can say that here is the data structure, here is a system [INAUDIBLE] that I'm going to use, but I'm not going to share it with anybody. An example would be a file. A file, by definition, for a process is an operating system entity once the process opens that file. That file descriptor is a data structure that the operating system maintains and now if you have multiple threads in the same application, all of the threads have access to that file descriptor, but if a thread of that application opens the file and it knows that it's not going to share that file with anybody else. It can communicate that intent through the shares mechanism. Through the shares mechanism, it can communicate that intent to the operating system. Saying that, here is a file that I've opened, but I'm not going to share it with anybody else. That hint is useful once again for the kernel to optimize shared data structures and in particular if you have a multi-core processor and if I have threads of an application running on multiple cores I don't have to worry about the consistency of that file descriptor across all these cores. That gives an opportunity for reducing the amount of work that the operating system has to do in managing shared data structures. Another facility that is there in Corey's dedicated cores, here the idea is that if you have a multi-core, you have lots of cores, might as well dedicate some of the cores for kernel activity. And that way, we can confine the locality of kernel data structures to a few cores. And not have to worry about moving data between the cores. So all of these techniques that are being proposed in the Corey system is really trying to attack the fundamental problem that there is a huge latency involved when you have to communicate across cores. Or when you have to communicate outside of the core into the, into the memory subsystem and so on. And all of these facilities are trying to reduce the amount of inter-core communication and core to memory communication and so on and so forth. Through this lesson, I'm sure you have a good understanding and appreciation for the hard work in the implementation of an operating system on a shared memory multiprocessor that ensures capability of the basic mechanisms like synchronization, communication, and scheduling. And this is not done just once. It has to be done a new, for every new parallel architecture that comes to market that has a vastly different memory hierarchy compared to its predecessors. Can we reduce the pin point of individually optimizing every operating system that runs on a multi-processor? Now what about device drivers, that form a big part of the code base of an operating system? Do we have to reimplement them for every flavor of operating systems that runs on a new machine? Can we leverage third party device drivers from the OEM's to reduce the pain point? To alleviate some of the pain points that I just mentioned, what we want to ask is the question, can virtualization help? We've seen how virtualization is a powerful technique for hosting multiple operating systems images on the same hardware without a significant loss of performance. In the context of a single processor. Now, the question is, can this idea be extended to a multiprocessor? And this is the thought experiment that was carried out at Stanford, in the cellular disco project. Cellular disco combines the idea of virtualization. And the needs for scalability of parallel operating system, commensurate with the underlying hardware. So there is a thin virtualization layer, which is the cellular disco layer. And the cellular disco layer manages the hardware resources namely CPU, the I/O devices, memory management and so on. Now the most hairy part in dealing with any operating system is the IO management. Even in a desktop environment and a PC environment most of the code is really third-party code that is device driver code that is sitting inside the operating system. And so that is the thing that is one of the hairy parts. Managing the IO subsystem. So in this start experiment, what cellular disco does is to show by construction that you can alleviate some of the pinpoints in building an operating system, especially with this I/O management. So I'm going to focus on just the I/O management part and on how IO is handled with the cellular disco sitting in the middle between the virtual machine that is sitting on top. And the, the physical hardware sitting at the bottom. So, this particular thought experiment was conducted on a machine called the Origin 2000 from, SGI. It's a 32 node, machine. And that was the shared memory multiprocessor on which this, thought experiment was conducted. And the, operating system is a flavor of a UNIX operating system called IRIX. That's the host operating system running on top of the, Origin 2000. The VMM layer cellular disco sits in between the guest operating system, and the. Host operating system, and the way visualization is done is a standard virtual machine trick, and that is trap and emulate. And what they've done is shown the construction that it is possible to do this and do this efficiently. And let's just walk through what happens on an I/O request. The guest operating system makes an IO request. And this results in a trap into the VMM layer, cellular disco. Cellular disco rewrites this request as coming from it, rather than from the guest operating system. And. Makes the actual I/O request, this is the virtual request coming from the guest operating system, so this is the actual I/O request that is passed down to the host operating system, Irix in this case. And the Irix operating system does its own thing, the, whatever the device travel is going to do, and carries out that operation, And once that operation has been scheduled, it might indicate that, yes, I've scheduled it. Let's say, it's a DMA operation. So it might say that, yes, I scheduled it, sends it up to them, the Host Irix operating system. And the Host Irix operating system passes it to Cellular Disco, passes it to the Guest Operating System. So this is the path of dispatching an IO request. Now, what happens when the I/O request actually completes? This is where the trick comes in of trap and emulate. Because Cellular Disco has made it appear that this request is really coming from it, it is installed, when it gave this I/O request, it installed in it The place that needs to be called in the VMM layer. So when the completion interrupt comes in, normally, in any vanilla operating system, completion interrupt will go to the host operating system. But Cellular Disco has faked it When it passed the request to say that when a completion request comes in, call me. That's what was the magic that was done in the forward path. And therefore, when the completion request happens, it really calls the VMM layer, and the VMM layer does what it needs to do and Makes it appear as though it's a normal interrupt coming from the device back to the host Irix operating system and the host Irix operating system in turn passes it back to Cellular Disco and then onto the Guest operating system. So this is the trick by which it does the trap and emulate for dealing with every I/O subsystem so there's no need to change Any part of the I/O subsystem in the host operating system, everything is being managed by this trick of trap and emulate that is happening in the cellular disco layer. So, the standard virtual machine trick of trap and emulate is being used extensively in providing the services that you need. In a guest operating system, that is running on a multiprocessor. So the start experiment was, was really to show by construction how to do this idea of developing an operating system for a new hardware, without completely rewriting the operating system, by exploiting the facilities that, that maybe their already In the host operating system. Once again this should remind you of another thing that we've seen before when we discussed operating system structures and that is, lead case, showing by construction. That a microkernel design can be as efficient as a monolithic design. Similar to that, what these folks have done is that by construction they have shown that a virtual machine monitor can manage the resources of a multiprocessor as well as a native operating system. And they showed it by construction. This cellular disco runs as a multithreaded kernel process on top of, the host operating system. Irix in this case. And the other thing that they have shown the construction is that the overhead of doing it this way providing the services that is needed for the desktop operating system. Through this cellular disco virtualization layer can be kept efficient, keep the overhead low, and the virtualization can be efficient, and they've shown that it can be done within 10% for many applications that run on the guest operating system. So that's the proof of the pudding is of course, the ED. And so what they have shown is that the virtualization overhead can be kept low, by really showing how applications can be run on a guest operating system and through the services provided by the VMM layer, cellular disco, they show that the drop in performance can be kept fairly low. So that completes the last portion of this lesson module, where we visited the structure of parallel operating systems and in particular looked at tornado as a case study. This completes the second major module in the advance operating systems course, namely parallel systems. I expect you to carefully read the papers we have covered in this module which I've listed in the required readings for the course, and which served as the inspiration for the lectures that I gave you in this module. I want to emphasize once more the importance of reading and understanding the performance sections, of all the papers. Both to understand the techniques and methodologies therein, even if the actual results may not be that relevant due to the dated nature of the systems on which they've been implemented. Welcome back. We now launch into the study of distributed systems. This is an exciting field. On a personal note, I started my academic career as a researcher of distributed systems in the mid 80s. Part of the fun, but there's a lot of parallels between distributed systems and parallel systems. What fundamentally distinguishes a distributed system from a parallel system is the individual autonomy for the nodes of a distributed system as compared to a parallel system, and the fact that the interconnection network that connects all the nodes in a distributed system is wide open to the world, as opposed to being confined within Iraq. Or a room or a box. However, as the feature size of transistors in silicon continues to shrink due to advances in process technology and break throughs in VLSI technology, many of the issues that are considered to be in the domain of distributed systems and are surfacing even within a single chip. But I digress. In this lesson we are going to learn the fundamental communication mechanisms in distributed systems and what an operating system has to do to make communication efficient. As in the previous lessons, you will see the symbiotic relationship. Between hardware in the form of networking gear and the operating system software stack, particularly the protocol stack, to make the communication efficient. We'll start this lesson module with a definition and a shared understanding of what we mean by a distributed system. But first, a quiz to get you started. So, I want to know, what do you understand by a distributed system and I am going to give you three choices, nice perfectly fine if you don't get it right but I just wanted to get you thinking in terms of what a distributed system is and what your understanding is. The first choice says that a distributed system is a collection of Nodes connected by a Local Area Network or a Wide Area Network. Second choice says that, a distributed system is one in which communication happens only via messages. And the third choice is that, a distributed system is one in which events that are happening on the same node, here like A and B. The time between that is called the event time. And the event that is happening across nodes, which is a communication event, Node N1 sends a message to node N2, it's a communciation event from A to C. So, the third choice is saying that communication time TM, is much more Significantly more than the event execution time. That is the third choice so you have three choices and I want you to think about what's your definition, what's your mental model of distributed system is and how any of these choices fit or do not fit your mental model of what you think a distributed system is. If you chose all 3, your right on. We'll talk about why all these 3 choices, make perfect sense. So a distributed system is a collection of Nodes which are interconnected by a Local Area Network or a Wide Area Network and this Local Area Network may be implemented using a twisted pair, coaxial cable and optical fiber And if it is a Wide Area Network, it could be implemented using a satellite communication microwave links and so on. And the media access protocols that may be available for communication of these nodes on a Local Area Network or a Wide Area Network, maybe ATM or Ethernet and so on and so forth. That's sort of the picture of What a distributed system is, number one. Number two, there's no physical memory that is shared between nodes of the distributed system. So the only way nodes can communicate with one another is by sending messages on the local area network to one another. So there is no shared memory for communication. Between the nodes of the distributed system. No physical memory for communication between the nodes of the distributed system. And the third property is the fact the even computation time, that is the time it takes on a single node to do some meaningful processing, that computation time is what we are calling as the event computation time. That is Te. And a node may also communicate With other nodes in the system and that's what we're calling as communication time or the messaging time, TM. And the third property of the distributed system is that the time for communication between nodes in the system, TM. Is much more significantly larger than the event communication. So these are the three properties which I would like to think of to make sure that we have a shared understanding of what we mean by distributed systems. That they are connected by some sort of local area network or wide area network. A collection of nodes. And their own physically shared memory, so the only communication, only way they can communicate with one another is via messages that are sent between the nodes using the local area network. And the third property, is the fact that the message communication time is significantly larger And even computation time that happens on a single node. You probably remember a good friend, Leslie Lamport. I introduced you to him when we talked about parallel systems, and I said that we will see him again. In parallel systems, he's the one who gave us the notion of sequential consistency, and the same person that we're going to be talking about in this lecture, Leslie Lamport. And in particular LAN port has a definition for a distributed system and the definition of a distributed system verbatim goes like this. A system is distributed if the message transmission time, Tm, is not negligible to the time between events in a single process. Cause there's a time between events in a single process, there's a message transmission time, and so the definition that Lesley Lamport gives is that a system is distributed is a message transmission time tm, is not negligible compared to the time between events in a single process. What is the implication of this definition? Interestingly, even a cluster is a distributed system by this definition. We've been talking about clusters a lot when we discussed parallel systems and I told you that clusters are the work horses of data centers today. Even a cluster is a distributed system by this definition because processors have become blazingly fast, so the event computation has shrunk quite a bit. On the other hand the message communication time is also becoming better but not as fast as the computation time that happens on a single processor and therefore even on a cluster which is all contained in a single rack in a data center, the message transmission time is significantly more than The event time. And so even a cluster is a distributed system by this definition. The importance of this inequality is in the design of algorithms that are going to span the nodes of the network. What we want to make sure, because the message transmission time is so significantly larger than the event computation time on a single node In structuring applications that run on distributed nodes of a system like this, one has to be very careful to make sure that the computation time in the algorithms that you're designing is significantly more than the communication time. Otherwise, we are not going to reap the benefits of parallelism. If, most of the time you're communicating. That's, the reason why, this definition of distributed system is extremely important. We're going to look at a fun example. This is me, and I'm going to India for Christmas holidays. And I'm going to make an airline reservation. I'm going to use Expedia to make the airline reservation. So what I'm doing on my computer, I'm sending a message to Expedia, saying, hey, make a reservation for me. And Expedia chooses to make the reservation using Delta, so it sends a message. a to b is a message that I sent to Expedia saying I need a ticket to go to India, preferences, and so on. Expedia then sends a message to Delta booking the reservation that I want. Delta confirms by this message, e to f, that yes, Kishore's reservation is in. And once Expedia has received this confirmation from Delta, it sends me a message, g to h. And this message is telling me that I've go the airline reservation booked. So all of these are messages. a to b is a message, a is the sending of the message, b is the receipt of the message. And c is the sending of the message from Expedia to Delta. e is the confirmation that my reservation is in from Delta to Expedia and finally g to h is the message from Expedia to me saying that yes, you have your reservation, you can go to India in December. That's good. And then, what I'm doing is, I'm directly contacting Delta, message from me, me to Delta, asking for my preference for food. Fortunately, it's an international trip, so I'm going to get a little bit more than peanuts on, on the Delta flight to India. So I sent a message asking for my meal preference and Delta confirms that yes, you have your meal preference. That's the message k to l, is the message that confirms that I have my meal preference, I'm all set. So everything that I've described here is what you probably do on a routine basis, every time you're making any travel plans. Either contacting Expedia or some other web portal to make your airline reservation. All of this makes logical sense, right? There are several beliefs that are ingrained in this picture here about the ordering of events in the distributed system that makes all of this work. In particular, when we look at the set of events that you're seeing here as events that I'm responsible for, we think that these events are happening in sequential order. So for instance, if you look at what Expedia is doing, it is receiving my message saying that I want an airline reservation to be made, does a bunch of bookkeeping. Then sends this message over to Delta saying that well, go ahead and make this booking for him, gets the acknowledgement back from Delta. And then it does a bunch of other bookkeeping, once it gets the acknowledgement from from Delta and then it tells me that, okay, you've got it. And after that, it does some more bookkeeping to say that, well, you know to show if booking is done and I'm going to make some internal notes on the details of this booking. And so those are all things that are happening as events within Expedia. So the beliefs that we have is that processes are sequential, that is, the events that we see happening in a given process, these are the events that are happening in a given process, we expects these events to be totally ordered, right? So for instance, you wouldn't expect given this ordering of events, that you see in Expedia's profile that this event m happened before sending this message c. Right? So that's the mental model that you have, that events are totally ordered within a single process, and that's why we're calling process sequential. That is, the execution of a process is a textual order that you see. At least the apparent effect of the execution of the process that you, as a user, experience is sequential. So if I look at this particular process, h happens before i, f happens before g, and d happens before e, so all of these are things that are ingrained in our mental model of processes being sequential. The other belief is that you cannot have a receipt of a message before the send is complete, right? So you have to send the message before it can be received. In other words, the receipt of a message, which is b here, has to happen after the messages are being sent from here. Similarly, this message reception f must have happened after the message was sent from Delta. So those are the core beliefs that we have about what is happening with events in a distributed system. That events within the process are sequential. And across processes, when you have communication events, send happens before receive. So these are two core beliefs that we have about the working of a distributed system. And we call these beliefs, as they happened before, relationship. So lets dig a little deeper into what we mean by the Happened Before Relationship. I'm going to denote the Happened Before Relationship with this arrow. A happened before B. That's what this notation means. What this notation is implying is one of two things, either A and B are events in the same process which means given a belief that a process is sequential A must have happened before B. If it was a textual order A is here and B is here then A must have happened before B and that's one possibility. Or if you're asserting that A happened before B and A and B are not events on the same process but A is an event in one process, B is an event in a different process. Then there must be a communication event that connects A and B. In other words, if A is a communication event of a message, and B is a receipt of that same message. Then, A happened before B, where A is the sender of the message and B is the receiver of the message. So, this is the implication of saying that an event in a distributed system A happened before B, and these events can be anywhere in the system. Anywhere in the system an event could be happening A, and another event could be happening B, and if we are asserting that A happened before B, what we are implying is one of these two possibilities. One is that A and B are events in the same process or A is the act of a sending a message and B is the act of receiving a mess, the same message on a different node of the distributed system. The other property of the happened before relationship is that it is transitive. What I mean by that is, if we're asserting that there is an event A that happened before B. And this event B happened before C. The implication is this relationship is transitive ad therefore A happened before C. So that's the transitivity of the Happened Before Relationship. Now that I introduced to you the Happened Before Relationship it is time for another fun quiz. Consider the following set of events on node N1 and node N2. N1 is sending a message and the act of sending the message, the event associated with that is F. And A is the act of receiving the same message on node N2. And B is another event on node N1. Textually follows this send event. And G is another event on node N2. Textually follows the receive event G. Now the question for you, what can you say about the relationship between the events A and B? What can you say about the relationship between these two events A and B? I'm going to give you three choices. The first choice is A happened before B, so I'm going to say that is the first choice. The second choice I'm going to say is B happened before A. And the third choice is neither. And it's okay if you get it wrong, it's just to, sort of, prime the pump for the next concept that I'm going to describe to you. The right answer, it's neither. That is you cannot say anything about the order between A and B, given what I'm showing you here. We'll explain more about this in the continuation of this lecture. Now that we understand the" happened before" relationship and the transitivity of" happened before" relationship. I also want to introduce this notion of concurrent events. That is, concurrent events are events in which there is no apparent relationship between the events. So, for instance, I'm showing you two nodes of the disterbent system. A is an event on one node, and b is an event on another node, and you can see that since a and b are not events on the same node we cannot apply the sequential process condition to say that there is an ordering between a and b, and by the same token, since a is an event here, b is an event here And there is no communication between these two guys that connections these events in any shape or form, either directly or transitively. There is no ordering between a and b, and therefore these two events are concurrent events, not sequential events, not connected where they happened before relationship, but they're concurrent events. So, in other words, We cannot say anything about the ordering of a and b in the distributed system. So, this is the fun thing about a distributed system is that has "Happened Before" relationship which looks at either events on the same process or events across process is connected by communication and the transitivity of "Happened Before" relationship Through the native happened-before relationships give at best a partial order for all the events that are happening in the system, so there's no way for us to derive a total order by looking at the events that are happening on the same process or just looking at the events that are happening on the different processes. In the distributed system and this is a very good example why it's impossible to get a total order for all the events that are happening in the distributed system. Because there are events they are going to be concurrent, that's the nature of the game that these procesors are executing asynchronously with respect to one another and therefore The event that is happening over here. You know, if I want to look at wall clock time in one execution of the distributed program, it's possible that a in real time, happened before b but the same program when I execute it again, the second time around, it could be that this event b happened before a. And therefore, these 2 events are called concurrent events. So the important point about these concurrent events that happened before relationship is that And structuring a distributed algorithm. It's important to recognize what events are connected by the happened before relationship, and what events are concurrent events, and once you have an understanding of these two concepts, then you can build robust distributed systems and robust applications. Because if you have any assumptions about the ordering of events that are unconnected by communication like this, that can lead to an erronious program. So one of the bane of distributed programs is synchronization and communication bugs and timing bugs. And this is a classic example. Of a timing bug that you can have if you mentally think that A happened before B and that's the way you want it to happen. It may not happen. Because these two events are concurrent events. Now that I've introduce to you all important basics of events and ordering of events in distributed system. It's time for another quiz. Now this quiz is an open-ended quiz. And in this, I am giving you the same example, the my fun example of me purchasing a trip to got to India for the Christmas holidays. And, so I am showing you all the communication events. The communication events are shown by these lines here, A to B, C to D, and so on. And this is a textual ordering of events. In, that same process. So these are eventually the same process. And the question that I'm going to ask you, like I said, it's an open-ended question. I want you to identify all the events, that are connected by, the happened-before relationship. I told you that this is the notation that I'm using for happened-before relationship. So I want you identify all the events. And I'm showing you a whole bunch of events starting from A working your way through to L. So all these events. And what I want you to do is identify all the events that are connected by this happened before the relationship. You can choose to connect both events that are directly connected that happened before, as well as transitively connected what happened before relationship. And in the second box, I want you to identify all the events. That are concurrent events in the system. So there's a whole bunch of events here, and I want you to identify the events that you think are concurrent events. Meaning there is no way for, for us to assert that there is any ordering of the events in the system. So what I'm showing you here is all the events that are within a single process. Like, all these events are events within my process. These events are within expedia's process. These events are within Delta process. And I'm sure that most of you have identified all these as events that are connected by, the happened-before relationship and here I'm showing you all the events that are connected by the communication events. A is the message sending from me to expedia. B is the receiver of the message. Similarly, C to D and E to F, G to H, I to J, K to L, those are all the communication events. All of them are connected by this happened-before relationship. And if you look at this event m, that's a concurrent event with respect to the events that are happening on my process as well as the Delta process. So those are the things that I've marked as concurrent events. If you didn't get all of these right, that's okay. Returning to our original example of me ordering a ticket to go to India via Expedia and Delta. Let's now identify all the events that are connected directly by the happened-before relationship. So, if these are the events in my process, then we know that E happened before H. And, we know that H happened before I. And, we know that I happened for L. So that's the textual ordering, and we know the process is sequential. This is the ordering of events, in my process. And similarly, we can see that, if these are the events in Expedia's process. Then all of these events have to be sequentially ordered. SO, B should have happened before C C should have happened before F, F should have happened before G, and G should have happened before M. So these are the orderings of the events in Expedia's process and similarly we can derive the order of events in the delta process, as sequential. And these are all the communication events that are directly relating events happening between any two processes that I'm showing you in this picture. So, for instance, E to F is a message from Delta back to Expedia confirming my reservation. So E is the act of sending the message from Delta, and F is the act of receiving the same message from Delta on Expedia. So, those are all the communication events. And now you can also look at transitive events, so for instance. What is the relationship between, let's say, event E and event A? Well, it turns out that A must have happened before event E. And the reason is, if you look at A, it happened before B, B happened before C, C happened before D. All of these are communication events, pretty straight forward. So from here to here, it's not a communication event, but since the process is sequential, B should have happened before C, and of course C happened before D. So, since it's a communication event, sequential process D should have happened before E. And that's what gives a transitive relationship between A and E, that A must have happened before E. Similarly, we can identify other events that are transitively connected to one another. Because of the happen-before relationship. So, for instance, D and M apparently don't have any direct connection. But, through the transitivity of events that are happening sequentially, and through the communication, and sequentiality of a process, we know that D must have happened before M. So those are transitive events. And finally, let's look at concurrent events. If you think about this event M that's happening in Expedia. Basically Expedia has confirmed to me that I have the booking that I want. Then it is doing some internal bookkeeping, to record some information about me, maybe my preferences in terms of airlines and so on and so forth. And from that point of view, it is making some internal bookkeeping and that's is event M. And now if you look at this event M, it has no relationship to any of the events that are happening here. I'm showing G, this event H must have happened after G, but what about H and M? There is no relationship between these two guys. This could've happened much later than, than this in wall clock time. Or it could've happened much sooner than event M. So you can see that, H is concurrent with M, and in fact, all the events that you see here, are going to be concurrent with M. And similarly, all the events that you are seeing over here, they're concurrent with So in fact, they're concurrent with M. After this, if you look at the Delta process, after Delta has sent this message to Expedia confirming my booking, it may have done a whole bunch of events, over here. All of those events are concurrent with M, because there is no ordering between these events, and the events over here. But there is an ordering between the event, G and the event J here. Because G happened before H, H happened before I, I happened before J. So, you can see that transitivity connects events across machines. But they could be events that are happening in the distributed system, that are unconnected to other events. And those are concurrent events. That completes discussion of the basics of distributed system. Next we're going to start talking about Lampard's clock. Now that you have learned the basics of distributive systems, in particular the happen before relationshipt. We are now ready to talk about Lamport's clock. What does each node in the distributed system know? Every node knows its own events, meaning, what are the computational events that are happening in its own execution. It also knows about its communication events with the rest of the nodes in the distributed system. So, for instance, if this process Pi sends a message to process Pj, that's a send event. Pi knows about that. And similarly Pj when it gets this event, it's a receive event. It knows about that. So, these are the only two kinds of events that every node knows about, its own computational events, and its communication events with its peers. For instance, process Pj has no idea about the local computational events of Pi. Lamport's logical clock builds on this very simple idea, and the idea is that we want to associate a timestamp with every one of the events that are happening in every process in the entire distributed system. How are we going to do that? Well, we're going to have a local clock, Ci over here, And Cj over here. And this local clock can be anything, can be counter, it can be a simple counter and what we are doing is when I want to associate a timestamp with a particular event that's happening in my process what I'm going to do is I'm going to look at this counter and see what the value of the counter is, associate that counter value as a timestamp for this event A. So for this instance here I've said that timestamp for a is two. And this counter monotonically increases as we pile up events in our system. So once I've associated this timestamp two with this event a, I cannot associate the same timestamp with other meaningful events that are happening in my process. And therefore, I'll increment the timestamp. It's completely up to your implementation as to whether you want to increase this counter value by one, or two, by a thousand, it doesn't really matter. So, in this case for instance I've incremented the timestamp counter by two, and so the next even that I'm going to have in my process B is going to have a timestamp of 4. So that's the idea behind having a monotonically increasing counter to associate logical timestamps with the events that are happening in my process. Now, what about these communication events? Well, in particular, let's look at this case here. A is a communication event on process Pi. And this communication event is going to have a timestamp of two associated with it, because that's the time that I generated this communication event, sent this message over. Comes over to process Pj. When process Pj receives this, that's an event. And so let's say we call it event number d. And we have to associate now a timestamp with, with event d. How do I go about assigning a timestamp with this? Well, we know that this timestamp that I'm going to associate with this event d, has to be greater than the timestamp associated with the send of that message, right? Obviously you cannot receive a message that has not been sent yet. And therefore, we're going to say that d should have a timestamp which is at least greater than a for sure. Now, what else does d depend on? Well, it depends on other things that are happening in my own process. For that, I need to know what the current state of my local counter is. So for instance, in my execution as I'm showing you here I haven't done anything meaningful yet and therefore my local counter is still pointing to zero indicating that there have been no meaningful events here. So when this message comes, that's the first time I'm going to do something meaningful in this process. And I have to associate a timestamp with d but I cannot associate the timestamp of zero with it because the timestamp that I associate with d has got to be greater than the timestamp that is associated with the send event on process Pi. And since the send event has this timestamp two, I have to associate something higher than that. And so I associate a timestamp three, with the receipt of this message. That is, this particular event is going to have a timestamp of, of three. So, the two conditions that we have talked about. One is that events that are happening in the same process I'm going to have a monotonically increasing counter and I'm going to use that to associate timestamps with the events that are happening in the same process. That's the first thing. That is if I have two events a and b in the same process and I know that a happened before b, because, textually, in the process because the process is sequential, a happened before b and therefore the condition is that the timestamp associated with a has got to be less than the timestamp associated with b. Pretty straightforward. And similarly the second condition is that if I have two events, a and b, and a happens to be a send event on some process and d happens to be the receive event on another process then we know that the receive event has to be after the send event. And therefore the second condition is that Ci(a), that is a timestamp associated with a send event, has got to be less than the timestamp associated with the receipt of the same message, Cj(d). So in order to make this condition, the second condition, valid, we're going to choose the timestamp to associate with the receipt of a message d as the max of the timestamp that is associated with the send event incremented by some quantity. Ci(a) plus plus. So in this case, it happens to be two. So increment it and then say three and then you want to know what the local counter is saying. So, Cj, so what we pick as a timestamp to associate with d is the max of the incoming timestamp with the message and the local counter, whatever it is pointing to. So that's how I'll pick the timestamp to associate with a receive event. This brings up a very interesting question and that is, what about timestamps of events are happening concurrently in a distributed system, and before I talk about that, I want to give you a little quiz. Let's say in the distributed system, there are two events. I don't know where they are happening. There's an event called a and there's an event called b. Somewhere in the distributed system, these two events are happening. And it so happens, when I look at the record of all the time stamp associated with the events, I see that the time stamp associated with a is less than the time stamp associated with b. So I want to make sure that you understand the premise of the problem here. What I am saying is that the time stamp that is associated with the event a happens to be less than time stamp associated with the event b. That's what I am observing, by looking at sort of a log record of all the events that took place in the system and now my question to you is. If C of a is less than C of b, does that mean that a happened before b or does it mean b happened before a or does it mean a happened before b with the condition that it's either the case that a and b are events in the same process or a is the act of sending a message and b is the act of receiving the corresponding message. So you have to pick the right choice among these three choices. This conditional statement is the right choice and the reason is because of the fact that the time stamps themselves don't give the whole story because all that we have are partial events happening on every node in the system. And we'll elaborate that a little bit more when we describe Lamport's logical clock in its entirety. So now we're ready to describe the conditions for the logical clock proposed by Lamport. First of all, the first condition is that if I have two events, A and B, in the same process, the first condition says that the clock value associated, or the time stamp that it associated with event A has to be less than the time stamp associated with event B. In other words, we have this counter or a logical clock on every node of the distributed system that is monotonically increasing as events happen on that process. The second condition is that when we have a receipt of a message, we want to make sure that the receipt of the message has a time stamp that is greater for sure than the sending time span. So in other words if A and D are the act of sending a message from process I and D is the act of receiving the same message on process J, then what we are saying is the time stamp associated with the event A has to be less than the time stamp that is associated with the event D. In other words, we want to choose the time stamp associated with D as the max of the time stamp that I see in the incoming message, incremented by some value. Whatever the local counter is saying. These are the two things that I'm going to look at and decide the max of that as the time stamp to associate with event D. And if the events are concurrent, in this case, if I look at this picture here, A is the act of sending the message, D is the act of receiving the same message. B is an independent event that's happening on process PI, it has nothing to do with this event D that is happening on process PJ. And these are concurrent events. So the concurrent events, we've already seen this when we talked about the happened before relationship. In the case of concurrent events, the timestamps are arbitrary. Just by looking at the time stamp, I cannot say that B happened before D because, if I see the time stamp associated with B here, happens to be four. Over here, we picked the time stamp for D by saying that it has to be at least greater than the incoming time stamp. So we gave it a time stamp of 3. And so if I look at these two events, B and D, D has a time stamp that is smaller than B, but that does not mean that D happened before B, because these two are concurrent events, and therefore there's no way to know which event happened before the other. So in other words, just because we find that there is an event X, which has a time stamp that is smaller than a time stamp associated with another event, Y, doesn't mean that X happened before Y. So, while this condition is an important condition, the condition that if an event A happened before B, as we show in this picture, we have to ensure that the time stamp associated with event A is less than the time stamp associated with event B. But the converse is not true. Or in other words, if I have two events, and it so happens that the time stamp associated with this event X is less than the time stamp associated with this event Y, that does not mean that X happened before Y. This is very, very important. What that means is that Lamport's logical clock gives us a partial order of events happening in the entire distributed system. So if I take any one process, I know all the events, the ordering of all the events that happened on this process both the events that happened sequentially in this process itself as well as events that happened in which this process happened to communicate with other processes. In this case it sends a message over here. Similarly by looking at the record of all the events accumulated on process Pj, I can know the order in which the events happened in this process, in which process PJ had a part to play meaning all the local events as well as communication events that PJ participated in when it communicated with the other nodes in the entire distributed system. This is what the Lamport's logical clock gives you is a partial order of all the events that happened in the distributed system. Is partial order good enough for constructing deterministic distributed algorithms? It turns out, it may be sufficient for many situations. The airline reservation example I started with would work fine with a partial order of events dictated by Lamport's clock. But there are situations where there maybe a need for a total order of events in the distributed system. Let's look at an example. Here is an example to illustrate the need for a total order. I'm going to use my own, personal life example to illustrate the need for total order. I have, one car and my family consists of my wife, my son and my daughter, and we share this single car. And what we want to do is make sure that we can make local decision on who gets dibs on using the car at any point of time. And we're going to use Lamport's clock for this. So what we do is, whenever we want to get the car for our personal use, we're going to text everyone with a time stamp. I'm going to associate a time stamp, if I'm, if I'm requesting the car, I'm going to text every one, and associate a time stamp with that request. And it is a logical time-stamp and similarly my wife would do the same thing, son and daughter all of us do the same thing. And how do we pick the winner, well, locally we can look at the time-stamp of request that have come in from others and my own request. And whoever has the earliest time stamp wins. Pretty simple, right? So pretty simple, everybody is going to make a local decision, looking at the time stamps of request that have come in from others and say well, you know right now, it's my son's turn to use the car, or my daughter's turn to use the car and so on. But what if the time stamp, because these are locally generated by each one of us, happens to be the same. So, for instance, let's say my son makes a request. Takes, sends a text message. My son makes a request, sends a text message with a time stamp ten to all of us. So this is the blue arrow that's going everywhere, so that's indicating to all three of us that he wants the car, and timestamp ten is when he generated the request. So happens, my wife also makes a request for using the car, exactly with the same timestamp ten. And that's the purple arrow that you see. So, now we have a problem. And the problem is, all of us are looking at these text messages and trying to make a decision, who's got the dibs on using the car? How will my son and my wife know, given that both the time stamp is the same, which one is the winner for using this car? Now, what we do is, we're going to break the tie, and I'm going to stipulate that age wins. And therefore, in this case, if the time stamp happens to be exactly the same, then my wife, by seniority, is the winner. She gets the car. So, that's how we break the tie. You can see, through this example, that there is a need for total order in decision making when you have a distributed system. And you want to make local decisions without bothering anyone, based on information that you have, but you have to make that local decision unambiguously, because you cannot have both my son and wife thinking that they have the car at the same time. That'll be a problem. So, whenever there is a tie, we have to break that, and that's the need for the total order. So having seen the need for total order and an ambiguous decision making in the distributed system let's now introduce Lamport's Total Order formulation. This is the notation we're going to use for Lamport's Total Order. And so what we are saying here is that if there are two events A and B in the distributed system, A happens to be an event in this case on PI And B happens to be an event, on process PJ. If we want to assert that, a totally is ordered ahead of b, that is a precedes b in the total order. This is true only under the condition that either, the time stamp associated with A is less than B or the time stamp happens to be the same and there is some arbitrary other function that helps us to un ambiguously decide which event precedes the other and for instance I might say that the process ID that I associated with PI and PJ, that maybe the one that I use to break the tie. In my family car example, I told you that the seniority of the family member makes the decision in terms of how we break the tie. So for instance in this, in this case if process Pi has a process ID 100 and process Pj has a process ID 200 then we could say that the arbitrary decision making in the case of a tie is that whichever process has a lower process ID, that's going to be the winner. So we might, we might decide that in this case, if time stamp happens to be the same, then since Pi is less than Pj, I'm going to say that a preceeds b. So it's an arbitrary, well-known condition for breaking a tie. So every process in the entire distributive system is going to use the same well-known, arbitrary condition in order to break to tie. So that also brings up another interesting point, and that is there is no single total order. The single total order comes from the choice of the arbitraty well known condition. I gave you the example of my family car, we broke the tie in the family car situation by saying that the seniority is the winner. Tomorrow as a family we could make the decision that the youngest person is going to win. In that case, my daughter will have dibs over the car over everybody else in the case of a tie. The other important point to understand is that all of this idea of associating logical time stamps with events, and then deriving a total order from the logical time stamp using this kind of a, a method of saying we going to just believe the time stamp associated with the respective events. We going to believe that time standard are associated with the events and use those time stand as a way of ordering them, so that we can develop total order. And if it happens to be a tie, everybody uses a well known arbitrary condition to break the tie and that's how we derive a total order, and once we have derived the total order, the time stamps are meaningless after that. We don't care about them anymore. The whole idea of having these logical time stamps. Creating a partial order. And from the partial order deriving a total order using this formulation. For total ordering. Is so that we can get a particular total order. And once we get the total order, timestamps are meaningless. So, in this question, I'm showing you three processes, P1, P2 and P3. And these are the events that you can see happening in these three processes. And, what I want you to do is to derive a total order and in deriving the total order, we're going to use Process ID to break the tie. And smaller the Process ID, the higher the priority. In other words, P1 dominates P2 dominates P3. So, using that, go ahead and derive a total order for this set of events that I'm showing you happening in the distributed system. So first we can write down the partial orders that we see in the distributed system. You can see that 'a' happens before 'b', 'b' happens before 'c', 'c' happens before 'e' and these are purely coming from the chain of communication and local events that are happening over here. When we come over here, we also observe that 'f' happens before 'e' that's because of sequentiality Of this process. And similarly, we can see that in process P1, a happens before d. So these are the partial orders and we already have an ordering for the events that follow this chain because the logical time stamp are assigned in this fashion. And in order to derive a total order we said we basically will believe the event time stamps. To order them totally. So we can order these events totally. A1, B2, C3, E5. We can order them totally. Now let's look at the concurrent events that are happening. The concurrent events that are happening is d over here is concurrent with all the other events in the other processes. D is concurrent with b and c. It is concurrent with f and e. And similarly if you look at this event f, it's concurrent with all the events that are happening in process P2 and P1, right. So this is what you as the concurrent events. So now what we have to do is, given that these concurrent events, we have to somehow Fit them into a total order. As I said before no problem in fitting these guys into a total order because they already have time stamps that distinguish them from one another. The time stamp associated with a is 1, b is 2, c is 3, e is 5. No problem with that. So the real problem comes with f and d. Now the sequentiality of this process is what made the time stamp associated with e to be 5 because the message that came over here had a time stamp of 3 but we associated a time stamp of 5 with e. Because the local event preceding e have a time stamp of 4, so we pick 5 as the time stamp to associate with e, so that the sequentiality of this process p3 is respected. So, we've got this, and now we've got f and d, and so f obviously is going to sneak in Before e, that, that's no problem, that is coming from the sequentiality, but where will we put d, do we should be put d after e? Or should we put d before e? In the total order. This is where the breaking the tie using process id comes into play, Because these two guys are concurrent events in the system, we are breaking the tie using the process ID. P1 happens to be less than P3 in process ID space and therefore we are going to say in the total order, dis going to be ahead, totally ordered, before e. So the final ordering that we end up with, the total order that we end up with, is a0, b, c, F, and then d. And then e, so that's the total order that we come up with. Respecting the logical time stamp associated with the events, and breaking the tie using the process ID. Now, low let's put Lamport's clock to work for implementing a distributed mutual exclusion lock algorithm, and it is going to be very similar to the car-sharing example that I showed you before. And also, you will notice that we've talked about locks in a shared memory multiprocessor where we have shared memory to implement the lock. But now, in a distributed system, we don't have shared memory. And we have to implement a mutual exclusion lock using Lamport's Logical Clock. So, essentially what is going to happen is that any Process that needs to acquire this lock is going to send the message to all the Processes. And of course the intent to get a lock may, may emanate simultaneously from several processes. That's perfectly feasible. So the algorithm is as follows. Every process has a data structure, a queue data structure. And those are the queue's that are associated with process P1. This is the queue that is associated with process P2. And this is the queue that is asssociated with process Pn. Every process has its own private queue. And the private queue is ordered by the happened-before relationship that we have discussed so far. So requests for a lock are going to be time stamped and the protocol is as follows. To request a lock, what a process is going to do is send a message to all the other Processes that I want this lock and my time stamp is such and so. So it's going to associate the local timestamp that it has from its counter, which is its logical timestamp. It's going to associate that timestamp as the request time for the lock and send the message to all its peers. And all the peers, what are they going to do? Well, two things. One is they're going to stick that request into the local queue. When a request comes from process Pn, P1 puts it into its queue, appropriate place in the queue, because it is ordered by the timestamp. The smallest time stamp being the highest priority request pending. So it puts it in its queue. And the second thing it does is every process, when it gets a request, puts it in its queue and then acknowledges the request to its peers. So let's look at the process P2 here and P2 generated it's request at timestamp 10. What it did was first put its request in its local queue and then it sends a message to its peers. And these guys, when they get the request, they look at their own local queue and say well you know there is a request pending In my queue, which has a time stamp of 5, and this request that I just got from P2 has a time stamp of 10, so I'm going to order that behind the previous request. I put it over here. I put it over here and once I do that I'm going to acknowledge this request by sending a message back to P2. And similarly this guy sends an acknowledgement back to P2. So that's how the protocol works. Every request is sent to all the other processors and every process when it receives a request, it puts it ordered by Lamport's clock in its own local queue. And then acknowledges the request with an act message. Now, what happens when there is a tie? Well, when we have a tie, we break the tie by giving priority to the process that has a lower process ID so that's how this algorithm works, so that every process can unambiguously make a decision as to where to place an incoming request in the queue. So an example of the state of the queue is as shown. The thing that should jump out at you immediately is that the state of the queue is not the same in all the processes. For instance Process 1's queue contains its request that it generated at time 2, but I don't see it yet in the other queues. Is this possible that the queue can be inconsistent with one another? Of course it is possible. The reason it is possible is because a Process, when it generates a request, puts it in it's queue and then sends a message out. This message is going to take some time to reach the other nodes in the distributed system. So, it sent the message and after it sent the message it got requests from other Processes and it has put it in its queue. And it is possible that this message, all the messages may not take the same amount of time to traverse a network. We have no idea what's going on in the network and therefore it so happens that P1's message is still in transit. Whereas the request messages from P2 and Pn have already made it everywhere, and it is in the queues of all the Processes, but P1's message unfortunately, it's taking a slow route throught the network and it is still in transit. And in fact, P1 has subsequently received P2's and PN's message and put them into its its local queue. It is just that P1's message hasn't reached its peers yet and that's how you get this situation. So the whole purpose of this exercise is to unambiguously get the mutual exclusion lock for some process that is competing for it simultaneously. Now how does a process know that it has the lock? So I have to make the decision that I have the lock. How do I make that decision? Two things have to be true for me to think that I have gotten the lock. So the first thing is that my request has to be at the top of the queue. So now you see the messages that I talked about, that is P1's message to P2 and Pn not having reached the destination, eventually they reach the destination. And they have acknowledged it. And, as a result of that the queues are consistent now. P1's request is at the top and it also has received acknowledgements from everybody else. So the way you can make a decision that you have the lock, unambiguously, in the entire distributed system. Two conditions. First thing, my request is at the top of the queue. The second thing is I've received acknowledgments from all the other nodes in the system. In this case, all the other nodes were not requesting this lock so they've sent me acknowledgments. And I've received all the acknowledgements and there is no other request that is ahead of me. I've also received lock requests from P2 and Pn and they are later than mine and that's how they've been ordered in the queue. So the two conditions I'm going to look for to make a decision locally that I have the lock is my request is at the top of the queue and I've either received acknowledgements from all the other nodes in the system, if nobody else is competing for the lock at the same time, or all the requests that I've gotten so far are later than my own lock request. Let's say that I haven't received the acknowledgement for my request from Q, Q2, and Qn. Can I go ahead and assume I have the lock? Yes, I can. Why? Because Even though these guys have not sent me the acknowledgment yet, it's coming, slowly coming. But I've received lock requests from them, with timestamps 5 and 10 respectively. Therefore I can make an unambiguous decision that my lock request precedes all the other lock requests at this point of time. And I can go ahead and get the lock. I'm sure you've figured it our already but since we are following Lamport's cCock in implementing this mutual execution lock algorithm, the ACK message for a particular lock request is going to have a later timestamp than the time stamp associated with the request itself. So you can see that Lamport's Logical Clock, with the addition of a way of deriving a total order from the partial orders given by the Lamport's clock, allows us to unambiguously make a decision locally based on the state of local queue as to whether I have the lock or not. Now lets talk about how I go about releasing the lock. So if I have the lock. I have used it for a while and now I am ready to say, well I am done with a lock, I can release it. What do I do? Well I am going to send an unlock message to all the other guys. The first thing that I do, of course, is get rid of the entry that I have in my queue because I am done with the lock. I can remove it from my queue. Once I remove it from my queue I am going to send an unlock message to everybody else. So the state of the queue indicates that the unlocked message hasn't reached yet. It is in transit. It is going to eventually reach these guys. And when the peers receive the unlocked message they're going to basically remove the entry, the corresponding entry, from the respective queues. So P1's turn with using the lock is complete now. It has done its lock and has done its unlock and now other Processes in the system, if they're competing for the same lock, can use the same decision making process to figure out whether they are the winners for getting the lock next and using it and entering the respective critical sections. So, we can informally talk about the correctness of the distributed mutual exclusion lock algorithm. The correctness is based partially on some assumptions and partially on the construction. The construction is that the Q's totally ordered by Lamport's logical clocks and the PID to break the ties. But that's part of the construction of the algorithm. But it also is based on some assumptions that we make and the assumption is, messages between any two processes arrive in order. So messages don't crisscross each other but if I send a message and I send another message, the first message is going to reach the destination first, second message is going to reach the destination second. And that's what is meant by saying that messages arrive in order at every node in the distributed system. And the second assumption is that there is no loss of messages. So every message that is sent is definitely received, in order. So these are two fundamental assumptions that are responsible for this algorithm being correct. Now that you have seen Lamport's mutual exclusion lock algorithm, time for another quiz. So the question for you is, how many messages are exchanged among all the nodes for each lock acquisition followed by a lock release? Every process, what it is doing is making a lock request and it follows the algorithm that we just mentioned, uses the lock in the critical section. And once it is done with that, it unlocks by sending messages again as you saw. And the question to you is, how many messages are exchanged among all the nodes for each lock acquisition followed by a lock release? That is, combination of lock plus unlock. How many messages does it generate? Is it N minus one messages? 2 to the n minus 1? Or 3 to the n minus 1, where n is, the number of nodes in the distributed system. The right answer is, 3 times N minus 1, and we'll explain that in the next frame. So let's look at the messaging complexity of the mutual exclusion lock algorithm. The lock primitive, when a process makes a lock request, it sends N minus 1 request messages. Because there are N nodes in the distributed system, there are N minus 1 peers, and so every node has to send a request message to all its peers, so N minus 1 messages are the request messages sent out. And in response to these request messages, every peer is going to acknowledge a request message. So, they're going to be N minus 1 messages traversing the network, which are the acknowledgement message for this lock request. And then, the process is happy using the lock for the critical section. And it gets to the unlock primitive, and the unlock primitive, once again, we're going to send N minus 1 unlock messages, through the unlock primitive and while sending a unlock message to every one of the peers in the distributive system, so N minus 1 messages are sent over the network. Interesting thing that you notice is that there's no acknowledgment for the unlock message because of the assumption that we make that messages are never lost. And therefore, when I send an unlock message, I know that everybody will get it, everybody will remove my request from the respective queues and go on with life. And therefore there is no acknowledgment for that. And so if you count the number of messages that are involved and a lock plus unlock, totally, we have 3 to the N minus 1. That's the total number of messages that are incurred, that is a message in complexity, of the distributed mutual exclusion lock algorithm. That begs the question, can we do better? And the answer is yes, and the reason is going back to the condition that I said that is used in making a decision as to whether I want the lock or not. If you recall, I told you that the condition is, my request has to be at the top of the queue, and second, I should have either gotten acknowledgements for that request from everybody else, or I've received a lock request from my peers that have a time stamp that is later than my own lock request. If the lock request that I received from my peers have time stamps that are later than mine, I know that they are going to wait for me to be served before they're going to use the lock. And therefore, what I can do if I am a receiving node for a lock request, when I see a lock request, normally I would go ahead and do an acknoweldgement. But, when I get a lock request and I see, hey, this guy's lock request is going to be after mine, so I don't have to send an acknowledgement yet. What I can do is wait til I'm actually going to unlock. My unlock itself can serve as the acknowledgement for that particular node that has made a lock request that is later in time than my own. So in other words, we can defer the acknowledgements if my lock request precedes my peers lock request. So we're combining the acknowledgement for a lock request with the unlock. So if I do that, then I can bring the message complexity down to 2 the N minus 1. So what we're doing is to gather the acknowledgement messages if in fact our own lock request is ahead of an incoming request that I see from up here. That's how we can reduce the messaging complexity of this algorithm to be 2 to the N minus 1. By the way, the distributed mutual exclusion lock problem have been a fertile ground for researchers to think about new algorithms that can shave the messaging complexity even further from this 2 to the N minus 1, and I invite you to look at the literature to see other works that have been done to reduce the message complexity to even smaller numbers than 2 times N minus 1. So far, we've been dealing with this funny virtual time or logical time. But there are many real-world scenarios where this logical time may not be good enough. And in such situations, this logical clock may not be sufficient. I'll give you an example. Let's say that I owe you some money. And I tell you by calling you on the phone, that I'm going to credit my account in my local branch at 5 p.m. I'm telling you on the phone that I'm going to credit my account at 5 p.m., and so any time after 5 p.m., you can withdraw money from my bank, and we'll be square. Now you're a nice guy, so you want to give me some leeway. So you tell your branch that, you know, at 8:00 PM, go ahead and debit from Kishore's account the money that he owes me. So your branch is going to basically do a debit call to the central bank server asking for the money that is owed by Kishore to be transferred to your account, so that's what is going to happen. And so you schedule that at 8 p.m. You've given me enough time to make sure that you have indicated to your bank, you have enough money, so that my debit transaction can go through. And you would think it should go through, right? But it turns out, that your branch's local time is far ahead of real time. Well, it thought it was 8 p.m., it was not quite 8 p.m., yet. Because it's way ahead of real time. And so I am exactly at 5 p.m., keeping my word, exactly at 5 p.m., my branch happens to be good with the time. It's, it's in sync with the real time. And so at 5 p.m., I've done the credit of the amount that I owe you to my central bank server. But unfortunately, the central bank server, in real time, it got your message much earlier than the time at which I sent my message. It's not looking at any logical time. It is looking at real time, saying, well, there's a debit transaction coming in. Is there money in the bank for paying those debit transactions? No, it isn't. So your request is declined. And this is coming about because of the fact that in real world scenarios, logical clocks are not good enough. And in particular, what caused this problem is the fact that your notion, your branch's notion, of real time is completely at odds with real time. And the reason that can happen is because the computer at your local bank may have a clock that is drifting with respect to real time. So is drifting meaning that it is not keeping up with the real time. It's either going faster than real time or it is going slower than the real time. It so happens that my, my branch's time is is perfectly in sync with the real time, but that doesn't help me. And this is a real world scenario that you have to worry about. And such anomalies occur due to two things. One is individual clock drifts. Because if you think about a clock, clock is a piece of circuitry, and you expect that for every second of real time, your clock is also going to click a tick by the same one second. But if it doesn't, your idea of real time is going to slowly drift. That is called individual clock drift. And also, there is a possibility that there could be a relative drift between the clocks that are there in different processors. This clock can be ticking at a particular rate, and this clock would be ticking at a different rate from my clock, and that can cause a second source of discrepancy. And these anomalies are nasty things that we have to avoid in order to make sure that in real world we can have some guarantees about what goes on. So that brings us to Lamport's Physical Clock, and the notation we're going to use for that is this funny symbol here. So, this is saying that in physical time, in real time, event a in the distributed system happened before b. So if I want to make sure that, that an event a in the distributed system anywhere happened in absolute, real time before b, so Pi, there's a process Pj. Event a is happening on Pi and event b is happening on Pj. And what we want to make sure is that the time stamp associated with a has to be less than the time stamp associate with b. If I want to guarantee that a, in real time, happened before b, so, a in real time happened before b, that is how you have to read this notation, that, in real time, the event a happened before b. And in order to satisfy that, the condition is, the time stamp associated with a has to be less than the time stamp associated with b. So to guarantee this, and we are talking about real time here, so real time stamp associated with a and b. In order to ensure that the real time associated with these events give you this guarantee, you have to have certain conditions associated with the clocks that are on the machines Pi and Pj. And the first condition, which I'll call PC1, I'll refer to that as PC1 later on, the condition is a bound on individual clock drift. So PC1 is a condition which gives a bound on individual clock drift. Informally, what this condition is going to tell you, is that the clocks don't drift that much. So let's talk about this. If, what is the time that is read on process P1 at time t? If t is the real time, at time t, real time t, I look at the clock on my machine and that is Ci of t, what should it read? Well, it should read t. Now, if it doesn't, that's when we are saying it is drifting with respect to t. And so what this equation is saying is dci over dt is the clock drift. The absolute value of that drift is a very, very small. So in other words, what we are saying is, all the clocks in the distributed system, whether we are talking about Ci on Pi or Cj on Pi all these clocks are running approximately correctly. So that the clock individual drift, is very very small. So k is the individual clock drift, and we are hoping that it is very, very small. And you can see that if Ci of t is equal to t. Then dCi of dt should be equal to 1 and therefore this would be a 0. So, the left hand side of this will be a zero and so that's why we're saying that k has to be a very small number. The second condition is that the mutual drift between the clocks on different nodes of the distributed system should be very small. Should, there should be a bound on mutual clock drift, so that is captured in this condition saying that. For all ij, any pair of nodes in the entire distributed system, the difference between the time that I read on my clock and the time that I read on somebody else's clock is very, very small. because this is the mutual clock drift. As I said earlier, at real time t, my clock should also be reading t. This guy also should be reading t. If it doesn't, that's when you have a drift. What we're seeing is the mutual clock drift between any two nodes in the entire distributed system is bound by a small quantity, epsilon. So k and epsilon are the two important parameters In the physical clock condition. Intuitively, we're going to argue that these values, the absolute values of these individual clock drift and mutual clock drift, has to be negligible compared to the inter-process communication time. So what we're going to look at now is the relationship between the inter process communication time and both the individual and the mutual clock drift that I described to you. Let Mu be the lower bound on the inter process communication time. So let's now derive the conditions under which we can assert that if we have an event A on P I. And in real time, it's supposed to precede an event B on Pj. What are the conditions that should hold in terms of mu, k, the individual clock drift time, and epsilon that is the mutual clock drift time? This first condition is pretty straightforward, It is coming from the fact that Ideally, the clocks are perfectly synchronized. You know that at time, real time t, ci of t and cj of t should be exactly the same, right? You expect that if it is, if the clocks are perfectly synchronized and it is keeping with real time, ci of t is equal to cj of t, equal to t, where t is the real time. But they could be in the original clock drift and user clock drift, which makes them different from each other. And all that you're seeing through this first one is that this is the act of sending a message and this is the act of receiving the message. The time stamp that I'm going to give to this, the real time that I'm going to give to this by reading my clock it better be higher than the time at which the message is actually sent. And in order to guarantee that we have to look at what would be the time that it takes for this message to go from here to here. That's coming from this mu this lower bound on IPC. So, if the message is sent at Ci of t with respect to Pi. Then, the time on PI when this message is received over here, is going to be CI T plus mu. This is a local reading of the clock, when the message would have arrived at PJ. So this is the time elapsed between sending the message, and when my peers should have received the message. So, what we are saying is in order for making sure that pj will have a time stamp that is at least greater than Ci, you want to make sure that the, the time reading that I have on my local clock, t plus mu, should be greater than the time reading at the time that I sent the message. So the time that I sent the message from Pi my pierce time was CG of t and and all that we are saying is in order to make sure that there is no anomaly the first condition has to hold that says that. The disparity between the two clocks is within this interprocess communication plane. That's all this is saying, that the disparity of the mutual drift is within this interprocess communication plane. And the second equation is basically a difference equation formulation of the formula that I gave you, which I called PC1. And this is basically saying that if K is zero when I read the clock at time key plus new and see the difference between the clock reading now and the clock reading when I read it at time T it should exactly be mu. But because of the fact that I may have individual clock drift, it may not be exactly mu, but it may be something different from mu. Either more or less, but very small difference. And so all that we are saying is that the amount of individual clock drift should be negligible compared to the interprocess communication time. So the first thing is saying the interprocess communication time is much bigger than any clock drift that exists between two different clocks. And the second equation is saying that the individual clock drift is very small compared to mu. And if we put all these things together, you can derive the expression for inter-process communication time. What it should be relative to mutual drift and individual clock drift. If this inequality is satisfied, you can avoid anomalies in your distributive system. So, informally, would you expect this k is very very small. Which means the denominator is very close to one. So, all that we're saying here is that the mutual clock drift, which is represented by epsilon, is very small compared to the interprocess communication time. Which is what is captured by this apparent condition that I laid out here. Let's return to our example earlier of me owing you money, and let's say it so happens that the mutual clock drift is 5 between you and me. So my clock, when it reads 3:00 PM, your clock reads 8:00 PM, so that is the mutual clock drift that we have. And let's say that the interprocess communication time, the lower bound on that is 2. So what happens is that, as I told you earlier, I am telling you any time after 5:00 PM. You can get the money from me, from my bank. And so you instructed your bank, to debit at 8:00 PM. But unfortunately, your 8:00 PM, is my 3:00 PM, because, our mutual clock drift is 5. So when your request went out, it took 2 units of time to get to the server. Went out and the Central Bank got your message asking for a debit transaction, but the credit is not there yet, because I'm waiting till 5:00 PM to actually send my credit advice to the bank. And therefore your request which came in at 4:00 PM, because you sent it relative to me, you are five hours ahead, and in terms of real time, you're actually six hours ahead, and so the message is received at 4:00 PM, and your request is declined. And this is coming about, because of the fact that your interprocess communication time Mew is less than the mutual clock drift that we're seeing between these two clocks which is five. On the other hand, if the mutual clock drift is less than the inter-process communication times, in this case, let's say that the clocks are more well behaved. Epsilon is one, meaning the mutual clock drift between you and me is just one. And so, exactly the same scenario. When it is 3:00 PM, in my branch, your branch is saying your time is 4:00 PM. Of course, you've given the advice to your branch to debit at 8:00 PM. So at my local time, 5:00 PM, I send a credit advice. Received by the bank, and at your time, 8:00 PM, which is not quite in sync with real time, and is also drifting with respect to my time, but the bound is less than the lower bound on the interprocess communication time. And therefore when you send your debit request at 8:00 PM, your local time, which actually in real time 6:00 PM, it's perfectly fine because when it is received in the Central Bank, the real time is 8:00 PM. And it is received later than the current request, and so your request is honored, you're happy and you can go home. So the important takeaway is that in constructing distributed applications which depend on real time, it is important to make sure that, that are bounds on individual clock drift. As well as mutual clock drift. So individual clock drift is what my, my clock is reading at any point of time and how far off is it, from real time. So that is individual clock drift and you want that to be bound by some small value, which we call k. And the other important thing is that you want to make sure that the mutual clock drift is very very small also. So that there is no anomaly when the interactions like what we showed here and the whole thing hinges on the relationship between mutual clock drift, the individual clock drift k, and the interprocess communication time. Informally, so long as you make sure that the interprocess communication time is significantly higher than the clock drifts, whether it is mutual or individual clock drift, you can ensure that there are no anomalies in the system. Lamport's clock serves as the theoretical underpinning, for achieving deterministic execution in distributed systems, despite the fact that that there are nondeterminism that are existing due to vagaries of the network and due to drifts in the clocks, and so on. It's a nice feeling that we can come up with. Conditions that need to be satisfied in order to make sure that we can have deterministic executions and avoid anomalous behaviors using Lamport's clock, both logical clocks. Where it is sufficient, as well as the physical clock conditions. In the next part of this lesson module. We will discuss techniques for making the operating system, communication software stack efficient for dealing with network communication. Lamport's clock gave a fundamental ordering mechanism for events in a distributed system. This theoretical basis is all the more important in this day and age, when so many everyday services, email, social networks, e-commerce, and now even online education are becoming distributive. Incidentally, this is the same Lamport who gave us a way to order memory accesses in a shared memory multi-processors, through the sequential consistency memory model. In the next part of the lesson we turn our attention to more practical matters in distributed systems. Specifically, given that network communication is the key to performance for distributed services, the operating system has to strive hard to reduce the latency incurred in the system software for network services. Lamport's clock serves as the theoretical underpinning for achieving deterministic execution in a distributed system, despite the non determinism that exists due to the vagaries of a network. In this lesson, we will discuss techniques for making the operating system softwares tag efficient for network communication. Both by looking at application interface to the kernel as well as inside the kernel in the protocol stack itself, but first a quiz. Lets say it takes me one minute to go from my office to the classroom, but the hallway is wide enough that five of us can walk side-by-side going to my office to the classroom. So the question to you is to illustrate the difference between latency and throughput. The question is, what is the latency incurred by me to get to the classroom? And the second part of the question is asking you, what is the throughput achieved if I walk with four other students side-by-side to get to the classroom? This is just a fun quiz to get you thinking about latency and throughput. I am sure that most of you would have gotten that, the time to get to the classroom, after all, from my office is one minute, so that's the latency I'm going to observe, getting to my classroom. The interesting thing is the throughput. The throughput is five per minute, because the hallway is wide enough. For five of us to walk side by side, so the throughput that we can get is five per minute. The important thing I want you to see is that, the latency is not the inverse of throughput. On other words, if tomorrow I widen the hallway to make 10 people walk side by side to get to the classroom from my office, that's going to increase the throughput, but it does nothing to the latency, the latency is always going to be the time it takes for me to get from my office, to the classroom. So it's important to understand these two concepts of latency and throughput. Latency is the elapsed time from an event. If it takes me one minute to walk from my office to the classroom, that's the latency that I'm going to experience for that event of walking from my office to the classroom. That's the elapsed time. Throughput is the number of events that can be executed per unit time. Bandwidth is a measure of throughput. So once again with this analogy of walking to the classroom from my office, if the hallway is wide enough to allow five, ten of us to walk in parallel side by side to the classroom, increases the throughput but it does nothing to the latency. The latency is going to be determined by how fast I can walk from my office to the classroom. So, the difference between latency and, and throughput is very important to understand. In other words, I can increase the bandwidth and that'll improve the throughput but it is not going to do anything to the latency itself. So in other words, higher bandwidth does not necessarily imply lower latency. You'll work hard to lower the latency. RPC is a basis for client server based distributed systems. And performance of RPC is crucial, specifically in the context of this lesson. Latency refers to the time it takes for an application generated message to reach it's destination. So for instance, if you're doing an RPC call from a client to the server, then the RPC call entails sending the argument from the client to the server. And there is work to be done here, work to be done in sending the message, work to be done here before the server can actually execute the server procedure. So it's the latency that we are concerned about. And what we will see is all the software components that comprise the latency for RPC based communication. And performance of RPC is very crucial in building client server systems. There are two components to the latency that is observed for message communication in a distributive system. The first component is the hardware overhead and the second component is the software overhead. The hardware overhead is really dependent on how the network is interfaced to the computer. So, typically in any computer, what you have is a network controller that interfaces the network to the CPU. And typically, the network controller operates by moving the bits of the message from the system memory of the node into it's private buffer, which is inside the network controller. And this part of it, moving the bits from the memory of the node into the internal buffer of the network controller is accomplished using what is called direct memory access. Meaning the network controller is smart enough to move the bits directly using the bus that connects the memory to the network controller without the intervention of the CPU. And this is what is called dir, direct memory access. And that's how the bits are moved from the memory of the system into the buffer of the network controller, and once it comes here, the network controller can then put the bits out on the wire, and this is where the bandwidth that you have connecting your node to the network comes into play. But there are also other types of network controllers where the CPU may actually be involved in the data movement, and in that case, the CPU does program I/O to move the bits from the memory into the buffer of the network controller, from which the network controller will then put it out on the network. But modern network controllers tend to be built using DMA technique, meaning that the network controller, once the CPU tells the network controller were in memory the messages to be sent on the wire, network controller does the rest in terms of moving the bits into it's internal buffer, and then from the buffer putting it out onto the network. The software overhead is what the operating system tax on to the hardware overhead of moving the bits out onto the network. So the latency, if you think about the latency as a whole for doing a network transmission, there is the software overhead incurred in the layers of the operating system to make the message available in the memory of the processor, ready for transmission. Once it is ready for transmission, the hardware overhead kicks in, and the hardware, the network controller in particular, moves the bits from the memory into it's buffer and then out on the wire. The focus of course being an operating system designers work, is to reduce the software overhead and take what the hardware gives you and think about how you can reduce the software overhead so that we can overall reduce the latency involved in transmission. Which is a sum of the hardware overhead and software overhead. Let's now discuss the components of the RPC latency. By now we are all familiar with the semantics of RPC, namely, in RPC the client, is making a remote procedure call to a server, and it has to send the arguments of the call to the server so that the server can execute the server procedure and return the results back to the client. So if you look at the components of the RPC latency, it starts with the client, with a client call. So the client call subsumes a number of things. Number one, it is setting up the arguments for the call. The client has to set up the arguments for the procedure call. And then it makes a call into the kernel. And once the kernel is being informed that it wants to make this call, the kernel validates the call, and then marshals the arguments into a network packet, and sets up the controller to actually do the network transmission. That entire set of activities that the client program and the kernel is involved in, in getting ready a network packet to send out, is subsumed in this one line, which I say is the client call. The second part of the latency is the controller latency, and this is the part where the controller says, well, there is a message to be sent out. I know where it is in memory. I have to first DMA that message into my buffer and then put the message out on the wire. That's the controller latency and so this part of it is in hardware, and as operating system designer we're going to take what the hardware gives us. Controller latency is what you have, that given by the hardware. The third part of the latency is the time on the wire. Now this really depends, as one might imagine, on the distance between the client and the server. The limiting factor of course is speed of light. So, depending on the bandwidth that's available between the source and the destination, perhaps if you have to go through intermediate routers and so on. It is going to take a certain amount of time to go from the client to the server machine, and that we call as the time on the wire. So then, the message arrives over on the destination node, and it arrives in the form of an interrupt to the operating system. So, the interrupt has to be handled by the operating system, and part of handling the interrupt is moving the bits that come in on the wire into the controller buffer. And from the controller buffer into the memory of the node. So all of that activity is subsumed in this item number four, which I call the interrupt handling. So once the interrupt handling is complete, then we can set up the server procedure to execute the original call. Now, what is involved in that? Well, you have to locate the server procedure, and once you locate the server procedure, you have to dispatch the server procedure. And once you dispatch the server procedure, you have to unmarshal the network packet that comes in as the actual arguments for the call that the server procedure has to execute. So all of that setup is first done, and then the server procedure can actually execute the call. So this is the five-step process. From the time the client says, I want to make a RPC call to the point of actually executing the call, these are the layers of software and, of course, hardware and time on the wire, by which time you're ready to execute the server procedure. So even though it looks like a simple procedure call from the clients point of view, there is all this latency to be incurred in executing a remote procedure call. So at the end of step five, the server is all set to execute the procedure. Let's see what happens then. So step number six is server execution, meaning that it is actually executing the procedure. And of course, this is not under our control as operating system designer, because at this point, the amount of time that the server is going to execute this procedure depends really on the logic of the program that has been written as a client server program. And then, finally, once the server procedure has completed execution, then it says okay, I am ready to send the reply back to the client, and that's where we pick up again, so, what happens is that at that point, you are receiving the results. So, once again, just like when the client wanted to send the arguments, you have to convert the actual arguments into a network packet and send it out on the wire. Similarly,when the server is ready to reply, you have to take that reply, which is the results of the execution of this procedure, and make it into a network packet. And at this point, once it has been made into a network packet it has to be handed over to the controller, and the controller does exactly what we did on this side, which is to say the controller latency is gon, is going to be incurred. So that's why you see item number two appearing all over again in the return path. Similar to sending the arguments over to the server on the wire, the results have to be sent on the wire back to the client. And so you see that item number three, which is the time on the wire, is reappearing on the return path as well. Come over to this side. The incoming result message is going to result in a, an interrupt on the receiving node, the client node. And that is exactly similar to what happened on the server side item number four. So you see number item number four reappearing on the return path as well. So that is the interrupt handling part. And once that interrupt is handled, the operating system on the client side said, oh this was on behalf of this client, let me redispatch the client, set up the client so that the client can then receive the results, and restart execution where it left off. So the only two new things that we added in the return path was item number six and seven. Two, three, and four was exactly the same as what we saw on the way over to the server, that is being repeated on the way back to the client. So that's the seven step latency involved in the RPC, not worrying about the actual execution time of the server core itself because that is not in the purview of the operating system, it is in the purview of the client server program that the app developer has done. Now that we understand the components of RPC latency, let's understand the sources of overhead that creeps in in carrying out all the different functions, going from the client to the server and back to the client. So far as the client is concerned, this looks like an innocuous procedure call, right? So it just says, I want to call a procedure S.foo, and here are the arguments. Well unfortunately, this call is not a simple procedure call but it is a remote procedure call. And the sources of overhead that creeps in, in a remote procedure call, are marshaling, data copying, control transfer and protocol processing. So we'll look at each one of these things in more detail. Now how can we reduce the overhead in the kernel? What we want to do is, think what the hardware gives you in order to reduce the latency incurred for each of these components of the RPC latency. First let's look at how we can reduce the overhead and marshaling the arguments and the data copying. Just to jog your memory, marshaling refers to the fact that the semantics of the RPC call being made between the client and the server. It's something that the operating system doesn't have any clue about. So in other words, the arguments that are actually passed between the client and the server, has semantic meaning only between the client and the server. The operating system has no knowledge of it. And therefore marshaling is the term that is used to say, let's accumulate. All of the arguments that is there in the call and make one contiguous network packet out of it, so that we can give it to the kernel and the kernel can send it out. That's what is being described as marshaling, and the biggest source of overhead in marshaling is the data copying that's going to happen, and I'll explain that in a minute. Potentially, in doing the marshaling, there could be three copies involved. Where are these three copies coming aboard? Well, first of all, the client is executing. When a client is executing a procedure, all the arguments for the procedure call that it wants to do is living on the stack of the client. And there is an entity, we'll introduce this terminology even before, called the client stub, and the role of the client stub is to take the arguments of the call which are living on the stack. And convert it into a contiguous sequence of bytes called an RPC message, so the RPC message has no semantic meaning, and it's just a contiguous string of bytes, which you can pass to the kernel, and the kernel can then send it out on the wire, just like any other message. So that's the first thing that the stub does, and that's the first source of overhead. The client stub is making the first copy, from the stack, in order to create an RPC message. Now remember that the client is a user program, so it is living in the user space outside the kernel, and so this RPC message, which is being created by the stub, it is pulled off the client's address space. Which is living outside the kernel. So, this RPC message is in user space and the kernel has to make a copy of the RPC message from the user space into its own buffer, the kernel buffer. And that's a second source of overhead. The second source of copy in doing the marshalling of the arguments. So now it is in the buffer of the operating system kernel, now the operating system can kick the network controller and say, hey, go ahead, take this buffer, send it out in the wire to the desired destination. And the network controller, at that point, is going to move the bits from the buffer, which is in the system memory, of the operating system, into its internal buffer using DMA. And this is the third copy that is happening. The copy that is done by the network controller, using DMA to move the bits of the RPC message, copied from the user space into the internal buffer of the kernel. And now this movement is being orchestrated by the hardware to move it from the kernel buffer into the internal buffer of the network controller, so that it can then get out on the wire. So those are the three copies involved in marshaling the arguments of the call, before it can be put out on the wire. And the copying overhead is the biggest source of overhead for RPC latency. Now, how do we reduce the number of copies? Well, it turns out that the third copy that you're looking at here, moving the bits from the system memory into the network controller, there's a hardware action. That is unavoidable, and therefore, we're going to live with it. Unless the network controller is completely redesigned, if the network controller is saying, well, I need to DMA the bits from the system memory into my buffer. Well, I have to DMA the bits from the system memory into my internal buffer, before I can put it all on the wire. Then this third copy is inevitable so we live with it. But we would like to see if we can try to reduce the number of copies involved here. The first idea is, can we eliminate this copy that is done by the client stub? Why is that happening? Well, it has to create a network message in order to send it out on the wire. As we said. That the semantics of this call is only known to the client and the server, and the client stub is taking the argument and making a network packet out of it. And it was doing it in user space. And so what we're going to do is, we're going to marshal it directly into the kernel buffer. In other words, we've now moved this stub, the client side stub, from the user space down into the kernel. If you can move it into the kernel, then from the stack a stub can directly marshal it into the kernel buffer. And so that intermediate copy that we had here creating an RPC message and copying it. Again into the kernel buffer is avoided if the stub can directly work on the stack, and write it into the kernel memory. So what this means is that, at instantiation time, the client stub is installed inside the kernel. At bind time, when the client binds with the server at the bind time, what we going to do is, we going to say that here is the client stub, please put it inside the kernel so that, later on, you can use that in order to do the marshaling. So the synthesized procedure is installing the kernel for each client call, so for each client server relationship. We synthesize the procedure, which is the client's job, install it in the kernel for use every time, I make this call. This stub can be invoked to convert the argument that are living on the stack, into a network message, and directly put it into the kernel buffer. So this, obviously, will eliminate. From the two copies down to one copy because, the intermediate copy of converting the arguments into an RPC message is now eliminated. Now, the problem with this idea is that we're seeing, let's dump some code into the kernel and that may not be something that is, so palatable. So this is a solution that's possible if the RPC service that is being provided between the client and the server, the trusted service and therefore we can trust, who is putting the stub into the kernel. In that case the solution maybe a reasonable one to adopt. An alternative to dumping code into the kernel, is to leave the stub in the users piece itself. But have a structured mechanism, for communication between the client stub and the kernel. And that structured mechanism is a shared descriptor. And the shared descriptor, is a way by a which the sub can describe to the kernel that here is some stuff sitting in the user space. And I am going to tell you how exactly you can extract this information, from the user space and construct it into this buffer for transmission on the wire. Recall what I told you earlier and that is the kernel has no idea, of the semantics of this call and therefore, it has no knowledge of the actual arguments. The data structures that are being used in the call. So what we're going to do is use the shared descriptor as a vehicle for the stub to communicate to the kernel the data structures that need to be passed. So for instance, let's say that the argument for the call has four parameters. Then this descriptor has four entries, and each entry is saying, this is a starting point of a particular Data item, and this is the length of the data item. This is the starting point of the second data item, and this is the length of the data item. Third data item, fourth data item. Kernel doesn't have to know the semantics of these data items. All it needs to know is, what is the starting address for a particular data item, and what is the length of the data item. That's all it needs to know. And this is the descriptor that. Allows the stub to inform the kernel about the arguments, how many arguments there are and what are the size of each argument. It doesn't have to tell the kernel, oh, here is an integer, here's a floating point, here's an array. No, none of that. All that the stub is doing is, it's saying. Here is the starting address for an argument, and here is the length of that argument. Because usually data structures are organized contiguously, so if you have a, an integer, it is occupying full contiguous bytes and memory. If you have floating point number, it is occupying some number of contiguous bytes in memory, and therefore. What the stub is doing is, is creating the shared descriptor that is providing the information of the kernel in the layer of the arguments on the stack, and once the layer of the arguments and the stack are known to the kernel, then the kernel can use these contiguous data items. That are living on the stack, describe the shared descriptor, and create a contiguous packet in its internal buffer. That's a second way you can do, in order to reduce the number of copies from two to one. So in both cases, what we have done is either the first approach of pushing the client stub into the kernel or the second approach of having a shared descriptor between the user stub, which is living. In user space and the kernel in order to describe the layout of the data structures, that need to be assembled into a data packet by the kernel using the shared descriptor. Both of these allow us to reduce the number of copies from three down to two. Either the one copy that is happening Going from this stack into the kernel buffer and this second copy, as I said, is unavailable if the network controller is requiring DMA to be done from the system memory into its internal buffer before the bits can be pushed out of the wire. So that's the first source of overhead, these are techniques that we have looked at. Two differnet techniquest for reducign the copying overhead that is the dominate part of marshalling the arguments. And this happens on both sides. It happens when the client has to push the arguments to the server side. And it happens again on the server side when the server has to push the results back to the client. So the marshalling is happening on both ends, and for both ends we can Use this technique of using a shared descriptor or pushing the clients dub or the service dub into the kernel in order to reduce the number of copies from two down to one. So the second source of overhead is the control transfer overhead. And that is the context switches that have to happen in order to effect an RPC call and return. And let's now look at the context switches that happen in doing the RPC. Let's say this is the client machine, and on the client machine the client is making a call. So, they're making the call Another call is made. The kernel has to say, oh, this client wants to make an RPC call and we know that the semantics of RPC is that, the client is blocked until the results are returned. And therefore, the operating system on the client side, will switch from the current client that is making the call, to another process, let's call it C1. So this is the first context which that's going to happen. On the client box, the RPC call is sent out in the wire, reaches the server machine, and when it reaches the server machine, the server machine is executing some arbitrary process let's call it S1, so when the call comes in, the kernel has to switch to the particular server process, that is going to handle this incoming RPC call. So this is the second context switch. So, the server machine and the operating system on the server machine is currently executing some process S1. So it has to S in order to answer the incoming RPC call. So that is the second context switch that happens. Then the server procedure executes. And once the server procedure is completed execution, it's now going to send the results out, and when it wants to send the results out, at that point, the work is done for the server. And so the server operating system has to switch from S to some other process S 2, so that's again a context which that's going to happen because the server is done with whatever it has to do. So that's the third contact switch. Then the RPC result is coming over the wire. Come to the client side. Exactly similar to what happened over here. When it comes back to the client side, the kernel at that point is executing some process, C2. And this particular result message is coming back, saying well the original call sent out on behalf of this client's seed, the result have come back. Now it is time to reschedule this client, so that this client can receive the results, and continue with his execution. Remember that the semantic it's like a procedural call, but it is a remote procedural call, the client is blocked For the result to come back and when the result comes back. The kernel can schedule the client to continue with its execution. So that's what is going on. So potentially, there are four contact switches that are going on. Now let's look at these contact switches and figure out what contact switches are critical. Now, this contact switch is essentially to make sure that, the client box is not being underutilized, right? So once the client has made this call, til the result comes back, the client box is underutalized, and therefore, the operating system says, well let me switch to some other process that can lose some useful work on this node. So that is, this contact switch. Not critical from the point of view of the latency for RPC. Now when the message comes over here. This context switch is crucial because, at this point, when the RPC call comes in, this guy, the server box is executing some of the process S1. So it has to switch to this server process, S, which can actually execute the RPC call. So this is an essential part of the RPC latency. And, similarly, to this context which, that I talked about This contact switch is happening in order to make sure that the server's machine is not underutilized. When the server is done with the RPC call, it's going to send the results back, and therefore, we need the contact switch out of this server process. To some of the process that we can utilize this server box. That's this contact switch. Again, similar to this context switch, this context switch is not in the critical path of RPC latency. The result comes back and when the result comes back the client box is executing some process C2. The kernel has to switch to this client, so that is can see the results, an continue with it's execution. So this context switch, is again in the critical path of RPC latency. So if you look at RPC call, the two contact switches that are in the critcal path of RPC latency. There's a context switch that happens here and the context switch that happens here. So only two context switches are in the critical path of the latency, the context switch that happens on the server machine when the call comes in, and similarity, the context switch that happens on the client machine when the results come in. So this context switch that happens on the client machine to keep the client machine Utilizied, can be overlapped with the network communication to send the call out. So in other words, this context which is not the critical path of RPC latency and therefore do this context which, while the RPC call. Is in transmission on the wire. So while overlapping the context switch that happens on the client box after the call has been sent out with the communication time on the wire for the opposite call. Similarly on the server side, once the server is completed execution and it is ready to send the results out, send it out of the wire and in parallel with sending it out in the wire, it can overlap The contact that happens here in order to keep the server box busy doing useful stuff, S2, S2 that can be overlapped with this network communication. So, only this contact switch and this contact switch are in the critical path of latency. So, we can reduce the number of contact switches down to two. Originally we started with 4, we can reduce it down to 2. By observing that the context which is that happened on the client end box and the server box to keep them utilized can be overlapped with a network communication for sending the arguments over to the server, or sending the results over to the client. Of course we are greedy. Can we reduce it to one? Can we actually reduce the number of context switches Down to one. Let's think about this. So we said that when this RPC call was made, the operating system on the client side said, well, this is a blocking semantic, and therefore, this guy is not going to do any useful work, so I'm going to block him and wait for the results to come in. So this context switch that c, the operating system did on the client side Was essentially to keep this client box from being under-utilized, but do we really need to do the switch? Well, it really depends. If the server procedure is going to execute for a long time, then, you know, this client box is going to be under-utilized for a long time. And in that case, it might be a good thing to context which in order to make sure that we are utilizing the resources that we have. But on the other hand, If, suppose, this RPC call, we know that this RPC call is going to come back very soon. And if it is on a local area network and the server procedure that is going to be executed is not going to take a long time. Then perhaps that RPC call will come back very quickly. If that is the case, we can get rid of this context switch that we talked about here. In order to keep the client box busy, we did this context switch. Don't do that. We can spin instead of switching on the client side. And if you do that, then the client is reading but is not being context restored. It is just that the box is underutilized, so the only context which that we incur. Is the context which on the server because you never know when an RPC call is going to come in. So when an RPC call comes in, you obviously have to contact switch into the server context in order to execute the call. That's the necessary evil. We'll incur that. But on the client side, what we're going to do is, we're going to spin instead of switching so that. Even though the box is underutilized, you're not doing anything on the client side, just sending the call out and waiting. And in that case, we've gotten rid of the second context switch that you need to incur. Because another context was stalled and therefore this context switch which we said is in, inevitable, because. It has to be done in order to receive the results for the client. Well, we can get rid of it if we never switched in the first place, and that's the trick here. To reduce the number of conflicts which is down to one, we can spin on the client side instead of switching so that we can be ready to receive. The results of the RPC call execution when the server is done. Again, the intent here is that we wanted to the latency that is incurred in the RPC call. And since these two contract switches were in the critical path of the latency, we would really like to see how we can elimintate at least one of them. And this context which is inevitable and this context which we can eliminate, by spinning on the client side instead of switching in the first place. So we talked about marshalling, data copying, and content switches, and the fourth component that adds to the latency of the RPC transmission is protocol processing, and that's the next thing that we're going to look at. Now the question is, what transport should we use for the RPC? And this is where we want to see how we can take advantage of what the hardware is giving us. If we are working in a local area network, the local area network is reliable, and therefore our focus should really be on reducing the latency and not worry so much about reliability. It is often the case that performance and reliability are at odds with each other. If you focus on reliability, then performance may take a back seat. So here, since the RPC performance is the most critical thing that we are worried about, we're going to focus on reducing the latency. And we're going to assume that the LAN is reliable and therefore let's not worry about the reliability of message transmission in a local area network. That's the idea behind, the next thing that we're going to look at. Let's think about all the thing that could go wrong in message transmission, and see why some of those things may not be that important, given that we have a reliable local area network. The first thing is, you send a message, it might get lost. But, if in a local area network, the chances that messages will actually get lost, is not very high. It happens in wide area internet, because messages have, have to go out through several different routers, and they maybe queuing in the routers, and there may be loss of packets in the wire and so on. But that's not something that you have to worry about in a local area network. So that assumption that messages may not get lost, suggests that there's no need for low level acknowledgements. Why? Because you're sending a call and the call is going to be executed and the result is going to come back. And usually in network transmission, we send acknowledgements to say that, yes, I received the message. Now, in this case because the semantics of RPC says that the act of receiving the RPC call is going to result in server procedure execution and the result is going to come back, the result itself serves as the ACK. And therefore we don't need low level ACKs to say, oh, I received you arguments of the call. You don't have to do that. And similarly, you don't have to have a low level ACK that says oh, I received the results. Because the results were not received, the caller, the client is going to resend the client call. So the high level semantic of RPC can itself serve as a way we can coordinate between the client and the server and we can eliminate low level ACKs and if we eliminate low level ACKs, that reduces a latency in the transport. The second thing is in message transmission on the Internet, we worry about messages getting corrupted. Not maliciously or anything like that, but just due to vagaries of the network messages may get corrupted in going on the wire that connects the source and destination. And for that reason, it's typical to employ checksum in the messages to indicate the integrity of the message that checksum is usually computed in software and appended to the message and sent on wire. But in a local area network things are reliable, we don't have to do extra overhead and software for generating the checksum, just use hardware checksum if it is available, just use hardware checksum for packet integrity. Don't worry about adding an extra layer of software in the protocol processing for doing software checksum. So that's the second optimization that you can make to make the protocol processing leaner. The third source of overhead that comes about in message transmission is once again related to the fact that messages may get lost in transmission. And therefore in order to make sure that if messages are lost in transmission, you usually buffer the packets. So that if the message is lost in transmission, you can re-transmit the package. Now once again let's think about the semantics of RPC. The client has made the call and the client is blocked and since the client is blocked, we don't need to buffer the message on the client's side. If the message gets lost for some reason, you don't hear back the result of the RPC from the destination, in a certain amount of time, you can resend the call, from the client side. And therefore you don't have to buffer the client side RPC message but it can reconstruct the client side message and resend the call. Therefore client side buffering is something that you can get rid of, once again, because the LAN is reliable. The next source of overhead, similar to client side buffering, happens on the server side, and that is the server is sending the results and the results may get lost. LAN reliable may not happen that often but it could happen ,and therefore we do want do the buffering. On the server side because if we don't buffer it then you have to reexecute the server procedure to produce a result and that's not something that you want to do because it involves reexecuting the server procedure which may be much more latency intensive then simply buffering the packet that corresponds to the result of executing the server procedure so you do want to buffer on the server site but. The buffering on the server side can be overlapped with the transmission of the message. So in other words the result has been computed by the server procedure. Now go ahead and send the result. While you are sending the result back to the client, do the buffering. That you can overlap the service side buffering with the result transmission, and get it out of the critical path of the latency for protocol processing. So, removing low level asks, employing hardware check sum and not doing check sum in software. Eliminating client side buffering all together. And overlapping the server side buffering with the result transmission are optimizations that you can do. In protocol processing, recognizing that the LAN is reliable and therefore we don't have to focus so much on the reliability of message transmission. But focus rather on the latency, and how we can reduce the latency, by making the protocol processing lean and mean So once again recapping what we said. The sources of RPC latency are the following. Marshaling and data copying. Context switches, both at the client side and the server side. Similarly marshaling and data copying also happens both in the client side and the server side. And the actual protocol processing in order to send the packet on the wire. These are all the things that are happening in software. Those are the things that as OS designers, we have a chance to do something about. And what we saw were techniques that we can employ for each one of those, reduce the number of copies, reduce the number of context switches, and make the protocol processing lean and mean so that the latency involved in RPC is reduced to as minimum as possible from the software side. And we are going to take whatever the hardware gives us. If the hardware gives us an ability to, to do DMA from the client buffer, we'll use that but if it doesn't, then we have to anchor that. So that's what we are seeing here. As the opportunities for reducing the total latency in going from the client to the server and back to the client in RPC. In the previous lesson, we learned some tricks we can employ as operating system designers for optimizing the RPC communication software which powers client-server communication in the local area network from the point of view of reducing communication latency. Of course, user interactions go beyond the local area network to the wide area Internet. The primary issue, once a packet leaves your local node, is to route the packet reliably and quickly to the destination. Routing is part of the functionality of the network layer of the protocol stack of an operating system. What happens to a packet once it leaves your node? Well, the intermediate hardware routers between your node and the destination have routing tables. That help them to move the packet towards the desired destination node by doing a table look-up. The routing tables evolve over time, since the Internet itself is evolving continually. That's the big picture. And there are lots of fascinating details which you can learn in a course that is dedicated to computer networking. For the next part of the lesson on distributed systems, we want to ask the question, what can be done in the intermediate routers, to accommodate the quality of service needs of individual packet flows through the network? Or in other words, can we make the routers en route to the destinations smart? The specific thought experiment we are going to discuss is called active networks. And then, we will connect the dots from active networks to the current state of the art, which is referred to as software defined networking. Thus far in the course, we've been focusing on specializing operating system services for a single processor, or a multi-core or a parallel system, or a local area network. In this lesson, we will take this idea of specializing to the the wide area network. Specifically, we will study the idea of providing quality of service for network communication in an operating system by making the network active. Normally, when we think about routing of packets on the internet, typically what happens is. At the source node, you create a network packet and go through the layers of software stack on the sending node, and send the packet out on the network. And this network packet has a desired destination. And of course, it has to go through a whole number of intermediate routers in order to get to its eventual destination. And the routers on the Internet that are intermediate between the source and the destination, they don't inspect the packet for the contents or anything like that. All that they're doing is when the packet comes in, they're looking at the destination known for that packet and figuring out what is the next hub that I have to send the packet to. So each router is making the determination of the next hub for the package, and it makes the determination by doing a table lookup. So every router has a routing table, and the routing table is telling. Given a particular destination, what is the next hop? And that's how the packet flows from source to destination through a whole bunch of intermediate routers and finally gets to the destination. So in other words, the routers en route to the destination from the source, are simply forwarding packets. That is, the nodes are passive. And they're just doing a table lookup in order to figure out what is the next hop that I have to send this packet to? Now, what does it mean to make the nodes active? What we mean by making the node active is that the next hop for sending this package towards a destination is not simply a table look up. But it is actually determined by the router executing code that is actively as opposed to doing just a passive table look up. So, in other words, the packet in addition to the payload that is intended for the destination also carries code with it. And the code is being executed by the router in order to make a determination as to what to do with this packet in terms of routing it towards the desired destination. This sounds really clever because it can provide customized service for networks flows that are going through the network. And every network can have its own way of choosing what may be the desired route, in terms of going from source to destination. And so in other words we're saying. Well, this is an opportunity to virtualize the traffic flow from my network traffic independent of other network flows. This should be very familiar to you all because we've been talking about customizing operating system services in the SPIN operating system, and the XO kernel and so on. But, of course, the problem that we're talking about here, much, much, harder because the network is wide open. Our network traffic flow is going through the public internet infrastructure ,and we are talking about specializing the network flow for every network flow independent of others. There are lots of challenges to this values of active network. In particular, how can we write such code that we can distribute and send it over the wire so that routers can execute it. And who can write such code? And how can we be sure that the injected code does not break the network? Or in other words, for a particular network flow, there is a code that is going to be centered on. How do we make sure that is not going to in some way [UNKNOWN] other network flows? These are things that we have to worry about and sort of opening up the router and saying that we're going to take network flow specific decisions in each of the routers. Let me give you an example to motivate why this vision of active networks is both intriguing and interesting. You may all know that Diwali is a big festival in India, just like Christmas is in the Western world. And let's say that I am sending Diwali greetings electronically to my siblings, who are in India. What I can do, is I can send individually a greeting message to each of my siblings. And so, there'll be N messages going out on the Internet from source to destination. So I send out N messages, and, you know, to reach any of my siblings. That's one way of doing it. A nicer approach would be, given that all my siblings are clustered in one corner of the globe, it would be nice if I could send just one message, traverses the Internet, gets to close to the destination of where my siblings are, and the router that is at this end, demultiplexes my message and sends it to all my siblings. Obviously, the second method is more frugal in terms of using network resources. I don't have to send N messages. I can send one message, and finally at or close to the destination, an active node takes this one message, recognizes, oh, this is intended for multiple recipients, and demultiplexes them, and sends it to all the eventual recipients of this message. Of course, we can generalize this idea and say that this idea of active router is going to be spread out throughout the Internet, so that even if my siblings, let's say, are distributed all over the world, then I could still send a single message from my source, and it gets demultiplexed along the way, depending on where all the eventual recipients are for this particular message that starts from me. So in other words, we can sprinkle this intelligence that is in this one particular router to all the routers in the Internet and that way we are making the entire Internet an active network. That's the vision behind active networks where the nodes in the internet will become, not just passive entities, but actually active, in looking at the message, and figuring out what to do with it, in terms of routing decisions. Now that we've motivated the vision, let's see how we can implement the vision. In order to implement this vision, the operating system should provide quality of service APIs to the application. And these quality of service APIs could be things like, oh, this particular network flow that I'm creating has certain real time constraints, because it has video data and so on and so forth. And those are the hints that the operating system is going to use, in terms of synthesizing code that corresponds to the API that the operating system is providing you, to give hints to the network. So, the code that the operating system is going to synthesize is essentially taking the quality of service constraints, and putting them as executable code that can be, then passed it on as part of the packet. So in other words, the protocol stack of the operating system has to be enhanced to service these quality of service requirements, and generally to synthesize the code that has to be part of the payload. So the application is not only providing a payload, but it is giving quality of service constraints. And the operating system, in addition to the payload, generates or synthesizes code corresponding to this quality of service instructions. And this slaps on the IP header for where this particular message is eventually to be delivered, and hands it over to the Internet. And in the Internet, if we assume that the Internet routers are capable of executing this specialized code, then depending on the nature of what is being requested, a particular order may say, oh this particular packet I have to send it to multiple destinations, so let me send this down this link, down this link, and similarly when it comes over here, this router may say, oh, this packet has to go to multiple destinations. And so on and that we can see that intelligent routing decisions can be taken in the network. That's out of the road map of how we can take this vision and try to implement it. But the problem with carrying out the vision, in terms of this implementation that I just sketched, is that changing the operating system is non-trivilous, especially the protocol stackers, they have already mentioned TCP IP has several hundred thousand lines of code, so it is non trivial to change the protocol stack of every node in the entire universe. To handle active networks. And also, the second part of the challenge is that the network routers are not open. So, in other words, we cannot expect that every router on the Internet is capable of processing the code that I'm going on slap on to this payload and be able to make intelligent routing decisions. So there is a impedance mismatch between the vision and the implementation that I've sketched right here. So, the ANTS toolkit, ANTS stands for Active Node Transfer System, took a different approach to show the utility of the vision. Since modifying the protocol stack is nontrivial, instead, the ANTS toolkit is really an application-level package. And this toolkit is available for the application programmer to say, here is my pay load and quality of service constraints. And what the ANTS toolkit does is to create an ANTS header to this payload. So, the new payload looks like this. And this is what is called a capsule. And a capsule consists of an ANTS header and the actual payload. And this is what is given to a normal operation system protocol stack. And so, this normal operating system protocol stack looks at this as the payload that has been given to it and it knows the destination address, where this has to go sticks on the IP header for it. So, the new packet that is generated by the protocol stack, looks like this. It has the IP header, and the rest is payload so long as This protocol stack is concerned, but we know this payload consists of two parts. One is the normal payload that the application generated, and in addition to that there is the ANTS header that have been slapped on by the ANTS tool kit and this is what traverses the network and when it traverses the network if a node in the network is a normal node, meaning it is not a smart node, but it is a normal IP router. Then it simply uses the IP header to say, well, here is what I have to do in terms of sending the packet to the next hop, towards the destination. On the other hand, if a node that receives this packet is an active node. Then, it can actually process this ANTS header, and say, oh, this particular packet needs to be. Demuliplexed, and sent to two different routes. And it might take that intelligent routing decision based on the nature of that node. So that's the idea, that we can push one of the paint points out of the operating system, into an enhanced toolkit that lives above the operating system. So that's sort of the ANTS toolkit vision. That's one part. Now, the second part, and of course the fact that the internet may not be open to opening up all of the routers to to be processing the specialized code that comes in the capsule. So, what we do is we keep the active nodes only at the edge of the network. In other words, the core IP network is unchanged, and all of the magic happens only at the edge of the network. So, once again, if I go back to my example of sending greetings to my siblings, then only the edge nodes have to do the magic in order to take my original message and process the code to deliver it to multiple destinations. So the rest of the network can remain unchanged. So the core of the IP network can, can be unchanged, and intelligence can be at the edge of the network. So this is, sort of, allowing, sort of, matting this active network vision with the core IP network being unchanged. So having given you the high level description of what ANTS toolkit does, let's dig a little deeper and look at the structure of the ANTS capsule as well as the APIs provided by ANTS in order to do capsule processing. First of all, the header as I told you consists of three parts. The original IP header, which is important for routing the package towards the destination if a node is a normal node, not an active node. And this is of course the payload that was generated by the application. And in the middle is this ANTS header, and there are two fields in this ANTS header that are particularly important. One is a type field, the other is a prev field. The type field is a way by which you can identify the code that has to be executed to process this capsule. And this type field is really an MD5 hash of the code that needs to be executed on this capsule, and we'll come back to that in a minute. And the second field that I said is important is the prev field. And this prev field is the identity of the upstream node that successfully processed the capsule of this type. And this information is going to be useful for us in terms of identifying the code that needs to be executed in order to process this capsule. We'll come back to how these two fields are actually used in processing a capsule once this capsule arrives at an active node. The short hint that I'll give you is that the capsule itself, as you see, does not contain the code that needs to be executed to process this capsule, but it only contains a type field. And this type field is a vehicle by which we can identify the code that needs to be executed to process this capsule. More on that in a minute. First, let's talk about the API that ANTS toolkit provides you. The most important function that we want to accomplish using the ANTS toolkit is forwarding packets through the network intelligently. So routing the capsule is the most important function that needs to be done. And that's most of what this ANTS API is all about. And that part is contained right here, saying that, well, route this packet in this manner, and deliver the packet to an application. And this is the set of API calls that allows you to do routing of the capsule through the network. This is where what I said about virtualizing the network comes in. Regardless of the actual topology, physical topology, I can take routing decisions commensurate with the network flow requirements contained in the capsule that arrives at a node. So the second part of the API is API for manipulating what is called a soft-store. Now, soft-store is storage that's available in every routing node for personalizing the network flow with respect to a particular type of capsule. And I mentioned earlier that the type is only a pointer to the code, not the code itself. And the soft-store is a place where we can store the code that corresponds to a particular capsule type. So the primitives that are available for manipulating the soft-store are things like put object and get object. The soft-store is basically key value store and in this key value store, you can store whatever is important for personalizing the network flow for capsules of this type. An obvious candidate for storing in the soft-store is the code that is associated with this type. So you can store the code that is associated with this type, so that future capsules of the same type, when it arrives at a particular node, they can retrieve the code from the soft-store and execute the code that needs to be executed for processing capsules of this type. Other interesting things that you might put into this soft-store are things like computed hints about the state of the network, which can be used for future capsule processing for capsules of the same type. And the third category of API that's available is querying the node for interesting tidbits about the state of the network or details about that node itself, for instance, what is the identity of the node that I'm currently at, and what the local time is at this node, at this node, and so on and so forth. So these are the kinds of things that are available. So the key thing that I want you to get out of looking at this ANTS API is that it is a very, very minimal setup API. So the number of API calls fits in this little table here. So that's the idea. Remember that routers are in the public Internet. And if you're talking about executing code in the router that is part of the public Internet, the router program that we're executing at a router node has to have certain important characteristics. Number one, it has to be easy to program. Number two, it should be easy to debug, maintain, and understand. And number three, it should be very quick, because we are talking about routing packets, and so the router program should not take a long time to do its router processing. So the API, this very simple API, allows you to generate very simple router programs that are easy to program because the APIs are simple, easy to debug, easy to maintain and understand. And the program itself is pretty small, that it's going to take not a humongous amount of time to do the packet processing. Now let's talk about implementation of the capsule, and in particular what are the actions taken on capsule arrival at a particular node. Now, I mentioned that the capsule does not contain code, but it is passed by reference. Or in other words what the capsule contains is the type identify which is really a fingerprint for the capsule code. And the way this type is generated it's basically a cryptographic fingerprint of the original capsule code. In the original implementation of Ann's tool kit they used an MD5 hash of the code and used that as a fingerprint. And it was the case that at the time that this particular research was done, MD5 hash was a cryptographically strong hash function, which was not broken. But subsequently MD5 hash has been broken. But nevertheless, the key thing that I want you to remember is that this type field is a cryptographically strong fingerprint that is derived from the capsule code. And that serves as a reference for the code itself. So when a node receives a capsule, one of two things are possible. The first possibility is that this node has seen capsules of this type before. If that is the case, then it is quite likely that in the soft store of this node the code that corresponds to this type is already existing. In which case, it's a simple thing for the current node to retrieve the code from its soft store, and execute the capsule and proceed with forwarding this capsule on to where its desired destination. On the other hand, if this capsule that arrived at this node is the first time that this node is seeing a capsule of this type, then, obviously, it's not going to have the code that corresponds to this type. In this case, what this current node is going to do is use the previous node field of the capsule and send a request to the previous node saying that, hey I got this capsule of this type, I don't have the code. Do you have the code? If you have, please send it to me. And when this request comes in, the previous node, obviously it has processed this capsule before. It's quite likely that this previous node has the capsule code residing in its soft store. And so it retrieves it from its soft store and sends it to the next node so that the next node has the capsule code now, can execute it, and also store it now locally in its own soft store so that future capsules of the same type when they arrive here, it can be processed using the code that is now stored in the soft store. And the key point to take away is that typically when we are talking about network flows, we are sending a whole bunch of packets one after another. And therefore, even though the first packet that comes to a node may not find the code that is associated with that particular type, and we have to do a little bit of heavy lifting in, in terms of going and reaching back to the previous node to retrieve the code. Because network flow is a whole bunch of packets, there's going to be a whole lot of other packets that come down the pike. They're all going to be processed using the soft store and the code that is stored in the soft store. In other words, we are exploiting the locality for capsule processing by storing the code that arrives in response to our request back to a previous node in the local soft store. One concern that we may have is that how do I believe that the code that I got from the previous node is actually the code that corresponds to this type or not. Well, this is where the cryptograpically strong fingerprint comes into play. What this node is going to do is, when it retrieves the code from the previous node, and when the code arrives, it is going to compute the fingerprint of the code that it just got. And see if that fingerprint matches the type field of the capsule. If it does, then it knows that this code is genuine. If it is not, then obviously somebody is trying to spoof my node by giving bogus codes so I'm going to reject it. So code spoofing can be avoided by having a fingerprint that is cryptographically strong, so that I can recompute the fingerprint match it against the type, and know that the demand loaded code that I got is actually the code that is associated with this particular capsule. And as I mentioned already, once I get this code, because I'm going to most likely see capsules of this type in the future as part of this particular network flow, I'm going to save the code in the soft store for future use. So when a capsule arrives at a node one of two things will happen. One is I will reach into my soft store and see if I have the code that matches this particular type in the capsule. If it isn't then I don't have the code, I'm going to reach back and get it from the previous node. But what if I go back to the previous node and the previous node does not have the code that corresponds to this type. So the action of a node when it cannot find the code that corresponds to a type either locally in its soft store or retrieving it from the previous node, is to simply drop the capsule. Because what's going to happen is that if this capsule is dropped, higher-level acknowledgements that are happening in that particular network flow is going to indicate to the source that something did not get through, and that source node is going to retransmit that capsule. This is exactly the same thing that happens with IP routing on the Internet. That is, if a node cannot process all the packet that it gets, it simply drops the packet. That's the same semantic that is used for capsule processing also because we're relying on higher level protocols, the transfer protocol that sits on top of the network protocol do end to end acknowledgement to make sure that all the packets that they're expecting have actually reached. And therefore at the level of capsule processing, we don't have to worry if we cannot process the capsule either locally by using the code in the soft store or by retrieving the code from the previous node. Simply drop the capsule. Now, is it likely that, when we reach into the previous node, that node does not have the code for it? Because, after all, it did process this capsule, and send this capsule on to me. It can happen, because the soft store is limited. Every router node has only a finite capacity, and all of its capacity it's not going to give to a single network flow. It's going to give only a part of its storage for the network flow that is corresponding to a particular capsule type. And therefore, I have to live within my means. So, the capsule code may have to throw away stuff every once in a while if it is storing more than what its capacity is in the soft store. So it is possible that the code that it originally stored in the soft store, it had to replace it. Because it is like a cache and therefore, it may have replaced it and therefore, it may not have it. And this is particularly possible if this request comes at a much later time because one of the things that you associate with this kind of routing code is that it is going to be timely. So there is going to be a time associated with the validity of things that I want to keep in my soft store. So if I, I get a request at this node at a much later point in time because the capsule arrived at this node traversing all over the network, it may not have the code that corresponds to the type. So it is possible, so if it happens, simply drop the capsule and let the higher level entities in the protocol stack take care of retransmitting the capsule if need be. So, how useful is active network? There are lots of potential applications that can be built using this active network's paradigm. In particular, what we want to do is, when ever we desire certain ways to visualize the behavior of the network. Then active networks become very useful. So, for instance, for implementing protocol independent multicast reliable multicast, or noticing congestions in the network and notifying the source and the destination about the congestions, private IP, any casting. These are all the kinds of things that are useful to implement using active networks. And as you can see from this list, the kinds of things that you want to do, using active networks, are things that are related to network functionality. Not high-level application, but network functionality. In particular, it is useful for building applications that are difficult to deploy in the internet. So, when you rely on the routing and the internet, it is entirely an administrative set up, and the administrative set up tends to mirror the physical set up. But, for your particular network flow, you may want a set up that is different from the physical set up. And the example I give you of me sending a greeting that will be a single message for most of the traverse through the internet, but at some point, it may actually get demultiplexed and get sent to several different destinations. Those kinds of multicasting and things like that, are specific to network flow. So, we are in some sense, overlaying our own desire on topology, on top of the physical topography of the internet, by using the active network paradigm. But, the key properties that you want to have for applications that you want to build using the active networks paradigm, is that the applications should be expressible and it should be compact and it should be fast and it should not rely on all the notes being active. These are key things to note in building applications that live on top of the active networks. So all of these suggest, once again, what I said already and that is, it is for providing network layer functionality, not end application functionality. So, what you want in the network layer, that is something that again orchestrate using the active networks paradigm. So having talked about the vision and the practical implementation details of active networks, lets talk about the pros and cons. The pro is something that we have been stressing all along and that is the flexibility from the point if you have an application perspective that fact that you can ignore the physical layout of the network and slap on your own visualization of what you want to accomplish for your netowork flow. On top of the physical infrastructure is the key selling point for active networks. But this selling point comes at the cost of perhaps certain cons. What are all the cons? Well, one concern could be protection threats. What I mean by that is that eroding infrastructures carrying network flows. Not really mine but yours and the third person and so on and so forth. And just like in an operating system when we have a process, we want to make sure that that process is not doing anything malicious to other processes in the same node. Same way, my network flow should not do anything that is detrimental to your network flow on the internet. So that's what we mean by protection threats. So there are some safeguards in the ANTS toolkit to address these protection threats. The first is the run time safety of ANTS program that is running on a router node. And the way they ensure that is by first of all implementing ANTS itself in Java and using Java sandboxing technique on the router node, so that anything that a router code is doing for capsule processing Is limited to the Java's hand box that is executing in. And so, it cannot effect the flows of other network flows that are flowing through the same routing fabric. That's the first thing. The 2nd protection thread is whether there could be code spoofing that can happen. We are talking about cold being injected into the router, and of course there was a good reason for doing that, that I wanted a certain behavior to be observed by the network routers, in response to packets flowing through the network that belongs to me. But I want to be sure that the code that is being executed is the code that I wrote not some malicious code that is being spoofed to that node. Well here, here the safeguard is making sure that you have a robust, a robust fingerprint associated with the code. And so what you do is you generate a tight field for the capsule that is a crytographically strong finger print of the original code. And it always matching when you get the code from a previous known In response to your requst, you're goint to compute the fingerprint once more. Check it against the fingerprint that is contained in the capsule, ensure that there is no code spoofing happening. That's how you can overcome this protection threat. And the third concern can be integrity of the soft state. And what I mean by that is, the soft store that's available at an ordered node is limited in size, and you don't want any particular network flow to arbitrarily consume all of the soft state. And yet again, there is a restricted API that is provided in ANTS, that's the safeguard. For this protection threat. So these protection threats are concerns. But at least in the ANTS tool kit, they offer solutions to ensure that these protection threats are not show stoppers for active networks. The second concern that one might have is Resource Management threats. Now what I mean by that is Because we are executing code at a router, the result of that code execution could be that I proliferate packets in the internet. I'll give you an example of resending a message to my siblings I send one message, and at some point That one message becomes n message. So in some sense, we can start flooding the network with capsule processing. Is that a threat? Well it is, but internet is already susceptible to this kind of resource management threat and yes capsule adds to it, but it is not anything new that it is adding in terms of resource management threat. On the other hand, we can ask the question at each node is it going to consume more resources than it should. And this again comes back to the safegaurd that they have in the ANTS toolkit that the api is a restricted api and therefore the amount of resources that you can consume at a node It's fairly restricted, so there is sort of a mixed answer to this resource management concern. At a given node, the resource management concern doesn't quite exist so long as you adhere to the restricted API of ANTS. And the second concern that the capsules may flood the network can happen, but it already happens, we all experience spam on The internet so this is not adding any new problem but it is perhaps exasterbating an existing problem. So having looked at division and looking at division and the practicality of active networks, time for a quiz. In your opinion, what can be roadblocks to the Active Network's Vision? And I'm going to give you multiple choices. And you can pick more than one choice if you think it fits the answer to this question. The first choice is that the active network's vision needs buy-in from the router vendors. Router vendors are vendors like Cisco. Who make the routing fabric and the first choice is saying it need, you, you need buy in from the router vendors, the second choice is saying that because the routing was active network is happening in software, the software routing speed cannot match the throughput needed in the internet core. That's the second choice, the third choice says that. It makes the internet more vulnerable, and the fourth choice says that it makes the router more susceptible to code spoofing. The right answers are the first two boxes here. And let's talk through this. Now clearly, if we want to do anything in the router, we need buy in from the router vendors. So this is a big challenge and convincing the router makers that, yes please open up the network so that I can dump some code in it and execute. The code, so that is a big challenge. The second is also a big challenge and that is, if you look at the traffic on the internet today it's just humongous. There's so much traffic on the internet, and this is the reason why routers are dumb animals. All that the do is when a packet comes in, it's all happening in hardware, they do a table look up to figure out. Given the destination, what is the next stop that I send this package to? So the internet core, the routing fabric is operating at huge speeds, because even at the edge of the network today, we are already seeing gigabit speeds. Which means that the core of the network, you have several hundreds of gigabytes of packet processing that needs to be done and therefore it is important that the core of the network be blazingly fast. And software routing is not going to be able to match the speed that is needed in the core of the network for packet processing. So these two choices are good choices. Now, does active network's make the Internet more vulnerable? Not really because the internet is already vulnerable. Perhaps, it adds to it but not particularly making it more vulnerable than what it is already. And the last choice regarding code spoofing, so long as we make sure that the fingerprint that we generate to associate with the code, that is going to be used for processing the capsule when it arrives at a node. Is, cryptographically strong, then we can make sure that this code spoofing does not happen, so I would say, these are the two choices that apply for this question. So let's talk about the feasibility of the vision of active networks. The reality is, router makers like Cisco are loath to opening of the network. So while the idea of active networks is very fascinating, that we can be frugal about the resources that we use in the Internet for different network flows and we can actually virtualize the physical infrastructure by slapping on. Our own idea of what should be the, the kind of network flow that I want for my packets, seems very attractive, but really it's not going to be feasible given that we have to open up the network. So it's going to be feasible only at the edge of the network. Secondly, when we are using active networks, we are talking about executing code in a router in order to determine the routing decision at that node. Or in other words, we're doing software routing. Software routing cannot match the hardware routing, because at the core of the network, there's so much of traffic being handled that you really want to do this in hardware, and doing this at software speed is not going to match the hardware speed of packet processing in the core of the network. So once again, this argues that active network is only feasible at the edge of the network and finally there are social and psychological reasons why active networks is maybe a little bit hard to digest. It is hard for the user community to accept arbitrary code executing in a public routing fabric. If my traffic is flowing through the network and if the router is going to actually execute some code in order to do the processing of my packet, that worries me. Already, we talk a lot about privacy and the fact that in corporate networks, in university networks, we are losing a lot of privacy. People are watching what's going on. And now, saying that the routers are going to do something intelligent, a smart processing packets, that might be a socially and psychologically unacceptable proposition. So these are reasons why it would make it difficult to sell the idea of active networks to the wide area internet. On the other hand, the idea of virtualizing the network flow is very appealing. And if you put together the two thoughts that I had, one is the idea that we can virtualize the network and the second that active networks is only feasible at the edge of the network, that brings up a very interesting proposition, which I am going to mention in my concluding remarks. Active networks was way ahead of its time, and there was not a killer app to justify this particular line of thought. Further, active networks focused more on safety and less on performance, so in the 90s it seemed more like a solution looking for a problem. But difficulties with network management, rise of virtualization, the right hardware support and data center and cloud computing have all given active networks a new lease of life in the form of Software Defined Networking or SDN for short. Specifically, cloud computing promotes a model of utility computing where multiple tenants, by that I mean businesses, can host their respective corporate networks simultaneously on the same computational resources of a data center. Not that this is going to ever happen but imagine Coke and Pepsi, corporate networks running on the same data center resources. What this means is there is a need for perfect isolation, of the network traffic of one business from another, even though each of the network traffic is flowing on the same physical infrastructure. This calls for virtualization of the physical network itself, and hence the term, software defined networking. You will learn more about SDN if you take a companion course on networking that is offered in this same program. By now, I'm sure you have recognized that an operating system is a complex software system. Especially the protocol stack that sits within an operating system is several hundred thousand lines of code. And developing such a complex software system, in general, Protocol stack in particular, to meet specs and also deliver very good performance, is no mean challenge. Let's skip over to the other side of the fence and see how complex hardware systems for example, a CPU chip like this one with billions of transistors on it, is built. [UNKNOWN] technology uses component based approach to building large and complex hardware systems. Can we mimic the same method for building large complex software systems? That is, rather than start with a clean slate. Can we reuse software components? It's very tempting since component based design will be easier to test and optimize at an individual component level and also it'll allow for easy evolution and extension through the illusion and the leash and of components. The idea of component based design is orthogonal to the structure of the operating system that we've seen in previous lessons. Be it a monolithic kernel or a microkernel based design, the idea of component based design can be applied to each subsystem within an operating system. Of course, there are challenges. Can this lead to inefficiencies in performance due to the additional component level of function calls? Could it lead to loss of locality when we cross boundaries of components? Could it lead to unnecessary redundancies, such as copying, et cetera? Is it possible to get the advantages of component based design without losing performance? The short answer is' yes' if you can put theory and practice together. We will explore this topic more in this lesson. The idea is to synthesize Network Protocol Stack from components. How do we do that? But that's what were going to see in this lesson. And in this lesson, kernel's ensemble project is used as a backdrop to illustrate how to build complex systems by using components. And using a component based approach to putting together a large complex hardware system. So, the idea is to put theory and practice together in the design cycle. Theory is good for expressing abstract specifications of the system at the level of individual components. For this part of the design cycle, namely specification of what we want to build. A theoretical framework called IO automator is used and it's syntax is very similar to C like syntax. So, in terms of writing the specification, it's very intuitive in how we go about using IO automator to develop the system requirements and specify them using the syntax provided by our automator. And more importantly, the composition operator that's available in IO automator allows expressing specification of an entire subsystem that we want to build. Example, if you want to build a TCPIP protocol stack, then all of the functional relationship between the components of the subsystem can be expressed using the powerful specification language, primitives, available in IO automator. The second part of the design cycle is converting the specification in IO automator to code that can be actually executed. And for this purpose, the programming language that is used is a high level programming language called OCaml. Stands for object oriented categorical abstract machine language, and it has several properties that make OCaml a very good candidate for doing component based design as we are proposing to do in this lesson. Number one, the formal semantics that is available in OCaml is a nice compliment to the specification that we've done using IO automator. That's number one reason why OCaml is a good vehicle for converting the specification into code. The second nice property that it has is it's object oriented. Being an object-oriented language, it has some very nice properties such as no side-effects for things that you do in the program. And in fact, this comes from the fact that OCaml is a functional programming language, so some of the properties of the functional programming language, including no side effects and so on, make this an appropriate vehicle to convert the specification into code. And third, perhaps the most important characteristic from an operating system designer's point of view, is that the code that you can generate with OCaml is as efficient as C code. This is super important when you're developing operating systems. Because you care about performance. Object oriented is good. Formal semantics is good. But you also want to know that you get good performance out of it. And yes, you can get pretty good performance out of OCaml. Because, the object code that we can generate from the OCaml compiler is very efficient, similar to C. All of these make OCaml a good vehicle for going from specification to actual code. At this point, what we have is an unoptimized code that faithfully implements the specification that we started out with. And remember that we are doing component-based design, so it's going to be highly unoptimized because there is a lot of craft that goes between these components that you put together, like level blocks. So we have to optimize it before it is ready for prime time. Now, how do we do that? Well, once again, return to theory. NuPrl is a theoretical framework for optimization of OCaml code. The input to this framework is the OCaml code and the output that you get is an optimized version of a functionally equivalent OCaml code. So that's what this optimization framework gives you, takes the input, unoptimized OCaml code and produces optimized Ocaml code. And the optimized OCaml code can be theoretically verified to be functionally equivalent to the unoptimized input OCaml code. That's the beauty of this theoretical framework. And the way it does that is a little bit beyond the scope of this course. It uses a theorem-proving framework in order to do this and verify through theorem proving that the resulting code that is generated is equivalent to the input code which is unoptimized. I've given you the big picture of the design cycle, now it is time to go into the weeds So what I'm showing you here is sort of a software engineering road map to synthesizing a complex system. So what we're going to do is, first we're going to go from specification to implementation. And the figure shows a workflow in building a complex system. In particular, what I focused on is a TCP/IP network protocol stack using this methodology of the design cycle that I presented. So I start with. What is called an Abstract Behavioral Spec. Now this is where we use this IoA automata. And this abstract behavioral spec is where we describe the functionality of the subsystem in terms of requirements. And we are presenting the high level logical specification of the components and the specification is really talking about the properties that we want, the subsystem to adhere to. Not how it is going to be accomplished, or how, how it is going to be executed, but the properties that it has to adhere to. For example, in the protocol. If we desire that the package should be delivered in order, that's a property that we want for the subsystem. Those are the behavioral properties that are going to express using the I/O Automata. Another behavioral specification properties that you can have are things like, well, on every packet there should be an acknowledgement. That's another property that you want the protocol to have. You can express that using the I/O Automata. So the abstract behavioral spec is simply and lends itself to deriving properties about the behavior of the system. Not the execution, but the behavior of the system, such as [UNKNOWN] and so on. It is not executable code, even though I told you that the syntax of I/O Automata is similar to C, it's not executable code. But the interesting thing is that Once you've expressed the abstract behavioral properties, you can actually verify whether the behavioral specification meets the properties that you want in the sub-system. So proving that the behavioral spec meets the properties is facilitated by the I/O Automata theoretical framework. That's the nice thing about this behavioral spec. Once we know that the behavioral spec is meeting the properties that we set up for the subsystem, then we are ready to roll our sleeves and go down the path of implementing the behavioral properties. Next step in the process is getting to a concrete behavioral spec. We get to this concrete behavioral spec from the abstract behavioral spec through a whole set of refinements to this abstract behavioral spec. For instance, the refinement could be that, if I have a Q Data structure. I want first come, first serve property, how do I make sure that the abstract behavioral spec adheres to the additional execution condition that I want of first come, first serve. Those are the kinds of things that I can do, and refining the abstract behavioral spec to get a concrete behavioral spec. This is still not code, but it is closer to code than abstract behavioral spec. It's closer to implementation. And from the concrete behavioral spec, we get to the actual implementation using OCaml as the programming language. And between the implementation and the concrete behavioral spec, there is not a whole lot of difference. It is really the scheduling of the operations that we want in the concrete behavioral spec that is being detailed. when we go from this step to this step and producing OCaml code, which is finally an executable code for the original abstract behavioral Spec that we started with. I already mentioned some of the reasons why they chose OCaml as the implementation vehicle. Functional programming language, formal semantics, and it also leads to compact code. High level operations and data structures and also it has features like automatic garbage collection, automatic memory allocation, and marshalling and unmarshalling of arguments. This is very important because I mentioned that we're going towards Building a complex system, from a specification, using a component based design approach, which means that we're going to take these components and mesh them together, just like you take Lego blocks to build a toy. Similar to that we are taking components and meshing them together to get the complex implementation, and when we do that We are necessarily going across different components and when we go across different components you have to adhere to the interface specifications of those components which means you have to marshall the arguments and unmarshall the arguments when you go between these components and Ocaml Has facilities for doing these kinds of marshalling and unmarshalling built into the language framework which makes it an ideal vehicle for Component-based design. And also the programmability of Ocaml is similar to C and the definition of the primitives in OCaml makes it a good vehicle for developing verifiable code. At this point what we have, is an unoptimized version of the abstract behavioral spec. Implemented, ready to run, but it is unoptimized. Remember that as component-based design, there's going to be a lot of crap that is going to be there in terms of the interfaces between the components that have been assembled together. To build the complex system. One word of caution though, I mentioned that using IO automata is fantastic from the point of view proving that, the properties that we want in the original subsystem is actually met by the behavior spec. That is facilitated by the IO automata framework. But this path that we took going from the abstract to concrete behavior spec, to the implementation finally ending up with unoptimized version. Which is executable there is no guarantee that this an optimized version, of code that we produced is actually meeting this behavioral spec. Or in other words this leg of design exercise proving properties. Of the spec. That it meets what we set out for the original subsystem, is in no way guaranteeing that those properties are actually in the implementation. There's no easy way to show that the OCAML implementation is the same as the IO autometer specification. This brings to mind a famous quote that is attributed to Don Knuth. Many of you. We know him as an expert in algorithm development. He's written several books on it. And a quote that is attributed to him is beware of bugs in the above quote. I've only proved it connects, not tried it. [LAUGH] So in other words, he is, he's and expert in developing algorithm but he's saying I need convert the algorithm to code. There's no way to prove that the code is actually, faithfully reproducing the algorithm. That's sort of the same thing that is going on here as well. That we have an abstract behavioral spec. We can prove properties about the behavioral spec, and we can convince that whatever we set out for the subsystem is actually met, in terms of the specification but there is no way to prove. That this implementation is actually faithfully reproducing the behavioral spec. This is the software originating road map, for synthesizing a complex system starting from the behavioral spec, all the way to an un-optimized implementation of the system. So those are the first two pieces of the puzzle, namely specification. And then implementation. And the third piece of the puzzle of course is going from this unoptimized version to the optimized version. As I mentioned earlier, for going from the implementation to optimization, we once again turn to the theoretical framework. And in this case, we're going to use this Nuprl theorem proving framework. The Nuprl theorem proving framework is a vehicle by which you can make some assertions and prove some theorems about the equality of optimized and unoptimized code and let me elaborate. First of all, you start with this OCaml code, which is unoptimized and there is a tool that converts this unoptimized OCaml code to Nuprl Code. This is once again an unoptimized version of the original OCaml code, but it is a Nuprl code and this Nuprl code is one which is at the base of this theorem proving framework and what we can do with this theorem proving framework is convert this Nuprl code to optimized Nuprl code. And through a whole series of optimization theorems that are in this framework, we can actually show that the optimized Nuprl code is equivalent to the unoptimized Nuprl code. So we showed so far as the operating system design experience is concerned, we're going to treat this as magic. And if you are interested in digging deeper, I welcome you to do that. But for the purposes of this lesson, we're not going to go into the theoretical details of how this theorem proving framework does its work of ensuring that the optimized Nuprl code is the same as the unoptimized Nuprl code. Now, once this step is completed, then there is another tool that converts this optimized Nuprl code back into the optimized OCaml code. Now, we are ready to deploy this. So this is sort of the design cycle going one full round, going from specification to implementation, implementation to optimization and from optimization back to deployable code, that we can then take and put it on the system. Okay, now let's put this methodology to work. Specifically, we're going to look at how to synthesize a TCP IP protocol stack using this methodology. Starting point of course is the IO otometer. We specify the protocol in all the detail that we want, abstractly. That's the abstract specification of the protocol. And we can prove that this abstract specification meets the properties that we want in the protocol, using the IOA framework. And from the abstract specification, through a whole bunch of refinements, we can get to the concrete specification. We saw that already. Those are the two steps that we already mentioned how we take and synthesize in complex software. The next part, how do we synthesize the stack from the concrete spec? In other words how do we generate this code, the O comma code. That represents this abstract specification for the TCP/IP protocol stack. So we need to have a starting point for that. The starting point to get an unoptimized Ocaml implementation starting from this concrete spec, is to use this on symbol suite of microprotocols. That the authors of this ensemble paper that I've assigned to you have synthesized at Cornell. Why use ensemble? Well, remember, our goal is to mimic the methodology that is used in building real site, large-scale, realized circuits. I mentioned at the outset that today, building a billion transistor CPU chip, the way it works is they use a component-based design, taking components of hardware structures that have already been implemented And assemble them together in order to realize this big, mammoth chip. And we are trying to do the same thing for building complex software systems. So the starting point has got to be a set of components, and that's what Ensemble Suite of microprotocols give you. Now, if you think about a protocol like TCP IP, that protocol has a lot of features in it, and each of those features require nontrivial amount of code building. So, for instance TCP IP has mechanisms for sliding window management... Flow control, congestion control, scattering and gathering packets in order to assemble messages into units that can be delivered to the application. All of these things are features of the TCP/IP protocol. And the ensemble suite of microprotocols has components for each of those functionalities. That you might have in any complex transport protocol, like TCPIP. And if you recall the paper that we read earlier by Tacket and Levey about optimizing RPC in a local area network, we said that one size fits all is not a good way to think about designing complex software systems. Depending on what the environment is, you have to adapt what the layers of systems off it do. And therefore, even though the TCP/IP protocol is well laid out in terms of what the functional requirements are, whether we need all of the components in a particular implementation of TCP/IP or not is something that is up for the designer to decide. And having a suite of micro protocols, gives you that freedom to mix and match the components that make sense in this specific environment for which to building a protocol stack. And that's the reason for using the ensemble suite of micro protocols. It has about 60 protocols that makeup the whole ensemble suite. The suite of micro protocols of ensemble written in Ocaml and [UNKNOWN] component based design of a complex system such as TCPIP protocol stack. And the micro protocols have well defined interfaces that allows composition. Every micro protocol has an interface for the layers that sit on top of it. An interface for interacting with layers that sit below it. And this is exactly the kind of component that you want so that you can actually assemble these components layer by layer in order to get the functionality that you want for the original protocol stack that you intended to build. Just to reiterate what we said in the, as the, just to reiterate what we set as the original goal, we want to mimic VLSI design in building a complex software system. And this well defined interfaces of the on symbol, microprotocol suite facilitates component based design. Given a behavior spec for a protocol say TCP/IP, can a stack be synthesized as a composition of the ensemble micro protocols? Given 60 micro protocols in the ensemble suite, there are way too many combinations for a brute force approach. Instead. In the ensemble approach, the user heuristic algorithm for synthesizing the stack given the desired behavioral spec and designer's knowledge of the micro protocols. The result is a protocol stack which is functional, but not optimized for performance. Of course, as operating system designers, we're always worried about perf, performance. And we naturally have to think about how to optimize the protocol stack, so that it is not only functional, but also performant. In particular, the fact that we've assembled this protocol stack like Lego blocks. Putting together all these micro protocols leads to layering and layering leads to inefficiencies. Now this is where the analogy to VLSI component based design, breaks down. And the reason is because in VLSI component based design, even though we are building a complex chi, like a CPU. By putting together components, the components just fit together very nicely. There is, there is no inefficiency in going between these components. But in software components unfortunately, they have interfaces. And interfaces usually mean that, that there is well defined boundaries between these components and so to cross the component boundary, you may have to copy parameters at here to interface specifications of the components and so on. All of those things leads to inefficiencies and this where the VLSI component based design idea breaks down little bit when we just put together the components of software in order to build the, the large scale system. So we have to do the extra work that is needed in order to optimize the component-based design so that it can perform well. Fortunately there are several sources of optimization that are possible. For instance I mentioned that OCaml has implicit garbage collection. Now it is good that it has implicit garbage collection as a fall-back. But, maybe we don't want to use it all the time and we want to be explicit about how we manage our own memory that can be more efficient, that is a source of optimization. I mentioned that OCaml has ability for doing marshaling and unmarshaling of arguments to go across layers, but is also a very good thing to do, in order to have a component-based design. But, when you're going across layers, these things can add overheads and this is another source of optimization that is possible by avoiding these marshalling and unmarshalling across the layers but collapsing the layers. Another opportunity that exists especially in networking systems, is the fact that there's going to be communication and computation. If you think about TCPIP Protocol, it has to necessarily buffer the packets that it's sending out. Because if the packets are lost for some reason, it may have to re-transmit it. This is again, a situation where we can overlap this buffering which is computation on the node that is sending the packet with the actual transmission. So, we can overlap that. That's another opportunity for optimization. Another opportunity is compressing the header, especially when we have this layering, at every layer it might add a new header specific to that layer, and that may have common fields for instance, size of the packet or check sum for the packet and so on. Those are common things that you can eliminate when we are going across these layers. So the header compression is another possibility for optimization. Another thing that we always have to worry about, is making sure that the code that we execute fits in the caches and this has been something that we've talked about all along when we talked about single node systems to parallel systems. That locality enhancement, making sure that the working set of whatever code that is executing on a processor fits in the cache is very important. So, this again is an opportunity by identifying common code path across the different layers of the protocol stack, and co-locating that common code path across the layers. In order to make sure that we can enhance locality for processing is another source of optimization that is possible. This is all good. So there are lots of opportunities for optimization but do it by hand manually, that's tedious. So how do we automate the process of optimization so that we don't have to do it manually? So, this is where we turn to NuPrl. As I mentioned already, NuPrl is a theoretical framework. And there is a way by which the OCaml code can be converted to NuPrl code. There is a tool that allows that transformation to be done very simply. We use that tool to convert the unoptimized OCaml code to the NuPrl code and once you have the NuPrl code, then we can roll up our sleeves and say how do we go about optimizing this NuPrl code. There are two step process for this. The first step is what is called static optimization and this requires that a NuPrl expert and the OCaml expert sit together and, in a semi-automatic manner, they go layer by layer in the protocol stack, and identify what transformation code can be applied in order to optimize each of the layers. We're not going across layers, but for every layer we're asking the question, is it possible to simplify the functionality of what is happening in one layer by looking at the NuPrl code and using the NuPrl framework and the optimization theorems that is part of that framework to come up with a more optimized implementation of each and every one of these layers. That's what we're doing in this first step. This is where both the NuPrl expert and the OCaml expert have to sit together because you want the NuPrl expert to know whether the optimization that they are attempting to do is kosher with respect to the functionality that is desired in the OCaml code. And that's why this is semi-automatic. Because the optimization itself can be done, using the theorem proving framework. But whether that optimization is an appropriate one or not is something that has to be verified with manual intervention. And the kinds of code transformation that can be done, are things like function inlining. If you have function calls, then you can make them inline. And that is one way of optimizing it within a layer. And directed equality substitution is another optimization that can be done. There are some things that are very specific to functional programming, leading to code simplicity. And all of those things are what is being done in the static optimization of every layer in the protocol stack. Remember we are not going across layers, but every layer we're doing the static optimization. This is good, but unfortunately, that's not enough. Because we have lots of layers between a message arriving and the application getting that message or vice versa. And what we want to be able to do is, if possible, collapse all these layers. Because when you go through these multiple layers we're adding latency to the overall processing of any message. And this is where we are going to turn to the power of the NuPrl theorem proving framework. So the next step is dynamic optimization. Which is attempting to collapse these multiple layers, and it is completely automatic. The previous one I said, layer by layer, we're doing semi automatic fashion, there's completely automatic, and it is the power of that theorem proving framework which is actually going do that for you. And in order for that theorem proving framework to do this work, what we need to identify is, what are common things that happen in order to do this collapsing of layers. The trick to this dynamic optimization, is recognizing what the state of the protocol is at any point of time, and how the protocol has to react to an input event. Whether the event is coming from the top, or from the bottom, how the protocol has to react to that event is the trick, and this is what is called the common case predicate. CCP for short. For example, if we receive a packet and the state of the protocol says that if a particular sequence number is received, then we are ready to deliver the packet to the application. If that is the state of the protocol, that is what is called a Common Case Predicate. These Common Case Predicates are predicates that can be derived From the state of the protocol, by looking at the conditional statement. So, from the conditional statements in the microprotocol that have been implemented and available to you. We can actually synthesize these common case predicates. And once these common case predicates have been synthesized, then we can say, well, if the common case predicate is satisfied, then you don't have to go through all the craft indicated by the multiple layers of the micro protocols assembled on top of one another. But instead, we can do a much simpler processing and that's what is called a bypass code. So, in the dynamic optimization, once these common case predicates have been identified by looking at the conditional statements in the micro protocol, dynamic optimization framework, generates these bypass code and inserts it into this framework. So I'm going to complicate this picture a little bit now, so what we have is A Common Case predicate. For instance it says, is the sequence number in the packet what I am expecting it to be. Is this the particular sequence number? And if that is the sequence number I am looking for, then I can execute this bypass code and completely eliminate all these multiple layers of protocol and go directly to the upper layer perhaps all the way up to the application. On the other hand, if it is not the common case predicate that is being satisfied, then, you have to do the normal processing, of giving the packet to this micro protocol and it does its own thing. And you can see that this kind of framework can be applied to every layer. So you can take, this is a single layer, and find out what the common case predicate for this micro-protocol, and use that common case predicate to signify whether we want to use the bypass code, or whether we want to go to the micro-protocol. And we can collapse multiple layers like this. And derive a common case predicate that collapses all of these layers into a single predicate, and if it is satisfied, then it can eliminate processing the packet to all of these layers, and simply go through the bypass code to get to the upper layers. That's the beauty of the dynamic optimization, and it's completely automated. And it comes from the power of the theorem proving framework of Nuprl. And the optimization theorem that is in the Nuprl framework proves the equivalence of the bypass code, to the layers of protocol that it is replacing. That's the beauty of the Nuprl framework, that in the theoretical world of the theorem proving framework, we can actually prove through optimization theorems. That this bypass code does exactly the same job as all of these multiple layers of micro protocols that were to process this message. So once we have done this dynamic optimization, and collapsing the layers, generating the bypass code, starting of course with the common case predicate derived from the micro protocols, then we are ready, to convert the optimized new Pearl called back to OCaml. And again, I mentioned that there is a tool, that, that's available. Straight forward tool that converts the new Pearl code to the OCaml code, and the final OCaml code that we generate through this conversion process is the optimized version of the original Ocaml code and, the theorem proving framework can assert that the original unoptimized ocaml code is equivalent. To the new optimized Ocaml call because of the power of the theorem proving framework. This is where we've sort of put together theory and practice to get the best of both worlds. A word of caution, however. There's a difference between optimization and verification. All that the NuPrl framework is doing is optimization, not verifying whether the Ocaml code is adhering to behavioral spec of ioatometer. That's not what is being done. What we're doing here, is saying that. We've taken unoptimized Ocamel code and produce an optimized Ocamel code. And through the theoretical framework, we can assert that the two are functionally equivalent. So this exercise has shown the path to synthesizing complex system software, starting from specification, to implementation to optimization, putting theory and practice together. As operating system designers, the natural question that comes up is, okay, all this sounds good. But do I lose out on performance for the convenience of component-based design? This is the same question that came up when we wanted to go for a microkernel-based design, away from a monolithic design of an operating system. The Cornell experiment takes it one step further and argues for synthesizing individual subsystems of an operating system from modular components. Just like putting together lego blocks to get the desired functionality. I encourage you to read the paper from Cornell in full. That shows that this methodology, applied to one specific subsystem, results in a performance competitive implementation of the protocol stack, compared to a monolithic implementation. Welcome back to the next module of the advanced operating systems course. Recall that the Cornell experiment that we saw as the last piece of the previous module argues for a component based design to reduce the pain points in the development of complex software systems. And industries that are designing and commercializing production operating systems and distributed services through the client-server paradigm, there is another important pain point. And that is how to design for the continuous and incremental evolution of complex distributed software systems, both in terms of functionality and performance. The short answer to the puzzle is distributed object technology. We saw how object technology is employed in the Tornado parallel operating system as a structuring tool to allow the scalability of operating system services in a parallel system. In this module of the advanced operating systems course, we are going to see examples of how distributed object technology is influencing commercial offerings in the computer industry. We'll start this lesson module with the discussion of the Spring system, which was designed and implemented in Sun Micro Systems as a network operating system for use in a local area network. Later on, Spring was marketed as Sun's Solaris operating system. Before we discuss the Spring system, a little bit of history and some personal connection. Yousef Khalidi, one of the chief architects of the spring system, got his PhD from Georgia Tech in 1989 developing the cloud's distributing operating system, which is an object based operating system. And he was my numero uno PhD student incidentally. Not surprisingly, the Spring system was heavily influenced by Yousef's work with clouds. And Spring came out commercially as Sun's Solaris MC product. And for the trivia buffs out there, Yousef is now heading Microsoft's Azure Cloud Computing product. By the way, Azure has nothing to do with the cloud system that Yousef developed as a grad student at Georgia Tech. Later on, when we discuss giant scale services and cloud computing, we will feature an interview with Yousef wherein he shares his thoughts on future evolution of distributed system services. Now back to our discussion of the Spring system at Sun. There is always a quadrum of how to innovate in the operating system. Academia is ripe for pursuing ideas that are on the lunatic French but, if you are an industry, you are always worried about, should we do a brand new operating system? Or do a better implementation of a known operating system. Research industry is usually constrained by the market place that it serves, specifically if you're a company like Sun Microsystems; which in its heydays, between 1980 and the 2005, it was making Unix workstations. And it was building large complex server systems which run 24/7 for a variety of applications, such as airline reservation and so on and so forth. And if you are in that marketplace, the question becomes should we build a brand new operating system or build a better implementation of a known operating system? Marketplace demand says that, well, there are legacy applications that are running on your current operating system and therefore building a brand new operating system may not be that viable in an industrial setting. So the approach they took in the Spring system at Sun Microsystems, is to be different but innovate where it makes sense. And, it is a sort of like the, you may have seen commercials that says, Intel inside, and the idea is in processor architecture, Intel is dominant and a lot of interesting computer architecture research happens in innovating under the covers in the micro architecture. So the external interface is still well known interface like the Intel processor but underneath they do a lot of innovation in the micro architecture. In a similar manner, if you are a company like Sun Microsystems that peddles Unix boxes and you want to retain your customer base, then you want to make sure that the external interface remains UNIX and external interface remains as UNIX. But under the covers, you innovate where it makes sense. And in particular, you want to make sure that everything that you do in the operating system allows third party vendors, to develop software against the new APIs that you may provide in the operating system and integrate that into operating system. While at the same time, making sure that such integration is not going to break anything. Or said differently, you want to preserve all the things that are good in standard operating system, but at the same time you want to make sure that the innovation allows extensibility, flexibility and so on. That's sort of the approach that Spring system took and for all the things that I just said, using object orientation is a good choice in order to make sure that we can do innovation under the covers, while keeping the external interface the same. That brings us to a discussion of procedural design versus an object-based design. You're all familiar, I'm sure, with procedural design where you're writing your code as one monolithic entity, and in a procedural world, you have shared state in terms of global variables, and you may have private state in the caller and the callee. And state is now distributed all over the place. So, basically, the interface between the caller and the callee is through the normal procedure call mechanism that one sub-system may make a procedure call that goes into another subsystem. And this is how monolithic kernels are built where state is now strewn all over the place. There maybe some shared state and private state of subsystems and so on, and this is typically how monolithic systems are built. To contrast this procedural design to object-based design, in an object-based design, objects contain the state that is entirely contained within this object, not visible outside. And there are methods that are inside those object that manipulate the state that is part of this object. So in other words, externally, the state is not visible. The only thing that is visible are the methods for invocation and these invocations work on the state that is local to the object. So what you get with an object-based design is strong interfaces and complete isolation of the state of an object from everything else. Contrast that with the procedural design, where the state can be strewn all over the place, and the shared state can be manipulated from several different subsystems that are part of a big monolith. But in this case, what we have is, strong interfaces that completely separate one object from other objects. And the state that is specific to an object is contained entirely inside this object, invisible to other objects outside except via well-defined invocation methods. That have been exposed by this object implementor to the outside world. As OS designers, the immediate question that might come up is, well, if we have these strong interfaces, it sounds similar to what we discussed when we talked about the structure of operating systems early on, and that is border corssing across protection domains. Is it going to cost us? But, there are ways around it. To make these border crossing performance conscious as well. Now, where to apply this object orientation? Well, in Spring, for instance, they applied object orientation in building the operating system kernel. So the key point to take away is, if object orientation is good at the level of implementing a high performance operating system kernel, it should be good at higher levels of the software as well. And while I am expounding the virtues of object-based design here, we have already seen this when we talked about Tornado system. That was also using an object-based approach to building operating system kernels. So, the Spring Approach to building operating system is to build strong interfaces for each sub-system. What that means is, the only thing that is exposed outside a sub-system is what services are provided by the sub-system but not how. In other words, the how part of it can be changed at any time, so long as the external interface remains unchanged. So that is what is meant by strong interfaces, and this naturally leads to object orientation. And they also wanted to make sure that the system is open and flexible. And this is important if you're an operating system vendor and you want to integrate third party software into your operating system. You want to make sure that your interfaces are open and flexible and at the same time, you want to maintain the integrity of your subsystems, and that's why strong interfaces are extremely important. And being open and flexible also suggests that you don't want everything to be written in one language. You don't want to be tied to a particular language for implementing all the system components, and this is the reason that in Spring they chose to use IDL, which is the interface definition language, and this is from the OMG group. There are IDL compilers that are available from several third party software vendors, and what that allows you to do is, you can define your interfaces using IDL. And third party software vendors can use that IDL definition of the interfaces and use them in building their own subsystems that can integrate with the Spring system. And the other part of a Spring approach is extensibility and extensibility naturally leads to microkernel based approach and that's what you see here. This is the structure of the Spring system and what you see below this red line is Spring's idea of a microkernel and in fact there are two parts to it. There is a nucleus, which in Spring is the entity that provides the abstractions of threads and interprocess communication among the threads. And the kernel itself is made up of nucleus plus the virtual memory manager. So if you have put these two things together, the nucleus gives you threads and IPC and the VM manager gives you memory management. And if you remember back to our good old friend Liedtke's principle of what a microkernel should provide. You see that what is below this red line is exactly Liedtke's principle that is the microkernel is providing the abstraction of threads in IPC and an abstraction of memory. And everything else is outside the kernel. All the things that are above the red line are outside the kernel, and, in particular, I mention that Spring is Sun Microsystems' answer to building a network operating system. Because this is a time when transitioning was happening to services that are being provided on the network. And so, they wanted to go from an operating system that runs on a single node to a network operating system using the same interface. Namely the Unix interface, and so this entity that you see here, which is called the network proxy. We'll see that, more of it, in later discussion in this lesson. This is the entity that allows machines to be connected to one another. All the ovals that you're seeing that are outside the kernel provide different services that you might need in your desktop environment. For instance an X11 server is a display manager and you may need ability to do shell level programming, and you need file system, and you need a way by which you can communicate in the network, meaning that you need a protocol stack. Nucleus is micro kernel of a spring and it is subset of leap case prescription as I mentioned just now. In the sense that, nucleus manages only threads and IPC. The abstractions available in nucleus are the following. There is this domain. A domain is similar to Unix process, it's a container, or an address space, and threads can execute in a particular domain. These threads are similar in semantics to P thread that we have seen before, and this abstraction called door, is a software capability to a domain. It's you can think of it as a real life analogy of. Opening a door in order to get into a room. In a similar manner, if you have a handle to the door you can open the door and enter a target domain. So that's the idea behind door. So, any domain can create these nucleus entities called doors, which are essentially entry points for entering the target domain. With the object orientation, I told you that the only thing that you can do is make invocations on objects, and the entry points available and the objects that are contained in a domain are represented by this abstraction called door, that's provided by nucleus. Let's say, I'm a file server. What will I do? Well I have entry points in my file server, such as, opening a file, or reading a file, writing a file, and so on. Basically, I will create those entry points as doors into my domain. And if I'm a client, how do I get access to the entry point that's available in the target domain? Well, the way I do that is exactly similar to how you may be opening your file in a Unix file system. What you do is an f open, and when you do that, you get a file descriptor, which is a small integer that is a handle for you to access that file. In a similar manner. If I'm a client and if I want the ability to invoke a target domain, a particular entry point, then what I want is an access to this door and the way I get that is by getting a door handle. So I get a door handle. So every domain will have this door table, which is similar to the file descriptors that you may have in a Unix process. And every door ID that you have in this door table points to a particular door. If I have a door handle in my door table for a particular door, what that tells me is that, oh, I have the ability. To, make an invocation in the target domain that this particular door corresponds to. So the possessor of a door handle, is able to make object invocations on the target domain using this door handle. And as you can see, a particular client domain can have a door table that has access to several different target domains. So in this case, these two entries in my door table points to this door, which probably are entry points into this target domain. And other door, which are a different set of entry points, and I have access to them as well. And multiple clients may have access to the same door, because if it's a file system, for instance, you may be able to access the file system, I may be able to access the file system, and so on. So, the door table is something that is unique to every domain and it gives that domain an ability to access the entry points in the target domain, so that they can make object invocations. So, the way to think about this door, it's basically a software capability to a domain. Since we are using object orientation, it is represented by a pointer. To a C++ object that represents the target domain. And door can be passed from domain to domain but it is a software capability and it can be passed from domain to domain, and when it is passed from domain to domain it gives the ability for those domains to actually get access to the entry points specified through the door, to the target domain. And the spring kernel itself is a composition of the nucleus plus the memory management, that is inherent in the fact that these domains represent an address space. Now, how do you go about making an object invocation, that is, you want to make a protected procedure call into a target domain from a client domain. How do I do that? Well, the nucleus is involved in every door call, so they won't open the door. I need the permission of the nucleus. And what I do is, when I make the invocation using the small descriptor that I have, which is a door handle, the nucleus looks at it, says okay this domain has the ability to do this invocation. And it allocates a server thread on the target domain, and executes the invocation that is indicated by this particular door handle. It's a protected procedure call, and since it is procedure call semantics, the client thread is deactivated, and the thread is allocated to the target domain, so that it can execute the invocation for the method that is indicated by this door handle. And on return from this target domain, once that protected procedure call is complete, the thread is deactivated. And the client thread is reactivated so that the client can continue with whatever it was doing before. So, this is very very similar to the communication mechanism that we discussed in the lightweight remote RPC paper before, in the sense that, we're doing very fast cross address space calls using this door mechanism. This protected procedure call is in illustration of how nucleus makes sure that even though it has an object based design and it is using object orientation in the building of the, in the, it is using object orientation in the structuring of the operating system kernel. It ensures that it'll still be performant, in the sense that you can do this cross domain calls very quickly through this idea of deactivating the client thread and quickly activating the thread to execute the entry point procedure in the target domain and on return reactivating the client thread. And all of this results in very fast cross address space calls through this lower mechanism. That's how you make sure that you get all the good attributes of object orientation and not sacrifice on performance at the same time. As I mentioned, Spring is a network operating system. So, what I described to you just now, is how object invocation works within a single node. But these doors are confined to the nucleus on a single node. And we need to be able to do object invocation across the network. The client domain may be over here and the server domain may be on a different node on the local area network. Object invocation between client and server across the network is extended using network proxies. For example, on the client box there is this Proxy B and on the server box, there is the Proxy A and proxies can be potentially different for connecting to different servers. So, this client may talk to this server using this proxy. And may talk to a different server, which I'm not showing here using a completely different proxy. In other words, the proxies can potentially employ different protocols. That's where you have the opportunity to specialize. Whether the communication that's happening between the client and server is on the local area network or on a wide area network, and so on. Depending on that, you can employ the protocol that is appropriate for use in the proxy. So, this is a key property of building a network operating system in Sun where they wanted to make sure that decisions are not being ingrained in the operating system of a single node, in terms of the connectivity of that node to other nodes on the network. Depending on where the servers for a particular client is going to be maintained, that is where the location of the server is. You can employ different protocols to talk between the proxies that are on the client machine and the server machine. And also the proxies are invisible to the client and the server. In other words, the client and the servers are unaware whether they are both on the same machine or on a different machine, and they don't care. Let's see how this client-server relationship is established using these proxies. So when a client-server connection has to be made across the network. The first thing that happens is, you instantiate a proxy on the server node and establish a door for communication between the Proxy A and the server domain through the nucleus on the server machine. And now what does Proxy A is going to do, is to export a network handle embedding this Door X to its peer proxy, B that is on the client domain. And see that this interaction that's going on between Proxy A and Proxy B is outside of anything that is in the preview of the nucleus. So the network handle that is being established has nothing to do with the primitives or the mechanism that are available in the nucleus of the Spring system. So what proxy is doing, is to create a network handle embedding this Door X. And it is going to export that to this Proxy B and Proxy B has a door that it has established locally on Nucleus B so that the client domain can communicate with it. And now what Proxy B will do, is it will use the network handle that has been exported by Proxy A to establish a connection between the two nuclei. So this network handle and the communication that goes on between these two guys is not through the nucleus. That's important for you to understand. So now, how does the client make an invocation on the server domain? Well, when the client wants to make an invocation, it thinks that when it is accessing Door Y, it is accessing the server's domain. But it isn't. What it is. What it is accessing, is this Proxy B and of course access to this Door Y, which is in Proxy B, is blessed by Nucleus B, and when this invocation happens, Proxy B then is going to communicate through this network handle that it has with its peer Proxy A. And the peer Proxy A, when it gets this client invocation proxied through this Proxy B and arriving at Proxy A, will know that oh, this is really intended for the server domain. And I know how to access that through the door that I have in the server domain, and it uses the door it has in the server domain in order to make the actual invocation. So to recap, what is really going on, the client wants to open this Door X. It doesn't have a direct handle on Door X because server domain is in a different node of the network. And therefore, the way remote invocation is accomplished, is by the server domain's door which is the entry point into the server domain, is passed on by this proxy via a network handle to its peer proxy on a different node, in this case the client node. And once this network handle is available to Proxy B, it can establish the connection between these nuclei, and once this connection is established. Then the client domain, it thinks it is making an invocation call for Door X, but in fact it is being passed through Door Y to this proxy. And the Proxy uses a network handle to communicate that invocation over to Proxy A which then uses the actual door that will open the invocation call under server domain and execute the client domain's call. It may often be necessary for a server object to provide different privilege levels to different clients. For instance if you have a file server the file server may have different access privileges to different classes of users. And in order to facilitate that kind of a differential invocation of objects, the security model that Spring provides is via what is called a front object, so this is the underlying object. And an underlying object may have a front object that is completely outside of the Spring semantics for object invocation. The connection between the front object and the underlying object is entirely within the purview of the implementer of the service. In other words, this connection is not through the door mechanism that I told you about that Spring system provides you. So, all that the client domain is going to be able to do is access the front object. And the front object will register the door for accessing it with the nucleus, so that the client can go through this door to this front object, and the front object is the one that is going to then check the access control list, ACL, in order to see what kind of privileges this client domain has in order to make an invocation on the underlying object. And it is possible to have multiple front objects to the underlying objects with distinct doors registered with the Nucleus for different implementation of control policies that you want for a particular service. So, in other words, the policies that you want for accessing the services available in an underlying object can be implemented in this front object or different instances of this front object depending on how many different control policies you want. So when a client invocation comes in through this door to the front object, the ACL, the access control list is checked before allowing this invocation to actually go through to the underlying object. As I mentioned earlier if this client domain has access to an invocation entry point in a server, that is it has access to a door, the client domain can pass this around because of the software capability. And the software capability can be passed around by the client domain to other domains in order to use that same capability to access the same object. But in so doing the client domain can decide whether it wants to give the same privilege for accessing this object or lesser privilege than what it has. Those are the things that can be implemented as policies through this front object. For example, let's say that the user wants to print a file foo. The user, of course, has full access to the file system for this particular object, that is the file that the user has created. This is a reference to the object foo and user has full access to that. But it wants to print the file. But it doesn't want to give privilege to the printer object any more privilege than it needs to have to print this. In particular, if I want to print a file, then all I need to do is give a one-time privilege to the printer object in order to print that file. So what I'm going to do is I'm going to take this capability that I've got for this file foo, reduce the privilege level and say that you've got a reference to the same object, but you have a one time reference. Now the printer object can access the file system and present its capability, and the front object, which is associated with the file system, will verify that yes, the one-time ticket that this guy has is not expended yet, and therefore it is allowed to access this file so that it can do its job of printing. But if it tries to present the same handle again, it'll be rejected by the front object associated with the file system because this is a one-time reference. The capability that is being provided by the user the printer is a one-time capability. So we've seen how object invocation can happen efficiently through the door mechanism and the thread hand-off mechanism that I mentioned within a single node, and it can happen efficiently across the network through the proxies, and it can also happen securely by the fact that you can associate policies in front objects that govern access to the objects. So these are all the mechanisms that are provided in the Spring kernel and this is where the innovation happens. Or in another words, the external interface, even though it is a Unix operating system, under the cover the Spring system does all of these innovation in terms of how to structure the operating system itself using object technology. This question concerns the abstractions that is available in Nucleus. Remember Nucleus is the microkernel of Spring. And the question asks, what is the difference between the primitives, or the abstractions available in Nucleus, and Liedtke's prescription for what a microkernel should look like? And these are the features that I am talking about. The first feature is the abstraction of threads, second is interprocess communication, and the third is address space. And what I want you to do is fill this table in terms of, what are the abstractions that I have mentioned here available in Nucleus versus Liedtke's prescription for a microkernel If you've been with me so far, you know that nucleus is providing only threads and IPC. It doesn't provide the abstraction of an address space. Because this is implemented inside the kernel of the Spring system. But, it is not what Spring calls as X micro-kernel because nucleus that contains only the threads and the IPC. Whereas Liedtke's description says you should have all three in the microkernel, and in fact, the Spring system does have it. It is just that in the Spring system they name things differently. They call nucleus their microkernel, but the idea of a kernel in the Spring system contains all three entities, even though the nucleus doesn't contain the address space. So virtual memory management is part of the kernel of spring, and now we will talk about how virtual memory management happens in the spring operating system. There is a per machine virtual memory manager, and the virtual memory manager is in charge of managing the linear address space Of every process. As we know, the linear address space of a process is what the architecture gives you, and what the virtual memory manager does is to break this linear address space into regions. And you can think of regions as a set of pages. So you take the linear address space given by the architecture, that's the process address space Break that up into regions, but each region is a set of pages. And each region can be of different sizes. The second obstraction in the virtual memory management system is what is called a memory object, and the idea of breaking up this linear address space into regions. Is to allow these regions to be mapped to different memory objects. So, for instance, this region is mapped to this memory object. This region is mapped to a portion of this memory object. And these two regions Different regions of the same address space are mapped to the same memory object and this is perfectly fine. So, this is how the Virtual Memory Manager takes the linear address space and maps it to these memory objects. And what are these memory objects? The abstraction of a memory oject allows a region of virtual memory to be associated with a backing file. Or it could be the swap space on the disk, and things like this. So this memory object is the mechanism by which portions of the address space can be mapped to different Entities, which maybe on the dis as swap space or files in a file system. All of those are available to the abstraction of the memory object, so that regions in an address piece can be mapped to the backing entities. And it is also perfectly possible that multiple memory objects may map to the same backing file that is also perfectly possible. so the way to think about these abstractions is linear address space broken into regions, regions mapped to memory objects, and memory object is an abstraction for Things living on backing store, meaning a disc. It could be the swap space on the disc, or it could be specific files that are being memory mapped in order to access from a process address space. Those are the abstractions available in the virtual memory management system, now we'll see how these memory objects Are then paged in and brought into the physical memory. So here is a virtual memory manager and it is responsible for an address space that it is governing and this is the guy that is going to worry about breaking a linear address space into regions and mapping those regions to specific memory objects. For a particular process that is living in an address space to access a particular memory object. Obviously, this memory object has to be brought into DRAM and that is what a pager object is going to do. Which is equivalent to the idea of what is called external pages in other systems, such as Mark. A pager object is responsible for making or establishing the connection between virtual memory and physical memory. And a portion of the virtual memory that is a region of the linear address space has been mapped to this memory object, and it is the responsibility of this pager object to make sure that this memory object has the representation in the physical memory when the process wants to access that portion of the address space range that corresponds to this memory object. So this pager object creates what is called a cached object representation for the memory object in the DRAM. So now, the portion of the address piece, that is the region of the address piece That this virtual memory manager mapped to this memory object one becomes available for the process to address in its DRAM because of the work done by this pager object in mapping this memory object into this DRAM. Similarly, a different virtual memory manager object. Managing a different address space can similarly map another memory object and clear a cache representation for this address space to map a region of its address space to this memory object using this pager object. I mentioned that the address space manager can make any number of such mapping between regions of the linear address space and memory objects. For instance, there's another region of the linear address space that is mapped to this memory object, too, and there may be a pager object that governs the paging of this object into a DRAM representation. So there's a cached object representation for this memory object which is part of the region Of the linear address space of a particular process managed by this VMM1. So in this example, this pager one is a pager for two distinct memory objects, memory object one and memory object two, and which are cached by VMM1 on behalf of a process. So there are two pager objects. One for each one of these things. So the important point I want to get across is that there's not a single paging mechanism that needs to be used for all the memory objects. So it gives you an ability to have different regions of the linear address space of a given process by associating different pager objects with each of the regions that correspond to a particular memory object. And all of these associations between regions and memory objects can be dynamically created. So for instance, this address space manager may decide to associate a region in this linear address space to this memory object. If it does that, then there is a new pager object that. Is going to manage their association between the region of the virtual address space that is mapped to this memory object three and the cached object representation is the DRAM representation of this memory object created by a pager object that is managing the relationship between this region. And this particular memory object three. Now this is an interesting situation, because you have a memory object that is shared by two different address spaces. And there are two distinct pager objects associated with managing The region of the address space in VMM 1 that maps to this memory object, and the region of the address space in VMM 2 that maps to the same memory object. Now what about the coherence of the cache representation of this object that exists over here? And the cached representation of this object that exists over here. Who manages that? Well it's entirely up to the pager object [INAUDIBLE] instantiated. In order to manage the mapping between this memory object and the cached object. So if coherence is needed. For the cache representation of this memory object in the DRAM of this address space and this address space, then it is a responsibility of these two pager objects to coordinate that. So it's not something that string system is responsible for, but it provides the basic mechanisms through which these entities can manage the regions that they are mapping. In terms of the memory objects and the DRAM representation of those objects. So in other words, external pagers establish the mapping between virtual memory, which is indicated by these memory objects, and physical memory, which is represented by the cached objects. So in summary, the way memory management works in the spring system is, the address space managers. Are responsible for managing the linear address space of a process, and they do the mapping of the linear address space of a process by carving them up into regions. And associating the regions with different memory objects, and these memory objects maybe swap space on the disk. Or it could be files that are being mapped into specific regions of the linear address space. Entirely up to the application, what they want to do with it, but these abstractions are powerful for facilitating whatever may be the intent of the user. And mapping the memory objects to the cache representation, which lives in DRAM, is the responsibility of pager objects. And you can have any number of external pages that manage this mapping. And in particular, through this example I've shown you that you can have, for a single linear address space, multiple pager objects that are managing different regions of that same address space. And that's the flexibility and power that's available in the structure, of the spring system, using the object technology. So to summarize the facilities of the perimeters available in the spring system. Object orientation, object technology permeates the entire operating system design. Its used as a system structuring mechanism in constructing a network operating system. To break it down, in the spring system you have the nucleus which provides you threads. And IPC among threads. And the microkernel prescription of lead k is accomplished by the combination of nucleus, plus the address space management that is part of the Spring System's kernel boundary. And everything else lives above. This kernel, meaning all the services you normally associate with an operating system such as file system, network communication and so on, were all provided as objects that live outside of this kernel. And the way you access those objects is through doors. And in every domain there is a door table that has a set of capabilities that a partiuclar domain has for accessing doors on different domains. And this door and door table is the the basis for cross domain calls. And through the object orientation, and through the network proxies you can have object invocation implemented as protected procedure calls both on the same node and across machines. And finally, it does virtual memory management by providing certain basic parameters, such as the linear address space, the memory object, external pages, and cached object representation. Now to contrast this to Tornado. In Tornado also we saw that it was using object technology, but the contrast is pretty distinct. In Tornado it uses clustered object as an optimization for implementing services. For example, weather a particular object is singleton representation, or it has multiple representation for each processor, etc. Those are the kinds of optimizations that are being accomplished using the clustered object in the Tornado system. Whereas in the Spring system, object technology permeates the entire operating system design in that it is used as a system structuring mechanism, not as just an optimization mechanism. In constructing a network operating system. Spring is a network operating system and the clients and the servers can be on the same machine, can be on different nodes on a local area network and in the Spring system, what they wanted to do was this idea of extensibility. They wanted to carry it to saying the client and the server should be impervious to where they are in the entire network. So the interaction should be freed. Or in other words, the client server interaction should be freed from the physical location of the clients and the servers. So for instance, in this picture, the clients and the servers are on the same machine. We've decided to replicate the servers in order to increase the availability, and now we have several copies of the servers and the clients are dynamically loaded to different servers for load distribution. And for those of you who are familiar with, you know, how services like Google work today, this is exactly what happens in services that we use on an everyday basis when we access Google. Our client requests are being routed to different servers and this is the same sort of thing that is happening in the Spring system that once you replicate the server, you want the client request to be routed to different servers depending on the physical proximity of the client to the servers, as well as the load that is currently being handled by one server versus another. Another variation of the same theme is where the server is not replicated, but the server is cached, for instance if it is a web server. Then there could be a proxy for the web, web server that is cached, and in that case the client request need not go to the origin web server, but it can go to the cached copies that are available. And so here again this decision of routing a client request to a particular cached copy of the server is dynamically taken. Not all of this sounds like magic in terms of how this client server relationship is being dynamically orchestrated, whether are in the same machine, or whether we dynamically decide to replicate the servers and decide to route the request to different servers, or we want to cache the servers and route the client request to different cache copies. All of these are dynamic decision that are taken. And how is this done? Well that's the part that we're going to see next. The secret sauce that makes this dynamic relation between the client and the server possible is this mechanism called subcontract. It's sort of like the real life analogy of off loading work to a third party, you know, you give sub contract to somebody to get some work done. That's the same analogy that is being used here, in the structure of the Spring network operating system. I mentioned earlier, that the contract between the client and the server is established thru the IDL. That is the Interface Description language, used to create the contract between the client and the server. And the subcontract is the interface that is provided for realizing the IDL contract between the client and the server. So here is the IDL interface and the client is using the server's IDL interface to make invocation calls on the server. An implementation of this IDL interface is accomplished through the Subcontract mechanism. Put differently, subcontract is a mechanism to hide the runtime behavior of an object from the actual interface. For instance, there could be a singleton implementation of the server, or it could be a replicated implementation of the server. The client does not care, and does not know. And all of the detail of how this client's IDL interface is satisfied is in the details of the sub contract itself. So what that means is, the client side stub generation becomes very simple because all of the detail of where the server is. How to access the server? Whether the server is on the same machine or on a different machine and are there multiple copies of the server? Which copy of the server should I go to? All of those details are in the subcontract mechanism. That makes the life of client side stub generation very, very simple. So subcontract lives under the covers of the IDL contract and you can change the subcontract at any time. So, for instance, if you don't like the work being done by one contractor, you give it to a different subcontract. Same sort of thing that can happen here is that the subcontract is something that you can discover and install at runtime. So, in other words, you can dynamically load new subcontracts. For instance, if a singleton server got replicated, then you get a new sub-contract that corresponds to this replicated server, so that now you can access the replicated servers using the subcontract. And nothing needs to change above this line. The client stub doesn't have to do anything differently. All of the details are handled by this subcontract, seamlessly. So in other words, you can seamlessly add functionality, to existing services, using the sub contract mechanism. Now let's look at the interface that's available for the stub that is on the client side and the server side through the subcontract mechanism. The first interface, of course, is for marshaling and unmarshaling. So the client side stub has to marshal the arguments form the client and in order to do that, it has calls that it can make on the subcontract saying that marshal these arguments for me. The subcontract will do that for you. Depending on whether this invocation is going to go to a server that is on the network or is it on the same machine. Or, is it on different processor on the same machine? All of those details are buried in the subcontract. And therefore, when the client stub wants to marshal the arguments for a particular invocation, it just calls the subcontract and says please marshal these arguments for me, and the subcontract knows the way in which this particular invocation is going to be handled, and so it can then do the appropriate thing for marshaling the arguments based on where the location of the server is. That's the beauty of the subcontract mechanism, and this is true on the server side as well as on the client side. And once the marshaling has been done, the client side can make the invocation. And when it makes the invocation, once again the subcontract says I know exactly where this particular invocation is going to go to. So it takes care of that. So the subcontract on the client side has this invocation mechanism obviously because the client is the guy that is going to make the invocation. On the service side the subcontract gives a different set of mechanisms. It allows the server to revoke a service, or it allows a server to tell the subcontract that yes, I'm open for business by saying I'm ready to process invocation requests. So what you see is that the client side and the server side, the boundary is right here. The client stub and the server stub don't have to do anything differently, whether the client and the server are in the same machine or in a different machine. Replicas of the machine, cache copies of the machine, none of those things make a difference in terms of what the client, when I say client I mean the client application plus the client stub, and similarly, the server plus the server stub, they don't have to do anything different. All of the magic happens down below in the subcontract mechanism. So to recap, the innovations in the spring system. It uses object technology as a structuring mechanism in building a network operating system and it ensures through the object technology that it is providing strong interfaces, it is open, it is flexible, and it is also extensible because it is not a monolithic kernel. It has a microkernel, and all the services are provided through these object mechanism living on top of the kernel. And the other nice property is that the clients and the servers don't have to know whether they are colocated on the same node or they exist on different nodes of the local area network. And object invocations across the network are handled through the network proxies. And the subcontract mechanism allows the client and the servers to dynamically change the relationship in terms of who they are talking to. You can get new instances of servers instantiated and advertise that through the subcontract mechanism so that the clients can dynamically bind to new instances of servers that have been created without changing anything in the client side application or the client side stub. So those are all the powers that exist when you decide how to innovate under the covers, which is exactly what Sun did with the spring system. The journey in this lesson should have given you a good idea of how it is possible to innovate under the covers. Externally, Sun was still peddling UNIX boxes, but internally they had completely revolutionized the structure of the network operating system through the use of object technology. If fact, the subcontract mechanism that Sun invented as part of the Spring system forms the basis for something that many of you who are Java programmers are using a lot, namely Java RMI. In this lesson that you're going to look at, we are going to study Java RMI and also Enterprise JavaBeans. In this lesson, we will continue to see examples of how, distributed object technology is influencing commercial offerings in the computer industry. First, we'll discuss java RMI, which has it's roots in the basic principles of distributed systems that we have been seeing so far. Before we start talking about Java RMI ,let's have a fun quiz to prime the pump. Now this question is asking you what was JAVA originally invented as a language for use in? Was it invented for using the Android platform? Was it invented as a language of embedded devices? Or was it invented as a language for B2B commerce on the world wide web? Some of you may not have been born yet, but this this language Java was originally invented as a language for using embedded devices in the early 90s. Java was invented by a gentleman by the name of James Gosling at Sun. It was originally called Oak, and it was intended for use with PDAs. Then when in the 90s, there was a lot of interest in video on demand using the internet, Sun thought that Java maybe the right language for programming set-top boxes, but unfortunately the cable TV industry went with SGIF for the VOD trials, and so Oak fell flat at that point. And Sun all but gave up on Oak then the World Wide Web caught on and Java got a new life with the need for containment of what happens on the client boxes connecting to the World Wide Web. And today a lot of internet e-commerce depend a lot on the Java framework. The intent in this lesson is not to talk about the Java language itself but the distributed object model of Java. The nice thing about the Java remote object model is that much of the heavy lifting that an application programmer has to do in building a client-server system using RPC. Things like marshaling, unmarshaling, publishing the remote object on the network for the clients to access. They're all subsumed under the covers by the Java distributed object runtime. And this is where one can see the similarity between the subcontract mechanism in the spring system that we saw recently, which was the origin in some sense for the Java RMI. Before we dig deeper, let me give you at a high level the distributed object model of Java. The term remote object in the object model of Java, refers to objects that are accessible from different address spaces. And the term remote interface is used in the distributed object model to say what are all the declarations. For methods in a remote object. Once you have a remote object the interface, remote interface is saying what are all the declarations for methods that are existing in the remote object that are accessible from clients anywhere. That's what remote interface is. And then the distributed object model of Java, the clients have to deal with RMI exceptions. So that's the failure semantics of the distributive model that the clients have to deal with exceptions that might happen when a remote method is invoked by a client. And there are some similarities and differences between local objects in Java and remote objects. The similarity is that you can pass object references as parameters when you make a an object invocation. An object invocation arguments of the invocation could include object references. That's the similarity, but the difference is that the parameters. When it is passed, it is passed as value result. That's the difference between local object and, and remote object. In the case of local object, when you pass an object reference as a parameter, then the method that is invoked can reach into that object that has been passed as a parameter as a reference and make modifications to it and that modifications will get reflected. In the original object but in the distributed model, because the object references are passed as value result. If going across a network, that is passed as value result meaning the copy of their object is actually sent over to the invoked method and that invoked method is seeing the copy of the object and it is even though there is a reference being given. But the reference is actually converted into value result in the parameter passing mechanism for the distributed object model. So that's a fundamental difference in parameter passing. So they're both similarities in the sense that you can pass object references as a parameter. But the difference is the reference is passed in a value result mode, as opposed to a pure reference. So, in other words, once an object reference has been passed as a parameter to the server. If the client makes changes to that particular object whose reference is being given in the invoked method, those changes, the server will not see it, because that is local to the client. That's fundamentally different from between the local object model of Java and the distributed object model of Java. Let's put this distributed object model of Java to work. And, the example that I'm going to construct is ,a bank account server. And, the server has obvious API's for accessing your bank account, you can deposit, you can withdraw, and you can ask for a balance. So, those are the API's that the server is going to provide as a service. Now, the question is. How best to implement it using Java. In particular, given that ,there is the remote object and the remote interface available as mechanisms in the distributed object model, what would be the best way to construct this service as a distributional object, accessible from clients anywhere in the network? Let's consider two possibilities. So the first possibility, or the first choice, is reusing a local implementation. So let's say that the developer has access to a local implementation of a class called account. And she then takes that class account and derives and extends it to, to create public methods in the API bank account, and creates a bank account implementation. So she started with a local implementation of a class account and extended it to implement the bank account. Now this bank account service that has been created, she has to make that visible outside, in the network, so that any client can access it. So this is where she uses the built-in class available in the distributed object model of Java called the remote Interface. And what she does is, using that built in remote class that's available in the distributed object model of Java, she makes the methods in the bank account class that she created visible on the network. As a result of this, so now she's created this interface derived from the remote interface she has created, this interface for her bank account object. And so this interface becomes now publicly available for anyone to access. So clients on the network can have access to this interface bank account. Now she instantiates this bank account implementation. . What happens when the bank account implementation is actually instantiated? The location of the bank account implementation object, the instantiation that she has done, is not visible to the client. All that is visible to the client is the interface. The actual location of the object is not something that is visible to the client, and therefore the implementer has to do the heavy lifting of finding a way to make the location of the service visible to the clients on the network. So in the first choice, all that we used was the built-in class and the distributed object model of Java, which is a remote interface to publish the interface for a facility that we created. In this case, the bank account and the methods in it. And once we publish it, that interface is available. Because we derived it from the remote interface, the int, the specific interface, bank account interface, is available for any client on the network. However, when we instantiate the object, the location of the object where that service is available is not something that is readily visible to the client. And so the heavy lifting has to be done by the implementer to make that object that had been instantiated visible on the network. Now a second choice is reusing the remote object class that's available in the Java distributed object model. As before, the developer writes the bank account object providing the methods for deposit, withdrawal and balance and publishes the methods that are in the object using the remote interface. So that, this bank account interface now becomes available for any client that wants to access that object. The reason this interface becomes available to the client is because you're extending this remote interface class that's available in the distributed object model of Java. However, note how the bank account implementation is actually derived. It is derived from the Java built in classes for remote object, and remote server. So, you extend the remote object and the remote server in order to get this bank account implementation object. Now, when you derived your bank account implementation object from the built in distributed object model of Java, namely the remote object and the remote server classes. Now when you instantiate your bank account implementation object, it becomes instantly visible to the network clients. You don't have to do any of the heavy lifting. So once the public methods for the bank account implementation are written by inheriting these built-in classes of Java, all the way starting from the remote interface, the remote object and the remote server. When the bank account implementation object is instantiated the server becomes instantly visible, magically visible to remote clients through the Java runtime system. That's the power of the distributed object model of Java. So the second choice of reusing remote object class in order to derive this implementation results in the heavy lifting being done by the Java magic. So, all of the heavy lifting needed to make this bank account implementation object visible to network clients is being done by the Java runtime. Now that I've given you the difference between the two choices, one using remote interface, and the other deriving your object from the remote object from the remote server. Time for a quiz. Which implementation, would you prefer, would you prefer deriving your service by extending the local implementation or by extending the remote implementation? In the first case when we used the local implementation and used only the remote interface. The implement that makes instances of the object remotely accessible. So the heavy lifting needs to be done by the implementer. That's not quite preferable. One virtue of the local implementation is that the service provider can make selected servers visible to the selected clients. So that's the only virtue that one can associate with the first implementation that uses local. You cannot make a hard case, a strong case for this choice. So this choice where we are extending remote object and the remote server. We are letting Java RMI do all the heavy lifting to make the server object visible to the network lines. And that's the more preferred way of building a network service, and making it available for clients anywhere. So let's see Java RMI at work. On the server side, it's a three-step process to make the server object visible on the network. You instantiate the object. Once you instantiate the object, you create a URL, whatever you want to call the URL. And then you go to the Java run time, and the facility that's available in the Java run time For binding the URL that you created with the instance of the object that you created. The object that you instantiated; it binds it to this URL and it is now in the naming service of the Java runtime system. For clients to be able to discover the existence of this new service that you created and made it available on the network. Now let's look at the client side. Look at the ease with which any arbitrary client on the network can access the server object. What a client will do is, look up the service provider, by contacting a boot strap name server in the Java RMI system. And, it does a look up of the URL and when it does this look up, so this URL is published, once it is published, then it can look up this URL. And when it does this look up of this URL using the facility that's available in the Java RMI system, then a local access point for that object is created on the client side. And so now we've got the access to the object that is at the server through this local name account. And once I have that, then I can do invocations on the methods that is available in this server object by simply calling those methods. I can do a deposit. I can do a withdrawal. I can do a balance check. All of this, they look like normal procedure calls so far as the client is concerned. But each of this is really a call that is going out to the server wherever that server happens to be, and the Java Runtime system knows how to locate that server in order to do this invocation. That's the power of the Java RMI. The client does not know and does not care the location of the server. And if there a failures in any of these function executions, then remote exceptions will be thrown by the server through the Java run-time system back to the client. Of course with the networked nature of this client/server relationship, if a remote expression is thrown and the client sees that the invocation did not succeed, which is a reason the client saw an exception thrown. It may have no way of knowing at what point in the invocation the call actually failed. And this is one of the problems when you have services that you have to reach across the network and you have to handle the exception. Now that we understand the distributed object model of Java and how to build services using the distributed Java object model and publish it and make it accessible to the clients. Lets look at how the RMI is actually implemented? At the core of the RMI implementation is this layer called Remote Reference Layer, RRL. And that's the place where a lot of magic happens. The client site stub, is going to initiate a remote method invocation call using this Remote Reference Layer, and all of the magic with respect to marshaling the arguments. In order to send it over the network and so on is handled entirely by this Remote Reference Layer. And similarly when the result comes back unmarshaling the results into the data structures, that the client understands is once again accomplished using this RRL layer. On the server side. The skeleton that exists is therefore unmarshaling the arguments that comes from the client. And in order to unmarshal the argument, the skeleton uses a Remote Reference Layer, because a Remote Reference Layer knows how to unmarshal the arguments that are coming in. And the the skeleton then makes the call up to the server that is implementing the remote object. Once the server is done with the service, the skeleton marshals the result, and once again goes through the Remote Reference Layer to send it over to the client. And when it comes back, the Remote Reference Layer and the stub work together to deliver the results in a digestible format to the client. So, marshaling and unmarshaling, they're also called serializing and deserialization in the Java world. Marshaling and unmarshaling are also called. Serializing and deserializing Java objects, and all of that is being done by the RRL layer. So basically, the objects that are being passed this arguments, they are serialized by the RRL, and deserialized. On the server end, and given to the server. And similarly, the result, which is also an object, is serialized using the Remote Reference Layer. And when it arrives on the client side, it is deserialized, and delivered as a result object back up to the client. Now where are the clients and the servers? Are they on the same machine, on a different machine? Of course we're talking about networked services, and, so the server's going to be remote. But the server could have several instances. There could be a single instance of a server, or there could be multiple instances of the server and the server may have ability for doing persistence and so on. Where is all that magic happening? Well, similar to the subcontract layer that we discussed in the spring system, the Remote Reference Layer is doing all the magic with respect to. Where the server is? How the server is handling request? Is it replicated? Is it a singleton server? All of these things are being handled through the Remote Reference Layer. So what that means is it allows for various invocation protocols between the client and the servers. And all of those things are buried in the Remote Reference Layer. And the actual clients and the servers can be impervious to those details. So, the Java run time stack, the RRL layer is a very crucial layer. And it has functionalities very similar to the subcontract mechanism and the spring system. And as I mentioned earlier Java RMI derives a lot from the subcontract mechanism. And, so there is not much surprise that there are similarities between the RRL layer and the subcontract mechanism. Having discussed RRL, let's go and talk about, the transport layer. The abstractions that the transport layer provides are endpoint, transport, channel, and connection. And I'll talk about each of these things in a little bit more detail. Endpoint you can think of as nothing but a protection domain or you can say it is a Java virtual machine. And it has a table of remote objects that it can access. And so this gives you a protection domain or a sandbox for execution of a server cord or a client cord can exist within the sandbox. That's what end point. Its basically a protection domain. Connection management, is the interesting piece that is what is about all the details of connecting these end points together. And in particular, the connection management of the transport layer of the Java Runtime system is responsible for setting up connections, tearing down connections, listening for incoming connections, and establishing the connection. And when a connection is established. Between two end points there's a distraction that I mentioned called transport comes into play. So, for instant between this end point, and this end point, thec connection manager decided to have udp transport. So a channel is established between this end point and this end point to do udp transport between these two end points. And so this is the functionality of the UDP transport that is incorporated in this transport layer. And similarly, between this endpoint and this endpoint, the connection manager decided to use a TCP channel, so the transport that is being used here is a TCP connection, both ends. We have a TCB connection at both ends to connecting this endpoint with this endpoint. Notice that a given endpoint, can have different transport for talking to different endpoints depending on a variety of parameters. What is the best way for this endpoint to talk to this endpoint may decide what kind of connection. This end point is going to establish with this end point. That is all part of connection management that is happening in the transport layer of the Java run time. And the connection manager is also responsible for locating the dispatcher for a remote method that is being invoked on this end point. So. A transport is listening on a channel. And when an invocation comes in, then this transport is responsible for identifying, or locating, the dispatcher on this domain, which will know how to carry out that invocation. And the connection managers also responsible for managing the liveness of the connection. Because, if any point goes away, it needs to know that and inform this domain that, oh this particular end point is gone. So, take the appropriate action. So that kind of liveness monitoring is part of connection management. So the last abstraction I mentioned is the notion of connection itself. So once a channel has been established. Then the transport can do IO on this channel using connections. So, the path for the transport layer is in connection management. It listens for an incoming request. When an incoming request comes in. It then establishes a channel, and the channel that is established for communication, which is a mutual agreement between these two endpoints. It choses a transport that is most appropriate for that and once the channel has been established, a connection. Is now made between this endpoint and this endpoint through this channel. And now these two endpoints can do I/O on the channel using the connection. So that's how the transport mechanism of RMI works. As we saw, the transport mechanism sits below the RRL layer. And so it allows all the object invocations to happen through the transport layer. And the RRL layer is the one that is deciding, what are the right transport to use depending on the location of the two endpoints, where the client is and where the server is. Depending on that, it might decide what would be the right transport to use. Whether it should use TCP or UDP and so on. And gives that command to the connection manager which is part of the transport layer of the software stack. So that the established channel can be established and then a connection can be used for actual transport. Of the implementation between the client and server. So in summary, the distributed object model of Java is a powerful vehicle for constructing network services and what we say in this lesson. Is a glimpse of the classes that are available in the distributed object model that makes the life of the developer easy in terms of creating network objects and making it visible for clients to use anywhere. And the power of the RRL layer in dynamically deciding how to make the client-server relationship. Similar to the sub contract mechanism that we saw in spring and we also saw the flexibility in the connection management. Allowing different kinds of transport exist between the client and the server depending on the location of the client and network conditions and so on. There are some more subtle issues involved in the implementation of the RMI system, including distributed garbage collection, dynamic loading of stubs on the client side, sophisticated sandboxing mechanisms on the client and the server sides to ward off security threats and so on. I encourage you to learn about these issues by reading the assigned paper and also surfing the Net. The main point I want to leave you with is that many ideas that start out as pie in the sky research becomes usable technology when the time is ripe. Welcome back! Let's connect the dots. We started with the technical issues in the structure of an operating system for a single CPU, then a parallel machine, then a distributed system. We saw how object technology with it's innate concepts of inheritance and reuse helps in structuring operating systems at different levels. Now, we go one step further. How do we structure the system software for a large scale distributed system service? It's too limiting to call it an operating system. Large scale distributed service. As we continue this lesson, we'll get a glimpse of how object technology has gone ballistic to provide the services that you're reliant on for your everyday internet e-commerce experience. In this lesson, we will describe enterprise java beans and the term java bean is used to signify reusable software components. That is many objects, java objects, in a bundle so that it can be passed around easily from one application to another application for reuse. I'm sure all of us use, routinely, services on the internet such as email through Google or Yahoo. And perhaps purchase things using eBay or make orders for airline reservations and so on. And when we do that, we think of an enterprise that we are accessing from our work station or laptop or personal mobile device. We think of an enterprise as a monolithic entity. But, in fact, if you look inside the enterprise, the enterprise, the intra enterprise view, is pretty complicated. There's a whole bunch of services and servers that are interconnected, there may be marketing division, the sales division, the production division, inventory division, the research division, and so on. All of these constitute what an enterprise is. So internally the view of the enterprise is much more complex than what you see from the outside coming in and using services provided by a particular enterprise. Things get a lot more complicated in this day and age because when we access an enterprise, in fact, the enterprises, they talk to one another. And this is, what is usually called a supply chain model, and so on. Where, the service that you are, requesting may not be serviced by, a single entity but may actually involve the entity that you're contacting, contacting other entities in order to. Put together a solution for a particular request, and what is even more challenging is when enterprises merge. For instance, if three companies merge together, and this happened a while back. There's a company called Digital Equipment Corporation that got bought out by Compaq and those two merged, and later on HP bought out Compaq. And so you can see, that when things like this happen, the inter-enterprise view is much more complex. And now when companies merge like this, the idea of an enterprise, the idea of an enterprise, becomes an amalgam of three different entities coming together, in this example for instance. So the enterprise transformation challenges are many: interoperability of the systems that constitute different enterprises, interface compatibility when such merging happens, system evolution. You know, things are not stagnant, now this transformed enterprise has to continuously evolve as well. Scalability, reliability and the cost of maintaining a complex system like that. All of these things are the challenges that have to be faced both internally and across enterprises. We often refer to services that we're using on an everyday basis, such as airline reservation or Gmail or websurfing, as giant scale services, as opposed to the services that you get within your organization. Example, a file server. There's a later module in this course called Internet Scale Computing, and in that we will discuss programming models and resource management issues for providing such giant scale services. The focus of this point however, is to show how object technology facilitates structuring such services. Let's look at an example to put things in perspective. Let's say you want to purchase a round trip ticket to go from Atlanta to Chennai, India. With a few clicks, you can send your request over to a portal such as Expedia. And Expedia then goes to work for you. It contacts a whole bunch of different airlines, gets the best options that are available from all these different choices, and then it comes back to you with a bunch of options. Now you may take your own sweet time deciding which one of those you may want to pick, based on cost, perhaps convenience, guarantees. For example, you don't want your baggage to end up in Timbuktu. So, based on all of that, you may want to make a decision. Maybe you have to talk to your spouse or significant other, siblings, children, so on, in order to make the final decision. Finally you decide, then commit to buying the ticket, and then Expedia will complete the transaction based on the choice that you made. And you get your ticket and life is good. Well, not so fast. While you are busy procrastinating with your choices, there's another person who is planning almost an exact similar trip to yours. Same dates, same constraints, same destination, and so on. And you can immediately see that, without your realizing, you are actually competing for resources. In this case, a physical resource, a seat on a particular flight going from Atlanta to Chennai, India, with others that you don't even know exist on this planet. Therefore, the service provider, in this case multiple enterprises involved, Expedia and all the airlines together, that are handling your request, they all have to work together to make sure that the result, any resource conflict that might occur between simultaneous requests across space and time coming from several different clients. So all the issues that we've discussed in the context parallel and distributed systems, synchronization, communication, and immediacy of actions, concurrency, all of these become important. And they surface in this very simple example across space and time. And additionally, all services need some common features. For example, a shopping cart on your browser. And in fact, even though this particular example is illustrating an airline reservation, if it comes to booking a ticket on a train, or getting hotel reservation, or booking tickets to go see a game, all of those things have similar requirements. And many of them are probably repeatable. And many of them, such as the shopping cart in this example, are features that might be needed even if the services that you're talking about are completely different, such as an airline reservation and hotel booking. So since the same issues crop up in the implementation of each new service, we don't want to reinvent the wheel every time. This is where object technology comes in, the power of reuse of components. Now such applications are what are called N-tier applications because if you look at the software stack that comprises an application such as this, you'll see several different layers. You have a presentation layer. And the presentation layer is the one that is responsible for painting the screen on your browser. Perhaps dynamically generating the page based on the request you made. And there may be application logic that corresponds to what the service is providing. And there is business logic that corresponds to the way airfares are decided, seats are allocated, all these kinds of things. And there's a database layer that accesses the database that contains information about all the things that the application and the business logic have to decide on in order to satisfy a particular request. And all these different layers have to worry about many of the issues that we're already familiar with, in the context of writing, parallel programs and distributive programs, and those include persistence for actions. For example, let's say I made a choice, but I haven't completed the booking. I may go away and come back later on, in order to complete that booking. So persistence is something that I might need. You need a notion of transaction because I have initiated a particular operation and I have not completed it and so transaction properties may be needed in order to make sure that a reservation that started Is finally complete and I have made the booking. Caching of data that you pulled in from a database server so that you can access the database more quickly. Clustering, which corresponds to taking a set of related services and clustering it together in order to improve the performance of the service. And similarly clustering the data that you're accessing from a data base server. All of these are issues, and of course one of the things that we worry about a lot this days in Ecommerce is security, in particular when we are communicating financial information, credit card information, and personal information like social security Id's and so on We worry a lot about services provided by the server and that my personal information is not compromised in any fashion. So these are all the sets of issues that N-tier Applications have to worry about in making sure that the services it provides are trustworthy from an end user's point of view. So, how do we structure an N-tier application like this? The things that we want to reduce is reduce the amount of network communication because that results in latency, reduce security risks for the users which means that the business logic should not be compromised. And increase the concurrency for handling an individual request. For instance, there's an individual request, but in processing this request, there's an opportunity to exploit parallelism. Often times, these are called embarrassingly parallel applications because even though this request seems like a single request, there's an opportunity to exploit parallelism. And the kind of parallelism is embarrassingly parallel because the same query. I want to find out the availability of seats on a particular date. I don't care about which airline I go by. That's an opportunity for parallelism for the expedia server, to go in parrallel to multiple airlines and find out the availability of seats on the dates that I requested. Similarly, there's opportunity for exploiting concurrency across several simultaneous requests that are coming in. And also for clustering the computation that may have to be done on the server for computations that are common across simultaneously arriving requests. We want to reuse components aggressively. By components we mean portions of the application logic that can be reused in constructing these applications as well as in the execution of the components in order to service the requests that are coming in simultaneously from several different clients. To structure the N-tier applications we're going to talk about one particular framework as an example. It's just as an example, there are other frameworks that provide similar functionality to the JEE framework which is called the Java Enterprise Edition framework. And in the JEE framework there are four containers for constructing an application service. And containers you can think of as protection domains implemented typically in a Java virtual machine. In the JEE version of, of building N-tier applications there are four containers there are the client contain and there is an applet container for the client, which will reside typically on a web server. And this is the one that - Interacts with the browser on the end client. And the presentation logic that I mentioned to you earlier is provided in a container, which is called a web container, and this is the guy that is responsible for dynamically perhaps creating the pages that have to be sent by the web server back to the browser of the client. And there is an EJB container, which is the one that manages the business logic that corresponds to what needs to be done in order to carry out the request that is come in from the end client. And there may be a database server, that the business logic is communicating with in order to Get access to the data that it needs to process the request that came in. So these are the four containers: the client container, the applet container, the web container and the EJB container that are, the containers that are available for packaging the objects that constitute. The entire application for providing a particular service, for example, airline reservation service or a hotel reservation service. The key hope is that we want to exploit as much as possible, reuse components, and for this purpose continuing sort of the coffee analogy starting with Java. The word bean is used to indicate a unit of reels, that is, a bundle of Java objects providing a specific functionality. For example, there may be a bean that provides the shopping cart function. So that becomes a unit that reuse in constructing an N-tier application. The containers that I talked about here host the beans. That is, a container allows you to package a whole bunch of Java beans and make it available in this container, in the JEE framework. There are four types of beans, one type of bean is called an entity bean. For instance an entity bean maybe a row of a database. If you think about the database holding employee records for instance, one drawer of the database may correspond to all the employees whose last names start with the alphabet," a". And typically entity beans are persistent objects with primary keys. And the persistence for the entity bean may be built into the bean itself and that is what is called bean management persistence, or it may be built into the container into which that entity bean is instantiated. In either case Since we are dealing with objects that may need persistence, it is important that the persistance for that object is handled somewhere either in the entity itself or the container into which that entity is being hosted. The second type bean is what is called a session bean. And the session bean typically is associated with a particular client. And a particular session, meaning a temporal window over which a client is interacting with the service. That is what a session bean may be holding. And a session bean could be a stateful session bean, meaning, for instance, I am ordering, let's say, a computer, by contacting a portal for Del. In that case, the session that I'm establishing with the Del portal, that session has to be stateful, because it has to remember what choices I'm making. I may actually keep those choices alive, go away for a while, come back the next day, and continue with my purchase. So, sessions could be stateful. There could also be stateless sessions. For instance if I start an email session using my browser with Google using Gmail, that session maybe stateless. I go away, everything that I did during that session can be thrown away because I'm going to start a brand new session when I re initiate a connection with a G mail server. So in that sense, a session bean could be a state-full session bean or a stateless session bean. And the third type of bean is what is called a message-driven bean. This kind of bean is useful for asynchronous behavior. For instance, I might be interested in getting the stock quotes, I might have a stock quote ticker on my browser and I might want to get updates on the movement of stocks of a particular company that I'm interested in. And that would be something that is accomplished using a message bean. Which is having this asyncrhonous behavior. Stock ticker is an example, news feed is an example, RSS feeds that you typically are using, these days, are examples of message driven beans. And the finer we make these beans, each of these beans is denoting a functionality, but if you can have fine grained Versions of these beans. That gives a greater opportunity to enhance a concurrency for dealing with an individual request that's coming into your application's server, or there could be concurrent requests. In addition to my own request, there could be parallel requests coming in. All of those can be dealt with more concurrently if we implement these beans at a finer level of granularity. But if you implement the bean at a finer level of granularity that means that the business logic is also getting more complex. So, there is always this trade off in structuring end clear applications that we made have a complex business logic In order to support finer grain concurrency or I might choose to keep the business logic very simple and use [UNKNOWN] beans. And this is where the core of what we are going to discuss lies, and that is. We are going to discuss. Different design alternatives for structuring such entity application servers. So the first design alternative that we're going to look at is using coarse grain session beans. In this structure that I'm showing you, we're only looking at the web container. The EJB container because, the applet container that interfaces with the client, is in the web browser. So we don't worry about that. We'll talk only about how we structure the web container and the EJB container in the different design alternatives. So the web container contains the presentation logic and in the structure that I'm showing you a servelet corresponds to an individual session with a particular client. So, this box represents a particular client. This box represents a second client. And there's a presentation logic commensurate with servelet one, that is client number one. And similarly, presentation logic commensurate with servelet two, which is client number two. And there's a coarse grain session bean that is associated with each of these servelet. And in turn the, session bean corresponds to the client that is being served through the servelet number one. And similarly, servelet number two is served by this session bean. And as the name suggests, the session bean is responsible for the specific needs of the particular client that it is serving for this particular session. Therefore, the session bean will worry about the data accesses that are needed to the database in order for the Business Logic to do its thing. So if, let's say, we're doing an airline reservation system and if this is requesting a particular booking, then the session bean is going to be the one that contacts the database server in order to pull the specific dates and airline reservation. Information that is needed for the business logic to do the pruning and selection commensurate with whatever this particular client is requesting. And there are multiple sessions that are contained in this EJB container, depending on the number of clients that have simultaneously temporally made requests to this particular service. So the EJB container has to provide some service for all the sessions that are concurrently going on in this server. All of the data accesses that are needed for a particular session is taken care of by the session bean. And therefore, the amount of help that we need from the EJB container, in terms of services, is pretty minimal for supporting this particular model, and, and in fact, it is confined to any conflicts that might arise in terms of external accesses for satisfying the request of these different session beans. So the EJB container service that would be needed is primarily for coordinating, if any, across concurrent independent sessions. An example would be, if they want to access the same portion of the database for writing some records. In that case, they may need some coordination help from the EJB container service. The other important attribute of this structure is that the business logic is confined to the corporate network. It is not exposed to the outside world because it's not contained in the web container, it is contained in the EJB container and therefore the business logic is not exposed beyond the corporate network. That's a good thing. So, the pros of this particular structure is that you need minimal container services and also that the business logic is not exposed to the outside world. But the cons for this particular structure is, this application structure is very akin to a monolithic kernel that we've talked about a lot. There is very limited concurrency for accessing different parts of the database. For instance, I mentioned that the services provided by these giant scale services tend to be embarrassingly parallel. So there is a lots of opportunities for pulling in data, example would be let's say, that the particular query is to compile demographic distribution of all the employees in the company. In that case, there's an opportunity to pull in lots of data simultaneously from the database. But unfortunately, the structure doesn't allow you to exploit such concurrency. So in other words, this core screen session been structure represents a lost opportunity. For accessing and pulling lots of data from the database in parallel for satisfying either the same request or even concurrent requests that may be accessing the same portions of the same database. The second design alternative I'm going to talk about mitigates exactly the problem that I had mentioned earlier. And that is, you want parallelism for accessing the database because, this is probably one of the slowest link in the whole processing of request because pulling in data from the database is going to take a lot of time. Both in terms of I/O that has to be done through discs as well as the network communication to pull in the data into the container where the processing needs to happen. And for that purpose the structure that we have here is to push the business logic as part of this container in which the servelet and the presentation logic was there we add the business logic also and make it a three tier software structure here. Servelet presentation logic and business logic, and all of the data access is going to be done through what are called entity beans. As I mentioned earlier, entity beans have persistence characteristics, and in this particular example i can think of the entity bean as representing one row of the database. So the data access object are implemented using a whole bunch of entity beans and you can decide as the designer whether an entity bean is responsible for one row of the database or maybe for a set of rows of the database. But in any event what we've done is, we've taken the parallelism that is available in terms of data access to the database and encoded it through the entity bean so that they can be parallel access to the unit of granularity that we have in terms data base access. So the EJB container now contains these entity beans. So now if a servelet that is serving a particular client needs to access some portion of the database, it can form more parallel requests to these entity beans, to as many entity beans as it wants. And all of those entity beans can work in parallel on behalf of a single client. And pull in the data that is needed and serve it up to the business logic so that the business logic can then do its thing. So we are reducing the time for data access by having this parallel structure and exploiting the available concurrency that may be there in terms of I/O performance, and also even if there are parallel requests, those parallel requests may want access to the same portion of the database. If you think about the example I gave you of two difference individuals wanting to make airline reservation for exactly the same dates and the same set of constraints. Then there may be an opportunity for this entity bean to cluster the request coming from several different end clients and amortize the access to the database server. Across several different clients that are temporally happening at the same time. And i mentioned entity beams usually are dealing with persistent state, which means that the persistence has to be provided at some level to these entity beans. So that persistence has to be provided to the data accessed object at some level, which are using these entity beans. It could be done at the level of individual entity beans. Which is called the bean managed persistence. If the bean is managing the persistence needs of the data access object, then that is called bean managed persistence. Or it could be that the container is providing that facility, in which case the persistence needs of the data access object is provided by the container and, and that is called container managed persistence. So these are two different design choices we can make in this structure. The structure is the same for the design alternate of two. That is, we're using entity beings to implement data access objects, and we're deciding the granularity of the data access object, based on the level of concurrency that you want in constructing this application service. But within that choice there are two possibilities again in terms of how we provide persistence for the data access object, either by providing it in the entity bean itself, or using the container service to provide that. So this, once again, points to opportunities for reuse of facilities that may be available. The same container-managed persistence may be usable for different types of applications. One application may be an airline application, another application may be a portal for hotel booking. All of those different applications may be able to reuse container-managed persistence that's available in the structure. So the pros of this structure is first of all, the concurrency that you can actually exploit concurrency for data access for the same client in parallel or even across different clients by amortizing the data access. That may be needed concurrently for similar services that are overlapping in terms of data usage. That's the good news. There's one con to this approach, and that is, because we moved the business logic into the web container from the EJB container, it exposes the business logic to the outside network. We are not confined to the corporate network but the business logic is exposed outside the corporate network in this design alternative. So, all the data access code that you used to be in the session beam in the previous structure that I talked about. All the data access code is now moved into this entity beam. That's how we get the parallelism in the fact that there are multiple entity beams that are carrying the same data access code and they could be accessing different portions of the database concurrently resulting in exploitation of parallelism and reusing the latency for the business logic to get all the data it needs from the data base in order to do its work. The second design alternative gave concurrency but at the cost of exposing the business logic, and the third design alternative that I'm going to explain to you, is going to correct that, it's using session bean with entity bean. The idea is that, we're going to associate it with each client session, a session facade, it's a design pattern that allows you to construct a session and associate it with a particular client. So, for instance, in this case, this session facade corresponds to servelet 1, which corresponds to the client that it is serving similarly this session facade, serving client number 2. And as in the first design, what you see is that the web container contains only the servelet and the presentation logic that is associated with that particular servlet. Now the business logic, is moved back into the EJB container, and it sits with the session facade. And we still have the data access objects implemented using the entity beam concept that I mentioned in the second design alternative. So, the session facade is worrying about all the data access needs of its associated business logic. Similarly, this session facade is worrying about all the data access needs of this business logic, and what the session facade is going to do is, it's going to form out the data access requests corresponding to the business logic associated with this session. So, there's an opportunity, again, to exploit parallelism because you can form out parallel requests to multiple entity beans. That are handling the data access to different portions of the database and similar to the second, design alternative, we're going to structure this database to be at whatever level of granularity that we think is the right one. So this entity bean may be responsible for an individual role. Or a cluster of rows, and so on. And that way, we can have the granularity that we want for parallel access so that the business logic can be served in parallel, and at the same time, we have moved the business logic back into the EJB container. So, the business logic is not exposed outside the corporate network. And we have a couple of choices of how we want to structure the session bean with the enity bean. Now the whip container is going to use RMI. A remote interface in the distributed object framework of Java. In order to communicate with the business logic, and the session facade is going to communicate with the entity bean either using RMI. In that case, the interaction between the session facade and the entity bean is very similar to the interaction between the servlet and the session facade. That is, it'll use RMI. Or, we can choose to construct the interface between the session facade and the entity bean, using local interfaces. And the reason why we may want to choose one or the other, using the RMI allows us to, sort of, keep this entity bean wherever we want in the network. On the other hand, if we chose the local option, what we're seeing is that we will co locate the entity beans in the same EJB container as the business logic and the session facade. The advantage of doing that, is that because it is local, we don't have to incur network communication, in order to go and fetch the data from the entity bean. The entity bean, of course, has to fetch it from the database servers, but once it fetches it from the database servers, it doesn't have to go through the network in order to give it to the business logic if you're using local interfaces. So that's another bifurcation within this design alternative. We can choose to construct this portion of the application using remote interface or local interface. And the prose of this structure is in some sense getting the best of both worlds. We're not exposing the business logic. Which was the virtue of the first design alternative, and we're also getting concurrency through the data access object encoded as entity beans. So you get the concurrency and the fact that the business logic is within the corporate network. Both of those good features are available in this design alternative. Now is there a con well, we are incurring additional network access in order to do the servers that we want for the data access, and that can be mitigated by call locating, the entity bean, and the session facade in the same EJB container. So that's a choice that you can make. So these are the 3 design alternatives that we talked about. One is a coarse grain session bean alternative. The second is a finer grained data access object alternative, and the third is sort of putting together the first two. Putting the session bean as a facade. To actually access the data access objects, which are encoded in entity bean so that you can get concurrency. Notice that in talking about these different design alternatives, we're only talking about how to break up that application, the logic of that application, which consists of presentation to the client, business logic that has to do with decisions that this enterprise has to make in order to service this request, and database access in order to get the data that is needed in order to do the work that is involved. In serving up the needs of a particular client. Lots of things that are needed in addition to writing about the application logic itself. And those are things like security, persistence, and so on. The power of object technology is the fact that So that's the thing that I wanted to start with and I wanted to give you the different design alternatives that exist in structuring these complex services using the object technology to reuse components instructed in structuring complex applications. So this lesson, showed the power, of the object technology, for structuring complex application servers. EJB, allowed developers to write business logic, without having to worry about crosscutting, cancer such as security, logging, persistence and so on. As a homework, what I would want you to do, is understand the design choices that we discussed in the lesson and analyze [UNKNOWN] the performance implications, of those design choices, with respect to concurrency, pooling of resources, number of object classes, lines of code and so on. And read, the paper that I've assigned to you, as part of the reading material for this course, in this topic, and relate the arguments they present in the paper, against your own qualitative analysis. A word of caution though, EJB has evolved, considerably, from the time of this paper. But, the principals that are discussed in this paper, apply, to the way complex entier applications are structured to this day. Welcome back. We started the discussion of the distributive systems with definitions and principles. We then saw how object technology, with its innate concepts of inheritance and reuse, helps in structuring distributive services at different levels. In this next lesson module, we will discuss the design and implementation of some select distributed system services. Technological innovation come when one looks beyond the obvious and immediate horizon. Often this happens in academia, because academicians are not bound by market pressures or compliance to existing product lines. You'll see this out of the box thinking in the three subsystems that we're going to discuss as part of this lesson module. Another important point, often the byproducts of a thought experiment may have more lasting impact than the original vision behind the thought experiment. History is ripe with many examples. We will not have the ubiquitous yellow sticky note, but for space exploration. Now close to home, for this course, Java would not have happened but for the failed video-on-demand trials of the 90s. In this sense the services we are going to discuss as part of this lesson module, while interesting in their own right, may themselves be not as important as the technololgical insights they offer on how to construct complex distributed services. Such technological insights are the reusable products of most thought experiments. There's a common theme in the three distributed subsystems we're going to discuss. Memory is a critical, precious resource. With advances in network technology leveraging the idle memory of a peer in a Local Area Network may help overcome shortage of memory, locally available in a particular note. The question is how best to use the cluster memory, that is the physical memory of peers, tend to be idle at any particular point of time in a local area network. Global memory system, GMS, asks the question, how can we use the peer memory for paging across the Local Area Network. And later on we will see DSM, which stands for distributed shared memory. Asks a question, if shared memory makes the life of application programmers simple in a multiprocessor, can we try to provide the same abstraction in a cluster and make that cluster appear like a shared memory machine. And a third work that we will see is this distributed file system, in which we ask the question, can we use the cluster memory for cooperative caching of files. Anyhow, back to our first part of this three-part lesson module, namely the Global Memory System. Typically, the virtual address space of a process running on a processor, let's say your desktop, your PC, your laptop, and so on, is much larger than the physical memory that is allocated for a particular process. So the role of the virtual memory manager in the operating system is to give the illusion to the process that all of its virtual address space is contained in physical memory. But, in fact, only a portion of the virtual address space is really in the physical memory. And that's called the working set of the process. And the virtual memory manager supports this illusion for the process by paging in and out from the disk the pages that are being accessed by a process at any particular point of time so that the process is happy in terns of having its working set contained in the physical memory. Now when a node, a desktop, is connected on a local area network to other peer nodes that are also on the same local area network it is conceivable that at any point of time, the memory pressure, meaning the amount of memory that is required to keep all the processes running on this node happy, may be different from the memory pressure on another nodes. In other words this particular node may have a much higher memory pressure because the work load on this load consumes a lot of the physical memory whereas this work station may be idle and therefore all of this memory is not being utilized at this point for running anything useful because no applications are running on these nodes. So this opens up a new line of thought. Given that all these nodes are connected on a local area network and some nodes may be busy while other nodes may be idle. Is it possible if a particular node experiences memory pressure, can we use the idle cluster memory, and in particular, can we use the cluster memory that's available for paging in and out the working set of the processes on this node? Rather than going to the disk, can we page in and out to the cluster memories that are idle at this point of time? It turns out that with advances in local area networking gear, it's already made possible for gigabit ethernet connectivity to be available for desktops. And pretty soon, 10 gigabit links are going to be common in connecting desktops to the local area network. This makes sending a page to a remote memory or fetching a page from a remote memory faster than sending it to a local disk. Typically, the local disk access speeds are in the order of 200 megabytes per second in terms of transfer rate, but on top of that, you have to add things like seek latency and rotation latency for accessing the data that you want from the disk. So all of this augers well for saying that perhaps paging in and out through the local area network to peer memories that are idle may be a good solution when we have memory pressure being experienced at any particular node in the cluster. So the global memory system, or GMS for short, uses cluster memory for paging across the network. So in other words in normal memory management, if virtual address to physical address translation fails then the memory manager knows that it can find this virtual address on the disk, meaning the page that contains this virtual address on the disk, it pages in that particular page from the disk. Now in GMS, what we're going to do is, if there is a page fault, that is, this translation, virtual address to physical address, fails then that means that the page is not in the physical memory of this node. In that case GMS is going to say, well it might be there in the cluster memory, in one of the nodes of my peers, or it could be on the disk if it is not in the cluster memory of my peers. So that's the idea that GMS is sort of integrating the cluster memory into this memory hierarchy. Normally, when we think about memory hierarchy in a computer system, we say there's a processor, there's the caches, and there's the main memory, and then there is the virtual memory sitting on the disk. But now in GMS, we're sort of extending that by saying in addition to the normal memory hierarchy that exists in any computer system, that is processor, caches and memory, there is also the cluster memory. And only if it is not in the cluster memory, we have to think of going to the disk in order to get the page that we're looking for. That's sort of the idea. So in other words, GMS trades network communication for disk I/O. And we are doing this only for reads, for reading across a network. GMS does not get in the way of writing to the disk, that is the disk always has copy of all the pages. The only pages that can be in the cluster memories are pages that have been paged out that are not dirty. And if there are dirty copies of pages, they are to be written onto the disk just like it will happen in a regular computer system. In other words, GMS does not add any new causes for worrying about failures because even if a node crashes all that you lose is clean copies of pages belonging to a particular process that may have been in the memory of this node. But those pages are on the disk as well. So the disk always has all the copies of the pages. It is just that the remote memories of the cluster is serving as yet another level in the memory hierarchy. So just to recap the top level idea of GMS. Normally, in a computer system if a process page faults that means that the page it is looking for is not in physical memory, it'll go to the disk to get it. But in GMS, what we're going to do is if a process page faults, we know that it is not in physical memory but it could be in one of the peer memories as well. So GMS is a way of locating the faulting page from one of the peer memories, if it is in fact contained in one of the peer memories. If not, it's going to be found on the disk. So that's the idea behind that. So when the virtual memory manager, the virtual memory manager that is part of GMS, decides to evict a page from physical memory to make room for the current working set of processes running on this node, then what the virtual memory manager is going to do is, instead of swapping it out to the disk, it is going to go out on the network and find a peer memory that's idle, and put that page in the peer memory so that later on if that page is needed, it can fetch it back from the peer and memory. That's sort of the big picture of how the global memory system works. So let's introduce some basic terminologies. In GSM, when we talk about cache, what we mean is physical memory. We're not talking about the processor caches. We're talking about physical memory that is the dynamic random access memory or DRAM for short. Which is the physical memory. That's what we mean when we use the term cache. And there is a sense of community to handle page faults of a particular node. So we'll get to that. In a minute. And I mentioned that we are going to use peer memories as a supplement for the disc. In other words, we can imagine that the physical memory at every node to be split into two parts. One part is what we'll call local and local contains the working set of the currently executing processes at this node. So this is the stuff that this node needs in order to keep all the processes running on this node happy. Now, the global is similar to a disk. This global part is where community service comes in, that is, I'm saying that out of my total physical memory, this is the part that I need to keep all the processes happy in my node, and this is the part that I'm willing to use a space for holding pages that are swapped out from my fellow citizens on the local data network. And this split of local and global is dynamic in response to memory pressure. As I mentioned earlier, the memory pressure is not something that stays constant, right? So over time, depending on what's going on in a particular node, you may have more need for holding the working set of all your processes. In which case the local part may keep increasing. On the other hand, if I go off for lunch, my workstation is not in use. And in that case, my local part is going to shrink, and I can house more of my peers swapped out pages in my global part of the physical memory. So, the global part is a spare memory that I'm making available for my peers And local part is the part that I need for holding the working set of the currently active process at my node, and this boundary keeps shifting depending on what's going on at my node. Pretty simple, normally if all the processes executing in the entire local area network are independent of one another, all the pages are private. You know, I'm running a process, my process, my pages, and the contents of that pages are private to my process. On the other hand, you could also be using the cluster for running an application that spans multiple nodes of the cluster, in which case it is possible that a particular page is shared, and in that case, that page will be in the local part of multiple peers, because multiple peers are actively using a page. So, we have two states for a particular page. It could be private, or it could be shared. If a page is in the global part of my physical memory, then it is guaranteed to be private. Because the global part is nothing different from a disk. So when I swap out something, I throw it onto the disk. Similarly, when I swap out something in GMS, I throw it into my. Peer memory is the global cache, and therefore what is in the global cache is always going to be private copies of pages, whereas what is in the local part can be private or can be shared, depending on whether that particular page is being actively shared by more than one node at a time. Now one important point, the whole idea of GSM is to serve as a paging facility. In other words, if you think about even a uni processor, if it had multiple processes, sharing a page, the virtual memory manager has no concern about the coherence about the pages that are being shared by multiple processes, that's the same Semantic that is used in GSM also, and that is coherence. For shared pages, it's outside GSM, it's an application problem. If there are multiple copies of the pages residing in the local parts of multiple peers, maintaining the coherence is the concern. of the application. That is not, that is not GSM's problem. Only thing that the GSM is providing is a service for remote at. That's important distinction that one has to keep in mind. In any workshop memory management system, what you do is when you have to throw out a physical page. When you have to throw out a page from physical memory. You use an algorithm, a page replacement algorithm and the page replacement algorithm that is typically employed in computer systems is some variant of an LRU or a least recently used algorithm. GSM also does exactly the same thing, except it integrates cluster memory management of the lowest level across the entire cluster. So the goal in picking a replacement candidate in GSM is to pick the globally oldest place for replacement. If, let's say that the memory pressure in the, system is such that I have to throw out some page from the cluster memory onto the disk. The candidate page that I'll choose is the one that is oldest in the entire cluster. So managing age information is one of the key technical contributions of GSM, how to manage the age information so that we pick. A globally oldest page for replacement in the community service for handling page faults. Before we dive into the nuts and bolts of how GMS is implemented, how it is architected, let's first understand at the high level what's going on in terms of page fault handling now that we have this community service underlying GMS. In this picture, I am showing you two hosts, host P and host Q and you can see that the physical memory on this host is divided into the local and the global part. Similarly, the physical memory on host Q is divided into the local part and the global part. And we already mentioned that these are not fixed in size but the size actually fluctuates depending on the activity and that's what we are going to illustrate through a example situations. So the most common case is that I am running a process on P and that page faults on some page X. When that happens, you have to find out if this page X is in the global cache of some node in the cluster. Lets say that this page happens to be in the global cache of node Q. So what will happen in order to handle this page fault is the GMS system will locate, oh this particular page it's on host Q. So it'll go to host Q. And the host Q will then send the page X over to node P and clearly, if there there was a page fault that means that the memory pressure on host P is increasing and therefore, it is going to add X to it's current working set. That is it's local allocation of the physical memory is going to go up by one but it cannot go up by one without getting rid of something here. Because the sum of the local and global is the total amount of physical memory available in this node and therefore, what P is going to do is, pick the oldest page that's available in the global part and send it over to node Q. So, in other words, what we are doing so far, as host Q is concerned is saying, well X happens to be currently in the working set, then resend it to host P. And host P says, well, my working set is increasing. Therefore, I have to shrink my community service and we going to reduce the global part by one. Pick the oldest page. Lets say it's Y. Send it over to host Q. So that the host Q can host this new page Y in the global cache on this node. The key take away for you is that, for this particular common case, the memory pressure pressure on p is increasing, so the local allocation of the physical memory goes up by one, and the global allocation, the community service part goes down by one on host P. Where as on host Q, it remains unchanged because all that we have done is we have traded Y for X. The second case is simliar to the common case, but the memory pressure on P is exacerbated in this case. In this case, there's so much memory pressure that all of the physical memory that's available at P is being consumed by the working set, while hosting all the applications running on host P. There's no community service happening on host P. And now, if there is another page fault, for some new page on P, then there is no other option on host P except to throw out some page from its current working set in order to make room for this missing page. So, we're going to do exactly similar to what we did in the previous case, with the only difference that the candidate that is being chosen as a victim, or the replacement candidate, which is called a victim, in the management of virtual memory system, is coming from the local part of host P itself. So we get the missing page and we send out the oldest page from local part of host P, recognizing that the local part is zero right now. So in this case you can see that there is no change in the distribution of local and global on P, because global is already zero, it's not going to be anything less than that. So the distribution remains unchanged. And as in the previous case, there's no change on host Q as well in terms of the split between local and global. The third case is where the faulting page is not available in the cluster memories at all. In other words, the only copy exists on the disk. So in this case, what has to happen is, when we have the page fault on node P for this x, we have to go to the disk to fetch it. And we're going to fetch it, which means that the working set on node P is growing, similar to the first case. And so the local part is going to go up by one. In order to make room for that, I have to necessarily shrink the global part as in the first case. So, I am going to shrink that global part by 1. By the way, I can pick any page from the global part, and both in the first case, as well as in this case, we can pick any page from the global part and send it out to a peer memory. And that's what we're doing here, so we are saying, here is a page that I have to get rid of. Who do I send it to? Well, I am going to send it to the guy that happens to have the globally oldest page in the entire cluster. So let's say there is some host R that has the globally oldest page in the entire cluster, and that globally oldest page in the host R could be on the local part or the global part of this host. So what we're going to do is, we're going to tell that guy, hey, I'm going to give you a page to hold, because this used to be in my global part, I don't have room anymore, because my local is increasing by one because of this page fault, and adding x to my working set now. So please hold on to this page that I'm going to give you, in your global cache. Now this guy has a split like this. So if it has to make room for this new page that is coming in from its peer, clearly it has to get rid of something. Where it will get rid of? Well, it has to throw it on the disk. Now, the interesting part is, if the oldest page on host R happens to be in the global cache of R, what can we say about that page z? Well, it has to be cleaned, because global part is nothing but a paging device. And therefore if it is here, it must be cleaned, and therefore, I can discard it without worrying about it. Just simply dump it. Drop it on the floor [LAUGH]. That's what I'm going to do. On the other hand, if it is on the local part, it could be dirty. That is, if the oldest page happens to be on host R, and it also happens to be in the local part of host R, it is conceivable that this page has been modified, in which case, that modified copy has to be written out to the disk. So in other words, when we pick host R to send this replacement page from my host, this guy, what he's going to do is, he's going to say, I have to get rid of a page in order to make room for this incoming global page, because i have the globally oldest page. And if I have the globally oldest page, then let me get rid of it by throwing it out onto the disk if it happens to be dirty. If it happens to be clean, simply drop it on the floor, because I know that all the pages are contained on the disk. That's the fundamental assumption we started with, that all the pages of the currently active processes are on the disk. It is just that the global cache of every node is acting as a surrogate for the disk, because it can be faster to access from the peer memory than from the disk. So similar to the first case, in this case also, the local portion of the physical memory allocation on host P is going to go up by one, and the global portion is going to go down by one. What about host R, well it really depends. If the oldest page on host R happens to be in the global cache, then there is no change, because I am reading the old page z for another page that is coming in from host P, that is y that is coming in. In that case there is no change in the allocation between local and global on host R. But on the other hand, if the globally oldest page happens to be from the local part of host R, what that means is that even though originally we thought z to be part of the working set of host R, clearly, the processes that were using it are either no longer active, or they're completed, or whatever, and therefore, we're throwing out this page. The local part is shrinking. If the local part on host R is shrinking, what that means is, I can use more of the memory that's available in host R for community service. That's the important message I should get out. That's the important message I want you to get out of looking at this particular page fault scenario. So far the cases that we considered are cases in which a page that we are faulting on is private to a process. So let's consider the case where a page is actively shared. That means that two hosts P and Q, and both of them are actively sharing a page X, and currently some processes on host Q, is using that page X so, is in the local part in the working set portion of host Q. And host P, the process is page faulting on the same page X. So when a page faults, GMS says, oh, let me find out what it is. Well, it finds that x is in the local part of host Q and because it is in the current working set of host Q. We want to leave it there. We don't want to yank it away from there. We want to leave it there. We want to make a copy of this page into the local part of host p so that the faulting process on host p will have access to the page x. Again, we have the same situation that the working set on host p is increasing. The local part has to allocate one more page. In physical memory for the active working set for the processes on B, so the global part has to shrink by one. So, this goes up by one, this goes down by one. Pick again some arbitrary page from the global part. It doesn't have to be the LRE page, because all, all that your going to do is you're going to put this into the peer memory somewhere. So in that sense, what we are doing is we're going to say well, take this y and host in some of the peer cache. So what are we, who are we going to send it to? Well, we're going to send it to the host R that happens to have. The globally oldest page. Remember that in this situation, the total memory pressure of the entire cluster is going up by one, not just this guy, because x is going to be still present in the working set of host Q, and x is also going to be present in the working set of host P. Which means the total memory impression in the entire cluster is going up by 1. And that is the case, then I have to necessarily pick a page from the entire cluster and throw it all under the desk, and that's what we're doing here. What we are saying is that host R happens to have the globally host stage, and this page that I am replacing from host P in order to accommodate The missing page and bring it into my local cache on host P is to throw out why over to host R and host R in order to make room for this guy in it's global cache has to necesarily pick a victim from it's physical memory and the victim it picks is going to be The LRU candidate, and that could come from the Local part or from the Global part. Again, if it comes from the Global part, we know it's private and we throw it away. if it comes from the Local part, it could be that this is a dirty copy, in which case we have to write all to the disc, or drop it on the floor if it is a clean copy because the disc always has. All the copies of the pages. In either case the important message is that if the LRU candidate that we're going to throw out onto the disc our drop on the floor comes from the local part of R. That means similar to the previous case, the working set on local R is shrinking. Is coming down by one. Which means are, host are, can do more community service in the future. And now the x is present in the working set of both host Q and host P which means actively some processor on host P, some process on host Q are accessing this page X. I mentioned earlier that this is not something that GSM worries about. If coherence is to be maintained for the copies of the same page existing on multiple nodes, that's not in the domain of GSM because GSM is simply a paging device. So it doesn't worry about coherence maintenance. There has to be something that the application or some higher level of the software has to worry about. So we can talk about the relative split between local and global. In this particular case, well, in this case host P local goes up by one, global goes down by one. In host Q Nothing changes because there is an actively shared page which may relieve the copy here, just give a copy over to P, so the balance between local and global doesn't change. On the other hand in host R if the replacement candidate came out of The global part, then then there is no change in the split between local and global and host R either. On the other hand, if the replacement candidate came from the local cache of host R, what that means is that the working set, memory pressure on host R is decreasing. And therefore L will go down by one and G will go up by one. I want you to think through all these four cases very carefully. The first three cases that we looked at were cases in which the pages were all private. And this is the only case where the page is potentially shared because it is in the [INAUDIBLE] It is in the local cache of Host Q and in the local cache of Host P as well, and now it is time for a quiz. So the question that I'm posing to you is. I'm giving you these four page faulting scenarios. And I want you to fill in the table with what happens to the boundary between the local and the global split on each node. And this column is giving you the location of the faulting page. Where it is in the entire system. And this column is saying what happens to the split of L and G on the faulting node. What happens to the split of L and G on the node that happens to have the faulting page? And what happens to the split between L and G for the node that has the globally oldest page? The LRU page in the entire cluster which shall be the victim of place replacement, throwing it out on to the disc. If some page has to be gotten out of the cluster because the memory pressure in the entire cluster exceeds the total memory that's available in the entire cluster. The first choice is the faulting page X, is in Q's global cache. That's a first choice. In that case I want you to think about what happens to the spread of L and G on node P, node Q, and node R. In the second case, faulting page X is in the global part of Q. And the difference is, P's global is already empty. In the first case, P's global was not empty, so I wanted you to think about the split between L and G, and note P as well. In the second case, I'm saying it is similar to the first one, with a difference that P's global cache is empty. And the third case is the faulting page is on the disk. It is not in the cluster memory of any of the nodes. And the last case is when this faulting page X is actively shared by both P and Q. All of theses cases are the ones that we just we discussed, and so I just wanted you to think about it and fill in this table. I'm sure your memory recall capability is very good and you would have come up with the right answers for all of these entries. Let me just quickly summarize what I'm sure that most of you have gotten already. In all cases except this, where the global part of the faulting node Ps cache is empty, the local part of course is going to go up by one, the global part is going to come down by one. Only when the global part is empty already, there's no change. And, in both cases, there's no change to the balance between L and G on both the node Q, which is supplying the page X. And the node R that happens to have the LRU page, because it is not even part of the action, in terms of the page fault handling, for these two cases. Now the third case, is where it is on the disk and therefore this is immaterial. Right? Because it's not in any cluster memory right now. It is on the disk, so it's not applicable. Well, the question is what happens to the guy that has the globally oldest page. If it is on the disk and we have to bring it into the faulting node, then necessarily we have to make space in the cluster memory as a whole for this extra page. Because this guy's balance is going to shift, and I have to throw this global page somewhere. I am picking this guy as the replacement candidates, because he has the globally oldest page. But what is going to happen on this node? Well, it really depends on whether the LRU page comes from the global part or the local part. If it comes from the global part, then there is no change on R, because you're basically throwing this into the global part, and he's throwing one of those global pages out onto the disk, right? If on the other hand, the global page that I sent over here results in replacing a local page on node R, because the local page happens to be the LRU candidate then L goes down by one and G goes up by one. So we had able to do more community service in this case and in the last case when it is actively shared, again, even though we find the missing page in note Q, there's no change in the balance of L and G because it is coming from the active part of L, because it is shared. It indicates if it is shared it has to be, from the L part of Q. Since it is actively shared, there's no change in the split between L and G and of course, this guy has to send one of its global pages to some other node, because the memory pressure as a whole is increasing on node P. This guy is going to send it to node R that has the LRU page, and just like in this case, where I have to make room for the cumulative memory pressure on the cluster by throwing out a page onto the disk. And similar to this case, if the candidate replacement page comes from the local part of R, then it is going to result in the local part shrinking by one and the global part increasing by one to accommodate the page that came from here. >From the high level description of what happens on page faults and how it is handled, you can see that the behavior of this GMS global memory management, is as follows. So overtime, you can see that if there is an idle node in the land, then that idle node, its working set is continuously going to decrease as it accommodates more and more of its peers pages that are swapped out to fit in its global cache. And eventually a completely idle node, becomes a memory server, for peers on the network. So that's the expected behavior of this logarithm. And the key attribute of the logarithm is the fact, that the split between Local and Global is not static, but is dynamic, depending on what is happening at a particular node. For instance, even the same node, if it was serving as a memory server for my peers, because I had gone out to lunch, I come back, and start actively using my workstation. In that case, I'm going to go back, and the community service part of my machine is going to start decreasing. So the Local Global split is not static, but it shifts depending on what local memory pressure exist at that node. Now that we understand the high level goal of GMS, the most important parameter in the working of GMS system is age managemet because we need to identify what is the globally oldest page whenever want to do replacements And let's see how that algorithm works and how it is managed. So in any distributed system, one of the goals is to make sure that any management work is not bringing down the productivity of that node. In other words, you want to make sure that Management work is distributed and not concentrated on any one node and you know in the GMS system, since we are working with an active set of nodes. Overtime, the node that is active, may become inactive and a node that was inactive may become and so on. And so you don't want the management of age information to be assigned to any given node. But it is something that has to shift over time. And that's sort of the fundamental tenant of building any distributed system. Is to make sure that we distribute the management work so that no single node in the distributed system is overburdened of that. That's a general principle, and we'll see how that principle is followed in this particular system. So we break the management. Both along the space axis and time axis, into what is called epochs, and there are two parameters that govern an epoch. One is T, which is the maximum duration of an epoch. That is, an epoch is in some sense a granularity. Of management work done by a node. And that management work done by a node is either time bound, maximum T duration, or space bound, maximum M replacements. So if, in the working of the system, if M space replacements. Have happened, then that epoch is complete, so we go to a new epoch. There'll be a new manager for age management. Or if the duration T is complete, then again, the epoch is complete, and so we pick a new manager to manage the next epoch. And we'll see in a minute how we pick the new manager. And T maybe on the order of a few seconds. And M, which is the number of replacements, maybe on the order of thousands of replacements. So at the start of each epoch what happens is every node is going to send the age information to the initiator. That is. Every node is going to say, what is the age of the pages that is resident at this node. All the local pages, all the global pages, universe of all the pages that exist at this node, what is the age information associated with that? And remember that the smaller the age, the more relevant the page is. So the higher the age, the older the page. So, in picking a candidate, we're always going to pick an old page to replace. So, that's the age information that each of these node ascending to the initiator. So, in one sense. Its set of pages, N2 sends its set of pages, and so on. Everybody is sending to the manager node that I'm calling the initiator, the age information. What the initiator node is going to do is two things. It's going to find out what is the minimum age of all the M pages that are going to be replaced. Remember that. Smaller the age, the better. So what it is going to say is out of all the pages that exist in the entire cluster, what are the oldest M pages that are going to be replaced in the upcoming epoch, and for those M old pages, out of those M old pages, what is the minimum age? So any page with less than the minimum age are pages that are active and that are going to survive this upcoming epoch. Whereas, any page whose age is older than that minimum age, is part of this set of M pages that are going to be replaced in the upcoming epoch, and those are the replacement candidates. That's minimum age. So it computes the minimum age. And it also computes given the minimum age and given the distribution of the age demographics coming from N1, I know out of these pages coming from N1 what fraction of the pages that belong to N1 are going to be replaced in the upcoming epoch. And that is the weight parameter for this particular node. For instance, if it turns out that N1 has 10% of the candidate pages that are going to be replaced in the next epoch, then its wage is 0.1. If N2 is going to account for 30% of the replacements 30% of the M replacements in the upcoming epoch. Then N2's weight is going to be W2 and so on. So what this initiative does is it computes this min age and it also computes the weight for each one of the nodes, and that's what is sent back to each node. So each node is going to see the min age and also the weight for all [UNKNOWN]. Each load is not only receiving its own weight, that is its own fraction of the [INAUDIBLE] pages that are going to be replaced, but it is also getting the fraction of the pages that are going to be replaced from each of its peer nodes in the entire cluster. And we'll see how this information is going to be used by each one of these nodes. And of course, we don't know the future, all that the initiator is doing. Is that is saying that it is expected replacement. W1 is expected share of replacement that's going to happen in N1. W2 is expected share of replacement that's going to happen in N2 and so on. And when the next epoch starts actually. That can be different, depending on what happens in these [INAUDIBLE]. But, that's the best that we can do is use the past to predict the future. That's what the initiates doing. It is using the past, the age information that it got from all the notes, in order to predict the future in terms of where these end replacements are going to happen. What is the minimum age of the pages that are going to be replaced in the next epoch. So that's what is being done by the initiator. I mentioned that this management function that this initiator is doing, you don't want it to be statically assigned to one node, because that would be a huge burden, and besides this node will suddenly become very active, and if that happens, then you must time in the computations that are going to happen here to do this management function. Now intuitively if you think about it, if a particular node, lets say node N2. Has a read of .8, what does that mean? What that means is that in the upcoming Epoch, 80% of the end pages are going to be replaced from the node N2, in other words, N2 is hosting a whole bunch of serious citizens, so it's a node that's not very active. Right? If an inactive node, then the replacement's candidates are probably not going to come from that guy and in predicting the future the initiator is saying that well, most of the replacements are going to come from here, so it's rated very high. So this information. The weight information for all the nodes is coming to every one of these guys. So intuitively if we think about it, the guy that has the highest weight is probably the least active. Why not make him the manager? So the initiator for the next Epoch is going to be the node with the maximum weight. Now how do you determine that? Well, every one of these nodes is receiving not only the min age of the candidate pages that are going to be replaced in the next epoch but also the weight distribution that says what is the percentage of these replacements that are going to happen in each one of these nodes. So locally each one of these guys. Can look at the Wi vector that it got back from the initiator and say, I know, given that node N3 has the maximum rate, that's going to be the initiative of the next epoch. So locally you can make the determination so that for the next epoch, in order to send the age information, you know, locally who to send it to. Now let's look at how this minimum age information is going to be used by each node. Remember that min age represents the minimum age of the m oldest pages. The smaller the age, the more recent that page is. And so those are the active pages. So if you, if you look at this as the age distribution, of all the pages, then if you draw a line here, then the pages to the left of this line, the minimum age line, are the active pages. And the pages to the right of this, with is the m oldest pages are the candidate pages that are going to be replaced in the next epoch. This is the information that we're going to use In the way we manage page replacements at every node. So let's see what the action is at a node when there is a page fault. If upon a page fault I have to evict a page y, then what I'm going to do is I'm going to look at the age of this page y. If the age of this page y is more than the minimum wage. Then I know it's going to be replaced. Even if I sent it to a peer node, it's going to be replaced because that is part of the candidate m pages that are going to be replaced in the upcoming epoch. And therefore locally I'll make a decision that page y has an age older than minimum age, and therefore, I'm going to discard it. Simply discard it. Don't worry about sending it to a peer. Remember that, in the description of how a page fault is handled, I said that, when you have a page fault in the node, I pick one of the replacement candidates, and I send it to a peer node. To store it in the global cache of the peer node. Well, we don't want to do that. If that page is going to be eventually replaced during this upcoming epoch, meaning it has to be thrown out onto the disk. In that case, you might as well discard it right now. So if the age of the page that you're evicting. From your node happens to be greater than MinAge, simply discard it. On the other hand, if the page happens to be less than the MinAge, then you know that it is part of the Active set of pages for the entire cluster, you cannot throw it away. You send it to Peer Node Ni. How do you pick Ni? This is where the weight distribution comes in. I know the weight of the nodes. At the end of this computation, the manager sent me the weight distribution for the upcoming epoch of all the nodes. So I can pick a node to send this page to based on the weight distribution. Of course I could say, well, send it to the guy that has the highest weight because that's the guy that is likely to replace, but remember that there's only an estimate of what is going to happen in the future. So, you don't want to hard code that. Instead, we're going to use some information drawn from these weights. We're going to factor that weight information Into making a choice as to which peer we want to send this eviction page to. Chances are that the node, that has a higher weight is a likely candidate that'll pick. But it will not always be the node with the highest weight. Because if that decision is made by everybody then we going to have a situation where. If the prediction was not exactly accurate, we would be making wrong choices. Basically you can see that this age management, geriatric management, is approximating a global LRU. Not exactly a global LRU, because global LRU. Computing that on every page fault is too expensive, so we don't want to do that. Instead, we are doing an approximation to global LRU by computing this information at the beginning of an epoch, and using that information locally in order to make decisions. So we think globally in order to get all the age information, and compute the minimum age and compute the weight for all the nodes as to how much of the fraction of the replacements are going to come from each one of these nodes. But once that computation has been done, for the duration of the epoch, all the decisions that are taken at a particular node Is local decisions in terms of, what we want to do, with respect to a page that we are choosing as an eviction candidate. Do we discard it, or do we store it in a peer, global cache? This might sound like a political slogan but, the important thing, in any distributed system, is as much as possible. Use local information in decision making. So think globally but act locally. So that's the key and that's what is ingrained in this description of the algorithm. So from algorithm description, we go to implementation now. This is where rubber meets the road in systems reset. In any good systems reset, the prescription is as follows. You identify a pain point. Once you identify the pain point. You think of what may be a clever solution to that. Then a lot of heavy lifting actually happens in taking that solution, which may be a very simple solution, but implementing that is the hard part. if you take this particular lesson that we're looking at The solution idea is fairly simple. The idea is that instead of using the disk as a paging device, use the cluster memory. But implementing that idea requires quite a bit of heavy lifting, and one might say that in systems research these technical details Of taking an idea and working out the technical details of implementing that idea is probably the most important nugget. Even if the idea itself is not enduring, the implementation tricks and techniques that are invented in order to do their implementation of their idea maybe reusable knowledge for other systems research. So that's a key takeaway in any systems research and this true for this one as well. Now in this particular case, the authors of GSM used a deck, little equipment corporations operating system as the base system. The operating system is called OS of 1 operating system. And there are two key components. In the OSF1 memory system, which is what we're talking about here. The first one is the virtual memory system, and this is the one that is responsible for mapping process virtual address space to physical memory, and worrying about page faults that happen when a process is trying to access the stack and heap and so on. So that it can bring those missing pages perhaps from the disk. And these pages are sometimes referred to as anonymous pages because a page is housed in a physical page frame and when a page is replaced, that same physical page frame may host some other virtual page and so on. So the virtual memory system is devoted to managing the page faults that occur for process virtual address space, in particular the stack and the heap. And the unified buffer cache is the cache that is used by the file system. And remember the file system is also getting stuff from the disk. But the other system wants to cache it in physical memories so that it is faster, so the unified buffer cash is serving as the extraction for the disk resident files when it gets into physical memory and user processes. Are going to do explicit access to files. When they do that, they're actually accessing unified buffer cache. So reads and writes of files go to this unified buffer cache. In addition to that, Unix systems offer the ability to map a file. Into memory, which is called memory mapped files. And if you have a memory mapped file, you can also have page faults to a file that has been mapped into memory. So the unified buffer cache is responsible for handling page faults to map files, as well as explicit read and write calls. That an application process may make to the file system. So normally this is the picture of how the memory system hangs together in any typical implementation of an operating system. You have the virtual memory manager, you have the unified buffer cache, you have the disk, and region writes from the virtual memory manager go to the disk, and similarly reads and writes from the unified buffer cache, go to the disk and when a physical page frame is freed, you through it into the free list. So that it is available for future use by either the virtual memory manager or the unified buffer cache. And the Pageout Daemon, it's role is to look at every once in a while what are pages that can be swapped out to the disk so that you make room for page faults to be handled without necessarily running an algorithm to free up pages. so that's the structure of a memory management system, and what the authors of GMS did, was to integrate GMS into the operating system, and this is where I said that. There is heavy lifting to be done, because you are modifying the operating system to integrate an idea that you had for global memory system. This calls for modifying both the virtual memory part as well as the UBC part of the original system, to handle the needs of the global memory system. And particular what you notice, is that the VM and the unified buffer cache, when it wanted a missing page, it used to go to the disk. But here what we're going to do is we're going to make it go to the GMS Because GMS knows whether a particular page, that is missing in the address space of a process, is present in some remote GMS and similarly, if a page is missing from the file cache, it knows whether that page. Is perhaps in the remote GMS. So that's why, what we do is, we modify the virtual memory manager and unified buffer cache manager to get their calls for getting and putting pages. Get page is when there is a page fault I need to get that page. I would go to the disc normally, but I now, I go to GMS. That GMS worry about getting it for me. Similarly if a page is missing in the unified buffer cache, I go to the GMS to say get me that page for me, and he will then do the work of finding out whether it is in remote GMS or if it is not anywhere in the cluster as we've seen in some of the cases. It could be under disk, it will get it from the disk. Notice also writes to disk is unchanged. Originally, whenever the VM manager decides to write to the disk, it's because a page is dirty. It writes to the disk. That part we don't want to mess with, because we are not affecting the reliability of the system in any way. Writes remain unchanged. Only when you have a page fault, you have to get it Get the page from the disk, you don't go to the disk anymore, but you come to GMS, and GMS then is integrated into the system so that it figures out where to get the missing page from. Remember that the most important thing that we have to do in implementing this global LRU at approximation to global LRU. Is collecting age information. That's pretty tricky. And the case of file cache is pretty straightforward, because it is explicit from the process where it is making regenerate calls, we can insert code as part of integrating GMS into this unified buffer cache, to see what pages are being accessed based on the region rights that are happening to the unified buffer cache. Because these calls are explicit from the application, going into the operating system and therefore, we can intercept those calls as part of integrating DMS to collect age information for the pages that are housed in the unified buffer cache. VM on the other hand, is very, very complicated. Because memory access, that a process does, is happening in hardware on the CPU. The operating system does not see the individual memory access that a user program is making, so how does GMS collect aid information for the anonymous, that's why these pages are called anonymous pages. In this case, the pages are not anonymous because we know exactly what pages are being reached in by a particular read or write by a file system, but the reads and writes that a user process is making to its virtual address space are the pages that are anonymous so far as the operating system is concerned. So, how does the operating system collect the age information? In order to do that, the authors of GMS what they did, was to have a daemon part of the memory manager in the OSF implementation. The daemon's job is to dump information from the TLB. If you recall, TLB, the translation lookaside buffer, contains the recent translations that have been done on a particular node. So the TLB contains information about the recently accessed pages on a particular processor. Periodically, say every minute, it dumps the contents of the TLB into a data structure that the GMS maintains so that it can use that information in deriving the age information for all the anonymous pages that are being handled by the VM. So that is how GMS collects the age information at every node. So this is where I meant, the technical details are very important. Because this idea of how to gather age information is something that might be useful if you're implementing a completely different facility. And similarly, when it comes to the pageout daemon. The pageout daemon, normally, what it would do is, when it is throwing out pages. If they are dirty pages, he'll write it on to the disk. And if it is clean pages, he can simply discard it. But now, when it has a clean page, it wants to discard a clean page, a pageout daemon. We'll give it to GMS, because GMS will say, well, don't throw it out. I'll put it into cluster memory so that it is available later on for retrieval rather than going to the disk. So this is how GMS is integrated into this whole picture, interacting with the Unified Buffer Cache, interacting with the VM Manager, interacting with the Pageout Daemon and the Free List Maintenance as well in terms of allocating and freeing frames on every node. Some more heavy lifting. Let's talk about the distributed data structures that GMS has in order to do this virtual memory management across the entire cluster. First thing that GMS has to do, is to convert a virtual address, which is a local thing so far as a single processor's concerned. And convert that virtual address into a global identifier, or what we'll call as a universal ID. And the way we derive the universal ID from the virtual address is fairly straightforward, if you think about it. We know which node this virtual address emanated from, IP address. We know which disk partition contains a copy of the page that corresponds to the virtual address. That we know. What are the i-node data structure that corresponds to this page? And what is the offset? So if you put all of these entities together, you get a universal ID that uniquely identifies. A virtual address. This is the offset within a page. So given a virtual address, the first three parts uniquely identify the page, and the fourth part identifies the offset within that page for that virtual address. And this we can derive it from the virtual memory system as well as the UBC. There are three key data structures: PFD, GCD and POD. These three data structures are the workhorses that make this cluster wide memory management possible. Let's talk about each one of these things. PFD is like a PH table. Normally in a PH table what you do is you give it a virtual address and the PH table says oh, I know what is the physical frame that backs this particular virtual address. That is the translation between a virtual page number and a physical page frame that hosts that virtual page is contained in a page table. Similar to that is PFD, it's called the page frame directory. It has a mapping between a UID, because your virtual address has been converted to a UID. Given a UID, it says, what is a page frame that backs that particular UID? That's this data structure. Because we're doing cluster-wide memory management, the page itself can be in one of four different states. It could be in the local part of that node. And if it is in a local part of a node. Then, that page could be a private page or it could be a shared page. These are two possibilities, there is living in the local part of the physical memory of a node. If it is in the global part, we know by definition, global part is only hosting clean pages, so the content of global cache is always. Going to be private pages and so the state of the page that happens to be the global cache of a particular node is guaranteed to be a private state. And the last possibility is that it's not in the physical memory of a node, but it is on the disk. So the page frame directory. Just like a page table, says that either this page that you are looking for go from VA to UID, that page is in physical memory and it is one of these three states or its not in the physical memory, it's on the disk. Now given a UID I know that some PFD in the entire cluster has the mapping for the UID saying what is the physical frame number that corresponds to it if it happens to be on that node or it's on the disk. That information is contained in some PFD in the entire cluster. So, if I have a page fault. I convert my VA to UID then which PFD will I go look up? I could broadcast to everybody and say, how do you have it? That will be very efficient. You don't want to do that, or we can say that there is some way of magically mapping, given a UID which node will have the PFD for me to look up. And find out the backing physical page. But we don't want to do a static binding of UID to the node that manages that UID. Because if we make a static mapping, then it pushes the burden on one node to take that, if some page has become really hot and everybody wants to use that. So what we want to do is distribute this management function also, just like the age management, we did not want to concentrate it on a single node. We want to distribute the management of giving this mapping between UID and which node has the PFD that. Can tell me information about the missing page. And that data structure is this Global Cache Directory, GCD. So GCD is a hash table, it's a cluster-wide hash table, so it's a distributed data structure. And the rule that GCD performs is given a UID, it will tell you which node has the PFD that corresponds to this UID. That's the role of this data structure, it's a partition hash table, so given a UID, I can say well. I go to the GCD, and the GCD will say, what is the PFD that has this UID? Because of the partition hash table, even though a part of this GCD is on every node, every node may not be locally able to determine where the PFD is. Given a UID, it can go, it has to know which GCD it has to consult. In order to know which node has the PFD that corresponds to this UID. So, we need another data structure that tells us, given a UID, which node has the GCD that corresponds to this UID. And that is the page ownership directory. So, the page ownership directory says, given a UID. Which node has the DCD that corresponds to this UID. And this data structure the page ownership directory is replicated on all the nodes. It's an identical replica that is on all the nodes. So, when I have a page for it first thing that I'm going to do, is go to my POD and that is. A replicated data structure and it's completed information, up to date information. So I go to this POD and as this question. Given this new id, how do I find out the global cache directory that has information about the PFD that can help me to. Map my virtual address to a physical address. Remember that we could have simply gone from here to here, but that would have been a static mapping, and this one level of indirection is giving a way by which we don't have to statically map. A PFD to a UID, but this intermediate step, allows us to move the responsibility of hosting a particular PFD to different nodes, using this intermediary which is a distributive hash table. I said that this page ownership directory. Is replicated data structure. Can it change? Well it can change over time because what this page ownership directory is saying is the following. The UID space is something that spans the entire cluster. If you take the virtual addresses of all the possibilities of the entire cluster. That universe of all the virtual addresses is this UID space because it is being mapped from a a virtual address of a single process to this UID space and this spans the whole cluster and what we have done is we have partitioned that UID space. Into set of regions of ownership and that's what is called the page ownership. So every known is responsible for a portion of the UID space and that is this global cache directory. Now if the LAN never revolves. In other words, if the set of nodes on the LAN is fixed. Then the page ownership director also remains the same. But if nodes, if new nodes are added and deleted and so on, that's when the page ownership directory is going to change. And if that happens, then we have to replicate again, we have to redistribute the page ownership directly. But this is not something that's going to happen too often. It's very rare that node is going to come down or new node is going to be inserted into a LAN. And therefore this page ownership directory does not change very often. And that's where it's replicated data structure that you can believe at every node. But if it changes, there is also a way of handling that. We'll see that in a minute. So the path for page fall handling is if you have a page fall you convert. VA to UID and once you have this UID then you can go to your page ownership directory, that's on your note. And ask the question, please tell me who has the PFD that corresponds to this UID? And this is going to tell me oh here is the note that contains the PFD for the UID that you're looking for. Then I can go to that PFD and from that PFD, I can get the page that I am looking for which might be in that note or it might say well it's not in my note any more. It's on the disk. So this is the pack for page four handling. So now that we understand the data structures, let's put the data structures to work. Let's first look at what happens on a page fault. On a page fault, the node first coverts the virtual address to a UID and once it converts it to the UID it goes to the page ownership directory. And as I mentioned, the page ownership directory is something that I can trust. It'll tell me, given this UID I'll tell you who the owner of this page is. And you go to him because he has the GCD data structure for this. So, node A finds out the identity of the owner for this UID, and that happens to be node B. So it sends the UID over to node B, and node B, because it is the owner for this UID, it looks up its GCD data structure and says, oh, the PFD that can translate this UID which actually represents this virtual address is actually contained in this particular node, node C. So, that's the content of this data structure, given a UID what is the node ID that contains the PFD. Remember that PFD is equivalent of a page table in the normal system and therefore that's the node that I want to send this UID to so that we can do the translation for this virtual address. So node B sends the UID over to node C, and node C contains the PFD that has the mapping between the UID and the page frame number that is backed by this node for this UID, retrieves the page. It's a hit. Sends it over to node A. Node A is happy. It can then map this virtual address into its internal structure and the page fault service is complete and it can resume the process that was blocked for this page fault. So you can see that, potentially, when a page fault happens I can have three level of network communication. Of course the first look up of the POD is local to my node because this POD data structure is replicated on every node. So from the UID, I can find the owner for the UID. But then I have to send a network message over to the node that contains the GCD for this UID. And then he'll then send it to the node that has this page so that that page can come back. Now this network communication I am willing to tolerate because it is equivalent to performing the role of what the disk would have done. And maybe it is much better than going to a disk in order to get the missing page. So, it is happening only on page fault and since it is on a page fault, this network communication is okay. But, this is an extra network communication. Fortunately, the common case is a page fault is servicing the request of a local process on node A, and so the page is a non-shared page and if it is a non-shared page most likely the UID space that corresponds to the missing page is also managed by this node. Or in other words, both the POD and the GCD corresponding to a particular UID is mostly on this node itself. And this is true for non-shared pages, and so A and B are exactly the same node. So there is no network communication to go from the POD to GCD to find out the PFD. So hopefully the only network communication that happens on every page fault is a network communication to go to this node that is serving the role of a disk and get the page from him. That's okay to incur because it probably is much lesser than going to the disk in order to do the page access. So the page fault service for the common case is going to be pretty quick. So the important message that I want you to get out of this is that even though these distributed data structures may make it look like you're going to have a lot of network communication, the important thing to note is that it happens only when there is a page fault. And since most of the page faults, the common case, is going to be non-shared pages. The data structure POD and GCD are probably co-resident on the same node. So, even though I've shown two different nodes here, A and B may be exactly the same node. And so looking up the PFD that currently is backing this particular UID is going to be local to this node and so we can directly go to the node that contains the PFD and make the page fault service pretty quick. Now the interesting question is, once I go to the node that I think is going to give me the page that I'm looking for, or at least information about the page that I'm looking for. Whether it is, if it has it, it's going to give it to me, or if it doesn't have it, it's going to tell me that it is on the disk. But, in either case, I'm going to get, I'm hoping, I'll get exact information about this missing page from this node which is supposed to have the page foame directory, the PFD that corresponds with this UID. Is that possible that I go to this guy, and he says no, I don't have it. Yes, it is possible in two cases. One case is, let's say, while this guy was sending this request over, this guy has made a decision to evict that page that corresponds to this UID because it had to make space for itself. In that case, that UID may have been thrown away from the PFD. And if it has been thrown away from the PFD, what he would have done is inform the guy who has the ownership for this UID, this node is the owner for this UID. If he evicts that page this guy has to tell this node that, hey, you know what, I used to back this UID in my PFD but I got rid of it. And, and I got rid of it by sending it to some other node, let's say, node D. So that is something that I have to communicate to this GCD, but it's a distributed system. Things are happening asynchronously. He may not have communicated that yet, that information is not there in the GCD of this node. This is the owner for this UID space. But the owner doesn't yet know that the PFD that corresponds to a particular UID has moved to some other node out here somewhere. And if it has moved to some other node, he will know about eventually, but he doesn't have it at this point. That's why this request was routed here, and this guy says I don't have it. It can have a miss. That's one possibility. Second possibility is the uncommon case that the POD information that I had is stale. When can that happen? That is when the POD is being recomputed for the local area network as a whole, either because there are new additions or new deletions of nodes. And therefore we are recomputing the redistribution of the UID space and deciding which node is responsible for which UID. That can happen. And in that case also, it is possible that the information that I started with was incorrect. Because I went here thinking that he has the GCD, he didn't have it at that point, but it is changing. And eventually I'm going to find out. So if there is a mess, either case. The first case is, this guy replaced that page, or the second case is, my POD information misled me. Both cases, I'll have a miss. And I'll say, oh, it was a miss. And I know that is probably the uncommon case. I'm going to retry that, by looking up my POD again. And by that time, the POD may have been updated, I'll go to the right GCD this time. Or, the GCD would have been updated and so I'll go to the same GCD, but the GCD will have the more relevant information of which PFD is currently hosting it. So, I'll go to him in order to get the page that I'm looking for. But the important point I want to leave you with is that the common case is when both the POD and GCD are coresident on the same node. And in that case, you don't have a network communication to look up the GCD, and also the miss happening when you do reach the PFD. That is also uncommon. It can because happen because of replacement that has happened on that node, or because the POD has changed. And this is something that is going to happen relatively infrequently compared to the activities that we're talking about in terms of page faults. The next thing I want to talk about is what happens in the system on Page Evictions. Remember that on Page Eviction, when a Node decides that it wants to throw out a page, it sends it in the algorithm that I described to the Node. Which is candidly ignored for hosting that page, and it might use the weight information to make the decision. Of which node to send that page that it is discarding from it's own node. When a page fault happens, the key thing is to make sure that we service the page fault so that we get the page. And restart the execution that has been stalled on this node. That's the important thing to do. The less important thing, but something that needs to happen in the universe of things that is being managed by GMS, is to also send the victim page from this node to the target node. That's part of the algorithm for eviction on page four. However, we don't want to do it on every page fault, but we want to do it in an aggregated manner. And for that reason every node has a paging daemon. And this is typical of virtual memory systems that when a page fault happens. That's not the time the virtual memory manager is running around trying to find the page free. It always has a stash of free page frames to allocate. To service this particular page fault. As I mentioned earlier, the paging daemon in the virtual memory manager is integrated with the GMS system. And what the paging daemon is going to do is, when the free list falls below a threshold, then the paging daemon is going to do put page of the oldest pages on this node. And remember that in the integration of GMS with the virtual memory manager in the put page, I said that the Paging Daemon is also modified to work with the GMS. This is where the modification comes in. Normally what the Paging Daemon would've done is when the free list goes below threshold, it would take a bunch of pages, LRU candidate pages, and put it all onto the disc. But with the integration with GMS, what the paging demon is going to do is, for the same condition when the freelist falls below threshold, it's going to basically do putpage of the oldest pages that it has on this node. And when we do the put page, we're going to choose the candidate node. Where we going to do the putpage based on the weight information that we got as part of the geriatric management that we talked about in the beginning. And so we do a putpage of UID. Into this PFD of this node so that this guy will be the one that will be backing this particular UID. And once I do this, I also have to update the GCD to indicate that the new PFD for this particular UID is C. So this update message is being sent to the owner of this UID. That is this node, and the owner the guy that has the portion of the UID space that is managed by this node. So the GCD data structure contains the mapping of the UID to the node that contains the PFD for that particular UID. And so I send this update message saying please update the GCD to indicate that this particular UID is now backed by PFD that sites on note C. So that's the information I'm sending to this guy. And this is not being done on every page eviction, but it is done by the paging demon in a aggregated, coordinated manner. When the free list falls below a threshold. So we've covered a lot of ground from just sort of the principal behind the thought experiment. That is, using network memory as a paging device, rather than disc. Because the networks have gotten faster. And we came up with an algorithm for age management globally in the entire cluster, and how to have that age management done in a manner that doesn't burden any one node. But it picks the node that is lightest in terms of load at any point of time. And we also saw given the solution for cluster white memory management for paging, how to go about implementing it. And all of the heavy lifting that needs to be done in order to take an idea and put it into practice. You'll see that in every systems research paper, there is heavy lifting to be done to take a concept to implementation. Working out the details and ensuring that all the corner cases are covered is a non-trivial intellectual exercise in building distributed subsystems. Like this one, the global memory system. What is enduring in a research exercise like this one? The concept of paging across the network is an interesting thought experiment, but it may not be feasible exactly for the environment in which the authors carried out this research, namely workstation clusters connected by a local area network. Each workstation in that setting is a private resource owned by an individual who may or may not want to share his resources, memory in particular. On the other hand, data centers today are powered by large scale clusters on the order of thousands of processes connected by a local area network. No node is individually owned in that setting. Could this idea of paging across the local area network be feasible in that setting? Perhaps. Even beyond the thought experiment itself what is perhaps more enduring are the techniques, distributed data structures and the algorithms, for taking the concept of implementation. In the next part of this lesson module, we will see another thought experiment to put cluster memory to use. In an earlier part of this lesson module, we saw how to build a subsystem that takes advantage of idle memory in peer nodes on a local area network, namely, using remote memory as a paging device, instead of the local disk. The intuition behind that idea was the fact that networks have gotten faster. And therefore access to remote memory may be faster than access to an electromechanical local disk. Continuing with this lesson, we will look at another way to exploit remote memories, namely, software implementation of distributed shared memory. That is, create an operating system abstraction that provides an illusion of shared memory to the applications, even though the nodes in the local area network do not physically share memory. So distributed shared memory asks the question, if shared memory makes life simple for application development in a multiprocessor, can we try to provide that same abstraction in a distributed system, and make the cluster look like a shared memory machine? Now suppose the starting point is a sequential program. How can we exploit the cluster? We have multiple processors, how do we exploit the cluster if the starting point is a sequential program? One possibility is to do what is called automatic parallelization. That is, instead of writing an explicitly parallel program, we write a sequential program. And let somebody else do the heavy lifting in terms of identifying opportunities for parallelism that exist in the program and map it to the underlying cluster. And this is what is called an implicitly parallel program. There are opportunities for parallelism, but the program itself is not written as a parallel program. And, now it is the onus of the tool, in this case an automatic parallelizing compiler, to look at the sequential program and identify opportunities for parallelism and exploit that by using the resources that are available in the cluster. So high-performance FORTRAN is an example of a programming language that does automatic parallelization, but it is user-assisted parallelization in the sense that the user who is writing the sequential program is using directives for distribution of data and computation. And those directives are then used by this parallelizing compiler to say, oh, these are opportunities for mapping these computations onto the resources of a cluster. So it puts it on different nodes on the cluster and that way, it exploits the parallelism that is there in the hardware, starting from the sequential program and doing the heavy lifting in terms of converting the sequential program to a parallel program to extract performance for this application. This kind of automatic parallelization, or implicitly parallel programming, works really well for certain classes of program called data parallel programs. In such programs, for the most part, the data accesses are fairly static, and it is determinable at compile time. So in other words, there is limited potential for exploiting the available parallelism in the cluster if we resort to implicitly parallel programming. So we write the program as a truly parallel program, or in other words, the application programmer is going to think about his application and write the program as an explicitly parallel program. And there are two styles of writing explicitly parallel programs. And correspondingly, system support for those two tiles of explicitly pavalled programs. One is called message passing style of explicitly pavalled program. The run time system is going to provide a message passing laterally which has primitives for. An application thread to do sends and receives to its peers that are executing on other nodes of the cluster. So this message passing style of explicitly parallel program is true to the physical nature of the cluster. The physical nature of the cluster is the fact that every processor has its private memory. And this memory is not shared across all the processors. So the only way a processor can communicate with, another processor is by sending a message through the network that this processor can receive. This processor cannot directly reach into the memory of this processor. Because that is not the way a cluster is architected. So, the messaging passing library is true to the physical nature of the cluster. That there is no physically shared memory. And lots of examples of message passing libraries that have been written to support explicit parallel programming. In a cluster they include MPI, message passing interface, MPI for short, PVM, CLF from digital equipment corporations. So these are all examples of message passing libraries that have been built with the intent of allowing application programmer to write explicitly parallel programs using this message passing style. And to this day, many scientific applications running on large scale clusters in national labs like Lawrence Livermore, and Argonne National Labs and so on, use this style of programming using MPI as the message passing fabric. Now, the only downside to the message-passing style of programming is that it is difficult to program using this style. If you're a programmer who's written sequential programs, the transitions paths to writing an explicitly parallel program is easier if there is this. Notion of shared memory, because it is natural to think of shared data structures among different threads of an application. And that's the reason making the transition from sequential program to parallel programming, using for instance the P thread library or SMP is fairly intuitive and easy pathway. On the other hand, If the programmer has to think in terms of coordinating the activities on different processes by explicitly descending and desisting messages from their peers. That is calling for a failuratical change of thinking in terms of how to structure a program. This was the motivation for coming up with this abstraction of distributed shared memory in a cluster. The idea is that we want to give the illusion to the application programmer writing and explicitly parallel program. That all of the memory that's in the entire cluster is shared. They are not physically shared, but the DSM library is going to give the illusion to the threads running on each one of these processes that all of this memory is shared. And therefore they have an easier transition path for instance, from going from a sequential program or going from a program that they've written on an SMP. To a program that runs on the cluster, because they don't have to think in terms of message passing. But they can think in terms of shared memory, sharing pointers across the entire cluster, and so on. Also, since we are providing a shared memory semantic in the DSM library for the application program There is no need for marshalling and unmarshalling arguments that are being passed from one processor to another and so on. All of that is being handled by the fact that there is shared memory. So when you make a procedure call, and that procedure call is touching Some portion of memory that happens to be on a remote memory. That memory is going to magically become available to the thread that is making the procedure call. In other words, the DSM abstraction gives the same level of comfort to a programmer who's used to programming on a true shared memory machine when they moved to cluster. Because they can use same set of primitives, like locks and barriers for synchronization, and the Pthread [GUESS] style of creating threads, that will run on different nodes of the cluster. And that's the advantage of DSM style of writing an explicitly parallel program. Now, having introduced distributed shared memory, I want to give sort of a birds eye view of the history of shared memory systems over the last oh, 20 plus years. My intent is not to go into the details of everyone of these different systems, because that can take forever, but it is just to give you sort of the space occupied by all the efforts that have gone on in building shared memory systems, both in hardware and in software. I encourage you to surf the web to identify papers and literature on these different systems that have been built over time, just to get a perspective on how far we have come in the space of building shared memory systems. A few thoughts, on the software side, software DSM was very first thought of in the mid 80s. The Ivy system that was built at Yale University by Kai Li and the Clouds Operating System that was built at Georgia Tech and there were similar systems built at UPenn. This, I would say, is the beginning of Software Distributed Shared Memory. Later on, in the early' 90s, systems like Munin and TreadMarks were built. I would call them perhaps a second generation of Distributed Shared Memory systems. And in the later half of the 90s, there were systems like Blizzard, Shasta, Cashmere and Beehive. That took some of the ideas from the early 90s even further. And in parallel with the software DSM, I would say there was also a completely different track that was being pursued. And that is, providing structured objects in a cluster for programming. And systems such as Linda and Orca, were done in the early 90s. Stampede at Georgia Tech was done in concert with the Digital Equipment Corporation in the mid 90s and continued on, later on, into Stampede Rt and PTS, and in fact, in a later lesson, we'll talk about Persistent Temporal Streams. And this particular axis of development of structured distributed shared memory is attractive because it gives a higher level abstraction than just memory to computations that needs to be built on a cluster. Early hardware shared memory systems such as BBN Butterfly and Sequent Symmetry appeared in the market in the mid 80s and, the synchronization paper that we saw earlier by Mellor-Crummey and Scott used BBN Butterfly and Sequent Symmetry as the experimental platform for the evaluation of the different synchronization algorithms. KSR-1 was another shared memory machine that was built in the early 90s. Alewife was a research prototype that was built at MIT, DASH was a research prototype that was built at Stanford and both of them looked at how to scale up beyond an SMP, and build a truly distributed shared memory machine. And commercial versions of that started appearing. SGI silicon graphics built SGI origin 2000 as a scalable version of a distributed shared memory machine. SGI Altix later on took it even further, thousands of processors exist in SGI Altix as a large-scale shared memory machine. IBM Bluegene is another example. And today, if you look at what is going on in the space of high performance computing. It is clusters of SMPs which have become the work horses in data centers. I very much want you to reflect on the progress that has been made in shared memory systems. And, invite you to, look at, some of the details of machines that have been built in the past, either in the hardware or in software, so that you can learn the progress that has been made. I've already introduced you to shared memory synchronization. Lock is a primitive and particularly the mutual exclusion lock is a primitive that is used ubiquitously in writing shared memory parallel programs to protect data structure so that one thread can exclusively modify the data and release the lock so that another thread can inspect the data later on and so on. And similarly, barrier synchronization is another synchronization primitive that is very popular in scientific programs and we have covered both of these in fairly great detail in talking about what the operating system has to do in order to have efficient implementation of locks as well as barriers. Now the up shot is, if you are writing a shared memory program, there are two types of memory accesses that are going to happen. One type of memory access is the normal reads and writes to shared data that is being manipulated by a particular thread. The second kind of memory access is going to be for synchronization variables that are used in implementing locks and barriers by the operating system itself. It may be the operating system, or it could be a user level threads library that is providing these mutual exclusion locks, or barrier primitives, but in implementing those synchronization primitives, those algorithms are going to use reads and writes to shared memory. So there are two types of shared memory accesses going on in the execution of a parallel program. One is access to normal shared data and the other is access to synchronization variables. Recall that in one of our earlier lectures, we discussed memory consistency model and the relationship of memory consistency model to cache coherence, in the context of shared memory systems. Memory consistency model is a contract between the application programmer and the system. It answers the when question, that is, when a shared memory location is modified by one processor, when, that is how soon, that change is going to be made visible to other processes that have the same memory location in their respective private caches. That's the question that is being answered by the memory consistency model. Cache coherence, on the other hand, is answering the how question, that is, how is the system, by system we mean the system software plus the hardware working together, implementing the contract of the memory consistency model? In other words, the guarantee that has been made by the memory consistency model, to the application programmer has to be fulfilled by the cache coherence mechanism. So coming back to writing a parallel program, when accesses are made to the shared memory, the underlying coherence mechanism has to ensure that all the processes see the changes that are being made to shared memory, commensurate with the memory consistency model. I want you to recall one particular memory consistency model that I've discussed with you before, that is sequential consistency. And in sequential consistency, the idea is very simple. The idea is that every process is making some memory accesses, all of these, let's say, are shared memory accesses. And from the perspective of the programmer, the expectation is that, these memory accesses are happening in the textual order that you see here and that's the expecation so far as this programmer is concerned. Similarly, if you see the set of memory accesses that are happening on a different process of p2. Once again, the expectation is that the order in which these memory accesses are happening are the textual order. Now, the real question is, what happens to the accesses that are happening on one processor with respect to the accesses that are happening on another processor if they are accessing exactly the same memory location? For instance, P1 is reading memory location a, P2 is writing to memory location a. What is the order between this read by P1 and this write by P2? This is where sequential consitency model says that the interleaving of memory accesses between multiple processors, here I'm showing you two, but you can have n number of those processors. Making accesses to shared memory all in parallel. When that happens you want to observe the textual program order for the accesses and the individual processes but the interleaving of the memory accesses coming from the different processors is arbitrary. So in other words, the sequential memory consistency model builds on the atomicity for individual read-write operations and says that, individual read-write operations are atomic on any given processor, and the program order has to be preserved. And, in order to think about the in, interleaving of the memory axises that are happening on different processors. That can be arbitrary and that should be consistent with the thinking of the programmer. And I also gave you the analogy of a card shark to illustrate what is going on with a sequential consistency model. So the card shark is taking two splits of a card deck and, doing a perfect merge shuffle of the two splits, and that's exactly what's going on with sequential consistency. If you can think of these memory accesses on an individual processor as the card split but instead of a two-way split you have an n-way split, and we are doing a merge way shuffle of all the n-ways. Splits off the memory accesses to get the sequentially consistent memory model. With the sequentially consistent memory model, let's come back to a parallel program. So, a parallel program is making read write accesses to shared memory, some of them offer data, and some of them offer synchronization. Now, so far as the sequentially consistent memory model it does not distinguish between accesses coming from the processors as data accesses, or synchronization accesses. It has no idea, it only looks at the read write accesses coming from an individual processor and honoring them in the order in which it appears and making sure that they can merged across all these processors to preserve the SC guarantee. So the upshot is that there's going to be coherence action on every read write access that the model sees. If this guy writes to a memory location, then the sequentially consistent memory model has to ensure that this write is inserted into this global order somewhere. In order to insert that in the global order somewhere, it has to perform the coherence action with respect to all the other processors. That's the upshot of not distinguishing between normal data accesses and synchronization accesses that is inherent in the SC memory model. Now, let's see what happens in a typical parallel program. In a typical parallel program that you might write, you probably get a lock, and you have, mentally, an association between that lock and the data structures that are governed by that lock. Or in other words, in writing your parallel program, you decided that access to variables a and b are governed by this lock. So if I wanted to read or write variables a and b, I'll get a lock and then I will mess with the variables that are governed by this lock. Once I'm done with whatever I want to do with these shared variables, I'll unlock indicating that I'm done. And this is my critical section. So within the critical section, and we're allowed to do whatever I want on these data structures that are governed by this particular lock, because that is an association I as the programmer has made in writing the parallel program. So if another processor let's say P2 gets the same lock. It's going to get the lock only after I release it. So only after I release the lock, this guy can get this lock because the semantics of the lock, it is a mutually exclusive lock. And therefore, only one person can have the lock at a time. And consequently, if you look at the structure of this critical section for P2, it gets a lock. And it is messing with the same set of data structures that I was messing with, over here. But, by design, we know that either P1 or P2 can be messing with the data structure at any point of time. And that's a guarantee that I know comes from the fact that I designed the pilot program. And the lock is associated with these data structures. So, in other words, p2 is not going to access any of the data that, that is inside this critical section until p1 releases the lock. We know this because we designed this program, but the sc memory model does not know about the association between these data structures and this lock. And, in particular, doesn't even know that memory accesses emanating from the processor due to this lock primitive is a different animal compared to the memory accesses coming from the processor as a result of accessing normal data structures. So the cash coherence mechanism that is provided by the system for implementing the memory consistency model is going to be doing more work than it needs to do because it's going to be taking actions on every one of these accesses, even though the coherence actions are not warranted for these guys until I release the lock. So what that means is that there's going to be more overhead for maintaining the coherence [UNKNOWN] with the SC memory model, which means it's going to lead to a poorer scalability of the shared memory system. So in this particular example since P2 is not going to access any of these data structures until P1 has released the lock there's no need for coherence action for a and b until the lock is actually released. This is the motivation for a memory consistency model, which is called release consistency. I'm sure just from the keyword release some of you may have already formed a mental model of what I'm going to say. Basically, we're going to look at the structure of the program As follows that the Peddler program consists of several different Peddler threads P1 is one such, and if it wants to mess with some shared data structures, it is going to acquire a lock, we'll call it A1, and in the mind of the programmer there is an association between this lock and the data structures governed by it. So, so long as they hold the lock, they can modify the data structure and r1 is the release of this lock. So every critical section you can think of as composed of and acquire followed by data accesses governed by the lock and then release. If the same lock is used by some other process at P2, and if the critical section of P1 preceded the critical section of P2 or in other words, P1's release operation P1, r1. The release operation and P1 happened before, this most be familiar to you from our discussion of Lampert's logical clock. P1 R1 happens before P2 R2, that is the acquire operation that is being done by P2 if this acquire operation for the same lock happened after the release by P1 R1. All we have to ensure is that all the coherence actions prior to this release of the lock by P1 has to be complete before we allow P2 to acquire this lock before we allow P2 to acquire the same lock L. That's the idea of release consistency. So we take the synchronization operations that are provided by the system whether it is hardware or software And we label them as either an acquired operation or a release operation. So, it's very straight forward when you think about mutual explosion law, acquiring the log primitive is an acquire operation. And the un log primitive is a release operation. So if there is a lock primitive and there is a PCD unlock primitive, so we have to ensure that all the coherence actions happen before I do the unlock so that when this guy gets the lock and accesses the data, the data that he is going to see are going to be data that is consistent with whatever modifications may have been made over here. That's the idea behind the least consistent memory operation. Other synchronization operations can also be mapped to acquiring release. If you think about barrier, arriving at a barrier is equivalent to an acquire, and leaving the barrier is equivalent to a release. So, before leaving the barrier, we have to make sure that any changes that we made to shared data structures is reflected through all the other processes through the cache coherence mechanism. Then we can leave the barrier. So, leaving the barrier is a release operation, in the case of barrier synchronization. So, what that means is that, if I do a shared memory access within this group of sections, and that shared memory access would normally result in some coherence actions on the interconnect reaching to the other processes and so on, and if we use the SC memory model, you will block processes if you want until That particular memory access is complete with respect to all the processors and the shared memory machine. But if we use the least consistent memory model, we do not have to block P1 in order for coherence actions to be complete to let the processor continue on with its computation. We only have to block a processor at a release point to make sure that any coherent actions that may have been initiated up until this point, are all complete before we perform this release operation. That's the key point that I want you to get out of this release consistent memory model. So the least consistent memory model allows exploitation of computation on P1, with communication that may be happening through the coherence mechanism for completing the coherence actions corresponding to the memory accesses that you're making inside the critical section. So now we come back to our original parallel program and the parallel program is making normal data accesses and synchronization accesses. There are different threads running on all these processors. They're all making these normal data accesses and synchronization accesses. And if the underlying memory model is an RC memory model, it distinguishes between normal data accesses and synchronization accesses. And it knows that if there are normal read/write data accesses, it doesn't have to do anything in terms of blocking the processes. It may start initiating coherence actions corresponding to these data accesses, but it won't block the processor for coherence actions to be complete until it encounters a synchronization operation, which is of the release category. If a synchronization operation which is a release operation hits this RC memory model, it's going to say, ah-ha. In that case all the data accesses that I've seen from this guy, I want to make sure that they're all complete globally, communicated to all the processors. It's going to ensure that before allowing the synchronization operation to complete. So the coherence action is only when the lock is released. So let's understand how the RC memory model works with a concrete example. So let's say the programmer's intent is that one thread of his program is going to modify a structure A. And there is another thread that is going to wait for the modification, and then it is going to use the structure A. So this is the programmer's intent. Right? So P2 is going to wait for the modification, use it, and this guy is the guy that is modifying that particular structure A. And of course these are running different on processors, and therefore we don't know who may be getting to their code first. So let's say that P2 executes the code that corresponds to this semantic. That is, it wants to wait for the modification. So in order to do that, it has a flag, and this flag has a semantic that is, 0 indicating the modification is not done, and 1 when the modification is actually done. And to make sure that we don't do busy waiting, we use a mutual exclusion lock. We lock a synchronization variable, let's call it L. And if the flag is 0, then it is going to execute the equivalent of a pthread_cond_wait. You know, pthread_cond_wait has the semantic that you're waiting on a condition variable and you're also releasing the lock that is associated with this condition variable. So you execute this pthread wait call, and the semantic you know is that at this point, thread P2 is blocked here, the lock is released, and he's basically waiting for a signal on this condition variable c. Who's going to do that, well, of course P1 is the guy that is modifying the structure, so its the responsibility of P1 to signal him. So let's see what happens. So P1 is executing the code for modifying the data structure A, and once it is done with all the modification, then it is going to inform P2. So in order to inform P2, what it does is acquires this lock L, and it sets the flag to 1. And the flag is the one that I inspected over here to know that, oh, the modification is not yet done here, and I'm waiting on this condition variable. So, P1 sets the flag to 1 and signals on the condition variable, c. And, you know that signaling on the condition variable is going to wake up P2. And, of course, it cannot start executing here until P1 has released the lock, and once the lock has been released, that lock will be acquired implicitly by the operating system on behalf of P2, because that is a semantic of this condition wait here. So when I wake up, I'll go back, and as a defensive mechanism, I'll recheck the flag to ensure that the flag is now not 0, indicating that the modification has been done, so I'm now ready to get out of this critical section. I unlock L, come out of the critical section. Now I can use this modified data structure. So that's the semantic that I wanted, and I got that with this code fragment that I'm showing you here. So the important thing is, if you have an RC memory model, then all the modifications that I'm making here that are modifying shared data structures can go on in parallel. With all this waiting that may be going on here, I don't have to block the processor to do every one of these modifications. The only point at which I have to make sure that these modifications have been made globally visible is when I hit the unlock point in my code. So just before I unlock L, I have to make sure that all the read write accesses to shared variables that I've made here in my program have all been taken care of in terms of the coherence actions being communicated to all my peers. Only then, I have to unlock it. So, in other words, this code fragment is giving you pictorially the opportunity for exploiting computation in parallel with communication. If the model was an SC memory model, then for every read-write accesses that are being done in modifying this data structure A, there would have been coherence actions that would have gone on, and those coherence actions, each of them has to complete before you can do the next one, and so on. But with the RC memory model, what it is allowing you to do is, you can do the data structure modification you want, and the coherence actions inherent in those modifications may be going on in the background, but you can continue with your computation until you hit this unlock point. At this point, the memory model will ensure that all the coherence actions are complete before releasing the lock, because once the lock is released, this guy's going to get it, and immediately he'll start using the data structure that has been modified by me. So it is important that all the coherence actions be complete prior to unlocking. So that's the intent of the RC memory model. And that's how you can exploit computation going on in parallel with communication if the memory model is an RC memory model. So to summarize the advantage of RC over SC, is that, there is no waiting for coherence actions on every memory access. So you can overlap computation with communication. So the expectation is that you will get better performance in a shared memory machine if you use the RC memory model, compared to an SC memory model. Now I'm going to introduce you to a lazy version of the RC memory model. And it's called LRC, stands for lazy RC. Now this is the structure of your pilot program so a thread acquires a lot, does the data accessing, releases the lock. Another thread may acquire the same lock. And if the critical section for P1 Precedes the critical section for P2, then the RC memory model requires that, at the point of release, you ensure that all the modifications that have been made on processor P1, that is, the coherence actions. That are commensurate with those data acceses, are all communicated to all the peer processes including P2. Then you release the lock. That's the semantic. And this is what is called eager release consistency, meaning that at the point of release you're insuring that the whole system is cache coherent The whole system is cache coherent at the point of release, then you release from the lock. And the cache coherence is with respect to the set of data accesses that have gone on on this process up to this point, that's what we are ensuring has been communicated and made consistent on all the processes. Now let's say that the timeline looks like this and P1's release happened at this point. And P2's acquire of the same lock happened at a much later point in time. That's the luck of the draw in terms of how the computation went, and so there is this time window between P1's release of the lock and P2's acquisition of the same lock. Now if you think about it there's an opportunity for procrastination. Now we saw that procrastination often helps in system design. We've seen this in mutual exclusion locks. If you insert delay between successive trials of trying to get the lock, that actually results in better performance. We saw that in processes scheduling too. Instead of eagerly scheduling the next available task Maybe you want to wait for a task that has more affinity to your processor. That results in performance advantage. So procrastination often is a good friend of system design. So here again there is an opportunity for procrastination. So Lazy RC is another instance where procrastination may actually help in optimizing the system performance. The idea is that, rather than performing all the coherence actions at the point of release. Don't do it, procrastinate. Wait till the acquire actually happens. At the point of acquire, take all the coherence actions before allowing this acquire to succeed. So the key point is that you're deafening the point at which you ensure that all the coherence actions are complete to the point of acquisition as opposed to the point of release. Even if all the coherence actions commensurate with the data accesses that have gone on up until this release point, are not yet complete when we hit the release, go ahead. Release the lock, but if the next lock acquisition happens, at that point, make sure that all the coherence actions are complete. So in other words, you get an opportunity to overlap computation with communication once again in this window of time between release of a lock, and the acquisition of the same lock. So the Vanilla RC is what is called the eager release consistent memory model and the new memory model is called LRC, or Lazy release consistent memory model. Let's see the pros and cons of LRC with respect to Vanilla RC, or Lazy RC with respect to Eager RC. So what I am showing you here are timelines of processor actions on three different processors, P1, P2, and P3. And this picture is showing you what happens in the Eager version of the RC model, in terms of communication among the processors. So when processor P1 has completed its critical section, does the release operation, at the release point what we're going to do is all the changes that we made, in this example I'm showing you to make it simple I'm showing you that in this critical section that I wrote in this variable x, so the changes to x is going to be communicated to all the processors, P2 and P3. It could be, depending on whether it is an invalidation based protocol or an update based protocol, what we are saying is we are communicating the coherence action to all the other processors. That's what these arrows are indicating. Now then P2 acquires the lock, and after it acquires the lock it does its own critical section. Again, let's say we're writing to the same variable X, and it releases the lock. And at the point of release once again we broadcast the changes that we made. Notice what is going on. P1 makes modifications, broadcasts it to everybody. But who really needs it? Well, only P2 needs it. But unfortunately the RC memory model is Eager, and it says I'm going to tell everybody that has a copy of X that I have modified X. And so it's going to tell it to P2. It's going to tell it to P3 as well. P3 doesn't care, because it's not using that variable yet, and P2 cares, and it of course is using that. But when it releases its critical section, it's once again going to do exactly the same thing that happened over here, and that is it's going to broadcast the changes it made to shared memory locations to all the other processes, in this case P1 and P2. And then, finally, P3 does its acquire, and then reads the variable. So, all these areas are showing you the coherence actions that are inherent in the completion of shared memory accesses that are happening in the critical section of programs. Now let's move over to the Lazy version. In the Lazy version, what we are doing is when we release a lock, we are not doing any global communication. We simply release a lock. Later on the next process that happens to acquire that same lock. The RC memory model. The first thing it's going to say is, oh, you want to get this lock? I have to go and make sure that I complete all the coherence actions that I've associated with that particular lock. In this case the previous lock holder had made changes to the variable x, so I'm going to pull it from this guy and then I can execute my critical section. And then when P3 executes its critical section, it's going to pull it from P2 and complete what it needs to do. So, the important thing that you see is that there is no broadcast anymore. It's only point-to-point communication that's happening between the processors that are passing the lock between one to the other. So, in other words, the number of arrows that you see are communication events. You can see that there's a lot more arrows here. Forget about the arrows that I introduced. But the black arrows that you see are the arrows that are indicating communications commensurate with the coherence actions needed for this set of critical section actions. And correspondingly, the black arrows here are showing the communication actions for the same set of critical section actions shown in both the top and the bottom half of this particular figure. You can see, there's a lot less communication happening with the Lazy model. It's also called a pull model, because what we're doing is at the point of acquisition, we're pulling the coherence actions that need to be completed over here. Whereas, this is the push model in the sense that we're pushing all the coherence actions to everybody at the point of release. Having introduced the Eager and the Lazy RC models, it's time for a quiz. So the question is, what are the pros of the Lazy RC model over the Eager RC model, and I want you to write in free form what you think are the pros. And I want you to think about what are the cons of the Lazy over the Eager in memory consistency model. So in the lazy model, as we saw in the picture, there were less communication events. Which means you have less messages that are flowing through the network, in order to carry out the coherence actions, compared to the eager model. But there is a downside to the lazy model as well. And that is, at the point of acquisition, you don't have all the coherence actions complete. And therefore you may have to incur more latency at the point of acquire to wait for all the coherence actions to get complete. So that's the cons one might think about for the lazy model compared to the eager model. Procrastination helps in releasing the number of messages. But it could result in more latency at the point of acquiring the lock. So far, we've seen three different memory consistency models. One is the sequential consistent memory model, the release consistent memory model. And, strictly speaking, I would say, the eager version and the lazy version are just variants of the same memory model, namely the release consistent memory model. And now we're going to transition and talk about software distributed shared memory, and how these memory models come into play in building software distributed shared memory. So we're dealing with a computational cluster, that is, in the cluster, each node of the cluster has its own private physical memory, but there is no physically shared memory. And therefore, the system, meaning the system software, has to implement the consistency model to the programmer. In a tightly coupled multiprocessor, coherence is maintained at individual memory access level by the hardware. Unfortunately, that fine grain of maintaining coherence at individual memory access level will lead to too much overhead in a cluster. Why? Because on every load or store instruction that is happening on any one of these processors, the system software has to butt in, and implement the coherence action in software through the entire cluster. And this is simply infeasible. So what do we do to implement software distributed shared memory? So, first part is to implement this sharing and coherence maintenance at the level of pages. So the granularity of coherence maintenance is at the level of a page. Now, even in a simple processor or in a true multiprocessor, the unit of coherence maintenance is not simply a single word that a processor is doing a load or a store on. Because in order to exploit spatial locality, the block size used in caches in processors tend to be bigger than the granularity of memory access that is possible from individual instructions in the processor. So we're taking this up a level and saying, if you're going to do it all in software, let's keep the granularity of coherence maintenance to be an entire page. And you're going to maintain the coherence of the distributed shared memory in software by cooperating with the operating system that is running on every node of the processor. So what we're going to do is, we're providing a global virtual memory abstraction to the application program running on the cluster. So the application programmer views the entire cluster as a globally shared virtual memory. Under the cover, what the DSM software is doing is, it is partitioning this global address space into chunks that are managed individually on the nodes of the different processors of the cluster. From the application point of view, what this global virtual memory abstraction is giving is address equivalence. And that is, if I access a memory location x in my program, that means exactly the same thing, whether I access the memory location x from processor 1, processor 2, and so on and so forth. That's the idea in providing a global virtual memory abstraction. And the way the DSM software is going to handle maintenance of coherence is by having distributed ownership for the different virtual pages that constitute this global virtual address space. So you can think of this global virtual address space as constituted by several pages, and we're going to say some number of these pages are owned by processor 1. Some number of these pages are owned by processor 2. Some number by processor 3, and so on. So we split the ownership responsibility into individual processors. Now what that means, is that the owner particular page is also responsible for keeping complete coherence information for that particular page and taking the coherence actions commensurate with that page. And the local physical memories are available in each one of this processors is being used for hosting portions of the global virtual memory space in the individual processors commensurate with the access pattern that is being displayed by the application on the different processors. So for instance, if processor 1 accesses this portion of the global virtual memory space, then this portion of the address space is mapped into the local physical memory of this processor. So that a thread that is running on this processor can access this portion of the global address space. And it might be that same page is being shared with some other processor n over here. In that case, a copy of this page is existing in both this processor, as well as this processor. Now it is up to the processor that is responsible for the ownership of this particular page to worry about the consistency of this page, that is now resident in multiple locations. For instance, if this node, let's say, is the owner for this page. Then this node will have metadata that indicates that this particular page is currently shared by both p 1 and p n. So that is the directory that is associated with the portion of the global virtual memory space that is being owned and managed by this particular processor. So statically, we are making an association between a portion of the address space and the owner for that portion of the address space in terms of coherence maintenance for that portion of the global virtual memory space. So this is the abstraction layer seen by the application that is giving this illusion of a global virtual memory. This layer is the DSM software implementation layer that implements this global virtual memory abstraction. In particular, this DSM software layer, which exists on every one of these processors, knows that the point of access to a page by a processor, who exactly to contact, as the owner of the page, to get the current copy of the page. For instance, let's say that there was a page fault on this processor one. For a particular portion of the global address space. That portion of the global address space is currently not resident here in the physical memory of processor one. So, there is a page fault over here and there is cooperation, as I mentioned earlier, between the operating system and the DSM. So, when the page fault happens, that page fault is going to be communicated by the operating system to the DSM software saying that here is the page fault, you handle it. What the DSM software is going to do is, it knows the owner of the page, and so it's going to contact the owner of the page. And ask the owner of the page to get the current copy of the page. So the current copy of the page resides over here. So the owner, either it itself has the current copy of the page or it knows which node currently has the current copy of the page, and so it's going to send the page over to the node that is requesting it. The current copy of the page is over here. It's going to come over to this guy, and recall what I said about ownership of the pages. The residency of this page doesn't necessarily mean that this is the owner of the page. The owner of this page could have been this node, so the DSM software would have contacted the owner. And the owner would have said, oh you know what. That particular place to cut and copy is on this node. So the DSM software would go to this note, and fetch this page, and put it into this processor. So that this processor is happy. So once the page has been brought in to the physical memory, then the DSN software contacts the virtual memory manager and says, I've completed now, processing the page fault, brought the page that is missing and put it into a particular location in the physical memory. Would you please update the page table for this guy, so that he can resume execution? Well, then the VM manager gets into action. And updates to page table for this thread to indicate that the faulty virtual page is now mapped to a physical page, and then the process or the thread can resume its execution. So this is a way coherence is going to be maintained by the DSM software. The cooperation between DSM software and the VM manager. And the coherence maintenance is happening at the level of individual pages. An early examples of systems that built software DSM include Ivy from Yale, Clouds from Georgia Tech Mirage from UPenn, and Munin from Rice. All of these are distributed shared memory systems and they all used coherence maintenance at the granularity of an individual page, and they used a protocol which is often referred to as a single writer protocol. That is, I mentioned that the directory associated with the portion of the virtual memory space managed by each one of these nodes and the directory has information as to who all are sharing a page at any point of time. Multiple readers can share a page at any point of time, but a single writer is only allowed to have the page at any point of time. So, if there is the writer for a particular page, let's say that loose page, which is now currently in the memory of two different processors, if this guy wants to write to this page, then he has to inform through the DSM software abstraction, inform the owner for this page. Let's say this guy is the owner for this page, that I want to write to this page and at that point the owner is going to invalidate all copies of that page that exist in the entire system, so that this guy has exclusive access to that page so that they can make modifications to it. So this is what is called single writer multiple reader protocol. Easy to implement. At the point of right to a page, what you do is, you go through the DSM software, contact the owner. And the owner says, I know who all have copies of the page, I'll invalidate all of them. Once it has invalidated all the copies, then the guy who wants to write to that page can go ahead and write to it, because that'll be the only copy. Now the problem with the single writer protocol is that there is potential for what is called fault sharing. We've talked about this already in the context of shared memory multiprocessors. Basically the idea of fault sharing, or the concept of fault sharing, is that data appears to be shared even though programmatically, they are not. Let's consider this page-based coherence maintenance. In the page-based coherence maintenance, the coherence maintenance that is done by the software, DSM software, is of the granularity of a single page. A page may be 4K bytes or 8K bytes,depending on the architecture we're talking about. And within a page, lots of different data structures can actually fit. So, if the coherence maintenance is being done at the level of an individual page, then we're invalidating copies of the page in several nodes in order to allow one guy to make modifications to some portion of that page. And that can be very severe in a page based system due to the coarse granularity of the coherence information. So for example, this one page may contain ten different data structures, each of which is governed by a distinct lock. So far as the application programmer is concerned. But, even if I get a lock, which is for a particular data structure that happens to be in this page. And this guy has a lock for a different data structure which is also on the same page. When I get the lock that is going to manipulate a particular data structure in this page, and if I want to make modifications for it, I am going go and invalidate all the other copies. When he wants to make a change, he's going to come and invalidate my copy of the page. So the page can be ping-ponging between, between multiple processes. Even though they are modifying different portions of the same page, still the coherence granularity being a page, will result in this page shuttling back and forth between these two guys, even though the application program is perfectly well behaved in terms of using locks to govern access to different data structures. Unfortunately, all of those data structures happened to fit within the same page. Resulting in this fault sharing. So, page level granularity and single writer multiple reader protocol don't live happily together. They will lead to fault sharing. And they will lead to ping ponging of the pages, due to the fault sharing among the threads of the application across the entire network. That brings us to a new coherence protocol, which is multiple writer protocol. So, the idea is we want to maintain coherence information still at the granularity of pages. Because that is the granularity at which the operating system operates, and therefore, the DSM can be integrated with the operating system if the granularity of the coherence maintenance is at the level of a page. But, at the same time, we want to allow multiple writers to be able to write to the same page, recognizing that an application programmer may have packed lots of different data structures within the same page. So we are going to see how the multiple writer coherence protocol works, and in particular we're going to use that in concert with these lazy release consistency. The background for what I'm going to describe is covered in the paper that is assigned for you, which is the Treadmarks paper. I encourage you to read that paper to get all the details. But here, I'm going to give you a high level view of how LRC is integrated with multiple writer protocol in the Treadmarks system. So the processor P1 acquires a lock and makes modifications. This notation that I'm using is to indicate that these pages, X, Y, and Z, actually they are data structures that are being modified, but we are maintaining coherence of the level of pages so we'll say that the data structures that we're modifying within this critical section are contained in pages X, Y and Z, and so those are the pages that are being modified within this critical section when processor P1 executes this piece of code. Now the operating system has no knowledge of the association between the lock, L, and the pages that have been modified. All that it knows, is that within the critical section these are the pages that were modified. That's what the operating system knows. And what we're going to do is, we're going to create a diff of the changes that were made to the pages x, y and z in this critical section. So we know at the beginning of this critical section what the contents of the page x, y and z is. And at the end of this critical section we're going to find out what is the difference that has been made, or what are the changes that have made and compute the diffs between the original page, and the modified page. Xd, Yd and Zd are the diffs to pages X, Y, and Z respectively as a result of executing this critical section. So, the coherence protocol we are going to use is LRC, or lazy release consistency. So the next time the same lock L is requested by some other process of P2 we're going to first invalidate the pages we know were modified by the previous lock holder, because this is information that is available to the DSN that at the point of unlock it knows that these were the pages that were modified by this critical section. It doesn't know what part of the pages are modified. That's contained in the diffs. But it knows that pages X, Y, and Z are associated with this lock L, and therefore, when P2 makes the lock request L, the DSN is going to first invalidate. If copies of pages x, y, and z are locally resident in the processor P2, then the DSM software is going to invalidate those pages x, y, and z at the point of lock acquisition. That's consistent with the lazy release consistency model. So, once we've invalidated these pages, then you can allow this guy to get the lock and start getting into its critical section. Now once it is in the critical section, it can do whatever it wants. But if it tries to access page X, at that point we know that page is invalid because we've done that at the beginning of this critical section, invalidated page X. And at this point, the DSM software also knows that the previous lock holder has the modifications that need to be made to the original page to get the current version of the page. The current version of the page is with some owner for this page. I mentioned this ownership based protocol in the DSM software. So the DSM software knows who the owner of the page is. From the owner of the page I can get the original content of X. I'll do that, but I'll also go and get the diff that is created by the execution of the previous critical section by the previous lock holder. So the DSM software brings at the point of access to X, Xd and the original version of the page from the owner of the page, it can then create the current version of the page for use by P2. Some of you may have thought of this already, and that is, prior to P two getting its lock, it is possible that maybe another processor, say P three, also used the same lock. And so, when it executed its critical section, maybe in its critical section, it modified the page X again, and it creates its own def, let's call it Xd prime. Because all of these locks are the same, When, the DSM software knows that now there are two difs associated with this lock, L, one dif is with the processor P one, and another dif is residing with processor P three, and therefore when processor P two tries to access x The DSM software has not only to get the diff from P one, but it also needs to get the diff from P two and apply it to the original, pristine version of the page that is with the owner of the page so that it can create the current version of the page. And it can extend this to any number of processors. That may have made modifications, their own modifications, to this page under the provision of the lock L. All of those diffs are going to be applied in order for the process of P two to access the page as the current page. If after accessing the page x and its execution P two. Touches let's say page Z at that point once again the [INAUDIBLE] knows that, oh, I know that Z was modified by the previous lock holder Zd is the diff I know where to find it I'll bring the original copy of z from... The owner of Z and apply the diffs to it before letting P two access Z. So you can see, that even though the invalidation was done right at the beginning, we're procrastinating getting the this til the point of access. So this is what LRC allows you to do is just bring in what this guy needs. So for instance, inside this critical section maybe only X is accessed. Y and Z are not accessed at all, in which case, we never bring the diffs from P one to P two for y and z.P two On the other hand, it is possible that P two, as part of its execution of its critical section. Modifies another page Q, different from X, Y, and Z. So now, the DSM software knows that this particular lock is associated not just with X, Y, and Z, but it is also associated with Q. So future lock request for L will result in invalidating X, Y, Z, and Q because all of those may have been modified and the next critical section that wants to access this lock L has to get the current versions of all the pages that were ever associated with L. So let's talk about the Multiple-Writer part of it. Note that it could be multiple user data structures present in a given page X. If that is the case, the programmer probably has different locks for accessing different portions of the data structures that happened to all fit within this page X. So it is conceivable that when all of this was going on. There was another processor, let's say P4, and a thread that is running on the processor P4 got a completely different lock, let's say L2. And it is accessing some data structure that happens to be in the same page X. This is perfectly fine. The DSM software Is not going to do anything in terms of the diffs that it has created with respect to the page X because of lock acquisition L. That's completely different set of actions compared to a different lock acquisition, say L2. So if in fact that other thread that is running on P4 executed in parallel with P1, got its lock, say L2, and modified x. When P2 gets its lock L, the liaison software is going to bring the dif only from the previous users of the same lock L. P4 was not using L. It was using L2 even though it accessed the same page. And modifying a different portion of that page. And therefore the DSM software is going to assume that that change made by P4 to x is irrelevant so far as P2's critical section is concerned. So that's the important thing, and that is where the multiple writer coherence protocol semantic comes in. That Simultaneously the same page could be modified by several different threads on several different processors. And that is perfectly fine, so long as they're using different locks. So the association between the set of changes to a page Is only to specific lock which is being used to govern that critical section and this is the reason why this is called a Multiple-Writer Coherence Protocol. And we saw how this Multiple-Writer Coherence Protocol lives in concert with LRC to reduce the amount of communication that goes on in executing critical sections of an application. I've always said that the fun part of any systems research is the technical details and implementing an idea. So let's look at some of the implementation details here. So what's going to happen is that when a process or a thread on a processor Tries to write to a page X. At the point of writing to that page X, the operating system is going to say this guy wants to write to this X, I'm going to make a twin for this page, so there's the original page, and then the twin for the same page, and the original page is writeable by this process. That mapping is there in the page table. This new copy, a twin, has been created in physical memory. It's not mapped into the page table of any process. It is just additional copy of the same page created by the operating system as a twin. So this page has been made writable and therefore the thread can make changes to the X, which is the original copy of the page. So the thread reaches the release point. So when the thread reaches the release point, what the DSM software is going to do, is compute the depth between the changes that have been made, and the original version. The original version, we created a twin, right? So this is the twin, and this is the original page, but this original page we made modifications to. X has now become x prime, and this is the twin, which is containing the page as it was before The thread started writing to it. So the DSM software at the release point, is going to compute the diff between the original copy of the page, and the modified copy of the page. And the diff is going to be computed as a run link encoded diff. Meaning that all of the Page is not been modified. It's only this portion and this portion of the page that have been modified. So the diff is going to be computed as oh, the page is changed starting from here up until here and starting from here up until here. This is the starting point for the change and this is the amount of change and this is the content of the change. The diff in the data structure that had been created by the DSM software to remember the changes that had been made to this data x prior to release as you may have imagined already when the same block That governs [UNKNOWN] to this page which was released over here is acquired by a different processor at the point of acquisition. What we're going to do is we're going to invalidate all the pages that were touched In this critical section, including x. So, x will be invalidated at the point of acquisition of the same lock that is governing this critical section. And when that processor has a page fault for page x. A DSM software knows that, oh, there is a Diff lying around on this node which is needed in order to update the page and give it to the current lock acquirer. So, that's part of what goes on under the covers in the implementation. But so far as this node is concerned When the release operation is done, at that point, the DSM software is going to compute the diff between changes made to this page and its original copy of the page and keep that as a diff data structure. And there are multiple pages, all of the diffs will be created. At the point of release. And once this thread that was in this critical section has completed its release operation, we will write protect this page X. We're write protecting it to indicate that this guy cannot write to it anymore unless he gets in the critical section and we have to do the coherence actions again. And that's the implementation of the protocol. And at this point we write-protect the original page and we can also get rid of the twin. The use for this twin is complete. We only needed it in order to compute this dift. We've computed it and we write-protected the original page and everything that needs to be done on this node is complete and we can get rid of this twin and getting rid of this twin essentially means That we are freeing up the physical memory that we allocated for creating the twin in the first place. I mentioned earlier it's a multiple writer protocol, which means that this action that's going on can be happening simultaneously for the same page X on different nodes of the cluster. That's perfectly fine so far as the protocol is concerned, because the assumption is that the user has an association between locks. And the data structures governed by the lock. So, even if the same page is being modified, hopefully different portions of the same page has been modified because concurrently if a page is being modified, that means that different locks are protecting the portions of the page that are being modified by the different processes. Now if writes are happening to the same portion of a page under different locks, that's a user's problem. That's a data race. That's not the problem of the DSM software. It's an application problem because it represents a data race that should not have been there if the application is constructed correctly. But if the application is constructed correctly and the multiple data structures are hosted in the same page and the data structures are all governed by different locks. DSM software has a way of ensuring that changes made to a critical section under a particular lock is propagated from one processor to the next processor where the first processor is the current owner of the lock, and the next processor is the next user of the lock. So this implementation that I've detailed here is an example of the cooperation between the distributed shared memory software and the operating system to make it all happen. And in particular, TreadMarks implemented this LRC multiple writer coherence protocol on a Unix system. And in the Unix system, the operating system generates an exception called SIGSEGV in the operating system layer when a shared page is accessed by a thread. This exception is caught by the thread block's runtime handler. And at that point the DSM software get into gear, contacts the owner of the page, checks the status of the page. And if the page is invalid then it gets the page and the difts for that page and once it brings in the contents of the page and the difts it creates a current version of the page. And if the process that is trying to access the page is making a read access, then there is no problem. But if the process that wants to use that page wants to write to it, at that point it creates a twin and does all the things that I just mentioned. So one thing that you will notice is that there is space overhead for the creation of the twin and the point of the write. You have to create a twin. And then at the point of release, you have to create, of course you can get rid of the twin, but you're creating a dift data structure. So, the twin and the difts are all data structures. Of the implementation of distriuted shared memory. And as time goes by, there could be a lot of these difts that are lying around in different nodes. Imagine that a page was touched by ten different processors. In that case, there are going to be difts lying around in ten different processors and if eleven processor wants to access the same page the DSM software has to go and bring the dift's from this ten prior users of the page. Get the original page from the owner. Apply the difts to create the new page. A lot of latency in, is involved before the guy who needs the page now can start using it. And also there is a lot of space over here in the fact that all these discs are lying around. So one of the things that happens in, in the DSM software Is garbage collection. And that is, you keep a watermark of what is the amount of difts that have been created in the entire system. If it exceeds a threshold then you start applying these dift's to the original copy of the page, at the owner, so that you can then get rid of the dift's completely. Why do you need that, well the difts that going to be lying it on till a long time till the next time the pages access by someone, you don't want that. So, what you're trying to do is, you're reducing the space overhead in the DSM implementation by periodically doing this garbage collection. And applying the difts to the original copy of the page so that he can get rid of it from the system. You don't want to do it too eagerly, but you don't want to wait too long also because if a page hasn't been accessed for a long time, difts are going to be lying around for a long time. So there will be a demon process in every node that every once in a while wakes up and sees how much difts have been created in my known. If it exceeds the threshold then it says okay time to get to work. Let me go and apply this difts to the original copy of the page so that I can get rid of the difts. In this lesson I've covered distributed shared memory and particularly I've given you a specific example of a distributed shared memory system called Treadmarks that uses lazy release consistency and multiple writer coherence. I just want to leave you with some thoughts about non-page based DSM systems before concluding this lesson. There have been systems that have been built that do not use granularity of a page for coherence maintenance. I mentioned earlier that if you want to maintain granularity not at the page level, then you have to track individual reads and writes that is happening on a thread. So one approach is what is called a library based approach. Here the idea is that, the programming framework, the programming library, is going to give you a way by which you can annotate shared variables that you're going to use in your program. Whenever you touch a shared variable part of creating the executable is to cause a trap at the point of access to the shared variable so that the DSM software will be contacted, and the DSM software can then take the coherence action at the point of access to that shared variable. So, in this case, there is no operating system support needed because in the binary itself we are making sure that at the point of access we're going to result in a trap that will get us into the trap handler that is part of DSM software so that it can take the coherence actions. And examples of systems that use this mechanism include Shasta, that was done at Digital Equipment Corporation, and Beehive which was done at Georgia Tech. And because we are doing this sharing at the level of variables, you don't have any fault sharing which is possible with page based systems and single write or cache coherence protocol. So once the DSM software takes the coherence action, which might include fetching the data that is associated with the variable you are trying to access, then the DSM software can resume this thread that caused this trap in the first place. Another approach to providing shared abstractions is not at the level of memory locations, but at the level of structures that are meaningful for an application. And, this is what is called structured DSM. So, the idea is that there is a programming library which actually provides abstractions that can be manipulated in an application program. And the abstractions can be manipulated using API calls that are part of the language runtime. So when the application makes the API call, at that point, when an application makes those API calls that point, the language runtime gets into gear and says what coherence actions do I need to make in order to satisfy this API call. All of those coherence actions are going to be taken at the point of that API call, and that might include fetching data from a remote node in the cluster. And once the semantics of that API call have been executed by the language runtime, then it's going to resume this thread that made the call in the first place. Again, there is no OS support needed for this. And the structured DSM is a very popular approach that has been used in systems such as Linda, Orca, and Stampede that was done at Georgia Tech, and successors to Stampede called Stampede RT and, and PTS. And in this course later on, we're going to see PTS as an example of a structured DSM system. DSM is providing the application developer with a programming model on a cluster that is akin to P threads on an SMP. It looks and feels like a shared memory threads package. That's good. But what about performance? Will the performance of a multi-threaded app. Scale up as we increase the number of processors in the cluster. Now, from an application programmer's point of view, your expectation is that, as you add more processors, you're going to get more performance. That's your expectation. And what we are doing is, we're exploiting the parallelism that is available both in the application because it's structured in that way. And in the hardware, in order to get increase performances as you increase the number of processors. But the problem is as you increase the number of processors because things are happening in software, there is increased overhead as well. And this overhead increases as the number of processors, so the actual performance is going to be actually much less than your expectation. Mitigated by the increasing overhead with the number of processors. And this buildup of overhead with a number of processes, happens in a true memory multi-processors. And this is even more true in the case of liason. Which is implementing the shared memory extraction in software on a cluster. So we have the allusion of shared memory, which is implemented by physical memories that is strewn all over the entire cluster, and the hope is that the application that is running on the different nodes of this cluster will actually get speed up with increasing number of processors, but such speed up is not automatic. If the sharing that we're doing, even though DSM gives you the ability to share memory across the network, recall what our good friend Chuck Thacker told us. Shared memory scales really well when you don't share memory. So, if the sharing is too fine-grained, then no hope of speed up, especially with DSM systems. Because it is only an illusion of shared memory via software, not even physical shared memory. Even physical shared memory can lead to overheads. So, this illusion through software can result in even more overhead so you've gotta be very careful on how you share and what you share. So the basic principle is that, the computation to communication ratio has to be very high if you want any hope of speed up. So in other words, the critical sections that are execute to modify data structures better be really, really hefty critical structures before somebody else needs to access the same portion of the data. So what does this mean for shared memory codes? Well basically, if the code has a lot of dynamic data structures that are manipulated with pointers, then it can lead to a lot of implicit communication across the local area network. You think you're executing code accessing a pointer, but the pointer happens to be pointing to memory that's in a remote processor. So that implicit access to a data structure pointed to by a pointer in your program, can result in a network communication across the network. Fetching something from here, into your local memory. This is the bane of distributed shared memory. That pointer codes may result in increasing overhead for coherence maintenance, for distributed shared memory in a local area network. So you have to be very careful on how you structure codes that can execute efficiently in a cluster using DSM as the vehicle for programming. In reality, DSM as originally envisioned, that is, a threads package for a cluster is dead. Structured DSM, namely, providing higher level data abstractions for sharing among threads, executing on different nodes of the cluster, is attractive, to reduce the programming pain for the developers of distributed applications on a cluster. We will discuss one such system, called persistent temporal streams, as part of a later lesson module. All of us routinely use file systems. Quite often, not the one that is on our local machine, be it a desktop or laptop, but one that is on a local area network, connecting us to a workplace or university. NFS, which stands for Network File System. And just like the word xerox is used often as a verb, to denote copying, NFS has become a generic name to signify any file system that we access remotely. So here is a trivia quiz for you. NFS is a name given to the first network file system that was ever built. In this question I want you to name the company and the year that NFS was first built. And the company choices for you are all familiar names: IBM, Sun Microsystems, HP, Apple, Microsoft, and Google. And here are the choices of years for you 1975, 85, 95, 2005. Some of you may have thought IBM because IBM has so many firsts to its credit. But the first network file system labelled NFS, was built by Sun micro systems, which was in the workstation business and it wanted a solution. For uses files to be accessible over a local area network. And so they built the first ever network file system and called it, very simply, NFS. And was built in 1985. Network file systems has evolved over time, but the idea is still the same. You have clients that are distributed all over the local area network and you have file servers sitting on the local area network and these file servers are central, so far as each client is concerned. Of course, the system administrator may partition these servers and say that there is one server designated for a certain class of users. For instance, if you take a university setting, you might have one server serving all the faculty's needs, and maybe another server serving all the student needs, but so far as a single client is concerned, it is still a centralized view, access to a central server over a local area network. Now, since the disk being electromagnetic is slow, the server will cache the files that it retrieves from the disk in memory, so that it can serve the clients better by serving it out of the file cache that is in memory rather than going to the disk all the time. So this is a typical structure of a network file system. A centralized server, which is the model used in NFS, is a serious source of bottleneck for scalability. A single server has to field the client requests coming from the group of users that it is serving and manage all the data and metadata for all the files that are housed on this particular server. And the data and the metadata of files are persistent data structures, and therefore, the file server has to access these data structures over the IO bus, which is available for talking to the disc sub-system. So, with a centralized server like this, there is limited bandwidth that's available for the server to get the data and the metadata in and out of the disc. And the file system cache is also limited, because it is confined to the memory space that's available in a given server. So, instead of this centralized view of the file system, can we implement the file system in a distributed manner? What does that mean? The vision with a distributed file server is that there is no central server any more. Each file is distributed across several servers. What does it mean to avoid the unscalability of a central server? We want to take a file and distribute it across several different nodes in the local area network. Since the DFS is implemented across all the disks in the network, if a client wants to read or write a file then it actually is contacting all the servers, potentially, to get the data that is looking for. And which means that since each file is distributed across all of these different servers, the idle bandwidth that's available cumulatively across all of these servers can be used to serve the needs of every individual client. Also, this allows distributing the management of the metadata that is associated with the files among the server nodes that are available. Furthermore, we have more memories available in all of these servers cumulatively, which means that we have a bigger. Memory footprint available for implementing a file cache, including all of the server memories, plus the memories that may be there in the clients as well. And that's where we can actually go towards cooperative caching among the clients as well. So, in the extreme, we can treat all the nodes in the cluster, whether we call them. S1 or c1, we can look at all of the nodes and say, they're all the same with interchangeable roles as clients or servers. That is, we can actually make this DFS a serverless file system. If we allow the responsibility of managing the files. Saving the files, cashing the files, equally distributed among all the nodes of the class here so the nodes are interchangeable between clients and servers. That brings us to this specific lesson that we're going to cover in this lecture. DFS wants to intelligently use the cluster memory for the efficient management of the metadata associated with the files. And for caching the file content cooperatively among the nodes of the cluster for satisfying future requests for files. In other words, what we would like to do, is since we know that a disk is slow, we would like to avoid going to the disk as much as possible. And retrieve the data from the memory of a peer in the network if in fact that peer has previously accessed the same file. And that's the idea behind cooperative caching of files. But in order to fully appreciate the question that we're asking and the answer that we're going to discuss in this lesson, it is important that you have a very good understanding of file systems. If you don't have it, don't worry. We have supporting lectures that will help you get up to speed, and you can get this knowledge by diving into a good under graduate textbook that covers this material well. And the textbook that is used in the under graduate systems course CS 2200 at Georgia Tech is a good resource for that. So to describe the ideas that are discussed in this distributed file system lesson, I have to introduce you to a lot of background technologies. The first background technology that I'm going to introduce you to is what is called RAID storage. RAID stands for redundant array of inexpensive disks. The idea is a given disk may have a certain IO bandwidth available. Now, if I can string together a number of disks in parallel, then cumulatively I can get much more IO bandwidth coming out of all of these disks. That's the idea of the RAID technology, which is redundant array of inexpensive disks. Since we have an array of disks, we are also increasing the probability of failures. And that is why in the RAID technology they also use an error correcting technology associated with RAID. And the basic idea, is that when you write a file, you're going to do the following: You take a file, let's say the file has four parts to it. What I'm going to do, is when I write this file, I'm going to write part one to this disk, part two to this disk, three to this and four to this. Because my data is on multiple disks, I'm also increasing the chance that there may be failures that can hurt me. And therefore, what we do is we compute a checksum for this particular data that I've stored on these disks and store that in a fifth disk. And this is what is called Error Correcting Code. So an Error Correcting Code allows errors to be detected when I read things from the disk and I see, oh, something is wrong I can correct it using this extra information that I'm writing on the fifth disk here. So that's the big picture of how striping a file to multiple disks works. Basically, what we do is we take a file and decide that if it is going to be striped over four disks, we stripe it on the four disks and we also write an error correcting code for the data that we have striped across these disks. So that if in fact there is an error that manifests itself in the future, with anyone of these disks that error can be corrected using this error correcting data that we have written to augment the original data. So failure protection is being achieved through this error correction code. That's the idea of striping a file in the RAID system, the drawback in the RAID technology is first of all the cost. The fact that we have to have multiple hardware drives in order to store a single file. And the second problem is what is called a small write problem and that is if my file is really really small, then I'm saying that a part of this file is going to be written on each one of these disks, and that's inefficient in terms of how you store data. And the reason why it is inefficient of course is the fact if it is a small file and I've striped it across multiple disks, in order to read this small file, I have to get data from all of these disks, and that's inefficient. And that's the thing that is detrimental about the hardware RAID in terms of handling a normal file system that may have a population of small files and large files, and so on and so forth. But the idea of having an array of disks to serve the file system is a good one, because it increases the overall I/O bandwidth that's available in the server. So, how can we solve the small write problem? That brings me to another background technology that I have to explain to you and which is called log structured file system. The idea here is that when I make a change to file y meaning I either append to the file or make some modifications to it. What I'm going to do is rather than writing the file as is, I'm going to write the change that I made to the file as a log record. So, I have a log record that says, what are the changes I made to this file x. Similarly, I have a log record of all the changes I made to this file y. And this is being done in a data structure which I'll call log segment. And I'll keep this log segment data structure in memory, of course, to make it fast in terms of the file system operation. So with this log segment data structure, what I can do is, buffer the changes to multiple files in one contiguous log segment data structure. So this log segment data structure, I can write it out as a file, and when I write it out, I'm not writing a single file, but I'm actually writing a log segment which contains all the changes made to multiple files. And because the log segment is contiguous, I can write it sequentially on the disk and sequential writes are good in the disk subsystem. And what we want to do is, we want to gather these changes to files that are happening in my system in the log segment in memory, and every once in a while, flush the log segment to disk, once the log segment fills up to a certain extent, or periodically. And the reason, of course, is the fact that if it is in memory, you have to worry about reliability of your file system, if, in fact, the node crashes. And therefore, what we want to do is, we want to either write out these log segments periodically or when a lot of file activity is happening and the log segment fills up very rapidly. After it passes of threshold, then you write it out to the desk. So in other words, we use a space metric or a time metric to figure out when to flush the changes from the log segment into the disk. And this solves the small write problem because if y happens to be a small file. No problem, because we are not writing y as-is on to the disk. But what we are writing is this log segment that contains changes that have been made to y in addition to changes that have been made to a number of other files. And therefore, this log segment is going to be a big file. And therefore, we can use the RAID technology to stripe the log segments across multiple disks. And give the benefit of the parallel IO that's possible with the RAID technology. So this log structured file system solves the small write problem. And in log structured file system, there are only logs. No data files. You'll never write any data files. All the things that you're writing are these append only logs to the disk. And when you have a read of a file, the read of a file, if it has to go to the disk and fetch that file, then the file system has to reconstruct the file from the logs that it has stored on the disk. Of course, once it comes into the memory of the server, then in the file cache the file is going to remain as a file. But, if at any point, the server has to fetch the file from the disk, it's actually fetching the log segments. And then reconstructing the file from the log segments. That's important. Which means that, in a log structured file system, there could be latency associated with reading a file for the first time from the disk. Of course, once it is read from the disk and reconstructed, it is in memory. In the file cache of the server, then everything is fine. But the first time, you have to read it from the disk, it's going to take some time because you have to read all these log segments and reconstruct it and that's where parallel RAID technology can be very helpful, because you're aggregating all the bandwidth that's available for reading the log segments from multiple disks at the same time. And the other thing that you have to worry about, when you have a log structured file system, is that these logs represent changes that have been made to the files. So, for instance, I may have written a particular block of y and that may be the change sitting here. Next time, what I'm doing is perhaps I'm writing the same block of the file. In which case, the first strike that I did, that is invalid. I have got a new write of that same block. So, you see that over time, the logs are going to have lots of holes created by overwriting the same block of a particular file. So in a log structured file system, one of the things that has to happen is that the logs have to be cleaned periodically to ensure that the disk is not cluttered with wasted logs that have empty holes in them because of old writes to parts of a file that are no longer relevant. Because those parts of the file have been rewritten, overwritten by subsequent writes to the same file. So logs, as I've introduced you, is similar to the disks that you've seen in the DSM system with the multiple writer protocol that we talked about in a previous lecture. You may have also heard the term, journalling file system, there is a difference between log structured file system, and journalling file system. Journalling file systems has both log files as well as data files, and what a journalling file system does, is it applies the log files to the data files and discards the log files. The goal is similar in a journaling file system, and the goal is to solve the small write problem, but in a journaling file system, the logs are there only for a short duration of time before the logs are committed to the data files themselves. Whereas in a log structured file system, you don't have data files at all, all that you have are log files and reads have to deconstruct the data from the log files. The next background technology that I'm telling about is software RAID. I mentioned that hardware RAID has two problems. The first problem being small writes and we said we can get rid of the small write problem by using log structure file systems. But the hardware RAID also has another problem and that is it is employing multiple hardware drives. And hardware RAID, generally speaking, is very expensive proposition. On the other hand, in a local area network, we have lots of compute power distributed over the LAN and every node on the LAN has associated with it are disks. So could we not use the disks that are available on local data network for doing exactly the same thing that we did with hardware RAID? And that is, stripe a file across the disks of all the nodes that are in the local area network. So that's the idea behind the Zebra file system that was built at UC Berkeley, which was the first one to experiment with this software RAID technology. It combines both lock structure file system and the RAID technology, lock-structured file system in order to get rid of the small write problem, and the RAID technology to get the parallelism you want in a file server to be able to read from the disks in parallel, so that you can reduce the latency for serving client requests. The idea here is that you're going to use commodity hardware available as nodes connected to disks on a local area network. And since LFS, lock structured file system, is good for getting rid of the small write problem, we are going to employ LFS as the technology for the file server. So in this case, what we have are not data files but we have log segments that have to be written out to the disk in an LFS. And what we're going to do is we're going to stripe the log segment on multiple nodes of the disks in software and that is the RAID technology. So if this is a log segment that represents the changes made to several different files on a particular client node, then the software RAID, what it will do, is then take this log segment and stripe it. Part one of the log segment on this node, part two on this, part three on this, part four on this and the ECC that corresponds to these four parts of the log segments into a fifth drive. That's the idea in software RAID. Exactly similar to the hardware RAID except software is doing this striping of the log segment on multiple nodes available in a local area network. Now it's time to put together the background technology that I introduced to you, plus some more, and describe to you a particular distributor file system called XFS which is also built at UC Berkeley. XFS builds on the shoulders of periodic technologies. The first one, the log based striping that I just mentioned to you from the Zebra file system. And another technology called co-operative cashing which is also a prior UC Berkley project. And in addition to these two technologies, XFS also has introduced several new nuances in order to make the distributor file system truly scalable and get towards what is called serverlessness, or in other words, no reliance on a central server. And those techniques include dynamic management of data and metadata. Subsetting of the storage servers, and we'll talk about all of these techniques in much more detail in the rest of this lecture. To motivate the need for dynamic management of data and metadata, it's useful to look at the structure of a traditional NFS server which is centralized. In a traditional centralized NFS server what you have is the data blocks are residing on the disks. So in the memory of the server, the contents include the metadata for the files like the iNote structures. And file cache which is files that have been brought in from the disk are stored in the memory in what is called the file cache. So that future requests for the same files can be served from the memory of the server rather than going to the disk. And the server also keeps the client caching directory. That is, who on the local area network are currently accessing files that is the propriety of this particular server. And in a Unix file system the server is unconcerned about the semantics of file sharing. In other words, the assumption is that the server is caching for each client completely independently. And therefore if clients happened to share a file that is completely the problem of the clients, and the server is not concerned about that. So all of these contents that I just described to you, metadata, file cache, client caching directory. All of these are in the memory of a particular server. And if the server happens to be housing hot files used by a lot of users that is being served by this particular server. Then that's bad news for the server in terms of scalability, because it has to worry about the requests simultaneously coming from lots of clients for these hot files. And so it is constrained by the bandwidth that's available to access the files from the disk. It is constrained by the amount of memory space it's got, for caching files, and the metadata of the files, and so on. At the same time, there could be another server that also has adequate bandwidth to the storage, and, and memory space. But unfortunately it may be housing cold files. And therefore, there are not many clients for this server. So you can immediately see that the sort of centralization of the traditional file system results in hot spots. And that's the thing that we're trying to avoid in a distributed file system, and that's where dynamic management comes into play. So in XFS, it provides the same functionality as a centralized NFS Server, but it is distributed and the metadata management is dynamic. And that is, in a centralized file server, the mapping between the manager node for a file and the location of the file is the same. Or in other words, if the file happens to reside on the disk of this server, then this server is the guy that is going to handle the metadata management for this file as well. On the other hand, in XFS, metadata management is dynamically distributed. So, let's say that you have F1, F2, and F3 are the hot files. In that case, metadata management for files F2 and F3 can be done by some other node, say S3. And this server may have the cache for the file. So, in other words, all the data structures that we've talked about that has to reside in the memory of a particular server like metadata, file cache, and the caching information about who's having the files and so on. All of that can be distributed with dynamic management of data and metadata, which is the idea in XFS. And I'll shortly explain how exactly this dynamic management is facilitated by the implementation of XFS. So, in any systems research, there is always first the idea and then there's implementation. So the idea in XFS is that we want to manage the data and metadata management dynamically, and we'll see how that is done. And also, what we want to do is we don't want the cache for the files to be only at the server. What we would like to be able to do is, if a file is accessed by several different nodes, then they're living in the client caches of the different nodes. If a file is residing in the cache of a peer node, then it makes sense that if a new request comes from the same file, then getting that file from a peer cache may be much more efficient than getting it from the disk. And that way we can also conserve the total amount of memory that's available on the servers and use it more frugally by exploiting the memories that are available in the clients, so that the caching of the files can be done cooperatively among the clients. And that's the other nugget in the technical contribution of XFS, is the cooperative client caching. I mention that XFS uses log based striping in software. So let's understand that technology in a little bit more detail. You have clients on the local area network that are writing to files. When a client makes a change to a file. The changes that are made by this client to files are written to an append only log. And this append only log is a data structure. This log segment is a data structure in the memory of the client in the distributed file system and this append only data structure contains ,the changes made to files on this client node. And for instance, this could be a change for a particular file X, this could be a change for another file Z and so on, and so forth. And this is an append only data structure that is residing at this client. Similarly, there is a log segment that is available at this client. For the changes made to files on this node, and when this log segment fills up beyond a certain capacity, you decide on a certain threshold, and once that threshold number of fragments have been written in this log segment, then you decide that it's time now to write to the disk. So you take these log fragments,and compute the parity, which is the check sum or ECC, whatever you may want to call it, and this becomes the log segment that I want to write to the disk. And you take this and stripe it across storage servers. So you take this,particular log fragment along with its ECC and stripe it on storage servers so that it is now available on the storage system and as I mentioned, you want to do this periodically in order to avoid the chance of [INAUDIBLE] Data loss due to failures of a particular node, and that's what you're doing every periodically. Same thing is happening on this so if you look at the storage server, you see that the storage server has the log segments that have been written by this client, and the log segment that has been written by the log segment that has been written by this client. All of this, gets into the storage servers, and the other thing that you want to do is, you don't want every log segment to be written on all the disks available on the local area network, you don't want to do that. This again is concerned with solving the small rate problem, and therefore, what we want to do is subset the storage servers, and say if, let's say i have 100 storage servers available on the local area network. I might decide that every log segment is going to write its log over a small fraction of that maybe 10 servers. And this client may similarly choose a subset of storage servers to write it on. The subset of storage servers that is used for striping a given log segment is called a stripe group, for that particular log segment. So, using the stripe group for a long segment avoids first of all the small-ray pitfall. So for instance here, we might decide for certain log segments x, y, and z the stripe group is this. And for another set of log segments, say p, q, and r the stripe group is this. And for another set of log segments, L, M, and N, the stripe group is this. So we're subsetting the servers into these stripe groups, and what that allows is parallel client activities. If the log segments X,Y, and Z, belong to a particular client. And if the log segments P, Q, and R, belong to, a different client, and L, M, and N, belong to, a different client then you can see that the client activity corresponding to this, this particular stripe group can go on and parallel with this. And similarly this activity can go on and parallel with other stripe groups that exist in the system and so on. And so it increases the availability of the server in saying that not all the servers has to be working on the same client request. Different subset of servers are working on different client requests and that results in higher throughput for overall client processing. And also. When it comes to cleaning the logs, remember that I mentioned earlier that every once in a while, you have to clean the logs because the logs may have been overwritten by new rights to files on a particular client machine, in which case there are logs that Have to be recycled. And if you don't recycle them, you're filling up the discs with junk. And therefore, every once in a while, we have to go and clean up the log. And so efficient log cleaning is again facilitated by the fact that you have different stripe groups, so you can assign different. Cleaning service for different stripe groups that increases the parallelism in the management of all the things that need to be done in a distributed file system. An increased availability also means that you can survive multiple server failures. So let's say that these two disks fail for some reason. You can still serve the clients who are being served by this particular strip group. That's the idea of sub-setting the server group for striping so that you increase the availability and allow incremental satisfaction of the user community in spite of failures that may be happening in the system as a whole. Next lets talk about how [UNKNOWN] uses the memory's available in the clients for cooperatively caching the files and reducing the stress on the management of data files. In [UNKNOWN] as supposed to traditional Unix file system, they also worry about the coherence of files. I mentioned earlier that in Unix file system, the file server assumes that it is serving each client independently, and therefore it doesn't worry about sharing a file, if a particular file happens to be access by multiple users at the same time, the server doesn't worry about the coherence of that file. But on the other hand in XFS the file system worries about cache coherence and I have already introduced the idea of cache coherence in the context of multi processes and distributed shared memory. So you are familiar with the terminology single writer, multiple reader meaning that a particular file. We have, at any point of time, only a single writer. There cannot be multiple writers to the same file, but it can have multiple readers at the same time. And the unit of cache coherence that XFS maintains is at the level of file blocks, not an entire file. But at the level of individual file blocks. So if you look at, a manager for a file, the guy that is responsible for the metadata management for the file. It has information about the files for which it is the manager. Let's say it has a file f1 for which it has a mana, it is a manager. Then the metadata in the memory of this manager will have information about the current state of that file. For instance, this particular entry says that a file f1 managed by this manager is being read concurrently by two different clients, c1 and c2. So there are two different clients, c1 and c2, and they have copies of this file that they have retrieved from this manager at some point of time, which means that the client caches, c1 and c2, contain the contents of this file. Now for simplicity, I'm showing this as a file, but in fact the granularity at which the coherence and information about files is kept is at a file block level. So at a block level the manager says a particular block of a file is in the cache of client C1 and in the cache of client C2. And yet I'm using the word cache to mean that it is in the memory. Of these clients. So the semantics that is observed for cache coherence, a single writer, multiple readers. So if client c3 makes a request to the manager for writing to this file, f1, and again, I have to mentioned that, the request is going to be at the granularity of a file block, but for simplicity, I'm showing it as a write request for this file f1. But you understand that the granularity at which this request is being made is for writing a particular block of that file. So the manager gets this write request. It looks up the metadata for that particular file. And it sees that this file is now currently read-shared by two different clients, c1 and c2. This guy wants to write to the file. That results in a conflict, a read/write conflict, and what the manager is going to do is basically say, well, if somebody wants to write to that file I have to tell the guys that currently have the file. They cannot have it anymore. So just as in the case of cash coherence in a multi-processor. This manage is going to send an invalidation message for the file F1 to C1. And to C2, and they are going to acknowledge to the manager saying that yes, we have invalidated our local copies of the files, and once the manager gets that indication back from the clients, at that point the manager can tell the client C3 that Okay, now you have got dibs on writing to this file. That's the protocol that is being observed in [UNKNOWN]. To keep the copies of the files consistent so at the end of this exchange C3 will have the right to, right to this particular file. Now how long does it have that privilege? Well the write request when it is granted the client gets the token And the manager, at any point of time, can revoke the token that was given to C3. And this in particular will happen when a future read for the same file comes to the manager. At that point, the manager will go to C3 and say, I'm revoking the token from you. You cannot write to that file anymore. You can read it because the request that I got is only a read request and therefore you can keep the file, but you cannot write to it anymore. If you want to write it again, then you have to make a request again. This is the protocol that is observed. And of course, if a particular client is writing to a file, and another client also wants to write to the same file, at that point the manager once again is going to invoke the token, invalidate the file at this client, pull back the contents of the file, and then distribute it to a future requester who wants to write to the same file. That's how cache coherence works. And using the fact that copies of the file is existing in multiple clients excepts as exploits that fact to do cooperative caching. What that means is that if a client is currently having a copy of the file, lets say, after this interchange. C3 has a copy of the file. There is also writing to A future read request comes. When it comes, that read request can be satisfied by getting the contents of the file from the cache of C3. And that is what cooperative caching is all about, where instead of going to the disk to retrieve the file We can actual get the file content from the cache of one of the clients that happens to have a copy of the file. I mentioned log cleaning that has to be done. Now as client activities go on, log segments evolve on the disk. So for instance, if on a particular client node some blocks were written to the log segment, a log segment may fill up like this. And now it is sitting on the disk. So the blocks that are containing this log segment corresponds to write to file blocks one, two, and five. And these file blocks may belong to different files, but it is okay. So, so far as the file system is concerned, segment one is a contiguous file. It is a log segment, but it is a file. And that's the one that is residing on the disk. On the client node, this particular block one may get overwritten due to activity on the client. So now we have a new content for that same file block one, one double prime. And once this new content has been created, this is no longer valid, and so block one is overwritten, which means we have to kill the old block that was in this segment. So there's a new segment in which we wrote the contents of the new file block, one double prime, and we have to go back to this old segment and kill this particular copy of that block, which is a stale copy of that same block. So we don't need that anymore. So you can that see as client activities progress we are going to create holes in the segments. Remember that these segments are persistent data structures on the disk. This segment was valid at some point of time. But once that particular block one was overwritten on the client node, this segment contains the latest and the current copy of that same file block. And therefore we nuke this particular file block and so we create hole in this log segment. And subsequently, let's say that the client writes to other file blocks, three and four. So the log segment two contains one double prime, three prime, and four prime are the blocks that it contains. And segment one contains two prime and five prime. One prime is not relevant anymore because it has been overwritten by one double prime. Activities continue on the client box and a third segment is created. And that third segment contains two double prime. That is, this block number two is overwritten by new contents, two double prime. And when this happens the file system has to create another hole by nuking this two prime to indicate that this block is not relevant anymore, because there is a more recent version of the block in segment three. So the block two is overwritten, killing the old block. Now you see that as client activities progress in the entire distributed system, we are creating a number of these segments, log segments on the disk, and these log segments may progressively have holes in them because they have been overwritten by new contents, by activities on the client machine. And this is what log cleaning is all about. It has to do with cleaning up the disk and getting rid of all of this junk so that we don't fill up the disk with unnecessary junk. So for example, what we want to do is recognize that we have three segments here, and the segments have holes in them. And what we are going to do is, we're going to aggregate all the live blocks from all of these segments in to a new segment. So we've got five from this segment that is still alive, and from this segment we've got one double prime, three prime, and four prime that are still alive. And from this segment we've got two double prime that is still alive. So we have coalesced all of the live blocks from the existing segments into one new segment. Now once we have aggregated this into one new segment, all of the old log segments can be garbage collected and that's what log cleaning is all about. And this is very similar if you think about it to the way we described cleaning up the diff files that are created in the DSM system, Treadmarks. And the same thing is happening except that these data structures are on the disk we are conserving the space on the disk by getting rid of all the old log segments and garbage collecting them and saving this space once we've aggregated all the live blocks in these segments into a new segment. So this is what log cleaning is all about and in a distributed file system, there is a lot of garbage that is being created all across the storage service. And we don't want this to be done by a single manager. We would ideally like it to be done in a distributed manner. This is another step towards a true distributed file system by making this log cleaning activity also a distributed activity. So log cleaner's responsibilities include the following. It has to find the utilization status of the old log segments. Then it has to pick some set of log segments to clean, and once it picks a certain number of log segments to clean, it has to read all the live blocks that it finds in these log segments that it has chosen for cleaning, write it into a new log segment. And once it has done that, it can garbage collect all of those log segments. So this is the cleaning activity that an LFS cleaner has to do and in XFS they distribute this log cleaning activity as well. Now remember that this log cleaning activity is happening concurrently with writing to files on the nodes in the distributed system. So there are lots of subtle issues involved in managing this log cleaning in parallel with new activity that may be creating new log segments, or writing to existing log segments. I encourage you to read the paper that I've assigned to you, the XSF paper, to get a good feel for all the subtleties that are involved in managing log cleaning concurrently with writing to the files. So in XFS they may make the clients also responsible for log cleaning. There is no separation between client and server. Any node can be a client or a server depending on what it is doing and each client, meaning a node that is generating a log segments, it is responsible for the segment utilization information for the files that they are writing. After all the activity is happening at the client end in terms of creating new files, and new file writes are manifesting as creating blocks and log segments in XFS. And so the clients are responsible for knowing the utilization of the segments that are resident at that client node. And since we have divvied up the entire space of servers into stripe groups, each stripe group is responsible for cleaning activity that is in that set of servers. And every stripe group has a leader, and the leader in that stripe group is responsible for assigning cleaning services to the members of that stripe group. Recall that the manager is responsible for the integrity of the files because it is doing metadata management. And, requests for reading and writing files are going to come to the manager. On the other hand, the log cleaning responsibility is going to the leader of a stripe group, and the manager is the one that is responsible for resolving conflicts that may arise between client updates that want to change some log segments and cleaner functions that want to garbage collect some log segments. Those conflicts are resolved by the manager. These again are subtle details which I want you to read carefully in the paper. Next let's talk a little bit about the implementation details of XFS. First of all, in any Unix file system there are these i-node data structures which give you a mapping between the file name and the data blocks on the disk. So given a file name and the offset that you want to get into that file, the file system has a way of looking up a data section called i-node and deciding where exactly on the disc of the data blocks that you're looking for. This is happening in any Unix file system. xFS data structures for implementing a truly distributed file system are much more involved. Let's talk little bit about that, first of all, I mentioned that the metadata management is not static. Even though, the file that you are looking for may be resident in particular node, the manager for that file may not be at that same node. The client action when it is looking for a file, it starts with a file name and this is a data structure that is a replicated data structure at every node in the entire distributed system. So any client node, when it starts with a file name. It consults this manager map data structure to know who's the metadata manager for this particular file name. And the manger node action is fairly involved. So, the client comes to the manager with a file name. And when you come to the manager with a file name, the manager looks up the first data structure, called the file directory. And that file directly has the i-node number. And that i-node number is the starting point for looking up the contents of that file. Now lets talk about all the data structures that are used by the manager node. On the manager node, when the client presents the file name, the manager node uses a data structure called a file directory. To map that file name to an i-number. And from the i-number, it uses another data structure called i-map data structure to get the i-note address for this particular file name. The i-node address is the i-node address for the log segment associated with this file name. And using this strip group map, which is telling. How this particular file is striped. It can locate the storage server that contains the log segment ID, that is associated with this file name. I mentioned earlier, that every log segment is actually striped on a whole bunch of disks, a stripe group. What of the stripe group associated with this particular log segment? That's the information that it gets from the stripe group map. Once it has the set of storage servers that contain this log segment, it can go to the set of storage servers to get the data blocks associated with this particular file name. That's the entire road map of what the manager will have to do. To go from the file name to actual data blocks that corresponds to that file name. Now this sounds like a lot of work being done, fortunately caching helps in making sure that this long path is not taken for every file access. We will see that, in terms of how reads and writes happen. In the XFS file system. Just to recap the data structures, file name to i-number mapping is contained in this data structure, FileDir. The mapping between the i-number and the i-node address for the log segment ID, that is contained in this i-map. Given the i-node address, I can consult the stripe group map to know which storage server actually has the i-node for this file name that is a large segment that corresponds to this file name. I can get that. And once I get that, then I can find out the stripe group that is associated with this log segment, and that'll say what are all the storage servers that I have to contact in order to get the contents of that log segment. Then I can go to those storage servers and get all the data blocks that correspond to a particular log segment. So that's the road map. As I said, fortunately, every file read is not going to result in going through so many hoops to get the data blocks. This is where caches come into play. So when you start with a filename and, and an offset on a client node, you look up the directory. And from that, you get an index and an offset. So, this is a data structure which is in the client memory, and once you get the index in the offset, and if this file has been accessed before, it's most likely in its own unique cache. This is a file cache of the file system, and if it is in your cache, then you get the data block. In other words, going from the file name to the data block that is associated with the file is all happening through the client memory because of local caching. And there's a fastest path for file access and hopefully there's a common case. Now it is possible that a file is shared. Or the same file is being read by different clients at different points in time, but in either case, there is a possibility that a particular file has been accessed by a client and therefore in the cache of that client, and so the next possibility is that you start with a directory You don't find it in you local cache. And if you don't find it in your local cache, then you have to go to the manager node in order to get a copy of the file. So this is where the manager map data structure, which is a replicated data structure, is available in the memory of every client and so given the index number and the offset. I can go and look up the manager map data structure and that tells me who's the manager that I have to contact to get this particular file. So this might involve a network cop because the manager node is different from the local node. Then I have to go to the manager node across a network. And when I get to the manager node, the manager node may say, oh you know what, this particular file, has been accessed by a different client. I know that because my metadata says that some of the client has got this, in their cache. So, the manager will tell the client that currently is holding a copy of the file in its cache. To please send the data over to the first guy that requested it. Now, the data that I requested is coming not from my local cache, but it is coming from the cache of a peer. And that's much better than going to the disk and putting it out of the disk, because network speeds are much faster than accessing the disk. So this is the second best path for file access. Sure, there is network communication involved here because, the first thing I that have to do is I have to hop over the network to get to the manager node if in fact I am not the manager node myself for this particular file that I'm looking for. That is a network hop. And there could be another network hop if the manager says, oh, this particular file is cached in a different node. In that case, there is another network hop to go to the client that currently containing that particular file. And once we get to that, there may be another network hop in order to send the data over to the requester. So potentially there could be three network hops in order to get The file that I'm looking for, but it could be less than that depending on the core location of the manager and the node that is requesting it or the manager and the node that contains a copy of that file itself. So this is the second best path for file access. But there is also the pathologically 'real long way' of accessing a particular file, and that path is shown here, using all of the data structures that I mentioned earlier that is available at the manager. You start with a file name, look up the directory, get the index number and the offset. It's not in your cache. You go to the manager by looking up the manager map data structure that's in my local memory. I go the manager and the manager then looks up its metadata for this file, finds that nobody has it in the cache So it has to pull it from the desk. If it has to pull it from the desk, then it has to look up its imap data structure, the imap data structure, and the stripe group map data structure in order to find out the location of the I node that corresponds to the log segment that I am looking for. That look up happens through the imap data structure, stripe group map data structure, then I can to the storage server and get the index node of the log segment ID for the requested data block of this client. Once I have that, then the manager has to look up the stripe group map to see what storage servers. Have this log segment striped, and out of that which storage server should I contact for the particular portion of the file that you're looking for? And that storage server will be contacted and that storage server is going to give the data block that is requested by the client. So you can see that in this long path, there is network hop as well as accessing the storage servers to pull the data blocks. It is possible that the index node for the log segment ID associated with this file has been previously accessed by this manager. In which case, it doesn't have to go to the storage server to get the Index node for the large segment, because it'll be present in the memory of the manager as part of its caching strategy. And therefore it can bypass these two network hops because directly from this strip group map it can figure out what the long segment ID is locally cached so that it can then go to this stripe group map data structure and figure out. Where on the disk the data blocks for that log segment is actually secured, so we might be able to get rid of at least two of these network hops if you're lucky and this particular log segment has been accessed before by this manager, it'll be in the memory of the manager. And therefore we can avoid these two network hubs. But the worst case scenario, if this particular log segment is never being accessed before then the long way to get to the data block that you're requesting is going through the data structures in the manager, network hub, storage look up, and get the data and give it to the client. Writing to a file by a client is fairly straightforward. What the client is doing is actually aggregating all the writes that is going on into this Log Segment Data Structure, which is in its memory. And at some point, it decides that it wants to flush this Log Segment. And put it on the disk. And when it decides to do that, it knows the Stripe Group, this particular Log Segment it belongs to, and so it is going to take this Log Segment and stripe it on the Storage Servers that are part of the Stripe Group. So once it does this write to its Stripe Group. The client will notify the manager on the lock segments that are being flushed to the disc so that the manager has up-to-date information about the status of the files it manages. So XFS is a research prototype of a Distributed File System. There have been other instances of Distributed File Systems such as. The Android File System and the Coder File System that were built at CMU and in fact the Android File System served a community of Users in the CMU campus in a little lesson we will return to discussing Distributed File Systems again. Specifically we'll look at Andrew File System and discuss the issue of security and privacy for User Files in a Distributed System. But I want to leave you with some closing thoughts on the Exophus File System. First of all, Log based striping, and particularly sub-setting the Storage Servers over which you'll stripe the Log. That's a Technical Innovation. The second Technical Innovation is combining Cooperative Caching with Dynamic Management of Data and Metadata. And the last technical nugget is the distributive Log cleaning, making sure that the responsibility for cleaning up the Logs on the Disk is not. Left to one Node, but it is actually distributed and especially taking advantage of the fact that the clients, who are the mutators for the file system, meaning they are the guys that are writing to the file system, they can keep a count of the changes that they are making. To the Log Segments and use that information in the Log Cleaning effectively. Today, network file systems are an important component of any computing environment, be it a corporate setting or university setting. There are companies that have sprung up such as NetApp solely to pedal scalable NFS products. In this lesson, going beyond NFS we learned a lot of concepts pertaining to the design and implementation of distributed file systems, in particular how to make the implementation scalable by removing centralization and utilizing memory that's available in the nodes of a local area network intelligently. Such techniques for identifying and removing bottlenecks are the reusable nuggets that we can take and apply to the design and implementation of other distributed subsystems in additions to file systems themselves. Overall, in the set of papers that we studied in this lesson module spanning, GSM, DSM and DFS, we discussed the design and implementation of subsystems that found creative ways to fully utilize memory that's available in the nodes of a local area network. Hello and welcome back to the next module of the operating systems course. And this module is on system recovery. System crashes can happen due to power failure, hardware, and software failures. As operating system designers, it is essential to understand how to build systems that can survive crashes. In this module, we will discuss technologies that will help us deal with system failures and recover from them. In this module, we will cover three systems. The first one, LRVM suggests providing a persistent virtual memory layer in support of system services. The second one RioVista, suggests how such a persistent layer can be implemented in a performance conscious way, with some ingenuity. The third one, Quicksilver, suggests a more radical approach. Making recovery a first class citizen in the design of operating systems. The first question that popped to your mind whenever we suggest a solution approach is why? In particular, whose pain point are we trying to solve? It turns out that many operating system's subsystems need persistence. For example file system, it has metadata such as inodes that say where and how files are actually stored on the disc. And this data structure, which is the inode data structure, is a persistent data structure and while the operating system may manipulate that data structure by bringing it into main memory for performance reasons. Finally, it has to put it back on the disk in a persistent state once permanent changes have been made to that data structure. And that's the reason why persistence is very, very important for building several such subsystems. So, a file system I gave you as an example. Similarly runtime systems of many languages may require support for persistent objects that need to be stored on permanent storage. And all of these subsystems, what they would do is in order to make it efficient from the point of view of your processing, they will have in memory a cache copy of the persistent data that lives on permanent storage such as the disk. But, if and when changes are made to those data structures that are in memory, they have to be committed back to the permanent storage so that there is consistency for actions that are being taken by these subsystems. So how can we provide persistent support for these subsystems? Well, one possibility is to make virtual memory persistent, because if you make the virtual memory system persistent then all the data structures that are contained in the virtual memory become automatically persistent. Now the upshot of doing that would be that the subsystems do not have to worry about flushing the persistent data structures from memory into the disk because some other layer in the software stack is going to take care of it if in fact we make virtual memory persistent. That also means that if the system were to crash either due to power failure or due to problems in the software, meaning software bugs, recovering from such crashes becomes easy because the data structures are persistent and anything that was done in the virtual memory of the process that represents that subsystem is going to be available in the persistent storage. The question is then, who will use it? Well, we are expecting that the subsystem designers will use such an abstraction if we provide it. But subsystem designers also care about performance. And therefore, subsystem designers will use it only if the abstraction is a performant abstraction. In other words, it is not enough to come up with an abstraction, but it is also important to make the abstraction efficient. So if an abstraction is cheap to use, easy to understand, and flexible, then subsystem designers will use it. If it is not, they won't. So the question is, if we want to make virtual memory persistent, can we also make it an efficient implementation of the abstraction of a persistent virtual memory? Now how can we make it efficient? Now if we think about it, the virtual address base of a process, and let's say that this virtual address space represents a process that belongs to a particular subsystem. It may have persistent data structures that are strewn all over this address space. And if these data structures, persistent data structures, are written into and if these persistent data structures are manipulated by the subsystem and if we want them to be committed to the storage, then it requires that every time we touch a persistent data structure in-memory version of it and modify it, the on-disk version of that same data structure needs to be updated. Now what that means is if these data structures are spread out all over the virtual address space, that could result in many I/O operations, first of all. Second, it could also mean that we may be writing, to different portions of the disk. And both of these are bad news, because disk by it's very nature, it's electromechanical in nature. And, disk is electromechanical in nature. And as a result there are latencies associated with the disc, namely, seek latency and rotational latency. And if the writing that we are doing to virtual memory, which is backed up by the persistent disk, is all over the disk, then it's going to result in many random writes to the storage system. And random writes are going to be resulting in very poor performance. So instead of doing this, what we would like to do is have what we will call a log segment. And the idea is very similar to the log structure file system that we have seen earlier when we discussed XFS. The idea is that, when you're making changes to the virtual memory and you know what portions of it are persistent, every time you write to a persistent data structure that is in memory, then you write a log record that corresponds to the change that you made to the persistent data structure. So this is the change you made to this persistent data structure, and this is the change you made to this persistent data structure. The log segment is itself a data structure in support of these subsystems when implementing the persistent virtual memory. We have this log segment as a data structure that the implementation uses to record changes to persistent portions of the virtual address space. And because we are putting it into this data structure called log segment, we can write this contiguously. And of course, we have to commit these changes to the disk. But now you can see the difference in the picture between here and here. Here we were doing random writes all over the disk. But here we are writing the changes to the persistent data structures to contiguous portions of a log segment and we can take this and store this contiguously on the disk, and that's the attraction. So these log segments are going to be stored contiguously on the disk. And therefore, even though we are making random writes in the address space of the subsystem that we are building that requires certain persistent data structures, because those changes are being recorded as logs in a log segment, and we are committing the log segment to the disk we can convert these random writes to sequential writes and write sequentially on the disk. So two things we are accomplishing by doing that. The first is we are not making individual I/O operations for every time we touch different portions of the virtual address space, because we are writing it into this log segment, which may be an in-memory data structure. And therefore, we are reducing the number of I/O operations that we need in order to commit these changes finally to the storage. And second because we are writing this contiguously, we don't have the random writes that we talked about. All of this, will help make the design and implementation of a persistent virtual memory much more efficient. That is the key idea, which is there in the paper that I've assigned to you for reading, which is the LRVM paper. Now let's discuss how we will design a server that requires some persistent support using this idea of a persistent virtual memory. So this is the virtual address space of the server. And note that, not the entire address space of the server needs to be persistent because as the developer, the subsystem designer knows what data structures in his design need persistence. So for instance if you take a concrete example like a file server design. Then the inode data structures, which may be living on the disk are the things that the file system designer would want to make sure that they're persistent. So if an I node data structure, let's say M1 is mapped into a portion of the virtual address space, then it is manipulation of M1 in particular, updates to M1 that needs to be reflected in the backing store. These data structures, M1, M2 and so on, are data structures, which are in memory versions of persistent data structures that live on the disk. We will call the collection of data structures that need to be persistent on the disk as a data segment. And a application may choose to use multiple data segments. That corresponds to persistent objects that it needs to manipulate in the course of its execution. So these data structures M1, M2 and so on, are the persistent metadata that have an on disk copy, as well as an in-memory version. Whereas all the other stuff in the virtual address space of the server are normal code and data structures. So in terms of flexibility for a server design, it would make sense to allow an application to create as many data segments as it needs in order to support what it needs to do in it's design. So what we need to do is to allow an application to map an external data segment. By external data segment what we mean is that there's a data segment that is external to the virtual memory living on the persistent medium, in this case a disk, and the application is going to map this external data segment to selected portions of its address space. What we will help the application do is to explicitly map the regions of the virtual address space to data segments that live on the disk. And as I said to allow flexibility, the application designer may choose to have multiple external data segments and map these different data segments to different portions of the virtual address space. Or in other words, the application completely manages their own persistence needs, and all that we are providing is the ability to specify external data segments to back persistent data structures that we're going to manipulate in memory. So what the application would do is, at startup, it'll map these external data segments to selected portions of its virtual address space in order to create the in-memory versions of the data structures it needs to manipulate during the course of its execution. And the mapping between this virtual memory space and the external data segment is also one to one. That is there is no overlap of external data segments in terms of occupancy within the virtual address space and this makes the design of a reliable virtual memory, that much more simpler. So simplicity is the key in the design so that it is easy to use, flexible, and performing. And those are the goals that the authors of reliable virtual memory set out to accomplish. And just as mapping is done at startup at any point of time, the application can choose to unmap a portion of the virtual address space that is mapped to the external data segments. And we will see later on that opportune moments for doing this unmapping would be when no commits are pending. For these in memory data structures, and external representation of those data structures in data segments living on the disk. Now we'll discuss the primitives provided by the lab of virtual memory for the app developer. Recall what I said earlier and that is we do not want the in memory data structures when modified to immediately start writing to the corresponding on disk version in the external data segments, because that would result in a lot of random rights. And therefore we suggested that what we want to do is use a log segment aggregate changes that we're making to the portions of the virtual address space so that the log segment can then be committed to the disk to record the changes that we're making to the virtual memory and so the first set of primitives that RVM provides is initialization. And in particular, this initialize primitive identifies the log segment to be used by the server process for recording changes to the persistent data structures of this process. And every process can declare its own log segment data structure for use in managing its persistence. So if I have a file system, it has it's own data structures. Because remember that RVM is not inside the operating system, but RVM is provided as a run time library in support of applications that lives on top of the operating system. So the library provides it's primitives and so initialize is allowing the process that is using this library to declare a log segment data structure, which will be the data structure into which RVM, in the course of execution of the process, will aggregate changes that this process is making to persistent data structures, so that later on those changes in the log segment can be committed to the desk. And even further, those changes can eventually be deflected in the data segments that those in memory versions of persistent data structures that present. The map primitive is the primitive that allows the application to say, what is the region of the virtual address space that I want mapped to an external data segment. I mentioned that there is a one to one correspondence between an address range and the virtual address space and the external data segments. So if I need to map different portions of the address space of the process to different data segments. I would execute multiple map calls to map different regions of my virtual address space to different external data segments. So this regional descriptor can contains both the address range that I want to be mapped as well as it names the external data segment. That corresponds to this particular address range. In unmap, thus delivers mainly it decouples the address range from the external data segment that it is associated with up. So in the body of the server code These are calls that an app developer would make. The begin transaction and the end transaction alerts the RVM run time that the application is about to make changes to persistent data structures between these begin and end transaction calls to the RVM library. And in fact, signals to the RVM library that the transaction has committed, meaning that all the changes that the application made in between begin and end to data structures that are persistent have to be flushed to the disk. That is, they have to be made persistent. On the other hand, a begin transaction could also end in an abort transaction which essentially signals to the RVM library that all the changes that the application made bound between begin transaction and abort transaction. Have to be thrown away by the RVM library and should not be committed to the disk, that is they should not be persistent. So the idea then is, between begin transaction and end transaction, the application developer is modifying the in-memory version of the persistent data structures And committing them to the persistency storage by calling this n transaction which is saying, commit my changes to the in memory versions to the persistent version on the disc. On the other hand, if the developer calls an abort transaction after a begin transaction then all the changes that he or she made to persistent data structures should be thrown by the RVM library and not persistent on the disc. The set range call is the very first thing that an app developer will do inside a begin transaction end transaction sequence. You can think of this code between begin transaction and end transaction like a critical section of the app developer's code base. And the first thing that happens within that critical section is a call to set range. What the set range is saying is even though an address region may be mapped to an external data segments, in this particular critical section by begin and end transaction, I'm going to modify only a portion of that address range, and that portion of the address range that I'm going to modify Is specified by the starting address and the size of that block that I'm going to modify. That's the purpose of the set-range call, which says, for this particular transaction, which I started here, for which RVM will return a unique transaction ID to me, when i make this call I'm going to use this transaction ID and tell RVM that for this particular transaction, I'm going modify only a block of memory starting at this address and bound by this address. That's the purpose of the set change code. So, it is only this portion of it that you are going to modify. And we will see how all of what I said is used by the RVM library in its implementation in a minute. But from the user's point of view, from the developer's point of view, this is all that developer needs to know and use in order to write his application that has persistent data structures. As simple as that. All the heavy lifting that is needed to accomplish the developers intent for persistence that are enshrined in these primitives is taken care of by the RVM run time which we will see in a minute. As we said earlier, RVM has to be efficient, otherwise, no one will use it. The run time does not actually write the persistent data, that is specified through the set range call. Directly to the external data segments. Instead, it writes the changes that it made to the block of addresses, specified by the set range call as a redo log in a log segment that was named in the initialized call, and we know that the log segment is the trick that we mentioned earlier also to avoid. Random writes to the disc. The log segment is an in memory data structure of the RVM run time and once a transaction commits at that point, the log segment, that contains the changes that have been made to the in memory version of persistence data structures. Will be committed to the disc. Or, in other words, RVM writes the changes that the developer's making to persistent data structures in between begin and end transaction as redo logs end the log segment. And the redo log that has been written into the log segment is committed to the disc at the point of end transaction. On the other hand, if the transaction aborts, then you don't have to commit those reader logs to the disc.Now remember that the changes that we make to the perisistent data structrues in memory versions of those persistent data strutures Have to be eventually committed to the external data segments. Now that part is done lazily by the RBM system. It basically applies the redo logs that have been committed to the disk to the external data segments at opportune moments, and once it has applied those Redo logs from the log segment in to the external data segments. The redo logs can be thrown away. And this part is what is called truncation. So there are two parts to managing the log. One is at the point of commitment, the log has to be forced to the disc, because you want to persist it. And second, once it is comitted to the disc. You have to eventually apply it to the external data segments and at that point, you can truncate the logs. So the log is going to be applied to the external data segments and once those external data segments have been modified, then you can throw away those reader logs. So flushing the log segment to the disk, as well as Truncating the log segment once they have been applied to the external data segments that they represent are all done automatically by LRVM. So the application developer doesn't have to do anything other than initialize his virtual memory The log segment he wants to use and write the code that contains these critical sections bound by begin transaction and end transaction and abort transaction. But RVM also provides flush and truncate as primitives for flexibility in writing the application. Or in other words, the application Can if it chooses explicitly manage the persistence and application of the [UNKNOWN] logs to the data segments. Shortly we will mention some optimization specified through the commit mode to the RVM [UNKNOWN] by the application programmer. To enhance performance. One of those optimization features allows the RVM to defer flushing to the disc at the point of commit. So one of those optimization features in the commit mode is to tell the RVM. Do not flush the changes to the log segment commit point yet. I'll take care of it explicitly by flushing. So these additional primitives are for the developers if she so chooses to explicitly controlling flushing of the logs to the disk and also manage the truncation of the logs by explicitly applying it when she wants it To the external data segments to conserve space, because if you think about it all these log segments are occupying space on the disc. At the midpoint you're flushing the redo records in the log segments to the disc. And so you're filling up the disc with a lot of redo records in addition to the external data segments. The truncation is a way by which. The subsystem designer, can explicitly say, go ahead and apply these edulogs to the extremity segments and truncate the log. So as i said, these calls are there, purely for flexibility, if their designer so chooses to use these primitives for explicitly Controlling and reducing the log space. And these additional perimeters are available in RVM as a way by which the developer can administer the design of a subsystem and performance tune his subsystem for optimality. The main thing to take away is that the RVM design is simple, with a small set of primitives. Easy to grok for a developer and use in the design of a subsystem. Also, we've been using the word transaction. But the semantics of the transaction as used in RVM. Is much more restricted compared to the traditional use of the word transaction in the database literature. In particular, the transaction as proposed and used in the RVM library, is intended for recovery management. It does not need all the properties that are usually associated With traditional transaction, so called acid properties, namely atomicity, consistency, isolation, and durability. For example, a transaction as defined in RVM, does not allow for nested transactions. It has no support for concurrency control. But all of these are things that you normally associate as transaction for the key design objective. In RVM is to make it simple, and performant, and easy to use. And if there is need for concurrency control, it is something that the developer has to implement at a higher level in their system software. So in this sense. It is useful to remember what I said earlier that the code that exists between begin transaction and end transaction is like a critical section in the application code. And all that the developer is trying to signal through the critical section to the RVM library is that it is making changes to a portion Of an address range and that potion of the address range, it needs to be persistent if in fact that critical section bound by this n transaction commits. On the other hand, if that critical section aborts then those changes should be thrown away. That's the intent of a set of primitives provided by RVM and that's the intent of this transaction semantic. Now let's look at how a developer may use this primitus in building a sub system. The first part of the code is going to be the initialization portion where the developer is mapping the address space of his process to external segments. Chosen regions of the address base to external data segments. And also specifying what the log segment is going to be for this particular code base that he or she is writing. And in the body of the code, there are going to be regions where they want to manipulate persistent data structures. And for that portion of the code they're going to say begin transaction, end transaction and within there the first thing that we'll do is set the range to indicate what is the block of contiguous adversities that they plan to modify in this critical section. And of course this block of adversaries should be contained in the range that has been mapped to an external data segment. And once they have done that, then the rest of the code is normal code that they write in terms of manipulating data structures. So they may be writing to a data structure m1, which is really a metadata that needs to persisted. And, if that is the case it is important that the user ensure that this data structure m1 is contained in the range that they specified at the beginning. And similarly m2 if it is a another persistent data structure that they're modifiying, it better be contained in the range again that they set out in the beginning. And similarly, when they are done with all of the changes and they want to commit, they can call end transaction and at that point all the changes that they made to these data structures are going to be written as a redo log into the log segment. So the first thing that LRVM would do inside this transaction code, is when you execute the set range call, it says, aha, this is the portion of the address range that the developer is going to modify within this critical section. And it's possible that this transaction, which is beginning now, may commit or abort. If in fact it aborts, then I have to make sure that all the changes that are made to persist in data structures are thrown away at the point of abort. And therefore the first thing that LRVM does is create what is called an undo record, which is really a copy of the virtual address base starting at this base address for this number of bytes. That's the portion of the address base that the developer intends to modify within this critical section. So LRVM makes an undo record which is the original version of that portion of the address base. So this under record is an in memory copy of the virtual address base starting here, for some number of bites specified by this number of bytes. And this is a temporary record, and in fact LRVM will create this under record only if it is needed by this transaction semantic. In the bigger transaction, there is a mode specifier, that the user can specify to the RVM whether this particular transaction is going ever abort. So in other words, if the developer is absolutely certain. That his transaction is never going to abort, then he can specifiy and know restore mode for this transaction, which tells RVM, that, look this transaction is never going to abort, therefore no need for you to create an undo record. That's the intent of that. So, again, we want to make sure that these primitives are performant. This is one of the ways by which the application developer can make sure that LRVM does not do unnecessary work. And in this case, if this transaction is never going to abort, then there is no reason to create an undo record. So that's the idea behind the no restore mode in the begin transaction. In any event, if the transaction eventually commits, at that point LRVM with throw away this undo record. This undo record is meaningful only if the transaction aborts because in that case. What LRVM would do is restore the original version of this portion of the virtual address space by copying the undo record back into that space. So during the body of this critical section, when the application is modifying These in-memory version of persistent data structure, no action by LRVM. All these changes are happening directly to the virtual address space of that particular process exactly where these in-memory copy of persistent data structures are living. So finally, if the transaction commits by calling an end transaction, at that point, all the changes that have been made to persistent data structures have to be written to the log segment that records the redo logs for this application. So at this point, LRVM creates a redo log in memory of the changes That have been made to the persistent data structures. That is, this region that has been modified, it's going to be written as a redo log. See it doesn't know within that region, LRVM does not know within this region specified by base address and the length. Where exactly these data structures are contained. All it knows is that, this is the portion of the address space that is being modified in the critical section, that's why it's so important as a developer to make sure that the data structures that you manipulating within this region, that you have signalled LRVM. LRVM is basically thinking that there is a continuous set of addresses starting here for a certain length that may have been modified in this critical section. So, the log record that it writes is basically saying, here is the start address, and here is the number of bytes, and this is the new data that goes into this virtual address base, that's what LRVM is creating As a redo log in memory. Remember that redo log is itself a data structure of LVRM in memory which should not confuse the redo log with the external data segments. External data segments are the persistent versions of the in memory data structures. Now this redo log Is the changes that are being made to the in memory version of the persistent data structures, that have not should been committed to the disk in terms of internal data segments. It's now available at this point at the end of end transaction, it's available as a redo log entry. In the log segment which was initialized by this application. And the semantics of this transaction is, if it commits, then all of these changes are now available on persistent storage. So, what the LRVM library has to do is Not only create the redo log record, which is a data structure in memory so far as LRVM is concerned, but it also has to flush these redo logs to the disk at the point of commit. And only after that, we can assume that all the changes that have been made in this critical section Has been persistent on the disk, so he has to flush to the disk synchronously, meaning that this end transaction waits for this redo log to be flushed to the disk. At this point it is on the disk, however, again [LAUGH] in order to make sure that we can have a perfomance implementation of LRVM, there is a mode available in the end transaction, and this mode. Says no flush, meaning at the point of end transaction, you don't neccessarily have to block the caller for the flush to be complete. Transaction semantics would require that the process that is executing this commit should not be allowed to go past this point until. The synchronised IO has completed from the redo log into the disk but in order to make it a more performance conscious design, if you think that power failures array and the chance is that your server is going to crash is not very high, then you can go ahead and say as a developer that At the point of end transaction, I want you to committed by the way, you don't have to block me. In other words, this mode if it says no flush, it is saying that yes, I want you committed to the disk, but don't block me from going further. So, the changes that are being made As a redo log record will be committed to the disk later depending on this specification of no flush. So as a developer if I say no flush, then the redo log is not going to be synchronously written to the disk. So I can go ahead. I might do another transaction. That might write More log records, so I can review the number of I/O operations in committing these log records to the disk. So that's an opportunity that I'm exploiting by giving this no flush mode in the end transaction. So, it's an opportunity for the application to both reduce the number of I/O operations. And also make sure that the application is not blocked here, waiting for the synchronous rights to the disk to complete. So once the transaction is committed, meaning that LRVM has created the redo log for this particular transaction, then the undo record is no longer needed because undo record, if you recall, was created Just for the eventuality that this transaction may not commit, but now that the transaction is committed, we can throw away this undo record. On the other hand, instead of the end transaction, the transaction may actually abort. If it aborts, then what. LRVM has to do is restore this portion of the virtual address space of this process from the undo record so that we have gone back to the state before this transaction ever happened. So in other words, we are making whatever code that the server executed as a critical section code between begin transaction and abort transaction. To go away, and we restore the computation to its state before the begin transaction by copying the undo record into the portion of the virtual data space that has been modified through this critical section code. That in a nutshell is how you would use the primitives provided by LRVM in constructing A server, that, has certain persistence requirement. I mentioned earlier that RVM provides opportunities for the developer to give hints to the library on optimizing the performance of the library for the chosen application. Or in other words, the transaction semantics of RVN is already a stripped-down version of the traditional transaction semantic. It doesn't what you are able nested transaction and so on, but still transaction by it's very nature requires that a commit point, you have to do a synchronous to the I/O to the disc. And similarly transaction by it's very nature says that, it has an all or nothing property. So the transaction is not going to commit, that is that it's going to abort, then we have to make sure that all the changes that have been made between begin transaction and abort transaction are thrown away. And similarly, if the transaction commits, then we have to make sure that at the commit point all of the changes that have been made to in memory copies of persistent data structures are committed to the disk. That's where synchronous I/O comes in. But if the developer wants to optimize his performance, RVM gives opportunities for such optimizations. The first opportunity is the no_restore mode in the begin_xact call. The no_restore mode in begin_xact is signalling to RVN that this transaction that I am starting is not going to abort, and therefore there is no need for you to create an in memory, undo record for me. Even though I am going to give you a set range call, don't bother creating an undo record for the range of addresses that I intend to modify in this beginning transaction. That's what is meant by the no_restore mode. So that reduce the amount of work that the RVM has to do in doing a memory copy and the application is going to gain because RVM is doing less work which means the overhead in performing a transaction is going to be less as seen by the application. The second optimization opportunity is a no-flush mode in the end transaction. As I mentioned, a transaction truly has committed only when the changes to the critical section between begin transaction and end transaction have been synchronously written out to the disk. So, the normal semantic of an end transaction, that is a commit of a transaction, would require that our VM should block the process that made that call for end transaction, until that redo record has been written synchronously to the disk. But if the application developer is opportunistic and believes that the chances of failure either due to power failure or due to his own software caching is pretty small, he could be brave enough to say no-flush mode. And what that no-flush mode is telling the RVM library is that there is no need to do a synchronous I/O. Of course I want you to write it to the disk but don't block me in order to write to the disk. So no need to do the synchronous flash of the redo log to the disk. And, in other words, what we're getting by doing a no-flush mode in end transaction is lazy persistence. We know it is going to be persistent on the disk, that is the work that RVM is going to do, but it is not doing it exactly at the point of end transaction. So the upshot is, there is a window of vulnerability. End transaction happened here, and maybe by the time RVM gets to write it out to the disk, so much time has elapsed. So this time window is the window of vulnerability. So if this axis is time, then n transaction happened here, and this is the point where the redo record was committed to the disk, so this is the portion which we're calling as a window of vulnerability, and the app developer is taking a chance, saying that I am so sure that no crash is going to happen in this window, I'm going to go ahead and say no-flush. So, in other words, the transaction is being used as an insurance, and this should remind you of the old at the age that we saw when we talked about shared memory systems. Shared memory systems scale really well when you don't share memory. Similarly, transactional systems scale really well, perform really well, when you don't use the full semantic requirement of transaction, in particular if you can get rid of synchronous I/O, it'll make the performance better. Remember the goal is performance efficient implementation of RVM. The restricted semantics of transaction help a lot in making sure that what we are designing as a reliable virtual memory is not very heavyweight. But the implementation of that used semantic, as to be efficient as well, and that's part of the reason why this is called light weight reliable virtual memory to indicate that it's light weight in terms of transaction properties. Now how to make it really perform well? The first thing is the strategy that they use for recording changes to the persistent portion of the virtual memory. The logging strategy is what is called no undo/redo value logging. No undo, meaning that we are creating an undo record of the changes that we're going to make to virtual memory, but it is not a log that is persistent on the disk. It is just an in-memory copy that is kept only for the duration of the transaction, and at the end of the transaction either it commits or aborts. We throw away that undo copy that we created. On the other hand, redo is the log that you create. First of all, in memory of the data structure in RBM, and we commit those data structure to memory, and in committing the changes of the redo logs, we are only writing the new value records of committed transactions to the log. So, even though the redo log. Consists of a transaction start and the changes that you're making, only new value records of commuter transactions are written to the log. Now this is the reason you have forward displacements, that a that we know where to append to the log segment on the disk there is a in memory version of the log segment too which you are writing the redo logs. And once you've written the redo logs, you're flushing it to the disk. On the disk, you have a on-disk version of this redo log record for this particular process, and what you're doing at a commit point is only writing the new value records of the committed transactions. A detail of the log segment. So we are only appending to that log segment the new changes that have been made within this transaction. So upon commit, what we need to do is we have to replace the old value records in the virtual memory with the new value records. But this is automatic because the way LRVM works is that it is created an undo record of the old value records of that portion of the virtual address base and all the changes that the developers making to the persistence data structures in memory are happening in memory and therefore replacing the old value records by the new value records in the virtual memory is automatic. Only if you abort, you have to undo the changes. But if you're committing then your virtual memory is already ready to go in terms of the changes that are being made within the transaction. At that point you have to force the redo log records to the log on the disc, and as I mentioned earlier, the optimization that's available in the implementation is to get transactions on the cheap. In particular, the no restore optimization allows implementation not to create an in memory undo record. That's time saved in terms of copying. And that means better performance for the application. And similarly, no flush, a commit point tells RBM that it can write the redo log to the disk lazily. It does not have to block the process that is making that call, the end transaction call. And that is an opportunity again to make that implementation more performance-conscious. So this lazy commitment of course has its downside, there is this window of vulnerability that I mentioned, and that is, there is a time window between n transaction and the point at which the redo log has been forced to the disc and this is the window of vulnerability and if The system caches within this time, then we have lost the redo records that we wrote to in memory. And that is important to understand, that there is a price you pay, in order to get this flexibility and performance in the implementation. You can see that this, redo log data structure, allows traversal in both directions. And this is for flexibility in implementing the RVM runtime. In particular writing to the on disk version of this redo log, having these forward displacements allows you to know where exactly you want to append to the existing redo log record from the changes that are being done in this particular transaction, and being committed at this point in time. And similarly the diverse displacements are helpful in traversing the log record during recovery. So if you look at the redo log, it has a transaction header, and in between the transaction header and end mark are all the changes that have been made in that particular critical section by the developer. And for each of the address changed within that critical section, what is the new data that corresponds to the changes that have been made to that page? Similarly what is the new data for this range, new data for this range. That's the structure of this redo log record that has been created and forced to the disk. Now, when we resume from the crash, what we need to do is make sure that the external data segments Are updated with all the changes that have been made and recorded in the redo log but have not yet been applied to the external data segments. So crash recovery is of course the whole reason for LRVM. And what we're going to do when we resume from crash is read the redo log starting from the tail Of the entire log segment, and that's where the reverse displacements come into play. And once you've read the log from the disk, apply to the external data segments where it has to go to. All of that information is contained in the redo log record in terms of the transaction. What is the address range that is being modified? What is the external data segment that particular address range corresponds to? All that information is contained in the redo log record. So you can take that and apply to the external data segment. And once you've done that, you can throw away the log. So that's how Crash Recovery works. So far we've been pessimistic that we may have crashes, but hopefully crashes are not something that happen very often. [LAUGH] And if crashes don't happen that often, crash recovery is not that important. But on the other hand, if crashes are not happening but the system is progressing along, And what is going to happen is that we're going to create lots of log records on the disk as the system is making forward progress. So we have external data segments, which of course we need because that is were the persistent objects are actually contained, but we are also creating these redo log records. There are reflections of changes that we made to the in memory versions of these persistent data structures. Eventually one of these redo logs that represent changes to the external data segment to be applied to the external data segments. Now the only time we're going to do that Is when a crash happens, that's being very pessimistic. Also, we may end up clogging the disk with a number of redo log records. We've seen the need for log truncation in the distributed shared memory systems as well. In the case of DSM, those logs were clogging physical memory. In the case LRVM, these logs are clogging the disk space. Regardless, these are unnecessary overhead in terms of space and clutter, and also, if a particular application needs to map an external data segment, then we have to know whether that data segment is up to date or not. And that depends on whether or not there are some redo logs pending. To be applied to those external data segments. So all of these things suggest that what we need to do is truncate the log periodically. What exactly is truncating the log? It means that we want to read the logs from the disk and apply them to the external data segments and get rid of them. Now this sounds exactly like what I described to you happens when we do recovery from a crash. Therefore for Log truncation, simply apply crash recovery algorithm. So any time the system, meaning the LRVM run time, decides that it is time to do some clean up, what it is going to do is it's going to go and pick some logs to clean, bring those logs into memory Read the redo log records. Apply them to the appropriate data segment. And throw away the lock record. So that's what lock truncation is all about. Of course we don't want to stop the world in order to do this lock truncation. So what we want to do we is. We want to do this lock truncation in parallel with forward processing by the application. And the way LRVM allows that to happen. Is it splits the log record into Epochs. It says this is a portion of the log record that I've chosen the cleanup and this is a truncation Epoch, and so this is the part that I'm going to use to read from the desk and apply to the external data segment. And in parallel with that, I'm going to allow the application to make changes. This is a current epoch where the application is making changes to the log record. And the new record which is not yet being used. So, the idea is that we are allowing RVM to do it's work in terms of Log truncation by reading a portion of this log which is a truncation epoch portion of the log, and applying to the external data segments. And in parallel with that, we're also allowing the application to make forward progress by writing new log records to the current epoch. So the crash recovery algorithm is being applied to the part of the log that is in this truncation epoch while allowing forward processing to the part of the log which is the current epoch that the server is working on. The biggest challenge in implementing LRVM is the log truncation code because there's so much coordination that is needed Between what the LRV and run time has to do and what the application may be doing in terms of morphing the current log segment. You need the log segment for recovery but it will also overhead when there are no crashes. And they take up a lot of disc space. And puts extra burden on mapping and data segment through the subsystem that wants to use it. So, managing the logs, truncating the log as efficiently as possible is one of the heaviest problems according to the authors of this paper in implementing LRVM because it directly has a consequence on the performance of LRVM. And in fact the bulk of the heavy lifting that is done in implementing LRVM run time and make it really lightweight and efficient goes in doing this lock truncation efficiently. What I've described to you here, is one course level of lot truncation where We are taking the redo logs and applying it. A more finer grained way of implementing log truncation would be to look at in memory copy of the log segment also, and trying to make sure that we apply it to the external data segments so that we don't even incur the cost of writing a disk version of the redo log. That is even more complicated and I welcome you to read details of that in the paper. LRVM is a classic systems research work. You understand what is the pain point for system developers. Once you understand the pain point, then it becomes easy to think about what solution you can bring to the table to solve it pain point. Managing persistence for critical data structures is the pain point that is identified by the LRVM work. LRVM proposes using lightweight transactions, that is, a transaction without all the heavyweight asset properties usually associated with transactions in database literature. And this lightweight transaction is going to give the needed persistent semantics for designing robust subsystems that can tolerate crashes. The APIs provided by LRVM, are designed to remove an important pin point for system developers, namely system crashes. We know, that system crashes can come from software errors as well as from power failure. That brings up an interesting thought experiment. Which is what we're going to discuss next. So how does LRVM do it? Well, it does it by providing transaction synaptics for persistent data structures. It calls itself lightweight since it eliminates all the usual heavyweight properties associated with transactions, which are usually called acid properties. And LRVM makes transactions light weight, for use precisely for the intended purpose which is recovery management. In LRVM, changes to virtual memory are written as re-do logs, at the end of a transaction. And these re-do logs are forced to the disk at the end of the transaction as common records Of changes made to virtual memory. And as it log force that happens at commit point is what is called a synchronist IO, because the application has to wait for the IO to complete before proceeding with further execution. So implementation, a precise implementation of the transaction semantics of LRVM requires a log force to the disk to make sure that the redo log that represents all of the changes made to virtual memory Within that critical section bound by, a begin transaction, end transaction is actually committed and persisted on the disk. It is precisely for this reason namely synchronous IO, that transactions are considered heavy weight, even though the synantics of transactions have been considerably reduced and made simpler in the design of LRVM. In other words, for a precise implementation of LRVM, at the commit point, it requires at least one synchronous disk I/O. The system design tend to avoid transaction despite the precise semantics of transaction due to the time penalty associated with disk I/O. If disk I/O can be eliminated, then transactions would be cheap. And if transactions become cheap then everyone will use it and life is good. Rio Vista, the lesson that we're going to review now, asks the question, "How can we eliminate synchronous disk I/O?" So in other words, Rio Vista is going towards a performance-conscious design and implementation of persistent memory starting from where LRVM left off. There are two orthogonal problems that lead to a system crash. One is power failure. The second is software failure, or in other words, the application crashes for some unknown reason, obviously a bug in the application. So Rio Vista poses a very interesting rhetorical question. Suppose we postulate that the only source of system crash is software failure. There are bugs in the software, and not power failure. How does that change the design and implementation of failure recovery? Now the question is how can we eliminate power failure. Well, we can throw some hardware at the problem and make power failure go away. For example, we can get a UPS power supply and connect it to the portion of the system that I want to be up even if the power fails. So the idea in Rio Vista is, if we can take the memory and take a portion of that main memory. And make that persistent to power failures by adding a battery backup. Then even if there's a power failure, this portion of the memory, whatever changes, you record it here. It's going to survive that power failure because it has battery backed up. That's the idea, if we can throw some hardware at the problem and make the problem disappear. So that we can then focus on how to recover from failure, assuming that the only source of failure is software crash. So with this amendment. Will this make the implementation of transactions as defined in LRVM cheap, so that designers will actually use them? Let's see how this can help. Let's revisit the semantics of LRVM and what it does. So this is a time axis, the application when it calls the begin transaction primitive of LRVN. What LRVN does is it says, okay, the application is going to modify some portion of the memory. So let me create a memory copy of the old contents of the portion of the memory that this transaction is going to modify. And that's what is called the in-memory undo record of LRVM. In the body of the transaction the program is doing normal program writes and they are going into the memory. No problem with that because LRVM has the undo record already stashed away. So all of these are writes to normal memory and there is no interaction with LRVM during this portion of the transaction code. Then the application reaches the end transaction, makes the end transaction call to LRVM. At that point, LRVM is going to write a redo record onto the disk when the transaction commits, because end transaction is synonymous with commiting, so far as LRVM is concerned. And therefore, at this point, the changes that have been made to virtual memory are written out as a redo log record and forced to the disk by LRVM. We know disk I/O is slow and the more you do it, the slower will be the subsystem that is using these LRVM primitives, and that's why LRVM provides the no-flush option in transaction call, which allows an applicaiton to tell LRVM write it out to the disc but don't stop me from progressing further in my computation. In other words, the applicaiton is hoping that there won't be any failures that will result in all the changes that it recorded in memory not being forced to the disc. So the LRVM is going to write out the redo log as the background activity and the hope is that there won't be any system crash during the time that it takes for it to do. And this is what we called as the window of vulnerability when we talked about LRVM. So what the no flash option does is to increase the vulnerability of the system to power failures in favor of performance. And that's a calculated risk an application developer is taking if they specify no-flush optimization in the end transaction. So, on the other hand, if you are conservative then what you would do is you let the end transaction have the normal transaction semantic ,which to say that adding transaction force the write of the log into the disc to ensure that the log segment has been commmited to the disc. And only then allow the application to proceed further. And at this point, at the commit point, the other thing that LRVM would do is, in addition to forcing the log record to the disc, it will also get rid of the undo record because the undo record is no longer needed for this transaction since the transaction was successfully committed. And we know as a background activity what LRVM does is to update the original data segments with the changes that have been recorded in the redo logs because, as you recall, the data segment contains a persistent data which are being brought into memory and modified during this transaction. And those changes had to be eventually persistent. Right now, they're sitting in the redo log records and what the log truncation part of the elarvian library does is to read the redo log records and apply them to the data segment and get rid of the redo logs. So this is the log truncation or clean-up of the disk space that is done periodically by LRVM. Because in the absence of crashes you have to make sure that you clean up the disk every once in a while. So the upshot of LRVM implementation is there are three copies of the VM space, done by LRVM to manage persistence for recoverable objects. Of course, it optimizes log force by dealing them at transaction endpoint. But in implementing LRVM one of the biggest sources of vulnerability is power failure. Because if you, in fact, use at optimization to defer writing out the log record to the permanent storage, then all the work that you did in this transaction may actually be wasted if in fact there is a power failure before the log force happens. Now what does providing a battery-backed DRAM give you? Before we talk about how we can implement RVM efficiently with a back to back DRAM. Let's first understand how we can use a back to back to DRAM to implement a persistent file cache. What we mean by that is, typically file systems use a file cache to hold data that is brought from the disk for efficiency of manipulation by programs or anything on the processor. And when we say it's a persistent file cache, what we're saying is, even if there is power failure, the contents of this file cache is still available when the power comes back up again. And in order to achieve that, what we're going to do is, we're going to string a UPS power supply, to back this file cache, which is implemented in DRAM. So the contents of the file cache never goes away. And we also postulate that, there is VM, a virtual memory protection. That have been built in to the operating system to prevent operating system errors such as wild writes to the file cache during software crash or power failure. Now there are two ways to use the Rio file cache. One is, when a process does file writes, actually they are writing to the in memory copy of the file. Typically operating system buffers the rights that you do to files in DRAM and then write them out to the disk in opportune moments later on. And now if these file rights go to the file cache which is battery backed then you make sure that these file writes are persistent. Normally if an application wants to make sure that when it writes a file, the contents of the file is immediately forced disk, the application will have to do an Fsync call in a Unix system, for instance. But if we have this battery-backed file cache, the application can simply do normal file writes and don't worry about doing an fsync, because writing to the file writes to the file cache, and the file cache is persistent by definition, because it is battery-backed. And similarly, another common operation that is done is Unix allows files to be mmaped, that is mapped into memory. And if an application maps a file into memory and writes to that file using normal program writes, those normal program writes become persistent because these rights are backed by the file cache. And the file cache is battery-backed, and therefore normal program writes becomes persistent also. So with the Rio file cache that is battery-backed, what we get is that file writes by a process as well as normal program writes to memory mapped files become persistent by definition. In the absence of this facility of a persistent file cache. If an application were to mmap a file, and write to that using normal program rights, then it will have to do what is called an msync call, in order to make sure that writes that it did to virtual memory actually get persisted on the disk. But with a file cache, real file cache that is persistent, there is no need to do that, because by definition, we are saying, any rights that get into this file cache, will persist. In other words, the contents of the file cache, will survive power failures. So if there is a system crash, whether it is a power failure or software crash, the file cache data, in memory, is going to be written onto the disk for recovery. And so the upshot is no synchronous rights are needed to the disk any more. It also means that writeback of files written to by an application can be arbitrarily delayed. What that means from the file system perspective is that if the lifetime of files is very short then those files go away. And so you don't have to write it to the disk in the first place. And this is common in compilation process for instance a number of temporary files are created. And those files live in the file cache for a short amount of time and when the compilation process is complete, those files are deleted so you never have to write those files to the disk. So that's a good thing about having this idea of a persistent file cache. Now let's look at how we can use RIO to implement RVM. So we've got this persistent file cache and we're going to ask the question, if we have this persistent file cache, how can we optimize the implementation of a reliable virtual memory? So, Vista is the RVM library that has been implemented on top of the Rio file cache, and let's see how that works. The semantics of RVM that is implemented in this is exactly the same as what we saw in the previous lesson, namely LRVM. It is just that the implementation takes advantage of the fact that it is sitting on top of a Rio file cache. So what we're going to do in implementing RVM using the Rio file cache is to map the data segment to the virtual memory. Exactly similar to what was done in the LRVM primitive. So, when we map the external data segment to virtual memory, by definition now, this portion of the memory becomes persistent because it is contained in the file cache and the file cache survives power failure because of the battery backup. And therefore, now we have made this portion of the virtual memory, that is mapped to the data segment, persistent. So when we hit the begin transaction call in the application, what we're going to do is make a before image of the portion of the virtual memory that we're going to modify during this transaction. Remember that in the RVM library, the set of operations that you do to virtual memory between begin transaction and end transaction. The user's intent is that those changes are for persistent data structures. And the persistent data structures that they want to modify, they would execute a set range call to say what portion of that address range needs to persistent. So. What we do in this Vista, which is an implementation of RVM is, at the point of begin transaction, we're going to make a before image, a copy, in-memory copy, of the portion of the address space that we intend to modify during this transaction. That will serve as the undo log. Now, note that this is also mapped to the file cache, so the undo log is mapped to the file cache and therefore this undo log, is by definition, persistent node. So the undo record that we create in memory, we back it up on the file cache and therefore this undo log that we have created is actually persistent. It'll survive failures. So when the program is executing the body of the transaction, it's doing normal program rights to a portion of the virtual address space where it has persistent data structures as well. So when it does this normal program rights to this virtual memory. The portion of the virtual memory that is mapped to this external data segment is, by definition, persistent and so these normal programs rights actually get into the data segment, their being persistent automatically because this portion of the virtual address space is mapped to this data segment. Which is in the file cache and therefore persistent because of the battery backing. So all the changes that we are making during the execution of the body of the transaction code is actually getting persisted in the original data segment. For which this was an in-memory copy, but the in-memory copy is actually sitting in file cache, which is battery backed. Then we reach the end transaction in the application code, and remember, in the end transaction is when a change has had to be committed. Well, you know what? The changes are already committed because that is the semantic of mapping the latest segment in virtual memory. And because the file cache is persistent, all the changes that we made to the virtual memory during these normal program writes are actually reflected in the data segment. So the end transaction at commit point, you don't have to do anything other than getting rid of this undo log, because the transaction is committed and therefore you can throw away this undo log and all the changes are already in there by design, by construction. Just as an aside, if you think about LRV implementation, commit point is the point where there is heavy lifting to be done. Because in LRVM, at the commit point, the redo log, which had been created by LRVM, to reflect the changes to the persistent data structures in memory, have to be forced to the disk. But in Vista, which is implementation of RVM on a persistent file cashe, no work needs to be done at the point of end transaction for commit because all the changes that the application developer intended to be committed to the data segment are already in there. And therefore, at commit point, all that needs to be done by Vista is to get rid of this undo log. On the other hand, if the transaction aborts, in that case, what needs to be done is the undo record that we created at the beginning of the transaction, the before image, we are to take that and copy it back into the portion of the virtual memory that we modified, because that is a semantic of RBM. That if the transaction aborts, we restore the virtual memory back to it's original state before the beginning of the transaction. So the before image that we saved at the beginning of the transaction, we copy it back into this portion of the virtual memory that has been modified during this transaction. And once we do that, we can throw away the undo log. And when we restore the before image into the virtual memory that we're also correcting whatever changes we made to the data segment automatically. Because, remember that this picture is just showing the virtual address space of the process and this is really the physical memory which is being used as a file cache battery backed and a portion of the virtual address space is in this battery pack file cache, rest of the address base of the application, does not need persistence, that can be a normal physical memory. Only the portion of the application memory that has persistence guarantees through the data segment needs to be mapped to this portion of the physical memory that is battery backed. So just to recap what happens at end transaction, if it's a commit, no work to be done except to get rid of the undo record. All the changes to persistent data structures are already in there in the data segment. On the other hand, if it aborts, restore the old image back into the virtual memory, I am back in business as though these transaction never happened. The implication of this Vista implementation, which is RVM on top of Rio file cache, is that there is no disk IO at all. And there is no redo log, because we directly writing into the data segments All the external data segments that we define in the initialization of the RVM library on the disk, they become memory resident when you map them into the virtual address space of the application. So the external data segments become persistent because of being brought into the file cache. And mapped into the virtual memory space of the server application. What do we do for crash recovery? Suppose the system crashes and come back. Treat it just like an abort. Recover the old image from the undo log. Remember that undo log will survive crashes, because it is in the RIO file cache. So if the system crashes and comes back up, you see there is an undo log. Take that undo log, apply it to the virtual address space that it corresponds to. You're back in business again. Could there be a crash during crash recovery? No problem with that. Because of idempotency of recovery, there's no problem in dealing with crash that might happen during crash recovery. Vista, by virtue of its simplicity, by making one of the problems go away, namely, the power failure, the implementation is very simple. 700 lines of code in Vista as opposed to more than 10,000 k lines of code in the original LRVM implementation. Why? Because there are no redo logs. All the changes that we're making to virtual memory for persistent data structure directly get in to the data segments. So no redo logs. Correspondingly, there is no truncation code, and check pointing and recovery code is significantly simplified, and there is no group commit optimizations. The upshot of all of these simplifications that comes from one simple trick and that is to make a portion of the DRam persistent. And implement the file cache with that persistent portion of DRAM can get rid of redo logs and get rid of truncation code and Vista has the simplicity of LRVM but it is also performance efficient. I encourage you to browse through the performance results that are reported in the paper on Rio Vista which I have assigned for your reading. To see how Vista performs compared to LRVM. In particular, Vista performs three orders of magnitude better than the original LRVM because of the simplicity and the fact that there is no disk IO. That is the biggest improvement in making Vista perform really well compared to LRVM. Rio Vista is a very interesting thought experiment. It basically shows how, if you change the starting assumptions for ae problem, you can come to a completely different design point. In this case, the starting assumption was that source of crashes was only software, not power failure. That changes everything. Now don't worry. This is not your computer misbehavior. How many times have you see often windows like what I'm showing you on this screen? Why does this happen? The simple answer, programs are not being hygienic, that is, they are not cleaning up after themselves. Now, you can not fault the applications themselves. If an application encounters an error or the user decides to kill an application, either ways, the system services that the application was using have to have a way of gracefully terminating and getting rid of the state, that is the bread crumbs it created along the way. And it may have strewn such breadcrumbs all over the computer. Some may be in data structures in memory, some may be visible, like the orphan windows that I'm showing you on the screen, and so on. Bottom line is these breadcrumbs are using up precious resources in the computer. It may be visible resources like real estate, on your display of failed applications or invisible resources like memory leaks, network bandwidth that is being chewed up by connections that are persisting beyod the life of an application. Persistent data structures on the disk and so on. LRVN and [UNKNOWN] which is really and implementation of the RVN semantics on top of a persistent file cache, took a very narrow view of recoverability, namely recovering the state that need to be persistent across system crashes. Either becaue of software failure or power failure. But imagine a system service that spans several machines and network file server is a good example. NFS is suppoed to be stateless which means that the client-server interaction maintains no state at the server, pertaining to the clients. So what that means is, taking down a file server and bringing up The file server again does not need any coordination with the clients that this file server may be serving at any point of time. But look at what is happening from the point of view of the system, especially the client boxes that are interacting with the file system. In this case, there are clients all over the network so far as this file server is concerned. And indirectly, and if a server is leaving breadcrumbs all over the LAN because of accesses that a client may have made with a file server, in each one of these boxes. Imagine what happens when a client program that initiated a file system call quits in the middle of its exchange with the file server? How will the file server know how to clean up all the mess? And in fact, with a stateless file server, doesn't even know that there have been breadcrumbs created on this node. The short answer is, the file system cannot know about these kinds of breadcrumbs that have been created all over the network. And I'm giving you a file server as one example, but in general, all of the system services that a client program is reliant on create state, and this partial states may live forever if an application crashes in the middle. Worse yet, you as a user may be asked by the operating system to do some cleanup, not knowing what the cleanup may entail for you, personally. Am I going to lose two hours of work that I just completed? I have no idea what will happen when I click the "OK" button here. So Quicksilver asked this question, if recoverability is so critical for so many subsystems, shouldn't recovery be a first class citizen in the design of operating system and not an afterthought? Because LRVM and Riovista are trying to fix a problem that manifested itself because the operating system was doing it's job well. So the question that Quicksilver is asking is. Can we make recovery a first class citizen in the design of the operating system? Users of a system want the cake and to eat it too. They want good performance from the operating system, but they also want the operating system to be robust and recover from such failures. That is, they want the operating system to be reliable. Conventional wisdom is that performance and reliability are opposing concerns. That is, you can have one or the other, but not both. Quick Silver's approach is that if recovery is taken seriously from the get-go, you can design the system to be robust to failures without losing much on performance. Let's have another trivia quiz. So from the introduction that I gave you about Quicksilver you can see that it has identified many problems that you and I face with our everyday computing. With orphaned windows and, and memory leaks and so on. So the question to you is, name the company and the year that Quicksilver was built. And the choices I'm giving you in terms of companies include IBM, Sun, HP, Apple, Microsoft, and Google. And the choice of years I'm giving you is, the early part of 80s, early part of 90s, early part of 2000s, or 2013. So pick your choice for the company and the year in which Quicksilver system was built. This might come as a surprise to you, because these are problems that we are facing even today, but Quicksilver was done by IBM back in the early' 80s. To be precise, 1984 was the year in which Quicksilver Project was started at IBM. If you look at the structure of distributed systems today, what you'll see is there are applications in the applications that users use. And there are system services and there is a micro kernel that's sitting at the bottom. And the system services include things like file server, web server, Window manager, database manager, and the network stack for applications to do network communication with servers that may be on remote machines. And the micro kernel itself may be responsible for process management and managing the hardware resources and providing inter-process communication, both intra-machine among these services. As well as inter-machine through the network stack, the remote machines on the local area network. We've seen that this is a structure we like to see in the operating system. And in fact, from many of the previous lessons that we have looked at in operating system design and implementation, both for a single processor, for a multi-processor, and a networked operating system, this kind of structure lends itself to extensibility and yet high performance. As I mentioned earlier, Quicksilver was done in the early 80s, and here is a sketch reproduced from their paper. An interesting thing that you notice is that this sketch is very similar to what I showed you as the current structure of network operating systems in the earlier panel. It's a microkernel based design and Quicksilver was the same vintage as Mach from CMU. And having seen the structure of operating systems in earlier lessons, this picture should be very familiar to you, that all the system services are provided above the microkernel. And the microkernel is only responsible for Process Management, IPC, and Machine Control, and all of the system services such as Window Manager, File System, Virtual Memory, and Communication, all of them sit above the microkernel, and the system services are implemented as server processes. Now, the late 80's was exciting time for distributed systems. There were no laptops in that time. We were moving from CRT terminals, Cathode Ray Terminals, connected to mainframe to the era of office workstations, what we routinely call as desktops these days. Quicksilver was conceived as a workstation operating system. And the ideas that are enshrined in the Quicksilver operating system predates over concurrent with many things that we take for granted today, such as network file system, remote procedure call, the Internet, the World Wide Web, and so on. What sets Quicksilver apart from the competition at that time when new services such as window manager for managing the real estate on the workstation screen and how it should be integrated into the operating system as a whole? Services that may not be available in the workstation itself, for example, a file server maybe remote. So integrating communication into the design of the operating system was key and services that are within a workstation and across workstation on the network in the distributed system need to recover from failures. Rather than ad hoc mechanism for each server to recover from such failures, Quicksilver was the first to propose transaction as a unifying concept for recovery management of the servers. It was the first operating system to propose transactions in operating systems and that's the reason you see transaction manager as part of the service provided by the operating system. A quick personal note, I was a graduate student at University of Wisconsin, Madison in the early '80s, and spent a year in IBM research working with the group that conceived and designed this Quicksilver operating system. My dissertation research, which was on hardware support for inter process communication, used as an experimental platform, a precursor to the Quicksilver operating system which is called 925. Recall what I told you earlier that Quicksilver was intended as a workstation operating system, and 925 was the name of the operating system that was a precursor to Quicksilver. And as you can imagine this is a pun on office workstation nine to five. That was the name given to internally to the precursor of the Quicksilver operating system. Because Quicksilver is a distributed operating system, IPC both within, and on the local data network is a crucial component of Quicksilver. And this picture shows the semantics of the IPC call. In the kernel, there is a data structure called service queue. Which is created by the server that wants the service, request from clients. And clients make a request, and the kernel does an upcall to the server to indicate that this is a client's request. The server executes the upcall associated with this particular request. When it completes the request, the completion, goes back into the service queue. And that is an indication for the kernel, to give a response back to the client. So then the synchronous client call where the client is waiting, til the request is actually serviced, and the completion response comes back to the client. And the service queue, is a global service queue, just like UNIX socket. So any process, anywhere in the network, which has knowledge about the service queue, can connect to it and make requests on the service queue. And so nearly any server process in the entire distributed system can service requests that are coming into the service queue. And there are some fundamental guarantees provided by Quicksilver for interprocess communication which includes no loss, or duplication of requests. So the request comes in, it will get done exactly once. And it also ensures that there's no duplication. It also ensures that there is no loss of the request. And Quicksilver also takes care of the liability of the data transfer that is inherent when the client and the server, are on remote machines. And because the service queue data structure is globally unique for every such service. There is location transparency, for client server interactions. Or in other words a client does not needs to know, where in the network its particular request is being serviced. For that is yet another feature of the IPC guarantee. Or the IPC call can also be asynchronous. What that means is, the client can make a request asynchronously. And continue with its own execution, whatever it wants to do, it doesn't have a block on this. The kernel is going to take the same action, and that is, if there is a server that is available, then the kernel is going to pass it to that server to execute that request. And, when the completion comes back in, it is buffered in the service queue by the kernel, waiting for the client to come back, and ask for the response. So the client, at some point, has to do a wait on the service queue to indicate that I'm ready to receive the response that may have come, back for the request that I made earlier. And when the client does the wait, if the original request has already been serviced by the server, and the response is sitting in the service queue, then the kernel, will deliver the response to the client. If not, the client will wait until the response comes back. So this is the asynchronous client call, but in either case, as I mentioned earlier. The IPC guarantees hold that there is no loss of the request, and there is no duplication of the request. As you can see from the semantics that I described just now, that Quicksilver IPC is very similar to remote procedure call. In fact, the remote procedure call paradigm was invented around the same time as the Quicksilver Operating System. And since all services are contained in several processes, IPC is fundamental to Quicksilver. The IPC semantic supported by Quicksilver allows, multiple servers to wait on a service queue. And the way they will do that is by making a call called offer which is essentially saying, I'm willing to offer my services for this particular service queue. Any number of servers can make this offer and that essentially means that if a request comes in, thatany one of these servers can be called by the kernel depending on the busyness of the servers with respect to handling requests that have come in for the service queue in the past. The client server relationship is interchangeable. For example, a client can make a call on a file system server and the file system server in turn makes a call. To a directory server and a call to a data server. So, in this case, the file system becomes the client to the directory server and the data server. So in that sense, the client server relationship is interchangeable. Now the only reason for me to spend some time describing the IPC semantics of Quicksilver. Is because the recovery mechanism is tied intimately with the IPC. And in fact, that's how you can have the cake and eat it too in Quicksilver. In other words, the client server interactions have to use IPC. So the recovery mechanism, using transactions, rides on top of the IPC, essentially bundling the recovery mechanism with ICP to get it cheaply. Another interesting footnote I wanted to mention. The Quicksilver system was first conceived in the early 80s, but the first paper that described it appeared in 1988. And this is certainly the difference between academic research and industrial research at least in the olden days. Academic research, we tend to shout often. I'm an academic myself, so I take part of the blame. At least in the olden days, industrial research used to take the approach of publishing, a paper, especially in systems designed, when it is fully cooked. Like I said, Quicksilver was designed and implemented in the early 80s, 1984 to 88, but the first paper came out in 1988. But nowadays I have to mention that everyone is shouting often, which explains the proliferation of conferences that you see around the country and the world. Let's understand how Quicksilver bundles IPC with recovery management. The secret sauce for recovery management is the notion of transaction. It's a light weight version of transaction. Not the heavyweight version that you normally associate with databases. In that sense, the notion of a transaction in Quicksilver is very similar to what we saw in LRVM. In fact, it is perhaps fair to say that LRVM inherits the semantics of transactions similar to what was proposed in the Quicksilver, because Quicksilver predates LRVM. And the IPC calls a tag with transaction ID. Let's say a client makes a call to a server, an IPC call to a server. Now, because of location transparency, the client and the servers can be on different machines in the entire local area network. Under the cover, the client contacts the Quicksilver kernel on the node that it is on, let's say node A, which in turn contacts the communication manager that's part of the operating system of Quicksilver and implemented as a server process above the kernel. Now, this communication manager on node A contacts the communication manager on node B via the kernel on node B. So all of this is happening under the covers when a client makes an IPC call to the server. Depending on the nature of the client-server relationship there's going to be state associated with this client-server interaction. Further the communication manager may itself have state when it is communicating with its peer on a different node of the network. We would like to make sure that the state that is associated with the communication manager as well as the state associated with the high level client-server relationship that I am showing you, all of these are recoverable from failures. And the failures may be things like link failure or crashing of the server. Any of these will result in state being left behind and what we would like to make sure is that all such state that is left behind in the entire distributed system is recoverable. So under the cover the communication manager on node A contacts its transaction manager, and the transaction manager on node A in turn contacts its peer on node B. And now a transaction link is established as a way of recording the trail of client-server interactions to later facilitate picking up the breadcrumbs that may potentially be left behind when the client-server interaction terminates either successfully, or unsuccessfully. So, the creator of the transaction is the default owner of the transaction, that is the root of the transaction T. Others are participants. In this example I'm just showing you that a client-server relationship exists here. The transaction manager contacts its peer, and this guy says yes I'm willing to participate in this transaction. So this is the transactional tree that gets established as a by-product of the original client-server interaction. So the owner, that is the root of the transaction, is the coordinator for the transaction tree that gets established. And as we will see shortly this transaction tree can span several sites, because a transaction that starts here, goes to the server, may go to other servers. So, the creator of the transaction is the owner of the transaction. But the owner can also change ownership as well as change the coordinator for this transaction, and we'll see how that is useful in a minute. The client-servers can choose to remain completely unaware of transactions if they so choose. In other words, the mechanisms are there provided by the Quicksilver Operating System, but it is up to each service provider whether or not to use them. And there is no extra overhead for the communication that I am showing you between these transaction managers on these different nodes, because it happens naturally as part of the IPC that has to happen anyway in the distributed system. That is the communication that is needed for these transaction managers to handshake, to say that yes, I've received your transaction request, I want to participate in it, that is piggybacked on the normal communication that happens to support this interprocess communication between the client and the server, through the communication managers on these different nodes. They are piggybacked on top of regular IPC. So there is no extra overhead for the communication among the transaction managers. So a chain of client-server interactions leads to a transaction tree as client-server interactions can span multiple nodes, or multiple sites, on the local area network. So there is the the root of the transaction tree, who is the owner that initiated the original transaction through the IPC call, and these are all the participants who said that yes, we are also part of this IPC chain, and we will participate in this transaction that was initiated by this owner. Examples of how these client-server interactions lead to the creation of transaction trees. You can have a client making a call to a window manager, asking for something to be painted on the screen. And that under the covers will result in a transaction link between these two nodes. Or a client could make a request to a file server for opening a file. That would result in a transaction tree getting established between the participating nodes. And you can see in these examples what kind of breadcrumbs will get created on behalf of the client. The window manager may have opened up a window on the display and that's a piece of breadcrumb that had been created on behalf of this client. And similarly, the file server may have opened a file and kept some pointers to where this client is in that particular file. That's part of the breadcrumb that the file server is creating on behalf of this client. And those are the states that we would like them to be recoverable if in fact there is any failure. I mentioned earlier that IPC calls are tagged with the transaction ID that gets created under the covers. And these transaction IDs are automatic. The clients and the servers don't have to do anything special for that. But at the same time, they don't have to care about them either, if they don't want to use it in any fashion or form. The key point is the transactions are provided in the operating system from the point of view of recovery. And because a client-server relationship can traverse multiple nodes or sites in the local area network, it is important to make sure that recoverability has to worry about multi-site atomicity. A transaction that originated in one node may traverse several different nodes and leave breadcrumbs all over the place. All those have to be cleared up when a transaction terminates. So there is coordination that needs to be done, and this is where the transaction tree is very useful. And since the transaction that is used as a secret sauce in Quicksilver is purely for recovery management, semantics are very simple and there is no worry about concurrency control either. So due to the client server interactions that happen with the system, I'm giving you one specific example here. A client makes a call to a file system. And the directory server of the file system makes call to the data server where the file is actually located. So all of these are IPC calls, but under the cover a shadow graph structure emerges which is a trail of the client server interactions. The client opening the file so the TM on the owner has a link to a TM on the directory manager and the directory manager is calling the data servers and two different nodes and correspondingly, there is a link from the transaction manager on the directory server to the transaction manager on the data server on these two different nodes. So, if you look at it from the point of view from the transaction tree, the client is the owner of the transaction tree and all of these sites are participants in the transaction tree. I mentioned earlier that as the owner of a transaction tree, the owner has the right to delegate the ownership to someone else. Why would they want to do that? We think about it from the point of view of the distributed system that we are talking about. The most fragile parts of the distributive system are the clients. They are the fickle minded ones who can go away, who can abort a file request that they started, and so on. So in that sense if the root of the transaction tree is on a client node then cleaning up the bread crumbs may become hard if the client box goes away. Because the owner will go away, and that's the reason the owner can delegate the ownership to any node in the system and that node becomes the transaction manager for this particular transaction tree in terms of what needs to be done to clean up anything that happens adversely during the interaction that I'm showing you in this particular picture. So the heavy lifting that Quicksilver does has to do with what it needs to do under the covers in keeping this order tree of transactions. Corresponding to the IPC interactions from the point of view of recovery. Let's talk about how this order chain is managed by Quicksilver in order to facilitate recovery management. By the very nature, transactions are distributed at each node, and the transaction manager is responsible for all the clients over interactions that touches that particular node. For example, the client at this node could have done the following, he could have opened the window for reading Gmail. It could of opened a window in which is accessing file for editing,. Now the transaction manager node A, has to manage the transaction keys corresponding to each of the client server interactions, that is initiated by this client. Opening a window for Gmail is one interaction. Opening another window. Which is accessing a file for everything with another interaction. The transaction manager is responsible for maintaining the transaction trees for each one of those separately. And the transaction manager where the client server interaction originates. Is the owner as well as the coordinator. But it is designatable to some of the node as I mentioned already. And there is a graph structure for the transaction tree. And this is very useful in terms of reducing network communication because all the transaction manages. That form this interaction, don't have to always communicate with the coordinator. For instance, the transaction managers at nodes C and D, have been contacted because of IPC that originated at node B, to node C and D respectively. In that case, they only have to report To node B and they don't have to necessarily report to coordinator of the whole transaction sheet. So, this is helpful in reducing the amount of network communication. All transaction managers are not equal. I mentioned earlier that Brittle nodes in the system are the client nodes, and therefore, it is possible that a transaction manager, that originated at a client node may designate the coordinator to be a more robust node like the file server. And there are different kind of failures that can happen. There could be a failure of a participant node In the transaction. Or there could be a connection failure. Or it is possible that one of the subordinate transaction manager failed to report. All of these are sources of failure. Now, one of the things that the transaction manager at each node has to do, is to log periodically to persistent store, state that is created on behalf of the client. Or on behalf of the server whatever is happening at that node this transaction manager is responsible for creating checkpoint records for recovability reasons. And these checkpoint records will be useful for warding off against failures or partial recovery of work. So the distributed system failures can happen at any point. If for instance this node fails, then the transaction manager of this node has also failed, and this is something that this transaction manager is going to find out about because it doesn't hear any response from this transaction manager, but a transaction represented by this graph here, is not aborted at the first indication of failure of a node. And the reason is because you don't want error reporting to stop as a result of failure. You want the transaction to be aborted only upon termination as requested by the transaction manager, the coordinator of the transaction tree. And the reason is as I said, is to make sure that partial failures,they may have left states. You want to clean up all of that, and that will happen when a coordinator initiates termination of the transaction. We'll see that in a minute. When the client server relationship that resulted in this formation of a transaction tree, completes it's action, for example, let's say that, the transaction tree got created as a result of a client opening a file, doing a bunch of reads of the file, writes of the file, and finally closes the file. At that point, that interaction between the client and the server is complete. That's when the transaction tree, that has been created as a shadow of the original client's relationship, would get into gear and say, okay, now it is time to clean up any resources that may have been partially created. In order to satisfy that original client-server relationship. And it is the coordinator that initiates the termination of a transaction and the termination initiated by the coordinator can be for commit, or it can be an abort. And the different color-coded arrows show you what are all the kinds of commands that the coordinator may issue to its subordinates. Who in turn will issue the same commands to their subordinates. If the transaction manager, the coordinator, decides that it is time to commit, it might ask for vote request on committee. Or it might ask for an abort request. Or it might send an end of commit or abort. These are all the communications that would be initiated by the coordinator of the transaction. And correspondingly, responses will come back up the tree commensurate with the request that was given. So using again the file service as an example, if a client that started a file service request crashed for some reason, then the transaction manager that is a coordinator for their client server relationship will then send an abort request to all the participating transaction managers that got touched by that particular client, as a result of that file service request. For instance it could a directory manager, it could be data servers that are hosting those files and so on. All those transaction managers will get contacted by the coordinator through this transaction tree with a request to abort that particular transaction ID. And when they get that request, these transaction managers can do local cleanup, whatever that might mean. And indicate to the transaction manager by response that yes, we have done what it needs to be done, to clean up the interaction that was started by that failed IPC chain. All the different kinds of communication that I've indicated here are opportunities to tailor the commit protocols, depending on the criticality of the states. That is, the nature of the bread crumbs that are going to be left behind in the different servers. So, for instance, if the transaction tree is representing a client-server relationship between a client that opened a window using the window manager on the screen. Then if the client crashes, then the transaction manager who was the coordinator for that particular client will indicate to the transaction manager at the window manager's site that this particular transaction ID is aborting. And in that case, the window manager can simply clean up the state, because the state that it has got is a volatile state. Namely, a window that it created on behalf of the client, so it can take care of it internally. It need not be persisted. On the other hand, if the interaction that we're talking about involves persistent data structure, for instance through a file server, then the file server may have to take more complicated action. So that is what this transaction tree allows you to do depending on the nature of the client server relationship. You can choose to use different commit protocols, depending on the criticality of the states that are associated and the nature of the breadcrumbs that may be left behind in the different sites, because of the client-server relationships. Persistent servers such as a file system may need a sophisticated commit processing, such as a two-phase commit protocol while window manager will only need a simple one phase commit protocol. So all of those are possible with the structure of the transaction tree and the kinds of requests that flow through the transaction tree both down the tree and up the tree in response. And this is the heavy lifting that is done by the operating system in order to support different classes of services that might exist in the operating system, which may require different recovery management. The upshot of bundling the IPC and recovery management is that a service can safely collect all the breadcrumbs it left behind in all the places that it touched through the course of its service. Examples of such breadcrumbs include memory that was allocated but not reclaimed, file handles, communication handles, or often windows that have been created on the display. All of these are breadcrumbs that may be left behind by failed clients or servers. And the fact that you have a transaction tree that records the trail of all the nodes that were touched and the temporary states that were created in all these nodes allows these breadcrumbs to be cleanly reclaimed. And there is no extra communication for the recovery management itself, because whatever communication is needed for the transaction managers to talk to one another in participating in the transaction that is shadowing the IPC, actually rides on the IPC itself, and therefore it comes for free. And Quicksilver only provides the mechanisms. The policy is entirely up to each service. And in fact, services can simply ignore these mechanisms if they don't need any recovery management, or choose a policy that is commensurate with the type of service that it is providing. Because there's a variety of mechanisms available in the operating system, simple services may choose to use low-overhead mechanisms. Whereas a more involved service such as a file server may use weighty mechanisms in order to recover from failures. The overhead for recovery management in Quicksilver is very similar to what we already saw in LRVM. The transaction managers have to write log records at every one of these nodes. Because they are handling the interactions between clients or servers at this node with respect to other sites, the transaction managers have to write log records for recovering persistent state. Similar to LRVM, Quicksilver also uses in-memory logs for the transaction managers. And these transaction managers flush the in-memory logs that they create to record persistent states to disk periodically. How often these in-memory log records are written out to the disk by the transaction manager is a performance factor, and it is a window of vulnerability just as in LRVM. Ideally, if you want to be absolutely sure that you can recover from failures, whenever any persistent state is modified and a log record is written by the transaction manager, that log record should be committed to the storage. But that costs synchronous I/O, and so if you're worried about that, you might want to do that more opportunistically, assuming there are not going to be too many failures. So there is a performance vulnerability trade off in how often the transaction manager writes log records to the storage device. So one of the key aspects of Quicksilver implementation is Log Maintenance. The transaction managers write the log records for recovering the persistent state. They're written in the log records that are in memory data structure of the transaction manager, and every so often, the transaction managers do a Log Force. Of the in memory log segment to the storage, for persistence. And the frequency of log force impacts performance, of course, because they are synchronous I/O operations. And log force can also be initiated by an application if it is concerned about about its persistent state. Getting onto storage but in Quicksilver an app has to be very careful about often they do a log for us. Because log maintenance is done by the transaction manager at a site, for all of the processes that are running at that node. And so the large record in Quicksilver actually contains all the modifications to persistent state. Required by all the processes that are running at this node. And therefore, if an individual client at a node decides to do a log force, it is actually impacting the performance of not only that particular client, but all the other clients as well. Therefore, services have to be very careful about choosing the mechanisms That is available in operating systems. Commensurate with their recovery requirments. The intent in this lesson is to give you a feel for how some enduring concepts stand the test of time. The ideas in Quicksilver, namely, using transactions as a fundamental operating system mechanism to bundle and stage recovery of operating system services. Found resurgence in the nineties, in the LRVM work that we discussed earlier for providing persistence. Again, in 2010, it found resurgence in the form of providing safeguard against system vulnerability and malicious attacks on the system. And another research operating systems called Texas. We will mention Texas when we cover a later lesson module on system security. What about the computer industry and the commercial operating systems? Well, they always focused on performance. Reliability always takes the back seat. You will be amazed, what goes on under the covers of an operating system to gain performance. Just as an example, you write to a file, and you think it is on the hard disk. Well, think again. It is a while before your file write actually gets persisted on the disk. What if the system crashes in the meanwhile? Well, too bad. Well things may change in the future. There are new kinds of memories called Storage class memories, that have latency properties that are similar to a DRAM and are yet non-volatile. Will this new technology result in a resurgence of exploring transactions once more in operating systems? Only time will tell. What are the systems issues, in managing large data centers? How do you program big data applications, such as search engines, to run on massively large clusters? How do you store and disseminate content on the web, in a scalable manner? Hello and welcome back. These are the kinds of the questions we will be addressing in this module of the course. This is what we mean when we use the term internet scale computing. A comforting factor for you is that you already know the answer to these questions. We've learned through the previous lessons, most of the techniques needed in constructing distributed services. What we're going to see in the upcoming lesson, is a hardening of the distributed systems issues to handle scale on the order of thousands of processes and failures. It's not a question of if. It's a question of when a failure's going to happen. Let's start off this lesson with a fun trivia question. Check off the ones that you see below as giant scale services. I just want you to think what are the services that you use on sort of an everyday basis which you think are giant scale services. Online airline reservation system, purchasing, for instance, your next laptop online, all of you may be using web mail, searches that you may be doing on the Internet, and let's say this weekend you want to watch a movie and you're streaming it from Netflix. As you may have guessed, the right answer is all of the above. yes, these days, most of what you do on an everyday basis with a few clicks on your personal mobile device, or laptop, or desktop, it's actually using some giant scale service somewhere out there on the Internet. All the way from airline reservation to watching a movie online. In this module, we will cover three topics. The first is systems issues in providing giant scale services on the internet. And the second topic is what exactly is a programming model that is used for the applications that are delivering the service that you're expecting on an every day basis? So-called Big Data applications. And the third topic is content distribution networks that are used for the dissemination of the content when you access giant scale services on the internet. So in this lesson, of course, we are focusing on what are the issues in giant scale services, from the perspective of the system. Imagine you're going on the Internet and reaching a Web portal, such as gmail, let's say, for accessing your email. In fact, while you're doing this, a million other clients are doing exactly the same thing, trying to reach the web portal through the IP network. The service that you're trying to reach may be architected to direct your particular request to a specific site. And the architecture within that site may look like this, a whole bunch of servers, maybe on the order of thousands or even 10,000, within a cold dark room, interconnected to one another through perhaps a high bandwidth communication backplane. And also all of the servers are connected to data stores for processing requests that may be coming in. And the servers may optionally use a backplane that allows them to talk to one another for servicing any particular request. Any of the servers may be able to handle an incoming client request. Now in the generic picture that I'm showing you here, the load manager that is sitting in between these servers, and the IP network redirects an incoming client's request to one the servers. The chosen server may than process the query and answer the client. The role of the load manager is to balance the client traffic among all the servers. Basically, the load manager has to make sure that no particular server is overloaded. One of the characteristics of most of the giant scale services is that the these client requests that are coming in are all independent of one another. Sometimes, this also called embarrassingly parallel, meaning that all of these client requests can be handled in parallel. So long as it is enough server capacity to meet all the incoming requests. So the rule of the load manager is to balance the client traffic and redirect them to the server so that no particular server is overloaded and all of the servers get equally utilized in servicing incoming requests. The other responsibility of the load manager is to hide partial failures. Because when we're talking about the scale of giant scale services, and the kind of computational power that resides in the data center for servicing these giant scale services, they're humongous, they're 10,000 nodes. And when you have so many nodes, compute nodes and data nodes and so on, it's not a question of if something will fail, but is a question of when something is going to fail. So the load manager has to make sure that it is observing the state of the servers and insuring that the incoming client requests are shielded from any such failures that may happen internally, within a particular site. Compuptational clusters are the work horses of giant scale services, any modern data centers employs on the order of thousands and thousands of computational nodes connected by high performance, high speed networks. It is such computational clusters, which are serving as the work horses for giant scale services today. And this figure reproduced from Eddie Gruer's paper circa 2,000 shows the number of nodes in a data center, maybe on the order of 1,000. But scale it up by ten or 100 in terms of today's computation capacity that you might find in a typical data center. And the kind of volume of queries that are going to be handled. By giant scale services today. Each node in the cluster may itself be an SMP. There are serveral virtues for structuring the computational resources inside a data center as a cluster of machines connected by high performance back planes. The advantages include absolute scalability. You can keep adding more resources. Without worrying about re-architecting the internals of the data center. Cost and performance is another important issue and that is by having computational clusters at every node is identical to others, is that as a system administrator we can control both the cost, And the performance of running a data center like that, and because of the independent components you can easily mix and match generational changes in the hardware, and the results are incremental scaleability you add more resources you get more. In terms of performance because, as I told you earlier, most of the queries that come into giant-scale services tend to be embarrassingly parallel. So the more resources you've got, the more queries you can handle. There is incremental scalability when you're using clusters because you can add more nodes to the cluster, increasing the performance. Or, if the volume, of request come down, you can scale back. That's the advantage of using computational clusters as the work horses for data centers that are serving giant scale services today. As a refresher, I'm showing you the seven layer OSI reference model of the protocol stack. Now, load management, as I mentioned, for dealing with client requests, can be done at any of the levels of the seven layer OSI model. From the network layer and up. And higher the layer, you have more functionality that you can associate with the load manager in the terms of, how it deals with the server failures, how it deals with directing incoming client requests to the different servers, all of these are possible, the higher the layer at which you construct the load manager. The load manager may be operating at the network level, that is, at the network layer level of the seven layer OSI reference model. In this case, the load manager is simply a round robin domain name server. What it does is, when client request comes in, they all have the same domain name, in order to come to this particular server. If you say gmail.com, then depending on how the server is architected, the gmail request coming from particular client may go to a particular site. And when it goes to that site, the domain name server, which is acting as a load manager, what it is going to do is, it is going to direct the incoming client request to one of the servers. So it is giving different IP addresses. Even though the domain name a client is coming in is exactly the same, the round robin DNS server assigns different IP addresses corresponding to different servers to the incoming client request. And that way these clients are being redirected to different servers, and because the load manager is doing it at the level of individual server addresses, you get very good load balance. And the the inherent assumption in this model, is that all the servers are identical, so an incoming request can be sent to any one of these servers, and perhaps the model is also that the data is fully replicated. So that if an incoming request goes to this server, it has access to all the data. If it goes to this server, it has access to all the data to satisfy the incoming client request. So the pro is good load balance because the Round Robin DNS scheduler can choose the least loaded server to redirect an incoming client to that particular server. But the con is, it cannot hide down server nodes, because it is assigning IP addresses of the server to the incoming request, the load manager is not able to hide down server nodes from the external world. So now we move the load manager up in the OSI reference model. Up to the transport level or even higher. The load manager could be layer four switches. That is transport level switches or even higher level switches. And, in fact, the layer four switches may be architected as switch pairs so that there is hot fail over from one fail switch to another fail switch at the load balancer lever. When you do this architectecting the load balancer at the transport or higher levels. It give you the opportunity to dynamically isolate down server nodes from the external world, and it is also possible, with this architecture of load manager, to have service specific front end nodes. For example, instead of dealing with arbitrary client requests, we're actually dealing with requests based on the kind of requests coming in from a particular client. For instance, this could be a Gmail client. This could be a Google search client. This could be for a photo server like Picasa. So those are different functionalities that can be implemented at the front end of this load manager to look at the service, the particular program, that is actually triggering a client request coming in to this particular site, and make decisions as to who should serve that particular request. So in other words, we're building more intelligence into the load manager. So that it can know the semantics of the client server communication that is going to happen based on the type of request that is coming in. And it is also possible, with this structure of having the load management at higher levels of the OSI reference model, to co-opt the client devices in the load management. For example, it's possible that the load manager may even know the device characteristics that the client is using, in order to come into the site. For example, if it's a smart phone, it might take certain actions commensurate with the device that is coming in, in terms of structuring the client-server interactions. Now, the data itself may be partitioned among all the servers. For example, let's say that you are doing a web search and it is a web search that came in as a clients request. And, if it is redirected to a particular server, this server may not have access to all the data that it needs. In order to do that, Google search that you came in with. In that case, the server may communicate using the back plane with other servers to get the data that it needs to serve the queries that is coming in, so that there is complete coverage of whatever the intended query needs to get as a result. Of course, if the data is partitioned, if a server that is holding a portion of the data is down, then there'll be data loss for the query. So for your specific search, the server that is handling your search may not be able to give you complete result for the search, because a portion of the data is down due to the partitioning of the data among the different servers at this site. Now, this is where replicating the servers for redundancy helps. So even though you may partition it, you may replicate partitions so that it is possible that even if one server is down, another server that has a replica of the same partition may be able to provide the data that is needed in order to satisfy a particular query. Thus far, I've given you a general idea of how an individual site that hosts computation resources and handles a whole bunch of incoming client requests may handle these incoming client requests in terms of load management. This brings us to the DQ principle. The server has all the data that is required for dealing with incoming client queries. So this the full data set, or DF if the full data set, that is required for handling any incoming queries to the server. Now clients come in with their queries and we will call Q0 as the offered load to the server, meaning this is the amount of requests that are hitting the unit per server time. Even though the offered load is still Q0, maybe the server could only manage to serve a portion of this incoming requests. And that portion of the incoming requests that is completed by the server is Qc. These are the completed requests. We define the yield Q as the ratio. Qc over Qo. That is the ratio of completed requests to the offered load. So, Q is going to be a fraction between zero and one. Ideally you want it to be one so that all the client requests are serviced, but if the server is not able to deal with the offered load entirely, then the yield is going to be less than one. Now each query that is coming into the server May require processing all of the data that's available in the server to answer this particular query. For instance, if there's a Google search and the search requires looking at all the corpus of data that the server has to answer the query, then you want to look at the full data set DF. However, it may be that because of either failures of some of the data servers or the load on the server, it is only able to process the query using a portion of the total data set, Dv. Dv is the available data. That is used in processing the query, in that case the harvest D is defined as the ratio Dv over Df. That is harvest is defined as the ratio of the available data to the full corpus of data. Again, this is going to be a ratio between zero and one. Ideally, you want to make sure that the harvest is one, meaning that incoming requests is completely served with all the data that it wants. Depending on the service I can give you different examples. So for instance if the incoming request is a search, you want to look at the whole corpus of data that is available to you to answer that search. But if you are not able to do that and the server is only using a portion of the full data in the harvest in terms of the quality of the results that is being given out to a particular client is less than one. The product DQ, that is the data server query, and the rate of query that is coming into the server. This DQ represents some sort of system limit for a given server capacity the product DQ is a constant. Said differently for a different system capacity, we can increase the number of clients that we are serving if we reduce the amount of data that we're using to process the incoming queries. That is, we're increasing Q by decreasing the harvest or D. Or can do the opposite and that is, we can entertain a smaller set of queries. That is the yield will come down but we will be able to give the complete data that is needed for serving that query. An example of would be, if I am a client that is accessing a server for my mail, I would want to get all my mail, not part of the mail. So in that case, I will be happy if the server gives me a harvest of one, even it means that not all the incoming clients maybe served by the server due to the capacity limitation of the server at any point of time. So as a system administrator, we have some choices to play with. The important thing is that in giant's scale services. The performance is limited by network and not by the I/O capacity. We can have as much I/O capacity as we want by having more hardware resources here, but what we are bound by is the network capacity. So if we increase the capacity of a server, we have a choice as a system administrator. We can either increase the harvest, keeping the yield the same. Or, we can increase the number of clients we are serving. That is, we can increase the yield, keeping D a constant. That's the knob that a system administrator can play with in terms of dealing with capacity increases at the server. Similarly, if I am a system administrator, and some nodes in the server fail, then again we have a choice of either decreasing the harvest or decreasing the yield, in order to deal with reduced DQ value. Of course, it all depends on how the server itself is architected, if the failures can be hidden through replication and so on. But the important message I want to convey to you is that this DQ represents a system constant, so far as the server is concerned, in terms of the capacity. Given a particular capacity of the server, DQ is fixed. So as a system administrator you have a choice of either sacrificing yield for harvest or harvest for yield. You cannot increase both the harvest and yield without increasing the server capacity. At traditional measure that has been used in servers, is the notion of IOOPS, or I/O operations per second, and these are meaningful in database kinds of applications. Where the applications are disbound, but the giant scale applications are network bound, and for network bound applications this manager DQ is much more intuitive as to what is going on in terms of managing the incoming load to the server - And managing the corpus of data that the server has to look at in order to satisfy incoming queries. Another metric that system administrators have often used is what is called up time. You may have heard of terminologies like mean time between failure and mean time to repair. Meantime between failure is, what is the expected time between two successive failures inside a data center? And similarly meantime to repair is if a failure is detected, how much time does it take to bring up a failed server, that need time to repair. And the up-time is defined as the ratio of the difference between, mean time between failure and MTTR, that is mean time to repair, and MTBF, that is uptime is equal to MTBF minus MTTR divided my MTBF. That is uptime. And you want the the uptime to be close to one. And usually uptime is measured in nines. Meaning point nine, nine, nine, nine. Five nines or six nines and so on. So you want the up time to be as close to one as possible for giant scale services. But again, this up time as a metric is not very satisfying because if there are no queries giving the time that is needed for repairing, that is, during the MTT par, the mean time to repair, if no queries come into the server, then you haven't made anybody unhappy. In that sense, the uptime is not a very intuitive measure of how well a server is performing. It is better to look at yield as the output metric. And the harvest, again, is the output metric for say how well a server is able to deal with the dynamism and scale of requesting handled by a particular giant-scale service. We'll see that this DQ principle is very powerful in advising the system administrator on how to architect the system. How much to replicate? How much to partition in terms of the data set that the server is handling? How to deal with failures? How to gracefully degrade the servers when the volume of incoming traffic increases beyond a server capacity? All of these are questions that can be handled very elegantly using the DQ principle. And the key underlying assumption in the DQ principle, is that the giant scaled services are network bound, and not I/O bound. In architecting the data store and the computational resources that back the data store at a server, the system administrator has a choice of either replicating or partitioning the data. If you replicating the data what that means is. That every data server has the full corpus of data that is needed to serve a particular request. Think of it as say, gmail. If it is gmail, then if a gmail request gets sent to anyone of these servers. They can handle the request that comes in because they have the full corpus of data to deal with a incoming request. We mentioned that failures are inevitable in giant scale services because of the number of computation elements that live within a data center. If failures occur, what happens when we have replicated data? What they means is that the harvest is going to be unchanged because if some data repository is failed. Because it is replicated, we can redirect a client's request to another live server that has full access to the later repository and therefore the harvest that you're going to get with replication in 100%, it is unaffected. On the other hand because the server capacity has come down due to these failures, the heal is going to come down. In other words, with replication. The service is unavailable for some users for some amount of time. But all the users that are able to get the service, get complete harvest in term of the fidelity for the query that is returned and answered by the server. The other alternative of course for architecting the internal data depository of the server is to partition the data. So if you partition the data, and lets say that There are end partitions of the full compass of data. And let's say that some of the service fail, then some portion of the corpus of data becomes unavailable. If this happens, what it means is that the harvest is going to be done because some portion of the data corpus. Is unavailable, and therefore the harvest is going to come down, but it is possible to keep Q unchanged, so long as the computation capacity is unchanged. Then we can serve the same number of user community. It's just that the fidelity of the result may not be as good as when All of the data corpuses available. For example, if this is a server that is serving search results and if some portion of the data repository is unavailable. In that case, each query that comes in will get service, but it is going to get. A subset of the query results for the search that they are doing. So the important message that I want to convey through these pictures is that dq is independent of whether we are replicating or we are partitioning the data. Given a certain server capacity, The D Qs are constant, because we are assuming that this is cheap. This space is cheap and therefore processing incoming requests are not disc bound, but it is network bound. The only exception is if the queries Result in significant in right traffic to the disk. Now this is rare in giant scale services. But if that happens, then replication may require more DQ than partitioning. But as a first order of approximation, so long as we assume that giant scale services are serving client requests which are network bound, The DQ is independent of the strategy that is used by the system administrator for managing the data whether it is replication or partitioning. Now what this also means is that this is giving an object lesson to the system administrator. In saying that beyond a certain point, you probably want to replicate and partition. Why? Because failures are bound to happen, and a failures are bound to happen if you partition the data, then a portion of the data corpus becomes unavailable. On the other hand, if beyond a certain point Even if you have partitioned data, if you replicate it, then you're making sure that, even if one replica is down, some of the replica, of the same partition is available for serving a particular client request. And replication, beyond a certain point is important because Users would normally prefer complete data or a complete harvest. Take for instance, you're accessing your email through the Internet. You would rather put up with a service being unavailable for some amount of time rather than seeing that you only have access to a portion of your main box. And that's the reason why replication beyond a certain point is better for dealing with giant scale services. On the other hand, for searches, it may be okay to have a harvest which is not complete. So in structuring different services as a giant scale service, one has to make a decision. Of whether partitioning is the right approach or at what point we may want to partition and then replicate as well. As a concrete example, Inktomi which is a CDN server uses partial replication It uses full replication for email because of what I just said, and that is a user would expect complete harvest for a service like email. But on the other hand, if it is a web cache that is being implemented as giant-scale service, it is okay if it is not replicated. And the DQ principle is also very useful for managing graceful degradation of service. So DQ defines to total system capacity. So if a server is saturated, meaning that we have reached the limit of the server in terms of DQ. That's a constant. DQ's a constant. And so if you reach that limit, then we have a choice of graceful degrading the service from the point of view of the client. One possibility is we keep the harvest the same meaning that every client request that comes in has complete fidelity in terms of the answers returned by the server. So D is fixed. Q comes down because DQ is a constant. That's one option. The other option, of course, is to keep the volume of clients that are service, that is the yield Q to be a constant, but decrease the harvest. So the fidelity of the results returned to the users is less than. 100%, but we keeping more of the user community happy by serving more of them. Because DQ is a constant, it allows us to gracefully degrade the service being provided by the server depending on the choice you want to make in terms of fidelity of the result or the yield that you want to provide the user community. In other words. The DQ principle gives us an explicit strategy for managing saturation. So as a system administrator, when we make these decisions on which to sacrifice and which to keep constant, we know how our decisions are going to affect the harvest, how it's going to affect the yield. How it is going to affect the up time and so on. So the choices that a system provider has or strategies that a system provider can use in structuring the servers, knowing that DQ is a constant, is when the server is saturated. They can do cost based admission control. You pay more, you get more. That may be one way to do it. Or priority or value based admission control. That may be another way to deal with service saturation or reduce data freshness. That is the harvest may be the same, but you reduce the fidelity of the data. For instance, if I'm serving videos, and I can serve the videos at different bit rates, if the server is saturated, I might decide to serve the video to all the users, all the videos that they want, but at a lower bit rate. So that is reducing the fidelity. Of the data that is being harvested. So that's the idea behind the DQ principle, how it might be used for graceful degradation of service when the server is saturated. As a giant scale service provider, another thing that the service provider has to worry about is online evolution and growth of the service. because services are evolving continuously. The Gmail that you're accessing for getting your email today may not be the same that you accessed a month back. So, since the services are continuously evolving, the servers at the data centers have to be upgraded with a new version of the software or maybe new nodes are being added, or maybe all the old nodes are being retired, and brand-new nodes are being brought in to serve the user community. Whether it is hardware or software upgrade that needs to be done, that has to be done with the understanding that while that upgrade is happening, there is going to be loss of service. Again the DQ principal comes in handy in measuring what exactly is the loss that we're observing of the service when we do this kind of online evolution and growth. The first choice, is what is called fast reboot. That is, bring down all the servers at once, upgrade them, and then turn them back on. So this diagram is showing you, time on the x axis, and the loss, DQ loss, on the y axis, and what you're seeing here, this is the time for the upgrade per node. So the amount of time a node is going to be down in order to do that upgrade, whether it is a software upgrade or a hardware upgrade. And this y axis segment is the DQ loss per node. So if I bring down a node and upgrade it, it's going to be down for some amount of time. And this is the amount of server capacity that is lost, the DQ that is lost, as a result of one server being down. If all the servers are down, which is what is happening with fast reboot, then for this entire time, the area bounded by this green-shaded rectangle, we're going to have no service. The ideal is here in terms of the total amount of DQ we want, and the access is saying how much is the DQ that is lost and what we are saying is, if we do this fast reboot, that fast reboot is going to bring all the servers down for a certain amount of time that it takes to upgrade them, and for the entire duration, the service is unavailable. This idea of fast reboot is particularly useful in situations where we can use diurnal server property. For instance, if you think about services that are being all across the globe. We can use factors such as oh, this is nighttime for the user community and the users have probably gone night-night and therefore, they are not going to access the servers. This is a good time to bring down the whole service and upgrade the service. So that might be a situation where fast reboot is very useful. So we are assuming that the user community is segmented so that they're directed to geographically different locations, and we can use the diurnal server property to do this off-peak fast reboot of chosen replicas of the services. In this figure, so in this figure this segment that I'm showing you here is the DQ loss per node. And the total DQ losses, of course, n times DQ, where n is a number of servers that are undergoing this fast reboot. So in other words, for u units of time, there is complete loss of DQ with fast reboot. An alternative to fast reboot is what is called a rolling upgrade. Now here, what we're doing is, rather than bring all the servers down at the same time, we are bringing one server down at a time. And upgrading one server, bringing down the next server, upgrading it, and so on. This is what is called rolling upgrade or wave upgrade. And in this case, services available all throughout, there is no time that we are saying that the service is completely unavailable. But, for the entire duration of time, the duration of time here is n times u, because u is the upgrade time per node, and the upgrade, because it is a wave upgrade, is going to last for n times u time units, where n is a number of servers being upgraded in this fashion. But during the entire time, service is available, but in every u units of time, there's a DQ loss of this much bounded by this area during the entire upgrade process. A third alternative is what is called a big flip. In the big flip, what we are doing is we are bringing down half the nodes at once. So in other words, the service is available at 50% of the full capacity. So you can see that with a big flip, half the nodes are down. So we have reduced the server capacity, the total DQ available by 50% for u units of time. And the total upgrade is going to last 2 times u because we have partitioned the entire server capacity in two halves, and upgrading one half, upgrading the second half for 2 times u time units we're not going to have full capacity. And during this 2 times U time unit we get the service at 50% capacity, and at the end of this it is back up to full capacity. So these are the three different choices that you've got, the fast reboot where the upgrade time for the entire service is just equal to the upgrade time for a single node. But here, we don't have the service available for any of the users for that period of time, and this might be a, a good strategy for services that can exploit the diurnal property of user community. Rolling upgrade is the more common one, where what we are doing is we are upgrading the servers one at a time but it's going to take a long time especially when you're talking about data centers having thousands and thousands of processors. Rolling upgrade is going to take a long time and it might be done in batches for instance instead of being exactly one at a time. And the extreme of that rolling upgrade is this big flip where we are saying we'll bring down 50% of the nodes, upgrade them, and then turn them back on, and then do the upgrade for the remaining 50%. So in the third case, in the big flip, the service is always available, but at 50% capacity for a certain duration of time. But there is no way to hide the DQ loss. And you'll see that the DQ loss, which is shown by the area of this shaded rectangle in each one of these cases, the area in the shaded rectangle is exactly the same. In other words, the DQ loss is the same for all three strategies, and it is the DQ loss of an individual node, the upgrade time for an individual node, and the number of servers, n, that you have in your server farm. That's the total DQ loss for online evolution, and all that these different strategies are saying is, as a system administrator, you have a choice on how you want to dish out the DQ loss. And make it apparent and not apparent to the user community. In this case, it's going to be apparent to the entire user community that there is upgrade going on. In this case, different segments of the user community are going to notice that the service has become unavailable for a short duration of time. And here it's sort of in between these two extremes and that half the user community may see a service being unavailable for some amount of time. So, in other words, using the DQ principle, maintenance and upgrades are controlled failures that can be handled by the administrator of a system service. So what we are seeing through this concept of DQ is a way by which a system developer and a system administrator can work together in figuring out how to architect the system in terms of how the data should be partitioned or replicated and how the system should fine-tune the service by looking at the instantaneous offered load and tuning whether to keep the yield the same or the harvest the same, knowing that DQ is a constant and the server is getting saturated. And finally, when service has to evolve once again, the system administrator can make an informed decision on how to do this online evolution by controlling the DQ loss that is experienced by the user community at any point of time. The key insight is that giant scale services are network bound and not disk I/O bound. This is what helps in defining the DQ Principle. The DQ Principle really helps the system designer in optimizing either for yield or harvest for a given system capacity. Also, it helps the system designer in coming up with explicit policies for graceful degradation of services when either servers fail or load saturation happens or upgrades are planned. We saw the system's issues at a macro level from the point of view of administering the engine that powers internet scale computing. How do we program services such as websearch and airline reservation on top of the data center cluster resources? How do they exploit the hardware parallelism that's available in these clusters? We will learn about the map produced programming paradigm that answers these questions. The term Big Data, has become a buzz word. Computations in giant scale services are usually simple, but they work over large data sets and hence, the name Big Data. And naturally, because they are working with a large data set, these computations take a long time to compute. For example let's say that I want to search for John F Kennedy's photographs in all the documents that are available on the web. It's going to take a long time. And that is an example of a Big Data computation. Similar examples include online reservations, shopping, et cetera. And applications that work on Big Data would like to exploit the pluralism that's available in the cluster at the data centers. I mentioned in the last lesson that data centers these days comprise of computation elements on the order of thousands and even 10,000 nodes that are ready to do computations. And we would like to exploit the computational resources available in such Big Data centers to do computation on Big Data. For example, if it is John F Kennedy's photographs, I'm looking in all the different documents. We can parallelize that's search for Kennedy's photo in all the documents in parallel on all the available nodes in the data center. And since computations are also called embarrassingly parallel computations. What does this mean? Well, there is not much synchronization or coordination that is needed among the parallel threads that comprise the applications and that run on different nodes of the computational cluster. Looking for John F Kennedy's photographs in all the documents on the web, is a good example of such an embarrassingly parallel application. What are all the issues in programming in the large, on large data sets, so the data sets are also large and the computational resources on which we want to run this big data application is also vast. So some of the interesting issues in programming in the large include, how to parallelize an application across let's say, 10,000 machines. How to handle the data distribution and plumbing between the producers of data and consumers of data. Remember these are embarrassingly parallel applications, but different phases of the application will require data from one phase of the application to be passed on to the next phase of the application. That's what we mean by plumbing between the producers of data, intermediate data to be more specific, and consumers of that intermediate data. And of course, one of the biggest challenges in Big Data applications on large computational clusters is failure handling. Recall what we said about the nature of these data centers. They employ thousands and thousands of processes. When you have so many parts, in any setting, failure is not a question of if it will happen. It is a question of, when it is going to happen? So that's a fact of life, and therefore, programming models for Big Data, have to worry about the fact that failures are to be expected in this environment. In this lesson we're going to look at one specific example of a programming environment for dealing with big data applications running on large computation clusters. And this programming environment is called map-reduce programming environment. And in this programming environment, the input to the application is considered as a set of records Identified by a key value pair. So the big data app developer, supplies the run time environment with two functions called map and reduce. And these are the only two functions that the app developer has to supply to the programming environment and both map and reduce - Take user defined key value pairs, as inputs, and produce user defined key value pairs as outputs. So both the input and the output to each of the map and reduce functions, written by, the domain expert, are key value pairs. Once again, these are key value pairs defined by the app developer. Specific to the particular application that he or she is coding. Let's consider a fun example. In this example, I'm going to say that we're looking to find specific names of individuals in a corpus of documents. Maybe the names are Kishore, Arun, Drew, and so on. These are the specific names that I want to find in a whole corpus of documents. So the input to the application is a whole corpus of documents, and we are looking for specific names and the number of times those names occur in those documents. So that's what the appllication is going to do. So the input key space for the application is the file name. And the value part of the key value bares the content of the file. So if you have n files in the corpus of documents that we want to analyze in this fashion, then you have N key value pairs corresponding to each of the different files and the respective contents. So this is going to be the input to the whole application and were going to structure this application using this programing paradigm of map reduce. And we will see how the app developer will use the map-reduce framework to accomplish the goal that we set out. Which is to find the number of occurrences of unique names, in this corpus of documents. In this example, the user defined map function is looking for each of the unique names that we're interested in the corpus of documents. So the map function will take as an input a specific file and the contents of the file as the key value pair, and emit a value that corresponds to the number of times each one of these names occur in the input file. Now, a simple version of the map function may emit a one every time it encounters the name Kishore in the input file, or the name Arun in the input, or the name Drew in the input file. The output of the map function is a key value pair, where the key is the unique name that you're looking for. One of the unique names that you're looking for, and the value is the number that says how many times it found that particular name. As I said earlier, a simple version of the map function could emit the value of 1 every time it finds one of the specified names that it is looking for in the file. Or a slightly more elaborate mapper may actually combine the number of occurrences of a particular name in the input file and then [UNKNOWN] that as a sum of all the times a particular name occurred in this key value pair. In either case, the output of the map function is itself a key value pair. Note that it is different from the input key value pair. The output key value pair that the mapper is emitting is a unique name and a value that is associated with that unique name and from this example it should also be evident that we can spawn as many instances of the map function as the number of unique files that we have in the input document corpus. Further, since each of the map function is working on a different key-value pair as an input, they're all independent of one another and there is no need for coordination among them. And that's the feature of an embarrassingly parallel application. The output of the mappers are the input to the reducers. In other words, the output key value pair from the mapper is the same as the input key value pair for each of the reducers that you see here. Again this reduce function is something that is written by the user. And you notice that what the mapper is doing is when it notates a particular name in this example that it is looking for like Kishore then this mapper is going to send the value associated with Kishore to this reducer. Similarly this mapper is going to send the value associated corresponding to Kishore to the same reducer. And all the mappers in the same fashion are going to send the respective values that they phone for this unique name Kishore, to this reducer. Similarly, the values phone for the name Arun, is going to be sent to this video set by all the mappers, and so on. So the number of reducers that we will have is the same as the number of unique names that we are looking for in the corpus of documents. This is where work needs to be done by the programming environment to plumb the output of the mappers To the inputs of the reducers. Potentially, the mappers could be executing on different nodes of the cluster, reducers can be executing on different nodes of the cluster, and the work that is involved in this plumbing is really making sure that the outputs of the mappers are fed to the corresponding correct reducers in the entire distributed system. For example, the output that is coming out of the mapper corresponding to Kishore has to be sent to this reducer from all the mappers. The output corresponding to Arun coming from all these mappers has to be sent to this reducer, and so on. That's what we mean by the plumbing that needs to be done by the programming environment to facilitate the communication That needs to happen between instances of the mapper and the instances of the reducers. The work performed by each one of these reducers is going to be aggregation of the values that it got for each of the unique names that we're looking for in the input document corpus. So this reducer's job is to aggregate all the instances of Kishore that it got from all the mappers. So it is receiving the key-value pair corresponding to the name Kishore and the number of instances that they found in the input key-value pair that they processed, and the reducer is going to aggregate that And the output of the reducers, all the reducers, is once again going to be a key value pair. It is exactly the same as the input key value pair that the reducers got. Namely, the key is the unique name that they've been assigned to aggregate, and the value is the total number of the occurrence of this unique name. In the corpus of documents we started with. So this is the roadmap for creating a map-reduce application for a chosen application that works on big data. So in this case, the chosen application was looking for unique names in a corpus of documents, and all that the domain expert had to worry about, is deciding what is the format of the key value pair, which is the input to the whole application, in particular, the map phase of the application. And also, what is the format of the intermediate key value pair that is going to be generated by the mapper function. And what is the work that needs to be done in each of the mapper and the reducer to carry out the work that needs to be done for this particular application. Beyond that, the app developer does not have to worry about any of the other details, such as how many instances of mappers do we need to create. How do we create the number of instances of reducers corresponding to what this application is wanting to do, and also worrying about the plumbing from the output of the mappers to the input of the reducers, all of those, are things that the app developer does not have to worry about. The question is why map reduce is a programming framework for big data applications. It turns out that several processing steps in giant-scale services are expressible as map reduce. For example, let's say that you're looking for seat availability for selected dates to selected destinations on different airlines. That can be expressed as a map reduce computation. Let's say you want to access frequency of the URLs on the website that you've designed. That can be quoted up as a map reduce application. Let's say you want to create. Word indexes to facilitate document searches on the web. That can be coded up as a map reduce application. Or let's say that you want to do ranking of pages. When a user is doing a search, how to present present the search results May depend on the popularity of pages and that is what is often referred to as page ranking. So if page ranking has to be done for all the documents that is another application that can be [UNKNOWN] up as a map-reduce application. The list goes on. All such examples that are mentioned share some common property. They're embarrassingly parallel, and they're common in giant-scale services. And all of them tend to work on big data sets. Therefore there is plenty of opportunity for taking advantage of the computation resources that are available in a data center. And all such applications need domain expertise in terms of what to do with the data. Which is expressible as a map function and a reduce function, and this is the only thing that is required of the domain expert to provide to the programming system, because that is domain expertise that lives with the app developer. So here is another example of how an application may be structured as a map_reduce application. And in this case, I'm showing you a page ranking application that is, I'm interested in knowing what is the popularity of different URLs that occur in a document corpus. So the keyspace that is input to this application is a set of URLs. And the key value pair that is coming into each of the mapper is a source URL and the contents of that web page that corresponds to this particular URL. So what this mapper is doing is, in the given page defined by this URL the contents of which is the input to this mapper it is looking for different targets. Maybe it is looking for a particular URL target one, another URL target two, and so on, target N, and that's what each of these mappers are doing. So the keyspace that is output from the mapper is unique target names. So the keyspace that is output from the mapper is unique, target URLs and, the value is the URL in which it was actually phoned. So the corpus of input URLs it is taking, and saying well, in this particular URL I phoned this target. If it did. It is emitting that this target was found in a particular URL. And this reducer is going to get all the hits for a particular target that was found in the input corpus of URLs. So all the mappers they're going to send their results to this reducer if they found in the input - Surl the target, target 1, if they did, they are going to send their results to this reducer. Similarly if they found target n, each of these mappers are going to send this reducer that in the input URL they found this target n and the job of the reducer [INAUDIBLE] Is once again aggregation, and you have as many reducers as the number of unique targets you're trying to identify in the input dataset. Very similar to the previous application that we went over. So the output of the reducer is going to be the specific target that this guy has been asked to accumulate. And, a source list, meaning all the source pages in which this particular target was found. So each of these reduces is going to find the number of times a particular URL is found in the input corpus of webpages that came into the system as a whole. For instance, if I wanted to find out how many times my webpage appears in the universal web pages all over the world, we can take the entire corpus of web page available in the universals input, and the map that's we're going to look for occurence of my web page in each one of those input web pages. And if they find that, they're going to send it to this reducer and if target one corresponds to my web page then this reducer is going to say, okay, show his webpage, was found in this list of source webpages all over the Universe. That in a sense gives a rank for. That particular web page that we're looking at. So we're able to rank the target web pages 1 through n based on the number of source web pages that contain that particular target. And that's what page ranking is all about. So I'm giving you yet another example of how This map reduce functionality can be applied for an application such as page ranking. All the heavy lifting that needs to be done, in terms of instantiating the number of mappers, instantiating the number of reducers, The data movement between the mappers and the reducers, all of those chores, are taken care of by the programming environment. All that the domain expert had to do was to write the map and reduce function that is unique to particular specifics of his application. The rest of the heavy lifting is all done by the programming framework. Now let's look at all the heavy lifting that needs to be done by runtime in order to facilitate this map-produced style of programming. The app developer writes his map function and reduce function, and instantiates the programming library by calling map produce. And the programming library splits the input files that is provided by the user, that is a key value space provided by the user, into M splits. The number of splits of the input key value pairs,namely M, can be automatic by the programming system. Or M can also be specified by the user. In any case, once M is specified or automatically determined by the programming framework, it splits the input key value space into M splits. The next thing that happens in the programming environment is to spawn the master for this particular map reduced computation, and all the worker threads That are going to carry out the work involved in map functions and reduce functions. The master is special because the master is the one that is sort of overseeing the whole operation and keeping tab on which workers are doing what functions, when are they done,when to start new work, when to say that the computation is done. All of those chores are being orchestrated by this master. The next thing that happens is that the master is going to assign some number of worker threads as the mapper worker threads and the number of mapper working threads may correspond to the number of splits that it has done in the beginning. So there are M splits, then M worker threads are going to be assigned to mapping function. So each worker thread is going to take one portion of the input file split that have been done, and apply the map function on that particular input split. The next thing that happens is that the master assigns reduce tasks to some number of worker threads and the number of reducers to be assigned to the workers, R, is something that is decided by the application. Recall in the example that I gave you about looking for specific names. In an input corpus, the number of unique names is something that the app developer is specifying. That's where the number R comes from. That's the number of splits that the user is specifying, and that parameter is going to be used by the master to assign some number of workers as reducers. The next thing that the master does. Is to plumb the mapper to the reducers. Because now when the mappers produce that output. That output has to be sent over to the consumers of the intermediate results of the mapper, namely the reducers. And setting up this communication path between the producers of data The mappers and the consumers of data that it uses is the plumbing that the master does as the next thing. Now it's time to get to work. The map phase, what it is going to do is, it is going to read its respective split. So each of these workers is assigned to the mapping function. So each of the worker is working on a particular split. And what they're going to do is read from the local disc, the split that they've been assigned, parse the input, and then call the user defined map function. The user defined map function is the one that is doing the core of the work that needs to be done on the input data set to produce the intermediate output. The rest of it are things that needs to be done in order to facilitate the work to be carried out by the domain expert in the map function. And the intermediate key value pairs that we produced by the mapper will be buffered in memory, so each one of these workers Is doing a portion of processing the input key value place and producing the respective outputs. And periodically the intermediate results are going to be written to files, which are on the local disks of the worker or the respective workers. For this guy, on its local disk, its going to write intermediate files corresponding to the output of the map function. Similarly this worker is going to write to its local disk, the intermediate files and so on. And because the application developer has specified that there are R splits in the reducers. Each worker, meaning each map function that is associated with that worker, is going to produce R intermediate file. One for each of the R workers that are going to be doing the reduce operation. And all these intermediate files are stored on the local disk associated with each of these computational nodes carrying out the worker function corresponding to the map phase. And when they are done with the map operation for the split that they are handling, the worker will inform the master that i'm done. And what the master's waiting on is waiting on all of these mappers to get done before letting the workers to get going on the input data set. So in this sense, the master is like the conductor of an orchestra. He is started this mapping function waiting for all of these guys to get done, and when they indicate that they have done the work by notifying the master. And all of the M mappers that have been assinged to these workers have completed that work, then the masters say, okay now it is time to move on to the reduced phase. In the reduce phase, there is more work to be done in pulling the data that is needed for each one of these reducers. Because, we know that the mappers have been executing on different nodes of the computational cluster, and they produce their intermediate results as files on their local disk. And this worker that is carrying out a particular split of the reduce operation has to reach out and pull the data from all of the m mappers that have stored their intermediate results on their respective local disks. So there is remote read that is involved. As the first thing in the reduce phase is to pull the data. This is part of what I mean by the plumbing that the runtime system provides is to recognize that, for this reduce operation to work, it needs the mapping results from all the m nodes that carried out the map function. And so it is going to do RPC in order to get all this data from all the local disks of the nodes on which the map was executed. And once it has all the data, it can sort it, and then call the reduce function. The reduce function is the one that has been written by the domain expert. And this is the point at which the domain expertise comes in, in saying, well, I've got the data now, let me do the processing that I want to do for the reduce operation. The sorting that is being done as part of the programming framework may be to sort the input that is coming in from all of these different mappers, so that all the same keys are together in the input data set that is going to be given to the reduce function. And once such sorting has been done, the programming framework will call the user-supplied reduce function for each key with the set of intermediate values so that the reduce function can do its thing, which is domain specific. Each reduce function will then write to the final output file specific to the partition that it is dealing with. So, if you think about the original example that we started with, if, let's say, this guy is accumulating all the instances of the name Kishore, then it will write the output file that says oh, I've found so many instances of the name Kishore in the input corpus of data. And similarly, this guy may be doing it for another name like Drew, or Arun, and so on. And once each worker has completed its work by writing its final output file for this partition that it is responsible for, then it informs the master, that, yes, I'm done with the work that was assigned to me. The user program can be woken up when all the reducers have indicated to the master that they have done the work, and at that point the map reduce computation that was initiated by the user program is complete. We said that there could be m splits of the input dataset, meaning the input key-value pairs, and there could be R splits of the output that has to be generated by the application as a whole. Now, the computational resources that are available in the data center, N, may be less than the sum m plus R. So there may not be a unique machine to assign for each one of the m splits that have been made. It is the responsibility of the master to manage the machines and assign the machines that are available to handing the m input data sets as well as the R reduce splits that need to be generated. So this is part of the heavy lifting that has to be done by the runtime. So, for instance, let's say that I have only 100 worker nodes available as mappers. And I have an input split of 1000, then what I'm going to do is, I'm going to assign one split to this worker. And when the guy says I'm done with it, then I'd say, oh, you're done? Okay, take the next split and work on it. Take the next split and work on it. So that's how we're going to manage the available resources for carrying out the work that needs to be done. So remember that this is all done as part of the heavy lifting by the runtime, the user doesn't have to worry about it. All that the user did was write the map function and write the reduce function, and the rest of it is magic so far as the map reduce framework is concerned. And similarly, the number of R splits specified by the user may be more than the number of workers that are available to carry that out. And in that case, again, the master is going to assign a particular split to this worker so that he can compute and generate the output file corresponding to that split. Once he's done with that and notifies the master, then the master will say, okay, now that you're done with that, work on the next split. And it'll do all the work that is associated with plumbing, meaning, bringing the data for the split that a particular worker is working on right now, sorting it to put all the corresponding keys together, and then finally calling the reduce function that has been written by the user. That's the kind of work that goes on under the covers, in the programming framework of map reduce. Lots of work to be done by the runtime system, to manage and map_reduce computation. The master data structures include the location of files created by the completed mappers. Remember that each mapper is working on some node of the computational cluster. Producing its output as files on its local disk. So the master has to have knowledge of where those files reside. Created by those mappers, and the namespace of the files that have been created by the mappers. And it also has to keep a score board, of the mappers and reducers that are currently assigned to work on different splits. Because the number of machines that may be available, may be less than the total number of machines that will be needed. If I wanted to do all of the input m splits in parallel and all of the R output splits in parallel. And therefore, the scoreboard is saying, at any point in time, who are all the workers carrying all the mapper functions? Who are all the workers carrying all the reducer functions? When are they done? And when they are done. How should I reassign that worker to a new task. So these are the kind of things that the master data structures facilitate the master to do. Then the big thing is fault tolerance. For a variety of reasons, the master may find, that an instance, of a map function that it started, is not responding in a timely manner. Maybe that node is down for some reason, maybe the link is down for some reason, or maybe that processor is taking a little more time than what the master expects. It can happen very easily, because in a. Data center environment where you have thousands of machines, there could be some machines that are slower than other machines, there could be generational differences between the machines that populate the data center, even though they may be homogenous in terms of the capabilities. Programming alignments and even the machine architecture. They may have speeds that are different because they correspond to different generations of processes. In any event, what might happen is that a master may notice that there is no timely response from a particular instance. Let's say of a mapper. In that case, it might say well I'm going to go ahead assume that that mapper is dead. I'm going to restart that mapping function on a different node of the cluster. Now, when that happens it is possible that the original mapper is actually dead, or it could be that it was just slow. In that case, you could get completion message from more than one mapper for the same split. So when you get multiple completion messages from redundant stragglers, the master has to be able to filter the mordancy. This is something that is redundant work. I don't have to care about that. So that's part of fault tolerance that the master has to worry about. Locality management is another important thing. In the memory hierarchy, making sure that the working set of computations fit in the closest level of the memory hierarchy of a process is very important, and this is another thing that has to be managed very carefully so that the computation can make good forward progress. In completing the map produce function. There is an inherent assumption in the fault tolerance model of the map produce framework. And that is item potency of the mapper. That is, even though the same input data split is being worked on by multiple mappers, It doesn't affect the semantics of the computation as a whole, and that is the reason why the master can simply ignore the redundant stragglers' message if in fact it was a slow computation engine that was working on a particular map function. And if it finishes later than when it was supposed to, and the master has already started another redundant computation to take care of that particular split, ignoring that redundant straggler message. Is okay because of the item potency assumption about the mapping operation. In a similar manner, the master may assign a new node to carry out reduced function corresponding to a particular split if there's no timely response from one of the reduced workers. And here again, the output file generation that is associated with a particular. A reducer split depends on the atomicity of the rename system call that is used to finally say that oh, this is the old profile generated, and the master says okay, I'm going to commit this as the real output file of the computation. Each reducer is writing a local file, and finally when the master gets the notification from the reducer, that it has completed its work, at that point, master renames that local file as the final output file, and this is where can get rid of redundant stranglers who come along later and say I produce the same output file for the same split. Master will say, I already got it, I don't need this, and it can ignore the completion message from that redundant straggler. The other thing that the programming environment has to worry about is locality management, and this is accomplished using the Google file system that provides a way by which, efficiently, data can be. Migrated from the mappers to the reducers. Task granularity is another important issue. As I mentioned, the number of nodes available in the computation cluster may be less than the sum M plus R, where M is the number of input splits, and R is the number of. Reducer splits. And it is the responsibility of the programming framework to come up with the right task granularity so that there can be good load balance of the computational resources. And the programming framework also offers several refinements to the basic model. The way the partitioning of the input data space is done, is by using a hash function that is built into the programming environment. But the user can override that partitioning function with his or her own partitioning function if they think that that'll result in a better way of organizing the data. And in the map reduced framework, there is no interaction between the mappers, but if there's some ordering guarantees needed in terms of ordering the keys and so on, that is something that the user may have to take care of. The other thing that the user may also do is combining. A partial merging to increase the granularity of the tasks that execute a mapping function. Remember that when we looked at the simple example of looking for specific names and input corpus of data, I mentioned that. The mapper can be as simple as emitting a one for a value every time it encounters the name Kishore for instance in an input file. Or, it could take an input file and count all the occurrences of Kishore in that input file and finally emit the total value as the output of the mapper. That kind of combining function is something that. A user may choose to incorporate in writing his mapping and reduce functions. It is very important to recognize that in order for the fault tolerance model of the programming environment to work correctly, map and reduce functions have to be item potent. That's a fundamental assumption. That the fault tolerant monitoring of the programming framework is based on. And there are also other bells and whistles in the programming framework for getting status information and logs and so on and so forth, but the basic idea I want to leave you with is the fact that the programming framework is a very simple one. It has two operations, map and reduce and using those two operations. You can specify a complex application that you want to call up, to run on a data center, using the computation resources that's available in the data center. The power of Map Produce is it's simplicity. The domain expert has to write just the Map and reduce functions for his application. All the heavy lifting is completely managed by the run time system under the covers. The Internet and the World Wide Web and intimately tied to everyday lives. It has put information that we seek at our finger tips. Remedies for the common cold, dinner recipes, sports updates, developments in the Middle East, you name it. You will find it on the internet. We know that the creation of such content happens from the basement of individuals like you and me, as well as businesses such as CNN, BBC, and NBC who's work it is to create such content. How is the content in the internet stored and distributed? In the first two lessons of this module, we looked at the server end of the giant scale services. How's a computational resources organized in the data center? How is the data organized internally in the computational. Nodes of the cluster. And what is the programming model for dealing with the big data associated with giant scale services and for exploiting all the computational resources that are available in the server forms and data centers. In this lesson, we'll look at content distribution networks. That is How is information organized, located, and distributed when we're looking for them? Once again, the issue is dealing with the scale of the problem. The content is generated worldwide and users are trying to access the content worldwide as well. First a trivia quiz for you. Who started content distribution networks, and why? And I'm giving you a bunch of choices for who may have started the idea of content distribution networks including big players such as IBM, Napster, Facebook, Netflix, Akamai, Apple, Microsoft, Google. And, the why part of it, is it for content distribution of world news, e-commerce information, online education, like what we're doing right now, music sharing, photo and video sharing? I'm sure the many trivia buffs around you will probably know that Napster was started, by teenagers. Who wanted to have a way of sharing music on the internet. That's how the CDN got started in the first place. Lets understand what content distribution networks are. I'm planning a holiday trip to India fairly soon and let's say on the holiday trip I record some nice videos of some of the places that I went sightseeing. And let's say I call the content that I generated thusly. [INAUDIBLE] India trip. To keep it simple, let me call the node ID of my computer as 80. And I want to publish this video that I generated of my interesting visits to some of the sights and sounds of (no period) India on the Internet, so that anyone on the Internet can find it and download it. Now, how do I name the content of my video? Well, textual names may not be very meaningful, because there could be name collisions. So what I'm going to do is, I'm going to create a content hash of the video, and let's say my content. Hash has enough bits to ensure uniqueness of the hash string that I generated, I'll call that the key. Now I have key value pair the key is a bit string that is a hash of the content of what I want to store and make available to everybody, and the value is my node ID where the content is stored. Lets say that the content turned out to be 149, and my node ID is of course 80. So the key value pair that I've generated for my interesting video is 149, 80. 149 is the key, and 80 is the value. Which essentially is the node ID of my computer where I have the content. In other words the key 149, Is a unique name that I've generated for the content which I want to distribute to others that may be interested in looking at this particular content. Now if you get this key value pair 149, 80, you know that 149 is the key that uniquely names my video it shows India trip and 80 is the value which says where you can get this particular video from. So you can then come to my node and get it from me. Now the question is. Where to store this key value pair, so that anybody can discover this key value pair. If they are looking for Kishore's India trip, maybe I published the fact that Kishore's India trip unique name is 149 somewhere. And so now, if, anybody wants to find a way of locating the server from which they can download the content they need to get this key value pair. So, when I created this particular, key value pair, I have to find a way to place it in the internet. So that anybody can get these key value pair, and from that, know the node from which they can download the content. Now we cannot put this on a central name cell because it does not scale. Because user generated content is proliferating on the internet. So using a central server to store. All the key-value pairs is just not scalable. We need a distributed solution. And this is where the idea of DHT, or distributed hash table, comes into play. The idea is quite simple. Anyone who generates a key value pair has to find a location. Where they can store it so that intuitively, anybody that is looking for that particular key will be able to discover it. So if I want to store a key value there, what I'm going to do is find a node who's ID is exactly the same as the key itself. Or if not, close enough to the key that I want to store. So in this case, the key that I generated, which is a content hash of my India video, is 149. And looking on the internet, I find that there is a node whose node ID is 150. Close enough to 149, and therefore, I'm going to store this key value pair 149,80. In this particular node 150. Now, if you're looking for my India trip video, you know that the unique signature associated with that is 149, and you will know because of the structure of the DHT that the place to look for the signature 149, or the key 149, Is a node whose address is also equal to this key value or close enough. 150 is the one that is close enough so you'll go to 150 and from 150 you will get this key value pair, and once you get this key value pair, you will know that all the content is stored in node 80 and then you come to me and get the content from me. This is how content distribution networks exploit the distributed hash table technology to store content on the internet. So that they can be discovered and disseminated to the users. Let's dig into some of the details of the DHT technology. The first name-space that the DHT technology has to deal with is the key-space name-space. So the way name-space is managed is, if you have content to disseminate. You generate, a unique key, for the content, by using some algorithm like SHA1 which generates a 160-bit key. The number of bits in the key or the signature is big enough that they will not recollision even if different content are using the same algorithm to generate this key. So that is how you create a unique signature for a particular content, using SHA-1. The second name-space we have to deal with is a node-space. So here, what we're doing is, we are creating an SHA-1 hash of IP addresses of nodes that want to share content. So, let's say that me and my buddies. Form a social network and all our IP addresses, we going to use this algorithm to encode them into this 160-bit node id. So now we've got two name-spaces, one is the key-space, name-space and the node-space, name-space, and both of them are derived using the same algorithmic technique, let's say. So in this case we've created a key for the content so that there's a unique signature for a particular content, similarly, we've created a node id from the IP address, which is again a unique signature for a particular IP address. The objective is, if I have a key, I want to store. That key in a node id N, such that the key is very close to the node id N. Ideally, if you're lucky, the key is exactly equal to N, but, you know, it's not possible to guarantee that this hash and this hash will result in exactly the same value. So long as it is close enough, like in the previous example I showed you. That if I generated a hash 149, I stored it in that node 150, which is close enough to the hash that I created. So the API for manipulating this distibuted hash table data structure would be putkey and getkey, so putkey would take two arguments. The key and the value. Value can be anything that you want to associate with that. In the previous example, I've said the value may be the IP address of the content that is associated with the particular key. And getkey takes one argument, namely the key, and returns the value that is associated with that key-value pair. CDN is an example of what is called an overlay network. Let me explain what an overlay network is. In the previous example, I said my node id is 80. And the content corresponding to the key 149 is stored at my node id 80. And this is something the chip discovered, and once you've discovered it, you want to come to me to get the content from me. But how will you do that? What does 80 mean from the point of view of the internet? As you know, the physical infrastructure of the internet works with IP addresses. But what you have is 80. It's not an IP address. And the operating system only understands IP addresses, to route packets on the Internet from source to destination. So we need, at the user level, because we are doing this sharing of content at the user level, a way of mapping such virtual addresses, 80, 60, and so on, to IP addresses. So we, as friends, who have decided that well, we want to form a social network to share information content, and so forth, we've exchanged such mapping information and constructed a routing table at the user level. And this is what is called an overlay network, a virtual network on top of the physical network. So, for example, if you, using that SHA1 hash, found that your IP address maps to node ID 60, you'll say, okay, sure, my node ID is 60. And my IP address is such and so that is how I'm going to construct a user-level routing table. All I know is your node ID, and that node ID maps to some IP address. And I can use this correspondence between the node ID and the IP address, to have a way of sending information to you. And maybe, a friend of yours has exchanged routing information with you, and he has told you that his node ID's 80. And he has also given you how to reach him. Given that node ID. And you've shared that information with me as well saying that you know, I have buddies, and these are all the note IDs of my buddies. So that's how I construct this routing table, which is really a table that consists node IDs of my friends and friends of friends, friends of friends of friends, and so on. That's how I construct this user level routing table. Now if i wanted to send a message to my buddy, let's say B, and his node ID is 60. Because B has given me his IP address, when I send a message to node ID, I can covert that to the IP address, give it to the physical network, and it goes to the physical network gets delivered to B. What if I want to send a message to some other node C. I know the node id of c because my buddy b exchanged that with me. But I have no idea what the IP address of c is. But on the other hand, I do know that b has a way of getting to c. And so in my routing table, what I'm going to say is that given a name of a particular node, I know the virtualized node ID associated with that node, 64B, 84C, and I also know what the next hop is in the user space, in the virtual space of getting to that particular destination. In the case of B. I know I can directly send it to 60, because I have an IP address for B. On the other hand, if I want to send it to C, I have the node ID of C, but I have no idea what that maps to in terms of IP address. I know that my buddy B has a way of getting it to C, and that's what I'm going to use. I'm going to say, if I want to send something to C, I simply hand it over to my buddy who's at node id 60. So the routing table says, given a node id, who's the next hop I should give it to so that it'll eventually get there? Now the physical network, of course, is much more elaborate than this user level, overlay network. Now, for instance, let's say I want to send a message to C. His node ID is 80, but I have no idea how to send it to him except to know that if I give it to my buddy B, who's at node ID 60, he'll know how to get it over to, to C. So, I'm going to send it to B, and when it comes to B, B knows. From his routing table, the IP address for C, and therefore he will send it to C and it'll eventually reach C. So, at the user level, you can see that so far as I'm concerned, if my node ID is 50, and my node name is A, when I wanted to send a message to C, what I did, was to send it to B. And it took two hops at the user level. It took two hops to go from A to C, went to B my buddy and B then sent it over to C. So this is a user level traversal of the message. But in reality, under the covers, what is going on, is when I send this message from A to B, it is going through the physical network and reaching B. And when I send to message to A to C. Even though it is two hops over here, internally in terms of the number of network hops that may have to be incurred in the physical network for the message to reach eventually the destination C, it might take many more hops. That's what an overlay network is. A virtual network on top of the physical network. And this particular overlay is a content distribution network, because it allows content to be shared and distributed among a set of users, who have exchanged information with one another so that they can discover one another, if not directly, indirectly through friends of friends. Overlay Networks is a general principle and at the OS level we already have an overlay network. There is the IP Network which is really an overlay on top of the local area network. You may have a particular IP address for your laptop. That IP address actually translates to a Mac address. Because the Mac address is the real address that is used by the physical network in the local area network to communicate with other buddies on the same local area network. So at the OS level, when you send a message using TCP/IP, that message, it will be delivered to your buddy who happens to be on the same local area network. It is actually getting converted to a Mac address so that it can traverse the local area network and get to the desired destination. So that it can traverse the local area network and get to the desired destination. Similarily, CDN, a content distribution network is an overlay on top of TCP/IP. So in particular, In the case of CDN, we have this node ID. There is some manufactured ID at the application level, but that maps to an IP address, and that is how at the application level, when I say I want to send a message to Node number 60, at the application level, I can convert that Node ID 60 to an IP address. To which I can send it and eventually the message will get delivered. DHT or distributed hash table, is an implementation vehicle for a content distribution network to populate the routing table at the user level. As I mentioned earlier, for placement you need a put operation. And the put operation is going to take 2 parameters, a key, and a value, and the key is some content hash that uniquely identifies something that the user community may be interested in finding. And value is the node ID where the content is stored. And retrieval of a key value is done using a get operation. And when you do a get and give the key, what you're expecting to get back is the value that is associated with this key value pair. So, what you get back is a value that is associated with this key, which was placed somewhere using the put operation. So using the put and get operation, let's see how to construct the routing table as new content gets generated. That is, you want to do placement of key value pairs using put operations, and retrieval of a value associated with a key, with this construction. The traditional approach, which I'll call the greedy approach, in constructing distributed hash table is when you want to place a key value, you pick a node, n, where n is very close to key. And now, if you want to retrieve a given key, K, the algorithm you know is going to be, you want to go to a node N, which is closest to this key K, because that is the algorithm that is being used for placing, so for retrieval you just do the reverse. When you want a key K, you go to a node which is closest to key K. That's how these routing tables get populated at different nodes in the distributed system at the user level. So the routing table at A says that these are the known peers to me whose node IDs I know and I know their mapping of the node ID to the IP addresses. Now the node space may be much bigger than the number entries I have in my routing table. So what do I do if I want to communicate with a node whose node ID I know, but I don't have a mapping for that node ID with respect to the IP address. Basically the routing table at every node is just saying these are the nodes that I know how to communicate with directly. That is all the nodes that are reachable from node A. Because, at the user level, I have a mapping between the virtual node ID that is used in the DHT and the IP address that corresponds to it. And remember that IP address is the only thing that the operating system is going to understand. And therefore I know how to communicate with node 60, because I can give the IP address that corresponds to node 60 when I want to send a message to node 60. Similarly for 79. Over here, node B knows how to communicate with 60, knows how to communicate with 109. And if these are the only entries that are in the routing tables of A and B, these are the ones to which node A knows how to communicate directly. What if I want to go to some other node that is not in my routing my table yet? For example let's say that I am trying to retrieve a key, 58 or 59. I know that 58 or 59, in terms of the DST construction, it's most likely stored in some node whose ID is very close to this key. Now in my table I have a node ID 60, close enough to the key that I'm looking for. So what I'm going to hope is that if I am looking for this particular key, 58 or 59, good chance that the key value pair that corresponds to 58 or 59 is stored in this node ID 60. So that's the one that I'm going to communicate with. It's possible that 58 is actually stored in a node ID 58, in which case my hope is the desired destination that I want to reach to is known to this peer, 60, who is close to the node number 58. So in other words, if I want to communicate with node number 58, my best bet is to communicate with node number 60 with the hope that 60 may actually know how to communicate with node number 58, because ultimately I'm hoping that that's where this particular key value pair may be actually stored. On the other hand, if the key that I am looking for is 80 or 81, then I'll say well, chances are this key is toward a node ID 79 for whom I have a mapping. Or if I go to him, he might know how to get to node number 80 which may be actually storing this key 80, which may be actually storing this key 80. So in other words, in the greedy approach what we're going to do is in placing a key value we're going to place the key value pair at a node N where N is equal to K, ideally, or close to K. Similarly, if I want to retrieve a key K, I'm going to go to a node N, where N is either equal to K if I know how to get to it from my routing table. Or get to a node that is close enough to the desired N. In this case, the desired N is 58 or 59, but the one that I can get to is 60, and my hope is that when I get to 60 he will know how to get me to node number 58 or 59. Or even better, he may be the one that is storing this particular key value pair that corresponds to the key that I'm looking for. So in other words, in the greedy approach we are trying to get to our desired destination as quickly as possible with the minimum number of hops to get to the desired destination. And when I say the number of hops, it is at the level of the virtual overlay network, not in terms of the physical network. Because at the user level have no idea how many hops my message may actually take going from source to destination. All that we are saying is at the user level we are trying to minimize the number of hops to get to the desired destination. The greedy approach leads to what is called a meta-server overload. Let's understand how that happens. Let's say that there's a whole bunch of users that are generating content, and one guy generates a content with a key of 149, another guy Generates a content with a key 148, 152, 153 and so on and now what they want to do is they want to place this key value pair this guy's node ID is 80. This guy's node ID is something else. Something else something else and all these guys want to keep the key value pair and what will they do in terms of this greedy algorithm well they'll try to find a node whose id is closest to the key that they want to place. Well it turns out that 150 is the closest. ID corresponding to all of these keys that we're talking about here, so all of these puts will result in going to one node, namely the node ID 150, in terms of the put operation. And, first of all there's going to be congestion here if a lot of. Content hashed to a key that are so closely together they all end up in the same node ID 150. Remember that the actual content is with the putter of this key value pair, namely node 80. And, similarly, the content Is with this putter, content with this putter. What this guy is storing is the metadata that allows everybody to discover the content provider. So not only is this node going to find a lot of traffic if lots of keys map to its node ID, also the nodes that are adjacent to this node In the overlay network, they are going to be affected also. Everybody is trying to reach this guy so you can see that as we go towards this, there's sort of a presaturation effect that's going on. The tree is rooted at this destination node, and the nodes that are in close proximity, the intermediate nodes in the overlay network space They get congested, and so on. And that is what is sometimes referred to as a desaturation problem. Now this is for the putters. Further, if my content whose key value is toward here, the video that I generated from my India trip. If that gets hot, and everybody wants to Find out how to achieve that particular video 149. They're all going to make get calls. And these get calls again end up with this same metadata server, because that's the guy that is storing my key value pair 149, 80. And so all these gets saying that I want to get 149, I want to get 149, I want to get 149. All of them. Go to the same node ID 150. Again there is congestion at the meta data so it can happen either because some content is so popular that everybody that wants to discover the content provider they have only the key and they have to go to the metadata server to find out the content provider So that results in this congestion, which is the metadata server overload problem. And the combination of these puts and gets result in what I called the tree saturation problem, the tree being rooted at the congested node and affecting all the nearby nodes In the overlay network because they are the gateway to get this node that contains this particular key value pair. But the problem doesn't stop there. Metadata is overloaded, but my video has gotten popular. And so all of these guys want to download the content from me, which means that everybody is coming to me, and I have an itsy bitsy server in my basement. And that is going to get inundated with all of these download requests from all of these different clients that want to get this video from me. So, there is a metadata overload and there is a content overload that is coming about because of the fact that something became really hot and everybody wants to get that content. And this is what is called the origin server overload. In general, there are two solutions to the origin server overload problem. The first solution is to have a web proxy. And you might be familiar with this already because every organization tends to have a web proxy so that they can limit the amount of requests that have to go out of an organization. And that way if users of an organization are trying to access popular content from the Internet, the local web proxy that's available in the organization can directly serve that content to the requesters. But it turns out that the web proxy solution is not good enough for what is called a Slashdot effect. And that is, if there is a breaking news and everyone wants to get the content, they will have to get the fresh content. A proxy's not good enough for that. Let's say that there's a Super Bowl going on and everybody wants to watch the Super Bowl, or at least get the latest updates from that. In that case, the web proxy's not going to be good, because the content in the web proxy is cached content. You want the live content. And the live content is only available at the origin server, not at the proxies, and therefore the origin server will get overloaded when we have such dynamic content. Content that is resulting from either breaking news, or live programming, and so on. And this is where the content distribution networks come into play. The idea is that the content is automatically mirrored from the origin server at selected geographical locations. And those locations are constantly getting updated from the origin server. So that going to any of the mirrored content is the same as going to the origin server. And then what happens is that, in the content distribution network, depending on the geographical area from which a particular user request originates, the user request is dynamically re-routed to the geo-local mirror of the content so that the origin server need not be overloaded. You may be familiar with companies like Akamai which have come about for the purposes of providing content distribution network for popular content providers, like CNN or broadcast television channels like CBS, NBC, and so on. And what these organizations, that is the content providers, do is get into an agreement with a content distribution network provider like Akamai, so that they don't have to worry about origin server overload because the content distribution network provider like Akamai have the solution for taking the content of the origin and automatically mirroring it in geo-local sites so that the origin server never gets overloaded. So, this is good if you are a big organization like CNN, or CBS. You can afford to pay the content distribution network provider to do this automatic mirroring, but how can we democratize content distribution. How am I, in my basement, generating a piece of video and I want to share it with the world, how am I going to make that available to everybody? If it gets hot, I don't want to get overloaded. I want the content distribution network to work without having to pay big bucks to a content distribution network provider like Akamai. And this is where the Coral System comes in. This is a paper that I've assigned you to read which has a technique for democratizing content distribution. And the details of the Coral System, and what we're going to look at next. So the Coral System addresses two issues. The first issue is the fact that if I generate some content, and I want to store the key value pair associated with the content in a DHT, I have to have a scalable way of doing it without saturating any particular node which can serve as a metadata server. So we want to avoid the tree-saturation effect and the second thing we want to do is also avoid the origin server overload. So both of those things are being addressed in providing a democratic solution for content distribution for the average Joe that may want to generate some content, and want to share it with the rest of the world. We will look at the details of the Coral System in the rest of this lecture. We mentioned that the greedy approach of constructing a DHT leads to tree saturation. And the congestion happens at the node, which happens to map to a lot of clustered keys. The coral approach is very simple, don't be greedy. What does that mean well what it mean the greedy approach,when we have a key K, we try to store it in the lower N who's note i.d is equal to K the coral approach use this as a hint not and absolute. But you still have to have a method to the madness, if you're going to store it in some place different from N. Then, those who are trying to discover it, have to have a method to the madness, of where you stored your key. We'll see how that is done. So the top level bit I want you to take away is that in the Coral DHT The get and put operations are satisfied by nodes that are different from the key K. The node IDN may not be close to key, and that's why the DHT that Coral implements is called a sloppy DHT, and we'll talk about details of that in a minute. The rationale of course, is you want to avoid tree saturation, that comes about when lot of keys map to a particular node ID. And also in the process, what we want to do is, spread the metadata overload so that no single node in this democratic process of helping one another is saturated or overloaded by being a good citizen. How does Coral do it? It has a novel key-based routing algorithm, which we'll describe, but the basis for that key-based routing algorithm is to compute the distance between, the source of a get or a put operation and the destination for that get or put operation. The destination by default is going to be the node whose ID is equal to the key that you're looking for. So the distance between the source and the destination is X or distance, meaning [UNKNOWN] or the bit pattern Of the node ID for the source and the bit pattern for the node ID for the destination that X or of the two qualities give you the distance between the two nodes in the overlay network space. And the reason why you want to do an XOR is because an XOR operation as opposed to, say, doing a subtraction, is going to be much faster. We do an XOR to find out the distance between the source and the destination that need to communicate. So, for instance, if the source at, this is fourteen, and the destination address is four. Remember that these are null IDs in the null ID space which at the level of the user. And if you do an X sort of that you get 10, so 10 is the distance between the source 14 and,and the destination for. The bigger the XOR value, the larger the distance between the source and the destination in the application namespace. And since we are dealing with fairly big numbers here, the node ID could be 160 bit quantity. And that is the reason we want to use some simple operation that will get us the distance. X R is a very quick operation to implement to get the distance between source and destination. Let's discuss Key Based Routing. First, we'll talk about the greedy approach. What I'm showing you here is the state of the routing table at the source node, which is fourteen. And the key that I'm looking for, let's say is four, and therefore in the greedy approach, I would assume that The key K, that is K being four, will be stored at a destination whose node ID is four. And what this table is showing, is the routing table at the source, and in particular, the entry that you have here, where you have a valid entry, those are the nodes that are reachable from. Node fourteen. So from the source node fourteen I know how to reach node number thirteen, I know how to reach node number three, I know how to reach node number two, zero and five. I don't know yet how to reach node number four because that is not in my routing table at this point of time. The intuition in the greedy approach is if I'm looking for a particular key, K, in this case, K is 4, then I know that the node that is likely to have that key, K, in this case 4, is going to be the node whose ID is also 4 and so this is my desired destination, but what I'm going to do in the greedy approach is get as close to the desired destination in the node_id namespace. If I look at the state of my routing table, I don't know how to get to 4, which is my desired destination, but I know how to get to 5 which is close to The desired destination in the name space of the node ID's, so what I am going to do, is ask this guy, because I know how to reach him, and he is close to my desired destination, do you have a way of getting to destination number 4? And my hope is that he may know the route to get me to the desired destination, and I'll be done. And getting the key value pair that I'm looking for. That's the idea in greedy routing. Take the minimum number of hops to get to the desired destination by using information that is available in my user level routing table, that has information about which are the nodes that are reachable from me directly. So the objective in the greedy approach to routing is reaching the destination with the fewest number of hops. That's the key. So in other words I am optimizing my own look up. This "me first" approach we know can lead to congestion and particularly the tree saturation that I mentioned earlier. So the Coral key-based routing takes a different approach. In the Coral key-based routing, rather than being greedy, we are going to slowly progress towards our desired destination. In particular, I mentioned that the distance between two nodes in the node namespace is given by the XOR of the node IDs. So, in particular, if I look at the source 14 here, and the destination four here, we can compute the XOR distance between the source and the desired destination. And the routing table now is populated with numbers. And what these numbers show is the XOR distance from any particular node to the desired destination. So, the XOR distance from my source, which is 14, to the destination, which is four, is ten. The XOR distance from the node whose ID is 13 and the destination is four is nine. And similarly, the XOR distance from the node whose ID is five and the destination four, is one. So what I have now in my routing table is the XOR distance of the desired destination. That's what I have in this entry of this routing table that says what is the XOR distance for my desired destination right now from each of the guys that I know how to get to directly? I know how to get to 13. I know how to get to three, and two, and zero, and five. And what I am looking at now is if I get to 13, what is the distance of that guy from the desired destination? What is the distance of this guy from the desired destination and so on? That's what these table entries are showing right now. These are the nodes that are directly reachable from me. And what they showing is the XOR distance of each of the nodes that are directly reachable from me to the desired destination. So in Coral, what we are going to do is in each hop I'm going to go to some node that is half the distance to the destination in the node ID namespace. Recall in the greedy approach since I have a way of getting to node number five, I directly went to him with the hope that he'll get me to my desired destination. Not so in Coral key-based routing. What we're going to do is in each hop we're going to go to some node that is half the distance to the destination in the node ID namespace. Now the XOR distance between the source and the destination, 14 and 4, is ten. So in the first hop I'll go to the node that is half the distance to my desired destination, desired destination being ten. I want to go to a node, number five. Second hop, I want to go to a node that is half the distance of five. That is two distant from the desired destination. And third hop I want to go to a node that is one distant. That is, finally I am home. So that's the idea behind the Coral key-based routing. But of course, when I have a particular distance metric, like ten in this case, and I want to go half the distance in the first hop I may not have a direct way of reaching the guy who is half the distance to the desired node. So let's see how actually Coral key-based routing works, given this particular example. So we look at the evolution of the routing table of this source using this coral approach to key-based routing. Where in every step, we are going the half distance toward the desired destination. Now reducing the distance by exactly half, may not always be possible because I may not have a way to reach that particular node. So it is approximately we reducing the distance by half, so lets run through this example to illustrate, how the there key based routers works? So the XOR distance between the source and the destination is is 10, so target for my first half is going to be,to a node that is five distant from the desired destination. The node that is five distant from the desired destination is node number one because the XOR of one and four is five. So this is my target that is half the distance to my desired destination. But unfortunately I don't have a direct way. Of reaching one because I don't have that entry in my routing table and therefore, I'm going to go to a node that is approximately half the distance and I could have gone to either two or zero, close to the desired half the distance metric, but to make forward progress towards the desired destination, I'll go to this guy who is Four distant from the desired destination. So I make a iiiicall to this node because I can not reach him. I go to him, and I'm going to ask him. Hey I am looking for someone who is two distant from my desired destination which is four. And when I send my request over to him He responds to me and says, look I have information on three nodes that are not exactly too distant but close enough, four, five and seven are the nodes that I have. Who are close enough to the two distant neighbor that you're looking for. The two distant neighbor is this guy. But even this node that I'm reaching that is four distant from my desired destination, he doesn't know anyone that is two distant from the destination, and so he is going to respond to me and say. I don't have exactly what you're looking for, but I have information about nodes four, five, and seven that are close enough to who you're looking for. That's the information that I get back. So when i get the response from this node, what I'm getting back is information on how to reach nodes number four, five, and seven. So I'm now going to populate my routing table to evolve it to this new state where it shows that in addition to what I started with originally, I have now new information about how to reach node number four Node number five, which I already had, and node number seven, which is the new information I got. So you can see that from this, I evolved to this by adding two more reachable entries in my routing table, namely nodes four and seven. So what do I want to do next? Well, the next thing that I want to do is, I want to go to someone who's too distant from the desired destination because I reduced the distance by half, and now I want to reduce it even further by half. That means I want to get to a node that is too distant from the desired destination, that is this guy right here. But I don't have a way of reaching him. So I'm going to go to a node that is close enough to the desired target. The target is too distant from. The desired destination. I don't have an entry for him. I could either go to the guy that is three distant from the desired destinatnion, or that is one distant from the desired destination. Obiviously I want to go closer to the destination so I'm going to go to this guy. Normally if I went to the guy who is Too distant from the destination I would have asked him for. I'm looking for someone who is one distant from the desired destination, but in this case, it turns out that I've already reached the guy that is one distant, because I did not have an entry for the guy who is two distant from the desired destination. So this guy is going to get my request that says I'm looking for somebody who is one distant from the desired destination and he looks at his [UNKNOWN] table and says, these are the nodes that I know, satisfy the criteria that you're looking for, 4, ,5, and 7. That's what I get back. I know that one thing is happening here, and that is - After my first hop, I got back node IDs for four, five, and seven, which are the response for my requests saying I'm looking for somebody who's too distant from the desired destination. Because it gave approximately the nodes that are having the characteristic of being too distant from the desired destination, including. The node number four itself. So my table evolved at this step with a direct way of reaching node number four as well. But notice that I'm not being greedy here. Because in the second step, I want to reduce my distance only by half from the first step, and that is. I want to go to somebody who's too distant from the design destination. That's what I did. And I got back this response. And when I get back the response, I'm going to populate my table again. But in turns out, I did not get any new information at this step because I only have the information about four, five and seven. At the end of my first RPC call as well. Now I have a way of reading the [INAUDIBLE] and in my conscious I can feel good that I'm not being greedy I can now the target is zero and now I can make the call directly To the desired destination and get back the response that I want. So you can see that in this coral's key-based routing, using this idea of reducing my distance to my desired destination by half at every harp. What it results in is the fact that the latency that I am going to experience in order to reach my desired destination is increased, because I have to take more number of hops in order to get to the desired destination. Even if I have a direct way of reaching my desired destination, I am not doing that. I am actually reducing my distance by half every time. In order to get to my desired destination and the reason is I'm placing common good more important than my own latency. I'm avoiding the tree saturation that can occur at the destination if everybody is greedy. That comes at the cost of increased latency But that's okay. What we are shooting for is common good. Now let's discuss the primitives that are available in the coral for manipulating the floppy DHT in particular the put and get operation. The primitives are exactly the same, it's just that the semantics of the put and get are very different in terms of how it is actually implemented. So put takes on two parameters Key and value. Key is the content hash, and value is the node ID of the proxy with the content for the particular key. Essentially, this put is announcing the willingness of the proxy to serve the content whose signature is key. The put can be initiated by the origin server with the new content, or it could also be initiated by a node that just downloaded the content and wants to serve as a proxy to reduce the load of the origin server. In both cases they will want to do a put operation. And the result of doing the put operation is to store this key value, in some metadata server. That is some node that is going to serve as a name server that can, answer queries coming in saying, I am looking for this key. In that case, that metadata server can return the value, associated with that key. So what we need to do when we do a put operation, is to place this key value in an appropriate node. Now, what do we mean by an appropriate node? Ideally, what we want to do is, given a key, we want to store it in a node ID whose ID is N equal to key. That's the desired node where we want to place it, but we want to do this without really causing a metadata server overload. Now how do we determine if a particular node is overloaded? Well, what we're going to do is define two states. One state is called a full state, and what that is saying is a particular node, lets say node n, is already storing l values for a key. Remember what I said earlier, this key value pair can be placed by either the origin server that is creating the content. Or it could be placed by a proxy who is saying, I'm willing to serve as a proxy for the content. So there could be lots of nodes that have the content, and are willing to serve the content. All of them would have done put operations. So the node that matches exactly the key. That, is being put. They already have quite a few candidates, that are willing to serve as the content providers for that particular key. That's this parameter full that's saying, I'm willing to host up to L entries, for this key value pair. Anything more than that, I'm not going to do that. I'll get overloaded. So the full is a condition, you could say it's a special condition that's saying, I'm willing to entertain up to l content providers for a particular key k. The second way a particular node may get overloaded is, if it actually starts getting a large number of requests for a particular key. So, this is a time metric that says a node has a beta parameter, and the beta parameter is the number of requests per unit time. That a node is entertaining. So if it says that I'm already entertaining beta requests for this particular key and therefore, if you want me to store the same key, I'm going to say no, I cannot do it because I'll get overloaded. So this is a space metric that's saying, how many values I'm willing to store for a particular key. Loaded is stating how many requests per unit time I'm willing to entertain for a particular key. Those are the two metrics we're going to use in determining whether to place a key value pair at a particular destination node. So let' say I'm a proxy, and I want to put a key value pair in the coral DHT. I'm going to use the key-based routing algorithm to place the key at an appropriate node. What does that mean? Well, I'm going to take the key, and even if I have the node ID that is equal to this key, I'm not going to go to him directly. Remember that, the Coral, key based routing algorithm reducing the distance by half. So I'm going to go to a node that is half the distance to the desired destination. So the desired destination is n. I'll go to a node that is half the distance, say n over two, n over four, and so on, til I get to the desired destination. And as I'm going, making these calls, saying that, well I'm progressing towards this desired destination node, using that key base routing algorithm that uses the XR distance between the source of the destination halving the distance at every step. What I'm going to ask is, are you loaded or full? These two states that we talked about. And this guy says, no I'm not loaded or full, and here is the next hop you can go to. So I keep going for, forward. And if none of these guys say that they are loaded or full, I would eventually reach my desired destination and place the key value over there. However, when I do this hop, going from hazardous to one fourth of this and so on, somebody along the way may say that, look for this particular key that you're trying to place. I'm already full, all loaded, one of those two conditions is already applying to me. So this node responds back to me and says, I am either loaded, or full. If that is the response I get back, then what I'm going to infer from the response is that the rest of the distance going toward the destination is all clogged up, because of the tree saturation. And therefore, I'm not going to even try to place the key value pair at the destination, at the desire destination. And not even at this guy because he's also loaded and or full for the same key. And so I'm going to retract my step and choose this node as the node to place the key value pair. So in other words, when I do a put operation, there are two phases to it. The first phase is the forward phase. In the forward phase, what we are doing is, we are going to the guy that is half the distance to the desired destination asking him are you full or loaded. He says, no, I am not. Then you go to the next guy who is even closer to the desired destination. He says, he's not full or loaded. Then you go to the next guy, who is even closer. Using that, key based routing algorithm that I described to you earlier. So keep going till you hit a node, that is either loaded or full. At that point you know, you don't want to go any further towards the desired destination because all of these guys are either loaded or full, dealing with this particular key. And therefore, the second phase of the algorithm that I'm going to use, is retract my steps and go back, and ensure that, that this guy is still willing to host my key value pair. Why would he change his mind? Well, between the time that that I am making this forward motion it is possible that this guy got either full or loaded. So I'll recheck the condition. It says still I'm good to host your key value pair, I'll choose this node for the Put operation. That's how the Coral Put operation works. So you can see that we are not storing the key in the desired destination, which should have been the way a greedy algorithm would have worked. But in the sloppy algorithm of Coral, we choose an appropriate node that is neither full nor loaded, so that it can entertain requests for retrieving this particular key value pair. So we've avoided the meta server overload by doing this key-based routing in the forward path during the put operation. So the get operation is going to do exactly similarly. That is given a key that I'm looking for, I'm not going to go directly to the destination that might be hosting it, as would happen in the greedy approach. But instead, what I would do is go to a node that it is half the distance to the key I'm looking for. And when I do that, my hope is I'll find the key somewhere along the way. Because some guy may be serving as the meta data server for that particular key. If not, I will go, to the, destination. If nobody has retrieved that key before, it will be available at the desired destination. I'll get it from there. But the hope is that, if, a content is popular enough, then, multiple people, multiple proxies may have gotten the key value pair. And therefore, and they may have gotten the key value pair, and in turn when they have gotten the content as well, they will have put their own node IDs as a potential node for the content. And so, our metadata server, when we are looking for a particular key may not necessarily have to be the destination which exactly matches that key. So now let's see Coral in action for putting and getting content, user-generated content, that can be distributed in a democratic fashion and the load for both serving as a metadata server as well as the content server can get naturally distributed. Because of the way the Coral system works in managing the sloppy DHT. Let's say, Naomi, who is at node number 30, has some interesting content, and she wants to share it with the world. So what she does, she creates a unique signature, a key, for this content by hashing it. And let's say that the key that she generated for this content is 100. So now, Naomi wants to put the key, 100, and the value, 30, indicating that this node has the content corresponding to this key, 100. She wants to put it out on the internet, and so she uses the Coral system. And she uses the Coral key-based routing. The node that she would like to store this key 100 is node 100 corresponding to David's computer, okay? Because David's computer has node ID 100 so that's the place I would like to keep it, but we are following the Corals key-based routing algorithm. So Naomi, what she's going to do is going to make a series of RPC calls to put 100, 30 key value pair. And she finds that none of the intermediate nodes are either full or loaded. And finally, she reaches David's computer. David also says my node is neither full nor loaded. How can it be, because she just created this content, 100. So this 100, key 100, is not known to the world. So nobody is at this point serving as a metadata server for this particular key. So David is the right place to keep it, so David hosts this particular key value pair, 100, 30. Jacques finds out that there is this interesting video whose signature is 100, so he wants to get it. And he knows that the likely place where it is contained is node number 100, but once again, he is going to use the Coral key-based routing, and he is doing a get call, and the get call follows the same key-based routing algorithm of halving the distance to the destination. And so we make a whole bunch of RPC calls, finally get to the destination itself because none of the intermediate nodes have this key value pair. So we get to David's computer, and David says, yes, I do have the key value pair 100, 30, and here is the value that you are looking for associated with the key that you are asking about, and the value is 30 indicating that 30 is the node that has the content that corresponds to this key 100. That's what Jacques is going to get back. So then Jacques gets his response from, from David that the value is 30, that value indicates the node ID from which Jacques can download the content corresponding to the key 100. That's Naomi's computer. So, Jacques goes to Naomi's computer and gets the content corresponding to key 100. Naomi sends the content, so Jacques is now happy. He's got the content that corresponds to 100. But Jacques is a nice guy too. So he says well, I have the content. Since I have the content, I can also serve as a proxy for Naomi. And what I'm going to do, is I'm going to put the key value pair 100 corresponding to this content that is now mirrored over here. And say that the value is 60 indicating that I'm willing to serve as a proxy for the same content. So I'm going to do a pull operation, and this pull operation is going to go down, and this pull operation is going to use the same key-based routing algorithm. And when it gets to David, David might say look I am not interested in holding more than one value for this particular key. And so if he says that I don't want to do it then I have to retract my steps and pick an intermediate node which said that it is willing. Because it is neither loaded nor full for this particular key 100, so it's willing to serve as a metadata server. We already have one metadata server, but this guy is willing to do that for only one value, and therefore this guy becomes a new metadata server for the same key 100. So in this metadata server, new metadata server, we've got this entry 100, 60 also stored. So now there are two metadata servers that can potentially answer queries that concern this video 100. Now if a third guy, Kamal, comes to know about this cool video that is now propagating on the internet and he finds that the key for that is 100. He can once again query the Coral system for that video and he is following the same key-based routing algorithm of Coral, and trying to get towards David's node, which is node 100. So he's going to follow that, but when he does that he hits this intermediate node and this guy says, you know what, I've got the key that you're looking for and the value that is associated with this key is 60. So Kamal doesn't have to go all the way to this metadata server. He can get the answer for his query, get 100, from this intermediate node itself which returns a different value. Different from Naomi's address. Namely node ID 60 that corresponds to the new good Samaritan, Jacques, who's also willing to serve as a proxy for the same video content. So 60 gets return to Kamal and Kamal can then go to Jacques and get the content from, from Jacques. And that way you see that the origin of this particular video, which started at Naomi, is now propagated to Jacques. So the origin server need not get overloaded and of course Kamal will turn around and become a good Samaritan himself and say that he is willing to serve as a proxy also. So his key value pair entry gets into another intermediate node. Now we've got three nodes that can serve as metadata server for this particular key 100. Not just the original metadata server node, David's computer, but intermediate nodes that also have become part of the metadata server network for this particular key 100. And similarly, there is no origin server overload also. Because now the content itself has gotten distributed in several proxies. And all of these proxies have dynamically gotten the content, and have shared their willingness to serve as proxies. So if a new node wants to get the same video 100, when it makes its get operation, that get operation is going to traverse the network, and either hit David himself or hopefully one of the intermediate metadata servers. And that way the request for the actual content may go to different content providers dynamically as the system evolves. So as a result you can see that the metadata server load is distributed. And the origin server is also not stressed. That's the nice thing about the Coral sloppy DHT approach. So the key takeaway in the Coral approach, is that even though an individual request may have a little bit more latency because we're not trying to reach the desired destination directly, but going through some intermediate hops. In particular halving the distance to the desired destination. You're going to increase the latency a little bit, but we are doing that in the common good that in these kinds of environments, giant scale services, big data, large numbers of users, and content suddenly becoming popular, all of this dynamism has be dealt with in a system that is as vibrant as the internet. And Coral is a step towards that by reducing the stress on the origin server, as well as reducing the stress on the metaservers by naturally distributing it. We covered a lot of ground in this lesson spanning DHTs, CDNs, key based routing and how to avoid overloading the matter data server and the origin server using the concept of sloppy DHT. What Coral System does is offer a vehicle to democratize content generation, storage, and distribution, through a participatory approach. I encourage you to read the paper in full, to understand how the system has been implemented. Of course, Commerical CDNs such as Acmite, do not operate in this way. They're in it for the money. They contractually mirror the content for a customer. And they deploy their own additional mirrors to deal with dynamic increase in volume of requests. With this discussion, we complete the lesson module on Internet Scale Computing. Traditionally general purpose operating systems have catered to the needs of throughput oriented applications, such as databases, and scientific applications. But now more and more applications, such as synchronous A/V players, video games, and so on, need soft real-time guarantees. That is they are latency sensitive. How do we mix the two kinds of applications in a general purpose operating system? Also, what are the pain points in developing complex, multimedia applications that need such real-time guarantees? How can they be alleviated? Welcome back. In this module, we will discuss approaches to addressing these questions. In this lesson, we will discuss time-sensitive Linux, an extension of the commodity general purpose Linux operating system that addresses two questions. The first question is how to provide guarantees for real-time applications in the presence of background throughput oriented applications, such as databases and so on. The second question is how to bound the performance loss of throughput oriented applications in the presence of latency sensitive applications. Time sensitive applications require quickly responding to an event. Think of playing a video game and shooting at a target. You want the instant that you shoot at the target for action to appear on the screen. Now, problem is there are three sources of latency for time sensitive events, in any typical general purpose operating system. The first source of latency is what is called the timer latency. That is coming because of the inaccuracy of the timing mechanism. For instance, the timer event went off at this point, but the timer interrupt actually happens only here. That is, this is the event that should have resulted in an interrupt, but because of the inaccuracy of the timer, there's a latency between the point at which an even happened, an external event. And the point at which the timer goes off indicating that external event. And this is primarily coming up due to the granularity of the timing mechanism that's available in general purpose operating systems. For instance, periodic timers tend to have a ten millisecond granularity in Linux operating system. So that's the first source of inaccuracy or latency for time sensitive applications. The second source of latency is what we call as the as the preemption latency. And this is because of the fact that the interupt could have happened when the kernel was in the middle of doing something from which it can not be preempted. For instance, when the kernel is modifying some critical data structure. In that case it may have turned off the interrupt and therefore even though the interrupt went off at this point the kernel cannot be preempted until this point. That may be the second source of latency or it could be that the kernel itself is in the middle of handling another higher priority interrupt. And in that case, this time interrupt is going to be delayed. So that the second source of latency for time sensitive applications running on commodity operating systems. Okay, finally the timer interrupt has been delivered to the system and now the scheduler can actually schedule the process that is waiting for this timer interrupt so that that application can take the appropriate action for this external interrupt. But wait, there is another high priority task that's already in the scheduler's queue and therefore the app that was waiting for this timer event cannot yet be scheduled because of this higher priority tasks that has dibs on the processor at this point of time. So this is the third source of latency, that is the scheduler latency. That is preventing this external event from being delivered to the application that is waiting for this particular event. Now finally the high priority app is done, and therefore now it is time for the app that is actually waiting for this event. So this is the point of activation of the Event app, that is, the app that is waiting for this timer event. So you can see that this is the distance TA to Th. The difference between TA and Th is the event to activation distance. The event happened here, but the app that can react to this event only gets scheduled at this point, and this is the latency that is coming in for time-sensitive applications between the actual event happening. And the activation of the app that has to deal with that particular event. For time sensitive applications to do well, it is extremely important that we shrink this distance between event happening, and event activation. Let's review the kinds of timers that are avaialbe in an operating system, and the Pro's and Con's. The timer that, many of you may be familiar with is a periodic timer that is standard in, in most Unix operating systems. And the periodic timer, the pro is, periodicity. That's the pro of a periodic timer that. The operating system is not going to be interrupted willy-nilly, but it is going to be interrupted at regular periods. But the con is the event recognition latency, because the event may have happened at a particular point in time, but because of the granularity of the periodic timer. The event is actually recognized at a much later point in real time, and that's the con for periodic timer. And the maximum time of latency is equal to the period itself, so for instance, if the event happened just after a periodic interrupt, then the event will have to be delivered by the next periodic interrupt. So the worst case latency is going to be the periodicity of the periodic timer itself. The other extreme to periodic timer, is what we call one-shot timers. And what one-shot timers are, they are exact timers. That is, you can program these timers to go off at exactly the point at which you want the entropy delivered. To the processor, so the pro is a timeliness of the one-shot timer. And secondly, there is also fielding these one shot timers as and which they occur, that's an extra overhead that the operating system has to deal with. If you're concerned with the interrupt overhead, one extreme position to take is to get rid of timer interrupts all together, and simply use soft timers. That is, there is no timer interrupt, but the operating system is going to poll at strategic times. To see if there is an external event. What would be strategic times? Typically any application is going to make system calls. And when a system call gets you into the kernel, at that point the kernel can see if there are any events that need attention at that point of time. And that is a polling method, but the downside of the polling method Is the fact that, there is latency associated with that and there is also, the fact that, you have to pull all the events to see if any of them have expired, and there is an overhead associated with that. These are, the two cons, for a soft timer but the pro is, the fact that you have reduced overhead for soft timers since there are no. Timer interrupts, per se, but the kernal is using strategic points during it's execution such as system calls or other external interrupts. For instance a network packet arrival or something like that. As a trigger for looking at the time related structures to see if any of the events may have expired at that point of time. Firm timer is a new mechanism that is proposed in TS Linux and as we will see shortly it combines the advantages of all the three. Timers that I just mentioned, the periodic timer, the one-shot timer, and the soft timer. Essentially, what firm timer is trying to do is, take the pros of all of these types of timers in its implementation so that it can avoid the cons associated with each one of these individual choices. The fundamental idea behind the firm timer design is to provide accurate timing with very low overhead. And what it does is it combines the good points of the one shot timer and the soft timers. As I mentioned, the one shot timer, its virtue is the fact that we can have the timer interrupt happen exactly at the time that we want. In other words, we get rid of the timer inaccuracy that we mentioned earlier. That plagues normal timer design in general purpose operating systems. The con that we said with the oneshot timer, is that there is overhead associated with processing oneshot timer events. As well as reprogramming them. So for that reason, what they do in TS Linux with the firm timer design is to have a knob, which is called the overshoot knob, and the idea is the one shot timer expired here. This is the point at which the event happened, but what we're going to do is. We're going to have a parameter associated with the one shot timer called the overshoot parameter and what that overshoot parameter is allowing you to do is, even though this is the point at which the event happened, the one-shot timer is programmed to go off at this point. So there is a distance between the actual event happening and the point at which the one-shot timer has been programmed to interrupt the CPU. This is the overshoot parameter. You may wonder, what is the advantage of this overshoot parameter? Well, within this overshoot parameter window, there could be a system call, which is a soft interrupt. We mentioned that applications typically make system calls, or there could be an external interrupt. Any of those things can bring you into the kernel, and so you may be already in the kernel at this point during this overshoot window. So what we could do now is, we'll dispatch the expired timers at this point of time and also we will reprogram the one-shot timer that expired for the next time that we need the one-shot timer to interrupt. Normally, the one shot timer would have gone off here because this is the overshoot window, but even before that, since the system call happened, at this point, we'll going to see that oh, there is this one shot timer that has already expired, its going to interrupt me not in the too distant future, might as well dispatch that expired timer right now and reprogram that one shot timer so that it is now ready for the next one-shot timer interrupt. The upshot of doing this, dispatching the expired timer at the point of the system call that is happening within this overshoot window, is that at the point at which this one-shot timer has been programmed to interrupt, that is gone. It'll not cause an interrupt because we've reprogrammed this one-shot timer to interrupt at the next one-shot timer expiry event. Because whatever action that needed to be taken with respect to this particular one-shot timer expiration has already been taken at this point and therefore at this point we will not get another interrupt. So we have saved on the one-shot timer interrupt that would have happened at this point of time by combining the hard and soft timer in this manner. I mentioned that fielding a one-shot timer interrupt is expensive. We avioded that. Because, fielding the interrupt happened as part of handling the system call and at this point itself, we have re-programmed the one-shot timer for the next event as well. So you can see that the firm timer design combines the good points of both the one-shot timer as well as the soft timer. It is giving us the accuracy that we want from the one-shot timer but at the same time, we are avoiding the overhead associated with one-shot timer by processing that interrupt using this over shoot parameter. Of course, with, within this over shoot parameter, there is no system call or external interrupt that brings us into the kernel, then we will have this one-shot timer going off at this point. But the hope is that by choosing this knob appropriately, between hard and soft timers, we can make sure that we reduce the number of times we get the one shot timer interrupts actually occurring. As you know, in Linux they use the term task to signify a schedulable entity. And so, we're using T1, T2, T3, to mean tasks, which are schedulable entities. And the timer-q data structure, what it contains is the tasks and the expiry time for that particular task. And the tasks are ordered in this Timer-q data structure maintained by the kernel, sorted by the expiry time. So task T1 is the earliest task to go off. Because it has the earliest expiry time. T2 is next, with an expiry time of 12. T3 with an expiry time of 15 and so on. So this is the way the kernel is maintaining the data structure to know when a particular task's expiry time is up for processing the event associated with that particular task. The basis for firm timer implementation is the availability of APIC hardware. APIC is advanced programmable interrupt controller, which is implemented on chip in modern CPU starting from Intel Pentium onwards, and the firm timer implementation in TS-Linux takes advantage of the APIC hardware. The good news is with the APIC timer hardware, re-programming a one shot timer takes only a few cycles. So there is not a significant overhead to re-programming a one shot timer on modern CPUs because of the availability of the APIC hardware. So if the task that is at the head of the queue, that had it's timer event go off, if it has to be reprogrammed, all that we need to do is execute a few instructions in modern processes to reprogram that one shot timer to go off at the next one shot interval. When the APIC timer expires, the interrupt handler will go through this timer-q data structures and look for tasks whose timers have expired. Some of these tasks may be periodic tasks, some of these tasks may have been programmed to deal with the APIC timer event. So, associated with each entry in this timer queue data structure is the callback handler for dealing with that particular event. And those callback handlers are going to be called by the interrupt handler upon the expiry of the APIC timer. The expired timers are going to be removed from those timer queue data structures. If the entry corresponds to a periodic timer, then the handler will re enqueue that particular task in the timer queue data structure after updating its expiry field for the next periodic event for that task. If it is a one shot timer that this task was using. In that case, the interrupt handler will reprogramme that task for the next one short event. The way the APIC timer hardware works is by setting a value into register which is decremented at each memory bus cycle until it reaches zero. At which point it'll generate an interrupt. Now given a 100 megahertz memory bus, for instance, on modern CPUs, a one short timer has a theoretical accuracy of ten nanoseconds. However, in practice, the time needed to feel the timer interrupt is significantly higher. And that becomes the limiting factor in the granularity that you can get with one shot timers implemented using the APIC hardware. But the important point is that APIC hardware allows you to implement very fine grained timers in modern processes. And as I already mentioned, by choosing an appropriate overshoot parameter in reprogramming the epic timer. We can eliminate the need for fielding one shot interrupts, because of soft timers going off within that overshoot period. Another optimization that we can do, in the firm time at implementation, is looking at the distance between one shot events. For instance, in this picture I'm showing you, the long distance between two one shot events. There is a one shot event happening here. There is a one shot timer event happening here. Another one shot timer event happening here. And if you have such a long distance, it is possible that there may be several periodic timer events that may be going off within this long distance. So this suggests that if periodic events are going to go off, and if it is close enough to a one shot timer that would have gone off, why not take advantage of that? So what we want to do is dispatch a one shot event at a preceding periodic event. The key thing for time sensitivity is not missing the timer event. If you're going to process it a little bit sooner, that's okay. So that's exactly what is happening here. Just as in the case of the overshoot parameter being used in combing one shot to the soft timer. What we're doing here is, because periodic timers are going to interrupt anyhow and if the kernel notices that there is a one shot event that is coming up fairly soon, then it can simply dispatch that one shot event at the preceding periodic timer event that is any how accepting the processor. And once you do that, then you can reprogram this one shot event to go off at the next expiry point for that one shot event. So basically, what we're doing when we have a long distance between one shot events, is to use a preceding periodic timer event, so that we can both avoid the overhead of dealing with this one shot event. And also the cost of reprogramming it. Or in other words, we completely eliminate using one shot events for situations where the distance between the one shot events is so big that we can simply use the periodic event instead of the one shot event. And the reasons for doing that are twofold. One is, periodic event data structures are much more efficient than the kernel, they're order of one data structures. Whereas, the one shot event programming data structures in the kernel. Tends to be order of log n, where n is the number of active timers. So as an optimization, if the one shot events happen at fairly long distances. That there are several periodic events that are going to happen anyhow within that. We will simply use the periodic timers instead of the one shot timer. So to summarize, the firm timer implementation. The first point is that the APIC hardware allows reprogramming one shot timers in few cycles. And secondly, by choosing the appropriate overshoot distance, we can eliminate the need for fielding the one shot timer interrupts if soft timers go off within that overshoot period. And third, if the distance between one shot timers is really long, then instead of using one shot timers, we will simply use periodic timers and dispatch the one shot event at the preceding periodic timer event. Those are the ideas that are enshrined in the firm timer implementation in TS-Linux, that essentially combines the advantages of one shot timer with soft timers with periodic timers. So this should give you a feel for how TS-Linux, by a clever implementation, reduces the timer latency, the first component of the latency from the point of event happening to event activation. The second source of latency, as I mentioned, is the kernel preemption latency. Now how do we reduce the kernel preemption latency? Timer goes off when the kernel is in the middle of something and so we have to wait for the kernel to be ready to take that interrupt. The first approach is the explicitly insert preemption points in the kernel. So that it can actually look for events that may have gone off and take action. So that is the first approach. The second approach is to allow preemption of the kernel anytime the kernel is not manipulating shared data structures. Now the reason why you don't want the kernel to be preempted is because it is manipulating some shared data structures and if you preempt that that can result in some race conditions in the kernel. Bad news. So what we want to do is, we want to make sure that the kernel is not preempted while it is manipulating shared data structures. But if it is not, then we want to allow preemption. So these are two different approaches that can be combined in order to reduce the latency associated with kernel preemption. A technique, due to Robert Love, called the lock-breaking preemptible kernel combines these two ideas. And by combining these two ideas it reduces the spin lock holding time in the kernel. The idea is the following. So there will be a long critical section in the kernel where in it is manipulating some shared data structures, but it also doing other things within this critical section. So what we're going to do is apply these two principles and break this long critical section as follows. So we're going to change this critical section code into two sections: you acquire the lock, you manipulate the shared data structures, and you release the lock. And the reason you're releasing the lock is because you're done with manipulating the shared data structures, and then we will reacquire the lock, and finally release it. So essentially we replaced this long critical section by two shorter critical sections, one manipulating shared data and the rest of the code here. What is the advantage of doing this? What we can do is, at this point, once we are done with manipulating shared data, we release the lock and at this point we can preempt the kernel safetly. And since we can preempt the kernel at this point it's a great opportunity to check for expired timers at this point. If there are expired timers, dispatch them, reprogram one-shot timers if need be. All of that stuff can be done at this point, and then you can come back, reacquire the lock, and continue with the critical section. So that's the idea of the lock breaking preemptible kernel that combines these two ideas of explicit insertion of preemption points and allowing preemption any time the kernel is not manipulating shared data structures. The third source of time latency I mentioned is the scheduling latency, that is, the timer event goes off, and we want to schedule the app that is going to deal with the timer event as quickly as possible, but the scheduler is in the way. How do we quickly make sure that that app gets scheduled? Firm timer implementation in TS Linux uses a combination of two principles. One is called Proportional Period Scheduling and in Proportional Period Scheduling what is being done is that, when a task, so these things represent tasks T1 and T2, when a task starts up it is going to say that it needs a certain proportion of the CPU time to be allocated to it in every time quantum. So there are two parameters associated with proportional period scheduling, Q and T. T is a time window, a time quantum, and this is the time quantum that is exposed to an application. And the application can say, within the time quantum T, I need a certain proportion of the time for my task. So T1 might say that, in any time T, I need two thirds of the time to be devoted to my task and if another task, T2, says in any time period T, I need one-third of the CPU to be devoted to my task, then these two requests can be obviously satisfied by TS Linux because the two add to the periodicity of scheduling T. And this is the idea behind proportional period scheduling, in which what the scheduler does is admission control at the time that a process starts up. If it asks for a certain proportion of time it sees whether it is possible to satisfy that request. So, for example, if already the scheduler has promised T1 two-thirds of the time, and T2 comes along and says I also need two-thirds of the time in every period TS Linux is going to say uh-huh, no, cannot do that, because it doesn't have the capacity to accommodate both these requests simultaneously. So that's the idea behind Proportional Period Scheduling. So it provides temporal protection by allocating each task a fixed proportion of the CPU during each task period T. And both this Q and T parameters are adjustable parameters using a feedback control mechanism. And so this, in essence, improves the accuracy of scheduling analysis that you can do on behalf of processes that are time-sensitive. The second technique that is used in TS Linux for reducing scheduling latency is to use priority scheduling. And let me motivate that by introducing a problem that plagues real time tasks and that is what is called priority inversion. So here is a high priority task C1 and it needs some service. And it gets that service by calling a server. And this call is a blocking call to the server. And the server itself may be a low priority server. For example, this client may contact a window manager to say, I need a portion of the window and this is what I want you to do in terms of painting that portion of the window. That might be a call that a high priority task is making to a low priority server and this is a blocking call and till the server is done with its work the high priority task cannot continue execution. So if you look at the timeline, C1 is running and it makes a blocking call at this point and the server takes over. And this is the service time for the server to execute the blocking call made by C1. So at the end of this service time, C1 is ready to run again. But not so fast. It could be that during the service time of this low priority server, on behalf of C1, some of the higher priority tasks C2, which perhaps was waiting for some I/O completion, becomes runnable again. If it becomes runnable again, then this higher priority task, compared to this server, is going to preempt the server and take over the CPU. So what happens at this point is that this medium priority task, because it is higher than the servers priority, it's going to take over, preempting the server, and that is essentially a priority inversion as far as C1 is concerned because C1 is higher in priority than C2. But unfortunately, at this point of time, the server is the one that is scheduled and that is lower priority than either of these two guys. And therefore, C2 happily preempts the server and takes over the CPU. But from the point of view of C1, that's a priority inversion. And it is a time-sensitive task that affects the sensitivity of the time-sensitive tasks. This is where the priority-based scheduling of TSL saves the day for us. Basically, the idea is that when C1 makes a request to the server, the server is going to assume the priority of C1. Even though normally, it has a static priority which is lower than the priority of C1. Because C1 is making this call, the server's priority is going to be boosted to be the priority of C1 itself. So for the duration of the service time of the server the priority of the server task is going to be the same as the priority of C1, which is higher. Now, C2, when it becomes runnable, it cannot preempt the server because the server is now running at the priority of C1. And therefore, we avoid the priority inversion that would have normally happened because of the priority-based scheduling in TS linux, so the upside is that there will be no priority inversion with this priority-based scheduling mechanism that is there in TS linux. So to recap, TS linux avoids scheduling latency to shrink the distance between the event happening and event activation by doing this admission control through the proportion period scheduler and also it avoids priority inversion by using this priority-based scheduling. So these two mechanisms allow shrinking that distance as well. The other advantage of the Proportion Period Scheduling is that TS Linux can have a control over how much of the CPU time is devoted to time-sensitive tasks so that it can reserve a portion of the time for throughput-oriented tasks. So for instance, it could say, within any period T, I'm going to reserve a third of the time for throughput-oriented tasks, so that even if there are time-sensitive tasks running throughput-oriented tasks are going to get their dibs for running on the CPU. And that way, we can make sure that while supporting the timeliness of time sensitive tasks, TS-Linux can also ensure that throughput oriented tasks are able to make forward progress. So the three ideas that are enshrined in TS Linux, for dealing with time-sensitive tasks are, first of all, coming up with the firm timer design that increases the accuracy of the timer without exorbitant overhead in dealing with one shock timer interrupts. Second, using a preemptible kernel to reduce the kernel preemption latency and third, using priority-based scheduling to avoid priority inversion and guaranteeing a portion of the CPU time to be allowed for time-sensitive tasks. Those are the ways by which the distance between event happening and event activation can be deduced and we can get good performance for time-sensitive applications even though the operating system is a commodity operating system like Linux. The upshot of attacking and fixing the three sources of latency of concern for real time task is that TS-Linux is able to provide quality of service guarantees for real time applications running on commodity operating systems such as Linux. At the same time, by admission control, using the proportional period scheduling, TS-Linux ensures that throughput-oriented tasks are not shut out from getting CPU time. The proof of the pudding is in the eating. I encourage you to read the details of performance evaluation carried out in this paper that I've assigned to you for reading, wherein the authors show that both the above objectives are achieved by their design. In the previous lesson, we studied an operating system scheduler adaptation for providing accurate timing for upper layers of software, especially needed for real time multi-media applications. Now we move up in the food chain. The focus in this lesson is in the middleware that sits in between commodity operating systems and novel multimedia applications that are realtime and distributed. We have seen how P Threads serve as an API for for the development of parallel programs. For distributed programs, sockets serves as an API for application development. And conventional distributor programming such as NFS servers and so on use socket API And build RPC packets on top of that. But unfortunately socket API is too low-level and insufficient in semantic richness for emerging multimedia distributed applications. Similarly sockets serve as the API for distributed program development. So conventional distributed programs are built using socket API, for example you man have a server like an NFS server, and clients connect to an NFS server using sockets, in order to do the distributed I/O between the client and the server. But unfortunately, socket API is too low-level, and it doesn't have the semantic richness that is needed for emerging novel multimedia distributed applications. Novel distributed multimedia applications tend to be sensor-based. And these sensors can be simple sensors, like temperature sensors and humidity sensors, or more interesting and complex sensors, like cameras and microphones and radars and so on. And these sensors are distributed, which means that you have to access them via the internet. And what we want to do with these sensor streams is live-stream analysis, and often, often such applications are also called situation awareness applications. Where what we are doing is we are gathering sensor data in order to analyze in real time what is happening in the environment and take appropriate decisions. So such situation awareness applications exhibit a control loop going from sensing, prioritizing the sense data to figure out what are important. For instance, if there are lots of cameras and there is no action in front of some cameras, some cameras are more interesting than others. That is the prioritization step, of figuring out what sensors are interesting and then devoting more computational resources to analyze the camera streams or other sensor streams and as a result of that, taking some actions. And this might involve actuating other sensors, actuating triggers, actuating other software entities, or even humans, to intervene in terms of what actions were needed to be taken. And part of this control loop may also be feedback to the sensors themselves to re-target them. For example, there may be a camera that you may want to point in a different direction. And there are cameras like pan, tilt, zoom cameras, which you may want to control depending on what you are observing in the environment. And such a control loop characterization of situation awareness applications can be applied to a lot of emerging distributed multimedia sensor-based, applications such as traffic analysis, emergency response, disaster recovery, robotics, acid tracking and so on and all such applications are computationally intensive. And because they are dealing with real live streams coming from the sensors, they have real-time properties, which means that there is a need to shrink the latency from sensing to actuation, so that we can take tiny actions based on the sensed data. Since they're computationally intensive, we need horsepower to run these large-scale novel multimedia distributed sensor-based applications. And so computational engines such as the clusters and the clouds may be deployed in order to cater to the needs of these large scale, sensor-based distributed applications. An example of a large-scale situation awareness application would be monitoring, for instance, activities in an airport. And in that situation what we are trying to make sure is that normal activities are okay, but anything anomalous, with respect to what we see in the environment has to result in triggers being sent to the appropriate software agents or even humans, in terms of actions they may need to take. And camera networks are becoming ubiquitous. For instance, in the city of London, there is on the order of 400,000 cameras that are deployed, blanketing the city. And the intent in such camera networks is to analyze the camera streams to, to detect for anomalous situations. When you have such a large number of sensors the first thing that one has to worry about is overloading the infrastructure itself with the amount of data that is being continuously produced 24/7 from all of these sensors. That suggests that it is important to prune these sensor streams at the source in order to make sure that the infrastructure overload is avoided. And the traditional way to do surveillance is to have humans sitting in front of banks of monitors looking for any anomalous activity, for instance, in an airport. This obviously does not scale if you have on the order of 100,000 or 200,000 cameras blanketing a city, which is becoming fairly common in many large cities like New York, Chicago, London, and so on. And so, cognitive overhead is another big thing that you have to worry about in large-scale situation awareness applications. And it is extremely important to avoid false positives and false negatives. With false positives we are identifying events that are not really anomalous, but the system thinks that it is an anomalous event. That's a false positive and similarly, false negative is saying, well, there is an event that should have been caught but, but did not get caught and that's a false negative. So both of these are very, very important metrics that one has to measure in large scale situation awareness applications. From a programming model perspective, one of the important take aways in just the discussion about large-scale awareness application is that the programming infrastructure has to facilitate the domain expert who is developing a large-scale situation awareness application to be able to deal with time sensitive data. So what are the pain points in developing large scale situation awareness applications? The first thing is providing the right level of abstractions, promoting ease of use. Simplicity is the key. Interfaces that allow seamless migration of computation between the edge of the network and the computational resources in a data center, for instance. Temporal ordering of events that are taking place in the distributed system, and propagating temporal causality. By that, what I mean is if an event, lets say, was captured by a camera at a particular point of time, it make be processed at a later point of time in real time but there's got to be a temporal causality for the time at which it was captured and the digest of information that was created. And such temporal causality is extremely important in the development of large-scale situation awareness applications. Another requirement in such large-scale situation awareness application is that there are live data that we are dealing with, but at the same time we may also have to correlate that with historical data in order to make a high-level inference as to what's going on. For instance, if we see a speeding car at this point of time on the highway we may want to ask the question, was this car involved in an incident in the last n days? And that is kind of historical data that has to be correlated with live data. So a programming infrastructure that provides facility for these kinds of things would make the life of the domain expert, writing such large-scale situation awareness applications, so much simpler. Let's look at the computation pipeline for a Video Analytics application. Let's say I'm interested in detecting and tracking some anomalous event. So I have cameras, which captures the images from a particular environment. And you're detecting for something specific. Maybe you're looking for [INAUDIBLE] in every camera frame, and once you detect, it shows face then you say it what? This guy is a suspicious individual, let's track him as he moves around. So, the tracking is taking a particular object of interest for the domain and asking the question What is happening to that object as time moves on. And in the process of tracking, you are recognizing who that person is. If there are multiple people in a particular alignment, your recognizing the specific individual that you may be interested in. And if that individual is recognized, then perhaps you want to raise an alarm. So, this is just a simple pipeline of tasks to illustrate what a domain expert may be doing in a video analytics application. So such a domain expert is an expert in writing detection algorithms, tracking algorithms, and. Recognition algorithms so that they can process the video stream in real time to generate alarms. So the objective in Situation Awareness applications is to process these streams of data for high level inference. In other words, we're not watching a YouTube video, but we're using the sensor streams to derive high level inferences to what is happening in the enlighnment. And that's what is the nature of Situation Awareness applications. As I mentioned Video Analytics is in the purview of a domain expert like a vision researcher or a developer, who knows how to write sophisticated detection, tracking, and recognition algorithms. But how do we scale that up to thousands of cameras? If an object is moving from one camera to another, these are the kinds of things that the domain expert. For Situation Awareness may have to worry about, this is where systems can come in with programming models that alleviate the pain points of a domain expert developing Situation Awareness applications. Persistent temporal streams, which is the focal point of this particular lesson. Is just one exemplar of a distributed programming system for catering to the needs of situation awareness applications. Such as, what I described to you, just now. I want to stress the fact that PTS is just an exemplar, not the last word. But it is good to look at one concrete example of a distributive programming system that can help reduce the pain points in developing Situational Awareness Applications. The PTS Programming Model is very simple and straightforward. It's a distributed application and in the distributed application, there are threads and channels. These are the two high level abstractions provided in the programming model. And the computation graph that you generate using the PTS programming model, in particular the extractions thread and channel provided with the PTS programming model. The computation graph looks very similar to a UNIX process socket graph. So in terms of a transition for a programmer that is familiar with the socket API. To transition to programming, using the abstractions provided by PTS, is not significant. While the computation graph looks similar to a process socket graph, the semantics of the channel abstraction is very different from the socket abstraction. In particular, what the channel holds are time sequenced data objects. So a thread is generating time sequence data objects can place those time sequence data objects into the channel. The other thing you will also notice from the structure of this computation graph that there could be multiple producers of data that go into a given channel And similarly, there could be multiple consumers of data that are getting data from a given channel. Or, in other words, a channel allows many to many connections. A thread that produces some data and wants to put it into a channel uses the primitive provided for. Operating on the channel, namely the put item primitive and the put item takes two arguments, one is the item itself, which is whatever the application decides as the content of the data that it wants to put in the channel. The time stamp that it can associate with the data that it is producing and placing in the channel. So if you look at the contents of any given channel, it's really showing a temporal evolution of data that is being produced by a particular thread. For example, this thread could be capturing data from a camera and every item that it produces is one frame of image captured from the camera associated with the wall clock time. At which that particular frame was captured. A computation that is needing to use the data will do a get on a channel and when it does a get on a channel it actually has to specify two arguments. One is a lower bound for a time stamp that it is looking for and an upper bound. Or in other words a thread that wishes to get data from this channel, for instance, will specify in the get primitive that I want to get the data that is in a particular channel starting at a particular time. Say 1:05 p.m., To an upper bound, 1:06 p.m. And what the thread is going to get back is all the items that this channel contains between this lower bound and upper bound. The programming model also allows an application to specify time variables in an abstract way. It could say, I want the oldest item from a channel, or the newest item from the channel. In addition to specifying explicit lower bound and upper bound markers if it so chooses. So in other words, a channel contains a continuous stream of data that have been snapshotted with different time stamps associated with them. So, this is the way one could write a program that uses the primitives provided by the PTS programming model. Let's say we are writing the code for a detector that is looking at video stream produced by a particular camera. In that case, the detector will associate a variable channel one with the video stream that corresponds to what that camera's producing. And continuously what it is going to do is, it is going to get a camera image from that particular channel. And it might specify a lower bound and an upper bound in terms of time that it is looking for. Process the data, produce an output, and it might place the output. That it produced as a digest of information that it processed into another channel, and it'll do that by putting the digest as a new item, and associate a time stamp. This is a place where the programming model facilitates. Temporal causality to be propagated in the distributed system. So if this thread is a detector thread that is looking at the camera images produced by a camera thread, then it can take the camera images, process it. And place it in its output channel, and when it places it in the output channel, it can use the same time stamp that it got for an input image in doing the put operation, and that way, there is simple causality that is being propagated through this graph by the computation using the primitive that is available from this PTS programming system. Quite often in situation awareness application, the computation may have to use data from several different streams in order to do its high-level inferencing. So a particular thread may get information from several different streams, and it may have to make high level inference based on the composite results that it is getting from all these schemes. And since all the data that is coming from the channels have time stamps associated with them, an intermediate computation like this that is taking multiple streams and generating a high level inference from it, can use the time stamps in the incoming streams to see which data items in these streams are temporally correlated with one another. And use that information to improve the kind of inferencing and hypothesis that can be derived in the situation awareness application. So in a nutshell, the PTS programming model, what it allows, is the ability to associate time stamp with data items produced by a computation. So the model allows propagation of temporal causality because the fact that a computation can associate a time stamp with an item that it produces based on the time stamp that it got. On the data that it obtained from its input channel and thirdly the fact that every stream is temporally indexed allows a computation to correlate the incoming streams. And recognize which data items in the input streams are temporally related to one another by looking at the time stamps associated with the items that it is fetching from the different channels. Another common need in building such situation awareness applications is that a computation may need to get correspondingly timestamped items from several different sensor sources. So, for instance, in this picture I'm showing you a video source, an audio source, a text source, and gesture source. And this computation, in order to do its high level inferencing, it wants to use the multiple modalities of sensing that's available to generate high level inference that can be more robust. And this is where bundling of streams that PTS allows comes in handy. PTS allows streams to be grouped together, as a stream group. And any number of such streams can be grouped together and labelled as a group. And in creating a stream bundle like this, a computation can specify that, for this stream group, this guy is the anchor stream. For instance, we could name the video as the anchor stream for this particular stream group. All the other streams within the same stream group are dependent streams on the anchor stream. And in PTS there's a primitive for getting a group get, say, get corresponding timestamped items from all of the streams in a particular stream group. The group get primitive is once again a way of reducing the pain point for a domain expert. And having to go out and fetch individual items from each one of these streams, and selecting temporally correlated items from all the other dependent streams. That burden is taken away by PTS by providing this group get, where you can get correspondingly timestamped items from all the streams in a given group. I mentioned that any system design should be simple. And power of simplicity is the key for adoption. So in this case, let's revisit the sequential program for video analytics that I showed you earlier. Converting the sequential program for video analytics into a distributed program, using the get put primitives, provided by PTS, is fairly straightforward. What you do is, you interpose between these computations, that are there in the original pipeline, channels that are named entities that can be used to hold the temporal evolution of output from a particular computation. So in this case, camera capture is a thread, and this capture computation captures images from a camera with a certain periodicity. And places it into a name channel called frames, and so the frames channel abstraction is going to contain the temporal evolution of output produced by this capture computation. And the detector can discover where this frame channel is, connect to it, and get images from this channel, process them and produce blobs that characterize the objects that it sees in any particular given frame. So this is a temporal evolution of the detector's output corresponding to the frames that it sees. And the tracker takes these blobs. And interesting blobs from that are the objects that it is tracking and it produces a temporal evolution of the location of objects that it sees and places it in its output channel and a recognizer may then look at these objects and consults. A database of known objects to see whether any of those objects that are being continuously captured, detected, and tracked, correspond to anything that may indicate that there's an anomalous situation. And those are the events that it produces, and those events may trigger an alarm. So basically, what, what I'm showing you here is converting the sequential program for video analytics into a distributed program using the channel abstraction and the get/put primitive available in the PTS programming model. So the ovals are the threads of the PTS abstraction and these rectangles are the channels that are there between any two computational entities. PTS provides simple abstractions, namely channel and simple perimeters to manipulate these abstractions, namely get and put. And these channels can be anywhere in the distributed system, just like UNIX sockets, these are named entitities. That are network-wide unique and therefore, you can have a large scale distributed computation in which handles exist everywhere and you can discover them and connect to them and do I/O on them using get and put. All the heavy lifting that is the systems work that needs to happen. In order to support this channel abstraction and the operations that you do want channel abstractions like the get and put operations, are all managed under the cover by the the run time system of pts. So PTS channels can be anywhere, just like unix sockets, can be accessed from anywhere, and they are network wide unqiue, and this is a similarity of the channels to unix sockets, in terms of the ubiquity,in the entire distributive system. But what makes PTS channels particularly attractive for situation awareness applications is that the run time system and the APIs, provided for manipulating the channel Treats time as a first class entity. What we mean by that is time is manipulatable by the application, in the way it specifies items to the runtime system. And it queries the runtime system using time as index into the channel, and that's what is unique about the PTS channels. The second unique property of the PTS extraction is that, it allows streams to be persistent under application control. Remember that these sensors are producing data 24/7. Therefore continuously data is being produced, and all of it cannot obviously be held in the memory of the CPU, so they have to be persisted on more archival storage like a disc, and PTS provides the ability to persist the streams under application control. The flip side of persisting streams is that when a query comes in. For a particular channel, it is actually saying get me items from a lower bound to an upper bound, and the lower bound may be yesterday, which means that we are asking for data that are historical in nature in addition to live data. So the PTS run time system and the semantics of channels, allows seamlessly handling live and historical data. The perimeters are just get and put, whether we are accessing data that is current or historical. You give a lower bone marker and an upper bone marker and the run time system gets to work in doing whatever heavy lifting that needs to be done. To bring the data that you're looking for between the lower bond marker and the upper bond marker. So that's the power of the PTS channel perimeter. It is simple to use, but at the same time it provides handles that will make the life of the domain expert that is developing. A situation awareness application, so much easier because it allows time to be manipulated as a first class entity recognized by the programming system. And it allows persistence for data beyond the lifetime of a computation by moving it to archive storage under application control. And thirdly, it allows for seamlessly integrating live and historical data. Having given you the abstractions and PTS, and the simplicity of the programming model. I will now introduce you to the heavy lifting that needs to happen under the covers in order to support this simple programming model from the point of view of the domain expert developing a situation awareness application. All the computations in the application can be considered as either producers or consumers of data. Producers of data, are putting things into the system and consumers of data are getting stuff from the system. And under the covers there are worker threads in the run-time system of PTS that react to these get calls coming from the producers and consumers. For instance, whenever, a producer puts an item, that results in new item triggers. That are going to be generated by the worker threads to the rest of the implementation. Implementation of the channel architecture is a three layer architecture. The top layer is the live channel layer of the architecture. And this is the layer that reacts to the new item triggers coming from these worker threads, working on behalf of a put call that is coming from a producer. So the channel abstractions result in these new item triggers to be sent to the live channel layer. Of the channel architecture. And the live channel layer, is the one that is holding a snapshot of items that have been generated on a particular channel. Starting from the oldest item in the channel to the newest item that just came in because of this new item trigger At the time of creation of a channel, the creator of a channel could specify what the semantics of the data that are kept in this channel are. In particular, a creator of a channel could say that, what I want the channel to contain are live data corresponding to a certain snapshot of real time. From oldest to new. For instance, I could say keep only the last 30 seconds of data in the channel. Rest of it, you can throw it away. So there is a garbage collection trigger that is part of the run time system, that is looking at information that is in the channel, that says. What items in the channel have become old and therefore can be thrown away. Those are the Gc triggers. And the Gc triggers will move data that have become ancient, so far as this channel is concerned, and move it into this garbage list. Meaning that these items are no longer relevant from the point of view of this application. And therefore they can be garbage collected. So they're put into this garbage list. And there is another garbage collection thread that is responsible for periodically cleaning up all the garbage that has been created and throwing away stuff that is no longer relevant. For this computation. So this is all the channel book keeping that's happening, under the covers in support of an application that is using the PTS library. But there's a lot more to it than just dealing with live data and data that is no longer relevant that has to be garbage collected or thrown away. As I mentioned one of the features of the PTS architecture is the fact that an application can choose to keep data for as long as they wants, and that is the persistence property supported by PTS run time system. So once again properties of the channel An application programmer could specify that, I don't want to throw away stuff that becomes old to keep in the channel, but I want to archive them, I want to persist them. And if those properties have been associated with the channel, then when items go past the window that has to be stored in the Live Channel Layer. The Live Channel Layer results in generating what are called persistent triggers to indicate that some items have become old in this channel, and they have to be persisted. The second main functional layer in the channel architecture is the Persistence layer. The Interaction layer is just a go between the Live Channel layer and the Persistence layer of the channel architecture. And what the Persistence layer does is, based on the persistence triggers that it gets from the live channel layer. It is going to take items from the channel and decide how to persist them. Now, here again, the application can have a say in how items need to be persisted. And they do that by having a Pickling Handler. That is, the application can specify Here is a function that I want you to use every time you decide to persist some item from the channel. For example, an application may specify that don't store all the images as is on archive or storage, but Condense them in such and so fashion. And that is a function that it can supply. And the runtime system, when it works on persistence, will automatically apply the application-specified function on the items that need to be persisted to create a digest. Which will then be persistent. Items that need to be persistent necessarily have to go to non-volatile storage devices, and here again, the PTS architecture supports several different configured Backends to store items that need to be persistent. And the Backend layer is the third layer in the channel architecture. And PTS supports several different Backends to support the persistence activities and it is an application choice as to which back end layer it wants to use for its specific application. The Backend layers supported by PTS include mySQL, it can use Unix file system as a persistence layer, or it can use a file system from IBM called GPFS. So mySQL, Unix file system, and GPFS are the three different Backends that are available. For the persistence layer to store channel data that needs to be archived for later retrieval. The nice property is that all of the persistence activities happen unbeknownst to the user. All that the user has done is in the creation of a channel. Specified certain properties to associate with that channel. For example, the property that may have been associated with a channel is that any items beyond the last 30 seconds persistent on the storage, and when you persist them on the storage, apply this function. Those are the things being specified by the user creation time of the channel, once that is done the heavy lifting that needs to be done during down time, is all handled under the covers, by the run time system. Of the channel architecture that takes items from the channel, pickles them using the function that has been specified, and uses one of the configured Backends to push those pickled items onto the persistent storage. On the other side, when. An application wants to get an item. That item range may span from something that is there in the live channel part of the three layer architecture, or it could be on the archival storage. Now it is up to the run time system to retrieve all the items between the lower bond. And the upper bound specified by get primitive. So get primitive that spans both live and archived items results in get triggers being passed from the light channel through the interaction layer to the Backend so that corresponding to the get interval that is specified. The Backend layer can pull the data from the archive storage and pass it up so that it finally gets to the application. What I wanted to illustrate through this picture is there is a lot of heavy lifting that needs to happen in order to support a simple programming model. The programming model is very simple But, in order to support the simplicity, all of the heavy lifting has to be absorbed under the covers in the run time system of the PTS programing model. Just as map reduce provides a simple and intuitive programming model for the domain expert to develop big data applications, PTS provides a simple programming model for the domain expert to develop live stream analysis applications. Time-based distributed data structures for streams, automatic data management, transparent stream persistence. These are then unique features of the PTS programming model that facilitates live stream analysis. I invite you to read the paper in its entirety to understand the many systems challenges that are identified and solved by PTS in providing this programming model. This completes our discussion in this course on the topic of real-time and multimedia. Computer system security has assumed enormous importance in the connected world that we live in today. Indeed, there are courses offered just on the topic of computer system security. And universities, including Georgia Tech, offer an entire Master's degree program on information security. In this course module our goal is to get an introduction to the terminologies in computer system security that we should be familiar with as operating system designers. Using a case study we'll also learn Cryptographic techniques that can be used as a tool by an operating system designer to provide authentication in a distributed manner. We'll start this module with a discussion of the terminologies first articulated by Jerome Solser. You'll be amazed at how computer visionaries thought of issues with information security, even before computers were connected to one another. It's always very illuminating to go back in time and look at the thought pieces from computer visionaries, and particular I want to draw your attention to this memorandum that was done back in 1963 that talks about intergalactic computer networks. I encourage you to do a web search and get this document and read it. Here is another one this document talks about the first time computers talk to one another. The first computer to computer communication back in October 29th, 1969. Now this is considered the first ever email that was written by Ray Tomlinson from BBN Technologies back in 1971, and with all the junk mail that is floating around, you might be wondering why email was invented in the first place, but it is a useful tool when it is not junk. This is a log book that was maintained by professor Leonard Kleinrock of UCLA who is considered one of the networking pioneers. And, this log book records the first time there was a computer to computer communication. A call was made from UCLA to Stanford Research Institute. Host to host communication recorded on October 29th, 1969. And this is the first ever computer to computer communication. Here is another interesting chart. This chart shows all the computers in the world in one chart. March of 1977. This is your entire internet on one chart back in 1977. And the model of a computer system back in 1975 was, you had a mainframe computer, which consisted of a CPU, memory, and I/O devices like a disk, and you have cathode ray terminals through which users can connect to the CPU and use the CPU in a time-shared manner. So this was the model of the computer. Back in 1975. Note that I'm not showing any connection to any network here. The reason I'm giving you this buildup with all the first in computer technology and the model of the computer system, is because the seminal paper by Jerome Swalser identifies terminologies, you and I know, as current day issues. Examples include denial of service, firewalls, sandboxing, objects, and so on. Let's talk about the terminologies identified by Saltzer in his seminal paper from 1975, that talks about the issues in computer security. The first issue that is identified is when to release information. In this day and age, we all know the importance of privacy of information. And there is a distinction between privacy of information and security. Privacy has to to with when, as individuals, we expect to release information or. Prevent information for being released. That's the individual's right and responsibility in terms of information that they own. Security, on the other hand, is dealing with how to make sure that the system is respecting the guarantees that the user needs both in terms of. Privacy of information as well as when they want to release information. Said differently, privacy is more concerning the individual and the information that they own, they possess, how their information is protected and released when. And if an individual wants to release information. Security on the other hand, is a system function. The system is guaranteeing certain properties about information that it is preserving on behalf of the user community. And in particular, the system has to protect information that belongs to individuals. And ensure that those information is released only when the individual that owns that information wants it to be released. So in this sense, there's an obligation for the system to ensure that users of the system are authenticated. Meaning, that if I go to the system and want to access some information. The system has to ensure that I am who I am. That's authentication, and then, it also has to make sure that whatever information I have access to, is something that is not violating any privacy of other individuals that may have put information into the system. So, protection and authentication go hand in hand. And building a secure system. This also identifies a comprehensive set of security concerns. The first security concern is unauthorized information release. If I have put some information into the system, then it is a responsibility of the system to make sure that the information that I've put in, is not released without my authorization as the owner of the information. So that is the first concern. The second concern is unauthorized modification of information. I may have released it for a certain set of users to access information that I've put into the system. But, I may not have given authority for them to modify the information, so, unauthorized modification of information is the second set of concerns. The third security concern is unauthorized denial of use. What that means is. If I've given authority to a set of users that they can access my information, there should be nothing that prevents that set of authorized users from being able to access that information. So that is what unauthorized denial of use is concerned about. And you may be much more familiar with the term Denial Of Service. That's exactly what this particular concern is all about. In fact, this is the first mention in the literature of Denial of Service attacks on a server, that prevents authorized users from being able to get service from the system. So what should be the goal of a secure system? Well the goal of a secure system should be to make sure that all such violations do not occur. That is, preventing all violations is the concern of a secure system. But unfortunately stated this way, it's a negative statement, preventing all violations, that's insuring that the system is somehow secure proof, meaning that nobody can attack the system. And because it's a negative statement it's hard to achieve. It's sort of like saying there, are no bugs in my program. There is no way to prove that a program has no bugs. And in a similar manner there, is no way to assure that the system prevents all violations. So what Saltzer argues is, the goal of the secure system should not be stated negatively, but positively. If the goal of the secured system is stated in this negative manner that it has to prevent all violations. At best, it can give a false sense of security because there is no way to assure that bad guys are not going to break into the system. So, making a statement like my system prevents all violation, is giving a false sense of security because it is not achievable in practice. Also goes on to identify four levels of protection. This is of course unprotected systems, so an unprotected system is where there is no level of protection. An example would be the early operating system for the personal computer, called MS-DOS. And it had hooks, in the system for preventing mistakes by the user. But there was no real protection. So mistake prevention is not the same as securing a system. That's important to see the distinction between mistake prevention and a secure system. Now, let's talk about the four levels of protection that, salts are identified in this classic paper. The first level is what he calls all or nothing. An example of a system that implemented that would be IBM's VM-370. And most of the time-sharing systems of the 60s and the 70s, they had this all-or-nothing property. For example, in the VM-370, Each user was given the illusion as though they have their own personal system. That's why it's called a virtual machine and the only way they can interact with one another is by explicitly doing I/O from one virtual machine to another virtual machine. That's the All-or-Nothing Property that VM-370 had. The second level of protection is what results are called controlled sharing, that is for example having access lists associated with files, so that an individual, if I create a file, I could say my file can be shared by My students, and here are the names of the students that have access to their file. So that's an access list that you can associate with information that you create and give to the system for safekeeping. That's controlled sharing. Another level of protection is what's also called user-programmed sharing controls. Now this would be facilities similar to what you find in Unix file system today, for instance, such as being able to associate different access rights for files for different groups of users. In unix, for instance, there are levels of protection that you can associate with a particular file that you created. What is the access right for the creator, the owner of the file? What is the access right for a group that is defined and what is the access right for the rest of the world? So there are three levels of, protection that you can associate of the file, and that's an example of user program sharing controls. Another level of protection is having user defined strings on information. For instance, in the military it is often common to have physical files labeled with top secret. That has to be opened by only some privileged set of users. Similarly, you can associate such strings on information that you create and store with the system. There's four levels of protection that I've identified here are not cast in concrete, because as the system evolves, as the user community evolves, You need to be able to deal with the dynamics of use of this information, use of the system. So that is another issue which you can call as a cross-cutting issue with respect to these levels of protection. For example, how do I change the permissions I've given to,a particular file to a set of users that I've defined as a group that can access that file. How do I change the set of users I've included in the access list? What do I do to remove somebody from the access list? What do I do to add someone to the access list for information that I've already created? So all of these are issues that deal with the dynamics of use of information. Salza identifies eight design principles that goes hand in hand with the levels of protection that we just talked about. In designing a secure system, the first principle to adhere to is economy of mechanisms, and by that, what is meant is that the mechanism should be easy enough, so that it can be verified. The mechanism should be easy enough so that it can be verified whether it works or not, that's important, Economy of Mechanisms. The second principle is what he calls fail safe defaults, the idea is you want to explicitly allow access to the system or information. The default should not be no access. The reason for saying this is because if the default is no access then there is no way to guarantee that, that information is protected. This is a negative statement again. So instead of a negative statement like that, you want to explicitly allow access to information. That's what is a second design principle, is in designing the system, you make sure that the default is fail safe. The third design principle is what he calls complete mediation. And by that what is meant is that the security mechanism should not take any shortcuts. For example in authentication, you want to make sure that when somebody presents credentials for authenticating themselves with the system, that has to be checked completely. And what that means is that, there should be no shortcuts to authentication. An example of a shortcut, will be a password file that lives on storage, if it is cached, for performance reasons, that can be a source of secuirty violation. Because the actual password file on the storage system may have changed, but the cached copy in memory may not be reflecting it, and that would result in violation of security. If the authentication uses the cached copy instead of the persistent copy. So that's an example of why you need complete mediation in designing a secure system, so that's another principle that a secure system should adhere to. The next design principle is what is called open design, meaning that you want to make the design completely open, so in other words you publish the design exactly spec it out and publish it, but protect the keys that are used by the design. What that means is that cracking the design, even though it is published, in order to access the design or get any useful service out of the system, you have to present keys. And those authentication keys should be so hard to break. Computationally breaking the keys should become infeasible. So that's what is meant by open design. And what does open design principle also foster is the underlying tenet that detection is easier than prevention. We don't know what all attacks are possible from the bad guys. And so it is not possible to prevent all of them. And therefore, what we want to do is, you want to publish the design but make breaking the protection computationally infeasible and detect that a violation has happened, than try to prevent it, detect that a violation has happened, so that's the key idea behind this open design principle. The next principle talks about separation of privilege. You may have seen quite often in banks, two keys may be necessary to open the vault of the bank. And the two keys will be held by two different individuals, so that both of them have to come together in order to open the vault. So this is the idea behind separation of privilege. The next principle talks about, least privilege. That is, we want to use the absolute minimum capability that we need, in order to carry out a certain task. So the controls in the system should be based on need to know. An example would be, to do certain things on your computer. For instance, if you want to install certain new pieces of software, you may be able to do that as a normal user on your laptop, but for certain things you may need administrative privileges. So clearly identifying what functions you need administrative privilege or superuser privilege, versus, what functions you need normal privilege and ensuring that the right level of privilege is provided or afforded. By the system to the user, is an important design principle. And in fact, this is the origin for the idea of Firewalls, that are common these days in organizational setups. What firewalls do is to insure that individuals within an organization are able access external information from inside the corporate network only on a need basis, or information that is inside the corporate network is allowed to get out only under authorized conditions. So that's the, the need to know based controls, and that has become very common these days with the prevalence of firewalls in, in any administrative setup. Another principle is, least common mechanism. Or in other words, if there is a mechanism that the system wants to implement to assure information security. At what level should that mechansim be implemented? Should it be implemented in the kernel? Or should it be available as a library that is outside the kernel? Because there are implications when something is inside the kernel, it has access to information that is also inside the kernel. Wheres, if it is a library sitting on top of the kernel, then you can limit the amount of damage a malfunctioning mechanism can do to the system as a whole. So that's the idea behind these common mechanisms. And the last design principle that Salza identifies is psychological acceptability. And by that, what is meant is the mechanisms being easy to use for the end user, so that they completely understand what they are doing. When they are using a particular mechanism that is provided by the system, so good user interface is something that is extremely important as one of the design principles in building a secure system. Two things jump out when you look at this set of design principles. That is lead out by Salza in his seminal paper back in the early 70s. The first thing that jumps out is all of these are positive statements. They are not negative statements. I mentioned earlier saying that, my system prevents all violations, is a negative statement because, it is saying that my system is bullet proof. No system is bullet proof, and therefore all the things that are being laid out as design principles are positive statements and they give a way by which you can say that these are the things that it can do. And you build enough into your design, such that you can detect violations when they occur, rather than trying to prevent them. Prevention is much harder than detection. The other thing that jumps out is the fact that all of these principles, laid out in the early 70s, are applicable to today's systems and these principles were crafted when computers were not even connected to the extent that they are today. So the two key takeaways from these design principles are, first of all, you want to build the system in such a way that cracking the protection boundary is computationally infeasible. That's the first takeaway. The second takeaway is you build the system to detect violations rather than prevent violations. Because prevention is much harder. Detection on the other hand, is something that is doable. This paper by SALSA is a classic. The issues identified in section one of the paper are classic information security issues, relevant to this day. To think that these issues were thought of at a time when computer to computer communication was not routine. And all the nodes connected to one another in the entire globe fits on one slide. Gives you how visionary our computer pioneers were. The Andrew File System was a bold new experiment in the CS department at CMU. The intent was to enable students across campus, to be able to walk up to any workstation on campus, and start using it. All the files stored in a central server on the local area network. Will be accessible in a safe and secure manner from that workstation. The assumption is that the network itself is untrusted. In this lesson we will use, the Andrews system as a case study to see how Private key cryptographic infrastructure can be used for the security and authentication of such a system. The focus in this lesson is a distributed file system being made available to a user community. And the year is circa 1988, and the state of the computing at that time looked as follows. Local area network and client workstations connecting the servers and the local disk on the workstation serve as efficient caches of files that maybe downloaded from the server. The stage set for the Andrew File System, AFS, is the CMU campus. And the user community can access file servers using workstations connected to a local area network. And as I mentioned, local disks on the workstation served as efficient caches. And the vision of the designers of the Andrew File System and the Coda file system, both of which were built at CMU for enabling the student community to access a central file system, is as follows. A user walks up to any workstation and, and logs into the work station. And your content magically appears on this workstation from the central server as soon as you log in. In other words, in the CMU experiment, what they wanted to afford the users to be able to do is have this ability to walk up to any workstation spread out throughout the campus and be able to access their personal information that is stored in servers that are central to the entire campus. Isn't that what today's cloud computing and mobile devices are trying to do to your content? In some sense you'll see that many of the technologies that we take for granted today had their modest beginnings in experiments such as the Andrew File System and the Coda file system at CMU. The architecture of the Andrew System looks like this. There are client workstations, which are called virtues. And these client workstations, are connected by insecure network links, to a local area network. And through the local area network, they can access servers, S1, S2, S3 and so on. And these servers are in a secure environment or in other words, accessing the servers from a client work station uses insecure links but once inside the boundary of the servers, the servers can talk to one another, exchange information, grab data from the disks and so on. This communication that happens inside this server environment is a secure communication. That's part of the reason they've named the server environment as Vice, and, the client environment, where you're accessing information from the secure servers, as the Virtue workstation. And the important thing to note is the fact that the clients have to access information from the server over insecure links. What that means is that in order to make sure that information that is sent on the wire to the servers or received from the servers to the clients. They have to be encrypted, since anyone can sniff the packets that are flowing on these wires. But inside the vice, the communication that happens among the servers, because it is secure, there is no need for information encryption. The client workstations run some flavor of Unix operating system. So inside each client workstation, there is a special process called Venus. And this process, which runs on the virtue workstation, is for the purpose of authentication of the user, and for client caching of files that the user may fetch from device servers into the workstation that they're logging in to. The user will use RPC in order to get the files that they want to work on, on the workstation. And this is where Venus comes in. Venus is the one that authenticates a user walking up to a virtue workstation, to the server and then acting as the surrogate for the user, in fetching the files that he or she wants to work on from the file server using RPC. And since I mentioned that the link that is used to communicate between virtue and vice is insecure. The RPC has to be a secure RPC. In other words, the RPC messages, both passing the parameters and receiving the results, is going to use encryption on the data that is being sent and received between the virtue and the vice file server. Let me give you a very quick refresher on encryption. There are two families of encryption systems. One is the private key cryptosystem. And in the private key cryptosystem both the sender and the receiver sse symmetric keys for encryption of the data and decryption of the data. A common private key system that we all are using probably on an every day basis is passwords. When we log into either a corporate network or a university network. We use a username and password and that is using a private key encryption systems. The idea is simple. The sender takes the data that they want to send a receiver and encrypts the data using a key and this key is a private key that is known only to the sender and the receiver space and nobody else. So by encrypting the data with this key they produce what is called a cyphertext and the cyphertext can go on insecure links. Anybody can see the bit pattern of the cyphertext, but in order to make sense out of the cyphertext they have to decrypt it. And to decrypt it they need the key and the key is only available between the sender and the receiver. So this is where Salsa's principle comes into play. That is, publish the design, but protect the key. The keys are protected between the sender and the receiver. You're publishing the method by which you are transfering information between sender and receiver. Namely, using a private key encryption system. But, for any bad guy to access information that is flowing on the wire they have to have the key in order to make sense out of the data that is flowing. And you make breaking the key hard enough, computationally, that the system is secure. Now one of the problems with the private key system is the fact that the keys have to be distributed to the sender and the receiver and key distribution problem is one of the difficulties with private key cryptosystems. Especially as the size of the organization becomes larger and larger. The public key cryptosystem overcomes this key distribution problem. And in this case there is a public key which is published. The name itself suggests the idea behind the public key cryptosystem. And that is, anyone can create an encrypted data by taking this public key, which is available, let's say published in the Yellow Pages. They can take the public key, take the data that they want to send and encrypt it, and send it on the wire. But in order to decrypt the data. The public key is no good. You need a private key that is the only way to decrypt this data. Well, in other words, the mathematical underpinning of the public key cryptosystem is that there are asymmetric keys, a pair of keys for information exchange. In order to encrypt you use a key that is a public key. Anybody can have access to that. But in order to decrypt that, you need a private key. And, conversion of data into a cyphertext using the public key is a one-way function, which is not reversible. Similarly, converting the encrypted message into the original data using the private key is another one-way function. So one-way functions are the mathematical basis for the public key cryptosystem. So now, the way you would send data, if you're the sender, take the data, encrypt it using the public key. You create a cyphertext. Once you have the cyphertext, you cannot convert it back to the data using the public key. The only entity that can convert the cyphertext back to its original data is the entity that has the private key. The other part of this asymmetric key infrastructure, namely, the private key and using the private key, you can decrypt the cyphertext and create the original data. So this is the workflow for a sender to encrypt the data, clear the cypher text, send it to the receiver, receiver decrypts it using the private key, and produces the original data. So here is a private key crypto system in action. Two entities, A and B, have exchanged keys. A will use the key KA to send a message to B, and B can decrypt the message using the same key KA. And similarly, when B wants to send a message to A it encrypts it lets say using another private key key B and when the message comes over here A will decrypt the message using the same key that was used for encryption namely key B. So one of the things that should be obvious is that, in order for this private key encryption system to work, both the entities need to know when they get an encrypted message who is the author of the message, because that is the only way they know what key to use in order to decrypt this message. So if a sends a message to b,it's sending this encrypted message. When this cyphertext arrives here, for B to know that it has to use this key KA, it needs to know the identity of the sender. So the identity of the sender has to be sent in cleartext. So this is the format of a message that is going from A to B. The identity of the sender in cleartext Meaning that, looking at this, we will immediately know, this message is from sender A, and therefore, I should use key k A to decrypt the cyphertext, and vice versa when A gets a message from B. Of course, k A and k B can be the same, or in other words, it is the same key that is used for, A to send a message to B as well as for B to send a message to A. But the important thing to take away is that the key that is used for encryption and decryption of a given message is exactly the same, that's the idea behind the private key encryption system. There a few key challenges in implementing this Andrew system, which is intended for campus environment. User community consisting of students, who can login to work stations anywhere on campus and those workstation are connected by insecure network links to central server. And a simple server is a depository for all the files of the entire user community. So the challenges include authenticating a user. That is, when a user says I'm Kishore, the system has to verify unambiguously, that the user indeed is Kishore. A second challenge is authenticating the server. That is, if I, as a user, log into a work station and I get a message from the server, I have to be assured that this message is actually coming from the server. And not some Trojan horse that is pretending to be the server. The third challenge is preventing replay attacks. And what that means, is that even though we using encryption to secure the data that is going on the wire, someone that is sniffing the wire Could grab a packet and then resend that same packet, and that should not fool either the sender or the receiver. That's what is meant by replay attack, and the [UNKNOWN] challenge is of course ensuring that the user community is shielded from one another. Either due to unintended interference of user or another, or malicious interference. Both of those situations have to be avoided. So that is what I meant by isolating users. So these are the design challenges for the Andrew system. So in the Andrew file system, they decided in order to make sure that all of these challenges are met, they have to use a secure rpc as the basis for client server communication. And, in implementing the secure rpc, they also decided to use private key cryptosystem, and I mentioned. That the public key crypto system does not have the key distribution problem, but on the other hand, for a closed community like a campus environment, the key distribution problem is not as big a challenge, so therefore, the design of the Andrew file system decided to use private key crypto system. And as I mentioned earlier in the private key crypto key system in order to identify what key to use to decrypt a message that is coming in, the identity of the sender has to be known. So this has to be sent in clear text. If you take a traditional operating system like Unix you have username and password which is the way you authenticate yourself for the system. But, in a campus environment if you're going to use, Secure RPC as a mechanism for communication between the client and the server. There can be lots of communication that is happening between individual clients and the servers. And if you're going to use, use a name and password all the time for such communication. Such overuse will result in a security hold. One of the things that I mentioned, as a principle proposed by Jerome Salser is you publish the design but protect the keys, but you publish the design but protect the keys. And what is meant by protecting the keys is making it competitionally hard to break the key but over exposing the key. Allows someone to take a long time to crack the key, and therefore overusing username and password as the basis for all communications, in a circular RPC, will pose a security hole. So the dilemma that the Andrew Fine system designers had to face, was what to use as the identity. That needs to be sent in cleartext, and what to use as the private key so that we don't always use any pair of identity and private keys for a long time. The solution that they took in Andrew is as follows. Only for logging into a workstation the user name and password are used. And of course, they have to be securely communicated to the server Over insecure links, we'll see how that is done. But the key point is that username and password is only useful logging in. Once a user is logged into a workstation, at that point onwards for the rest of the log-in session, the intent is to use ephemeral IDs and ephemeral keys But all the subsequent Venus to vize communication. Recall that Venus is the process that resides on the virtual workstation, acting as a surrogate for the user for file caching. And vize is, of course, the server that is living inside A secure environment. And this venus-vize communication happens over insecure links, that's where we going to use ephemeral ids and keys. So this gives rise to three classes of client-server interactions. The first interaction is logging in, imagine you're a student, and you're doing a course project. What is that you're going to do. You're going to walk up to a work station and log in presenting your username and password to the workstation. And this username and password is going to be used as the basis for, the client server communication that lets you authenticate yourself to the server. Now once you authenticate yourself to the server what are you going to do, well you want to do a cost project, and for the cost project you probably need to download some files to the server. And that requires that Venus that is running on your workstation establish an RPC session with the server in order to fetch the files that you need to work on for your course project. That's the second class of client-server communication, establishing an RPC session. Now once you establish an RPC session, then you can request the files that you want as a user. As a student you want certain files to be accessed during this RPC session. You bring those files and once you brought in the file that you want, you can close the RPC session, work on your project locally on the workstation. And once you're done with completing whatever work you needed to do, you may establish a new RPC session to upload the results,of your work back into the file server. So the three classes of client-server interactions are logging in, which happens exactly once for the entire loin session, and RPC session establishment, which may happen several times during the time that you're logged into work station. Every time you decide oh, I want to fetch a new file. Oh, I want to store this file back into the server. The third set of interaction is the actual file system access during the session. Once this RPC session has been established by venus with vize, then you as the user, you may want to work on a particular file You may open a file. At that point, Venus will go, using this RPC session, and fetch the required file and cache it locally for you to use it, and later on, if you close the file, at that point, Venus will commit the changes that you made to the file to the central file server again. So that's the kind of file system operation that's the third class of client server interaction. And for both RPC session establishment, and for the actual access to the file system during an RPC session, we want to use ephemeral ID's and keys. Let's first understand how the login process works? You walk up to a workstation and you login. And how do you login? Well you login using your username and password and this login process is special that runs on the virtue workstation. It, communicates with the login server that is inside the vice by presenting the username and password in a secure fashion. We'll see how that is done in a bit. So it presents the username and password securely over the insecure links. To the login server, and the login server, once it authenticates who you are, using the username and password, it is going to send you a pair of tokens, and remember all of this is happening on behalf of the user by This merge in the virtual workstation. The user has to do nothing special. All that the user is doing is logging in using a user name and password. Under the covers these things are happening. First, the log in process communicates securely with the log in server presenting the user name and password. And the login server then returns two tokens. One is called a secret token. The other is called a clear token. The clear token, is a data structure. Once again I have to mention that both the secret and clear tokens are communicated back To the log in process, by the log in server, in a secure manner over the insecure link. We'll see how that is done in a minute. So once, the log in process gets these two tokens, decrypts them and gets these two tokens, the log in process decrypts the message that contains these two tokens. And extracts the clear token and the secret token. The clear token is really a data structure which is known to the login process. And the data structure in particular contains a handshake key. We'll call it HKC. So from this clear token data structure, the login process can extract this handshake key And the secret token is just a bit string. So far as the login process is concerned. The weight is generated by the login server, is to encrypt this clear token. With a key known only to vice. It's not the same as HKC. It is a key that is known only to vice, to encrypt the clear token. And we will see how this key is going to be used, later on. So to recap, virtue sends securely the username and password to the login server. Login server securely sends secret token and clear token back to this login process. From the clear token The login process extracts the handshake key. And the secret token is basically a bitstream, which is an encryption of the contents of this cleared token, encrypted with a key that is known only to Vice. In other words, the secrettoken is unique for this login session, and It is a bit string, which means nothing to anybody that sees it on the wire. And therefore we can use this bit string that secrettoken represents as ephemeral client-id for this login session. Recall I said. We don't want to expose the user name and password Too often on the wire. This is Andrew's answer to dealing with a problem, by providing an ephemeral client ID for this login session. Once I use it as logged in, they get an ephemeral client ID, which is the secret token. And this can be used in the future communication between virtual And vice as the client ID. Now, how will vice know who is communicating with it when it sees this bit pattern? Remember that this bit pattern secret token is an an encryption of the clear token. And the key for decrypting it is know only to vice. So when the secret token comes as the client ID, vice can decrypt it and find out from that, what is the clear token associated with that particular bit stream which is representing the secret token. And once it knows that, it can also extract the handshake key that it gave to particular client, and that's how the identity of the client that is communicating with the vice in the future, presenting this secret token as the ephemeral client ID can be recognized by vice. So now, after the login is done, for all the future communication between Venus and Vice, HKC can be used as a private key for establishing a new RPC session. This pair of tokens, the secret token and clear token, is stored on the Virtue workstation by Venus. On behalf of this user for the entire login session. At the end of this login session, these two tokens will be thrown away by Venus. But during the login session, these two tokens are representing this particular that has logged in. And, so for the duration of this login session. Venus will use the secret token, which is a bit string that represents this particular user for this log in session, as the client ID to send information over to the vice. And, any information that Venus sends to vice to establish an RPC session is going to be. Encoded, with the private key that it had been handed now through the clear token data structure. That is the handshake key that had been given as part of this log in exchange, through the clear token data structure. At the core of the entire secure RPC system of. The Andrew file system is the bind mechanism, for setting up a client server connection securely, and that's what we're going to look at next. Now that the user had logged in to a client work station, Venus will establish an RPC session, on behalf of the client. And in order to establish the RPC session, the following message exchange is going to happen. And this is what is called the bind operation between the client and the server. So what Venus is going to do is send the client identity and an encrypted cipher. The client identity, as I mentioned earlier, in private keycryptal systems, you have to send it in clear text so that the server knows the identity of the client and in order to establish a new RPC session, Venus will use the secret token as the client identity. And it will use the HKC, the handshake key for the client that was contained in the clear token data structure that was given back to the work station by the server as part of executing the log in process. That is the key that will be used for encrypting the starting message for the RPC session establishment. Now what exactly is the massage that is being sent. Well all there is sent for initiating this RPC session establishment, is a random number which is new for each RPC session that Venus is establishing on behalf of this client. Every time it wants to establish a new RPC session within a login session, it creates a new random number and encrypts it using the handshake key. That's the cipher text. And the secret token is the client ID, and that's the message that goes from virtue to vice, the server. So when this message arrives at the server, how will the server decrypt this message? Well, the client ID is the secret token. Recall that the secret token is nothing but an encryption of the clear token, with a key that is known only to the server, and therefore, what the server can do is take the secret token, decrypt it using the key that it has and once it decrypts it, it gets this clear token data structure and from the clear token data structure, it can say well what is the key that is contained in this clear token data structure, take the key and decrypt this message. Cause, that key is HKC, and that is how the server can get the message that has been sent as the initiation of the RPC session. So in particular, what the server has gotten now is the random number that was sent to it by Venus Xr. Now what the server does, is it takes this random number, increments it by 1, and also adds a new random number, Yr, and that is a message that it is going to send back to the client. It has to encrypt this message of course, because its going on the wire, which is insecure. So it's going to encrypt it using a key, a handshake key which we'll call HKS and by design, HKC and HKS are exactly the same. So in other words, whatever is the handshake key that has been given to the client in the clear token, is the same handshake key that the server is going to use to communicate information back to the client as well. So, it encrypts this message with this handshake key and sends it over to the client. So what is the purpose of this message? When this message comes over to the client, the client can decrypt this message. How? It's going to use HKC which is the handshake key that it knows will be used by a genuine server to encode the message and send it to the client, and so it can use HKC to decrypt this message and once it decrypts this message, it gets these two numbers. There's Xr plus 1, that is the original random number that this guy sent over to the server incremented by 1. Now what is the purpose served by this number, Xr plus 1. It actually establishes to the client that the server is genuine. Anybody could replay a message that they see on the wire, but for this message to have the right content, what the client is expecting is that the response that the client is going to get for the original message that was initiating the RPC session. is that the response from the server is going to contain a number that is 1 greater than the random number that it sent the server in the first place. So, if this number, Xr plus 1, is what the client was expecting, then it establishes that the server is genuine. So this goes back to what I said about authentication of the server. That is one of the challenges that the [INAUDIBLE] system has. This is the way the virtue workstation, the Venus process on the virtue workstation on behalf of the client can establish the genuineness of the server that this workstation is talking to. Or in other words, it's not a Trojan Horse that is sending this message. But it is genuine server that is sending this message because this field is exactly what Venus expected it to be. Now what is this number? This is another random number that the server is sending as part of this message. Why is it sending this random number? It'll become clear in the next set of communication that's going on between Venus and Vice. Now once Venus has extracted this Yr from this message, what it is going to do is it is going to increment it by 1 and take that as a message, Yr plus 1, as a message, encrypt it using the handshake key, and send it over to this server. And when the server decrypts this message and extracts this field, it'll see whether this field is what it is expecting it to be. What the server is expecting is that this will be Yr plus 1. And if the client is genuine, then the client would've been able to extract Yr from this response that came from the server and generate Yr plus 1 and send it over to the server. So in other words, this message coming to the server is establishing the genuineness of the client, because this original message that we saw could have been just a replay, meaning somebody sniffed the network, got a hold of this packet, and replayed it. But if they replayed it, then they would not have the key, and they would not have been able to decrypt this message. So the fact that the client was able to decrypt this message, extract Yr increment it and send a new message that contains Yr plus 1 to the server is authenticating to the server that the client is genuine. Just as this message established to the client that the server is genuine, similarly, this message coming from the client to the server is establishing to the server that the client is genuine. And this pair of communication that you are seeing is also avoiding replay attack either from the client to the server or from the server to the client. In both directions, we're avoiding it by this trick of generating a new random number. For establishing an RPC session and communicating it back and forth between the client and server. Okay, now we know that the kind is genuine, and the server is genuine. Now what? Once again we go back to the principal of not overexposing any ID or password for too long on insecure links. Remember that the login session used the username and password exactly once. Now the RPC session, you may be establishing multiple RPC sessions. Over the lifetime of the login session. Once a login session has been established, then you are establishing the RPC session. Now within this RPC session, you may be making a number of File system calls. And for all of those file system calls, you don't want to over expose the use of this handshake key. And therefore what the server does in the Android file system is. Use this handshake key only for establishing an RPC session. Let's say within a login session, you have three or four different RPC sessions, with the server. Three or four unique RPC sessions. For each one of those unique RPC sessions, you have to use the handshake key to establish the RPC session. But, within an RPS session, what you're doing, is you're making a whole bunch of secure RPS calls, to the server, for opening files, achieving new files, writing files and so on, and for all of those we don't want to over use this hand shaky, and that's the reason, what the server does once it validates the indentity of the client,it generates a session key. Which is for this particular RPC session. Now it's going to send this new session key a generated this particular RPC session as an encrypted cipher to the client using the hand shaped key. As the private key for encrypting this message. And now Venus can extract the session key using HKC to decrypt this message and this sk is the session key for use for this particular RPC session. For the rest of this RPC session, any time the client wants to open a file or close a file or write a file or read a file, all of those file system operations is going to use session key as the handshake key for the rest of the RPC session with the server. The second field, num, is the starting sequence number that Venus will use on behalf of the client for all the file system related RPC calls that are going to be made as part of this particular session. For this session, SK, is going to be the handshake key, and the sequence numbers going to start here. There's again a safeguard against replay attacks on the server, by someone masquerading as a client, and generating packets with certain sequence numbers. So let's recap. What went over for session establishment. The first message coming from the client to the server tells the server that a client is wanting to establish a new RPC session. The server has to authenticate whether that particular session establishment request is genuine or not. And the client has to know that it is really talking to a genuine server, and that's this pair of communication that you're seeing here. One to establish to the client that the server is genuine, the second to establish to the server that the client is genuine. And once the genuineness of the client and the server has been established, the server says, for this particular RPC session, we will not use this handshake key anymore for the rest of the communication that we want to do for file system operations. I will generate a new session key, SK. Give it to you along with the sequence number to use as a starting sequence number for RPC calls during this session. That's the whole exchange that we're seeing here. Now it's time for a quiz. In the Andrew file system, the sequence, client sending a newly generated, random number, and the server responding with an increment of that random number back to the client. This exchange establishes one of two things. Either that the server is genuine or the client is genuine. The second question is the sequence, the server sending a newly-generated random number, Yr, and the client responding by taking the random number, incrementing it by 1, and giving it back to the server That establishes, that the server is genuine, or the client is genuine. This sequence establishes that the server is genuine, and this sequence establishes that the client is genuine. It turns out that login is a special case of the general bind operation that I described to you, and it is a special case in the following sense. Password that the user has for logging into the system at any point of time is used as the handshake key to start with. And username is used as a client ID. So, to initiate the login session, the login process, the username given to the user as the client ID, and the password as the HKC. But other than that, it follows the same sequence of validating itself to the server, and, similarly, validating the server to the client. And at the end of that validation of both the client and the server, what the server gives back to the login process, as I mentioned earlier, are a pair of tokens. The secret token, and the clear token. And these two tokens have to be sent securely on the wire, which is insecure. So what the server will do is encrypt these two using password as the handshake key. And therefore, the login process can use the password to decrypt the message that comes back from the login server, and get these two tokens, secret token and clear token. And these two tokens are kept by Venus for the rest of this login session. And as we know, once we've gotten these two tokens because clear token is a data structure that contains the handshake key needed by Venus for establishing RPC session, the rest of the life is made for us. Let's put it all together and see how Android file system enables authorized users who are members of a campus community to log in remotely from work station over insecure links and use private key crypto system to validate the users. And allow them to get useful work done on the work station. They login using the username and password. They get back a pair of tokens a secret token and clear token, and this is the first communication that happens between venus and vize. Venus can then establish an RPC session on behalf of the client by using the secret token and HKC. That's the second class of communication that happens between Venus and Vice, and as a result of this session establishment. What Venus gets back is a session key for use in this particular RPC session. Subsequently, any time the user opens a file, or closes a file, or writes to a file, all those file system calls. That requires Venus to communicate with vice, is going to be sent as secure RPC using the secret token as a client ID and SK, the session key, as the private key for encrypting the message that it wants to send. To vice, and similarly vice is going to send back responses uses the session key. So this is a third class of client server interaction that I mentioned. So there are three classes of client server interaction. The first one is login, second one is RPC session establishment, and the third one is the actual. Secure RPC calls that are being made for manipulating the files sytem that resides in the central server. So the upshot of this structure that Andrew Files System provides for the user community is that the user name and password is exposed only once per login session. So in other words if a student logs in to a work station once or twice a day, it's only that many number of times that the user name and passwords are going to be used on this insecure network. The hand shake key that Venus gets back as a result of setting up a login session for a user is used only for new RPC sessions. That's a second class of communication. And the validity of this hand shake key is the duration of the login session. You know that passwords have a long ability deed as valid as long as you don't change your password but, the duration of this HKC that is used for RPC session establishment, that is only for the duration of the login session. Once the user has logged off and left this work station. A secret token and a clear token are thrown away by Venus, so the duration of the HKC is only for this login session. And a session key is used for all the RPC calls that Venus is making on behalf of this client from manipulating the file system. And the duration of SK is the duration of a given RPC session. For instance, within one log in session, let's say, I have three different RPC sessions. For each of those distinct RPC sessions, I'm going to get a unique session key. And I'm going to use that session key only for that particular RPC session. Once that is done, I cannot re-use that key anymore. I'll have to re-establish a new RPC session, and get a new session key, and restart the RPC session. So in this lesson, module as a whole. First, we looked at taxonomies proposed by Solser, and then, we saw how we can build a practical system, which can be used by a campus community for securely accessing information. In this case. Files stored in a central repository through the Andrew file system. Now, it is time for a question that is really asking you to think about the Andrew file system and its strengths and weaknesses with respect to providing security guarantees for the user community. So I want you to fill out this report card, which says, how well is AFS doing as a secure system. And I want you to think about some of the challenges that a secure system has to face. The first one is mutual suspicion. Meaning can I trust the server. Can the server trust the client? Can I trust my fellow users in the community? Is that challenge addressed by AFS? I want you to give a binary answer, yes or no. The second attribute I want you to evaluate the Andrew file system on is whether the users are protected from the system, or in other words, can the system do something bad to the users? So that's the second attribute I want you to evaluate. Once again, a binary answer, yes or no. And the third attribute is confinement of resource usage. In other words, does the system ensure that no particular user can overuse resources available in the system. The resources could be network bandwidth and so on. Again, binary answer. The fourth thing that I want you to evaluate the AFS system is on authentication. Whether it provides authentication in both ways. Both authenticating the server to the client and the client to the server. The last attribute is the integrity of the server itself, meaning can the server be compromised in any fashion. That is the last attribute I want you to evaluate. Once again binary answer, yes or no. AFS passes the first test. It does address the issue of mutual suspicion from fellow users, as well as from the server. Unfortunately, AFS fails the second test and that is protecting the user from the system. There's no way to protect the user from the system because the user has to trust the system. Everything is stored on the system, so in that sense there is no protection for the user from the system itself. It also fails on the third attribute, confinement of resource usage. For instance, a given user can make a lot of calls on the server. In other words, they could consume a lot of network bandwidth. There's no way to confine resource usage in the Andrew File System. And this can also be the basis for Denial Of Service attacks. If one malicious user or an errant program that is running on the virtual workstation starts generating a whole bunch of network packets, genuine network packets, directed at the server, it can inundate the network and prevent others from getting useful work done. So, in other words, confinement of resource usage is not something that exists in the AFS system. It passes the authentication test. We can validate in both directions, the client and the server, so the answer is yes on that one. The last attribute is the integrity of the server, remember that in the Andrew system the assumption is that the clients running on the virtual work station, they communicate to vice using insecure links. On the other hand, the servers themselves are inside, in some sense, a firewall and therefore they don't need any secure communication. So their links that are inside the server network is assumed to be secure and therefore there is no authentication or encryption inside the server network. Therefore, if somebody were to somehow get into that firewall they can create havoc. So the only way you can give server integrity in the Andrew File System is by physically enforcing who can get into those walls of the server. So physical and social mores are the only way to secure the integrity of the servers themselves. So I'm going to give it not a passing mark on server integrity, because by design, the servers are assumed to be in a secure environment and that can be one of the biggest sources of vulnerabilities. And your file system has features that extend the privilege levels provided by the unique file system symantics. This includes groups and sub groups and. Accesses for files with both positive and negative rights. Negative rights is especially useful for quick revocation. Further, it also introduces the notion of audit chain for system administrators modifying the system. I urge you to read this paper in its entirety. To get a feel for the extensions proposed in the Android file system. The main takeaway, however, in this lesson is how, as operating system designers, we can take the best solution that is out there for information security and implement a secure and usable distributed system. Further, once designed and implemented, we should be able to bench mark our solution against the design principles laid out by Salser on information security, and know the vulnerabilites that exist in our solution. This last point is particularly important to safeguard our system against attackers. As we mentioned at the outset of this course module, the topic of information security is fascinating, and it is of high relevance in this information age. Hopefully this course module has spurred your interest. In information security, to seek, more, and future courses.