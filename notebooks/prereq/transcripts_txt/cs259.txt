My name is Andreas Zeller. I'm a researcher at Saarland University in Germany. And I am researching large programs and why they fail. I have done some fail work in automatic debugging, also in mining the histories of programs. I've been working with companies such as Microsoft, SAP, or Google, examined their bugs, finding out what was wrong, and it struck me that there's almost no teaching material available on debugging and how to debug programs. So today, I'm going start with you a course on how to do debugging systematically, effectively, and in many cases, even automatically--enjoy. Welcome to the Udacity course on debugging. The aim of this course is to teach a systematic approach to debugging and we're even going to explore a number of automatic tools that do the debugging for you. We're going to explore how debuggers work. In particular, the scientific method of debugging by which through a series of experiments we gradually refine a hypothesis until we end up with a diagnosis on why the program failed. On top of that, we're going to build our own interactive debugger in Python. In the next unit, I'm going to introduce you to one of the most powerful debugging tools ever invented, that is assertions. Assertions are statements in the program that automatically check whether the state of the program is still correct. That is, while you're program is executing, the computer constantly monitors the program on whether a bug has occurred. This allows you to very quickly and effectively find out where a bug was first introduced. On top of that, we're going to build a tool that makes you infer assertions from executions. In unit 3, I'm going to show you a technique named delta debugging which automatically simplifies problems. For instance, here's this 900-line HTML file which causes to crash in a program which processes it. With delta debugging, you can reduce this to just the eight characters that produce the bug just as well and all of this automatically. In the next unit, I'm going to show you how to find out where a specific failure came from. You see an execution as a series of states. We are going to explore techniques that help you in tracking the way of an error all through the program execution. And on top of that, we're going to build a tool that isolates such cause effect chamge automatically. In unit 5, we've been looking at reproducing failures. We're going to look at all the various input sources for your program and discuss how to capture and replay them such that you can faithfully reproduce a failure that happens in the field. Plus, we're going to explore statistical debugging which collects data from the field to tell you which parts of your program are most likely to be related to the failure. In unit 6, we're going to see how to mine information from bug data bases and change data bases in order to figure out where bugs have been in your program in the past, where they accumulate, and which parts of your program therefore are going to be the most back prone in the future. And again this is a fully automatic technique. This is so far, you've had no fun in debugging because it just sucks the life out of you. The aim of this course is to get most of the debugging effort off your shoulders because you can have the computer take care of most of the debugging work. So your mind is free for doing something more creative than debugging. So, here you are, you are sitting in front of your interactive debugger, stepping for the program, you're looking at the variables, stepping, and stepping, and stepping. Setting up breakpoints trying to figure out what's going on and no time asked. You sit in front of your screen. You dig through the code step by step by step by step. And you watch individual variables appear and disappear. Time passes. It's late, so you order a pizza. Maybe a cup of coffee. You keep on watching the screen and some more coffee and just keep on stepping and stepping to your program as time goes by. It's pretty late already and you keep on concentrating, stepping, stepping, stepping. You are so close now. That's when the phone ring. Ring. Ring. Ring. Who's on the phone? Who's on the phone? It's a significant other asking you, "When on earth are you planning to come home?" You are in a lose-lose situation. You either lose your concentration or you lose your significant other. You have 30 seconds left until you lose all the concentration you have so carefully build up. You have 30 seconds to decide what to do. I know a fair amount of programmers who when faced with such a problem have taken the only right decision. And I think that debugging is the number one divorce ground for programmers. So, here comes the quiz. What can you do to avoid such as a situation? Should you drink more coffee, give up smoking, don't take phone calls, don't write bugs in the first place, get a better debugger, or take the Udacity course on debugging? Hint. The correct answer is marked with a circle. The problem with debugging being time consuming and all actually translates into money and effort on a large scale. A commonly cited figure in the literature is that in any software project, at least 50% of the effort is spent on test and debugging. This number can even go up to 75%. According to study made in 2002, software bugs are costing the US economy only $59.5 billion a year and improvements in testing and debugging could reduce this cost of software bugs by a $30 billion or $22 billion a year. The main problem with debugging, however, is not that it takes time. The worst thing that it is a such process whose length is unpredictable. It can take anything between a few minutes, a few hours, and sometimes even days and weeks. Even if you don't know how much time it's going to take, be sure to use the systematic process, which gradually gets you towards the cause of the problem, but even if you never know how much time a bug will take, it's a bit of a blessing to use a process which gradually gets you towards its cause. Here are the bugs. Where does the term "bug" come from? There's a story of a Harvard Mark II machine in which on September 9, 1947, a moth got stuck in the relay. The moth got carbonized and then caused a short circuit in the machine, which caused the machine to break. Technicians retrieved the moth from the relay and this was then recorded as the first bug actually being found in a computer. The bug is now on display at the Smithsonian in Washington. The term "bug" as you can see was already known in 1947, and it's actually much older than that. You can even trace it back to Shakespeare who used the term in order to describe some form of spector sitting on the chest of people in the night and causing them nightmares. Welcome to the first unit on how debuggers work. In this unit, I'm going to introduce you to the scientific method of debugging, which is a process by which through a systematic experiment, you'll slowly get guided towards the cause of the problem. We're also going to see why talking to a teddy bear can be an effective method of finding out what's going wrong, and in the end, we're going to build an interactive debugger. Enjoy. Every debugging session starts with the program that failed in which we need to debug. For this purpose, let's build a simple program that we're going to use as an ongoing example. The idea here is to write a function that takes an HTML input, that is text together with HTML markup and returns just the text. For instance, if the input is HTML markup for bold followed by text foo followed by HTML markup for end of bold, then we want to return just the text. Anything that is within this angle brackets should be stripped away. How do we deal with this? A simple way is to process the HTML input character by character and distinguish to modes. When we are in tag mode, we ignore all input. When we are in non-tag mode, we add all input to the output and we switch between these modes, tag and non-tag, by looking at these angle brackets in here. When we see the beginning of an HTML markup as you can see in this less than sign, we enter tag mode. When we see the end of an HTML markup that is a greater sign, we exit tag mode. We can describe the behavior of our function, that's a finite state machine with two states non-tag mode that is the processing text and tag mode that is we're ignoring HTML markup. When we see a less than sign, we go into tag mode. When we see a greater than sign, we exit tag mode. For all other inputs, we stay in the same state. When we are in non-tag mode and see any character that's not the beginning of an HTML markup, we add this character to the output. Whereas in tag mode, we simply ignore the HTML markup that we processed in here. So when we are processing this HTML input, initially we're in non-tag mode. Now, we see the beginning of an HTML markup, we go into tag mode. We see the B, which is not the end of a tag then we get the end of the tag, go back into non-tag mode, see the F add this, see the O add this, see the O, add this again. So our output now is foo and now again we see the beginning of a tag where the process or more specifically where we ignore all characters up to the closing tag and then our output is indeed the text inside the HTML markup. If you've taken a Udacity class before, you've probably seen the Udacity IDE--well, IDE. It's a webpage where you can enter arbitrary Python functions and Python commands and execute them. For instance, we can have it say hello for some wrong and then we get the output hello. Let us now write a function which implements the finite state machine we just seen in order to remove HTML markup. We have the tag variable, which tells us which state we're in, and we have an out variable, which tells us what the output would be. Here's an implementation of the finite state machine. If we see the beginning of an HTML tag, we set the tag variable to two, so we go into tag mode. If we see the end of HTML markup, we set tag to false, so we exit tag mode. Otherwise, for all other characters, we add them to the output unless we're in tag mode. And finally, we return the output. Here's the input we just saw. We have a bold HTML tag, we have a foo as text, and we have an end-of-bold HTML markup in here. If all works well, then we should get just foo as the output and all the HTML markup should be removed. So we take on run and we get foo as the output--that is I removed HTML markup function, has properly stripped the bold and unbold markup from the input. And now for a quiz--which of these inputs is not properly stripped of HTML marker-- is it foo, is it a link to a page named foo.html followed by the text foo and the end of the link, is it an empty link, or is it a link to a page named greater than. Check all that apply. If you remember our program works going through the individual states of the finite state machine, you will find that this input is not processed correctly. What we have in here in our input is the character that can also be mistaken for the end of tag in HTML markup. A program does not know about the special meaning of double quotes, so what it does is-- it starts interpreting everything as HTML but only up to the closing tag. The double quote in here is interpreted as text input. The greater than sign is ignored even in a non-tag mode. The text is doing just fine. And the remaining HTML markup is ignored. So what we should get as output is double quote foo and you can see that the output still contains part of the original HTML markup. So this is the correct answer. The others all work fine. Let me demonstrate this in the IDE. Since I'm now having double quotes in my script, I use single quotes in Python, which I can also use to delimit a string. Rule of thumb--if your string contains double quotes, use single quotes as string delimiters. If your string contains single quotes, use double quotes as delimiters. If there's no quotes in your string, well feel free whatever you like. Here's our input with greater than and double quotes, we run the whole thing, and we see that the output indeed contains part of the HTML markup that is the double quote is still in there. The problem we're having here is that we're not taking care of the quotes in the HTML markup. Actually, anything that's within these quotes in HTML markup should not end HTML markup. In particular, not the greater sign in here, so we need to extend a program appropriately. The idea was as follows--rather than having two states, one for regular text and one for HTML markup, we're going to have a third state, which would handle anything that's within quotes. Just as before, we start in a non-tag mode and when we see a less than sign, we go into tag mode. This is what we do when we possess this very input and we stay in this mode until we find a quote-- that's when we go in quote mode and we stay in quote mode until we find another quote. That's when we go back into tag mode, which happens right here. When we see the greater sign, we exit tag mode and we possess the individual characters, and we add them to the output. Thus our output should now become foo as expected. The remaining tag is processed just as before. In order to implement this, we're going to use two variables-- one variable tag and one variable quote to indicate the three different states a program can be. Let's go and extend the Python code appropriately such that we now have three states instead of just two. So we're introducing a variable named quote, which initially is false, because in the beginning they were non-tag mode. And now, we simply check for double quote or a single quote, and if we see any of these, the codes variable gets inverted. And now, if quote is set, then the HTML markup characters should have no effect. So we make sure that the HTML markup characters only have effect if quote is set. Finally, we need to make sure that quotes only have a special meaning within tags, so we make this endtag instead. We still have our example down here, which previously failed, because of output quote foo. Let us see where this output now is properly handled and we click on run. The output is foo. All the HTML markup--you can read this complex combination of quotes and end of markup characters is now properly removed--great. And now for a quiz. There is some input which still produces output with HTML markup. So we still had the bug in this code. Is this foo in bold HTML markup, is it same as above but with foo in double quotes, or is it the same as the first but with the quotes outside of the text, or same as the first one but with the HTML tag names enclosed in double quotes. Try it out for yourself. Which of these still includes HTML markup in the output of remove HTML markup. And now for the answer. This is something that we can best find out by testing. Here's our first input, here comes the second, number 3 with the quotes outside, and number 4 with the quotes inside of the text. For the first input, the output should be just the text foo. For the second one, it should be the same but enclosed in double quotes as is the same for the third one. For the fourth one, we'd simply expect the text foo. Let's go and test this. Surprise, surprise. The output is very different from what we expected. To start with, the third output still has HTML markup, but for the second input has also something fishy going on because the quotes that were actually part of the text have been removed. In our quiz, we only cared for the HTML markup, so the third input definitely produces HTML markup. And therefore, this is the correct answer. At this point our program is supposed to be complete, but it still has a bug. If we put in foo in bold, in quotes as input, what would we expect is an output of foo? However, what we get is a completely different output. What we get is an output where the quotes are stripped but the HTML tags are still there and obviously, the output does not match what we actually expect it--we do have a bug in here. When I was a student, I never got any formal training in debugging. I had to figure out how to do the debugging myself. What I learned was how to use debugging output. That is I would go and scatter output statements everywhere in my program. In Python, this would be the infamous print statement. So here again, here is our program and here's the one single input that goes wrong. If you press on run, what we get is the quotes removed but the HTML markup is still there. When I was a student, I never got any formal training in debugging, so I had now to figure out myself what went wrong. The only thing I knew was how to use debugging output, so I would use the print statement in Python in order to figure out what had gone wrong. Essentially, I would go and scatter print statements everywhere. For instance, I can go and print all the local variables in here. Cool. So I know what the character is, I know what the status of tag and quote is. Now, we'll click on run--oh yeah, and here comes my big output. Now, I can scroll down here and say, "Oh yeah, sure, sure, sure." Obviously, of course now you can immediately see what's going on in here. No, I'm afraid I can't because this is just a long, long list. Think of a 1000 character input at this time. If you have a 1000 character input, you didn't have to go through 3000 lines of logs. This may help you but it's a total time waster. You have to enter these statements, use them for debugging and then remove them again. It is a total maintenance nightmare. Debugging statements left in the code may even come up with a security problem. In Mac OS versions 10.7.2/10.7.3, there was a security issue because a programmer had left debugging print statements in the code. This would result in the following situation--you as a user would enter your password into the Mac the Mac would then let you in or not let you in but at the same time would store your password in the clear in, a log that would be visible for anyone-- anyone with access to the machine of course, and this again would require a password. On a multi-user machine, for instance, or somebody having physical access to a hard drive, a mean checker could not go and retrieve your password in the clear, and that's simply because of some left debugging statement in there. Not only would I start using arbitrary debugging statements everywhere, I would also try to debug the program into existence. Just change things until they work. Let me try. The error of this may have something to do with quotes. So don't just simply go and remove all these extra quote checks in here. Let me see whether this makes any difference. Okay. I removed all the quote checks. Let's see whether it works right now, and we run the program and see--it's foo. The output is foo. It's almost correct. Only the quotes are missing. Well, now we can handle the quote--maybe we will just remove this in order to align in here, and may be we can change things until things are proper. Let me see whether this works. Same. Oh yeah, great. Let me see. Okay. And I can run the thing again and--however, I think I removed too much at this point because if you recall our original example--the one with the greater than sign here in the target URL that we now still get this error in here, the quote is in here which should belong here, which is why we introduce quote handling in the first place. So now actually, maybe I should go back to this earlier version, but how do I get back to the earlier version. Well, of course, I did never backup my earlier version. So I don't even know what the earlier version was. This more or less was my state of debugging. This is how I debug as a student, which was not the perfect way to do it. All of these strategies come from a chapter named "The Devil's Guide To Debugging," which was written by a guy named Steve McConnell. The book is being called Code Complete in 1993 and it encompass all the wisdom about programming that Steve McConnell had at that time. What we've seen so far is just three of these rules. The first one is to scatter output statements everywhere in the code. Output whatever you want, where you want it, just put it in there such that you will always be able to figure out what was going on or not. The second one is to debug the program into existence. Just keep on adding statements and removing statements until it works. Never ever backup earlier versions of your code. I mean who can't remember what he or she has done just 7 minutes ago. Rule number 4, don't bother understanding what the program should do. I mean, it's obvious isn't it. If it's not obvious, make it obvious. My all time favorite though is number 5, use the most obvious fix. That means fix the symptom instead of the problem. This technique of using the most obvious fix, so beautiful--I'd really like to show this to you. So here we have a complete program again. I managed to restore each from previous backup. And again, if we run this, you'll see that the HTML markup is not removed. So the technique of using the most obvious fix is very simple. We simply check for the input that's wrong and return the correct output. Lo and behold, this technique beautifully works and returns the correct output. Rumor has it that some programmers use this very technique to make their unit test work. As with any devil's guide, you should of course not do what's in here, but you should do the exact opposite. This means you should go and fix the problem not the symptom, understand what your program should do, and finally to proceed systematically. Let me come up with a quiz here--before you apply fix, what is it you should do-- understand what the problem is, understand what the code should do, or/and be able to predict how the fix would address the problem. Over to you. Indeed, the answer is that you should understand what the problem is, because if you don't understand what the problem is, how would you be able to fix it. Understand what the code should do because again if you don't know what the code should do, you won't know how to fix it. And finally, also apply a fix that addresses the problem and not only the symptom but the actual cause. Now let's go and proceed a bit systematically. The situation we are in is classical. We do have a program and the program gets some input and out pops a failure that can be seen by the user. What is a failure in here? A failure is an error. An error that's externally visible that is visible by the user. What is an error? An error is something that deviates from what's correct, right, or truth. Errors are typically unwanted and unintended. Failures are error. A defect is also an error. A defect, however, is somewhere in the code. The defect is what turns otherwise valid input in the end into a failure. The term defect is an error in the code as in several synonyms. Some people also called this a fault but then a fault can also be applied to data. The by far most common word for defect, however, is of course, bug. There is a bug in my code somewhere as if this had somehow crawled in into my code. Here is this foreign but, this is my code, it doesn't belong here. That's okay. In the end, it's us programmers who are actually introducing these bugs in there and so are very much prefer the term defect. The aim of debugging is to find the defect that causes the problem. For this, I want you to look a little bit deeper into how a program actually executes. We can see a program as a succession of program states. Each program states consists of several variables with values. As a program executes, it processes these states and transforms them into a new states. For instance, by reading variables and writing variables. This is the normal mode of operation. Now, however, since in the beginning, we have a normal input and in the end we have a failure, there must be a defect somewhere in our program that actually causes the problem. So let me assume that this statement we're executing here actually has a defect. What happens is that now, when executed, it introduces an error in the program state which we call an infection. This infection is now being propagated possibly to other state and eventually becomes visible as a failure towards the user. What we get in here is actually an entire cause-effect chain. You see, these failures, which is an infection, is caused by earlier infections and if we are at a state where the infection has no further origin, that is, the input state is the same, and the output is infected, and we know the statement that was executed at this precise moment which caused this transition from the same state to infected state, this is the statement which caused the infection, this is the statement which has the defect. When we're debugging now, we need to identify this cause-effect chain not only do we need to identify but we also need to break the cause-effect chain. If we can break this cause-effect chain from defect to failure, then we're done with debugging. So all of this looks very simple, however, in real life, it's much more complicated than that. To start with, not every defect automatically causes a failure. It may well be that the defect causes an infection which later simply is not propagated as a real life infection just as well. So the infection is not propagated and never ever becomes visible to the user. It may not even cause a failure at all or the statement with the defect may not even be executed or only under very specific circumstances may actually cause an infection and later a failure. This is the problem of testing. You can execute a program again and again, never have a failure and still have a defect in there, however, if a program fails, that is if we actually see a failure, then we can always trace it back to the defect that causes it. So if there's a failure, we can always fix it by following back the cause-effect chain. But then the next problem is: these states are huge. So over here we have 1, 2, 3, 4, ... 12 variables. Cute. In reality, we have 10,000 of such variables and not only do we have 10,000 of such variables, we also have 10,000 of steps between defect and failure. So tracing back the cause-effect chain can be much, much more complicated that it is in this simple picture. The longer the cause-effect chain, that is the longer the time we have to cover, the more states we have to cover, the harder is to debug it. And also, the larger the state, the more we have to search for an infection. Again, this makes debugging harder and harder. It's like finding a needle in a haystack except that the haystack sometimes is larger than any haystack you'll ever find on earth. With that in mind, let's come to a quiz. Which of the following is not true--every infection can be traced back to a defect that causes it, every execution of a defect causes an infection, every infection ends in a failure, and finally, every defect can cause a failure--up to you. Here's the answer to our tricky word quiz. Let's go through these answers one by one. First, every infection can be traced back to defect that causes it. This is actually correct because if we have an infection if we have an error in the program state that is, we can find out which defect causes it namely the piece in the program which got the same state as input and which produced an erroneous state of infection as output. Second, every execution of a defect causes an infection. That's actually not the case. It may will be that there's a defect in the code, which operates only under certain circumstances, which works fine most of the time, and only under certain circumstances produces an infection. So this is not correct. Every infection ends in a failure. If only it were so, then it would be far easier to make a program defect free. Note, like real infections, they can die out before they ever, ever cause real harm. Finally, every defect can cause a failure. How? That's tricky. Remember a defect is an error in the code, failure is an error in the execution and infection is an error in the state. A defect is an error in the code and you can have an error in the code even in code that's never executed or in code that's actually unreachable. How can be it be an error if it can never be executed. Well, it may well be that over time when the program is maintained, the error actually becomes executable and then it can actually cause a failure. It is not the case that every defect always can cause a failure because defects cannot be an unreachable code and therefore, they may not cause a failure. So this is not correct just as well. Now that we know how failure is going to be, let's look into how to systematically find our causes. What we need is a way to organize the debugging process and for this, we use the process called the scientific method. That's a cool name isn't it. It's actually a big name for little thing but this little thing again has big consequences. What the scientific method does is provide a systematic process by turning some aspect of reality into a theory that explains how this aspect of reality works, and that makes exact predictions on this reality. Let me come up with a simple example of how the scientific methods works. Suppose your a human--say, Isaac Newton, your laying in your orchard and all of a sudden your watching an apple fall down. So now you wonder, well I've just seen this apple falling. This is an aspect of reality. Why do apples fall down? What the scientific method does is provides you a way to come up with the most general prediction method available. So you can think that if an apple falls and you can repeat this experiment over and over by letting an apple fall, you may also think about well do bricks fall. Here's a brick and lo and behold, it falls. Do plates fall? Oh yes they do. Do cups fall down too? Oh yes they do. Put yourself in the mind of a 3 year old and you will find that everything falls down, and everything really means everything. So I can easily generalized that everything falls down. Unless you happen to see a say--a balloon. Here's the balloon and the lo and behold the balloon goes up. So not everything falls down, but you can easily extend your theory to say--well a balloon is larger than air. If the air falls down, then the balloon must go up. So still everything falls down. What does down actually means? Down means toward the center of the earth. Everything is attracted towards the center of the earth. Well, the sun is not attracted to the center of the earth. It's rather the other way around. So you see that from these observations on even experiments that you can conduct, you will eventually be able to come up with a full pledge theory that explains where things will be moving, that explains at which velocity they will be moving, which explains where the acceleration will be. In the case of Isaac Newton, then became the theory of gravitation. In the beginning, you do have some initial observation. >From this initial observation, we derive a hypothesis. A possible explanation for what you just observed. If the hypothesis explains your observation, the hypothesis must be useful to make a prediction. In our example, the initial observation was the apple falling. The hypothesis would be, well every solid objects falls down and then a prediction would be, if I take this plate and let it fall, it's going to go straight down towards the floor. It may also break along the way. This is what 3 years old do. The experiment means to actually verify the prediction so we take the plate and let it fall. >From the experiment comes the observation. What do you see? Well, the plate actually was attracted towards the ground and so you can now go and check whether what you just observed in your experiment is as predicted and therefore satisfy the hypothesis and we say it supports the hypothesis and we then can refine the hypothesis. Our case line says, we can generalized it towards other objects and to come up with a more general hypothesis. If the observation, however, is not in line with your prediction, then the hypothesis is rejected, and if the hypothesis is rejected, you need to come up with a new hypothesis, and this cycle going along from hypothesis through prediction through experiment through observation and gradually refining the hypothesis or coming up with alternatives. This is something you repeat again and again until your hypothesis becomes consistent with all observations and has so much predictive power that it becomes what one calls a theory. In a theory you repeat the process of refining the hypothesis through prediction, experiment, observation or creating new hypothesis--again going through the circle again and again and again until your hypothesis becomes a theory that is a predictive and comprehensive description of some aspect of reality. What is a theory--is a theory a vague guess, is it a framework that explains and predicts observation, is it a particularly useful hypothesis, or is it the outcome of the scientific method. Check all that apply. Now, for the answers. A theory is the outcome of the scientific method. A theory is also particularly useful hypothesis because we refined the hypothesis until it becomes a theory. And it also is a framework that explains and predicts observations. This is the official definition of theory. A theory, however, is not a vague guess when you're talking about the scientific meaning of this theory When you have scientist speaking about theory, they're talking about the framework that is consistent with all earlier observation and predicts lots and lots of future observations and its actually the best framework in most cases When you have layman speaking about theory, it's more like I have this theory on how things could be which does not necessarily have the same strength as scientific theory. The misunderstanding between theory in the sense of a layman using it and with the theory as scientist use it is the cause for lots of confusion in political debates. Think about the theory of evolution. For scientist, the theory of evolution is exactly exponential and prediction of lots and lots of observations. A very consistent and form many beautiful framework. For others, oh this is just a theory of evolution disseminating this as vague guess. Be careful when you heard the word theory. Its meaning depends very much on whoever says it. In our context, of course, we're using a theory in order to explain what the cause of a failure is and in our context we're not looking for vague guesses, we're looking for something that explains all earlier observations and predicts the future one. When we are debugging, we proceed the very same way. Indeed, we're treating bugs as if they were natural phenomenon. So, our hypothesis would be grounded on the initial observation of the failure and possible also all the knowledge we do have about our program. The failing run, more runs, the original failure, program code-- all of these goes into our forming a hypothesis. At the end of the process, we get a consistent description of how the failure came to be. We don't call this a theory. Theory is a bit far fetched for a simple failure. Instead, we call this a diagnosis of the original failure. It may sound a bit far-fetched to apply a method that has been devised for studying natural phenomena and for coming up with theories about natural phenomena on something as artificial as bugs. However, errors and nature have something in common, none of them are under our control. And the scientific method is precisely the method which you need for explaining something that is not under our control. Let's now apply the scientific method to a Python program which removes HTML markup. First of all, let us write down again what we have observed so far, what we expected, and what the output was. If our input was foo in HTML markup, then we would expect the output foo, and the actual output was foo, so this is just fine. If our input was the same thing in double quotes, we would expect the double quotes also to appear in the output, but instead we get the HTML markup still included in the output that is the whole thing fails because of all the observations we made and from this, we need to come up with the first hypothesis on what makes the error. So, here a quiz. Which hypotheses are consistent with our observations so far? Check all that apply. Is it that double quotes are stripped from tagged input, is it that tags in double quotes are not stripped, is it that the tag for bold is always stripped from the input, or is it that four-letter word are stripped. Let's check our individual hypothesis. So double quotes are always stripped from tag input. Well, here's double quotes. They're stripped. This is correct. Next, tags in double quotes are not stripped. Well, if we have tags over here, they are stripped. If they are in double quotes, they are not stripped. So this is correct as well. The tag bold is always stripped from the input. Not the case. Let's see over here. So not correct. And four-letter words are stripped--well, actually we don't know this because we don't have a four-letter word in here, but at this point, there's no reason to believe that four-letter words would be stripped. You can still try this out yourself. What we have now is two hypothesis which are consistent with our observations so far. These maybe two separate issues, but chances are that they are actually tied to each other. Let's focus on the first hypothesis because it's simple. If this is our hypothesis, we now must devise an experiment to further refine this hypothesis. Let's come up with a very simple input where we'd assume that this holds. If we put in just "foo" without any tags, we would assume that we would get the very same output. Let's do this as an experiment. Here again, we have a buggy function and now let's conduct the experiment. So we invoke print-html-markup with foo enclosing double quotes. We press run and the output is foo, without double quote. So, now we see the output It's not what we'd expected, the output being foo without quotes and this again confirm our hypothesis. We can try this out with even more strings, further strengthening our hypothesis. So if we put in bar, a very surprising thing is going to happen. We get bar as output and if we put in just a two quotes, what we get is an empty string, so we get more failures down the way. This hypothesis is confirmed now by a number of experiments, double quotes are stripped from tagged input, but actually we don't even have a new tag so we can actually scratch that. Double quotes are stripped from general input. So even if there's no tags, quotes are being stripped away. Now we can actually go and explore the cause-effect chain. If quotes have been stripped away, there must be a place in the code which does that. The only place in our code where quotes are handled, is right here in this line. Here we have: if there's a double quote or a single quote and tag is being set then quote be inverted and when quote is inverted, tags should not be recognized. Nothing too complicate, but why would the quotes be stripped? Because normally we should not be in tag mode. Maybe we are in tag mode, maybe the variable tag is set. So we can come up with a new hypothesis that explains precisely that. The error is due to the variable tag being set. How do we know that this variable is being set? Let me introduce you to one of the most powerful debugging tools ever invented which is the assert statement. The statement assert followed by a condition, evaluates the condition and aborts the execution raising an exception if condition is false--that is if the condition holds, we proceed as usual. If the condition does not hold, we throw an exception. With the statement, we can now go and check the value of tag all through the loop. So again we say, in our hypothesis tag is being set and we use assert to check that. With this statement assert not tag should tag ever be set will we immediately get an exception and again we can check this with foo enclosed in double quotes. So in order to confirm the hypothesis, we would expect an assert exception. What is the output really? Let's make this a quiz. Now that we change the program to include assert not tag, what's going to happen? Does the program raise an exception or is the case that the output is still foo as before. The assertion is not violated and tag is not set during the entire loop. Over to you. Here's our program, the assertion included, with our input as before and if tag should be set somewhere during the execution of this program, then the assertion should fail. So we press on run and we see--oh, the output is still foo. The assertion has not failed and this shows that during the execution, tag has never been set to foo. So the correct answer is the output is still foo. Since we do not get an exception, we can now reject our hypothesis. So we know that tag is not being set. Tag is always false. So let's go back to our code. Here's the only place where quotes are handled. Tag is always false. This condition: if we see a quote and tag, then only we should go into quote mode, it should actually never hold. But maybe there's something wrong with this condition. So let's come up with the new hypothesis. A new hypothesis is the error is due to the quote condition evaluating to true. Let's come up with an experiment to verify this hypothesis. If this condition evaluates to true, then the next line should be executed. So what we do is, we simply write an assert false in here, meaning that the assertion should now automatically fail, if this ever gets executed. So this piece of code should actually never be reached when executing the program. Our expectation this time is, with this input, this assertion should fail. Again, we make this quiz: after we inserted assert false, does the program now raise an exception meaning that the quote condition holds, and therefore, something's wrong with that condition or is the output still foo, that is the input with the quote strip, and then the quote condition obviously does not hold because the assert false is never reached? Try this out for yourself. Here comes the answer--as you probably found out for yourself, the program indeed raises an exception that is the quote condition holds, and this confirms our hypothesis. So at this point, let us recall what we have seen. The error is due to the quote condition evaluating to true. We've seen that and confirm the experiments. Before, we already had seen that, the error is due to tag being set, was rejected, that is tag is not being set. Our initial hypothesis and our initial observation, double quotes are stripped from the input, so we know at this point double quotes are stripped, tag is not being set, and the quote condition evaluates to true. Let's take a look again at our example. We see that the condition in here not only handles double quotes but also single quotes. Question is, is there a difference in how the condition handles double quotes and single quotes? We still have this hypothesis characterizing the error. Let's see whether it generalizes to arbitrary quotes, that is general quotes are stripped from the input, whether they'd be double or single. In order to check this hypothesis, we now use an input foo in single quote and we expect the output to have single quotes as well in the output as what's happening in here. Here again we have our program. We now need to remove these assertions in order to restore it to its original state. And now, we're going to invoke this with foo and single quotes and again, we need to enclose this with quotes to make this a Python string. General rule of thumb, if you have single quotes in a Python string, we need to enclose it in double quotes. If you have double quotes in a Python string, we need to enclose it in single quotes. And if you have both, we need to come up with an additional multiple strings enclosed in single quotes and double quotes. So, here we go, we put in foo and single quotes into the program and now let's see what happens. Surprise--what you see here, if you put in single quote, they are not stripped whereas just to recheck, double quotes; however, are stripped from the input. That's and important end. Double quotes are stripped and single quotes are not. So the output actually contains single quote which means that our hypothesis as up here as up here is rejected--the single quotes are not stripped. So what we have at this point is that the condition in our code becomes true when we see a double quote and it becomes false when we see a single quote. At this point, we should have enough material to solve the problem. What's the correct way to fix this condition-- should it be a check for the just double quotes and single quotes and tags, should we go and invert the condition such that not tag is being checked, should we put parenthesis around the or condition, or should it be none of the above and something completely different. This is the correct way to write this condition. In Python and most other programming languages, or has a little precedence than and. And if you write this without parenthesis, the effect of this is that, this will be implicitly prioritize as this, meaning that the condition that tag must be said applies only for single quotes but not for double quotes which would always be stripped. And this is the reason why double quotes are stripped whereas single quotes are not stripped. Well, they are stripped but only when we are in tag mode and when we are in tag mode, then the characters wont appear anyway. We can apply this very same fix in our program simply by putting parenthesis around this junction in here such as this junction takes precedence over the conjunction. Let's see whether our example with the double quotes now works. Yes it does. Now, we actually get the quotes properly in here. Let's try it a few more. So let's see whether the single quotes still work too and while we read it we can even add a few more to that--let's see what the output will be. First example, with double quotes are still there, not stripped. Second example, single quotes, not stripped. Third example, just the HTML markup, HTML markup is stripped. Next one, quotes around HTML markup, quotes remained, HTML markup stripped. So HTML markup is removed in all cases. How about our almost complex example, we have the reference with quotes within the tag. Are they still properly stripped? We try this out. Yes, everything is properly stripped. Quotes and greater than signs are handled just as they should be. What is it we have seen? We have started with an initial hypothesis. >From this hypothesis we have made predictions. We have made experiments. We checked with the experiments, validated the prediction. If they did, we refined our hypothesis. If they didn't, we came up with alternate hypothesis. We have repeated the process until we came up with a diagnosis. That is, a theory that is consistent with our earlier observations, and that also predicts future observations. In our case, the correct behavior. Based on this diagnosis, how the failure came to be, we have fixed a code accordingly, and we have seen in our additional experiments that the theory holds. That is, the diagnosis was valid from the beginning on, and therefore, was a correct explanation of how the failure came to be. Note that there are many ways to come up with an initial hypothesis. There are also many ways to refine a hypothesis and reject a hypothesis. Note that there may be multiple hypotheses to start with, and also of course during the scientific method, you may come up with different hypotheses. If you have two competing hypotheses for the same failure, what you do is set up an experiment that decides which of the hypotheses is supported, and which of the hypotheses is rejected. At the end, there should be only one hypothesis, there should be only one theory, and there should be only one diagnosis on how the failure came to be. Here is an example of an alternate starting point. We could have started with the observation that, if I have a strain "foo" contained within tags, contained within quotes, then the quotes are stripped but the tags are still there. And from that we could have devised the hypothesis that tags in double quotes are not stripped. If we will take a look into our code, now again in the buggy version, in order for the HTML markup to appear in the output, tag must be false. In order to have tag remain false, quote must be set. And quote can only be set in this line, and then this would also have left us with this precise condition to look at. So although we start with a different hypothesis, we end in the same diagnosis, and we also end up with the same fix. And here we go. So from our hypothesis, we can deduce the tag must be false, because otherwise the markup wouldn't be in the output. Therefore, quote must be true, and therefore we have again our erroneous condition, which sets quote. Alternate starting hypothesis, same diagnosis, and same fix. After we have fixed the program we need to check two things. First, we need to check whether the same error has been made elsewhere. In our case, if there is a programmer who has confused the precedence of ors and ands, it may be a good thing to check the code for other places, and to make sure that all of these have proper parentheses around them, which anyway is a good programming practice. Second, we want to make sure that the error does not occur again. and we can do so either by writing appropriate tests, see the other Udacity course for how to do that. Or, we can include appropriate assertion in the code that makes sure the error will not occur again. In our case, there is a single assertion, which could be placed in the loop body to catch these kinds of errors, and actually an assertion which could remain in the code. It is an assertion that captures the relationship between the variables tag. What is the correct assertion? Is it assert quote and not tag? Or quote or not tag? Or tag or not quote? Or is it assert tag and not quote? This quiz can easily be answered if you recall our initial state diagram. Initially, we were in the state no quote, no tag. If we see a beginning of a tag, we go into the no quote and tag mode. And in this mode, we can go into quote and tag mode, from which we exit again by seeing a closely quote, and when we see the closing tag, we go back into the non-quote, non-tag mode. These are the three states that a program can be in. Which state is missing? The state that is missing is the state in which we would have quotes, but we are outside of tags. This is exactly the problem we would be handling in our program. We would take care of quotes, even outside of tags. This state should not be reached. So how can we express this with our assert statement? What we want to make sure of is that this state can never be reached. So this is quote and not tag. The inversion of this is tag or not quote. You can see that either we are in non-quote mode, or if we are in quote mode then we are in tag mode, and the assertion that actually checks this is this one. This is, therefore, the correct answer. At this point, we have systematically fixed the first bug using the scientific method to systematically come up with a hypothesis and refine it, refine it again, possibly come up with alternatives until we end up with a diagnosis. So if you're an experienced programmer--and I assume you are-- you may now wonder, "Why on Earth should I actually go through all these explicit steps if I can just jump into my debugger and fix the problem right away?" It's actually likely that you have spotted the problem at the very moment you saw the code. If you're a very experienced programmer, you can probably spot errors like these in hundreds and hundreds of lines from a distance of several hundred meters. You just look at the code and immediately see, "Well, there's something fishy in there." And all of this is, of course, right. There are many problems which you can immediately see and which you can immediately fix within 5 minutes including the one we just discussed And it's perfectly okay, if it only takes 5 minutes, to jump right into your editor and to fix things, to jump into your interactive debugger and fix things, as long as it doesn't take more than 5 minutes-- because what happens if it takes more than that? If you recall our story about you debugging late in the night, with your significant other calling you, and you not knowing what to do, let me now come up with a way to avoid all of this. The answer here is Explicit Debugging. What does Explicit Debugging mean? It's simple. When you're debugging late in the night, it's usually because you try to keep everything in your head. You're making the hypothesis in your head, you're running the experiment, and you're keeping the results of the experiment in your head. This is okay for 5 minutes, but the longer this goes on, the harder it becomes to actually memorize all of this. And this is why everything gets so intense. You're so concentrated on trying to figure out what is going on, and this is why nobody can disturb you--because then you'd get out of your trance. And the alternative to this implicit debugging where you're keeping everything in your head, of course, is explicit debugging. Explicit debugging at first simply means to write down what you're doing, make notes of what you see, make notes of what you expect, and make notes of what your current hypothesis is. A common format for this, for instance, is to write down what the input was or generally what the experiment is, what you expected to see, and what you got instead. You may even want to write down the current hypothesis you're working on and whether this hypothesis is confirmed or rejected. This way you keep a log of your actions, and this log gives you multiple advantages. To start, you can always revise what you actually did and what the result was, so you don't have to memorize it, and you don't have to repeat it. Secondly, you can resume the session at any time because everything is already written down. You don't have to store this in your head. When your significant other calls you and asks you out for a nice dinner-- well, everything is written down, and you can resume the next morning with a fresh mind and a nice dinner on top. The third advantage, however, is that when you write things down and see them again-- this often already bears the solution at hand because forcing you to become explicit frequently makes it clear to you what the problem actually is, so it structures your thinking, and it helps you organize your thinking towards successful debugging. Sometimes it helps a lot simply to tell a colleague or a friend what you're up to. This is the problem I'm working on, this is what I'm seeing, this is what I tried out. In my experience, in 2 out of 3 cases, the problem resolves itself simply by telling it. When I was a teaching assistant at the university, students would come to me and come out with all sorts of problems. Now I'm a professor; they only come with the hard problems. But when I was a teaching assistant, they would come along and say something like, "Oh, Andres, Andres, you know, I have this problem over here. You see, here's this loop, and in this loop we have this binary search tree, and you know, I've been adding 2 elements here, added 3 elements here, and more and more and more, and now I'm doing this and I'm doing that, I'm deleting over here, but. . ." And of course I didn't understand a single thing. But I was still there and nodding and saying "yes, yes, yes, sure," as you do, but then the student would come up and say, "Oh, actually, now that I'm saying it, I'm not deleting anything at all. I think, yes! It must be! If the element is not in the tree, Yes, the element is not in the tree, that's why it's not deleting! Thank you Andres! Thank you so very much, everything is just beautiful. Thank you so much for helping me out here." I was happy, the student was happy, and I hadn't done anything. Of course, occasionally there were also problems where students would really come along and have real problems, and then ask me with a big puzzled face, and then I would have to come up again and say, "Okay, okay. Can you repeat this a bit slower? What's this thing about foo and a binary tree and the quotes and the tags and everything?" And then we would go into regular debugging, but in two-thirds of the cases, yup! Simply by listening, I would be able to debug it. Hey! I was a king of debugging. Here's a story that is reported by Kernighan and Pike in their book, "The Practice of Programming." One university computer center kept a teddy bear near the help desk. Students with mysterious bugs were required to explain the problems to the bear before they could speak to a human counselor. Picture that--speak to a bear, and in two thirds of all cases, the problem would resolve itself. That's how effective explicit debugging is. To make another case for explicit debugging, let me illustrate this with a game of Mastermind. You know Mastermind, don't you? Your opponent has 4 pins hidden behind a board, and your job is to guess the colors of these pins. So you come up with 1 trial, and you get points for every color that's in the right position, and for every color that's correct but not in the right position. This is what your opponent tells you. And so, you make 1 attempt after the other, until after normally 7 or 8 attempts or so, you finally guess the right combination. Mastermind is a prime example of applying the scientific method: coming up with 1 experiment after the other to validate or reject a hypothesis on what's behind these bars, and then based on your observations, you come up with a new hypothesis and a new hypothesis. You literally refine your hypothesis until you find the answer. Now, when you're playing Mastermind, you always record what you tried before and what the result was. This is the same as explicit debugging. You write down what you do and what the result was. And this is what makes you effective. Not writing down what you do is like playing Mastermind with a blindfold, trying to remember all the combinations and all the results. Sure, you can do that. You're a great thinker. You're a smart person. You could probably also play Mastermind blind. But why do that, if you can simply look at what you already did, interrupt the session at any time, and resume later at wish? Think about your significant other and remove your blindfold. Should you fix a bug as soon as you spot the problem? Always be sure to keep all details in your head because writing them down takes too much time? Explaining the problem--maybe to a teddy bear, to a colleague, or a friend--can be helpful. And finally, up to 50% to 70% of software development effort is spent on validation and debugging. Fix a bug as soon as you spot the problem? Well, if you can fix a bug, yes. But normally you'd like to think about the problem, whether you can generalize it, find about the best place to fix, reason about it until you have a very good diagnosis, so this is definitely not true. Always keep all details in your head. Well, the name of the last part was "Explicit Debugging," so you should not always try to keep all details in your head. This is wrong. Explaining the problem can be helpful. Yes--plenty of anecdotal evidence for that. And finally, up to 50% to 70% of software development effort is spent on validation and debugging. This is also true, and this is something we really, really, really must change. Welcome back again. This is the second unit in the debugging course. This topic is asserting expectations. Today, we will go and explore assertions. That is statements in the program that automatically check for errors during execution. Assertions are by far the most powerful debugging tool, in particular, because they are the key to automating debugging. That is debug while you sleep. If debugging were a video game and you would be hunted by bugs, then assertions would be a power pill that would help you catch all the bugs. Okay, specifically, we are going to learn how to learn how to write assertions, how to check assertions, and how to infer assertion from executions automatically. In our last lecture, we already had met the assert keyword from Python. Generally speaking, the statement assert condition, evaluates the condition, and then its behavior depends on whether the condition evaluates the true or to fault. If the condition holds, we proceed as usual. If the condition does not hold, however, then we interrupt program execution by throwing an exception. When you're writing your condition, you assume that condition will hold. This is normally useful when you're writing a test. For instance, if we want to test the function square root, we can write a test function that asserts the 4 is 2 and 9 is 3, and if any of these tests fail, that is if the result of 4 is not 2, then the assertion will fail and throw an exception. Such a functionality of assertion is available in all programming languages. You could even write your own assert function. So in case an assert function would not be available in your language, you could easily roll out your own. I'm coming up with my own assert function which I call "my own assert" takes the condition. If the condition is false, then we raise an assertion error. Now, if I invoke say "my own assert" on 2+2=4, very simple test, we can run this and well, nothing happens because the assertion is met. However, if we assume the 2+2=5, we can run this and we get an assertion error that's being raised in here, and this gives us almost the same functionality as the built-in assertions. While you can easily roll out your own assertion, built-in assertions typically do have a couple of advantages. There is identification--the built-in assertion typically tells you which assertion failed. There is location that tell you where the assertion failed--that is where in the code. They are optional--meaning that they can be turned on or off although turning assertions off is mostly a bad idea, which we will discuss later. And finally, they're standardized--meaning that everybody can immediately recognize an assertion as such because they always take the same form. In the languages C and C++ for instance, assertions come with a built-in function named "assert" as in Python which you get by including a special file assert.h, and then, as in Python, you use assert except that you put the argument in parenthesis, and again, you pass the condition as an argument to the assert function. If this assertion failed, you're going to get a message on the console right before the program exits, and the message will look something like this--there you go. And now, you can see that the message actually has everything we wanted in our built-in assertions. So first, the failing assertion is identified down here as coming directly from the code. You see the location here--it's in the file foo.c in line 9. In C and C++, assertions are optional, you can turn them on and off using a simple argument to the compiler, and finally, they're standardized if you include assert.h, that's always the form, and this is what programmers look for and this is what programmers understand. Now, compare the level of diagnostic output we've seen in C and C++ with the amount of diagnostic output you get in Python. If you write assert 2+2=5 as condition that evaluates the form. So which information does one get from a failing Python assertion. Do you get the failing assertion condition. In our case, 2+2=5. The location of the assertion in the program. In our case, line 1. The location of the defect in the code or the list of callers. Try it out for yourself, over to you. Welcome back. What's the answer to this? Well, very simple--we go and try this out. Here's a failing assertion: 2+2=5. Let's see, when we run this whole thing what we got? So here's the anti-traceback: we get the assertion error, we get the assertion that failed. At least we get the line and the code, which is the same. We get the location--line 1 in our function and we get the list of callers. So we've seen the list of callers, the location of the assertion of the program, and the failing assertion itself. What we do not get is the location of the defect in the code because the defect would actually causes the infection that later causes the assertion to fail, maybe in a completely different location. That's the whole point of debugging, trying to find the defect that causes an assertion to fail. However, during this course, you will find techniques that help you a lot in finding out where the defect is that causes an assertion to fail and all of these automatically. So far, we may have encountered assertions in tests, in particular, in unit tests. Here, assertions are being used to check the results of a single run, mainly the test run. What we're going to look into is assertions that are integrated into the program code itself. Where the check all runs at once. A classic example for the use of assertions is a square root program. In the beginning, when invoking square root, we want to make sure that x, the parameter, is non-negative. I'll skip the actual computation of square roots here because this is the subject of another class, but before we return to the actual square root, we want to make sure that the square root actually is a square root. How do we do that--well, we square the square root and check whether it's identical to the original argument x. These two kinds of assertions form a pair and both of them have very specific objectives. An assertion that's being called at the beginning of a function and which checks the properties of the arguments is called precondition. An assertion which checks the result of a computation before it's actually being returned is called a postcondition. If you have such explicit checks in your code, this greatly increases your chances of quickly finding the bug, because these checks would re-invoke every single time that the function is executed and if they don't fail, you know that the computation is actually correct, which is a good thing. If you remember, our cause-effect chain has a succession of program states, where a defect introduces an infection which is then propagate to become the failure, imagine what happens if you actually do have assertions in your code that check large parts of your state. Now imagine what happens if you do have assertions in your code that check large parts of your state for whether they're valid or not. Suppose you have an assertion that checks all of these area every time a function is invoked, then you wouldn't see the failure only at the last moment but you would see the failure as soon as the infection spreads towards. If you have another assertion that covers this part of the state, then you would see the failure at the very moment it is introduced. which of course also means that you can immediately identify the defect that causes the infection. The earlier you see an infection--for instance, because an assertion fails, the shorter the cause-effect chain, we need to investigate. Also, if you do have parts of the state that are covered by assertion and these assertions don't fail, then we know that the state in here is actually not infected and so in your search, you can narrow down your search towards those parts of the state, which are not covered by passing assertions and all of this happens automatically with every single run, every single test run, and if you want so, even every single production run. Assertion also easily allowed you to put the blame on specific places. Assume I have a function f with a pre and a post condition, and we call f and this precondition of f is satisfied. Now, f calls g and now, the precondition over here is violated and raises an exception. Quiz question, who is at fault here? Is it f because it violates the precondition of g? Is it g because it raises an exception? Or is it both f and g because they are incompatible? Over to you. The correct answer is f because it violates the precondition of g. Pre- and post conditions are generally considered specifications-- that is they have the authority to say what is right and what is wrong, and if you violate a precondition, this means that the caller is wrong. G raises an exception well and that's just the messenger. We don't shoot messenger. And f and g, well, obviously are incompatible--yes, that's right, but since pre- and post conditions serve as specifications are definitely f is at fault for violating this specification and g is fine--it just raise the exception. If you have assertions in your code, not only can you detect errors much more easily but you can also test your code much more thoroughly. In particular, you can separate two things. A test consists of two parts. First we need to generate on execution and second we need to check the outcome. With assertions in your code, checking the results is already done so we can focus on generating the tests. Let me illustrate with a very, very simple example. The idea is that we're going to take a square root function with assertions integrated and just use random testing to test it. Here's a very, very simple implementation of the square root function. We simply invert the built-in Python square root function from the math module, and we have set up appropriate assertions to check for the precondition and for the post condition Now for the test driver, I have set up a loop with 1000 iterations and we generate random numbers in the range of 0 to 9,999.99. This is random with random function which returned a value between 0 and 1. This is the floating point number and it comes from the random module and then when we feed this random number into a square root function turning assuming the result in z Note that we don't do any assertions down here because the assertions up here have already take care of everything. When all test are successful, we print out done. So this is our test and the question is, what's going to happen? Is it the program terminates gracefully, output is done, all test are fast. Is it that the square root precondition fail? Is it that the square root post condition fails? Or thinking of Python, do we see something completely different. Over to you. Thank you very much for your answer. Proof of the pudding is in the eating. So what we do is we simply press run and we press run again and we see-- we get an assertion error and indeed it is the post condition of square root that has failed. So the correct answer is the reduction square root post condition fails. So we saw that the post condition of square root mainly squaring the square root equals the origin of value failed. This is weird because all we use--we invoke the standard square root function of Python. Does this mean that Python square root function is wrong and now finally, through systematic random testing, we found the bug. Let me show you what's actually going on in here. But why does the post condition of square root fail? The answer is simple, yet not so simple. What we lack here is diagnostic information. We don't know the values of r or z or anything that happens in between. So what we do is, we set up a framework that allows us to access these. So what we have set here is we have set up a try except block around the square root function, which catches the assertion and when an assertion is being raised, or any other kind of error is being raised, we print out the offending parameter r and we exit the loop, so let's see what happens here if we invoke the whole thing. So here's the bad value, 8*436.9207865. Wow, magic value. So why can't we just square this. What's wrong here. So we print out the value and now we go and actually square the square root and multiply it by itself. Let's see what we got--again, we run the whole thing and 2123.01753488 and this is 2123.01753488. Well, by all means and measures, these two look identical to me. Why does the assertion above it failed. We can solve the mystery simply by looking at the actual difference between these two values. Let's see what the numerical difference is between the square root, square, and original value itself. Here comes another value which fails 9,049. something and here at the very end, you see the difference--it's 10^-12, meaning that 12 digits after the comma we do have a difference between r and the r The reason why this assertion failed is simply a rounding error, the numbers are the same up to the 11th digit. At the 12th digit, they actually are different and this is why the assertion failed. So, how are we going to address this? We're going to introduce a parameter, named epsilon, which tells us the maximum deviation from the correct result as an absolute number and we're going to give it a default value, say up to seven digits after the comma is still fine. Now, up to you, take this example and set up a suitable post condition in here, such that the difference between y and x is never greater than eps. So now here we have the correct assertion. We take the square of the square root compared with x and that absolute difference should be smaller than eps. We go and tick on run and if everything works well, our test should passed by now. See, prints done, which means that all assertions actually now have been satisfied. Just for the record, on my machine at this point, I can go and run a million test like this per second, which means that assertions can be a really great test device, compared that to the time it would take you to write a million number of test cases, which means that once you do have assertions, you can go and run test like crazy. Assertions in the code are there to stay. We use them for catching errors, which we can also use during testing, but they also serve as documentation tool for programmers because they show what the assumptions of individual methods are and they show what the individual method is supposed to produce. Now let's apply assertions to another program, an old favorite of ours-- our buggy HTML stripper from the first unit. So here again we have the function to remove HTML markup, which we already know from the first unit. Here as well, a simple assertion would have sufficed to catch the error, mainly an assertion that checks for the non-existence of HTML tags in the output. Such an assertion could be something like this--we simply check whether there is any less than sign that is the beginning of an HTML tag in the output. We assume, there's none and if there would be one, then the assertion would immediately fail. So here comes the quiz. With this assertion, which input would cause the assertion to raise an exception-- is it foo in bold, is it foo in quotes and bold, is it foo in bold and quotes, or is it foo in bold and single quotes. Which of these causes the assertion to fail? Over to you. Thank you for your hard work. Let's check these inputs one by one. The first one, well this is just tags and no quotes--always has worked. The second one--well, the second one will have the quote stripped from the input, but this will not be caught by the assertion because the assertion only checks for the presence of HTML tags and these will be removed. The third one, however, will not have the HTML tags removed, so this is the correct answer. And the last one has single quotes and single quotes actually were as they should be. We can verify this within our programming environment. We can add a simple test here. This is the input which should cause the assertion to fail, press and run, and as you can see down here, the assertion actually failed. I leave it to you to try out the other three inputs. Writing assertions is not always easy. You need to be prepared for a number of iterations until your pre and postconditions are accurate. A typical chain of tools looks like this. Here, we do have test inputs either random generated or systematic generated or coming from users or coming from production. These test inputs have been checked by preconditions, and the results of the computation will be checked by postconditions. Now, as it comes to preconditions and postconditions, you can be either too permissive or you can be too strict. If you are too permissive with your preconditions, this means that inputs, which shouldn't go into the function, go into the function and then result in arbitrary behavior. If you are too strict with the preconditions, this means that valid inputs will result in failing preconditions. The same also applies for postconditions. If you are too permissive with your postconditions, this means that you will not catch a number of bugs. If you are too strict, however, then you will catch bugs where there are no bugs. The key is to come up with the right balance between these two. You don't want to be too permissive. You don't want to be too strict. In theory, you like to have preconditions and postconditions that capture exactly what the program is doing. However, in practice, you will find that while writing preconditions is typically easy, writing postconditions or regular functions can be very hard. And therefore, it's very common for postconditions only to check a part of the actual state that is, your postconditions that are simple but a bit too permissive that is they missed errors. Whereas for preconditions, it's frequently feasible to come up with the right level of precision. Why is that postconditions are so complex? To give you a simple example of why writing correct postconditions is not easy at all, let me give you a very simple example. Let assume we do have a sorting function. The precondition for a sorting function is simple: basically, we simply assume that x is a list. Well, we could put up an appropriate assertion here, but the precondition is to accept essentially anything. For the postcondition, we have to make sure two things. First, we assume that the list y that we will return actually is sorted, but we also need to make sure that the list we returned is a permutation of our input list. So let's go and check whether the list is sorted. We iterate over the elements of y and if one element is greater than its successor, we return false, otherwise we return true. So what you see here is that in order to check the postcondition, we needed to come up with another function, which actually now, should again be checked against what? The property that is-sorted checks here, is essentially already described in here, meaning that any assertion you could have here would be just the same as anything you have in here so you simply have to believe that this is true. Well, I didn't believe that I actually ran a number of tests, so I'm pretty confident that this works. Now, this is just is-sorted. You also have to come up with is-permutation, this is also something you have to define. Helper functions like is-sorted or is-permutation will probably be helpful later in the future because you may be able to reuse them for other purpose. They may even become part of a library. Which of these post conditions will be hard to check? The audio level must not exceed 80 decibels, the animated cat should walk like a real cat, under specific atmospheric conditions, the sunset simulation shall produce a green flash, or response time shall be below 10 ms. Over to you. And now for the answer to our quiz. Generally, anything you can measure can also be turned into post condition, and if at some physical condition, then it's typically very easy. The audio level must not exceed 80 db. While assuming we have a way to measure the audio level, which any descent audio software should have, yup--this is easy to turn into a post condition. Turn this into a post condition and you will save millions and millions of ears from damage. Next thing, the animated catch should walk like a real cat. Ah, real cat. If there ever is a specification of what makes a cat a real cat, I would be very, very delighted to see that. I'm pretty sure cats know precisely what to do and what not to do. For everybody else, that remain a mystery. Under specific atmospheric condition, the sunset simulation shall produce a green flash. Lo and behold, that's what a PHD student at my department did. He was specializing in computer graphics and his PHD thesis was perfect sunset simulation, and indeed, in reality under specific conditions, you can have a green flash during a sunset for just a second. I think this has been witnessed by only a few hundred people and only banned on film twice so far, and well having green or some amount of green in an image--it's actually pretty easy to check. So I am pretty sure he also had a post condition that were done and show that he was successful, and finally response time shall be below 10 ms. That's straightforward. Time is something that's easy to measure. This can also become a simple post condition. So, at this point we have seen that writing perfect assertions that cover all bugs would be pretty hard, but this shouldn't keep us from writing assertion that catch as many bugs as possible, and this is particularly useful in debugging. Since assertions are automated, they can check several executions of a function at once--actually all executions. However, they can also check large portions of data at the same time and all automated. What do I mean by large portions of data? Let me illustrate this by an example. Let's assume we want to implement a time class. A time that consists of hours, minutes, and seconds and we want to use assertions to automatically check whether a time object is consistent. That is whether the hours are in the right range, minutes are in the right range, and seconds are also in the right range. Here is our time class--we start with an initializer or a constructor that takes three arguments: the hour, the minute, and the seconds, with default values of 0 for each and we assign these to individual attributes named hours, minutes, and seconds within a time object. Here's a number of inspector method that gives us access to the internal attributes-- hours, minutes, and seconds. After adding inspector methods that gives us the hours, the minutes, and the seconds, we now need a way to print out the time object. For this, Python provides a special method. It is called the repr for representation method, the internal method with two underscores before and after and what this thing does is this method is called whenever an object of the particular class is to be printed, and this returns a string representation of the object. So what does it do, we're using the string format method here, which takes a number of arguments, and formats each of these arguments given to the format specification written here in that string. So for instance :2d prints the numerical argument here with two digits-- self minutes also comes in two digits, self seconds also comes in two digits. This format is not perfect yet. We actually want leading zeros in here and we want hours, minutes, and seconds separated by colons. So here's a string, first the hours two digits leading zero, then the minutes two digits leading zero, and then the seconds two digits leading zero. Let's try out how this works I'm initializing a time object here 13 hours, 0 minutes, 0 seconds. We're using a 24-hour clock here and I'm printing this out. In printing this out should now invoke this repr method, which would automatically provide a nice string representation of our time. Let's run this whole thing and we get nice representation 13 hours, 0 minutes, and 0 seconds, 1 pm in the afternoon. Now, all of this is fine and works well. There's no bug in here, but there's nothing in there which prevents misuse of my class. For instance, nothing prevents me from entering numbers in here which make no sense at all. For instance, I could enter -1 as the hour or -2 as the minutes or -3 as the seconds. This is not a valid time or we would have to come up with entirely new definition of what the valid time is. However, we can still instantiate such an object and even printed out comes as -1, -2, -3. So here we're still talking three numbers, but since Python does not provide static checking of types, that is checking of types when the program is being compiled or rather executed, there's noting that prevents me from passing objects which have a completely different type. So I could pass a string in here for instance and then hours would be initialized with the string-- this means that I would get a time object in here, which is entirely invalid. So my situation is--I'm passing some string here in the hours attribute. And now for a quiz, so if I do this, do I get an error when initializing the time object or when printing the time object or never. We can find the answer to this question right within our environment. We first just run the initialization--that is just passing a string in here and we just run and nothing happens, no error message, so this is just fine. It's actually possible to pass a string object here. Now, when we try to print it out and run this whole thing, we get an error message because now what we have is that the format up here is no longer appropriate for an object of type string--that is Python does not know how to format a string with two digits with a leading zero. As you see, as soon as we are printing out the object, we get an error and therefore, the correct answer here is we get an error while printing. What you see here is an instance of a problem that's commonly known as a time bomb. A time bomb is infection in the code that's just waiting to explode into your face as a failure. Here's something that's definitely wrong and it's in there and it's sleeping. It can be there for million of cycles. Only when it's being accessed or processed that's when the time bomb explodes and this is hard to debug because you have to figure out where the time bomb originally was planted and set -- that's where assertions save the day. An assertion prohibits time bomb by checking whether the data is sane at the very moment it is being stored. When we are creating a timedobject, we could for instance use assertions to make sure that the arguments actually are all within specified ranges. For instance, we could say the hour must be between 0 and 23. At the same time, we give a hint to the user that time uses 24-hour format. We also want to make sure that the minutes and the seconds are within the proper ranges between 0 and 59, respectively. This special syntax you see over here is a speciality of Python which allows to coerce multiple comparisons into one. Actually now that I think of it, 59 is not correct here. There's years in which there's leap seconds and if there's a leap second, then there can actually be 61 seconds in a minute. So, in order to be perfectly correct, this needs to be 60 up here. This is still a very simplistic time class, it just assumes, say local time for instance. We don't care about time zones or calendars or daylight saving times or anything at this point. The real time class is way more complicated than this one. Now, let's see whether these assertions help us in avoiding time bombs. We still have the string passed in here. Let's see whether our assertion catches this. Again, press and run. And see, as we put in 2 minutes after midnight as a string, the assertion fails because the string is not between the values of 0 and 23. What happens if it pass negative values in here say hours is valid, minutes is negative, seconds is valid, press on run, and again we get a failing assertion-- now, we get the failing assertion that the minutes is not within the right range, which is exactly what we wanted to catch. With these assertions up here, any attempt to set any time object to an illegal state will immediately be detected. And again, if you think of a program as a succession of states that passing assertion can immediately tell you that there is large parts of the state where the infection cannot have taken place. If these were the time object in your state, you would know that all of them have correct values because none of their assertions has failed. One problem, however, remains without time class. You can still pass floating point numbers as arguments here and then get an error while we're trying to print the time. So here is your programming exercise, extend the code such that all parameters up here are automatically converted into integers. Welcome back. We have the problem that we can't pass floating point parameters up here. And these were done later, result in errors while printing, so what we want to do is we want to convert them automatically into integers. How do we do that--well, we simply go along and use the Python function int, which automatically creates an integer out of the object that's being passed in here, and in order to make everything real perfect, we also do so in the assertions. And this is the answer to the quiz. We now have seen how we can check the input parameters through the time class. But what happens if there is more setters and getters which can all compromise the state? Let's assume we have a method advance which takes a number of seconds by which to advance the time. For instance, a value of 3600 would advance the time forward by 1 hour whereas a -60 would set back time by 1 minute. Here's our advance method. I skipped the details of the computation right now, but instead, we focus on the postcondition. We assume we do have a method seconds-since-midnight which returns the number of seconds that have a lapse in this very day, then we can set up a postcondition which simply says that after we advance the time, then the new second since midnight should be the old value plus the offset of seconds, all of this modulo the number of seconds in a day, such that we probably take care out of advances that go beyond or before midnight. The method seconds-since-midnight is rather straightforward to implement: it's the hours times 3600 plus the minutes times 60 plus the seconds. So again, we use the helper function to help us define our postcondition, but for all of this to work, we need to make sure that even after the complex computation, the state of the time object is still sane, that is, minutes, hours and seconds are still within the right ranges, no matter what happens in this complex computation and this is not checked at this point. What we need here is a property that always holds. In Computer Science problems, we call such a thing an invariant. An invariant is a condition that always holds for some data object. What does always means? Always means from the perspective of a user of that object, that is at the beginning and at the end of each public method. While the method is executing, invariants can and frequently must be violated, but when the method is done, invariants should hold again. For our time object, the invariant condition is already encoded here in the argument checker, in the precondition of the initializer, but now we want to check this condition over and over again. At the end and at the beginning of each public method. What we do is we write an invariant checker. This invariant checker called checkRep here checks whether the internal representation is sane or correct. In our case, we simply set up three assertions to check whether hours, minutes and seconds are within the right ranges. Once we do have such a checker, we can now use this in all functions at the beginning or at the end, as appropriate. For instance, after initialization, we invoke checkRep then make sure whether the internal state is within the right ranges. This is also true for setters like advance. Before we change something, we check whether the invariant is satisfied and we do the same after the computation such that if the computation in someway messes up hours, minutes, or seconds, then checkRep will immediately find this. Let's see whether you can come up with an invariant check of function yourself. As an example, we're going to use zip code. The zip code is what you need to address a letter to a specific address. The zip code is a number or in some countries a sequence of numbers and letters that identifies a specific region in some country. For our example, we're going to use American zip codes, which in their most basic form are five digits, and those five digits go from five zero to five nine that is each of the five digits is between zero and nine. So, here's a zip code class. Again, we have a constructor, which takes the zip code which is a string consisting out of five digits. We set zip, internal zip attribute to the string--again, five digits. We also have a method to retrieve the zip code again as a string and now we need to come up with an appropriate representation checker invariant checker that checks whether the state is okay. We could, for instance, start with making sure that the length of the zip code is precisely five. Now, go and complete the invariant checker such that the content of the string is also checked such as that each of the digits is between zero and nine -- over to you. Here's the answer--what we need to do additionally in checkRep is we need to check whether every single element of the zip code is between 0 and 9, so we set up loop that iterates over the elements and check this for every single element of the string. We could also come up with a regular expression that ensures that we have a sequence of five digits between 0 and 9. I will leave that for the more advanced among you. So we have seen that with assertions we can ensure that objects are in a sane state all the time. You can imagine that this would be most useful for large, complex data structures. That is, structures with lots and lots of invariance that need to be maintained or that could possibly be violated due to programming errors. Let me illustrate this with one of the most devilish data structures ever invented, which is a Red-Black tree. Here's an example of a Red-Black tree. A Red-Black tree is a data structure to represent associative arrays also known as mappings. If I want to search an element in a Red-Black tree, I can do so in an almost logarithmic time. And I do so as in all search trees by checking whether the element I'm searching for is smaller or larger than the element I'm looking at. Suppose I want to check whether the number 22 is in my Red-Black tree: I start at the root, which is 13, I'm looking for 22, which is larger than 13, so I go along the right branch, 22 is larger than 17, so again I go along the right branch, 22 is smaller than 25 so now I go along the left branch, until I'll end up exactly in the element I was searching for. Red-Black trees do not only guarantee search in logarithmic time but also insertion and also deletion, which makes them a real nice choice for all sorts of search operations. But they also are very, very difficult to debug. The reason they're difficult to debug is they do have a number of properties that must be maintained at all times. To start with, they always must be trees. You can't have any cycle in it. A black node must have red children, and a red node must have black children. The number of elements in the subtrees should be fairly equal with respect to the number of black nodes, and if some element points to a child the child maintains a pointer then this should point back to the exact same parent. So search in Red-Black tree is easy but insertion and removal are really complicated. To get an idea just how complicated they are, let's take a look on Wikipedia what they wrote about Red-Black trees. So this is the Wikipedia page on Red-Black trees. Here's the insertion. These are two helper functions, grandparent and uncle. Now we go to 1, 2, 3 different cases all with individual code that needs to be handled. Here's case number 4, and here's case number 5. Removal is even more complicated. This is the formal description. We need a helper function called the sibling. Here's a helper function to delete one child, which is easy. And now come all the special cases: case 1, case 2, case 3, case 4, case 5, and case 6. And of course, you can imagine how easy it is to make mistakes in any of these cases. It would have some pointer pointing to the wrong node, and again it would easily create lots and lots of time bombs which is why we normally do not implement Red-Black trees but we rather rely on Red-Black trees as implemented in some library. However, if you were to implement a Red-Black tree, say part of a programming exercise for instance, it's cool. Or you're working on something that's similar to a Red-Black tree, but which doesn't have yet its page in the Wikipedia, then you really need to make sure that you don't screw up things and that you don't create a time bomb in here. If you work on a data structure as complex as this, the very first thing you need to write in an invariant checker. So here's our invariant checker for the class Red-Black tree. You want to make sure that the root node is black: you can make a mistake, say, move the subtree up here and all of the sudden the root becomes red. This assertion will take care of this. And as you can see here, we're setting up helper functions in order to break down the invariant into manageable small pieces. We want to make sure that there are no cycles in the tree. We want to make sure that pointers to parents are consistent, that is, if some node points to its parent then it should also be a child of this array parent. We want to make sure that in both subtrees the number of black nodes is equal. That's a niiice looong function name, which says it all. And finally, you want to make sure that red nodes have only black children and at the same time black nodes only have red children. Not sure whether we need to make this a separate method. Some of these checkers are rather trivial, that is one-liners, others require a traversal of the anti-tree and would typically call themselves recursively. And now for every single method that change the state, you can now invoke the invariant checker. You do so at the beginning, you do so at the end, and when all of your assertions pass, then you can be sure that your Red-Black tree never ever becomes corrupted. However, if something goes wrong, it would get the error message at the very moment you make the Red-Black tree inconsistent. You will get the precise assertion that failed and you won't implant any time bombs 'cause if the precondition is satisfied and if the postcondition is violated you know that it is precisely this method--the insert method in this case-- that caused the data invariant to be violated. This thing about Red-Black trees actually is a sort of a personal thing for me: ten years ago, when I was a teaching assistant, my professor had the cool idea to have the students implement Red-Black trees as a programming exercise. And we as teaching assistants, had the cool idea to come up with a real hard automated testing for the student assignments, which turned out a disastrous combination because writing a Red-Black tree is one thing and writing a Red-Black tree that works is another. At this time, I did not really know how to debug and neither did my students and neither did my professor so we as teaching assistants would be sitting with our students for hours and hours and hours in the night stepping and stepping and stepping through the program and try to figure out where on earth on the umpteenth insert operation this particular pointer would finally be moved to this other location, which was a total pain to figure out. In particular with regular debuggers, we typically step over the whole thing and then finally lose the moment so you'd have to restart again and restart again. And if I think about how many hours probably days and weeks I have lost digging through these data structures whereas with a single invariant checker I could've found them in minutes I'm still somewhat sorry. If you find that all these invariant checkers become tedious to write, you may consider a language where invariants, pre and post conditions, are all built in. This is the case in Bertrand Meyer's Eiffel language. In Eiffel, invariants become part of the classes. The invariant also are named, so if an invariant is violated, the name of the invariant becomes part of the diagnostic message. Here we have rootHasNoParent and what this does is simply checks whether the root attribute is a valid pointer--that is, it's not a void pointer. This is unequal in Eiffel, and this is what another language is called a null pointer. Here we have the invariant rootIsBlack and it simply checks the color attribute of the root note to be black and of course, there's more to it. Once you have defined such an invariant, the invariant will be automatically checked. Actually, it will be checked precisely according to the rules we set up previously. Do you still remember them? So when does Eiffel check its invariants? Is it whenever some class attribute is read, whenever some attribute is written, at the beginning of each public method in the class, or is it at the end of each public method? Check all that apply. Okay. Now for the answer. When does Eiffel check its invariants. Well, it does precisely so as we defined before and as we recommended to check the invariants that is at the beginning of each public method and at the end of each public method. If you look at whenever some attribute is read or written, this also happens within the methods of the class and in particular--say you first try to some attribute, then some other attribute, then some other attribute. In between, you may have some inconsistency that will be discovered by the invariant. So while some method of the class is operating including reading and writing attributes, we don't want to check the invariant which is why these two must not be checked. Now, for a bit of advice on assertions. First of all, assertions take time. If for every single operation on a large data structure, you're to traverse the anti-tree in order to check it for consistency, these operations will be no longer logarithmic time but at least linear time. A check on a square root result will not matter that much, but if you need to traverse large data structures, it will. This is why assertions can be turned off. The way assertions are being turned on and off differs from the language used. In Python, there is a -O option which turns assertions off. -O stands for optimized. In C/C++, the compiler option -DNDEBUG for no debugging also turns assertions off. In Java, there is an option -ea for enable assertions which turns assertions on. Java special by having assertions turned off by default. Not very surprisingly, the assert keyword in Java is the least frequently used of all keywords in Java, which in my view is a big, big shame. The fact that assertions can be turned off and frequently will also implies that that assertions must not change the program semantics. Whether assertions are turned on or turned off, the behavior of the program should be the same. Suppose you're developing this cool map application, which also supports setting pins for individual locations. Now, you want to make sure that if you remove a location, the appropriate function returns the right value so just remove it. And if you're right, assert map.remove location equals to True, in order to make sure that remove location returns the true value that's dangerous because if the assertion is turned off here, then not only does the check go away but actually also the call to remove. Generally speaking, if anything in your assertion has side effects such as this one, don't put it into the assertion. What you do instead is you put the functionality into code that is outside of the assertion and then use the assertion only for checking the result. This way if the assertion is turned off has no effect then the semantics of the program will still be unchanged except that the checks will be gone. Another implication is that you should not use assertions to check for public preconditions that is, conditions that your program must check anyway to ensure the integrity of data coming from third parties. As an example, if you have, say website or device, myaccount.com, where users can enter arbitrary amounts of data to be processed then do not use an assertion to check for the integrity of this data instead use a check that is not optional and will always remain active. So, in our example over here, here's something that you should not write. So here, we get an input, convert this into an integer, store this in deposit, and then we use an assert to check this external data to make sure the deposit is greater or equal than 0. As soon as I turned the assertion off, this check will no longer be performed, and therefore, it would be possible, say, to enter negative deposit numbers. What you should do instead is just a regular check. If you do have some invalid data in here that comes from a third party, raise an exception, abort execution, pop up an error message or do whatever is most appropriate for the situation. Finally, it's always a good question whether assertions, in production code, should remain enabled or not. Here are some points to consider. First, failing is better than bad data. If your program fails with an assertion, you know that there's an error. If it simply produces a bad result without actually checking it, then this may have far worse consequences than failing. Second, the more active assertions there are, the easier it is for you to find the defect. Third, defects in the field are hard to track, which means an assertion gives you a chance to find the defect before it goes into the field, in particular, in combination with thorough testing. And even in the field, if assertion are still enabled, you still have a far better chance to find defects because, well, you get the failure rather than producing bad data. On the detractor side, so what speaks against assertions being enabled in production code? We have performance, performance, performance. If you do have a large data structure with lots of global checks and leave this enabled, performance will suffer. However, there is large number of assertions which impact performance only in a marginal way. Checking some result is larger or equal than zero, ranges, like things. This will not impact performance in a measurable way. The answer here is to first leave the assertions on then measure which assertions actually do impact performance and possibly do turn this off and leave the others enabled. The second thing is asssertions, when they fail, typically caused the program to crash, or at least, aboard execution immediately. This is not very user friendly. You should give your users a chance to recover even if an assertion finds that something has gone wrong. But, then again, if your program crashes because of a failing assertion, what's the alternative? The alternative is to keep on working with bad data and bad data can produce arbitrary and even worse results later. You may have heard the story of Ariane 5 rocket which famously exploded on its first flight due to software bug sometimes called the most expensive software bug in history. Do you know why the Ariane 5 exploded? It exploded simply because it had a bug in a 64-bit integer to 16-bit integer conversion. The interesting thing though is that normally the code actually did have plenty of checks to track such illegal conversion. These checks were disabled in the Ariane 5 for reasons of performance. Picture what would have happened if the computer in the Ariane 5 had been just so slightly be more performant to cover this particular check of a 64-bit to a 16-bit integer then the assertion would have triggered even during the launch, but the Ariane 5 actually does have a very good recovery mechanisms for failing assertions. It would have recovered, may be the flight could have done well and we would have escaped the most expensive software bug in history. $317,000,000 went into the sky because of a missing assertion. With all of these considerations, let's imagine we're working on a traffic light system. We have three colors in here, red, amber, and green. Now, we have a little piece of code in here, which impacted the traffic based on the traffic light. Let's assume this is part of some traffic stimulation. So, if the light is red, then the traffic should stop. If the light is amber, then the traffic should prepare to stop. And if the light is green, then the traffic should go. Now, being good programmers, we always assume that something goes wrong if the light is neither red nor amber nor green, which in principle shouldn't happen, which due to some bug could happen. What should be here where the question mark is? I'll give you four choices. First, we print out "This can't happen," on the console in the hope that somebody will eventually see it. Second, we raise an assertion exception. We knew that the program will immediately abort. Third, we write down assert false thereby clearly documenting that this is not expected. Last option, you write down pass, which is a Python statement, which does nothing at all. After all, this bug in the traffic light may well eventually go away at the next cycle when we actually go back into some defined state. Which of these four is the best option to go down here considering the pros and cons about assertion that we see over to you. Thank you very much, now let's go for the answer. Let us discuss the options one by one. First, print "This can't happen". Cool, it's going to go to the console, probably going to go to some log but it is unlikely that anyone will ever notice this, and this means that our traffic light, well in our simulation, will be in some undefined state, possibly meaning that the traffic will also be in some undefined state. What happens if we have multiple traffic lights and one of them is neither red nor yellow nor green? Ow, this is going to cause lots and lots of interesting issues. So simply printing this out to the console is not enough. This problem is too hard: we are violating the data invariant here, which also rules out the last option. That's not a good option. Now for the two remaining choices and this is a tough choice. We could have the assertion in here, assert fault, which would be nice but keep in mind that this could actually also be turned off, for instance, when invoking the Python interpreter with -0. However, in this particular situation, there's no reason to turn this assertion off because well, if it gets executed then it raises an exception immediately and if it does not get executed then everything is fine, meaning that by turning it off, we don't get any savings. So the better option is simply to raise an assertion exception plain and simple and thereby to document that something is going really wrong in the program. This will not hurt performance at all obviously because it will only be triggered when all other checks already have failed. However, if these were the production program, you may wish to come up with some global try catch block in the program or in Python this would be a global try except block that would catch all assertion exceptions and then pop up a dialogue saying "Oh we encountered a fatal error" possibly offering some recovery mechanism or some ability to save the progress that we've seen so far such that the user is not confronted simply with all of his work disappearing. Sometimes during debugging, there are situations where you want to ensure that some property hold throughout the entire execution. Let me give you an example for that: suppose you have a very big Python program with dozens or even hundreds of modules, and suppose that due to some designed decision, there is one global variable called flag which is accessible from all these modules and all of these modules could possibly change the value of flag; they read flag, they write into flag. But now, you have a run in which flag should not be set and yet it is set. And you have all these places in the program where the variable could be set, but you don't know where this is. What you want to have in this situation is a check that is executed at every moment throughout the execution of the program and which monitors the moment in which flag is set to true. >From a previous unit, you may remember the tracing functionality in Python. By using the sys.settrace function, you can specify a function, here traceit, that it will be executed after every single line is executed in your program. And we can set up this traceit function such that after every line, it checks the status of the global variable flag and aborts execution as soon as flag becomes true. As any such tracing function, it returns itself such that it will be executed with the next line. As soon as you have such as checker in place, the flag will be checked after every single line of execution and you will be able to monitor exactly where the flag is set. And this of course is true for arbitrary data structures. If you do have some data structures, say store, again which can be accessed from lots and lots of places, then a call say to a store.checkRep will ensure that store is consistent for every single line that's executed. Just be sure to turn this off while store is executing its own methods because, while these internal methods are running, store would naturally violate its own internal data invariants. In Python having arbitrary read and write accesses all throughout the program towards a single location is rather uncommon. There are languages; however, where any function that execute can access any part of the memory on purpose and sometimes by accident. This is particular true for languages in which memory is under control by the programmer. Languages such as C and C++. To illustrate why languages such as C and C++, memory management can be a huge problem, let me show you a short piece of C code that can cause lots and lots of trouble. In C and also C++, you obtain memory by calling a special function which is called malloc for memory allocator. When you invoke malloc, it gives us an argument the number of bytes that you'd like to have for your data structure. Strictly speaking, it's the number of characters, but let's skip that. The effect of this statement is that you'll be getting a chunk of bytes and you will get a pointer that points to the beginning of this allocated memory. What you actually have here is an array of 10 characters. You can write these characters one by one, for instance, in location no. 5, let's store an x. In C, as on other languages, the array start with an index of 0. So 5 is actually the sixth element in the array. We can also read elements from the array. For instance, we can access the 10th element of the array and store the resulting character in y. The only problem is there is no 10th character. The array has only 10 elements and this would be the 11th element. The behavior of a C program at this moment becomes undefined. That is anything can happen. It is possible that some completely random value is stored in y. This is actually the most likely outcome. It is also possible that the program immediately stops. It is also possible that your program all of a sudden transforms into an adventure game and allows you to explore the colossal caves. That's a very unlikely outcome but keep in mind that in one release of the GNU Compiler, the GNU programmers had some fun and stating-- "Well, in undefined behavior, anything can happen, so let's simply go and make this a game." This was not very appreciated by the programmers. Needless to say, this behavior of C and C++ programs opens the door for many many ways of abusing the system. You may have heard buffer overflows, which exploit precisely this flaw in C and C++ where people not only read but write beyond the elements of an array in order to supplant malicious code and select locations of the memory. This opens the door for all sorts of interesting hacks, of course. How can one detect such errors? What we need is a system invariant that continuously checks the boundaries of an array against reads and writes. What a tool can do for instance is constantly monitor the uninitialized areas for reads and writes with every single instructions that is, and whenever the program tries to access some system memory that is not allocated, what will happen is that the invariant checker raises an exception or otherwise aborts the program and therefore allows us to detect this kind of error. Tools for C and C++ help you do that, include tools like electric fence, which is precisely that, places these blocks in front and before every allocated block and therefore detects when reads and writes happened outside of these allocated areas. And the second important tool here is Valgrind, which actually is an interpreter for x86 binaries in which also allows us to monitor accesses to non-initialized code for C and C++ programs. Buffer overflows are also the topic of one of my most favorite bug story which was reported by Isenstadt in '97. He once had a system that would work only under very specific circumstances, sometimes it would, sometimes it wouldn't. The crucial parts of memory in this story are two variables, one is the day of the week and the other one is a variable which we will simply call ok. Ok can take a value which is either yes or no. By default in this story, ok would be set to no. However, ok has to be set to yes in order to work. Now, the day of the week in this eight character array would be something say like Monday, and on Monday, the program would not work. It also would not work on Sunday. It wouldn't work on Tuesdays either but there would be one day of the week and only on this day of the week where the program would work just fine. What is this day over to you. So, now for the answer. I already gave you a couple of hints: doesn't work on Sunday, doesn't work on Monday, doesn't work on Tuesday either. However, what happens on a Wednesday? You see the word Wednesday doesn't really fit into this day-of-week array. What we get here is a simple buffer overflow, so the okay variable gets overwritten with Y, which actually is the exact value that the program needs in this variable in order to work. So we have the curious situation of a program that works only on Wednesdays. Assume you have this big, big program with no runtime checks at all and you suppose to do the debugging. Where in that program should you start? I would suggest the first thing to do is to define data invariants. This will immediately cover large parts of the program state and catch lots and lots of defects. The next thing is to provide preconditions which check the data invariants, of course, but which also check specific preconditions for the functions at hand. Finally, provide post condition in any method you find suspect. Start with the partial conditions and then you expand them further and further to capture more and more of the correct behavior. Why do we start with data invariants in preconditions? Well, because they usually way easier to write, catch lots of bugs, and because we only care about whether a method works or not if it actually gets the correct argument and gets a correct state to begin with. On top of that, if you are using C or C++, run a system invariant check up. You should run the system invariant check up for the simple reason that it will check for all sorts of memory corruption, and if your program does have any issues with memory corruption, then all of these other assertions are totally nulled because they will come up with random results. Running a tool like Valgrind can detect lots and lots of memory issues and all it takes is to run your program once, with Valgrind enabled. A colleague of mine recently transfer from academia to an oil and gas company and he was in charge of testing. He introduced the first assertion ever in their code and immediately, this one single assertion, uncovered dozens of bugs. The engineers were amazed. They have never seen anything like this before and this is an experience. Well, I'm not exactly sure whether you should have that experience too, but if you come along with code that has no assertion at all, start adding some and you will be surprised. Why should we start with data invariants? This is a quiz. First option, they cover much of the state. Second option, they are frequently checked. They form implicit pre- and post conditions because data invariants should hold at the beginning and at the end of each public method. Final option, they provide helpful documentation because they document exactly how the data structure is organized in which assumptions programers should not violate. Check all that apply. Over to you. Here comes the answer. Well, that actually was a rather boring quiz. Of course data invariants, if it's large data structures, cover much of the space. If these are data structures that are frequently used, they're frequently checked with every single use. Data invariants make up an important part of all sorts of pre- and postconditions. Finally, the assertions you write down here provide helpful documentation for programmers. Coming up with proper precondition, postcondition, and invariant can be a hard task. Other tools that help us doing that-- this is the idea of inferring invariants, having a tool that automatically provides data invariants, preconditions, and postconditions. I'm going to show you how such tools work, and we're going to explore how to build one. The Daikon tool by Michael Ernst and colleagues is a tool that dynamically detects invariants from program run. The idea is that you do have a set of executions. Here is one execution, here is another one, and here is five executions. What Diakon does is it analyses these runs and checks whether there are any properties or variables that hold for all observed runs. For instance, it could determine that the variable x is odd whenever the function f is being called. How does Daikon do that? The first thing it does is it gets traces. A trace is a listing of all functions that were called and all values of all variables. Very much like the sys.settrace() function in Python, which we have used to trace the programs. What Daikon has built in is a so-called pattern library. A pattern library of possible invariants. Here is such a pattern--$1 == 0. Here is such a pattern--we have a place holder which equals 0. Daikon now takes the trace, looks at all the variables, and checks which variable satisfies this pattern. That is it replaces the placeholder with every single variable found in the trace. So it checks whether x == 0, whether y == 0, whether z == 0, and so on. And only those patterns that match are being retained. Those that don't match are eliminated. X and y are not 0, then they're eliminated. This one is retained. Daikon checks these patterns for every single invocation of a function and only retains those that hold for all invocations of the function, which means that over time the set of instantiated patterns becomes smaller and smaller. It's like a sieve. At the end, if this instantiation is found to hold for all invocations of a function then it's actually retained and finally reported as an invariant. Daikon has hundreds of these patterns. It tries them all out one by one on all variables at all invocations. Yes, this takes a bit of time. So, if by applying the pattern library, Diakon has found out that x is always odd when f is being called, it reports this as an invariant. Let me show you how this works on an example. Here is a square root function. It takes x and returns the square root. Let's assume we invoke this with the values 2, 4, and 16. When we invoke the square root with the value of 2, we could infer that x has a value of 2 and eps has a value of 10^-7. However, these patterns would also be instantiated: x is smaller or equal than two, and x is greater or equal than 2, because these patterns also hold for the values that we observed. In the next iteration, we invoke the square root with the number of four, and now, the invariant of x always being 2 is eliminated. What we get now, however, is that x being less or equal than 4 still holds. We can do so by merging the earlier invariant with a new one. And x greater or equal than 2 still holds for the new value. When we invoke the square root of 16, we now retain the invariant that x is less or equal than 16 and greater or equal than 2, and this is what we get in the end: x is between 2 and 16, and eps is always 10^-7. For the postcondition, we get similar ranges for the return value. The return value is between the square root of 2 and 4, which is the square root of 16. However, what we also get is that the return value squared is equal to x, and we get this because Daikon has an appropriate pattern for that, namely a pattern where the multiplication of any two variables equals a third variable, and this is instantiated with a return value, again with a return value and with x and this pattern then holds for all runs-- at least for all runs with integer numbers. If we put in floating point numbers, then eps also comes into play, because of rounding errors, and then this pattern would no longer be discovered. So whatever Daikon can produce is constrained to the pattern library it has, but if you add more patterns, then you'll be able to discover more properties, which will take Daikon a bit longer, though, to discover them. Still, even with a perfect set of patterns, approaches like these are dependent on the actual numbers that are being fed in there. What Daikon produces is relevant for all the runs observed, but we all know that the real precondition for the square root does not have specific range constraints on x except that x should be greater or equal than 0. Likewise, the return value of square root is not necessarily between the square root of 2 and the square root of 16, but it can actually be anything that's, again, greater than 0. So, tools for dynamic inference of invariants can work well if they do have a good test suite in the beginning. How can we get the correct ranges for x and the return value? By invoking square root with a value of 0? By invoking square root with a value of 1? By invoking square root with a value of maxint, where maxint is the highest available integer? Or by invoking square root with a negative value? Hint: you need multiple invocations. Check those which you need to get the correct ranges. Over to you. Thank you very much, and now for the answer. The answer is if we want to have correct ranges for the input variables as well as for the output variables, we need to provide these ranges when calling square root. In our case, the correct range for x is 0 to maxint. If we invoke square root with 0, and if we invoke square root with the maximum integer, what we're going to get is that x is greater or equal than 0 and x is less or equal than maxint, which is actually the correct range. Likewise, we've learned that the return value is always greater or equal than 0, and it's always less or equal than the square root of maxint. If we invoke the square root with a value of 1, this won't change a lot. We will slightly expand the range, but we won't get the 0 in here. This is actually not needed. If we invoke square root with -1, then we will actually violate the implicit precondition. Let's hope that square root then will actually fail such that Daikon can then deduce that if we invoke square root with a negative number it fails. These two are the correct values. This unit is about simplifying. Why is simplifying important? Think about a plane catching fire as it leaves the runway. When you think about how could that possibly have occurred, you want to figure out what the cost of failure is. In order to explain how the failure came to be, you want to come up with the minimum of factors that explain how. So, here is our plane with passengers, luggage on board, entertainment system, air condition, and whatnot. Now suppose you had a means to turn back time and repeat history at will. Or suppose all of this were running in a simulation. You could try again and again to see whether the failure occurs or not. Then you could try to simplify things. You could, for instance, remove the passengers and see whether the failure still occurs. Oh, it still occurs. Remove the flight attendant and see whether the failure still occurs. Well, the plane still catches fire. Remove the seats. The plane still catches fire. Remove the air conditioning system and repeat the whole thing. And remove the entertainment system, remove the luggage, and you see that none of this actually is important. Whatever you remove, the plane still catches fire. You can go and remove the airport, and even with all the airport removed, everything happens as before. But as soon as you remove this little metal strap that's lying on the runway, then all of a sudden your plane takes off just as normal. But if that strap is there, then your plane is on fire. What we're going to explore today is how to simplify failures-- that is, finding what is relevant for the failure and what is not-- and how to do this automatically-- that is, automatically finding out the cause of the failure. This being a course on software debugging, let's take a look at a software problem. Mozilla is the corporation that produces the well-known Firefox browser. In the early days of the browser, Mozilla was still in beta, and they got lots and lots of problem reports from their early customers. Here is bug report #24735. You already see from the number that they got very, very many bug reports. What would happen here is that a user would start the Mozilla browser, would go to the website bugzilla.mozilla.org, which actually holds all the bug reports, including this one, in Mozilla. The browser would render the Bugzilla webpage. There would be a link "search for bug" and users would click on it. Then a number of menus would appear say selecting the operating system, priority or severity, where you would be able to enter or select search terms before pressing the big search button. If the user would now, with this Bugzilla page open, select "Print" from the menu, then the entire browser would crash with a segmentation fault. Such bug reports were not exactly uncommon in the early days of Mozilla. There would always be some HTML input that would come from the web server and then be rendered as a webpage which the browser couldn't properly handle. Let's take a look at the HTML code for bugzilla.mozilla.org. This is what the HTML code of bugzilla.mozilla.org looks like. It actually is almost 900 line of HTML, so you feed this into the Mozilla browser, select print, and it crashes. But why? In the early days of Mozilla, problem reports such as this one and failures such as this one came in quicker than Mozilla programmers could possibly simplify them or even look at them. But then the Mozilla product managers had a clever idea. They started the Mozilla Bugathon, which meant that volunteers would go and simplify test cases. This was a very simply process. What you needed is a text editor such as this one, and then you would go and remove parts of the HTML page, say this, for instance, run this through Mozilla, and see whether the failure still persists. And again, remove some part of the input and see whether the failure still persists, again and again and again, and you repeat this until you have a page left, a small page, in which everything is relevant for reproducing the failure. If you do this and keep on removing one part after the other, then all of a sudden you end up in a situation where the failure no longer occurs. After you remove some part the failure no longer occurs, what do you do next: do you put the part you just removed back in and try to remove other parts, or do you keep only the part that you just removed and remove everything else, or you keep on removing until the failure occurs again? Over to you. Thank you. Here comes the answer. The answer, of course, is you put the part back in, and you try to remove other parts. Why is that so? Well, if you remember our discussion of the scientific method in the first unit, what you just had is you set up a hypothesis on why the failure occurred. You removed some part, and you observed that after removing this part, your hypothesis on what the failure is no longer holds, so you go back to your earlier hypothesis and remove other parts. Trying to keep only the part and remove everything else makes little sense, and keep on removing until the failure occurs again also is very unlikely to happen. To make this process of simplifying even more rewarding Mozilla would offer rewards. For five problems simplified, a volunteer would be invited to the Mozilla launch party. For ten test cases simplified, he or she would also get an attractive stuffed animal the Mozilla mascot. For 20 test cases one would get a t-shirt signed by the grateful engineers. Let's see whether I can earn a stuffed animal. Let's remove this part. We invoke Mozilla. Failure still is there. We remove another part, invoke Mozilla, failure is still there. We remove another part, see how it gets simpler and simpler, and the failure still is there. Mozilla still crashes. Now I'm going to remove this entire Bugzilla page to a single line. If I now invoke Mozilla and invoke print, Mozilla crashes. How can we simplify this further? Should we try to remove NAME equal operating system? Remove the multiple attribute? Remove size equals 7? Remove select? Or remove the angled braces? Pick those three which have the highest chance of still reproducing the failure. Over to you. Hello again. Well, that was a bit of guesswork. But I think you might have guessed the right answer. We can do so by elimination. If we remove the angled brackets, the all of this HTML tag is going to turn into regular text. This will create a very different behavior in Mozilla, and therefore we are probably not going to reach the code which originally caused the failure. The same goes for select. If we remove the attributes--that is, name equals operating system or multiple or size equals 7--then our input will still be a select tag with no attributes at all, and this will still trigger the original failure. So, you see that in this big HTML input, only eight characters are relevant for reproducing the problem. This will certainly give us nice reward from the Mozilla developers. So, we have seen how to reduce an 800-line input to a single line and even reduce this single line even further to 8 characters that reproduce the bug. This is also what the Mozilla volunteers did, and within one night the first volunteers already had earned their t-shirts. This is obviously simpler than 800-lines of HTML code, but what is it that makes something simpler? Or when do we say that something is simpler than something else? What is it that makes something simple? First, there is the burden it takes to understand something. In debugging, an input that takes me 10 seconds to understand is simpler than an input that takes me 10 minutes. This is why one line of HTML is better than 800 lines of HTML, because the one line of HTML has a far smaller burden to understand why something went wrong. Second, the less burden it takes to explain something, the simpler it is, or maybe the other way around. If something is simpler, it takes less effort or explain something. If I say Mozilla cannot print a select tag, then it is way simpler than saying Mozilla can't print this Bugzilla page with the following 800 lines of code. Plus, a short explanation also points me directly to the code in question. If I know Mozilla can't print a select tag, I can go directly to the code in which Mozilla handles printing select tags. All of his, however, refers to humans. It is the human burden we're talking about. Things that are simple are easy to explain and easy to understand, but what is complex to one can be simple to others and vice versa. The same thing can be explained in simple terms and also in complex terms. In debugging, simplicity eases our lives from the very beginning. As an example, consider bug reports. If you ever maintain a big piece of software with dozens and dozens and maybe even thousands and thousands of users, you may have gotten feedback from users--that is, bug reports. In the early 2000s, I coauthored a debugger named GNU DDD. It was pretty successful, so it had many users and I also got lots and lots of bug reports from the field. These bug reports--even in these early times--of course came electronic mail. But I'm drawing actual snail mail, because it's cuter. Some of these bug reports would just contain the required information, such as the steps needed to reproduce the problem as well as the observed behavior, but then there would also be people who would send in their entire programs that they were currently debugging. Some would send in their entire home directory or even the contents of their entire hard disk just in case this would be needed to reproduce the problem. And others would send in just the one liner, "Your program crashed," which was less information than I would need. What you'd like to have in a bug report is information that is relevant. Here relevant means if it's different then it changes the behavior. Of course, when users are submitting bug reports they don't necessarily know whether their information changes the behavior, so to stay on the safe side they supply more information than would actually be required, but in order to have a simple bug report or a simple test case, we want information in there to be relevant. This calls for a quiz. When we simplify a test case, just as we did before for the HTML input, then everything is what? Simple, correct, relevant, or elegant? Over to you. And now for the answer. The correct answer after all we've discussed, of course, is relevant, meaning that every single information that remains in the input, changes the outcome if it is changed as well. If we say that Mozilla crashes with these 8 characters, removing any of these characters changes the outcome. Therefore, all of these 8 characters are relevant for the failure of Mozilla. Of course, this input also is simple, but this is not necessarily always the case. You can have very, very complex failures where lots and lots of information is relevant but still hard to understand and therefore not necessarily simple. Everything is correct. I don't know what a correct input is. This cannot be generalized. Elegant? Well, I'm not sure what an elegant test case is. There is elegant code and elegant fixes, but this does not necessarily apply. So, the correct answer is relevant. To put it with Steve McConnell, the goal of simplifying the test case is to make it so simple that changing any aspect of it changes the behavior of the error. Or, in our words, to make it so simple that every aspect in it is relevant. There is also a nice quote which is usually attributed to Albert Einstein. "Everything should be made as simple as possible, but no simpler." This of course calls for a simple process for simplifying. Namely, for every circumstance of the problem, check whether it's relevant for the problem to occur. If it is not relevant, remove it from the problem report or the test case in question. To see whether you got all this, let's have a quiz. Namely a word problem first stated by French novelist Gustave Flaubert in 1843. A ship sails the ocean. It left Boston with a cargo of wool. It grosses 200 tons. It is bound for Le Havre in France. The main mast is broken. The cabin boy is on deck. There are 12 passengers on board. The wind is blowing east northeast. The clock points to a quarter past 3 in the afternoon. It is the month of May. Now, for the question. How old is the captain? Over to you? Okay, that of course was not a real quiz. In fact, Flaubert's word problem is most famous for providing all these details, which all turn out to be completely irrelevant. Actually, none of this information helps in answering the central question. In debugging, we need to do the same. We need to found out which information is relevant and eliminate all the irrelevant information. Now, for a real quiz. Why is simplifying test cases important in debugging. Is this so because a simplified test case is easier to communicate? Or is this so because a simplified test case usually means smaller fixes? Or because it usually means smaller program states? Or because it usually means fewer program steps? Check all that apply. Over to you. Now for the answer. Let's check all that apply. A simplified test case is easier to communicate. This is correct, because if it's simpler, takes less time, easier to understand. A simplified test case usually means small fixes. Unfortunately, that's not the case. It may mean faster fixes, but it does not mean smaller fixes. The fix will be the same whether the test case is in its original complex form or whether it's simplified. A simplified test case usually means smaller program states. Yes. If the input is smaller, then all the data structures that represent the input will be smaller as well. And, a simplified test case usually means fewer program steps. This is correct as well, because it takes fewer steps to process the simplified input. And there we go. Now let's get back to our simplification scheme. Obviously, simplification works, but if you as a human do simplification, you may not be entirely happy with it. That is because manual simplification is tedious and boring. That is tedious, because you have to run these tests manually again and again and again. Second, it's boring, because it's a rather mechanical activity without great intellectual challenge. I mean, we just keep on stripping away some input, invoke the program again and again and again. Who really wants to do this? Fortunately, there's an answer to that. We do have something that helps us specifically for tedious and boring activities. This is called a computer, because we invented automatic machines precisely to relieve us from tedious and boring tasks. So we're going to hand over the process of simplification over to a machine, resulting in automatic simplification. Can we do that? For automatic simplification, we need two ingredients. We need a strategy that does the simplification for us-- basically that tells us how to simplify. And we need an automatic test that checks whether our simplification succeeded or not. Let's first start with the automatic test. For the Mozilla example, we would need a test that takes HTML input, starts the Mozilla web browser, feeds the HTML input into the browser, and then finally checks whether Mozilla fails or whether it succeeds. All of this needs to go into our automatic test. There is a number of ways to set up such tests. For instance, we could start Mozilla and then replay appropriate user events and see whether it crashes or not. We could also write JavaScript programs and have Mozilla execute them automatically. That is, we'd right a piece of JavaScript that would read the HTML input and then check whether this causes a crash or not. For this course, however, we'll set up a simulation that will do the very same thing, but actually without involving Mozilla. We will simply write a little Python function that does all the testing for us and all of this automatically on the machine. What we want is a function called "test" which takes a string s--this would be the HTML input-- and if this HTML input s would cause Mozilla to crash, tests should return fail, otherwise, it should return pass. We're simply going to search whether s contains a select tag. For this we set up a simple regular expression, which says if there is a some substring in s which consists of an opening tag, select, and then an arbitrary number of characters which are not a closing tag--that is not a greater sign-- this is expressed by this caret over here, meaning not the greater sign-- not the greater sign and then star meaning 0 or more instances of that and finally a closing tag. So we can have a less than sign, select, then anything except for a greater than sign, and finally a greater than sign. This is what makes an HTML tag. If this occurs within our string, we return fail. Otherwise, we return pass. This regular expression requires a function from the RE module, but we first need to import this. So, let's see what happens if we feed in a just a regular HTML tag into the testing function. What we get is pass, because there is no select tag in here. Let's expand this a bit, and now let's actually add a select tag in here as in this example. Now we see that our output changes to fail, because in here we actually do have a select tag. So this is what our testing function does. It simply simulates the behavior of Mozilla. Normally, we would pass this input to Mozilla and then see whether it crashes or not, but in order to keep things simple, we're simply having this simple testing function, which checks directly for what is supposed to be in the input. So far for the automated test now comes the simplification strategy. What could such a strategy possibly look like. In their book, The Practice of Programming, Kernighan and Pike describe a simple binary search simplification strategy. Throw away half the input and see if the output is still wrong. If not, go back to the previous state and discard the other half of the input. This is something that Kernighan and Pike meant to be used in a manual setting, which actually requires your editor in which you are editing the input to have a good undo facility. However, we can also translate this binary search approach directly into a program which does the simplification for us. We assume there is a function simplify() which takes an input, and based on the test automatically simplifies it. We assume that the test we're passing to the simplify function already fails. Since we want to test the two halves of the input separately, we need to split it into two parts. We're figuring out what the length of the input is, divide this by two, and we split the input into strings s1 and s2, which go up to this index, and start with this index such that s1 plus s2 concatenated still result in the original string. Now we test the first substring. If it's fail, then we proceed simplifying this first substring. What should we do if the first test does not fail? Well, I guess we then must go and work on the second string. How do you do that? Over to you? So, here is the answer. The second substring is actually being handled exactly like the first substring. If its test fails, then we go into simplify this substring instead. If neither substring fails, then we return the original string. So, what happens if we take this big HTML input and run it through our simplification procedure? Let's find this out. We click on "Run." You see that the simplification algorithm correctly returns select, which actually means that it has been able to very nicely simplify this complex input into just these eight characters. Let's take a look into how this actually operates. I'm inserting a print statement in here, which gives us a representation of the string as well as it's length, so we can see whatever simplify has been working on and down here we can see how simplify() operates. It first takes the entire string, then it takes the first half, tests this, fails again. Of this it takes the first half, fails again. Takes the first half, fails again, and there we go. With a technique like this, we can automatically simplify any input. All we need is an automated test that tells us whether the program in question passes or fails. Unfortunately, at this point our strategy is not yet perfect. We'll have to refine it a bit, because it doesn't work well for all inputs. What happens if we put in another HTML input--that is select, foo, and end of select? What do we get as a simplified input with this function? So, here's our quiz. What's the result of simplify() here? With this input: &lt;select&gt;foo&lt;/select&gt; Is it a string consisting out of the eight characters &lt;select&gt;? Is it a string consisting of &lt;select&gt; and the first two characters of foo? Is it a string consisting of &lt;select&gt; and all of foo? Or is it the entire HTML input? Over to you. And now for the answer. Well, we can find this out by ourselves simply by running this in the IDE. Here is our input: I click on Run and you can see that the binary search simplification does not simplify as much as one could imagine. In the beginning we had this 20-character string, which would then be reduced to this 10-character string just by splitting this in half and now if we split this again into two 5-character strings, none of the two halves will actually contain a select tag. So both will pass, and therefore we'll stick with the original string, which in this case simply then is &lt;select&gt; followed by "fo." So, for the quiz, '&lt;select&gt;fo' is the correct answer. So let's illustrate once more what happens if we take this HTML string and put this through our binary search simplification process. Here is our HTML string, and we first check the first half and see if it fails. Yes, this also fails. But now we are splitting this string into two substrings, namely this one. It passes. Now the second one, and it passes as well. In this case the binary search algorithm is stuck, because the first half doesn't result in a simplification, second half doesn't result in a simplification either. So, we stop here and simply return the entire string, which is what binary search does. As humans, of course, we have a good idea on what to do next. What we would do is,well, we could for instance go and cut away smaller parts. For instance, in here, we already would see the structure &lt;select&gt; and fo. We could, for instance, go and remove the "fo" because it's different from the HTML tag. A computer may not recognize these structures, but what a computer can do still is to try to remove smaller chunks. That is, rather than trying to remove entire halves, we would try to remove, say, quarters or eighths of the input until we're down to removing single characters. What we do in this situation is we increase the granularity, that is, we take away smaller parts more frequently until we end up removing single characters such that we really, really get an input where every single character or every single part is relevant for reproducing the failure. This precise strategy is called delta debugging and here's how it works. So, here's how delta debugging works: first, we split the input into n subsets where initially n has a value of 2. Second, if removing any of these subsets fails, proceed with this very subset. This is what we have for our input. We split the input into initially two subsets, and if any of these subsets fails, as in here, we proceed with this very subset, otherwise, we simply go and increase the granularity, that is, we double the number of subsets and we go again to step one. Let's try this out on our input over here. So, at this point we already tried out removing the second half of the input and removing the first half of the input. Neither removal of these subsets would fail, so now we have to increase the granularity by coming up with now four subsets. This is a 10-character input, so each of our subsets will have between two and three characters. For a change, let's try removing the first two characters. Well, no select tag, no failure. We put them back and remove the three characters. Still no failure, which gets us an "s ct" tag. No failure. Now, again we remove the next three characters--ct and the greater tag. No select tag, no failure. Now we remove the last two characters. At this point, we have the select tag by itself, and again we have a failure. What should we do in this case? Should we now go and start again with two subsets, that is, setting n to 2 or should we go and set n to the number of subsets we had before minus 1 because there's 1 less? Experiments show that it's best to stick with a granularity you once have achieved, because then you won't have to track back through all the levels of granularity you already have seen. So, we set n to n-1, which means that in our case we would have three subsets. However, we don't want n to be come less than 2, because otherwise we'd just have one subset. So, we set n to the maximum of n -1 and 2 such that n can never fall below 2. So, at this point, our input is just the select tag. It fails, and the number of subsets is 3. What is it that delta debugging would do next according to this description of the algorithm? And what is the next test that delta debugging has to conduct? Is this the select tag as a whole? Is this the select tag with the second half removed? Is this the select tag with the first 3rd removed? Or is this just ct&gt; that is the last 3rd of the input? Over to you. Now for the answer. At this point, n is equal to three, so we need to split the the input-- that is, the select tag--into three subsets. We now have eight characters in here, so the first subset has either two or three characters. This is not the right answer, because we now need to remove these subsets. This is the entire set. There's nothing removed in here. Over here there is not a 3rd removed but an entire half, namely the second half. Therefore, this is wrong as well. Over here, however, we have the first 3rd removed. That is, the less than sign and the s have been removed. This could possibly be a next test for delta debugging. Other alternatives could also be the less than sign, the s, and the e, which have been removed, but this option is not in here. The last option is just testing the last third of the input, and this means that not only 1/3 but 2/3 are removed, and this is also not in the description of the algorithm. This is the next test that delta debugging conducts. Removing any of these subsets will result in a passing test, which means that we again have to increase the granularity by doubling n in here. Now we split the input into six subsets, resulting in six more attempts to remove individual parts-- all of them will pass -- and then we have to increase the granularity again, which would be 12. But now our input only has 8 characters, and we can't split an 8 character input into 12 subsets unless these subsets would be empty. So we're going to fix this a bit by setting up this minimum function over here. We make sure that n cannot grow beyond the size of the input, which means that in our case n would now be 8. This is the final stage of delta debugging where it tries to remove one character after another, but all of these removals result in passing test cases, so what we get in the end is this very string &lt;select&gt; and we have now shown that every single character in here is relevant, because removing it changes the outcome of a test from fail to pass. So, what delta debugging gives us in the end is an input --a simplified input-- where every single part is relevant. At this point you may wonder how many tests does delta debugging actually take? You can see that at the very end when every single character has to be removed individually, the number of tests of course is proportional to the length of the simplified input. In the worst case--that is, for very pathological examples-- the complexity of delta debugging is squared with respect to its input. However, there is a nice situation in which delta debugging simply becomes a binary search and is as efficient as a binary search. So, when does delta debugging need a logarithmic number of tests? Is this when all tests fail? Is this when there is always 1/2 that can be removed and fails? Is this when all tests pass? Or is this never? Check all that apply. And now for the answer. Let's check each one by one. When all tests fail that's an interesting situation. It's rather unlikely, but let's assume this actually happens. Then we split the inputs initially into two separate sets, removing the first subset fails we proceed with this subset. N remains 2. And we do this again and we split again the input into two subsets and two subsets again and two two subsets again. This is exactly like binary search described by Kernighan and Pike, which is logarithmic. The next answer: where there always is a half that can be removed and fails, this is the same situation, expect that we may have to remove the first half first and getting a pass. But still in terms of complexity it's still logarithmic in proportion to the size of the input and therefore this is a nice situation in which delta debugging is very, very effective. This is actually one of the strings of delta debugging that if you can split the input into two halves and one of them fails, then it behaves like a binary search. By increasing the granularity later one, then it's no longer as efficient in the number of tests, but is very, very thorough by trying to remove small parts one after the other. It's like a binary search in the beginning--super efficient, and then it's as thorough as trying to remove every single part after another. When all tests pass, then delta debugging does not need a logarithmic number of tests. That's more like a linear number of tests. And never--that's not the case, because in these two situations, then indeed the number of tests is logarithmic. This is the answer. Now that we have seen how delta debugging works on paper, we should actually go and implement it as a Python program. Here we start implementing the delta debugging function. Formally, this is the delta debugging algorithm for minimizing, so we call it ddmin for delta debugging minimizing. It takes an input, and it takes a string as a parameter. This is the string to be simplified. We could make the ddmin even more generic by also passing the test function as a parameter. Then we could use it for arbitrary tests in arbitrary context. For this exercise, however, we'll simply use the hardwire testing function, which you already have seen. And we start with the assertion that, again, testing the entire input should fail. We have a variable n which sets granularity. As described before, this is initially 2. We now set up a while loop, which loops while the input still has at least two characters. If it has one character, then there's nothing left to simplify. We define the length of the individual subset by simply dividing the length of the entire input by occurring granularity. We have a loop guard in here. This variable will be set as soon as some tests fail. The variable start is a cursor over the input which tells us where to start chopping away individual parts of the input. Here precisely we do the chopping. Start tells us where to start chopping away things. That is, everything up to start should be included in our test. Then beginning from start plus the length of the subset, we also want this to be included in our testing input. What's missing in here, of course, is a string starting from start with the length subset_length. This is what we test, and we check whether the result is fail. That is, if after chopping away a subset the test fails, what do we do then? We set our input to the complement. That is, we keep on working with a now simplified string. We decrease the granularity by 1 but don't let it fall below 2, and we set the variable some complement is failing to true, because we need this to step out of this loop, which we do right away. If the test has not failed, then we have to proceed to the next subset, which we get by simply increasing our start cursor, which now points to the beginning of the next subset. This is something we do until we have reached the end of our string. Now, at this point we have gone through all the subsets and either some complement has failed. Then this variable is true. So far so good. We simply keep on working with these new reduced, simplified inputs and a decreased granularity. However, all of these tests pass. What should we do? Remember the third step in the delta debugging algorithm. We now need to increase the granularity but must make sure it doesn't get over the length of the total string. I will leave it to you as a programming exercise to complete this code. Over to you. Okay, welcome back. Let's finish coding this off. In our original description of delta debugging, we have to increase the granularity at this point. That is, n becomes 2n but must not become longer than the current input we're working on. Our working input here is s and must not become bigger than the length of s. However, if this works we simply double it. If we have reached the total length of s, and then we return the simplified input. Here again we have the HTML input for which the binary search simplification didn't simplify enough. Let's see how delta debugging fares instead. We click on Run. You see the output now is simply , showing that delta debugging has managed to really minimized everything away in here. Now for a quiz. How many calls of the testing function-- that is, without assertions--need to comment this out-- does ddmin require in this run-- that is, for this input. Please provide your answer over here. Now, over to you. This can be determined easily. All we need is a counter in the testing function. I have extended the test function a bit, such that there would be a test counter up here. With every invocation the test counter will be increased by 1 and so that we see what's going on behind the scenes I'm printing out the current value of test_count-- that is, the number of the test that is being executed-- and printing out the string that we're testing, and I'm printing out its current length. I'm also printing out the result--fail or pass. Now let's see what we get when we're executing this. You see this is the initial test. This is coming from the assertion. First test, select of foo, 20 characters long--failing. What you can see here is that delta debugging quickly progresses in its binary search phase from 20 to 10 to 5. Well, 5 doesn't work so it sticks with 8. In the 10th test already it already has determined the minimum. However, it still tries to remove single characters in here to make sure that the final result is really relevant. Overall, it takes 22 tests, but since the first test is due to the assertion, we overall have 21. The correct answer therefore is 21. What we have seen now is how delta debugging can take a specific test and use this specific test in order to automatically simplify a fairly complex input to an input where every single character is relevant. So far, however, we need to set up a specific test function for every failure. Is there a way to come up with generic tests that can be used for arbitrary situation? Here again, we have a testing function. And we want to turn this testing function into something that is more generic. For instance that simply checks whether some assertion has been raised. Here, I have converted this testing function into something way more generic. All that's left in here is the call to the function I want to test. In this case, remove HTML marker. If this function raises an assertion error--for instance because the built-in assertion failed, then the test fails as well. If there is no assertion that failed, then this simply return pass. This test function now can be adapted again and again for arbitrary function as long as they have an assertion or some other run time check that makes them fail. Here is a remove HTML marker function from the previous units and it has an assertion to make sure that the result no longer contain any HTML mark. And this is precisely what we can test in here. If the assertion fails and the test will fail and if the assertion does not fail, then the test passes. Remember this notorious input "HTML to regular text HTML marker." Which would not work at all when removing HTML marker. Indeed, this would make the assertion fail because the output would still contain HTML marker. Now let's go and use delta debugging to minimize this input such that every character in the input is relevant for producing the error. For a remove HTML marker, what is the minimal substring of this string including the double quotes that triggers the failure and as a hint it's just two characters long--over to you. Aanswer to our quiz--we can find this out by running delta debugging. Here we are again in our editor, and now we simply invoke ddmin on the HTML input to find out which part of the input actually causes the program to fail. You press on run and here we go. It takes 10 tests until delta debugging has simplified the input just to two characters and this is a quote and a less than sign. These two characters already sufficed to trigger the failure. This simplified input dramatically reduces the number of steps it takes us to go through the remove HTML model fashion. We have a quote--if a quote becomes set, this is the error. Since quote is set, tag does not become true and the quote gets added to the result, which causes the assertion to fail. Just two characters, fewer steps, smallest state--all of these are advantages of a simplified input. And just for the record, the correct answer is double quote and less than sign. Automatic simplification works especially well in situations where the tests are also generated automatically. If you have taken a testing class recently, you may have heard about fuzz test. Fuzz testing is a technique where you generate random inputs for a program or an API. It's a common testing method to see whether something breaks. To show you how fuzz testing works, let's first write a fuzzer, which is a function which generates such a random input. So here's our fuzzer function. We want to have a function that generates strings of up to 1023 characters filled with regular printable ASCII symbols. So first we determine the string length we'd like to have and for that, simply take a random number-- random.random returns a random number between 0 and 1 and multiply this with 1024, so we get a length of up to 1023 characters. And here we have a simple loop over the string length. We generate a character which takes an arbitrary value between 32 and 128 by using again the random function multiplied with 96 adding 32. We add the character to the out string and return the out sting at the end. Let us see what fuzzer produces--press on run and here we have a non-string filled with random characters. If we have a look at it again, we're going to get another random string, and here we go. I think what happens if I take such a random string and paste it into a function that accepts say a filename or an email address or a URL, all of these functions work well even when confounded with seemingly totally random input. In the age of the Internet, any input that is under control by a third parties can easily look like this, and fuzz testing is indeed one of the attack vectors of people wanting to penetrate into your system. Now, let me introduce you to the mystery test function. This is a function which returns either pass or fail. We would like to use fuzzer input in order to make it fail. Here's some fuzzer input, which we can pass to mystery test. Let's see where they passes or fails on that. Well, for this input, it passes. We now, however, is again a fuzzer generated input for which mystery test now fails. The first input passes, but the second input fails. So you wonder what it is in this input that makes a function fail. How can you find this out straightforward--by using delta debugging in order to simplify the first input such that you know which part of the first input actually causes mystery test to fail. And here is programming exercise, find the minimal input that causes mystery test to fail, and for this, use a fuzzer such that you'll find input that causes mystery test to fail. And once through fuzzing you found such an input, use delta debugging in order to minimize the input. Over to you. Here we are again, we have a mystery test function and what is it actually in here that causes the input? What is it that causes mystery test to fail? Well, we can again use delta debugging to figure this out. These are implementation of delta debugging. I'm going to extend this such that the test now comes as a parameter. And then, we can invert ddmin with an appropriate test just as we'd like. And I'm going to invert the ddmin with this input for which you already know that it causes mystery test to fail and then passing mystery test as a parameter. Let's see what's in here that causes mystery test to fail. Press 1, we press 1 and we see a single character, a single dot suffices. A single dot suffices to cause mystery test to fail. So what we have here is classical set. First, we use a fuzz tester to test a program and then we repeat and repeat this until the program breaks. Then we feed this into delta debugging which again runs the program again and again until we get a minimal failing input such as a single dot. Such minimal failing inputs are very important as it comes to convincing other people to fix a bug. Suppose your program for testing is a huge SQL server. Suppose you use a first tester to generate extremely long and complex SQL query that you sent to the SQL server. And then, the SQL server is going to choke on one of these super, super complex inputs and then you say to the SQL developers, "Hey, I have this very, very long and complicated SQL query and I can use this to break the server," but the developers are going to tell you, "Ha, a big query like this one, never ever happens in practice". So maybe this will crash our server but it's really not the top priority right now. We'd rather care about real queries, thank you very much. So what you'd do then is you run this big, big complicated query to delta debugging which then will give you some minimal failing input. which now will be way, way smaller and actually look like something so simple that it can actually happen in practice. And all of a sudden, the SQL developers will take you very seriously because all of a sudden this looks like something that could happen in practice. True story from Microsoft. Delta debugging can be used to simplify other domains rather than just input. For instance, it can be used to simplify changes. Let me illustrate this. For instance, it can also be used to simplify changes to your code. When would such a thing be needed? Well, simple situation. This is you and this is your major web hosting service The building or infrastructure and lots and lots of third party open-source libraries and since you want to keep your service up-to-date with respect to security patches and everything, you have to update these libraries on regular intervals So you're getting patches that is large changes to a code base on regular intervals. This is the same as upgrading the libraries to the newest version and the changes between versions that is the patches, can easily contain 10,000s of source lines. Here comes in new patch, patch 3 also with 10,000 source code lines. All of a sudden, your infrastructure fails after applying the patch. What is the cause of the error. This is an instance of a problem that's more generally called, yesterday, my program worked, today it does not--why? Changed something--as a result of the change, something no longer works and you want to figure out what is it in that change that makes the program go wrong? Again, we can use delta debugging to simplify this. Assume the patch that causes the failure can be broken down into several subpatches. For instance, these 10,000 of changed lines would be different locations of the program and each of these locations that means a separate place to patch or a different subpatch. The idea would now be that maybe a subset of these subpatches would already suffice to create the error. In other words, we would simplify the patch to find a minimal subset of the patch that creates the failure. Let us simulate the situation in our IE. Let us assume the individual subpatches all identified by numbers from 1 to 8. Again, we write a testing function that simulates the behavior of the system. We assume that if the patches 3 and 6 are applied, then the failure occurs. If patches 3 and 6 are not applied together, then the failure does not occur. Thanks to the beauty of Python, all of our infrastructures we had previously set up for string also works for lists. Let's see what happens if we test the entire set of patches. We see it fails, which is not much of a surprise because 3 and 6 are both in here. If we test the empty list instead, that is no patches applied, then the test passes. Again, we print out the list we're testing, its length and the result. Now, we can again invoke delta debugging with the set of patches and the above test function. Let's see how this works. We invoke ddmin to see whether we can simplify this list of patches to see which of these patches are relevant. Press on run, here we get the result, and the final result is patches 3 and 6 suffice to cause the failure. Well, that's something we knew all along, didn't we? If your original patch would have been 10,000s of lines long, then chances are that delta debugging reduces this to just two lines or another very small set of locations in your code for which you didn't know. But if you change these two lines, the failure will occur and if you don't change them, the failure will not occur. This approach for determining the culprit for regression has also been named the blame-o-meter, as a means to know who to blame. Where who to blame means which places in the code are to blame but it can also mean which patches are to blame and it can also mean which programmers made these changes which are now to blame. The idea is that such a scheme can be used in any situation in which an old version work and a new version does not. So when you have any regression test failing and the regression test does exactly that comparing the results of the new version against the old version, you can and possibly should run delta debugging in order to figure out which lines in the code are actually responsible for the failure. The only problem with delta debugging is that the test function is somewhat elaborate. It first must apply the patches to the code, then build the code, then run the test, and this again and again and again, which implies that your build facility must be automatic and of course, your version control system should also be able to produce exact and small differences between versions. There's even a version control system where such a scheme is already built-in. This is called git and the command git bisect will give you the exact change between two versions stored in git such that the old version will not have the failure and the new version will have the failure. So this does something very similar to delta debugging--pointing out the culprit, which has been changed such that the failure occurs. And now, for a quiz, assume that delta debugging gives you a failure-inducing change and you now go and undo the change that is revert to the previous version for these locations as returned by delta debugging. What is the effect--is it that the program builds normally, the failure no longer occurs, or is it that the problem is properly fixed. Check all that apply. Now for the answer--if I undo the failure-inducing change as returned by delta debugging, can I actually build the program. Yes, I can because this actually tested by delta debugging that the alternative makes the program passed, so the program should build normally and also the failure should no longer occur. Whether the problem is properly fixed though is another thing. I may have fixed the failure in question, but by undoing these changes, I may have introduced new bugs. In particular, these changes appear they're probably made with some specific feature in mind and the very least that will happen is that I'll lose the new feature that would be introduced by these changes, so I can't really say that the problem is properly fixed. I need to come up with a change that includes the new features or at least the effect or at least the intended effect of these original changes, but avoids the bad effect mainly the failure--so this is the final answer.. On the more philosophical level, when delta debugging returns, is not only to simply itself but is also to cause for the failure. What is the cause anyway? Assume i have a window and here comes a ball flying to the window and the window breaks. You would think later that the ball is the cause for the window to break. But why is it the cause? This is so called counterfactual definition of causality which is the most used in our context. It works as follows: We have two events, and A comes before B. Say A, is the ball flying towards the window? and B, is the window shattering? We now say that A causes B, if B had not occurred if A had not occurred. Applied to our example, we can say that the ball causes the window to shatter if the window had not shatter if the ball has not arrived which actually is true. With the ball, the window shatters. Without the ball, the window does not shatter. And since the ball precede the shattering, everything is in place to say that the ball caused the window to shatter. If this sounds complicated, we haven't seen much. Causality is one of the most disputed subject in Philosophy. Some philosophers are even saying that causality does not really exist. It's just an illusion. For our purposes; however, that is within debugging, this counterfactual definition does a good job. First, we have the ball then we have the window crashing forward. First, we have the defect and then we have the failure. So why does delta debugging returned cause? Simply because there might be a large set of events that all could or could not contribute to the failure. The sun is shining. The clouds are in the sky. There's a nice tree along. And what areas that none of this is relevant for shattering the window. Only the ball by simplifying the scenery towards a single element that is responsible for the test to fail, delta debugging returns the cause. This notion of causality is important in debugging because when you fix a defect we want to make sure to think first. We want to make sure that the defect is actually error, that is it is wrong. We also want to make sure that the defect causes the failure if A is the defect and B is the failure. We want to make sure that if the defect hadn't been there then the failure must not occur either. And these are the two things we need to show when we fix a defect. In order to make sure that the defect is an error, we must show how to correct it and to show that the defect causes the failure, we must show that the failure no longer occurs after we have changed it. It is important to show both the defect is an error and the cause in order to avoid situations in which a defect only is an error but does not cause the failure or that something causes the failure but is not an error. Let's look at the these. First, you can have an error that's not a cause. Here's you, looking at your program and you look at the program and find tons and tons of errors. Here's one, here's one, and there's one and you fix them and you think you're done, but you don't know whether any of these actually fixes the problem because it may well be that these errors you find that none of them actually causes the failure. It simply means that after any fix you need to verify whether your fix actually did the trick and better yet before the fix, you should be able to precisely predict that your change will actually fix the failure. The second category is causes that are not errors. If you think of our Mozilla example for instance, we found that the select tech in the input causes Mozilla to crash when printing, however, this is just a cause, it is not an error. This is perfectly legal html input and at the very least some input that Mozilla should normally be able to cope with. In particular Mozilla should not crash on any input. What we have here is the cause, but we haven't find the error yet. The error is somewhere in the, code probably in the piece of code that handles the printing of select tags. We can have errors that are not causes and causes that are not errors. The goal in debugging is to find errors that actually caused the failure. Finally, the general concept of a cause can be misleading. To think about the window shuttering, we said it was the ball. It's right. But, it could also have been the window maker. If the window maker had not made this particular window then the window would not have shuttered and if the house builder had not built this particular house with the window in it then the window would not have shuttered either. For a single effect, there can be multiple causes, and all of these are valid according to our earlier definition. This is where the concept of an actual cause comes in handy. An actual cause assumes and changes as little as possible, but it changes the effect. If we think about the causes we have discussed, a ball shuttering the window does not change much except for the window. The window maker not producing the window makes of a larger different because then we would have the house with one window missing for quite some time, which would change a lot. Parts of the house would get very wet for instance, would be cold or hot depending on the climate. If the house maker had not built the house, the change would be even bigger. So, what you want is an actual cause that assumes and changes as little as possible, which precisely what the ball is in here. For debbugging the same hole, you want to have a cause that assumes and changes as little as possible but still produces the effect, and that's precisely what delta debugging gets you starting with the general cause the entire input reproduces an actual cause namely a subset of the input in which everything is relevant for producing the effect. Here's a quiz. I invoke the GNU compiler, and it crashes. Which of these are general causes for the failure? Is it me because no me, no invocation, no crash or is it Richard Stallman, the founder of the GNU project because without Stallman no GNU compiler no crash or is it oxygen, without oxygen, no Richard Stallman, no me, no compiler, no crash, probably also no world as we know it, no computers, no electricity, interesting, or finally is it a bug in the compiler because no bug no crash. Check all that applied. Back to the answer, well formally speaking, all of these are causes because if I don't occur, the crash doesn't occur. If Richard Stallman doesn't occur, crash doesn't occur. No oxygen, no crash and no bug, no crash either. But now for the extra question, which of these is actual cause for the GNU compiler crashing? There's is only one. Go and pick it. Again, that should not have been too hard. The answer is, of course, a bug in the compiler. We can check all of these possible causes by the changes they have. No me would have some effect--well, you wouldn't be able to listen to this course right now. Richard Stallman--no Richard Stallman would have a huge impact. The GNU project is the source for so much great open source software out there. We would have a very different role. Oxygen is of more important. I can't even start to begin to imagine how our world would look like, and of all these courses, fixing a bug in the compiler has by far the smallest set of assumption and also the smallest effect because apart from fixing the bug and apart from fixing the crash, nothing else changes and therefore, this is the actual cause. Before we get to today's homework, let us see how we can make delta debugging faster. We must realize that delta debugging is a rather dumb algorithm where dumb means it's rather economical in terms of assumptions. It assumes nothing about the input structure, which requires little, but on the other hand, it gets the job done. The interesting thing is that the more knowledge about the structure of the input you put into delta debugging, the more efficient delta debugging becomes. Let me illustrate this by an example. When faced with a 20-character input, delta debugging would have to split this into two halves in the beginning. But since delta debugging doesn't know about the structure, the division will be right in the middle of the input. If delta debugging knew, however, that the input was html then it may come up with the split that would occur at the boundaries between htlml tokens and actual texts something like this for example. And if you repeat this now for the first substring, you will see that we were able to simplify the input much faster because now our simplification follows the structure of the input. Let me illustrate this with our example. Here again, we have delta debugging as is, and we're applying it on this very input as we just saw. When I press on run, we cannot see the individual runs. And in the end, one character after one character is being removed until we finally see after 29 tests and one assertion we see the final result, a simplified input select. How can we make delta debugging aware of the input structure? We have already seen that delta debugging works on lists as well as on character strings. So what we could is we could split the string into individual substrings according to the input structure and have delta debugging work on that list of elements instead. This is such a list of elements. First, we have the select tag, then foo, and then the end of select. Now, what we might have to do is we have to adjust our test function such that it will merge the individual parts of the list back again into a string, which we do up here. We're using the Python join function, which takes all the elements in the list and concatenate them with the first string as separator. In our case, an empty string. So the effect of this is that all the elements that are in s right will be merged together. And then, in the reconstructed entire string, again, we search for the selector. In our initial setting, it took us 30 tests until we minimized the input. So, on characters, we have seen it took ddmin 29 tests again excluding the assertion to simplify the input. How many tests will it take on a list of tokens that is on this list of tokens? And here a token is a substring with a decisive meaning. You can try this out for yourself or just estimate. Is this 4 tests? Is this 6 tests? Is that 12 tests? Or is that 29 tests? Over to you. To answer this question, all we need to do is press on run and you'll see that now we only require four tests. You see how delta debugging splits the list, quickly finds a sub-list that is just two elements, checks each of them and finds the one element that causes the test to fail. The correct answer here was 4. Welcome back to our course on debugging. Today's topic is tracking origins, meaning how to start with a failure and track back origins of the failure until you find the defect that causes it. But before we get there, a bit of a story. If you are a fan of Sherlock Holmes, you may recall this famous scene in "A Scandal in Bohemia." Here we have Dr. Watson and Sherlock Holmes sitting by the fireplace. They haven't seen each other for some time, and Sherlock remarks that Watson has put on 7-1/2 pounds since they last met. And Watson says, "How do you know?" And Sherlock answers, "I see. I deduce it." Not only does Sherlock find out that Watson has put on 7 pounds, he also remarks that Watson has been getting very wet lately and that his servant girl is clumsy and careless. Again, Watson asks, "How on earth did you find that out?" "Oh, it is simplicity itself," Sherlock says. What Holmes sees is that on the inside of Dr. Watson's shoe there is a small number of scratches, which he can see in the firelight. He deduces that these scratches have been applied by somebody who very carelessly tried to clean the shoe, possibly Dr. Watson's servant. And since the shoe had to be cleaned with such a rough device, this likely came from mud through which Dr. Watson had to run while getting very wet. This is how Sherlock Holmes, from a single observation, can deduce all the events that led to this observation. I should add that, faced with this explanation, Dr. Watson exclaims, "When, Sherlock, I hear you give your reasons, the thing always appears to me to be so ridiculously simple that I could easily do it myself. Though, at each successive instance of your reasoning, I am baffled until you explain your process." What Sherlock Holmes reconstructs in his reasoning over here is a cause-effect chain. First, Dr. Watson ran across mud; therefore, the servant had to clean the shoe. And therefore there were scratches found in the shoe. In debugging, we also have such a cause-effect chain: from the defect to the infection--that is, an error in the state-- and finally to the failure, which can be observed by a user. The difference in the Sherlock Holmes story, however, is that Sherlock makes just one observation and deduces everything that must have happened before; whereas in debugging, so far, we have assumed that we can actually observe what has happened before in the way we assume that we have perfect control over our program and therefore can observe everything. But what happens if this information is not available? If all we know is that the failure occurred but we have no history, or means to observe what was going on? This is where the Sherlock Holmes method of debugging comes into play. For we must, like Sherlock Holmes, think backwards from our observations to find out what has really happened. In debugging, we start with the failure and think backwards what could be the possible cause such that we can finally discover the reason. What we're going to discuss today is how errors propagate in a program, that is, from one variable to another, because these very dependencies then help us in reasoning backwards. And this is our second topic: how we can trace back these dependencies from an observable failure to an earlier infection and then to a defect in the code. And for this we're going to use the Sherlock Holmes method which, of course, is: "Elementary, my dear Watson." But first, a quiz. Do you like Sherlock Holmes? Here are your options. Is it more like: "I'm his greatest fan!" Or: "Isn't he, like, outdated?" Or: "Actually, I find him annoying." Or: "Sherlock who?" Hint: The answer to this quiz will neither determine your grading nor will it determine the next step. Enjoy. And now for the answer. Well, of course, there is no real answer to that quiz. Any answer is fine in here. However, a few comments on the individual options. If you're a fan of Sherlock Holmes in solving puzzles and all, this unit is right for you--as is debugging, for that matter. If you think that Sherlock Holmes is outdated, well, try and read one of the old stories and see how outdated they really are or check out one of the most recent adaptations of Sherlock Holmes. If you find Sherlock Holmes annoying because his way of reasoning is obnoxious or showing off his super-smartness, that's probably correct, but I think that Arthur Conan Doyle actually wanted to portray Sherlock Holmes as somebody who's not entirely likable. Finally, Sherlock Who? Here's something to discover for you. Enjoy. Needless to say, Sherlock Holmes would make an excellent program debugger if he were living these days. The problem in front of us can be stated as follows: "Something impossible occurred, and the only solid information is that it did occur. So we must think backwards from the result to discover the reasons." Quote from Kernighan and Pike, "The Practice of Programming" Thinking backwards from the result. This is the art of deduction applied to debugging. Let me illustrate this--how this works--using a familiar example. Here again we have a function to remove HTML markup. Just as a reminder, what this function is supposed to do is to take HTML input such as this one: a foo text enclosed into HTML tags, one to switch on bold rendering and one to switch off bold rendering, and to turn this into a text in which the HTML markup has been removed. This makes use of a number of variables: Tag checks whether we currently are processing a tag. That is, during these three characters, tag should be true. And only if we are not in tag mode do we actually add the characters to the out variable. And the out variable at the end is being returned. So while we are processing this code, tag would be true for these three characters. Then tag would be false. F-O-O would be added to the output. Finally, we would have 4 more tag characters which would not be added. In the end, what we get is this string: foo. Now let's practice the art of deduction on this example simply from the observation that this final assertion has failed. So all we know is that indeed this code can produce an output in which HTML markup is still present. This is the observation from which we start. Let's go and think backwards how the assertion possibly could have failed. What we know is that out contains an opening tag that is a less-than character. The only place where this character could have been added to the out variable is here. In order to reach that line, a number of conditions must be fulfilled. All of these conditions cannot have failed. Whereas this one must have failed. Let's focus on the tag variable here. What was the value of the tag variable when the less-than sign had been added? Over to you. Here comes the answer to the quiz. Since this here was executed and this condition up here is that tag cannot have been set, we can deduce that tag must have been false. But there is more that we can deduce from the code. Since the character was a less than sign, we have a condition which specifically checks for less than signs, up here. However, this condition apparently had not turned out to be true. Therefore, we can now deduce the value of the quote variable. What was the value of the quote variable here? Over to you. And now for the answer--c was a less than sign. This part of the condition is true, but this line was not executed. Instead this line was executed. And therefore, this whole condition must have been false. Since it was false, quote must have been true because that's the value which turns the entire condition to false. This is the correct answer. So, at this point, we know that the quote variable must have been set to true at some point. Because initially it was false, but we already know that if it had stayed false, then the less than character would never have been added to the output. So we now have deduce that quote must have been set, and there is only one place in the program where quote is set, which is down here. So we can deduce that this long condition, which checks for quote, must have held at some earlier point before the less than sign. Why must it have been true at some earlier point? Well, because at the moment when symbols left was the less than sign then the condition did not hold That simply assumed it was the character right before the less than sign then the variable tag had never been set to true. It was false from the beginning and it was false till the very end. If you look at this condition, what was the value of c and again over to you. We have assumed that tag was false all along, which looking at the intention of this condition means that this condition never ever should have turned out to be true anyway. With the eyes of Sherlock Holmes, however, we would immediately deduce that this condition can only hold when the first clause is true--that is c was a double quote. Now, it seemed that this condition can only hold when c was in double quote. So first c was in double quote and then c was in less than sign. >From all of this information, we can reconstruct the input that was passed over to remove HTML markup. It must have been something along the lines of first a double quote and then an opening tag. Between these two events, c being a double quote and c being an opening tag, there could also have been other characters before. But in our deduction, none of these matter. For us, it is sufficient to know that indeed there is an input, which can cause this assertion to fail. And second, while we are doing the reasoning, we actually also stumbled across the condition that was wrong here. >From the line of our reasoning, we can now deduce that if this condition had not been true, then for this input the assertion would not have failed. By adding appropriate parenthesis here, we can make sure that this input will not cause the assertion to fail and thereby through deduction we have found a fix without executing the program even once. And now for a really tough quiz in which you can show how good your deduction skills are. If I have applied this fix with this parenthesis, can this assertion ever fail? If yes, provide an example. If no, write down the individual steps of your deduction and let's see whether this is elementary, my dear Watson. That must have been a tough nut to crack. The correct answer is No. The assertion cannot fail. Let's see why this is the case in the next section of the course. Indeed, with this fix applied, the assertion can no longer fail. This is how the argument goes. In order to for the assertion to fail, the out variable must contain a less than sign. Since this is the only place where characters are being added, tag cannot be set. The quote variable; however, must be set, because otherwise, we would have gone in that first branch over here. But then quote can only be set in this very location, but this can't be the case because from step 2, we already know that tag cannot be set, and therefore, we have a contradiction between the 2, which means that the assertion will never fail. Quote erat demonstratum meaning what was shown to be proved. This was a nice proof, wasn't it? I must confess that I don't entirely trust my own proof. It's very much like in the quote of Don Knuth who once said beware of bugs in the above code. I have only proven it correct, not tried it. So in order to check whether my code now would be really correct with respect to this assertion, I went and used an automated deduction tool. The tool I was using is named Pex. This is a experimental tool for Microsoft research, which is a very, very thorough tester, which gets possible inputs from the program code. So Pex does the same deduction steps as Sherlock Holmes would do or as I would do or as you would do in order to come up with inputs that cause a built in assertion to fail. And my idea was, if Pex doesn't find anything, then chances are my program is really correct. Let's go and see Pex in action. The Pex tool has its own webpage where you can try it out. It's called www.pexforfun.com. Here you can enter a code which you want to be tested automatically by Pex. And sense Pex works for the languages C sharp, visual basic, and F sharp, I converted our remove HTML markup function into C sharp. Here you see the function, I have renamed it to puzzle in order to match the Pex for fun conventions. Here is the precondition. The string that has passed must not be now, which is a special C sharp thing. And here we have the loop over all the individual elements, setting tag and setting quote, or adding the characters to the result. At the end, we have an assertion. The result must not contain a less than character. Let's first try this out on the buggy version and let's see whether Pex can find the bug. We click on ask Pex, and now you see a number of interesting inputs, and these would all be inputs which cover various parts of the code. At the very end, what you see here, is an error message. The assertion has failed, and the assertion fails precisely on our buggy input, double quote followed by a less than sign. So Pex has been able, by going through the code, to come up with the exact same failing input as in our deduction example. Now I have gone and fixed the program. The question is whether Pex can now find an input that makes the assertion fail. And again, we ask Pex. And again, it comes up with a number of interesting inputs. All of these interesting inputs cause the program to run properly. That is, despite its best efforts, and Pex is really, really good in detecting errors, Pex has not found a single input which causes the assertion to fail. So for the fixed program, even Pex says this is correct, and I myself also have said this is correct. If you don't believe an automated deduction tool, you can believe me, and if you don't believe me, you can believe an automated deduction tool. Together we create a big deal of confidence in this program. Still, what automated tools can find and what humans can find, is only correct with regard to the specification. And at this point, we do not have a complete specification on what a remove html markup should really do. One of my PhD students sent me this input. Here is the html markup, and here is the actual text. If we feed this into the program, what is the output? What is the result of the remove html markup function when we give it this string? Over to you. We have 2 ways of finding this out. We can either run the whole thing with this input or we can also go through it and figure out what's going on. The interesting thing about this input is that it mixes double quotes and single quotes. I am going to use the green color for section where tag mode on, and I'm going to use the pink color for sections where quote is on. So initially, we are in tag mode. It starts precisely here, and it goes up to the place where we find the quote. While we are processing the quote, quote mode is also turned on. But now we see the second quote in here, and quote mode ends. We are still in tag mode. Now we see another quote, you see where the problem is. We don't discriminate between double and single quotes, and therefore, a string that starts with a double quote can also end in a single quote. See, here again we are in tag mode, but now we also start quote mode. And now as we are at the greater than sign that is the end of the tag, we are still in quote mode, and therefore, tag mode does not end. And sense we stay in quote mode until the very end, all of this is considered to be part of the tag, and therefore, the answer is the result is the empty string. This is the correct result. What you can see now is that, although we have formally proven that our code cannot violate this assertion, there still may be other issues, which simply aren't properly specified yet. So any proof can only be as good as the specification it relies on. Which, of course, again is a call for good assertions. Because if you do have good assertions, you get good specifications, and this will not only help you a lot in debugging, you will also improve your testing, and as we have seen, it may even enable the use of automated deduction tool. But now, for a programming exercise. It really, really annoys me that this function isn't entirely done yet. Please go take this function and fix it such that this html markup is properly removed. I promise this will be the last time we are going to be fixing this function. Over to you. Here again, we do have our remove html markup function with the parentheses already in. Here is the input that doesn't work properly. Note the usage of a backslash in order to escape the single quote, which otherwise would be seen as a delimiter around that string. We can print this out to see whether it contains the right value, and you see it has the don't in here exactly as in our example. In order to address this issue, we need to record which quote character we actually had and which we need to check for again. For this we need to change these lines in here. What we're going to is, we are going to use the quote variable to store the quote character, you see? Now quote is either false or it contains a double quote or a single quote, and this makes it impressibly true. Now this does only work when the quote variable is false, and this is how we start a quoted string. When we see that very quote again, we set quote to false, and this is what should now finally fix the problem in this html input. Then we press run. And now the single quote and the double quote is properly handled, and what we get is just the text without the html markup. After this brief diversion, let's get back to the actual deduction process. When you are reasoning what could have happened in a program, we again see the program execution as a succession of states. And then we reason backwards from what we observe to what could have happened beforehand. This reasoning, this backwards reasoning is actually structured along the ways that earlier events in the program could have influenced later events. The most important concept when reasoning backwards is called data dependency. The idea is as follows: We do have 2 statements, say A and B in the program, and now A writes a variable, which is later read by B. In our example up here, A may have written this variable, and this will then be read by B. Then we say that these 2 statements are data dependent on each other. B is data dependent on A because there is a flow of data from the statement A to the statement B. This concept of data dependency allows us to trace back the possible locations in the state as well as the statements that cause them back within the program execution and therefore to isolate the possible causes for an infection and later the failure. Let us take a look again at our remove html markup function. Here I'm going back to the earlier version, which is sort of fixed. Now let's take a look at this condition up here. We want to know where the core value could possibly have come from To do so, we follow the data dependencies. What are the statements that this statement is data dependent upon? To do so, we will look at the places where quote would be written. And which also would be executed before this statement will be executed. For instance, this statement would be data dependent on that earlier statement. Because here quote is being written, and here quote is being read. Therefore, the statement A up here sets the state that B later on keeps on processing. If you want to understand how B behaves, we follow the data dependencies to A and find that this is one of the statements that sets the state that B depends upon. Now for a quiz. Which are the other statements that B data depends upon? Is this for C in S, is this elf of C equals the greater sign and not quote, is this quote equals not quote, or is this out equals out plus C? Check all that apply. Let's check the variables that are read by B. This is the variable C and the variable quote. Lets first check C. Where is C being written? C is being assigned here in this for statement. Therefore, this statement indeed is what B is dependent upon. Next one down here. Here we have C and quote again, but they are only being read and not written. Therefore, these do not influence the data at B. Down here we have quote equals not quote. This is obvious. Quote is being set, and therefore we have a data dependency. Finally here, we have out equals out plus C. Here the out variable is being written, but nothing in here reads the out variable. Therefore, there is no data dependency. When we are reasoning about a program, our reasoning follows these dependencies. In this case, if we want to know where C comes from or where quote comes from, we follow the dependencies to the specific locations, and we can do so again. If we are up here, for instance, and we want to know where C comes from, well, C comes from S, and S comes from up here. S first goes down here, then defines C, and then C is being checked. Likewise, if we want to know where quote comes from, quote comes from this statement. So we see again, here quote is being written, here quote is being read. So this statement depends on itself, and quote, again, can either come from this location, or it can come from the original assignment. It is precisely along these relationships that we structure our reasoning. Where does this value come from, and where was it set? If we take a look at this statement, however, we will quickly find that data dependencies are not enough because the value of quote in here may depend on the earlier value of quote, but we also need to take into account that this statement had to be executed in the first place. In order to be executed, it depends on earlier conditions. This is the concept of control dependency. If there is some statement which controls whether the statement B is executed or not, then B is being control dependent on that very statement. In our example up here, for instance, the condition in A controls whether statement B is executed or not. Therefore, B is control dependent on A. Again, such dependencies guide our reasoning. We find that B was executed, and we want to know why it was executed, after all. In order to figure out why it was executed, we follow the control dependencies. Here is a quiz. Which other statement does B control depend upon? Is this for C in S? Is this the first if statement? Is this the first elif statement? Or is this the last elif statement? Check all that apply. The for statement up here, indeed, controls whether this entire block is executed or not. Therefore, it also controls whether B is executed or not. If this condition holds, then tag is set to true. If it does not hold, execution resumes for the remaining conditions. And therefore, this if condition also controls whether this is executed. Same thing happens for the other condition. The last condition, however, only controls whether the character will be edited out or not. It has no influence on whether code is being set or not. Therefore, this statement does not influence whether B is executed or not. B is not control dependent on it. If you look again at our earlier steps in the deduction process, you will find that each of these steps precisely follows either the data dependency or a control dependency. Let me use red to show data dependencies and green to show control dependencies. We start with the failing assertion. Here, out has a less than character in there. Where does out come from? We can follow it back, using the data dependency, to this statement. We can also follow it back to the earlier statement, but that is not so much of interest, because here, out is just empty, and with an empty string, this assertion would not have fired. By following back the control dependency, we find the tag was false. This is step 2 in the deduction process. And we also know that C in here was a less than character, and since this up here controlled whether the addition to out was executed or not, we also get a control dependency up here. This statement was not executed, and this condition controlled it. Therefore, we can deduce that quote was true. This is step 3 in here. So we now find the place where quote was set. This was down here, again, the data dependency, and since this was executed, the governing condition with a double quote character must have been true. So we start with a failing assertion, follow back the data dependency to out, follow back the control dependency to quote, follow back another data dependency to quote, and then, finally get to the governing condition in here, which also is the place where the defect is. If you keep on following data dependencies and control dependencies starting from a given statement, the set of statements that you get, that is, the set of statements that could have possibly influenced the statement in question, is also called a program slice, also called backward slice starting from statement, contains all of the statements that the statement S in question would transitively depend upon. You obtain a backward slice by starting with a statement S, and then following all the dependencies until you have reached a fixed point. The interesting thing about slices is that anything that is not in the slice cannot have influenced the stage at the statement S or whether S is being executed or not. Therefore, the slice can act as a filter. Let me illustrate this by an example. As an example, let's take a look at the statement, "tag is true". This is our statement S. Now, let's see what's all contained in the slice of tag. First of all, there is no direct data dependency because no variable is being read in here. However, this is control dependent on this F condition. This if condition, again, is dependent on the character C, so this is also part of the backward slice, and this condition up here reads from the variable S, which is passed as a parameter up here, so this is also part of the backward slice. Now let's go to quote. Where does quote come from? It is being initialized up here and set down here, so we end up in the place where quote is being set. Now, can you keep on doing this? Completing the backward slice of S? One by one, check all statements that are part of the backward slice. Okay. Lets complete this thing. We are here in quote equals not quote. This is control dependent on this earlier statement, which again reads from C, that we have seen the dependencies of C before, but it also reads from tag. So this is dependent on all the places that tag is being set, and then again this is up to this one up here, and then again whether tag is being set or not, implies a control dependency towards the governing conditions. What is usually more interesting in the backwards slice itself, is what is not contained in the backwards slice. That is, the statements that are not checked. What you are seeing here is the statements that are not part of the backwards slice, are statements that refer to out, setting out, checking out, and returning out, but nothing of what happens to out actually has any effect on tag at this point. And this is precisely what the backwards slice gives us. It tells us if we want to know why this statement was executed. Well, we know it can't depend on out, because there is no dependency of tag towards out in any way, or out cannot influence the value of tag in any way. When there are backward slices, there are also forward slices. A forward slice contains all statements that depend on a specific statement. As an example, let's come up with a forward slice of the initialization of the out variable. Very obviously, the final return statement is part of the forward slice because it reads from a variable which is set over here, namely, out; therefore, it is data-dependent on this earlier statement. And, therefore, it's part of the forward slice. Now for the quiz: Which other statements are contained in the forward slice of S? Check all that apply. That was fairly easy. What we need to check is simply statements that read the out variable up here, which is down here: out=out+c and the final assertion. We can now check whether anything else depends on the execution of these statements. The only one that's in the loop is here: out=out+c, but none of the other statements up here ret something from out. So if you want to know how this setting affects things downstream, the forward slice will tell you, "Well, it can only affect these 3 lines down here." Anything else in between will not be affected by the value of out. The interesting thing about slices is that they can be determined automatically. That is, a slicing tool can automatically determine backward slices of individual statements and forward slices of individual statements. Such static slicing tools, therefore, can help you focus on specific parts of the program, telling you the possible influences either off a statement or the possible influences towards a statement. Such slices become even more interesting, though, when we apply them not only to programs but to actual executions. These are then called dynamic slices. These apply to executions instead of programs. That is, rather than reasoning in a program where a value came from, you look at the actual execution, typically the failing execution, and therefore not only know what could have happened but actually see what has happened. The base of a dynamic slice is not the program but a trace instead. A trace lists the statements in the program in the order in which they were executed. So, if line 3, for instance, was executed 4 times in a row, then the trace will contain line 3 four times followed by line 4, followed by line 5, and possibly going back to line 3 in case there's a loop. Within the trace, we can now look again at dependencies. That is, looking at which variables have been read and which variables have been written. And if we find, say, that at the bottom of the trace some variable is wrong, we can now trace back the dependencies in the execution and again use this as a base for debugging when following back the cause-effect chain through the program. The first thing we need for this is a trace. Let me show you how to get traces for Python programs. So here, again, we have our original remove HTML markup function, and what I've written down here is, again, a tracing function which accesses the file name of the code as well as the current line number that's executed, and we print this out on standard output. We record this as our tracing function. Let's use the "buggy" version of the remove HTML markup function here in order to have some more fun, and we feed it with this very simple input, just 2 characters, double quote and less-than sign, which should expose the error. If we go and execute this, then we should be able to see the sequence of lines as they're executed. And we click on Run. What we see here is that first line 4 was executed, then line 6, then line 9, then line 11, line 13, line 14, and this is how we progress through the program execution. Now for a bit of programming exercise. Complete the program which you've just seen such that it prints out the actual code lines instead of just the file name and number. That is, replace the output of file name and number by the actual code line that is in that file at this position. So a very simple way--not really efficient, but it works-- is to open the file, read all the lines which then come in an array indexed by 0, and then we simply take the line number minus 1. In the array, the lines start with 0; however, in our trace the lines start with number 1. And since the lines already contain the required new-line character, we put in a comma at the end to suppress the second new-line character that would otherwise be issued by the print statement. Let's try this out. We run the whole thing, and now with every line the file name should be opened, line should be read, and the one, single executed line should be printed. We run the whole thing, and this is what we get. You see the first tag is false, being executed, then quote is false, being executed, out being set to the empty string is being executed, and here now you can see the entire program one by one as it produces the output. And this is then what makes the actual trace of the program. So here again we have our dynamic trace, now a bit bigger. Remember that the input was double quote followed by a less-than sign. What we get in the end is the output, which is just the less-than sign. In the dynamic trace, we can now, again, follow back the dependency. For instance, from out--out is being read here. Where was it last written? That's over here, so this depends on this earlier statement. Here we have out that's being read. Now in the dynamic trace you can actually follow this whole thing and see where it was last written, again, which is up here. And here we have the character c, which stems from the second iteration of the for loop. And this relates to s, which depends on the input. So, again, we have a chain of dependencies: out going up here, up here, up here, up here, and you can also see this as a cause-effect chain. S had this value; therefore, c became a less-than sign, which was edited out, which finally resulted in the input also having a less-than sign. Again, we can use these dependencies to build a slice. However, now the slice would be a dynamic slice because it is built on the dynamic trace. Starting with the assert statement, we see that out=out+c is being executed, and therefore part of our slice. We see that the initialization of out is part of our slice We see that the for loop is part of our slice. And, of course, when we call "remove HTML markup" where s is being set, this also becomes part of our dynamic slice. So far we only looked at dynamic data dependencies, but of course there are also dynamic control dependencies. Every condition that gets evaluated, that is, executed and controlled whether a statement in question is executed or not, builds a dynamic-control dependency. Therefore, since we executed out=out+c, the controlling conditions are also part of the dynamic slice. >From these conditions, we again get more dynamic data dependencies, which you can look up, again, in our dynamic trace. For instance, here the tag variable, which is being read over here, was last set up here. Therefore, this condition is data-dependent on this initialization. So here's a quiz. Which other statements in the program are also part of the dynamic slice? Is it quote=false? Is it tag=true? Is it tag=false? Or is it quote=not quote? Again, the dynamic trace gives us the answer. We have the variable quote, which is read over here and last set up here. So this becomes part of the dynamic slice. Likewise, we have quote which is read up here, for instance, and set up here. This also becomes part of the dynamic slice. However, these two lines, tag being set to true and tag being set to false, are never executed and therefore the values they set into tag-- therefore, these two statements do not become part of the dynamic slice. In our example, the dynamic slice simply excluded only 2 lines out of the program, namely these 2 lines. This may not sound like much in terms of reduction. In a larger program, however, a dynamic slice can reduce the number of statements to look at considerably. First, they contain only executed lines. That is, lines that are executed don't become part of the dynamic slice in the first place. And, of course, a line that's never executed can't have anything to do with a bug. It may have something to do with a fix, though. Second, they only contain statements that actually influence the output. If your output contains several independent parts, then a dynamic slice of just that part will be much, much smaller than the entire program. In 2010, we did a study on 7 bugs in Java programs and measured how big their dynamic slices would be. To start with, on average only 7.3% of the lines were actually executed, which means that a simple coverage tool which tells you which statements are executed and which ones are not will really focus the search to a very small part of the program. When looking at the dynamic slice, the reduction in size becomes even more apparent because, again, on average the dynamic slice encompassed only 2.8% of the lines in the program. Since there are nice tools available which compute dynamic slices automatically, using such a tool in the first place will immediately tell you which places of the program you can safely ignore because they are not executed or because they cannot possibly have influenced the output with respect to the bug. How do dependencies fit into our model of debugging? Well, that's fairly straight forward. When we see a failure, we see which part of the state is erroneous. Then we track back the dependencies to see which earlier states could possibly have caused that infection. We determine these possible states as well as the locations in the program where they would be caused through the dependencies. So, if we see an error down here, it could have come from here, from here, or from here. In one out of these three, or at least one out of these three, should contain the infection that we're looking for. So we use dependencies to find possible origins for each for each infection. In the second step we use the scientific method to track down infections. We have the choice between three possible origins here. So, we use the scientific method to find out which of these three parts of the state is at fault. We set up an experiment. We make up the appropriate observation, and we gradually refine or reject our hypothesis until we have come up with a diagnosis and figured out which part of the state is wrong. Then we repeat the whole thing back and back and back again. Again, choosing between multiple possible origins, following back the dependencies, and again using the scientific method to track down which of these actually is at fault. Instead of the scientific method, you can also use deduction to rule out specific possibilities. For instance, you may be able to show that neither this one nor this one can possibly have influenced the state under these circumstances. So, the only one that remains is the one up here. You repeat the process until you find a statement whose in-going state is all correct, but where the out-going state is infected. So, how do we call a statement whose in-going state is all correct, but its out-going state is infected? What is this? Is this a cause, a defect, or an infection? Well first and foremost, this is a defect because if a statement produces an infection then it must be erroneous itself, and an error in the code is what we normally call a defect. Since a defect in this situation implies that we can actually fix it such that the infection goes away, it is also a cause, because it causes the infection in question. The term infection, however, only applies to program states. Therefore, it is no applicable in here. Even if you do have dependencies and assertions that help you rule out large parts of state, you may still end up with a multitude of individual origins where a specific infection may come from. While you can rule them out one after the other using the scientific method, the question is of these origins are you going to look at first? The idea is to look at the most likely origins first. The question is what is a likely origin. What possible origin should I look at first? Here are a few guidelines: First and foremost, infections. If you know that some origin is wrong, go for it. Next is causes. If you know that some state causes the failure, because you can change it to another value. Such if the failure goes away, . which is something you can find out through Delta Debugging, for instance, follow it. Next up: Code smells--If you suspect some code to be wrong, or you have gotten warnings from a static checker, go for it. Next up is bug history. If you know that some piece of code has gotten a lot of problem reports lately, chances are what you're looking at will be another problem report. Next up is last changes. Code that has changed recently is way more likely to have errors. So go for code which has recently changed, and go for the state that comes out of this code. Finally, anomalies. If some code shows abnormal behavior before reaching the failure, for instance, by producing a log entry, follow this one as well. Of all these features infections are the strongest, because they are sure indicators of errors, followed by causes. Any cause implies a way to possibly work around the failure. As far as the others are concerned though, this is up to you and the specific project. In fact, this is where your knowledge about the specific project and its domain and the code comes into play. Being able to specifically focus on the most likely sources of a bug makes you a debugging expert for the specific project. To close, we're going to use Delta Debugging to do something pretty cool, namely to give us a full diagnosis on what's happening when the program fails. Our diagnosis will look something like this: First this variable had this value. This caused this other variable to get this other value. Then this third variable gets set to this other value. That is what finally made the program crash. In other words, what we get is cause-effect chain throughout the program, which explains how the failure came to be in all of this automatically. This cause-effect chain may or may not include the infected values, but frequently it does. Even if it does not, it immediately helps you understand how the failure came to be. The basic idea is as follows: If we can change any of these variables such that the failure no longer occurs, then we have found a failure cause. How should we change a variable? After all, variables can take arbitrary values. The rule is not to make them use arbitrary values but to use values from a successful run. That is, during execution we change variables from the values found in the failing run to values found in a successful run. If we can change a variable value such that the failure goes away, we do have a failure cause. Here again is our remove html markup program. If we invoke html markup with s being a single quote followed by a less-than sign, it passes. However, if it's a double quote followed by a less-than sign, it fails. So, this difference in the original input determines whether the run passes or fails. Let's go and execute the program a bit further but stop execution when the loop head is reached for the second time. Now again we can examine the state. The variable s stays unchanged at this point. The character c is still the first character being processed. Which is different? In the passing run, it's a single quote. In the failing run, it's a double quote. The variable tag is false in both cases. The variable quote is different. In the passing run, it is false. In the failing run, it is true. In the passing run, the out variable contains a single quote. Whereas, in the failing run, the out variable is empty. What you see at this point is that four variables, namely s, c, quote, and out, all have different values. You can now imagine that if we were in the passing run, and we would set these four variables to the values found in the failing run, then we would effectively make the passing run a failing run, meaning that these four variables, which defer, make up a cause for the failure. However, it suffices to set only a subset of these variables to the values found in the failing run. Only a subset of these variables need to be changed in order to cause the assertion to fail, and therefore, the entire run to fail. So, here is a quiz. Which of these four variables can be set to values from the failing run to make the passing run fail? Is it s--is it c--is it quote--is it out, or is it a combination of multiple variables? Hint: If you said all four, this is the correct answer. What I want is a minimum set of variables. Over to you. Here again is a remove html markup program. Here's the input which normally passes. If I click on run, what I get is just a single quote. That is, the opening tag was probably stripped, and of course, the assertion didn't fire. I am now setting up a trace function, which specifically monitors for the moment. The loop head here in line 8 is hit for the second time. That's when I'm setting the quote variable to true as found in the failing run. So the question is whether by setting the quote variable to the value found in the in the failing run at this moment, we can already turn the previously passing run into failing run. We figure this out by clicking on run. As you can see, simply by setting the quote variable to true, we now have triggered the assertion. What this means is that simply setting the quote variable to true is sufficient to make the passing run now fail. We thus have identified quote as a cause for the failure. The question is: Can we do such a thing automatically? The answer is yes. We can do so using Delta Debugging. The idea is as follows. At a given location we extract the state for the failing run as well as for a passing run. Then we go and compare the states. What we then get is a set of differences between states found in the passing run and states found in the failing run for each single variable. With Delta Debugging, we can now find a minimal subset of these differences that causes the passing run to fail. This minimal subset then at the given location is the variable that causes the failure. Welcome to the 5th unit in our debugging course titled Reproducing Failures. In the past units we have seen a number of techniques to systematically hand down a failure cause by following back dependencies and applying the scientific method to choose between various possible origins until we find the defect. So far, however, we have assumed that the program fails in the lab and that somehow we would be able to actually access these earlier states or at least run appropriate experiments where we would be able to gather additional. Contrast this with a program failing in the field where all we know is that the program failed plus possibly a few hints from the execution, but all-in-all only a spottier record compared to what you get in the lab. This is where reproduction comes into play. You have to reproduce the failing run from the field in the lab such that you can actually do the debugging. Why do you need to do that? We need to be able to observe the run, because once we have reproduced a failure locally, we can observe and experiment at will. This, of course, is invaluable for isolating the defect. Second, we need to be able to check the fix. Only if we can reproduce a bug can we be sure that we have actually fixed it, namely, if the failure no longer occurs. Reproducing a bug can be way harder than fixing the bug. In programmer's jargon, bug's fall into four categories. First, there is the Bohr bug from Bohr's model of the atom. This is a repeatable bug that many manifests reliably under a possibly unknown but well-defined set of conditions. Next category is a Heisenbug from Heisenburg's uncertainty principle in quantum physics. This is a bug that disappears or alters its behavior when one attempts to probe or isolate it. The next one is the Mandelbug, coming from the Mandelbrot set. This is a bug whose underlying causes are so complex and obscure as to make its behavior appear chaotic or even nondeterministic. And last but not least there is the Schrdinbug, which is MIT jargon coming from Schrdinger's cat thought experiment in quantum physics. You know--the situation in which you have a cat in a box, which may be dead or which may be alive, but you don't know until you open the box. This is a bug in a program that doesn't manifest until someone who reads the source or who uses the program in an unusual way notices that the program never should have worked, at which point the program promptly stops working for everybody until it is fixed. All these bug categories refer to various levels of difficulty as it comes to reproducing the bugs. Here is an example of a bug that was hard to reproduce that I once encountered. I had a C program that crashed all the time. In an extremely simplified version this is what it looks like. If I ran this program normally, it would crash. The assertion would fail. If I ran it in a debugger, however, it worked just fine. Quiz: What kind of bug is this? Is this a Bohr bug, a Heisenbug, a Mandelbug, or a Schroedinbug? Over to you. Now for the answer--Is this a Bohr Bug, which is repeatable under under a well-defined set of conditions? Well, if you include a debugger in the conditions, it could be a Bohr Bug. Normally, we don't do that. Heisenbugs change as the program is being observed, and therefore, this is a perfect instance of a Heisenbug. Mandelbugs have very complex causes--not the case over here. A Schroedinbug appears as soon as somebody looks at this program, and it probably stops working--Not the case over here. I would love to see a Schroedinbug in practice. It's a very interesting theoretical concept. So, how does this bug come to be? What we have here is a C-specialty. This is an initialized variable. When in a C program, we read an uninitialized variable as in here. Then the behavior of the program is undefined according to the semantics of C and also C+ does follow. Since the behavior is undefined, crashing was a perfect option. When executed normally x would simply take whatever random value was on the stack. Therefore, the assertion would fail. However, the debugger would always set the entire stack to zero before execution, and therefore x would take a zero value. Therefore, the assertion would hold. You can even have such distinction between running normally and being run on a debugger on purpose. Some malware actively checks if it's being run in a debugger and then automatically turns itself off. So, if you try to diagnose on malware, the malware may be specifically set up to prevent all sorts of probing. If you suspect a Heisenbug, think what the debugger does differently than the normal execution, and observe the program execution by at least two independent means, say a debugger and logging output or two different debuggers or whatever you have in order to find out what's going on. Let's now go for a systematic reproduction process. First we need to reproduce the environment in which the bug occurs. Second we need to reproduce the steps required to reproduce the problem. Both of these can be abstracted to input. All your program does is dependent on its input. Therefore, if you can reproduce the input, you can also reproduce the execution. With that we already solved the problem. Programs are nothing but mathematical functions. If they have the same input, they will reproduce the same output, right? Why is there actually a problem of reproducing? That's the difference between theory and practice. In theory, there is no difference. In practice, there is, at least that's the theory. Indeed, in practice the concept of what makes input to a program is very variable. We distinguish a number of typical inputs, each coming up with its own issues. We do have static data, user interaction, time of day or general time, randomness, that is randomness on purpose. We have the operating environment. We have schedules, in particular for parallel programs. We have physical influences, and we do have debugging tools. All of these constitute input to the program, and each of these brings up its own issues as it comes to reproducing failures. Let's start with regular static data. In a batch-oriented program, these are the inputs that is the files of documents your program processes. Static data is the easiest to reproduce, because it can be handled as an individual entity outside of your program. If your program operates on files, all you need to do is to ship the data files. If it works on a database, then use tables instead. This includes all data that is under the control of the user. If it's under your control, you probably already know it. So, you care about the variable stuff. For one thing, these are documents under the user's control as well as configuration files, in which user's store their preferences or the settings for they system at hand. Obviously, if you have all the documents and the configuration files that your program operates upon, then it's fairly easy to reproduce the exact problem. However, there's a small catch with that--just a tiny, tiny catch. This tiny, tiny catch can become a big issue. Let me illustrate this with an example. Here again we have our remove html markup function, and here comes an input as one could find it in, say, a user's document. We can run the whole thing with this input and very easily reproduce the failure. In this case, the failing assertion. Now for a quiz: What is the issue that we're having here in this variable html input? Is it that it contains too much data, that it contains confidential information, or is it that it contains invalid html? Over to you. Well, now for the answer--the issue with html input of course, is that it contains confidential information. We really don't want the entire world to know who we are and which websites we visit. So, we have seen that there is indeed confidential information in here. If we center out this data in order to reproduce the bug, this means that the sensitive data may be out of control. This means that developers may get access to it with plenty of possible issues that follow. There are multiple solutions to that. First to keep the data secret, for instance you could sign a nondisclosure agreement with your customer before you ever get access to such data. The second option is simply to anonymize the data. That is, you produce a version of the data with the sensitive information removed. You can also use a technique like Delta Debugging to simply file the input automatically and to keep only those characters that are actually required to reproduce the failure. In the good old days programs would read their input from data files only, which made reproduction an easy task. Modern programs use complex user interfaces, and these make it hard to observe and control the user's input. The standard approach to reproduce user interaction is to use a capture replay tool. A capture replay tool acts like a layer that interposes itself between the input, as provided by the user, and the actual program. These tools can operate in two modes. During recording mode all input coming from the user is passed down directly to the program as is all output from the program. So, the user interacts normally with the program as always. During the capture all input from the user, that is mouse clicks, keystrokes, and similar events, are recorded in the log file. During replay the program executes under control of the capture replay tool, getting its input from the previously recorded events and thereby ignoring regular user input. That is, all the events which are previously recorded and are passed onto the program just as if they were input by the user, and the program produces its normal output. Technically spoken, such tools realize capture replay by intercepting calls to library or system functions, which would normally provide user input. Let's demo this on an example. We have a program which takes commands. >From a command, it processed them, Here is the function input command, which is supposed to accept a command. Process is a function, which then processes the command as entered. This is how such an input function would be implemented in Python. The function raw input outputs the prompt on the console and accepts a line that will then be returned by the function raw input and therefore input command. The idea we are having in here, however, does not support interaction with programs. So we simulate the whole thing by reading commands from an array instead. Here we have a list of commands. With every invocation of input command, the next command is returned from the array. We do so by increasing the index, command index, by one every time input command is called. The first time it returns open. The second time it returns save--The third time it returns quit. Here's the function that actually handles the command. All it does at this point is to print out the command, and if the command starts with q , such as the quit command, then the global resume variable is set to false, meaning that a y loop over here simply exits. Now we have to move the main function at the end such that it gets executed after all the functions are refined. Let's see whether this works properly--It does. Here we have simply the three commands printed out, and with the quit command the loop simply exits. Here's how a capture replay tool works. It replaces the original input command function by a new function, which wraps around the original input command function, calls it, also returns its original value. On top of that it also saves the commands in a log. In our example here, we're simply going to use this list here wherever our new command is to be appended. This is a simple programming exercise, and I'll leave it over to you. Okay. Welcome back. Let's go and implement this thing. Since we're addressing a global variable, we need to declare it. All we do is we take the command that's being input in here and append it to our list. To see whether everything has been recorded properly, we print out saved commands at the end of the execution. Let's see how this works. Here again we do have our three commands as they're being processed. This is the recorded commands, namely: open, save, and quit. If you're running this outside of the web page-- that is, on a command line-- then you can actually uncomment this line and interact with the program itself. [The Infinite Abyss] Let's now go and make this a full capture replay tool, which comes in to mode either replay or record. This is set by the mode variable. If we are in recording mode, then we want to store the commands in variable save commands. We are in replay mode, however. Then we assume we already do have some stored commands in here. Now for another programming exercise. Go and extend input command, such that if it is in replay mode, the commands are being read from the saved commands variable. Over to you. Again this is fairly easy to realize, because we already have a template up here. So again, we have an index to the list of saved commands, which we call saved command index. So, at this moment we are in replay mode. While we are in replay mode the original input command function should no longer be called. All of the commands should now come from the saved commands in here. Let's see how this works. We get the same set of commands now being read from the saved commands variable. You read a program that is outside of the webpage one would now go and store the saved commands content permanently, say in a file for recording and for replaying. One would read this from a file. For programs that take text based commands, recording and replaying input is fairly easy to realize. However, as it comes to reproducing graphical user interaction it becomes hard to decide what of events to record and replay. Suppose you have a graphical user interface for a web browser. You have a captured replay tool that interposes itself between the user and the program and stores all events coming from the user in an event log such that they can be later replayed. The problem is: What is the abstraction level you're going to chose for your events? You could, for instance, go and record mouse clicks to gather with the coordinates of the mouse pointer. This is fairly easy to record. So, for instance, you could go and click here on search at the position, say 100x and 300y. Or, you could enter some text while the cursor is in the position 200 and 50. Click here on the file menu or here down on this roll-down menu. Of course, all of these then go into the event log. The problem is that for replaying, you need to have the exact same position of every element at the very same position on the screen. Now suppose your screen resolution changes. or the font size changes. Then some of the elements will still be more or less in their original positions, but others won't. We knew that the mouse click, which previously activated a particular button on the screen, will no longer work Likewise, if the position of some elements change, then as well, the recorded events will no longer work. Also, timing can become an issue. Suppose you recorded a click on this drop-down menu. Half a second later, when the drop-down menu appeared, you clicked on one of the elements in here. So far, so good. This is what you recorded. Now when you're replaying, and in particular, when you are replaying something from a web page, it may be that there are delays, and the drop-down menu will not appear on time. Then you'd be clicking somewhere completely different when replaying the recorded events, which stipulate that after half a second you'd click in this very position. Finally, of course, the program itself may change. If the search button is moved from down here to up here, then again replaying the event log based on absolute coordinates will click in the wrong position. In all of this, changes in size, changes in language, changes in speed, or changes in the program itself are all obstacles for record, replay, reproduction of graphical user elements. A better option is to have the GUI elements themselves do the recording. That is, when I'm clicking in here the search button knows it has been pressed, and what I am recording right now is not the fact that I clicked in this particular area of the screen, but what is recorded right now is that the search button has been pressed. Likewise, if we enter text in this field, what will be recorded is that in the URL GUI element, the text foo was entered. This way, if anything in the resolution changes or if the position changes, the events will still be tied to the GUI elements, and therefore, our recording becomes independent of the actual rendering on the screen. This of course is also true for rearrangements as long as the identifiers of the GUI elements stay the same, we'll still be able to replay earlier logs even though many details, in terms of the rendering, have changed. The best capture replay tools offer facilities to record and replay at the low levels. That is, mouse clicks and keystrokes, but also facilities to replay at higher levels. That is, directing some input specifically toward some element of the graphical user interface. For web pages, the Selenium Toolkit offers a nice mix of multiple abstraction layers. Check out what kind of tools are available for your specific environment and test them thoroughly before making a purchase. So now for a quiz: What are the risks of GUI capture replay tools? They may capture sensitive information, which should not go to developers. Recorded scripts may not be replayable in different environments. Different because of different local, different resolution, different font size, or different speed. Recorded scripts may be prone to change as the software or the GUI changes. Check all that apply. Over to you. Now for the answer: Well, GUI capture replay tools can capture sensitive information, just as sensitive information may be contained in files. If you enter a password, for instance, on a webpage, and this is recorded, this can even become a security issue. Next recorded scripts may not be replayable in different environments. Yes, we have seen that. Finally, recorded scripts may be prone to change as the software of the GUI changes, obviously. We also have seen that. All three apply. At this point we have seen how to reproduce static data simply by shipping the files over as the data is required by the program. We have seen how to record and replay user interaction. However, all of these are actually instances of the same class of input that is interaction between the program and its environment. The question is whether it would be possible to record and replay this interaction as a whole. A program interactions with its environment by means of function calls. We use function calls to access data. We use function calls to access user interaction or generally anything that comes form the environment. So, what we could do is to record and replay function calls. This way we would have a single mechanism to capture all sorts of interaction between the program and its environment. Our plan would be to record every function call with parameters in a log then later be able to replay them. Let's first do the recording. We've already seen our tracing functions where we can access the individual execution frames of a run. We know that each frame contains the function name as well as all local variables. When a method gets called this list of local variables is actually the list of arguments for the function. If, for instance, I have set up the traceit function, which gets called for every line as always. It also gets called when a function is called. In this case the event variable up here has a value of call. If this happens then we print out the name of the current function, and we print out the local variables, which should be the arguments. Let's try out whether this works. What we see here is we have a call to remove html markup function and s as precisely the value as here in our code. We would now like to turn the function name as well as the argument list into something that can be translated back into code. We want it to look precisely like this. If we turn this into a string that looks exactly like a function call, then we can pass this to the Python eval function, which takes a string and interprets it as code. This is something we will use later for replaying. Here you can see how the string is interpreted, and the result is printed. Here's the function that takes care of the printing of the local variables that is the arguments. Since we don't know the position of the individual parameters in the function, what we use is we print out named arguments instead. That is, we print out name equals value. This will be separated by commas. Now again we will remove html markup with this parameter. If everything goes well, the traceit function to print out this very call with the right argument. This is precisely what happens. You see, we print out remove html markup with the parameter s being the string foo exactly as over here except that we do have a named parameter and not a positional parameter. The same mechanism works for tracing or recalls as long as these are Python functions. So if we call square root of 2, for instance This would also get traced and reported in our output. As you see here, here is the count of square root and x has a value of 2. Now let's assume that we have a variable, which actually has stored all these calls. We could do so by recording it right away while the code is executing, or we could also say store this in a file and read this back from a file. Your job now is to write a piece of code that evaluates all these calls. First this one, and then the other one, and which prints out the appropriate results for each of these calls in the form function argument equals return value. Over to you. So here's the answer to our programming exercise: We simply set up a loop over our calls, evaluate the calls as recorded, store the result in the return variable, and then print out the call, an equal sign, and the result. Let's see what this does. And you see, we actually invoke remove HTML markup with the recorded value and get a new result, and we invoke square root again with the recorded value and get a square root of 2. With this we have provided a basic mechanism to record and replay arbitrary function calls together with their values. This mechanism, in general, allows to record arbitrary interaction between program parts, as well as arbitrary interaction between the program and its environment. But this is just a tiny slice of what can be done in record and replay. Actually, while being pretty general, it's also pretty limited yet. In particular, one could store the list of calls in a file such that it can be reproduced at any time, handle structured elements properly, that is, lists and maps, record, and possibly replay, return values too. Record global variables, or record only a subset of all calls. Python makes recording and replaying function calls very easy. Especially compared to compiled languages such as Java or C. So feel free to toy around and expand this code. At this point we've seen how to handle static data, how to capture and replay user interaction and even a generic mechanism to record and replay the interaction with the environment. Let me now briefly discuss some additional items that are of particular importance during debugging. The key concept for all of these is to make non-deterministic behavior deterministic and controllable at the same time. First, time. If your program depends on real dates and times, be sure to provide a means to set these, for diagnostic purposes. If it depends on real timing, make these limits controllable as well. Second, randomness. If your program has planned, non-deterministic behavior, for instance, if you're building a game, ensure that you can reproduce this at will. If you use a pseudorandom generator, be sure to save its seed. If you use a true random generator, save the sequence of random numbers. Third, schedules. Multithread programs may non-deterministically switch between threads. Again, find means to make these thread switches deterministic. Otherwise, you may run into hard-to-reproduce thread scheduling issues. Fourth, physical influence. Depending on your environment, it is possible that some physical influence causes software errors. If you place a strong alpha emitter on top of your CPU or your RAM, this will likely cause malfunctions. Cosmic rays may also result in spurious errors. But this is more an electronics problem rather than a software problem, so we leave it as it is. Fifth, debugging tools. Debuggers instument the code and alter its execution. The least they do is to influence the real timing. Therefore, they may induce Heisenbugs, again, bugs that appear or disappear when being observed. Let me close this section with my favorite story on bug reproduction. In 2002, I went to a conference to give a laptop demonstration of a complex software system. Before I left, we had taken every precaution so that the software would run perfectly. But then came the evening before the demonstration. I would be sitting in my hotel room and try out, for the last time, so everything would be working fine. Nothing worked. The software would run into timeout errors anytime, everywhere. I phoned my department at home. They had the same software, on the same machine, and everything worked. I debugged this for 3 hours in a row. My battery had even run out of power, so I had to recharge it. That's when I decided to restart from scratch. To my amazement, the demo now ran just fine. And so I wondered, why did the program work after restarting from scratch? Was this because of some caching problem? Did this have something to do with the battery? Was this because I reinstalled everything? Or because of cosmic radiation? Hint: I gave you an indication right within the story. Over to you. And now for the answer. Indeed, I gave a hint right within the story. "Battery" is the correct answer. After 3 hours, my battery had run out. Which means that until then, I had worked on battery power. In order to recharge my battery, I reconnected to AC power. And it turned out that it was this difference that caused the program to fail. When my laptop ran on AC power, which was precisely the configuration with which everything was installed, the program worked just fine. If it ran on a battery, however, the program failed. It turned out that my laptop ran slower when running on batteries. This saved energy, but also introduced interesting timeout errors. So I made a mental note to fix this as soon as we got home, and of course, I gave the demo on AC power to make sure that all our tests were not in vain. If a program fails in the field, rather than in the lab, this makes reproduction hard at first because you lack all the data you might need to do efficient debugging. However, having programs that execute in the field can actually also ease debugging. The basic idea is to exploit the many runs that are out there in the field. Suppose you have a number of runs, hopefully few, that fail and a number of runs, hopefully many, that pass. Or that don't fail. What you could look for is features of the execution that statistically correlate with failure. One such feature could be the execution of individual parts of the program. If you knew, for instance, that in all the nonfailing runs a specific function is never called, whereas the same function is always called in all the failing runs, and this information goes to you, of course the first thing you would do is to look at the function, F, in order to see what is it that could possibly go wrong in here. Instead of F, you could also come up with other execution features. For instance, this could be a specific line, or a specific return value, or a specific variable value. All of these features could possibly correlate with success or failure and thus provide important hints when it comes to the debugging. To get this to work, we need to collect these execution features. And we'll start with executed lines because this is plain and simple coverage. The idea, again, is to use a tracing function, and the tracing function would record for every single line in the file, whether it has been reached. So we have this global coverage variable and if we reach a new line, we extract the file name and the line number. And what we now do, if we do have the file name and the line number, is we check whether we've seen the file name before, and if we haven't, we create a new empty set. Then we add the line number to the set we found for this file name. Here again we set the tracer, and now we simply invoke this on a simple text without HTML markup. Let's see which lines are actually covered. Click on run, and here's the set. Lines 4, 5, 6, 8, 9, and so on, are being covered in here. Let's go and mark these lines, which are covered here in the code. There's line 4, 5, 6, 8, 9, 11, 13, 15, 16, and 18. The lines that are not covered are line 10, line 12, and line 14. That is all the lines that would set tag to specific values, or set quote to specific values. Since the result of passing some text without HTML markup is actually correct, we could now deduce that maybe we should come up with tests that check for these lines which we haven't covered yet, and see whether these fail, and then we could find out that errors would actually be related to the execution of these lines. But first, let's come up with an automatic mechanism to produce this markup; that is, whether a line is executed or not. The idea is to produce the program as output and prefix every single line with a star if it has been executed and with a blank if not. So we need to turn this set into the code prefixed with stars or not. Here is a starting point. Since we already have the file name, we can actually open the file name and read its context as an array of lines. Let's see whether this works. Indeed, what we get is a list of the individual lines in the program. So here is a bit of a programming exercise: Output each line prefixed with a star and a blank if it's covered and prefixed with 2 spaces if not. Over to you. Here's the answer to our programming exercise. We set up an index, I, which we let run over all lines. In the coverage array, now I is running from 0 to the number of lines minus 1. However, in coverage, lines are started with 1 and not with 0. So we check whether I + 1 is in the set of lines covered for that specific file name. If the line is in the set, then we print out a star followed by the line. Since the line already includes new line characters, we omit this by adding another comma at the end. If the line is not covered, then we output a blank in front of the line. Let's see whether we get a nice listing together with the covered lines. Unfortunately, our web page IDE strips away all leading blanks. But what we see still is the executed lines prefixed by stars and the nonexecuted lines which have no such prefix. Again we can see that we do have a number of lines that are not executed at all. That is, for this particular input, foo, without HTML markup. A coverage tool such as this one can be very helpful in debugging, as it is in testing. In testing, you'd like to come up with tests that cover as many execution features as possible. So if you see, for instance, that what we have executed so far does not cover these lines, you may wish to come up with additional test cases. Better coverage tools would also count the number of times a line has been executed or save coverage that has been achieved so far in a file and thus allow for accumulating coverage over multiple runs with different inputs. For now, however, we're going to use the computed coverage for debugging purposes. Namely, to find lines whose execution correlate with failure. Based on our coverage tracker, we can now compare the coverage for different inputs. I'm going to use the input foo without markup, as we've just seen it, I'm going to use foo with HTML markup, and I'm going to use the same enclosed in double quotes. Let's first start with foo as we've just seen it. Here the tag line is being executed, initialization of code, initialization of out is being executed. We have the loop, overall characters, we have the 1st condition, we have the 2nd condition, and we have the 3rd condition. As we've seen before, none of these conditions evaluates to true for any of the characters in foo. Instead, only adding characters to out is being executed. The assertion, finally, passes. Now let's see what this looks like for foo with markup. Again, the initialization is carried out for this input as well, as is the loop. However, here this condition immediately becomes true already on the 1st character, so tag is being set to true too. At the end of the tag, tag is being set to false again. We also check this condition, but since we don't have any quotes in there, this line is never executed, and, of course, we add individual characters. The assertion in the end passes. Now let's come to our input which should trigger a failure. That is, the same thing as before but now enclosed in double quotes. Again, all of this is executed, no way to avoid this. The 1st thing that happens in here is that quote is being set over here with the 1st character. And since quote is being set, these 2 conditions never hold. And everything, including the tag characters up here, is being added to the output. Since the markup characters are being added, the assertion finally fails. If you look at the coverage up here, you find that there is one line in the program whose execution correlates with failure. And this is precisely this line, namely, quote = not quote. This line is only executed in the failing run. So all we need in this situation is to look at the governing condition, and lo and behold, we do have a defect. Ha! That was easy, wasn't it? Unfortunately, things are not always that easy. So, here's a quiz. Go and provide an input that makes the assertion pass but still execute this line. Over to you. So now for the answer. What we need is an input in which quotes are contained within the tag. For instance, this could be an input like less than, double quote, double quote, and greater than. If we go through the program, we would see that tag would be set, then quote would be set, then quote would be unset, and then tag would be unset. The output would be empty, and therefore would not contain any HTML markup. Note, though, that this line would be executed and therefore also appear in the coverage. And then we can simply say that execution of this line directly is related to passing or failing. So when given an arbitrary program, it is unlikely that we will find lines that are only executed when the program fails. And anyway, such lines would be found by the 1st test that executes them. And we are pretty good at testing, aren't we? What we want to look for are lines that statistically correlate with failure. That is, they may occasionally pass, but by and large, they more frequently fail than pass. How do we compute such statistical correlations? The Pearson correlation coefficient is perhaps the best known indicator of correlation, but this is suitable only for linear dependents. That is if you have 2 ranges of values. What we want instead is a correlation between 2 binary values. On 1 side we have the outcome of the run, either fail or pass. And then for each line we know that it's either covered or uncovered. And we want to come up with a correlation between coverage on one side, and outcome of the run on the other side. So the Pearson correlation coefficient is not appropriate for this. But there is another measure for such correlation, and it was also invented by Pearson. It's called the Phi coefficient. The Phi coefficient starts with a table. In this table you count how frequently a line was covered in failing runs as well as in passing runs. And of course you also count how frequently it was not covered in failing runs as well as in passing runs. These 4 values take the names N11, N10, N01, and N00, respectively. Where the first digit stands for covered versus uncovered, and the second digit stands for fail versus pass. You also compute sums over all rows as well as sums over all columns as well as the total sum of events. This then is the Phi coefficient. It consists of multiplying the values in the first diagonal and subtracting the product of the second diagonal. This is then normalized according to the square root of the sums of the columns and rows. The Phi coefficient is high if 1 diagonal has a high value and the other diagonal has a low value. If it is this diagonal, which has a high value that's down here, then it becomes positive. If it's this diagonal, which has a higher value, then it becomes negative. So the higher the value in 1 diagonal and the lower the value in the other diagonal, the stronger the correlation. Our plan now is we compute the Phi correlation for each line, and then we rank the lines from high correlation to low correlation. And we start, of course, with the strongest correlation. Let me illustrate how the Phi coefficient is computed on a simple example. Let's assume that we have a line in our program and a number of runs. In the first run, line 10 is executed, and the run fails. So what we enter in the table for this line is that we now have seen 1 instance where it covered and failed. Next thing that happens with this line is that it's not covered and the run passes. So we add a 1 in this part of the table. Next thing we see, it's covered and it fails. So the value on that table becomes 2. And then again, we see uncovered and it passes. So now we have seen 2 instances of the line being uncovered and passing. Let's assume these are just the 4 runs that we observe. Then we have seen no instance of the line being covered and passing and no instance of the line being uncovered and failing. Let's quickly sum up the remaining rows and columns. We have seen 2 instances where the line was covered and 2 where it was uncovered. We have seen 2 instances where it failed and 2 instances where it passed. The total number of events is 4. Now here comes the quiz. For line 10, what is the Phi coefficient or the correlation between coverage and failure using this formula. Over to you. Well, that's actually something we can easily compute. The product of these two is 4 minus the product of these two. This is zero so we have 4 minus 0, which is 4. The square root down here is the product of all the individual row and column sums. So this is 2 x 2 x 2 x 2, which is 16. The square root of 16 is also 4. So we have value of 4 up here, value of zero down here, value of 4 down here. Four minus zero divided by 4 is precisely 1, which means that we do have a strong correlation between failure and coverage. Here we have the product of the first diagonal minus the product of the second diagonal, and here we have the square root over the product of the individual sums of the rows and columns. Let's fill the appropriate values of our table and compute the Phi coefficient. We click on run, and we get 1.0. Always nice to see that the computer confirms what we already had thought. Welcome back to the debugging course. In the past units we have discussed how to reproduce bugs, how to track their origins, how to simplify them, and how to find the defect that causes a failure. Today we will consider the management side of bugs. There is how to track bugs, how to organize the debugging process, how to make sure that bugs don't reappear, and how to find for a project where and why bugs occur. And again, this scans with a dose of automation. So we look and build tools that help us automate these things. But first a little story from the trenches. In 1992 I did my master's thesis on a tool that would take a program and visualize this in various graphical forms. I will produce correctly formatted text, flow charts, or NassiShneiderman diagrams. And you'd even be able to edit these in an editor. Later I went on pursuing my Ph.D., which I completed in 1997 on a topic named configuration management with feature logic. The idea was to use description logic to model changes in variance and to detect inconsistencies. This was pretty cool, but the problem was that apparently no developer was willing to learn description logics. Plus, configuration management was essentially solved so I was disappointed. That's when a student of mine, Dorothea Litkehaus, came along, and we developed the idea to use my old library for visualizing programs to visualize data structures rather than programs. Finally, I would be doing something useful. The resulting tool became a debugger named DDD for data display debugger. This is what DDD looked like. It had a command line interface so you could enter arbitrary commands. You'd also see the source code. You would be able to set break points, and see the current execution position. The interesting thing however about DDD, as the name says data display debugger, was the ability to show data structures. For instance, up here we have visualized that pointer list, and now I can double click on the pointer to see where it points to. This is the element the pointer points to. I can check out what self points to. Obviously, it points to itself. I can look up the next value. Again, look up self, and again look up the next value and see that the whole thing becomes actually a linked list. Needless to say, as I keep on stepping through the program, and as the values change, the display would be updated automatically just as well. DDD eventually became free software, and was even adopted by the GNU project. So it became GNU DDD, and it became very popular with C and C++ programmers. Today there is even python support build into DDD, but I'm not maintaining this anymore so I don't know how well this works. But being the maintainer of a popular software also means you have to provide lots of support, which meant that I got plenty of bug reports. These ranged from the absurd, as in DDD crashed, thought you'd like to know to the rather questionable, DDD crashed on our Cray. I have enclosed a core dump. Please be aware we're working on super-secret data here, so don't share. Attachment: Core dump 50 MB. Some of them were very questionable. DDD hangs on my new pets@home project. All files are enclosed. Attached: home directory. The home directory here actually included passwords and bank account information. So much for free software. On some days I could easily get dozens and dozens of such bug reports. Some with helpful information, many without. Of course, I would have been able to ask for more information on each of these, but then, you know, I wasn't exactly paid for the job. Finally, I would set up DDD such that whenever it crashed, it would ask for vital information that I needed to reproduce the failure. What is this vital information that needs to go in the bug report? In a 2008 study involving 165 developers from Apache, Eclipse, and Mozilla, the most important facts the developers needed were facts about the problem. First, the problem history. That is, steps needed to reproduce the problem. For instance, start preview and then open the attached file. Second, diagnostic information. That is, core dumps, meaning memory dumps of the final stage of the program before it crashed, stack traces, the functions that were active at the moment the failure occurred, or logs. Whatever the system has recorded about the final state, and what the program has logged so far. Next, the experienced behavior. This is what the user saw. For instance, preview crashed. Next, the expected behavior. This is helpful as a reality check. Does the user expect the same as the developer? Mostly, this is just the opposite of the experienced behavior. Finally, a one-line summary. This is typically the base for searching for a bug report, as well as for deciding the severity of a problem. For instance, preview crashes when opening PDF file. Now for the quiz. One of these 5 facts allows to infer most, if not all of the others, and, therefore, it is the most crucial of these 5. Which one is it? Is it the problem history? Diagnostic information? The experienced behavior? The expected behavior? Or a one-line summary? The one fact that allows to infer most, if not all other facts, is the problem history. First of all the problem history is crucial to reproduce the bug. If we repeat the steps, then we will hopefully find the same behavior in contrast to the expected behavior. We may also be able to summarize what's going on and possibly even observe the state at the moment the failure occurred, getting more information such as a core dump or the currently active functions. So, the problem history of all these facts is the most crucial and the most important in a bug report. Many operating systems allow you to submit bug reports more or less automatically as soon as a program crashes. This is the dialog that appears on a mac os system. Essentially Apple asks you just one single question, and that is again the steps necessary to reproduce the problem. Everything else can be deduced from that one. On top of that the bug report also includes problem details and system configuration, which is, for instance, the version number of the program that crashed, the current process, date and time, the functions that were active at the moment of the crash, as well as any hardware attached to the machine. Here are the steps needed to reproduce the problem, and we send the whole thing to Apple. So, when you are managing a successful product, you will get many of these bug reports, and you must make sure that no bug report ever gets lost and also that all these bug reports eventually get handled in finite time. Before that you'll have to store these problem reports somewhere that you can classify them, mark them, and evaluate them. This is the talk of a problem database. Simply speaking, a problem database holds all problems that ever occurred with some product. Whenever somebody reports a problem, either though an automatic means or as a regular user, all of these problems get stored in the problem database. You may wonder how many of these bug reports end up with large companies over time. Actually, these problem databases are huge. I don't know how this is for Apple, but I've been the first non-Microsoft researcher ever to peek into the problem database of Microsoft. I can't tell you exact number of bug reports that are in there, but if you consider that Microsoft has sold 450 million of Windows 7 licenses. If every user just experiences one such crash per month, that's more than 37 million bug reports each year. This is only from people who actually click on the send button. I guess there's a number of people who run Microsoft but don't have a proper licence and prefer not to tell Microsoft about their problems. In most problem databases, however, it is real people who enter and classify problem reports. Let's take a look at a real problem database to get an idea what these look like. Fortunately, for many open source projects there are problem databases, which are readily available. For instance, let's take a look at Bugzilla, the bug database for all Mozilla products, in particular the Firefox browser. This problem database contains more than 700,000 problems. Of course, most of them have been fixed at this point. Let's assume I have a problem with a blinking cursor. I enter the search terms for the bug, click on quick search, and here I see all the issues that occur currently, or at least that have been reported by users, with respect to the text cursor and blinking. For instance, here we have blinking cursor does not move when space is entered. We can get details on this bug. This is the original bug report filed ten years ago. Let's go for a more recent one. Here's add key bindings to CSS value adjustment. You see the bug report--you see the product it applies to, in this case Firefox. You see when it was reported. Here's the original bug report, and now you see the developers talking about the problem. You can also follow back the history as developers suggest patches for the bug. In the end, they discuss possible alternatives on how to address that bug. Now for a quiz--When was bug number 915 filed? So, was this in 1992, 1998, 2002, or 2009? Let's go and find this out--we simply enter the bug number in the search field and get bug number 915. So, when was it reported? This is one of the oldest bugs in Mozilla, and at this time in 2012 it still hasn't been fixed. That is after almost 14 years. Still this bug has quite some history. Hundreds and hundreds of people-- Well, let's make that dozens and dozens of people have made hundreds and hundreds of comments over time. This seems like a tough bug to fix. So, the correct answer here is 1998. How are all these problems classified? Let's look on advanced search to see what the individual attibutes of a bug report are. Up here you can see the status of the problem. Is it a new problem or is it a resolved problem? If it's a resolved problem, you can look up the resolution here. Was it fixed, or is it something that won't be fixed? Further down you can see the version of Mozilla that is affected by the problem as well as the severity and the priority of the problem. What do these fields mean? Let's look at them in detail. So, what are all these fields? First there's severity, which describes the impact of the problem on the user. The most severe problems are classified as blockers or show stoppers. These are problems that effectively halt all further development. Then the are critical problems, major problems, minor problems, down to enhancement requests. Enhancement requests are issues a user would like to see in some future version. They are also stored in the problem database. Let's say that other problems are more severe at this point. Priority defines how soon the problem will be addressed. The problems with the highest priority will be addressed first. Problems with lower priority will be addressed later. The Bug ID is a unique identifier or number, which allows you to precisely identify a single problem. Comments are left by developers adding additional information about the bug or proposing fixes on how to address the bug. Finally notification. These are the stakeholders. Whoever is listed here will be notified automatically whenever a problem gets a new status. Now for a quiz.. Which problems gets fixed first? Is it those with the highest severity? Those with the highest priority? Those which have been around for the longest? Or those which are the easiest to fix? Pick your choice. As stated before, the priority defines how soon a bug will be fixed. Of course, it may be desirable if severe bugs were fixed first, but a severe bug may affect only a single user. Hundreds of other users may suffer for minor issues, which therefore may have to be addressed first. The same goes for bugs which have been around for the longest. Again, these may not be important for many users. Those which are the easiest to fix. Well, if a problem is easy to fix that certainly increases the chance to get fixed soon. Then again, these problems may not be important. Many organizations use a software change control board, or SCCB to set priorities. This is a group of people who look into the problem database and take care of the handling of the problems. Such a group typically consists of developers, testers, and managers. What they do is the keep track of resolved and unresolved problems, assign a priority to individual problems, and then assign these problems to individual developers. As you can see, a problem database is a very important tool in the management of a product. All the discussion on all the features eventually is stored in here. You can even use it as a tool for requirements management. When the project starts you enter a single problem in here, which states the product is missing. Then you add up more requirements and more requirements, which eventually will have to be fulfilled. A board like the SCCB then decides who should take care of which issue and when. Suppose you just have filled a new problem report. What happens next? In a problem database like Bugzilla, the problem goes through a number of stages. Initially the problem report is unconfirmed. If all the information in the problem report is valid, then it goes into the new state. A manager or the software change control board assigns the bug to an individual developer who now works on it. The developer now resolves the problem, and for resolving the developer can choose between multiple resolutions. be fixed, meaning that the problem has actually been addressed. The problem can be marked as a duplicate, meaning that the problem already exists somewhere else in the database and therefore possibly somebody else is already working on it. The problem can have a resolution of invalid, meaning that the problem is not a problem or does not contain the relevant facts. A resolution of won't fix means that the problem will never be fixed, which is a somewhat sad outcome for the one who originally submitted it. Then we have works for me as a resolution, meaning that the developer could not reproduce the problem. Note that if the bug report is invalid or a duplicate this may also be found out at an earlier stage of this, and the problem immediately gets resolved, of sorts. If the resolution is fixed, then the fix will typically be verified by the quality assurance team and as soon as the final product finally ships with the fix in it, then the bug report is marked as closed. In case the problem reoccurs again, it goes into a state of reopened and then needs to be reassigned to a developer. This can also happen from the resolved state. If additional information becomes available, for instance, that makes the original resolution obsolete. We have a user who reports that his drawing program cannot open a document saved by a previous version. He attaches the document in question to his bug report. Here is developer Dora. She can confirm that the failure actually occurs and that the bug report is a real bug report. Here is developer Erol. He fixes the bug and can now properly process the document. What is the stage of the bug report at this point? Is it assigned? Is it resolved? Is it verified? Or is it closed? Check the correct answer. Now for the answer. The user has reported the bug, and Dora has confirmed it is valid. It got assigned to Erol, but Erol now already has fixed the bug. So, we're currently in a resolved state. What has not happened yes is that somebody else has verified the fix,, and what also has not happened yet is that the fix actually shipped to the user-- for instance, by releasing a new product. Therefore, the correct answer is resolved. As your problem database fills up with more and more problem reports over time you'll want to do some housekeeping. Because as these databases fill up, there are a number of issues that pile up as well. The first one is duplicates. If you have one user who's reporting a problem chances are that other users will be reporting just the same problem. That is, you have multiple problem reports that all relate to the same class of failures. These problem reports are call duplicates. As a manager, your task is to identify such duplicates. You want to do so in order to avoid them cluttering the statistics, but you also want the duplicates to refer to each other. This way when you come across a problem report, you will find, hey, this is a duplicate of this original bug report, and all of these others are also duplicates. You like to keep the duplicates, though, in your database, because all of these may report on different angles of the problem and these angles may all be helpful for resolving the problem. Note that automatic diagnosis mechanisms, such as statistical debugging or delta debugging, are great tools for identifying duplicates because they'll find commonalities between all the individual bug reports with respect to similar features in the input or in the execution. Next up is obsolete problems. Over time your database will fill up with unresolved problem reports-- problems could not reproduced or problems that may have been fixed in some later version and low-priority problems. Having thousands of unresolved problems will drag developers down. They clutter up searches in the database, and they are bad for the morale. A problem database that has plenty of obsolete problems is like an overflowing drawer of socks. You don't find the socks you need, and the drawer makes you feel guilty for not throwing away your old socks. What you should do is over time simply declare problem reports obsolete and thus get rid of socks you don't want anymore. When is a problem obsolete? A problem is obsolete if it will never be fixed. For instance, because the program is no longer supported or the problem is old and occurred only once or the problem is old and occurred only internally. You don't want to actually delete these problems, but you can tag them with an appropriate resolution. In Bugzilla, for instance, there is a special WONTFIX resolution for such obsolete problems. Finally, problems are not only stored in the problem database, but that may also be test cases, which reproduce the exact problem. As a rule of thumb, as soon as you do have a test case that reproduces the problem, the test case makes the problem report obsolete. That is, as soon as you have a test case you can actually put a special flag on the problem database that the problem is now being addressed by the test. This last point called for a quiz. Why is it better to have an automated test rather than a problem report? Is it that you can always check whether the problem persists by running the automated test? Is it that you can always reproduce the problem? If the test fails, you can start debugging right away? Or you can query as much additional information as you need? Check all that apply. With an automatic test, and this is the main advantage, you can always check whether the problem persists by running the automated test. An open automated test by definition reproduces the problem. If the test fails, yes, you can start debugging right away. And in the run you can query as much information as needed, because you can always reproduce it. So, all four apply. So, we now have seen that whenever a user reports a problem in the problem database or a developer for that matter or anyone, eventually a developer or a team of developers will take a look at the problem and make an appropriate fix to the program. Such fixes are also stored in a database-- namely, a version database where all changes are stored. Such a version database is also called a change database, a repository, or the configuration management system, a version control system. Pick your choice. There are plenty of version control systems around these days which help storing these changes and the resulting versions. Since using a version database is the first thing to use in any kind of civilized software development, I will simply assume that you use such a thing on a daily basis anyway. An interesting thing happens, however, when you link the information from the problem database to the information from the version database. Let's assume that the problem database has a problem report #347 where it says removehtmlmarkup fails. Let's assume the version database has recorded a change to function name removehtmlmarkup in precisely this location with a comment that this now closes problem report #347, which is a change which may well have been made after the problem was initially submitted. We can now got and relate the change to the actual problem report, because the change message has the actual number of the problem report in here, and we can use that to retrieve the precise problem report. Since we also know where the change has been applied, namely in this part of the file, we now have a link from the problem database to a specific place in the code. This allows us for every piece of code to identify the problems that were associated with it. What we do is we take the piece of code, look at all the changes that were made, and look at the problems that these changes refer to. We can then, for instance, find that removehtmlmarkup over the history of this very course has had three fixes until it finally worked. Three fixes until a function actually works is pretty bad. We should really worry about the quality of our coding. The interesting thing is that we can do this for all parts of the program. For every single function in the program, we can look up the changes and find out which problems were addressed in that specific file or in that specific function. What we get this way is a defect count for every single location. That is, the number of problems that have been fixed in that very file or function. In 2007, my students and I built such a tool that would create such a mapping from the version databases and problem databases of open source programs. For instance, we would apply this on Firefox--the web browser-- in order to find out where the most defects were. More specifically, we would be looking at security defects-- that is, problems that relate to security issues. What we would get is precisely the location where the most security bugs would be. What you see here is a representation of all the classes in Firefox. Every class here in this picture is a rectangle. The larger the rectangle, the more lines of code in that class. These rectangles are nested into folders and packages. The color of the rectangle indicates how many security issues have been fixed in that particular class. What we see here, for instance, is the document object model, which has a fair share of security fixes. But there is more areas with plenty of security issues. Here we have JavaScript. Here we do have HTML layout. Here we have a library for displaying the content. Now for the quiz. Which of these four packages has had the most security issues in the past? Is it JavaScript? Is it HTML layout? Is it the DOM? Or is the content base? The is JavaScript. Down here you see more than a dozen classes that all have had plenty of security issues. This is the correct answer. One may wonder why is it that issues end up in a small number of places and that such large parts of the code remain without any issues. For JavaScript this is pretty clear. This piece holds the JavaScript interpreter, which interacts through many, many interfaces with the system. All of these are possible attack vectors. It's no surprise that JavaScript holds these many security issues. Plus an interpreter is notoriously hard to get right. HTML layout may come as a surprise. After all, this is just the rearranging of appropriate user interface elements on the screen. Why would there be security issues related to that? The reason is cross-site scripting. As soon as you layout multiple sources on one page, it may be that one sources tries to access elements of the other source, using your screen as a tunnel. These issues are right here within HTML layout. The document object model also allows accessing and manipulating individual elements. This again is an open door for security issues as well as for content base. I don't really know. We have created such distributions for several systems-- at Microsoft, at Google, at SAP, and on many open source programs. What we always found was the so-called Pareto principle-- that is, 20% of all modules contain 80% of the bugs. The numbers vary from project to project, but what we always found was there was a relatively small number of modules that would contain lots and lots of issues. Initially, we were just excited of being able to create such distributions more or less at the touch of a button, but as you look at these distributions you begin to wonder where do these bugs actually come from? Do these modules that actually are specifically bug prone have something in common? If they do have something in common, could we use this very feature to make predictions? We dug a bit deeper and checked a number of interesting features. The first question we ask is does the bug density correlate with the experience of the developers that wrote the programs. That is, possibly more experienced developers make fewer mistakes. For these questions I'm going to ask you for a guess on your side. These will not be rated. So, what do you think? Does bug density correlate with developer experience, yes or no? The answer to that question is yes. It correlates. The more experience, the higher the bug density. This may come as a surprise to you, but here is the story behind it. We mined the Eclipse bug database and check for the experience and contributions of the individual developers. It turned out that the Eclipse project lead, Eric Gamma, had the second highest defect density in his code across all the developers. Now, Eric Gamma is anything but a nobody. He gave the world unit tests. He gave the world design patterns. And he gave the world the Eclipse programming environment, always with a team of course, but still. Why would his bug density be the second highest across all Eclipse developers? The reason is simple. Suppose this is you. You have been assigned to fix a bug. You look at the problem and you find, oh, this is terribly hard. What you do is you delegate the problem to your boss, who is way more experienced than you, Now your boss is looking at the code and says, "Ahhhhh...this is something I can't handle." He delegates this to his lead, and this guy says, "Ah, this looks really, really, really hard. Only one person in the world can do that." This is Eric Gamma, the team leader, who has no one else to delegate to. Being that the team leader, he gets the toughest problems-- that is, those problems where the changes of screwing up are highest. Still, he is the man, because anybody else dealing with these problem would, on average, make a worse job than Eric Gamma. This is how the more experienced people get the tougher tasks, possibly introduce more defects, but still overall they are precisely the right persons to do the job. If there are lots of bugs in a specific place, will there be more in the future? Remember that if we say we found lots of bugs in one place, we already fixed them. What's your guess? It turns out that, yes, if there have been many bugs in one place, it is likely that there will be more bugs in the very same place. This is the more interesting, because all the bugs we see have been fixed, so you'd assume that over time there would be fewer bugs. But that's not the case. With bugs it's like fishing. You find plenty of bugs in one place one day. The next day fish will assemble at the same place again, possibly because fish like the same places again and again, except that for fish we have a good theory on how the reproduce. For bugs, we do not. There is a bit of a hypothesis though. The assumption is that the same factors that have contributed to bugs in the past will keep on contributing in the future. Generally speaking, past fixes and a bit less so past changes are good predictors for future fixes. Next hypothesis--complexity. Is it so that complex code has more bugs than simple code? Can we use, say, code metrics to predict where the most bugs will be in the future? Pick your choice. Well, the answer is complexity matters sometimes. Sometimes in the project there is a correlation between some complexity metric and the real number of bugs. But very frequently there isn't. Then in every project there is some other complexity metric that correlates, so it might just as well be random. So, no. Complexity is not related to bugs as found in production code. Next hypothesis--tests. You can measure how well-tested individual parts of your product are. If a piece of code has a high testing coverage, this means that it would be well-tested. The question is is code that is well-tested less buggy? Pick your choice. The answer to that question is no. Actually, the opposite is true. The more thoroughly a piece of code is tested, the more bugs it is likely to have. The story goes the other way around. Good managers have a good intuition of where the bugs are, and then when deciding on where and how to test they go for the locations where most bugs would be suspected. Therefore, in a program like Firefox for instance, there is lots and lots of testing that's being done for JavaScript, for instance. Therefore, this has high coverage, but still many, many bugs are left because testing apparently can't find all the bugs. The next interesting hypothesis--does the team structure have an influence of how many bugs are produced by that very team? The answer to this question is yes, or at least that team structure can matter. A very interesting study at Microsoft looked how teams that were responsible for individual features were compose. More specifically, the study would look for how distant the common manager of a team was. If there would be one direct common manager for all members of the team, that distance would be 1, and if their common manager would be, say, two levels removed, then the distance would be 2. If the lowest common manager of a team would be say, Steve Ballmer, the manager of Microsoft, then the distance would be 7 or 8 or whatever the number of management levels at Microsoft is. Now, it turned out that the higher this distance was the more defect prone the modules produced by that team were. The assumption is that if there is no joint management in a team where every decision first has to go up to the highest level and then back again, then this makes teamwork particularly hard. It also makes it hard to make decisions, and it makes it hard to create some common responsibility for the module you're working on. This study was made on Windows Vista. As a result of that study, Microsoft reorganized the teams for later versions of Windows such that situations like these would no longer occur. Last hypothesis--is it the problem domain? That is, the domain of the problem that your module is trying to address? Pick your choice. The answer to that is a clear yes. In studies of Firefox and Eclipse we found one specific feature of the code that dominated all others. These were the imports made by individual modules. That is, the other modules that the module in question would interact with. More specifically, whatever a module imported would determine its likelihood to have a defect. In Firefox, for instance, if you're module included nsIPrivateDOMEvent.h and nsReadableutils.h--that is, used these specific APIs or interacted with these specific APIs, then you're code would be doomed. Because 20 modules that also included these two files l had at least one security issue. Likewise, in Eclipse if you imported something that dealt with internal features of the compiler, your code would be 4-5 times as error prone as code that only dealt with a graphical user interface. Why is that so? Well, if you write import compiler internal, this means you're going to write some compiler code, and compiler code is more error prone than user interface code, in particular because if you worked with a user interface, most errors you make will be immediately visible to the human eye. Whereas if you deal with compiler internals, it's a long path from a bug in the compiler to a bug in the actual compiled program, which then, again, has to be executed in order to have the bug cause a failure. None of this cause needs to be discovered right away. All of these are reasons why this domain, namely the compiler, is way more error prone than the user interface. So, we have looked at individual developers and past bugs, at complexity, at tests, at team structure, and at the problem domain. Developers get assigned to tasks that are hard in the first place, and tasks that are hard call for more bugs. That is, also more past bugs. Also, more testing. All of this leads us to the domain as being the most important factor in determining where bugs actually come from. If the domain changes frequently, this will lead to more bugs. If the domain is complex in itself, such as JavaScript or in eclipse the compiler, this will lead to more bugs. If the domain is not well-defined-- for instance, because the team cannot agree on what to do-- then this also calls for more bugs. What we can do, though, is by looking at past bugs, identify which parts of the domain and possibly other influences correlate with past bugs. This may give us a handle on how to avoid such mistakes in the future. At the end of the day, what the area of mining for such information has found out, though, is that although there may be general rules there are also lots and lots of project specific features that correlate with bug density. Therefore, it is advisable to first take a look at your own defect data and then figure out what the hot spots are and think about possible ways to learn from past mistakes and improve things for the future. We're going to the seventh and last unit in our debugging course. In this unit, we're going to do two things--first, we're going to recap the material of the other units and we make sure they all fit well together, and second, we're going to explore a few options on what to do to avoid debugging altogether. Plus, we're going to have a collection of the best bug stories. Enjoy! Welcome to the last unit in our debugging course where we review the material we've seen so far and put it all together. Now at the end of the course, let's put everything together to see what we do when we encounter a new bug. When we see a new bug, the first thing we do is to make sure it gets processed. So we enter it in the problem database. We've seen the stages the problem report goes through from unconfirmed via new to assigned, when it's assigned to an individual developer, and finally resolved and closed. Any new problem needs to go into the problem database. In the long run, checking problems also helps you in coming up with statistics on how long it takes you to fix a bug, how many bugs there are that still need to be fixed, as well as where are the bugs are that need to be fixed? Such that in your program, you know where the locations are that overtime have the highest bug density, and therefore needs special attention. For all of this, tracking the problem is the first prerequisite. The next step is to reproduce the problem because only when you are reproducing a problem, do you know whether you will actually be able to fix it. We have seen what the individual inputs are that all influence a program execution. Most important being static data, user interaction and the interaction with the environment but also more trouble issues like the debugger which influences the program or time randomness and other issues that may be hard to control. In order to reproduce the problem, you must get all of this under your control. Reproducing your problem can be particularly hard when you're collecting such data from the field because data and user interaction may contain sensitive data. This is why we have also seen the techniques of statistical debugging which relate execution features such as allowing the program being executed on that to failures and success by collecting such data from the field, you'll be able to find out which features of the execution correlate with failures and these features can be features such as executed lines or returned values of functions. The next step in debugging is to automate and simplify. The idea being that you write a test gate at which you can reproduce the bug at any time automatically. At first, this helpful, of course, for doing regression testing such that you can always verify whether the problem is there or not. However, results are helpful to automatically simplify the problem if you have a big input which causes a failure, then an automatic test can help you automatically simplifying this input. The technique of delta debugging takes this big input and with the help of an automatic test automatically simplifies it to an input in which every single item is relevant for reproducing the failure. This can be a great aid as it comes to understanding what makes the failure occur and what not. Once you have simplified the problem, the next step will be to find possible infection origin. If your program fails, you can see its execution as a succession of state. The last state is what you see as a failure and you want to figure out where did this failure come from. The concept of dependencies helps you figuring out the possible sources of an infection and rule out all the other ones. When you're tracking back where a failure came from, you frequently have the choice of looking at multiple possible origins. But you'd like to do is to focus first on those origins which are most likely. If you know that some state is already infected that is wrong, you will focus on this one first. If you know from some earlier state that this caused this later state, you will also like to focus on this one. If you know from statistical debugging that some feature is correlated with the failure, go for it. And if you have reason to believe that some particular state is likely to be buggy anyway, for instance, because of it's bug history, you also go for it. And we have seen techniques how to determine all of these in particular assertions which help a lot for finding out immediately during the execution whether some part of the state is erroneous or not. The sixth step is to isolate the infection chain. What this means is for every likely origin, you go and conduct an experiment to see whether it actually is the cause of the failure. For this, you used the scientific method. You set up a hypothesis. This could be the cause. This could be the cause. This could be the cause. Make up an appropriate prediction. Set up an experiment and based on the observation, you either refine your hypothesis or you reject your hypothesis. Be sure to make these steps explicit as it helps structure your thinking and as it helps you interrupting and resuming your debugging activities as needed. You repeat this going backwards throughout the program until you arrive at a place where the incoming data is same but the outgoing data is infected. The statement which generates this infection is the defect. This is the last step of debugging. To correct the defect, such that it no longer produces an infection but actually makes the program behave correctly. Before you fix the defect, you should be sure that the defect actually is the defect. That is that by changing it appropriately, the problem will be fixed. You should have a clear understanding about how your correction fixes the defect. And then, there are also some chores to be made. You need to verify that the failure no longer occurs. You need to make sure that your correction did not introduce new problems and again assertions as we've seen them in this course, helped you a lot with that. You may also wish to search your code to check whether the same mistake was made elsewhere and fix these locations as well. Note how these seven steps of debugging can be easily memorized by looking at their first letters which formed the word TRAFFIC. Traffic may not immediately be related to debugging unless you're debugging traffic programs but the way that infections propagate to the program and what's happening during a program execution can be as confusing as more than traffic in a real big city. With these seven steps, you're done with debugging. Congratulations! And now, let's move on to the next part. At the end of the day, we'd all prefer creating over debugging. So how can we avoid debugging in the first place? Here's some suggestions on what you can do. First thing, get your requirements right. Only if you know precisely what to do will you be able to know what not to do. That is, only with precise requirements will you be able to tell failure from success. A famous quote coming from Brian Kernighan: "Without specification, there are no bugs--only surprises." So you could think that if you have no specifications, then you wouldn't have to do any debugging. Unfortunately, surprises need to be debugged just as well, except that debugging surprises is harder because you don't know how they would not surprise you. Increase precision and automation--next suggestion. In the end, you do have this huge program and you want to be able to find as many bugs as possible. Of course you can have developers read it, review it, test it, but in the end, the time of your developers will always be a precious resource. In other words, you won't have the developers, you won't have the time, and so you'll be pressed to ship anyway. If you can automate these tasks, then resources will be much less of a problem because you can have an arbitrary number of computers doing the checking for you. This calls for techniques such a contracts, assertions, or other forms of specification that can be validated automatically, plus, of course, lots of automated testing that follows. The next step is to reduce complexity. The higher the structural complexity of a program, the more ways there is for some part of a program to influence another and the easier it is for infections to spread. Plus the higher the complexity in a program, the more possible sources for infections there are, and all of this makes debugging way harder. Next step, set up assertions. Every single assertion rules out large parts of the program or of the program state as an infection cause. And the more assertions you set up, the less state and statements do you have to examine in order to figure out where the infection came from. Next thing, test early and test often. The earlier you test, the more precise your specification will be. And the more you test, the more defects you will catch right away. Plus, if you test frequently, you will be able to isolate the change that led to the test failing by building on the past history of successful tests. Next, review your code or have it reviewed by your colleagues. Having your code being inspected by others has been shown again and again to significantly increase productivity, quality, and project stability. Next step, go and analyze the problem history of your project in order to figure out which components have shown risk in the past and which ones haven't. Obviously, components that had troubles in the past would always be the first ones to focus on as it comes to improving quality. Analyze their history, their common features, common causes, and adjust the development process accordingly. Last, learn from your mistakes. Every bug you make is a bug too many. Improve your development process such that bugs are caught as quickly as possible. If a bug escapes into production, make sure it won't happen again. Some of my very favorite bug stories of all time are related to the F-16 fighter plane. It turns out that when the F-16 was conceived, it had a very interesting feature. Whenever it would fly and fly and cross the Equator, the plane would instantly flip on its backside and keep on flying. This, fortunately, only happened in simulation. But imagine what would have happened in real life. Well, the F-16 can actually fly on its back for hours and hours. It turns out that this was a signage bug in the navigation system. Picture what would have happened if this navigation system had been used for commercial airlines. If you see the Fasten Seat Belt sign, yes, be sure to keep your seat belt on at all times. Another nice bug story is related to Google's Android operating system for mobile phones. One of the first versions of Android had a diagnostic mode turned on where an engineer would be able to use the serial port to enter arbitrary UNIX commands. Interestingly enough, if the phone would not detect a device on the serial port, it would fall back to the built-in keyboard instead. What this meant was that anything you entered on the keyboard was also interpreted as a UNIX command. This was discovered when somebody got an SMS asking him, "What did you do all day?" and he answered, "Reboot." And lo and behold, with this command the phone rebooted. Picture this: Anything you say is interpreted as a UNIX command. He could also have said, "Remove files," or "Drop database," or whatever. This was very, very quickly fixed. Here's another really mysterious bug. I had a program and the program crashed. So far, so good. So I inserted a print statement in order to figure out what was going on. To my big amazement, after I inserted the print statement, the bug was gone. Classical Heisenberg. What was even better, I removed the print statement again and the bug still was gone. Now, that's an interesting way to fix bugs. What was going on there? It turned out this was a problem in the linker. The linker had 2 modes. It had an incremental mode in which you would simply link in changes, and it had a non-incremental mode where it would start linking from scratch. And if I made a small change to the program, it would go into the incremental mode. In the incremental mode the bug was not triggered. So this is why in the first attempt, in the non-incremental mode, the bug came to be and later, in the incremental mode, the bug was turned off again. But this was hard to work around. Essentially, what I did was I made a comment in the build script that would trigger the incremental mode by simply linking twice. And this is how we worked around the bug. The idea is to use this trace-it function to build your own interactive debugger. We're going to call this my spyder for my simple Python debugger with the idea of a spider being something that catches all the bugs. My spyder is a super simple debugger as it says in the name. My spyder is a command line debugger, so it prompts you to enter a command and the command can be something like break in line 70. Run the program and when it reaches line 70, it stops and as it has stopped, we can print individual variables--for instance, an x and x saying has a value of 42. How do we do that? We need the number of variables that controls how the debugger works. First, a variable name stepping which tells us whether we are stepping line by line through the program or whether we're just running the program. We also have a list of breakpoints, actually it's a mapping of individual line numbers to arbitrary values, but if this mapping say has an entry for the number 7, this means that whenever the debugger reaches line 7, then execution should stop. For our current setting, we'll make a lot simple. We'll have a number of predefined breakpoints. We will have for instance a breakpoint in line 9, and possibly another one in line 14. Our trace-it function is being called for every single line and of course we want our debugger to stop whenever either a breakpoint is reached or when we're stepping from one line to the next. We want to access these global variables, so we need to declare them accordingly. If the event is that we reached a new line, we check if we're stepping or if our breakpoints dictionary has a value for the current line and if this actually is the case, we print out a proper diagnostic information which may be helpful for the debugger--what has happened, where we currently are, the functions that executed as well as the local variables. Once we have printed out our current information, we read a command, we're going to define appropriate function input command for that and then we're going to invoke a function named debug with the command with the current argument and with all local variables. We have our debug function--return a value named resume. If resume is false, we stay in this current event--that is we don't proceed in our code, and if resume is true, then we exit the loop, step out of our trace it function and then execute the next line in the program. Let me first come up with the debugging function. It accesses the state of the debugger by these two global variables--stepping and breakpoint. If the command we have entered as an argument, then this should be a space between the command and the argument and we get the argument by splitting the entire string, getting the second element in the resulting list, which is the argument. Otherwise, the argument is just none. Now, we can check what the command actually is and what it should do. For instance, if we say s or any command that starts with s and want to step to the program, so what we do is we set stepping to true such that in the next line the program will stop again and we return true such that after stepping, execution resumes and actually goes to the next line. We can now add more commands--for instance, continue here which sets the stepping variable to false and still ask the debugger to resume. Now, what does this continue command actually do? What happens if we set a stepping variable to false? Is it the program resumes until it exits, the program resumes until the next breakpoint is reached, the program resumes until the next function is called, or does the program stay in the same line? Keep in mind that all of these options assume that we return true in our program. Now, let's look at the answer. What happens if we set stepping to false. Let's take a look again at our code if we set stepping to false in here. As for this continue command, what's going to happen downstream. So we return true. So receiving will be true. So we're going to exit this loop immediately. Meaning that the next line will be executed, and now we have over here the stepping variable being set to false--this means we're going to resume execution line by line unless this condition becomes true--that is unless we actually reached a breakpoint. The correct answer is the program resumes until the next breakpoint is reached. We need an input function that will return the command to be executed. If these were an interactive program, that is if we would run this in a console on our own machine, this is the command to be used until we could interact with it. This is what you can use on your own machine in order to get your debugger to work. In this web IDE, however, such an interactive program does not work, so we need to simulate this. So what we have here is, I have set up a list of predefined commands to execute. In here, this would be step, step, and quit, and we're using the method pop(0) to return the first element of the command list and at the same time also to remove the first element from the commands list. So the command is now missing its quit, so we have to add this. If we have a command that starts with quit, we call sys.exit of 0--meaning that our debugger exits. All is set now--initially, we do not step through the program, but we have a breakpoint in line 9 and in line 14. We want to execute three commands--step, step, and quit. What should happen is that execution is normal until it reaches line 9, then we step through two more lines and then we quit the program. Let's see whether this actually works and indeed--it has worked exactly as we wanted. First, we reached line 9, then we step--we reached line 11, then we step--we reached line 13, and then we quit and this is precisely the output we would have expected. So now for your homework, go and extend my spyder with the following commands-- we have s step one line--this is already in there, we have c for continue execution, and we have quit--this is already in there. Now come the extra command--we have p for print all variables and also with an extra argument, which should print the variable as variable equals value. We want to have commands to set breakpoints--b at line sets a breakpoint in line, meaning that after you enter this command, execution will always stop whenever that line is reached, and finally, special command and is specially useful-- if you typed in w for watch point or anything that starts with w such as watch and give a variable name as argument, then the program should stop whenever the variable changes its value. With these commands, you can actually go and debug the remove HTML markup function step by step, printing out individual variables, but as we discussed today, you should go and use the scientific method for your homework. We will test your debugger by coming up with commands such as these and of course, expecting very specific output. This is an extra part that will not be graded, which you can do just for fun. This gets us directly towards our homework. I want you to build a tool called Bonsaikon, for Bonsai Daikon or tiny Daikon. The idea is to take a number of executions, like these, and produce assertions, that is, invariants that hold for all runs observed. For the starting point, all I want from you is ranges of variables as you have seen them. What Bonsaikon should produce is a range of variable values seen at the call and at the return of each function. Here is an example, for square root, how this should look like. At the call of the square root function, x is always between 2 and 16. And the variable epsilon is always 10 to the power of minus 7. And as you can see, I want you to output these ranges and if the range has just one value, then I want this to be an equal sign. I want you to output these as assertions that can be inserted immediately into the code. This would be pre conditions that hold when a function is called, but I also want you to report post conditions that hold when the function returns. Here we have the range for the variable y at the end of square root and also the return value, which I gave this special name, "ret." Return value is between square root of 2 and square root of 16 as seen for these 3 runs. The question is how do you get these ranges? The answer to that, again, is our tracing function, which allows us to record the state of the program at every call and at every return. What you set up is a special trace-it function, which gets the current frame that executed, the event that's taken place, and an argument. Let's first take a look at the event. The event is either call or return, which translates directly into these two situations you'd like to monitor. The current frame tells you which code is being executed. frame.f-code.co-name gives you the name of the function that's executed. In our case, that would be square root. The frame.f-locals attribute is a dictionary that gives you access to all local variables such as y or x or eps. And finally, if the event is returned, the argument that's passed over here gives you the return value. So what you do is you run your program with the trace function enabled. You record the call and return events. You check for every single function that's being called. and is called and is returned. You check all the variables through frame f-locals, and over time you record the ranges. When the execution is done, you output the ranges that you have observed. So in the beginning, first run, square root of 2, x is just 2. Then x is between 2 and 4. And then x is between 2 and 16. Likewise, for y where the square root gets extended over time. Likewise for the return value, where again the range of return values extends as more and more runs are observed. For your homework, I want these ranges of variables, as seen. But if you want true challenges, here are some more. For instance, you can go and also record the types of variables, say, the type of x is the same as the type of 2. Or you could go and check for set patterns. For instance, you could report that the variable x is in the list of values 2, 4, and 16. The relationships between variables-- In the square root function, the y variable always has the same value as the return value. That's not very surprising, because we actually return y, but that's an interesting property. Likewise, the return value of a square root function is always less or equal to the value that we want to compute the square root for. You can obtain such relationships between variables by coming up with a pattern, say like equal or less than, and instantiating that pattern with all variables, and then seeing which patterns actually hold over multiple runs. With that, enjoy building your own dynamic invariant checker, and make it easier for future generations to write assertions. Thank you. And now for the homework. It comes in two parts. In the first part, you get a mystery-test function that is the function which returns pass or fail, for which you don't have the source code, and you get 5 string inputs all representing HTML input. Your job is to determine 5 simplified inputs in which every character is relevant and which make the mystery-test function fail. So, from a hundred-line input, this is supposed to return the 1 line or the 8 characters or whatever it is that suffice to make the mystery-test function fail. Hint: There is Delta Debugging for it. The second part is actually just the same as above. However, try to produce these simplified inputs with a minimum of tests. For this you can rely on the HTML structure, as we just saw. Your challenge, of course, is to split these inputs into tokens, attributes, and regular text in the first place. Second, not only should your approach take a minimum of tests for these 5 inputs but also for other arbitrary inputs. And also, possibly, for other arbitrary mystery-test functions. We might even make a competition of that. Who is able to come up with the best simplifier for HTML? The winner will get La Grande Coupe de Dbogage Delta, which is how Delta Debugging is called in French. Enjoy. What you could get out of that is an automatic cause-effect chain isolating the causes for different places during the program execution. For instance, at the beginning s had a value of double quote and <, then c became a double quote again are well-defined in the failing room, then quote became true, and then out became <, which caused this version to fail. This cause-effect chain is what you get when you apply delta debugging on state differences at various places during the execution. In producing such a cause-effect chain automatically is your homework. This is a very cool outcome but is also a pretty tricky homework. In particular, setting variables while the program is executing is a feature that's very rarely used, very rarely tested, and therefore full of surprises. On the web page, you will find a number of hints and code templates that help you in accomplishing this task. Having such a cause-effect chain and getting it automatically sounds like a great promise for debugging In particular because it very neatly tells you the story of how the failure came to be. In particular the relationship of c having a double quote and then the quote variable becoming true points you directly to the condition, which is at fault at this point. It should--oh, sorry. This is already the closest to a fully automatic debugging tool we can get at this point. However, it is worth to very much consider the limitations. First and foremost you need to similar runs. The higher the similarity, the higher the chances of finding a small set of failure-inducing differences. Even in our homework implementation, there is many things we don't handle. We'll only handle scalar values and strings. We won't go into details of how to find differences between say, this or dictionaries or graph structures, and finally, all of these are still very experimental, but the fact that it's experimental calls for many ways to extend this, and this again can be part of your homework. Again, on the web page, you will find a number of ways of how to further expand this. So go and build fully automatic debugging for Python programs. Enjoy. Now for your homework--your homework for this unit will be statistical debugging on a mystery program. The ideal is that you get the mystery program and the number of runs which pass or fail and your job is to apply statistical debugging to find out what is it in the mystery program that causes the failures. However, this time we're going to look at another set of execution features that is not lines executed but instead the return values of individual functions. One of the outputs of your task could be that the program fails whenever f returns a value that's greater than zero. For this, you need to track the return values of individual functions and again, the Python trace it function will do that for you. The event is return and the return value is contained in the argument. Plus you need to categorize the return values in multiple booklets. I want you to use precisely three categories--namely less than zero, zero, and greater than zero. For numerical values, this is straightforward. Depending on the sign you put them into one of the three categories, but what for non numerical values say list or strings or dictionaries or sets. For these, you compute the length. If a string is empty, then it comes into the zero category. If it has one character or more, then it comes into the greater than zero category. Same goes for sets--no elements, one element or more, list in dictionaries likewise. The Boolean values false and true, also going to zero and greater than zero. Since less than zero is frequently used to indicate some exceptional return value, we're going to use this category for special values such as none, not a number. as well as for exceptions. That is if a function does not return a value but raises an exception instead, then you categorize this in the same way as the return value that is less than zero. What you then do is for each function you compute the five coefficient for each category as well as for its respective complement. What do I mean by complement? Well, let me illustrate this with an example. So again you set up a table and count in how many runs the function f return the value that was less than zero. Do this for the failing runs as well as for the passing runs. The complement here of course is any value that is greater or equal than zero and again you count the number of runs and this is something you do for each category. For the zero category, we check whether the return value was zero or whether it was non-zero and finally, we do the same for the third category namely greater than zero. Now your task will be to compute the five coefficients for all function and all categories as well as their compliments and then to find out for the mystery program, which is the one function whose return value category correlates the strongest with failure. Here's your chance to apply statistics to do the debugging. Enjoy.