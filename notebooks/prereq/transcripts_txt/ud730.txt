Welcome to this deep learning lecture. My name is Vincent, and I'm a research scientist at Google working on machine learning and artificial intelligence. Deep learning is an exciting branch of machine learning that uses data, lots of data, to teach computers how to do things only humans were capable of before. Myself, I am very interested in solving the problem of perception. Recognizing what's in an image, what people are saying when they are talking on their phone, helping robots explore the world and interact with it. Deep learning is emerge as a central tool to solve perception problems in recent years. It's the state of the art having to do with computer vision and speech recognition. But there's more. Increasingly, people are finding that deep learning is a much better tool to solve problems like discovering new medicines, understanding natural language, understanding documents, and for example, ranking them for search. So let's take a look at what you'll be doing during this class. This course has four parts. Just now, you're going to lay the foundations by training your first simple model entirely end-to-end. For that, we'll need to talk about logistic classification, stochastic optimization and general data practices to train models. In the next session, we're going to go deeper. You'll train your first deep network and you'll also learn to apply regularization techniques to train even bigger models. The third session will be a deep dive into images and convolutional models. The fourth session, all about text and sequences in general. We'll train embeddings and recurrent models. This is going to be a very hands-on course. The assignments are IPython notebooks that you can read and modify easily. Play with them and try things out. We will use them alongside the lectures to make everything you will hear about very concrete. All the code will be written in TensorFlow, a very simple Python-based deep learning toolkit. By the time you are done with these lectures, you'll be able to apply all this knowledge to new problems entirely by yourself with a handful of lines of code. Many companies today have made deep learning a central part of their machine learning toolkit. Facebook, Baidu, Microsoft, and Google are all using deep learning in their products and pushing their research forward. It's easy to understand why. Deep learning shines wherever there is lots of data and complex problems to solve. And all these companies are facing lots of complicated problems, like understanding what's in an image to help you find it, or translating a document into another language that you can speak. In this class, we'll explore a continuum of complexity, from very simple models to very large ones that you'll still be able to train in minutes on your personal computer to do very elaborate tasks. Like predicting the meaning of words or classifying images. One of the nice things about deep learning is that it's really a family of techniques that adapts to all sorts of data and all sorts of problems, all using a common infrastructure and a common language to describe things. When I began working in machine learning, I was doing research on speech recognition. Whenever I met someone working on computer vision or natural language processing, we had very little in common to talk about. We used different acronyms, different techniques, and collaboration was pretty much impossible. Deep learning really changed that. The basic things that you will learn in this class are applicable to almost everything, and will give you a foundation that you can apply to many, many different fields. You'll be part of a fast growing community of researchers, engineers, and data scientists who share a common, very powerful set of tools. A lot of the important work on neural networks happened in the 80's and in the 90's but back then computers were slow and datasets very tiny. The research didn't really find many applications in the real world. As a result, in the first decade of the 21st century neural networks have completely disappeared from the world of machine learning. Working on neural networks was definitely fringe. It's only in the last few years, first seeing speech recognition around 2009, and then in computer vision around 2012, that neural networks made a big comeback. What changed? Lots of data and cheap and fast DBU's. Today, neural networks are everywhere. So if you're doing anything with data, analytics or prediction, they're definitely something that you want to get familiar with. So let's get started. This entire course, I'm going to focus on the problem of classification. Classification is the task of taking an input, like this letter, and giving it a label that says, this is a B. The typical setting is that you have a lot of examples, called the training set, that I've already been sorted in. This is an A, this is a B, and so on. Now, here's a completely new example. And your goal is going to be to figure out which of those classes it belongs to. There is a lot more to machine learning that just classification. But classification, or marginally prediction, is the central building block of machine learning. Once you know how to classify things, it's very easy, for example, to learn how to detect them, or to rank them. Let's take an example, the task of detection. Imagine you have a camera in your car looking at the street ahead. You want to detect where pedestrians are in front of you, so that you don't hit them. How would you use a classifier to do that? Type in what approach you would take in the box provided. One way to turn the detection problem into a classification problem is to have a classifier that classified small patches of the image into two classes. Pedestrian or no pedestrian. You can then run this classifier many times over the image and whenever it says pedestrian that gives you the location that you're looking for. Here is another example. Web search ranking. Imagine you have a search query, and you want to find all the web pages on the web, that are relevant for that query. How would you use a classifier for that? Type in what approach you would take, in the box provided. Here's a way to do it. Make the classifier take the pair query and webpage, and output one of two classes, either relevant or not relevant. Of course, it would be a lot of webpages to look at if you ran that classifier on the whole web. But search engines take shortcuts, and only try to classify promising candidates. So let's learn about classification. Here is the plan. I'm going to tell you just enough, so that you can put together a very simple, but very important type of classifier, called the logistic classifier. With that, you'll be able to run through the first assignment which will have you download and pre-process some images for classification. And then run an actual logistic classifier on that data. Once you've connected the bit of math that's coming up and the code in that first assignment, everything else will just be building up from there. We will be building together all of the pruning on top of this piece, block by block. So let's get started training a logistic classifier. A logistic classifier is what's called the linear classifier. It takes the input, for example, the pixels in an image, and applies a linear function to them to generate its predictions. A linear function is just a giant matrix multiply. It take all the inputs as a big vector, that will denote X, and multiplies them with a matrix to generate its predictions, one per output class. Throughout, we'll denote the inputs by X, the weights by W, and the biased term by b. The weights of the matrix and the bias, is where the machine learning comes in. We're going to train that model. That means we're going to try to find the values for the weights and bias, which are good at performing those predictions. How are we going to use the scores to perform the classification? Well, let's recap our task. Each image, that we have as an input can have one and only one possible label. So, we're going to turn the scores into probabilities. We're going to want the probability of the correct class to be very close to one and the probability for every other class to be close to zero. The way to turn scores into probabilities is to use a softmax function, which I'll denote here by S. This is what it looks like. But beyond the formula, what's important to know about it is that it can take any kind of scores and turn them into proper probabilities. Proper probabilities, sum to 1, and they will be large when the scores are large and small when the scores are comparatively smaller. Scores in the context of logistic regression are often also called logits. Let's play with this Softmax function for a little bit. Let's say you have a classifier which outputs three scores for three classes. My question is, what probability does the Softmax function give you? Define the function softmax(x) to compute the softmax probability values given this set of scores. Let's plot how the probabilities vary as we modify the scores, for example, of the first class. Note that the scores that are passed through the softmax here can be an numpy array with one row for each score, three in this case. And some arbitrary number of columns, one for each sample. Your function should be able to handle such input and return a numpy array of the same shape. Here is an example solution. We take the exponential of the scores and we divide by the sum of the exponential of the scores across the other categories. Let's say testra. Here is the result. Notice that the probabilities do sum to one. Let's add some legends to make this a little bit more clear. As you can see, the probability of the class one increases with the score x. It starts near zero and it ends close to one. At the same time, the probabilities of the other classes start pretty high, but then go vanishingly down to 0. Feel free to go back and play with the softmax function. For example you could draw a bar chart to convince yourself that the probabilities do sum to one for every value of X. Here is something to keep in mind about the Softmax. Say you take all the scores and multiply them by 10, what happens? Do the probabilities get close to either 0.0 or 1.0? Or do the probabilities goes close to the uniform distribution? If you multiple the scores by 10, the scores get either very close to 1.0, or very, very small. Therefore, when you multiply the scores by 10, the first answer is correct. Now divide the scores by 10. What happens? Does the probabilities then get close to either 0 or 1? Or do they get close to the uniform distribution? When you divide the scores by 10, the probabilities become very close to the uniform. One third, one third, one third. The second answer is correct. In other words, if you increase the size of your outputs, your classifier becomes very confident about its predictions. But if you reduce the size of your outputs, your classifier becomes very unsure. Keep this in mind for later. We'll want our classifier to not be too sure of itself in the beginning. And then over time, it will gain confidence as it learns. Next, we need a way to represent our labels mathematically. We just said, let's have the probabilities for the correct class be close to 1, and the probability for all the others be close to 0. We can write exactly with that. Each label will be represented by a vector that is as long as there are classes and it has the value 1.0 for the correct class and 0 every where else. This is often called one-hot encoding. Here's some class labels, and they're partially filled out when hot hang putting vectors. Type in the remaining values so that they are consistent, you can leave the zeros blank. Image that your classifier is giving you these predictions. Now, which class is the most likely according to this? Enter your answer here. For a consistent one hut encoding, we need to pick ones such that each class gets a unique position on the vector. Here's what I picked. Given this encoding the most likely classes is C. When hot encoding thing including works very well from most problems until you get into situations where you have tens of thousands, or even millions of classes. In that case, your vector becomes really, really large and has mostly zeros everywhere and that becomes very inefficient. You'll see later how we can deal with these problems using embeddings. What's nice about this approach is that we can now measure how well we're doing by simply comparing two vectors. One that comes out of your classifiers and contains the probabilities of your classes and the one hot encoded vector that corresponds to your labels. Let's see how we can do this in practice. The natural way to measure the distance between those two probability vectors is called the cross-entropy. I'll denote it by D here for distance. Math, it looks like this. Be careful, the cross-entropy is not symmetric and you have a nasty log in there. So you have to make sure that your labels and your distributions are in the right place. Your labels, because they're one hot encoded, will have a lot of zeroes in them and you don't want to take the log of zeroes. For your distribution, the softmax will always guarantee that you have a little bit of probability going everywhere, so you never really take a log of zero. So lets recap, because we have a lot of pieces already. So we have an input, it's going to be turned into logits using a linear model, which is basically your matrix multiply and a bias. We're then going to feed the logits, which are scores, into a softmax to turn them into probabilities. And then we're going to compare those probabilities to the one hot encoded labels using the cross entropy function. This entire setting is often called multinomial logistic classification. Okay, so now we have all the pieces of our puzzle. The question of course is how we're going to find those weights w and those biases b that will get our classifier to do what we want it to do. That is, have a low distance for the correct class but have a high distance for the incorrect class. One thing you can do is measure that distance averaged over the entire training sets for all the inputs and all the labels that you have available. That's called the training loss. This loss, which is the average cross-entropy over your entire training set, Is one humongous function. Every example in your training set gets multiplied by this one big matrix W. And then they get all added up in one big sum. We want all the distances to be small, which would mean we're doing a good job at classifying every example in the training data. So we want the loss to be small. The loss is a function of the weights and the biases. So we are simply going to try and minimize that function. Imagine that the loss is a function of two weights. Weight one and weight two. Just for the sake of argument. It's going to be a function which will be large in some areas, and small in others. We're going to try the weights which cause this loss to be the smallest. We've just turned the machine learning problem into one of numerical optimization. And there's lots of ways to solve a numerical optimization problem. The simplest way is one you've probably encountered before, gradient descent. Take the derivative of your loss, with respect to your parameters, and follow that derivative by taking a step backwards and repeat until you get to the bottom. Gradient descent is relatively simple, especially when you have powerful numerical tools that compute the derivatives for you. Remember, I'm showing you the derivative for a function of just two parameters here, but for a typical problem it could be a function of thousands, millions or even billions of parameters. In the coming lectures, I'll talk about these tools that compute the derivatives for you, and a lot about what's good and bad about grading descent. For the moment, though, we'll assume that I give you the optimizer as a black box that you can simply use. There are two last practical things that stand in your way of training your first model. First is how do you fill image pixels to this classifier and then where do you initialize the optimization? Let's look into this. This is where we have to talk a bit about numerical stability. When you do numerical computations, you always have to worry a bit about calculating values that are too large or too small. In particular, adding very small values to a very large value can introduce a lot of errors. Try this in Python. Take the value 1,000,000,000 and then add to it the value 10 to the minus 6 1,000,000 times, then subtract 1,000,000,000 again. What do you get? If you did the math in your head, you probably got 1.0, but that's not what the code says. Here you get something around 0.95. In the example in the quiz, the math says the result should be 1.0. But the code says 0.95. That's a big difference. Go ahead, replace the one billion with just one, and you'll see that the arrow becomes tiny. We're going to want the values involved in the calculation of this big loss function that we care about to never get too big or too small. One good guiding principle is that we always want our variables to have zero mean and equal variance whenever possible. On top of the numeric issues, there are also really good mathematical reasons to keep values you compute. Roughly around a mean of zero, an equal variance when you're doing optimization. A badly conditioned problem means that the optimizer has to do a lot of searching to go and find a good solution. A well conditioned problem makes it a lot easier for the optimizer to do its job. If your dealing with images, it's simple. You can take the pixel values of your image, they are typically between 0 and 255. And simply subtract 128 and divide by 128. It doesn't change the content of your image, but it makes it much easier for the optimization to proceed numerically. You also want your weights and biases to be initialized at a good enough starting point for the gradient descent to proceed. There are lots of fancy schemes to find good initialization values, but we're going to focus on a simple, general method. Draw the weights randomly from a Gaussian distribution with mean zero and standard deviation sigma. The sigma value determines the order of magnitude of your outputs at the initial point of your optimization. Because of the soft max on top of it, the order of magnitude also determines the peakiness of your initial probability distribution. A large sigma means that your distribution will have large peaks. It's going to be very opinionated. A small sigma means that your distribution is very uncertain about things. It's usually better to begin with an uncertain distribution and let the optimization become more confident as the train progress. So use a small sigma to begin with. Okay, so now we actually have everything we need to actually train this classifier. We've got our training data, which is normalized to have zero mean and unit variance. We multiply it by a large matrix, which is initialized with random weights. We apply the soft max and then the cross entropy loss and we calculate the average of this loss over the entire training data. But our magical optimization package computes the derivative of this loss with respect to the weights and to the biases and takes a step back in the direction opposite to that derivative. And then we start all over again, and repeat the process until we reach a minimum of the loss function. We've just gone over the essential things that you need to be able to train a simple linear classification model. Now, it's your turn. How well does this work? Pretty well, as you will see in your first assignment. The first part of it will have you download and pre-process an image data set that you will be reusing throughout the lectures, and that you can use yourself to experiment with. The second part will have you go through an exact re-implementation, piece by piece, of all the components of the logistic classifier that we've discussed so far. Make sure that you carefully step through the code before coming back to the videos. Run it, break it, tinker with it. We'll be building on top of it for the rest of the lecture. You'll also quickly see some of the problems with the simple approach. It can be very slow. Even for a simple model like this linear model on a very small amount of training data. Scaling this up is what deep learning is all about. Now that you have trained your first model. There is something very important I want to discuss. You might have seen in the assignment that we had a training set as well as a validation set and a test set. What is that all about? Don't skip that part, it has to do with measuring how well you're doing, without accidentally shooting yourself in the foot. And it is a lot more subtle than you might initially think. It's also very important because as we'll discover later, once you know how to measure your performance on a problem you've already solved half of it. Let me explain why measuring performance is subtle. Let's go back to our classification task. You've got a whole lot of images with labels. You could say, okay, I'm going to run my classifier on those images and see how many I got right. That's my error measure, and then you go out and use your classifier on new images, images that you've never seen in the past. And you measure how many you get right, and your performance gets worse, the classifier doesn't do as well. So what happened? Well imagine I construct a classifier that simply compares the new image to any of the other images that I've already seen in my training set, and just return the label. By the measure we defined earlier, it's a great classifier. It would get 100% accuracy on the training sets. But as soon as it sees a new image it's lost, it has no idea what to do. It's not a great classifier. The problem is that your classifier has memorized the training set and it fails to generalize to new examples. It's not just a theoretical problem. Every classifier that you will build will tend to try and memorize the training sets. And it will usually do that very, very well. Your job, though, is to help it generalize to new data instead. So, how do we measure the generalization instead of measuring how well the classifier memorized the data? The simplest way is to take a small subset of the training set. Not use it in training and measure the error on that test data. Problem solved, now your classifier cannot cheat because it never sees the test data so it can't memorize it. But there is still a problem because training a classifier is usually a process of trial and error. You try a classifier, you measure its performance, and then you try another one, and you measure again. And another, and another. You tweak the model, you explore the parameters, you measure, and finally, you have what you think is the prefect classifier. And then after all this care you've taken to separate your test data from your training data and only measuring your performance on the test data, now you deploy your system in a real production environment. And you get more data. And you score your performance on that new data, and it doesn't do nearly as well. What can possibly have happened? What happened is that your classifier has seen your test data indirectly through your own eyes. Every time you made a decision about which classifier to use, which parameter to tune, you actually gave information to your classifier about the test set. Just a tiny bit, but it adds up. So over time, as you run many and many experiments, your test data bleeds into your training data. So what can you do? There are many ways to deal with this. I'll give you the simplest one. Take another chunk of your training sets and hide it under a rock. Never look at it until you have made your final decision. You can use your validation set to measure your actual error and maybe the validation set will bleed into the training set. But that's okay because you're always have this test set that you can rely on to actually measure your real performance Does splitting your data into three separate data sets sound overly complicated? Let's look at this in action in the real world. Kaggle is a competition platform for machine learning. People compete on classification tasks, and whoever gets the highest performance wins. Here, I'm showing the example of a scientific competition on data that relates to the Higgs Boson. Kaggle also has three data sets, the training data, the public validation data set, and a private data set that is not revealed to the competitors. Here Kaggle shows you the performance of the top competitors when measured on the private test sets. The green and red arrows show how different the ranking was compared to the ranking on the public set. Let's look at the rankings. The top competitors were doing well on the public validation data and they remain at the top once their private data was revealed. If you go further down the leaderboard, however, it's a real bloodshed. Many competitors who thought they were doing well, were not doing well at all in the private data sets. As a result, their ranks went down dramatically once the private data set was revealed. Why is that? Maybe they had a good model that did well in validation just by chance. What's more likely however, is that by validating themselves over and over dozens of times on the validation set, they ended up over fitting to the validation set and failing to generalize. The top competitors] however, had good experimental design. They were not misled into thinking that they were doing well. They probably took out some of the training sets to validate their algorithm or used more sophisticated methods like cross validation and didn't make many decisions based on the public data set scores. I'm not going to talk about cross validation here, but if you've never encountered it in your curriculum, I'd strongly recommend that you learn about it. I am spending time on this because it's essential to deep learning in particular. Deep learning has many knobs that you can tweak, and you will be tempted to tweak them over and over. You have to be very careful about overfitting on your test set. Use the validation set. How big does your validation and test sets need to be? It depends. The bigger your validation set the more precise your numbers will be. Imagine that your valuation set has just six examples with an accuracy of 66%. Now you tweak your model and your performance goes from 66% to 83%, is this something you can trust? No of course, this is only a change of label for a single example. It could just be noise. The bigger you test set, the less noisy the accuracy measurement will be. Here is a useful rule of thumb. And if you're a statistician, feel free to cover your ears right now. A change that affects 30 examples in your validation set, one way or another, is usually statistically significant, and typically can be trusted. Let's do some back of the envelope calculations. Imagine you have 3,000 examples in your variation set, and assume you trust my hand wavey rule of 30. Which level of accuracy improvement can you trust to not be in the notice? A difference from 80% to 81%, a difference from 80% to 80.5%, a difference of 80% to 80.1%? If your accuracy changes from 80% to 80.1%, that's only at most three examples changing their labels. It's 0.1 x 3000 divided by 100. That's very few. It could just be noise. And it definitely doesn't meet my rule of thumb of 30 examples minimum. Same thing going from 80% to 80.5%. At worst, only 15 examples are changing then. When you get an improvement of 1% going from 80% to 81%, that's now a more robust 30 examples that are going from incorrect to correct. That's a stronger signal that whatever you're doing is indeed improving your accuracy. This is why for most classification tasks people tend to hold back more than 30,000 examples for validation. This makes accuracy figures significant to the first decimal place and gives you enough resolution to see small improvements. If your classes are not well balanced, for example, if some important classes are very rare, his heuristic is no longer good. Bad news, you're only going to need much more data. Now, holding back even 30,000 examples can be a lot of data if you have a small training set. Cross-validation, which I've mentioned before, is one possible way to mitigate the issue. But cross-validation can be a slow process, so getting more data is often the right solution. With that out of the way, let's go back to training models. Training logistic regression using gradient descent is great. For one thing, you're directly optimizing the error measure that you care about. That's always a great idea. And that's why in practice, a lot of machine learning research is about designing the right last function to optimize. But as you might experienced if you've run the model in the assignments, it's got problems. The biggest one is that it's very difficult to scale. The problem with scaling gradient descent is simple. You need to compute this gradient. Here is another rule of thumb. If computing your loss takes n floating point operations, computing its gradient takes about three times that compute. As we saw earlier, this last function is huge. It depends on every single element in your training set. That can be a lot of compute if your data set is big. And we want to be able to train lots of data because in practice on real problems you will always get more gains the more data you use. And because gradient descent is intuitive, you have to do that for many steps. That means going through your data tens or hundreds of times. That's not good, so instead, we're going to cheat. Instead of computing the loss, we're going to compute an estimate of it, a very bad estimate, a terrible estimate in fact. That estimate is going to be so bad, you might wonder why it works at all. And you would be right, because we're going to also have to spend some time making it less terrible. The estimate we're going to use is simply computing the average loss for a very small random fraction of the training data. Think between 1 and 1000 training samples each time. I say random because it's very important. If the way you pick your samples isn't random enough, It no longer works at all. So, we're going to take a very small sliver of the training data, compute the loss for that sample, compute the derivative for that sample and pretend that that derivative is the right direction to use to do gradient descent. It is not at all the right direction, and in fact at times it might increase the real loss, not reduce it. But we're going to compensate by doing this many, many times, taking very, very small steps each time. So each step is a lot cheaper to compute. But we pay a price. We have to take many more smaller steps, instead of one large step. On balance, though, we win by a lot. In fact, as you'll see in the assignments, doing this is vastly more efficient than doing gradient descent. This technique is called stochastic gradient descent and is at the core of deep learning. That's because stochastic gradient descent scales well with both data and model size, and we want both big data and big models. Stochastic gradient descent, SGD for short, is nice and scalable. But because it is fundamentally a pretty bad optimizer, that happens to be the only one that's fast enough, it comes with a lot of issues in practice. You've already seen some of those tricks. I asked you to make your inputs zero mean and equal variance earlier. It's very important for SGD. I also told you to initialize with random weights that have relatively small variance, same thing. I'm going to talk about a few more of those important tricks, and that should cover all you really need to worry about to implement SGD. The first one is momentum. Remember that at each step, we're taking a very small step in a random direction, but that on aggregate, those steps take us toward the minimum of the loss. We can take advantage of the knowledge that we've accumulated from previous steps about where we should be headed. A cheap way to do that is to keep a running average of the gradients, and to use that running average instead of the direction of the current batch of the data. This momentum technique works very well and often leads to better convergence. The second one is learning rate decay. Remember, when replacing gradient descent with SGD, I said that we were going to take smaller, noisier steps towards our objective. How small should that step be? That's a whole area of research as well. One thing that's always the case, however, is that it's beneficial to make that step smaller and smaller as you train. Some like to apply an exponential decay to the learning rate some like to make it smaller every time the loss reaches a plateau, there are lots of ways to go about it, but lowering it over time is the key thing to remember. Learning rate tuning can be very strange. For example, you might think that using a higher learning rate means you learn more or that you learn faster. That's just not true. In fact, you can often take a model, lower the learning rate and get to a better model faster. It gets even worse. You might be tempted to look at the curve that shows the loss over time to see how quickly you learn. Here the higher learning rate starts faster, but then it plateaus, when the lower learning rate keeps on going and gets better. It is a very familiar picture for anyone who's trained neural networks. Never trust how quickly you learn. It has often little to do with how well you train. This is where SGD gets its reputation for being black magic. You have many, many hyper-parameters that you could play with. Initialization parameters, learning rate parameters, decay, momentum. And you have to get them right. In practice, it's not that bad. But if you have to remember just one thing, it's that when things don't work, always try to lower your learning rate first. There are lots of good solutions for small models. But sadly, none that's completely satisfactory so far for the very large models that we really care about. I'll mention one approach called AdaGrad that makes things a little bit easier. AdaGrad is a modification of SGD which implicitly does momentum and learning rate decay for you. Using AdaGrad often makes learning less sensitive to hyper-parameters. But it often tends to be a little worse than precisely tuned SDG with momentum. It's still a very good option, though, if you're just trying to get things to work. So, let's recap. We have this very simple linear model which emits probabilities which we can use to classify things. We now know how to optimize its parameters on lots and lots of data using SGD and its variants. It's still a linear, shallow model, though, but now we have all the tools that we need. It's time to go deeper. In the last lesson we've trained a simple logistic classifier on images. Now, we're going to take this classifier and turn it into a deep network. And it's going to be just a few lines of code, so make sure you understand well what was going on in the previous model. In the second part you are going to take a small peak into how our optimizer does all the hard work for you computing gradients for arbitrary functions. And then we are going to look together at the important topic of regularization, which will enable us to train much, much larger models. The simple model that you've trained so far is nice, but it's also relatively limited. Here is a question for you. How many train parameters did it actually have? As a reminder, each input was a 28 by 28 image, and the output was 10 classes. Enter the number of parameters here. The matrix, W, here, takes, as an input, the entire image, so 28x28 pixels. The output is of size 10, so the other dimension of the matrix is 10. The biases are just 1x10. So the total number of parameters are 28x28x10, plus another ten, that gives you 7,850. That's the case in general. If you have N inputs, and K outputs, you have (N+1)K parameters to use. Not one more. The thing is, you might want to use many, many more parameters in practice. Also, it's linear. This means that the kind of interactions that you're capable of representing with that model is somewhat limited. For example, if two inputs interact in an additive way, your model can represent them well as a matrix multiply. But if two inputs interact in the way that the outcome depends on the product of the two for example, you won't be able to model that efficiently with a linear model. Linear operations are really nice though. Big matrix multiplies are exactly what GPUs were designed for. They're relatively cheap and very, very fast. Numerically linear operations are very stable. We can show mathematically that small changes in the input can never yield big changes in the output. The derivates are very nice too. The derivative of a linear function is constant. You can't get more stable numerically than a constant. So, we would like to keep our parameters inside big linear functions, but we would also want the entire model to be nonlinear. We can't just keep multiplying our inputs by linear functions, because that's just equivalent to one big linear function. So, we're going to have to introduce non-linearities. Let me introduce you to the lazy engineer's favorite non-linear function: the rectified linear units, or RELU for short. RELUs are literally the simplest non-linear functions you can think of. They're linear if x is greater than 0, and they're the 0 everywhere else. RELUs have nice derivatives, as well. Which of these plots do you think best represents the derivative of a RELU? When x is less than zero, the value is 0. So, the derivative is 0 as well. When x is greater than 0, the value is equal to x. So, the derivative is equal to 1. The second answer is the right answer. Because we're lazy engineers, we're going to take something that works, a logistic classifier and do the minimal amount of change to make it non-linear. We're going to construct our new function in the simplest way that we can think of. Instead of having a single matrix multiplier as our classifier, we're going to insert a RELU right in the middle. We now have two matrices. One going from the inputs to the RELUs, and another one connecting the RELUs to the classifier. We've solved two of our problems. Our function in now nonlinear thanks to the RELU in the middle, and we now have a new knob that we can tune, this number H which corresponds to the number of RELU units that we have in the classifier. We can make it as big as we want. Congratulations, you've built your first neural network. You might ask, wait a minute, where's my neuron? In the past, when talking about neural networks, I remember seeing diagrams with dendrites, axons, activation functions, brains, neuroscience. Where is all that? Yes, I could talk about neural networks as metaphors for the brain. It's nice and it might even be true, but it comes with a lot of baggage and it can sometimes lead you astray. So I'm not going to talk about it at all in this course. No need to be a wizard nor a scientist. New networks naturally make sense if you're simply a lazy engineer with a big GPU who just wants machine learning to work better. Now, though, is a good time to talk about the math. One reason to build this network by stacking simple operations, like multiplications, and sums, and RELUs, on top of each other is that it makes the math very simple. Simple enough that a deep learning framework can manage it for you. The key mathematical insight is the chain rule. If you have two functions that get composed, that is, one is applied to the output of the other, then the chain rule tells you that you can compute the derivatives of that function simply by taking the product of the derivatives of the components. That's very powerful. As long as you know how to write the derivatives of your individual functions, there is a simple graphical way to combine them together and compute the derivative for the whole function. There's even better news for the computer scientist in you. There is a way to write this chain rule that is very efficient computationally, with lots of data reuse, and that looks like a very simple data pipeline. Here's an example. Imagine your network is a stack of simple operations. Like in your transforms, whatever you want. Some have parameters like the matrix transforms, some don't like the rellers. When you apply your data to some input x, you have data flowing through the stack up to your predictions y. To compute the derivatives, you create another graph that looks like this. The data in the new graph flows backwards through the network, get's combined using the chain rule that we saw before and produces gradients. That graph can be derived completely automatically from the individual operations in your network. So most deep learning frameworks will just do it for you. This is called back-propagation, and it's a very powerful concept. It makes computing derivatives of complex function very efficient as long as the function is made up of simple blocks with simple derivatives. Running the model up to the predictions is often call the forward prop, and the model that goes backwards is called the back prop. So, to recap, to run stochastic gradient descent, for every single little batch of your data in your training set, you're going to run the forward prop, and then the back prop. And that will give you gradients for each of your weights in your model. Then you're going to apply those gradients with the learning weights to your original weights, and update them. And you're going to repeat that all over again, many, many times. This is how your entire model gets optimized. I am not going to go through more of the maths of what's going on in each of those blocks. Because, again, you don't typically have to worry about that, and it's essentially the chain rule, but keep in mind, this diagram. In particular each block of the back prop often takes about twice the memory that's needed for prop and twice the compute. That's important when you want to size your model and fit it in memory for example. So now you have a small neural network. It's not particularly deep, just two layers. You can make it bigger, more complex, by increasing the size of that hidden layer in the middle. But it turns out that increasing this H is not particularly efficient in general. You need to make it very, very big, and then it gets really hard to train. This is where the central idea of deep learning comes into play. Instead, you can also add more layers and make your model deeper. There are lots of good reasons to do that. One is parameter efficiency. You can typically get much more performance with fewer parameters by going deeper rather than wider. Another one is that a lot of natural phenomena that you might be interested in, tend to have a hierarchical structure which deep models naturally capture. If you poke at a model for images, for example, and visualize what the model learns, you'll often find very simple things at the lowest layers, like lines or edges. Once you move up, you tend to see more complicated things like geometric shapes. Go further up and you start seeing things like objects, faces. This is very powerful because the model structure matches the kind of abstractions that you might expect to see in your data. And as a result the model has an easier time learning them. Why did we not figure out earlier that t-models were effective? Many reasons, but mostly because t-models only really shine if you have enough data to train them. It's only in recent years that large enough data sets have made their way to the academic world. We'll look at another reason now, we know better today how to train very, very big models using better regularization techniques. There is a general issue when you're doing numerical optimization which I call the skinny jeans problem. Skinny jeans look great, they fit perfectly, but they're really, really hard to get into. So most people end up wearing jeans that are just a bit too big. It's exactly the same with deep networks. The network that's just the right size for your data is very, very hard to optimize. So in practice, we always try networks that are way too big for our data and then we try our best to prevent them from overfitting. The first way we prevent over fitting is by looking at the performance under validation set, and stopping to train as soon as we stop improving. It's called early termination, and it's still the best way to prevent your network from over-optimizing on the training set. Another way is to apply regularization. Regularizing means applying artificial constraints on your network that implicitly reduce the number of free parameters while not making it more difficult to optimize. In the skinny jeans analogy, think stretch pants. They fit just as well, but because they're flexible, they don't make things harder to fit in. The stretch pants of are called L2 Regularization. The idea is to add another term to the loss, which penalizes large weights. It's typically achieved by adding the L2 norm of your weights to the loss, multiplied by a small constant. And yes, yet another hyper-perimeter. Sorry about that. The nice thing about l2 regularization is that its very, very simple. Because you just add it to your loss, the structure of your network doesn't have to change. You can even compute its derivative by hand. Remember that the l2 norm stands for the sum of the squares of the individual elements in a vector. Which of these formulas gives you the derivative of the l2 norm of a vector? You know that the derivative of 1/2 of x squared in one dimension is simply x. So when you take that derivative for each of the components of your vector you get the same components. Therefore, the answer is the third one. There's another important technique for regularization that only emerged relatively recently and works amazingly well. It also looks insane the first time you see it, so bear with me. It's called dropout. Dropout works likes this. Imagine that you have one layer that connects to another layer. The values that go from one layer to the next are often called activations. Now take those activations and randomly for every example you train your network on, set half of them to zero. Completely randomly, you basically take half of the data that's flowing through your network and just destroy it. And then randomly again. If that doesn't sound crazy to you then you might qualify to become a student of Jeffery Hinton who pioneered the technique. So what happens with dropout? Your network can never rely on any given activation to be present because they might be squashed at any given moment. So it is forced to learn a redundant representation for everything to make sure that at least some of the information remains. It's like a game of whack-a-mole. One activations gets smashed, but there's always one or more that do the same job and that don't get killed. So everything remains fine at the end. Forcing your network to learn redundant representations might sound very inefficient. But in practice, it makes things more robust and prevents over fitting. It also makes your network act as if taking the consensus over an ensemble of networks. Which is always a good way to improve performance. Dropout is one of the most important techniques to emerge in the last few years. If drop out doesn't work for you, you should probably be using a bigger network. When you evaluate the network that's been trained with drop out, you obviously no longer want this randomness. You want something deterministic. Instead, you're going to want to take the consensus over these redundant models. You get the consensus opinion by averaging the activations. You want Y e here to be the average of all the yts that you got during training. Here's a trick to make sure this expectation holds. During training, not only do you use zero out so the activations that you drop out, but you also scale the remaining activations by a factor of 2. This way, when it comes time to average them during evaluation, you just remove these dropouts and scaling operations from your neural net. And the result is an average of these activations that is properly scaled. Now it's your turn. In the assignment you'll be building a network by making the logistic classifier deep. You'll also play with the various forms of regularization. The amount of code that you need to change is very small, so don't hesitate to play around and try new things. So far, we've talked about new networks in general. But if you know something about your data, for example, if it's an image or a sequence of things you can do a lot better. The idea is very simple. If your data has some structure and your network doesn't have to learn that structure from scratch, it's going to perform better. Imagine for example, that you're trying to classify those letters, and you know that color is really not a factor in what makes an A an A. What do you think would be easier for your classifier to learn? A model that uses the color image or a model that only looks at the grayscale. Intuitively, if a new letter comes up in a color that you have never seen before it's going to be lot easier for your model that ignores the color to begin with to classify that letter. Here's another example. You have an image and you want your network to say it's an image with a cat in it. it doesn't really matter where the cat is, it's still an image with a cat. If your network has to learn about kittens in the left corner and about kittens in the right corner, independently, that's a lot of work that it has to do. How about you telling it instead, explicitly, that objects and images are largely the same whether they're on the left or on the right of the picture. That's what's called translation invariance,. Different positions, same kitten. Yet another example. Imagine you have a long text that talks about kittens. Does the meaning of kitten change depending on whether it's in the first sentence or in the second one? Mostly not. So if you're trying to network on text, maybe you want the part of the network that learns what a kitten is to be reused every time you see the word kitten, and not have to relearn it every time. The way you achieve this in your own networks is using what is called weight sharing. When you know that two inputs can contain the same kind of information, then you share the weights and train the weights jointly for those inputs. it is a very important idea. Statistical invariants, things that don't change on average across time or space, are everywhere. For images, the idea of weight sharing will get us to study convolutional networks. For text and sequences in general, it will lead us to embeddings and recurrent neural networks. Let's talk about convolutional networks, or convnets. Convnets are neural networks that share their parameters across space. Imagine you have an image. It can be represented as a flat pancake. It has a width and a height. And because you typically have red, green, and blue channels, it also has a depth. In this instance, depth is 3, that's your input. Now imagine taking a small patch of this image and running a tiny neural network on it, with say, K outputs. Let's represent those outputs vertically, in a tiny column like this. Now let's slide that little neural network across the image without changing the weights. Just slide across and vertically like we're painting it with a brush. On the output, we've drawn another image. It's got a different width, a different height, and more importantly, it's got a different depth. Instead of just R, G and B, now you have an output that's got many color channels, K of them. This operation is called a convolution. If your patch size were the size of the whole image, it would be no different than a regular layer of a neural network. But because we have this small patch instead, we have many fewer weights and they are shared across space. A convnet is going to basically be a deep network where instead of having stacks of matrix multiply layers, we're going to have stacks of convolutions. The general idea is that they will form a pyramid. At the bottom you have this big image but very shallow, just R, G, and B. You're going to apply convolutions that are going to progressively squeeze the spatial dimensions while increasing the depth, which corresponds roughly to the semantic complexity of your representation. At the top you can put your classifier. You have a representation where all the spatial information has been squeezed out and only parameters that map to contents of the image remain. So that's the general idea. If you're going to implement this, there are lots of little details to get right and a fair bit of lingo to get used to. You've met the concept of patch and depth. Patches are sometimes called kernels. Each pancake in your stack is called a feature map. Here, you're mapping three feature maps to K feature maps. Another term that you need to know is stride. It's the number of pixels that you're shifting each time you move your filter. A stride of 1 makes the output roughly the same size as the input. A stride of 2 means it's about half the size. I say roughly, because it depends a bit about what you do at the edge of your image. Either, you don't go past the edge, and it's often called valid padding as a shortcut. Or you go off the edge and pad with zeros in such a way that the output map size is exactly the same size as the input map. That is often called same padding as a shortcut. Imagine that you have a 28 by 28 image. You run a 3 by 3 convolution on it with an input depth of 3 and an output depth of 8. What are the dimensions of your output feature maps? When you're using same padding with a stride of 1, when you're using valid padding with a stride of 1, and when you're using valid padding with a stride of 2. If you're using the so-called same padding and a stride of 1, the output width and height are the same as the input. We just add zeroes to the input image to make the sizes match. If you use the so-called valid padding and a stride of 1, then there is no padding at all. So if you want to fit your little filter on the input image without doing any padding, you're going to have to remove one row and one column of the image on each side. So you're left with 26 features in each of the maps at the output. If in addition you use a stride of 2, then you only get half as many outputs. So 13 in width and 13 in height. In all cases, the output depth isn't changed That's it. You can build a simple covenant with just this. Stack up you convulsions, which thankfully you don't have to implement yourselves, then use trieds to to reduce the dimensional ID and increase the depth of your network, layer after layer. And once you have a deep and narrow representation, connect the whole thing to a few regular fully connected layers and you're ready to train your classifier. You might wonder what happens to training, into the chain rule in particular, when you use shared weights like this. Nothing really happens. The math just works. You just add up the derivates from all of the possible locations on the image. Now that you've seen what a simple convnet looks like, there are many things that we can do to improve it. We're going to talk about three of them, pooling, one by one convolutions and something a bit more advanced called the inception architecture. The first improvement is a better way to reduce the spatial extent of your feature maps in the convolutional pyramid. Until now, we've used striding to shift the filters by a few pixel each time and reduce the future map size. This is a very aggressive way to downsample an image. It removes a lot of information. What if instead of skipping one in every two convolutions, we still ran with a very small stride, say for example one. But then took all the convolutions in a neighborhood and combined them somehow. That operation is called pooling, and there are a few ways to go about it. The most common is max pooling. At every point in the future map, look at a small neighborhood around that point and compute the maximum of all the responses around it. There are some advantages to using max pooling. First, it doesn't add to your number of parameters. So you don't risk an increasing over fitting. Second, it simply often yields more accurate models. However, since the convolutions that run below run at a lower stride, the model then becomes a lot more expensive to compute. And now you have even more hyper parameters to worry about. The pooling region size, and the pooling stride, and no, they don't have to be the same. A very typical architecture for a covenant is a few layers alternating convolutions and max pooling, followed by a few fully connected layers at the top. The first famous model to use this architecture was LENET-5 designed by Yann Lecun to the character recognition back in 1998. Modern convolutional networks such as ALEXNET, which famously won the competitive ImageNet object recognition challenge in 2012, used a very similar architecture with a few wrinkles. Another notable form of pooling is average pooling. Instead of taking the max, just take an average over the window of pixels around a specific location. It's a little bit like providing a blurred low resolution view of the feature map below. We're going to take advantage of that shortly. But first, I want to introduce you to another idea, it's the idea of one by one convolutions. You might wonder, why might one ever want to use one by one convolutions? They're not really looking at a patch of the image, just that one pixel. Look at the classic convolution setting. It's basically a small classifier for a patch of the image, but it's only a linear classifier. But if you add a one by one convolution in the middle, suddenly you have a mini neural network running over the patch instead of a linear classifier. Interspersing your convolutions with one by one convolutions is a very inexpensive way to make your models deeper and have more parameters without completely changing their structure. They're also very cheap, because if you go through the math, they're not really convolutions at all. They're really just matrix multiplies, and they have relatively few parameters. I mention all of this, average pooling and one by one convolutions because I want to talk about a general strategy that has been very successful at creating covnets that are both smaller and better than covnets that simply use a pyramid of convolutions. It's called an inception module. It's going to look a little more complicated. And you can skip this section if you'd like. It's not essential to the rest of the lecture. The idea is that at each layer of your cognate you can make a choice. Have a pooling operation, have a convolution. Then you need to decide is it a 1 by 1 convolution, or a 3 by 3, or a 5 by 5? All of these are actually beneficial to the modeling power of your network. So why choose? Let's use them all. Here's what an inception module looks like. Instead of having a single convolution, you have a composition of average pooling followed by a 1 by 1, then a 1 by 1 convolution, then a 1 by 1 followed by a 3 by 3. Then a 1 by 1 followed by a 5 by 5. And at the top, you simply concatenate the output of each of them. It looks complicated, but what's interesting is that you can choose these parameters in such a way that the total number of parameters in your model is very small. Yet the model performs better than if you had a simple convolution. This is one of the fun things about neural networks because you have this general framework and lots of small building blocks that you can assemble in that framework. You can explore lots of ideas quickly and come up with interesting model architectures for your problem. Next, we're going to look at how to handle text as an input to a deep model. The next assignment takes the deep networks that you've trained so far and turns them into convolutional nets. There are lots of architectures that you could try on this problem, including inception. Note though that this is where GPUs begin to shine. Covenants take a lot of compute. Well, the code in the assignment should run final CPU, and that's all you really need to complete the assignment. Doing a lot of exploration might not be practical, unless you have a big GPU, and modify the code to use it. Let's talk about how to handle text in a deep model. Imagine that you want to classify a document. Is it talking about politics, or business, or science maybe? You're going to have to look at the words in that document to figure that out. Words are really difficult. There are lots of them and most of them you never, ever see. In fact the ones that you rarely see, tend to be the most important ones. If you know that your document contains duh, you don't know much at all. But if you know that it contains retinopathy, you know that it's probably a medical document. Retinopathy appears with a frequency of 0.0001% in English. It's even possible that you've never seen that word yourself before. For deep learning, rare events like that are a problem. We like to have lots of training examples to learn from. Another problem is that we often use different words to mean almost the same thing. You can say cat or you can say kitty. They're not the same, but they mean similar things. Remember that when we have things that are similar, we really, really would like to share parameters between them. But kitty is completely different from cat. So if we want to share anything between them. We're going to have to learn that they are related. So that's a very big problem for deep learning. We'd like to see those important words often enough to be able to learn the meanings automatically. And we'd also like to learn how words relate to each other so that we can share parameters between them. But that would mean collecting a lot of label data. In fact, it would require way too much label data for any task that really matters. So to solve that problem we're going to turn to unsupervised learning. Unsupervised learning means training without any labels. It's easy to find texts, lots of texts, take the web for example, or even just Wikipedia. That's plenty of text to train on, if you can figure out what to learn from it. Here's a very very simple but very powerful idea that we're going to use. Similar words tend to occur in similar contexts. If I say the cat purrs or this cat hunts mice, it's perfectly reasonable to also say the kitty purrs or this kitty hunts mice. The context gives you a strong idea that those words are similar. You have to be catlike to purr and hunt mice. So, let's learn to predict a word's context. The hope is that a model that's good at predicting a word's context will have to treat cat and kitty similarly, and will tend to bring them closer together. The beauty of this approach is that you don't have to worry about what the words actually mean, giving further meaning directly by the company they keep. There are many way to use this idea that similar words occur in similar contexts. In our case, we're going to use it to map words to small vectors called embeddings which are going to be close to each other when words have similar meanings, and far apart when they don't. Embedding solves of the sparsity problem. Once you have embedded your word into this small vector, now you have a word representation where all the catlike things like cats, kitties, kittens, pets, lions, are all represented by vectors that are very similar. Your model no longer has to learn new things for every way there is to talk about a cat. It can generalize from this particular pattern of catlike things. Let's look at one way to learn those embeddings. It's called Word2Vec. Word2Vec is a surprisingly simple model that works very well. Imagine you had a corpus of text with a sentence say, the quick brown fox jumps over the lazy dog. For each word in this sentence, we're going to map it to an embedding. Initially a random one. And then we're going to use that embedding to try and predict the context of the word. In this model, the context is simply the words that are nearby. Pick a random word in a window around the original word, and that's your target. Then train your model exactly as if it were a supervised problem. The model you're going to use to predict this nearby word is a simple logistic regression. Nothing deep about it, just a simple linear model. Now, it would be great to see for yourself that these embeddings are clustering together as you'd expect. One way to see it, is by doing a nearest neighbor lookup of the words that are closest to any given word. Another way is to try to reduce the dimensionality of the embedding space down to two dimensions, and to plug the two dimensional representation. If you do that the native way, for example using PCA, you basically get a mush. You lose too much information in the process. What you need is a way of projecting that preserves the neighborhood structure of your data. Things that are close in the embedding space should remain close to the ends, things that are far should be far away from each other. One very effective technique that does exactly that is called t-SNE. You'll get to play with this visualization technique in the assignment Two more technical details about Word 2 Vec and about methods to learn embeddings in general. First, because of the way embeddings are trained, it's often better to measure the closeness using a cosine distance instead of L2, for example. That's because the length of the embedding vector is not relevant to the classification. In fact, it's often better to normalize all embedding vectors to simply have unit norm. Second, we have the issue of trying to predict words, and there are lots of them. So in Word 2 Vec we have this set up, we have a word that we're going to embed into a small vector, then feed that into a simple linear model with weights and biases and that outputs a softmax probability. This is then compared to the target which is another word in the context of the input word. The problem of course is that there might be many, many words in our vocabulary. And computing the softmax function of all those words can be very inefficient. But you can use a trick. Instead of treating the softmax as if the label had probability of 1, and every other word had probability of 0, you can sample the words that are not the targets, pick only a handful of them and act as if the other words were not there. This idea of sampling the negative targets for each example is often called sampled softmax. And it makes things faster at no cost in performance. That's all there is to it. You take your large corpus of texts, look at each word, embed it, and predict its neighbors. What you get out of that is a mapping from words to vectors where similar words are going to be close to each other. You actually get a little more than vector that just tell you when things are alike. And it's really fun. You have vectors. Imagine that you could do math with words now. Let's try an experiment. What do you think the results of puppy- dog + cat should be intuitively? How about taller- tall + short? Let's see, you take a puppy, you remove what makes it a dog, and add what makes it a cat, and you get a kitten. Or you take taller, remove what pertains to being tall and add what pertains to being short, and you get shorter. Saying that a kitten is to cat what a puppy is to a dog is a semantic analogy. Just like saying that shorter is to short what taller is to tall is a syntactic analogy. Here's a fun emergent properties of those embedding factors. They let you express analogies like this in terms of mathematical operations. If you have a good model, and you take the vector for puppy, subtract the vector for dog and then add back the vector for cat, then you get another vector. If you look for what that vector is closest to in imbedding space, chances are that it will be kitten. You obviously need a very good model train on a lot of data to get this kind of result. And it's likely that the model that you're training in the assignments is just not trained with enough data to show this. But you can try. There are many other ways to learn embeddings that I won't cover. Now, that we have models for individual words, how do we deal with the fact that text is actually a sequence of words? So far, your models have only looked at inputs that were a fixed size. Fixed size means you can always turn things into a vector, and feed it to your neural network. When you have sequences of varying length, like speech or text, you can no longer do that. Now what? You've already seen how convolutional network uses shared parameters across space to extract patterns over an image. Now you're going to do the same thing but over time instead of space. This is the idea behind recurrent neural networks. Imagine that you have a sequence of events, at each point in time you want to make a decision about what's happened so far in this sequence. Imagine that you have a sequence of events, at each point in time you want to make a decision about what's happened so far in the sequence. If your sequence is reasonably stationary, you can use the same classifier at each point in time. That simplifies things a lot already. But since this is a sequence, you also want to take into account the past. Everything that happened before that point. One natural thing to do here is to use the state of the previous classifier as a summary of what happened before, recursively. Now, you would need a very deep neural network to remember far in the past. Imagine that this sequence could have hundreds, thousands of steps. It would basically mean to have a deep network with hundreds or thousands of layers. But instead, we're going to use tying again and have a single model responsible for summarizing the past and providing that information to your classifier. What you end up with is a network with a relatively simple repeating pattern with part of your classifier connecting to the input at each time step and another part called the recurrent connection connecting you to the past at each step. To compute the parameter updates of a recurrent network, we need to backpropagate the derivative through time, all the way to the beginning of the sequence. Or in practice, more often, for as many steps as we can afford. All these derivatives are going to be applied to the same parameters. That's a lot of correlated updates all at once, for the same weights. This is bad for stochastic gradient descent. Stochastic gradient descent, prefers to have uncorrelated updates, to its parameters, for the stability of the training. This makes the math very unstable. Either the gradients grow exponentially and you end up with infinities. Or they go down to zero very quickly. And you end up not training anything. These problems are often called the exploding and vanishing gradient problems. And we're going to fix them, surprisingly in very different ways. One using a very simple hack, and the other one with a very elegant but slightly complicated change to the model. The simple hack is called gradient clipping. In order to prevent the gradients from growing inbounded, you can compute their norm and shrink their step, when the norm grows too big. It's hacky, but it's cheap and effective. The more difficult thing to fix is gradient vanishing. Vanishing gradients make your model only remember recent events and forget the more distant past, which means recurrent neural nets tend to not work well past a few time steps. This is where LSTMs come in. LSTM stands for long short-term memory. Now, conceptually, a recurrent neural network consists of a repetition of simple little units like this, which take as an input the past, a new inputs, and produces a new prediction and connects to the future. Now, what's in the middle of that is typically a simple set of layers. Some weights and some linearities.A typical neural network. With LSTMs, we're going to replace this little module with a strange little machine. It will look complicated, but it's still functionally a neural net. And you can drop it into your RNN and not worry about it. Everything else about the architecture remains the same, and it greatly reduces the vanishing gradient problem. So, what does this little machine look like? Remember, we're doing this to help R and Ns memorize things better. So let's forget about neural networks for just a moment. Imagine that you want a system to have memory. You need to do three things. First, you need to write the data into the memory. Then you will be able to read it back, and then you also want to be able to erase it, or forget. Here is one way to write this very same idea down as a diagram. You have a memory, which is maybe a matrix of values. You also have inputs coming into the memory, and a little instruction gate that tells whether you are going to write to that memory or not. You also have another gate that's says whether you are going to able to read from that memory or not. And then you have one final gate that says forget the data or write it back. You can also write at those gates as multiplying the inputs either by 0, when the gate is closed, or by 1, when the gate is open. That's all fine, but what does it have to do with neural networks? Imagine that instead of having binary decisions at each gate, you had continuous decisions. For example, instead of yes-no gate at the input, you take the input and multiply it by a value that's between 0 and 1. If it's exactly 0, no input comes in. If it's exactly 1, you write it entirely to memory. Anything in between can be added partially to the memory. Now that becomes very interesting for us, because if that multiplicative factor is a continuous function that's also differentiable, that means we can take derivatives of it. And that also means we can back propagate through it. That's exactly what an LSTM is. We take this simple model of memory, we replace everything with continuous functions. And make that the new Lowell machine that's at the heart of a recurrent neural network. The gating values for each gate get controlled by a tiny logistic regression on the input parameters. Each of them has its own set of shared parameters. And there's an additional hyperbolic tension sprinkled in here to keep the outputs between -1 and 1. It looks complicated but once you write down the math It's literally five lines of code, as you'll see in the assignment. And it's well-behaved, continues, and differentiable all the way, which means we can optimize those parameters very easily. So why do LSTMs work? Without going into too many details, all these little gates help the model keep its memory longer when it needs to, and ignore things when it should. As a result, the optimization is much easier, and the gradient vanishing, vanishes. There is one more thing to discuss about LSTMs. It's regularization. You can always use L2, that works. Dropout works well on your LSTMs as well, as long as you use it on the inputs or on the outputs, not on the recurrent connections. Now you know. What can you do with an? Lots of things. Imagine that you have a model that predicts the next step of your sequence, for example. You can use that to generate sequences. For example, text, either one word at a time or one character at a time. For that, you take your sequence at time, t, you generate the predictions from your RNN, and then you sample from the predicted distribution. And then you pick one element based on its probability. And feed that sample to the next step and go on. Do this repeatedly, predict, sample, predict, sample, and you can generate a pretty good sequence of whatever your RNN models. There is a more sophisticated way to do this, that gives better results. Because just sampling the next prediction every time, is very greedy. Instead of just sampling once at each step, you could imagine sampling multiple times. Here we pick O but also A for example. When we have multiple sequences, often call that hypotheses, that you could continue predicting from at every step. You can choose the best sequence out of those, by computing the total probability of all the characters that you generated so far. By looking at the total probability of the multiple time steps at a time. You prevent the sampling from accidentally making one by choice, and being stuck with that one bad decision forever. For example, we could have just picked A by chance here, and never explored the hypothesis that O could lead to a very sensible next word. Of course, if you do this the number of sequences that you need to consider grows exponentially. There is a smarter way to do that, which is to do what's called a Beam Search. You only keep, say, the most likely few candidate sequences at every time step, and then simply prune the rest. In practice, this works very well for generating very good sequences from your model. Let's look at some other fun use cases for sequence models. Remember, we've seen that one of the most entertaining aspects of deep models is that you can play Legos with them. You can mix them and attach them together, and then you can use back prop to optimize the whole thing directly end to end. If you think about RNNs in general, they are a very general, trainable model that maps variable-length sequences to fixed-length vectors. In fact, thanks to the sequence generation and Bing search that we've just discussed, they can also be made to map fixed-length vectors to sequences. Start from a vector, map it to the state of your RNN, then produce a prediction. Then sample from it, or use Bing search, and feed them back into the RNN to get the next one. Now you have those two building blocks, and you can stitch them together. It gives you a new model that maps sequences of arbitrary lengths to other sequences of arbitrary length. And it's fully trainable. What can you do with? Many things. Imagine that your input is a sequence of English words and you output a sequence of French words. You've just built a machine translation system. All you need is some parallel text. Imagine that your input is sounds and your output words. You've just built an end-to-end speech recognition system. Real systems based on variations on this design exist and are very competitive. In practice, they do require a lot of data and a lot of compute to work very well. We've also talked about conv nets, which basically map images into vectors that represent the content of that image. So picture what would happen if you took a conv net and connected it to an RNN. You have an image going in, and a sequence of things coming out. A sequence of words maybe. Maybe the caption for that image. To do that, you need training images and captions. There are a few data sets out there that you can use for that. Most notably the coco data set. It does images and crowdsourced captions for them. You can train a model that uses a cov net to analyze the image and generate captions from them. And it works. You can get great captions, generated completely automatically, and it sometimes fails in very, very funny ways. What's really cool about those examples, is that you don't have to know much about the problem that you're trying to solve. All you need is data. Lot's of data. And computers. Lot's of them. And off you go. You can build a machine or any system that often does much better than specialized, handcrafted approaches to the problem. So, here we are. With those tools in your back pockets, you can now train models that can predict from images, texts, sequences, or any kind of data, for that matter. If you're curious, there's a ton more to learn in the field of deep learning. Things also change every day as new techniques get developed and people find new ways to use those models. Even with just what you've learned in this class you have the tools to design and train real models on real data, so what are you going to try first?