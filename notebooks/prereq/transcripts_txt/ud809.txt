The purpose of this lesson is not to teach you how to do analytics. We've got other courses for that. Here we'll look at the end results that are being achieved by aggregating and then analyzing data, at least in part from digital health records. We're clearly heading into a new era of medicine and healthcare. And this lesson's intended to give you at least a feel for what it might be like. The world is awash in data. It's growing in exponential rates. People have even coined the term big data to refer to this phenomenon, but can't quite agree on what that means. What separates big data from what I guess you could call everything else little data. Caesar Hidalgo at MIT's media lab says that to qualify for this distinction, data must be big in size, resolution, and scope. To reframe this idea in a way that is directly relevant to transforming health care delivery systems, data must represent many patients and providers, must do so in great detail, and must be combined with other data to give a context within which that care is delivered, and the external rules and policies within which that delivery system must operate. The data must, in summary, be sufficient to represent the behavior of the complex adaptive health care system we've been discussing. Where in the health IT landscape might you best go to find big data as we just defined it for health care? Would that be in an enterprise EHR for a major health system, all the installations of a specific EMR in a state or region, a service area health information exchange, or none of these? Well, actually, the answer is none of these. Because as we define big data, you would need information from other sources, beyond the clinical data in these systems Historically the gold standard for developing new medical knowledge was the controlled clinical trial. Give two randomly selected groups of patients alternate therapies or perhaps a new therapy and a placebo and see who does better over time. These are difficult to do. Finding the right patients can be hard, these trials take a long time, and as a result of these and other factors, they're expensive. However, if the question is which treatment works best in actual human patients, there's currently not a good alternative. Consider a different question, such as what is the optimal treatment strategy among already known and available options, or if a particular chronic care model was changed in a specific way, what would the outcomes be on clinical quality and cost. As opposed to the classic research question of which drug works best, determining optimal treatment would require many alternative experiments. That is, trying every possible Treatment Strategy on groups of similar patients. To answer the second question, an experiment could be conducted by changing the clinical process in half of the clinic and comparing it to the other half that is operated as before. However, this would be costly and complex to do. These are the types of questions best answered through modeling and simulation using digital health data. There are a number of techniques for doing this. Again, this is not an analytics course, so we'll describe them, but not go into the details of how they're done. Finding an answer to, which of these questions, would require a traditional controlled clinical trial? A manufacturer wants to know what to charge for a new drug. A manufacturer wants to know if it's worth developing a new drug. A manufacturer needs to prove their new drug works better than existing drugs. Or, none of these. Well, the answer is the third one. A manufacturer needs to prove their new drug works better than existing drugs. This is a classic problem for a clinical trial where you're comparing two therapies. Decision trees are directed graphs. There is no recursion. You can't loop back. You only go one way one time, and time is not represented. Here's a classic generic clinical decision strategy where different treatment choices yield different outcomes. Markov models are also trees, but introduce recursion where steps can be repeated and time is now represented. However there is typically no memory, so that each step is based entirely on the current state of things. Here's a diabetes example where the patient risks and outcomes evolve over time. Note that unlike decision trees there are many paths to the model. And it is possible to go back as shown here to a prior point depending on actions and probabilities. In a Discrete Event Simulation, entities such as patients and their attributes, such as their diagnoses are represented. There are also cues where they wait for something, typically because of finite resources. A classic healthcare example as shown here is the Emergency Department. Anyone who's ever sought treatment in one knows all about cues. Agent-based simulations are richer in attributes than discrete event simulations. And agents can interact with each other and their environment. We'll look at an example of one of these from Georgia Tech, a simulation of the Center for Health Discovery and Well-Being, a novel clinic whose objective was to identify and mediate risks of future chronic disease. You've been asked to help a hospital optimize the workflow in their suite of operating rooms. To do this might use decision trees, Markov models, discrete event simulation, agent based simulation. Justify your choice in the space provided. So the answers are Discrete Event Simulation and Agent-Based Simulation. You would justify that choice by explaining that these are techniques that are useful for modeling workflow and process. Which of course is the objective or the technique that would be used in trying to optimize the workflow in a suite of operating rooms. You are doing research on the progression of the risk factors and complications of both treated and untreated hypertension over time. To model this, you might best use Decision Trees, Markov Models, Discrete event simulation, Agent-based simulation. Explain your choice in the space provided. The answer here is Markov Models. Some of you may have been tempted with Decision Trees, but keep in mind in a Decision Tree, there's no recursion. Here, we're talking about the progression of patients, both treated and untreated patients with hypertension over time. A patient, for example, may be well-controlled, but a certain percentage of those patients may stop taking their medications so they're going to go back to a more complicated state. Decision Trees can't model that sort of behavior, but Markov Models can. Here is a visualization of the agent based simulation of the Center for Health Discovery and Well-being. A special clinic established to explore the potential for preventing the development of chronic disease. Here you can see physical areas, such as the education room Where trained coaches interact with patients. When the simulation is actually run, all the clinic personnel and the patients being served move along these pathways governed by the time it takes to do each element of the clinics work, the available resources and other factors. The simulation was constructed to help the clinic identify a set of processes and a revenue model that would make it both self-sustaining and of value to its parent organization. Today, I'm talking with Dr. Rahul Basole, Associate Professor in the School of Interactive Computing here at Georgia Tech and a colleague of mine. He and I actually worked together on the research we're going to be describing. Rahul, you're also the Director of the Tennenbaum Institute here at Georgia Tech. Can you tell us about that? Sure, the Tennenbaum Institute is a multidisciplinary research institute at Georgia Tech that draws on all colleges here at the university. And we're fundamentally interested in how large, complex enterprises change and we use a systems engineering approach to study that. And we were really brought together when you decided to use that approach to model the Center for Health Discovery and Well Being, a joint Emory/Georgia Tech project, that helps people who have risks of developing chronic disease later in life. Change their lifestyles and behavior in order to litigate those risks. We actually developed a business strategy using this model and we fit the needs of all the stakeholders. Doing this required modeling all these interests. Can you describe how that was done? Certainly. So this particular model involved a four level approach. Which consists of people, processes, organizations and the ecosystem. And there are interactions that occur within and across these levels. Each of these levels really represents a stakeholder as well. So at the people level, we would look at providers and patients. At the process level, we would look at how these healthcare processes are actually delivered, which are often delivered by one or more organizations to third level in the model. And ultimately, they are ecosystemic forces that impact all of these different levels. Now the students are familiar with the various approaches to modeling and simulation. So can you tell us which approach was used at each level? Definitely. So, we did use a multi-simulation paradigm to approach this particularly enterprise. So at the people level, we used an agent based modeling approach. Processes tend to be model as discreet event simulation. But we also found that later on, that agent based simulation was actually quite appropriate. The other two layers were done both with systems dynamics, but all is age and base modeling approaches. So a mixture of different approaches? Absolutely. In one model? That's correct. So I know, because I was involved, how difficult and time consuming it was to get the process layer defined. Have you found that to be the case in other healthcare projects and what do you think is different about healthcare that makes its processes so different, so difficult to pin down? Sure. It's quite remarkable how complex healthcare is. So we found that process modeling in itself in health care is quite challenging, as compared to other domains, where it's actually very well defined. Manufacturing is probably the best example, where you have well defined processes, and you know how wages flow through the system. In health care, you have enormous amount of stakeholders have come together, you have teams and collaborations, you have patients moving through the system, and all of that is sort of distributed both geographically and also temporally, and that makes process modeling quite challenging with health care. Collecting the discreet data about events is also a challenge. As opposed to manufacturing. Can you talk about that a minute? Certainly. So I think that with an increasing amount of digital data now in healthcare as well, it's still quite challenging to really clearly lay out how processes happen. So we can leverage lots of different data sources with that. Administrative data, clinical data, but even that, can be very noisy, can be very messy. Sometimes there's missing data as well. And then actually identifying sequences of events in health care data is actually very, very difficult. So, we're now exploring a more automated approach to that called process mining. Can you explain how that works? Sure. Process mining really has its roots in the European academic disciplines. What they have done is they have used data mining approaches and visualization to infer processes from digital data. And it involves lots of sophisticated analytical tools. And the idea is that, can we use digital data to sequence together how events map out? So we've done some preliminary work on that, in the treatment of pediatric asthma in the emergency department. Can you tell us how that worked out? Sure. So we used digital data for roughly 6,000 pediatric asthma patients, and the idea was to obviously identify variations in care by patient strata, but also by providers and the idea was to identify variations in cost, quality and outcomes. We have developed a tool, a visual analytic tool, that could actually map different processes and allows you to filter by different criteria, across patient strata, providers, patient demographic data. And this visual analytic tool can be used by clinicians to explore how processes are delivered, where there might be actual differences in care, and potentially, how to improve them. So I'm sure the students at this point, near the end of the course, can appreciate just how challenging what Rahul just described is to do in practice, given all of the problems we've discussed throughout the course in terms of the interoperability of the systems from which the data needs to be collected. The differences in the way the data's represented, the data quality as we mentioned a few minutes ago. A really exciting area and you should stay tuned, see what we end up doing in a few years. Thank you. Thank you. Earlier we talked about the potential applications of analytics to the determination of optimal treatment. That often now means the best results at the lowest costs. These are of course not precisely defined terms and there is serious debate about issues such as the value of spending substantial sums on treatments that only prolong the life of terminal patients by small increments in time. The research shown here illustrates the potential to help with less controversial clinical contexts. A partially observable Markov decision process, a variation that introduces memory, was developed and compared to the actual treatment of nearly 6,000 depressed patients, abstracted from their electronic records. I leave it to you to read this section in the text for the details, but as shown here, under certain assumptions the model delivered improvements that were better than or almost as good as real physicians, but at a better cost per unit of improvement. As we seek to re-engineer the health care system, technologies like this may become a routine part of clinical decision support. Some would even speculate that, at least in certain circumstances, they may replace physicians. But I leave it to you to consider that. Congestive heart failure occurs when the heart is no longer an effective pump. It's the single most expensive ICD-9 code, so there's great interest in improving our care of it. Early on, the symptoms are subtle, so it may not be diagnosed for some time, even after they begin. Since earlier treatment can forestall expensive complications, earlier diagnosis is preferable. To help with this, researches first analyze electronic patient records, including the parts that were free text, such as the patient reported symptoms. They developed feature extraction algorithms, as illustrated here, that use the structured and text components to develop a set of from 10 to 100,000 clinical features. These were classified using both logistic regression, a statistical technique commonly used to predict whether a patient has a condition based on characteristics of the patient, and random forests. Which is a method that uses multiple methods to classify objects. This determined which features are in fact predictive. As shown here, the resulting model substantially outperforms the diagnosis of CHF based solely on the medical literature, when as few as 50 features with high predictive power based on the model are added. So today I'm talking Doctor Jimeng Sun from right here at Georgia Tech. Jimeng joined us earlier this year after many years at IBM, where his work has depended heavily on analyzing data from electronic health records. Jimeng thanks very much for taking the time to be in the course. I've heard you talk many times about the difficulties of working with EHR data. Could you explain some of those to the students? Sure. First, Mark great to be here. So electronic health records very important for my research and many other people who work on clinical informatics, house care informatics, also need to access electronic house records. But electronic house record is very messy, and lot of missing data in there, because they are collected for billing and clinical operation purpose. They're not designed for research. So there are a lot of information in the HR systems, but for a given patient they only have a small subset of those, and they're not uniform. So some patient have a lot more information than the others, and that makes the electronic health records very messy, very difficult to deal with. The other thing is, there are just a lot of different kinds of information that are in EHR systems, and it's getting more and more. For example, there are diagnosis information such as ICD code in there. And their medication information, there's lab test. And their clinical notes represented as free text. And their medical images. And more and more genomic data getting into the EHR system. So all of those different type of information requires different kind of techniques to process them, to analyze them. For example, for text data like clinical notes, you need natural language processing. And for image, you need specialized medical image analysis tools. For structured data like diagnosis medications, you would use general data mining techniques. So people have to be able to pick the right tools to deal with the right data. In a moment, we're going to talk about some work you've done around the early diagnosis of congestive heart failure. Which is interesting, particularly within the context of this course, because chronic disease has really been the exemplar I've used throughout. But first, in that work, you've had to use that free text data. And in fact, you've extracted from it, clinical features. That can be used by a machine learning algorithms. So how do you do that? How does that work? Right so first for people who are not so familiar with clinical notes, it's very rich information. A lot of subtlety and a lot of symptoms are represented in the notes. Only in the notes, not in the structured data. So, it's very important to process the clinical notes, to extract those symptom information, the severity information that are hidden in the text. So, that's the reason of why we need to process clinical notes. And the way we did that is to use a softer pipeline for dealing with test called UIMA, unstructure information management architecture that's originally developed at IBM research and later on becomes an open source project that many people are using that. Including later on the Watson project. So it's facilitate natural language processing development and you develop a pipeline of extractors to go through the text, extracting the information that you cares about like symptoms that related to heart failures so we used heart failure signs and symptoms, and also the context. You not only want to know the symptoms are present in the texts, but also whether it's in the positive context, meaning that the patient is confirmed to have the symptoms, or it's in the negative context, the patient doesn't have the symptom. So, you also need to know the context. So, we did many different extractors and to extract that information and also the context. So, it's a lot of work. So armed with that information and the structured information, you've developed a machine monitoring algorithm to diagnose congestive heart failure earlier. I should mention to the students without a clinical background that congestive heart failure is the single most expensive medical condition. And that diagnosing and treating it early, as we discussed in the course, is true of most chronic diseases, we can hope to avoid the complications that are very expensive to treat. But it's very subtle at first, there is no test like a blood glucose you can take, or a hemoglobin A1C to say you've got congestive heart failure. So clinicians often don't pick up on it as early as might be the case. Your model does, can you tell us how you did that? Right, so that's a project we started when I was at IBM Research a few years ago in collaboration with Geisinger health systems. So we would visit Geisinger and I mean, trying to learn about what are the most important or challenging problems that we can tackle together, using data-mining machine learning techniques. And they identify heart failures, one. And at times, there are already many groups are interested in heart failures. Mostly in the post-diagnosis phases in terms of readmission, hospitalization, readmission after heart failure's diagnosis. But that groups in Geisinger, the focuc, they wanted to look at pre-diagnosis, look at what are the signs and symptoms that lead to diagnosis of heart failures. Can we identify the patient of high risk of developing heart failures earlier? So the goal is for early detection. And we leverage The first thing we did is we want to see whether that's even possible. Are there relevant signal in the data that earlier than the diagnosis time? And we look at the clinical notes and that's where we first developed natural language processing techniques to extract that and the findings from those notes are very shocking. In fact, just defining from the clinical notes, we're able to publish in the journal in cardiac failures. And thus, are medical journals that clinicians write. There are some exciting findings in terms of signs and symptoms that in the electronic house records two years prior to the diagnosis, and that's very surprising. Many people don't believe those. I mean% of patients have that sign and symptom already in the clinical notes two years before the diagnosis. So those make us believe we can do something. Then the question is how do we really leverage that reach the HR information. So, it would be the integrated data analysis tools to integrate diagnosis, lab measure, and medication, and also clinical notes together to view the very reach patient profiles. Then score those patient profiles using advanced machine learning techniques to be able to predict a model. And that helped us to develop this very accurate predicting model that can predict heart failures diagnosis six to 12 months before the actual diagnosis. So that's really interesting. Everyone's heard of Watson, IBM's Watson, and everybody knows that Watson won Jeapordy! I think a lot of people realize IBM is now trying to move Watson into healthcare. The results you just described are really amazing. Do you think that we're heading into an era down the road, where tools like Watson are going to replace doctors? So first, before I answer that question, first I want to give a little background about how Watson compete in Jeopardy. Besides the advanced analytics techniques they have used, the amazing natural language processing pipeline they have developed. Another secret I think is very important for the success of Watson winning Jeopardy is to reach training data source that are available that related to Jeopardy challenge. In fact, there is a website called www.jarchive.com that record all the historical competition in the past in Jeopardy since 1983. And a lot of those questions are general knowledge that there are associate Wikipedia articles that provide very rich meta information about those questions. So those two important data sources helped the Waston Team to develop their algorithms, to train their algorithms on those historical questions. So that lead to a very amazing result. But in medicines, neither of those sources are available. There is no good question database or question/answer database that available. There's no good matter information related to all those clinical concepts. So it's a lot more complicated even to get to that stage. So what they have done before I left is, they look at the US medical licensing exam questions. And trying to at least tackle that first. Because those are the available questions and answers. But just the space of the medical domain is so large. And the question and answers are a lot of times there's no clear yes and no answers. So it's a lot harder. But clinical institutions are very interested in using Watson. So there's a very big medical research center at MD Anderson, I mean, Sloan Kettering Memorial Cancer Centers. They all partners with Watson, to use Watson for their diagnosis. So that's definitely a promising direction. I hope they succeed. It's fascinating to speculate about what something like Watson might be able to do, if you could train it on that vast amount of data. So as you look ahead for the next few years, five years or so, where do you think we're going to be in terms of big data and analytics in health care? Yeah, so it's definitely a very hot area. Big data analytics in health care. And so there's a paradigm shift from the traditional, more small scale statistic approach, to a more system approach. When I say system, I mean computing system. So they're not single algorithm that going to work. It's a pipeline of sufficity computational tasks that put together and run in a various scalable platform like high performance computing centers or in a cloud environment like Amazon web services. So, in particular, I think there is a great need for making the data more useable. Using analytics techniques. So, there's a new push called phenotyping. That really means, how do you convert those messy electronic health records into meaningful clinical concepts using algorithms? Rather than require the data to be good to begin with, why don't we develop algorithm that can automatically clean the data and turn them into more usable high quality data that can support research and discovery? So that's the phenotyping effort that a lot of people are working on. That's about the input to the analytics platform. Then, there's the middle part, where people are trying to view those analytics, predictive analytics. That used to take months, like for every disease target, every data set, you have to spend months just to get the right data, view the model, then validate that. But now, everybody want to use predictive analytics and there's thousands of disease and conditions people want to view the model for. So how can you scale up to view this model quickly? You need a very scalable, large scale platform to support that. So, alot of people leveraging parallel computing and like map redo's to do that kind of work. That's the processing part. And also, finally, the output, right? So we so far only talk about accuracy and how good the models are. But in order to make use of those model in clinical decision support, you really have to make the model intuitive. So that the decision maker, like clinician, doctor can understand. Can explain the result to the patients, so that they believe and follow through the recommendation. And their work on leveraging similar patients' history and recommend what worked for the similar patient to the current patient as intuitive method to explain the recommendations. So that's the final output part, right? So all the entire system, input processing and output need to be new computational method. And I think that's very a promising future for health care analytics. Well thank you very much. I think you really helped the students understand, not only where we are, but to see that there are many, many years of interesting research and development ahead of us. So maybe some students will decide to join that, which would be a great result. So thank you very much. Thank you. We now understand that cancer is a far more complex family of diseases than previously thought. And that the results of treatment in any individual patient depends on their particular genomic, proteomic, and metabolic pathways, other personal factors, and on similar factors in the cancer. Given the high mutation rate in cancers, there's usually one more than one cell type, essentially more than one cancer in each patient. Traditional chemotherapy is a metabolic poison. It selectively targets rapidly growing cells and works because cancer cells divide more rapidly than normal cells. However, our bone marrow, hair cells, and the lining of the GI tract also grow rapidly and are attacked by the treatments, leading to side effects that can be awful. More recently, we've developed mechanistic treatments that have far fewer side effects because they selectively attack metabolic pathways in the cancer if it has the specific pathway that the treatment targets. These are very expensive treatments, and precious time is lost if they don't work. Without an analytic tool, it's hard to know which patients will benefit from a therapy. This research was aimed at developing a predictive model to assist physicians in selecting the right mechanistic drug for each patient. It's based on a comprehensive model of known cancer biochemical pathways, a small part of which is shown here. The blue boxes are the components. And the red circles are their reactions with each other. The researchers report then when genomic data from the patient and from the primary cell types in their cancer, including cancer stem cells, if possible, are fed, the model has successfully predicted the efficacy of treatment. It's now being commercialized by Alacris, a startup company in Germany where the research was done. Why might the metastasis in the patient not respond to the drug that worked before? Select the most likely reason. Is it because the patient is mutated? The cancer started growing faster than drug could kill it. The cancer had mutated. Or none of these. The answer of course is that the cancer had mutated, something that cancers unfortunately do frequently. Up through now, we've been discussing how medical diagnosis and treatment can be improved through analytics. We turn now to an area that's only recently begun to get the attention it deserves, how similar techniques can improve the care delivery system. This research considered various configurations of the beds in a surgical suite and their impact on the overall efficiency of the suite. Traditionally, there are three types of beds, the pre operative holding area, where patients are held before the surgery. The post anesthesia care unit where patients go immediately after surgery, and the level 2 area, where they recover further from the surgery. Georgia Tech researchers developed a model using a tool called MED-BPM, a healthcare specific modeling tool, and showed that an alternative approach using universal beds that could be reconfigured for all three purposes could serve the same number of cases, but with substantially fewer beds. With their records now digital, can we infer the underlying clinical processes that patients receive as they traverse the complex service areas of a hospital? More importantly, can we identify differences in these processes and the impact these have on outcomes and costs? This analysis is common in other industries and is referred to as process mining. However, these industries typically have mechanized processes, where automated sensors routinely and accurately provide digital timestamped data. This is not yet the case in health care, which is still a largely manual industry and one where care steps may be documented digitally but either incompletely, inaccurately, or after the fact. Nevertheless there are situations where process mining appears to be feasible. This is research from the Netherlands, where process mining originated in the late 1990s. It examined the care of ischemic stroke patients, patients with a clot in the arteries of their brain, in two hospitals shown here. The results clearly indicate significant differences, with one hospital on the left here emphasizing the use of treatments aimed at protecting brain cells to limit the damage from the ischemia. This is the more state of the art approach. This is early work, but here at Georgia Tech we feel that process mining is a potentially important technology for re-engineering a health care delivery system. To make it safer, more cost effective, and ultimately, a more valuable and responsible component of our society. Which of these are limitations of the Dutch ischemic stroke study? Check all that apply. The first two, if you look at the diagram in the ischemic stroke study, you'll see that there is a very small number of patients and a really small number of hospitals, only two. The fact that it looked only at ischemic stroke is not an issue because the study was about ischemic stroke. As we said at the outside of this course, one of the promises of the adoption of digital records by providers and patients would be the availability of huge quantities of new data from which analysis could yield insights into how to diagnose and treat patients better. And even how to improve our system of care delivery. You've now seen early examples of each of these. There are still problems with the data itself. It is often noisy, inconsistent, or even wrong. Some patients may have a rich data set for a particular clinical concept, while others may have no data at all. The interoperability problems we've stressed throughout the course are still an impediment. Finally, as we discussed with Don Detmer, we may have gone too far with patient privacy and made it too hard to obtain the data that could make a valuable contribution to health and healthcare delivery. The solutions are ahead of us. And it will continue to assure that health informatics will be an exciting and dynamic field for years to come. We've now reached the end of this course. We began with the nature and problems of our healthcare system and the key role that chronic disease plays. We made the case for digital records and data sharing. And we learned what the Federal Government is doing to encourage their adoption. We examined the key underlying technologies and looked at how they're being used in actual systems and tools for providers, patients, population and public health, and clinical research. We explored in more detail how data can be aggregated for analysis. And we concluded with a number of exciting examples of the results that can be achieved. We're only at the beginning of a long journey. It's my hope that this course will encourage you to come along and be part of a generation that actually achieves the long held dream of transforming healthcare through health informatics. Please don't hesitate to ask me if I can help. Good-bye, I wish you well. In this lesson we'll explore the unique characteristics of the U.S. healthcare system, with a particular emphasis on the mismatch between what it's good at and its structural issues which contribute to poor management of chronic disease, the problems that account for most healthcare costs. Now it's time to meet Marla, a chronic disease patient who will help us explore these issues. This is Marla, a 59 year-old woman who has become obese as she aged. She has also smoked cigarettes for many years. As a result, she's developed several chronic diseases. Through Marla and her story we'll learn about chronic disease and the problem it creates for the US healthcare system. We'll also gain the background to appreciate the key role health informatics can play in helping solve those problems. First, Marla is not unusual. The Federal government's Medicare program provides healthcare for some 40 million people over the age of 65, and pays for nearly half of all US hospital care. It is projected to be a major contributor to future increases in healthcare spending. 83% of Medicare beneficiaries have at least one chronic disease, but the 23% that have five or more, account for two thirds of Medicare's costs. Here is a list of relatively common chronic diseases, so now you know what some of them are. I could have added cancer, since many people were surviving it and being treated for years, but we'll use this list for now. If you, anyone in your family, or anyone you know closely, have had hypertension, diabetes, chronic obstructive pulmonary disease, heart disease or chronic kidney disease, check that now Before we get back to Marla, there's one thing you should know about chronic disease. Medical problems like the flu and most other infections or fractures are called acute medical problems. Because they either go away or can be cured in a relatively short time frame. Chronic problems are, by definition, not curable. The best we can do is help the patient manage them so they don't get worse and the often terrible complications of these diseases, including death either don't occur or are delayed. Before answering this question, I want to urge you to carefully read all of the answers. It's entirely possible that more than one will technically be correct. We're looking for the best answer. So by now it should be clear that chronic diseases are very important. So let's make sure you understand their key characteristics. Which of these is not a chronic disease? Check any and all that apply. As you can see, there are two answers, an upper respiratory tract infection. Remember, we said infections were an example of what is usually an acute medical problem. And an arm fracture, which is the other example we gave of what is usually an acute medical problem. Which of these are not true of chronic disease? Once again, check any and all that apply. Here, again, there's more than one answer. Chronic diseases don't recur after going away for long periods of time. They're chronic. Once you have them, you have them. And, children are getting chronic diseases. In fact, because more and more children are of these in the United States, more and more children are developing chronic diseases at an earlier and earlier age. By now you should have a clear picture of the key differences between chronic and acute diseases. Unlike acute medical problems, chronic diseases are more often caused by behavior, generally can't be cured. And particularly if not well managed can cause other chronic diseases and complications that are difficult and expensive to treat. Now with Marla's help, we'll turn again to what for our purposes is probably the key issue with respect to chronic disease. Though they're very common, our healthcare system is not well designed to manage them. Now back to Marla, who already has four chronic diseases. She developed diabetes 15 years ago, and is now showing early signs of lung problems, called chronic obstructive pulmonary disease, or COPD for short. In recent years, her blood pressure has become elevated, and she's also taking medications for that. She was also recently diagnosed with a heart problem, called coronary artery disease, and needs even more medications. We already know one thing about chronic diseases, they can't be cured. Marla illustrates two other key points. They are often caused by behavior. Marla's obesity is caused by her poor diet, rich and processed foods with too many calories from fats and carbohydrates, combined with far too little physical activity. This weight gain contributed to her developing diabetes and high blood pressure. Her lung disease was caused by her smoking. The second key point about chronic diseases, is they can cause other chronic diseases. Marla's diabetes and high blood pressure contributed to the development of her heart disease, and puts her at greater risk for a heart attack, or stroke. If her diabetes and high blood pressure are not well controlled, she could develop chronic kidney disease, and other more serious problems. Marla and the treatment of her chronic diseases can also help us understand a key problem in our healthcare delivery system. It's fragmentation, and lack of coordination, and how informatics can help fix that. One way is by enabling a different approach to care. Marla may have many problems, but she's still one person, so her problems need to be managed in a coordinated way, and within the context of her capabilities, lifestyle, resources, and social situation. The term for this is patient-centered care. This may sound like a strange term, but the traditional health care system is often been designed and optimized around the needs of providers, not patients. As a result, care can be inconvenient. Patients have to wait for appointments and may have to wait even once they arrive in the clinic. All of this discourages coming, particularly for routine preventative care. Patient-centered care is designed around the needs of patients who may even be able to be seen the same day they request an appointment. This care is delivered by a team so that routine problems that don't require a physician can be handled by a nurse or other provider, again, on a much more convenient and timely basis. A key tenet of patient-centered care is that all patients should have a Primary Care Physician, or PCP. This is usually a family physician, general internist or OB-GYN for adults and a pediatrician for children. Their job includes having a complete view of a patient and coordinating the care delivered by the team or even other physicians. In a well-coordinated healthcare system, every patient has a PCP. Since they may not have the skills or knowledge to manage all of the problems a patient like Marla has, part of the PCP's job is to refer patients to specialists when needed but still to coordinate and oversee all of their care. In this last role, the PCP is often referred to as The Gatekeeper, the person who makes decisions about when their patients need expensive specialized care. Here in the US, the PCP as gatekeeper is almost always used by Health Maintenance Organizations or HMOs. There is data to show that this provides better care quality at a lower cost. Because of their key role in coordinating care, HMO physicians almost always have electronic patient records and have for some time now. However, HMO's also almost always provide a more limited choice of physicians as a means of cost control and to facilitate managing care quality. Marla, like many Americans, gets Health Insurance from her employer. At work, she didn't select an HMO mostly because she wanted a wider physician selection. And she didn't like the idea of someone deciding for her if she needed specialty care. She preferred to do that herself. As a result, she doesn't have a PCP, and gets all of her care from many specialists. Each of whom focuses on the problem she has that is in their domain. In general, too much patient care in the U.S. is provided this way. In part, as we just discussed, this is because of patient choice. But, if all patients wanted a PCP, we don't have enough of them to go around. Only 12.3% of US patients practice primary care versus around 50% in France. The same comparison holds true with other advanced industrialized nations. You can see here, the U.S. lags substantially in the percentage of primary care physicians. The full reasons are beyond the scope of this course. But one of them is that specialist, shown in green here, makes significantly more money than PCPs. Yet, as also shown here in blue, they leave medical school with the same debt level. So it shouldn't be a surprise, as shown here, that over the years the percentage of medical school graduates selecting primary care, the bottom blue line here, has decreased, as the salary of a PCP, as a percentage of what a specialist makes, has also declined. By now it should be clear that PCPs are important, so let's make sure you understand their key characteristic. The jobs of a PCP includes which of the following? Check all that apply. If fact, they're all correct. PCPs do directly care for their patients like any other physician. They refer their patients to specialists as needed. We said that. They coordinate all of the care for their patients, no matter who provides it. We also said that. And they promote patient-centered care. Which of following are true about PCPs in the United States? Again, check any and all that apply. In this case, none of the answers are true. We certainly didn't say that PCPs usually practice in a hospital. We know that they're in under, not oversupply. We know that they make less than more specialized physicians, and as a result of that we also know that primary care is decreasing in popularity among medical school graduates. So which of following is not a PCP? Once again, check any and all that apply. Here only the first answer is correct, a heart surgeon. We specifically listed family physicians, obstetricians and pediatricians as primary care physicians. So, now you understand a bit about the alternate models of care delivery. But you may be asking, why does this matter? For the answers to that, we turn again to Marla to explore some of the serious problems facing US healthcare. This will set the stage for understanding the key role that health IT may play in improving our healthcare system. So why is what we've covered so far, important? Because these are some of the reasons that the US spends far more per capita. Typically twice as much on healthcare, than other advanced industrialized nations. That's certainly true if you again compare the US to France. Moreover, while increases in US healthcare spending have slowed in recent years, for reasons that aren't yet clear, historically our healthcare cost have grown much faster than those in the same group of nations. The inexorable growth of chronic disease, is one of the key reasons why. That spending might be okay if we were getting a great result. But again, comparing the US here, to this group of industrialized countries and even some others, we rank 26th in life expectancy shown here. We're also a clear outlier, spending far more, but dying younger than the citizens of these other countries that spend far less. Clearly a serious problem, but you may be asking, what does it have to do with health informatics? We turn to that now. Here's a key number, 14. Studies show that patients like Marla, patients with four or five or more chronic conditions, are seen on average each year by 14 different physicians. So why is this the case? Well they have a lot of problems that involve many different parts of their body. In Marla's case, her lungs, her heart, her blood vessels and the insulin control system, which is centered in an organ called the pancreas. There is a specialist for each of these body systems. A cardiologist for the heart. A pulmonary specialist for the lungs. An endocrinologist for diabetes. And an internist to manage hypertension. Unless Marla develops a particularly severe issue, a PCP might manage all of these problems, with occasional help from a specialist. Since she doesn't have a PCP, she ends up just seeing specialists. Unless things change, this trend toward specialty care is projected to grow. By now you're probably wondering why this is a problem. Primary care physicians have been shown to deliver similar quality to specialists for certain conditions, such as diabetes and hypertension, often while using fewer resources. They often provide better preventative care than specialists, reflecting their ability to better manage the whole health of patients and can help reduce hospitalization rates. In fact, it's been shown that if you increase the supply of primary care physicians by just 1 for every 10,000 people, there's a significant increase in quality, and a significant decrease in the cost of Medicare, which you may remember, takes care of older people who are more likely to have chronic disease. Conversely, if there's a similar decrease in the availability of primary care physicians, there's an almost equal decrease in quality. And a nearly equal increase in the cost of Medicare. You may be asking why? Well, one of the major reasons is that specialists charge more. And they do far more tests and procedures. Moreover, how do they coordinate their care? How does each of these specialists, and remember Marla may not even see the same cardiologist every time she visits that office, know what all of the others are doing. In particular, how do each of these specialists know what medications all of the others have prescribed? This is important, because the same research that showed Marla will likely see 14 providers each year, also showed she would likely fill 50 prescriptions that same year. However, even a PCP alone may not solve a coordination problem without some electronic assistance. It's too complicated for that. Let's discuss the care coordination situation facing the typical PCP who sees a lot of chronic disease patients, like Marla. Remember there are 14 physicians involved in the care for each of those patients. Studies have shown that there are a total of 86 physicians involved in caring for all patients like Marla in this one PCP's practice. Overall, this typical patient-centered medical home interacts with 229 physicians each year. In fact, no matter what the care setting, without electronic assistance, physicians often don't coordinate well. The Commonwealth Fund surveys patients like Marla. In 2011 they found that, quote, to varying degrees in all countries chronically ill or sicker patients encounter failures of providers to communicate with each other or coordinate care. Yet in each country, patients with primary care practices that help them navigate the care system and provide easy access are far less likely to encounter duplication delays and failures to share information. Unsaid here is that almost all patients in patient-centered practices, shown in red here, are using electronic records. And you can see that they have lower, actually significantly lower, reported rates of care coordination problems. In fact, using electronic medical records is part of the definition of one of the most common forms of patient centered care, the patient centered medical home. Elsewhere, a key culprit in creating these coordination problems has been paper records. They can physically be in only one place and are not easily shared. Even if shared, they may not be easily understood due to legibility issues or differences among physicians in the terms they use to describe the same thing. Most of Marla's specialists use paper records. Until recently here in the US most physicians used them. This is rapidly changing because of the new federal policies we'll explain in lesson two. Electronic records and data sharing can help coordinate care. Here's data from a recent survey of physicians who are using these technologies, and they substantially agree that it helps. Improved communication, more available data. Interestingly and importantly, both PCPs and specialists with electronic records say care is more coordinated and communications are improved. Presumably because data is now immediately available when and where it is needed, as a majority of these physicians reported. And that's the primary goal of contemporary health informatics, to turn our fragmented, poorly coordinated health care system into a system that uses digital information in a seamless way to provide optimal care to all patients, even those as complicated as Marla. To understand that in more detail, in Lesson 3 we'll go with Marla as she visits her endocrinologist, her diabetes doctor, Doctor Johnson. The National Academy of Sciences was formed by President Abraham Lincoln to provide independent objective advice to the nation on matters related to science and technology. For many years its healthcare component, The Institute of Medicine, which recently became The Academy of Medicine, was instrumental in raising the alarm that U.S. healthcare has the problems we've been discussing. The institute has called for a new, learning health system. We don't have time to discuss that in detail, but here's a simple health infomatics perspective that shows how electronic records, if they're adopted and can share data, become a source of information from actual patient care. It can then be aggregated and analyzed to develop new knowledge, that in turn, through an informatics powered feedback loop, leads to continuous improvement. We're progressing toward the learning health system. As a result of the federal efforts we'll discuss in lesson two, electronic record adoption levels are very high. And the focus is now shifting to the next challenge, the inability of the hundreds of systems in use to share data. The term for this is interoperability. And we'll discuss it in more detail in lesson 5B. We'll place particular emphasis on a new approach that is rapidly gaining traction, and that appears to provide a long-sought practical solution to interoperability. It's called Fast Healthcare Interoperability Resources, or FHIR, F-H-I-R. Through FHIR, access is provided by web services such as rest API's. And health care data is packaged in the JSON or XML formats. Or to use widely on the internet and by other industries. One of the exciting results, if FHIR were widely adopted, is that it could constitute a universal health app platform. Developers could then use that platform to help solve some of the limitations of current EHR systems we'll discuss in lesson six. They can also provide innovative tools for patients, the topic of lesson seven. In fact, such a platform already exists. Harvard's Boston Children's Hospital received federal funding to develop a universal health app platform called SMART. The lasted version, SMART on FHIR, is based on the new standard and you can visit the app gallery on their website using the link provided in the instructor's notes to see examples of SMART on FHIR apps. These could be plugged directly into any EHR that supports the platform. I'll interview the chief architect of the platform, Dr. Josh Mandel at the end of lesson 5B. HIMSS is the largest health IT meeting in the world. At the last two HIMSS meetings, he demonstrated multiple EHRs running the same third-party apps using this approach. Cerner and Epic, the two largest enterprise healthcare software companies, have announced FHIR support. We believe it may not be long before your most promising ideas could plug into actual electronic record systems. You could then validate your team's approach with real practicing physicians, other clinical personnel, and their patients. Finally, it is also entirely possible that FHIR will be further developed to solve the analytic challenges, we'll be discussing in the lessons eight through ten Georgia Tech is the first university to offer a FHIR platform for students to develop and test their innovative ideas. It provides access to synthetic health data for some 10,000 make believe chronic disease patients like Marla. You access that data using some key FHIR resources, for which we've implemented the ability, with some limitations, to create, read, update, and delete, called CRUD. You'll also have access to the SMART on FHIR app platform. You'll form teams, each of which will turn its innovation into at least a working prototype using FHIR as the source of patient data. Each team will be assigned to a TA, who will help resolve any technical issues and answer any questions. If you want one, we'll try and connect you with a domain expert relevant to your project. It's important that each team have people with the right set of skills. So we'll be providing assistance to you to form effective teams. We'll discuss FHIR in lesson 5B, but I suggest you get started exploring it now by visiting the websites I've listed in the instructor's notes. With Marla's help, we've covered some of the key problems in US health care. And some of the principal arguments for the increased use of health IT. Most health care is concerned with the management of chronic diseases. Proper management requires a highly coordinated, more continuous approach to care that welds together many providers into a seamless delivery system. At the center of such a patient-centered system of care, is the primary care physician whose role is knowing everything about their patients so they can coordinate their care. In practice, this is very difficult to achieve because of the shortage of primary care physicians here in the U.S. and the large number of providers who are involved. Particularly in the care of patients like Marla, who have multiple chronic diseases, coordinating this requires electronic records ideally for the providers and patients and the seamless exchange of information among them. This same information can be used to manage the health of populations of patients, or the public at large. It can also be used for research to gain new medical knowledge, and even learn how we can improve health care delivery. Next, we're going to travel all the way to Baltimore to the Johns Hopkins Bloomberg School of Public Health to speak with Dr. Gerard Anderso. A professor there and the first author of the paper I site more than any other during this course, The Rising Tide of Chronic Disease in America. Gerard, thank you so much for being with us today. It's a thrill to meet you. I'm glad to be here. Great. A decade ago, you and your colleague Jane published what I view as a seminal paper, A Growing Version of Chronic Disease in America. The students of the course, if they've been paying attention, are quite familiar with it by now. And, it opened my mind to the aggregate problem of chronic disease in a way that really, no previous paper had done. Which is why I view it the way I do. So we're ten years later, have we made progress? Where are we? Well, we have made progress. I think what you knew already is that, you saw that patients of that chronic disease, but you didn't know how many patients there were. And now, we know that there are 150 million Americans, almost one out of every two Americans, that have one or more chronic conditions. And that is sort of the new information that we were trying to bring ten years ago. Now what we're trying to do, now that we have the attention of policy makers, is to start changing In the delivery system. That starts with prevention, but it goes to care and cure for these chronic conditions. And there, we're making a lot less progress. I was more optimistic ten years ago than I am now. Most everything that we've tried recently has just not worked very well. Well, here in the last few years, we've tried to make the healthcare industry adopt Health IT. And we've tried to create health information exchanges. We're still really early in that, but are you optimistic that that will happen and that it'll make a difference? Well, it is happening, so I'm very optimistic that it will happen. What I don't think we've solved very well so far is that dashboard problem of a doctor. I have ten minutes to take care of a patient, and she has seven things wrong with her. She's seeing nine other doctors, and I have ten minutes. If I spend most of that time looking at her medical record, I now have five minutes to talk to her. And so the challenge is, how can we get that information out to the physician in a timely fashion for that complex patient. For the patient that only has diabetes, it's pretty easy. But for that patient who has diabetes, and congestive heart failure, and beginning Alzheimer's, and arthritis Brightest. And seeing all these different doctors, and taking lots of different medications, that's the challenge. I hope the students remember that comment because that's basically what we're going to talk about in Lesson 6 of the course. Great! Well, you also mentioned incentives. Clearly, the healthcare system has never been incented. Do you see that working? Are you optimistic that that's actually going to happen in this endlessly complex adaptive healthcare system? So I was just amazed when I started this work in the 1990s. That Australia had tried care coordination programs, and they had all failed. And then, I support the United States can do it differently, or the UK could do it differently. And so, I've been looking around the world for those models and I just don't see them yet. We know what we need to do which is to get everybody on the same team, probably with a nurse practitioner, talking to all the doctors. But we haven't been able to figure it out. So almost all of the demonstrations that we've tried, both in the public sector and in the private sector, just haven't been able to save money, improve outcomes, and improve satisfaction. We just don't have very many of those things that hit all three of them very well. And we're still working. And so, I'm optimistic that we will find it. But so far, we just haven't been very successful. Your paper actually ends on an optimistic note. You point to the re-engineering of the healthcare system a century ago around new knowledge in public health. Is it so fundamentally different now that that optimism might be misplaced? No, but I think if you read it a little more carefully, what you'll see is I said we move in 50 years from a system oriented around infectious disease to one that's oriented around acute illnesses. And then, we got that done by about 2000. And I said it's going to take us 50 years to reorient the healthcare system around chronic disease. We have to start with the evidence base. We have to know how to take care of somebody who has seven things wrong with her when the NIH and the FDA exclude all those people from clinical trials. We don't have the educational system in the medical schools oriented around that. We don't have the financing. And so right now, there isn't a real strong incentive for hospitals and doctors to really go after these people to take care of them in an effective way. In fact, because they dont know how to do it, they want to run away from them in most cases. But this is where the money is. Two-thirds of Medicare spending is by people with five or more chronic conditions. We've gotta get the system right for these people. We just don't know how to do it yet. So I can't speak to 50 years ago. But 40 years ago, I was a medical student first getting into this. And I remember going into meetings. And they would say, well, here's where we are. And if these trends continue, and we don't do anything, by right about now, health care will be consuming 20% of the gross domestic product, a number that's absolutely not sustainable. Right. Now, I've seen projections from OMB and others that if we don't do anything, it'll be 40% in 50 years. Do we have 50 years? Well, when I started working and trying to control healthcare costs in the 1970's, I said that we'll never be able to sustain 10% on GDP on healthcare. Now, it's close to 20. So, I've stopped making projections as to whether or not we can afford to do something. What we know is that by the time it gets to almost 30%, which isn't that far along, almost all the growth in the economy. All the productivity growth in the economy will, in fact, go to support the growth in healthcare, and that's probably not sustainable. But, we don't know what's going to turn this ship around. Maybe it's IT. Maybe it's changing the financial incentives. But since everybody's making so much money in healthcare and jobs are important and financial security is important, it's going to be a really tough change. So, many of us speculate that eventually with our backs against the wall we'll adopt a single payer system like every other country on Earth. You think that's going to happen here in the US? I was more optimistic in the 90s than I am today. Basically, Americans love choice. All you have to do is go to the grocery store and go down the cereal aisle. And look at 40 to 100 different choices of cereal. Whereas if you go to France, you will see two or three in most cases. Choice is what makes America, and it's going to be really hard to get the private insurance industry out of it. We can't even get right now many of the states to embrace Medicaid. So, if they can't embrace Medicaid, which is free money. To them, why would we think that they would go to a single pair? I mean, I think it makes sense. It means that we pay a lot more for healthcare than other industrialized countries because of this choice, but America seems to really like choice. Well, thank you very much. It's been a pleasure. Thank you. And maybe we'll be able to get together in ten more years and see what's happened. Hopefully, it'll be more optimistic that time. There are historical reasons why our healthcare system is the way it is and why we've been so slow to adopt health IT. For more on that, and more details on the US healthcare system, refer to chapter one in the text. Now that we have the background to understand why fixing health care is a national priority, we'll look at what the US federal government is doing to foster the adoption of health IT and create new financial incentives to encourage more use of patient centered care. In lesson one we introduced the disconnect between what is required to successfully manage the chronic diseases that drive most health care costs, and the structure of the US healthcare system. We also suggested that health IT could be a key tool, for restructuring healthcare to help address these issues. We'll review that in this lesson before turning to the new US federal policies to encourage adoption of health information technology, and new models of patient centered care. >From an engineering perspective, chronic disease presents a data logistics problem. Here's a more dramatic network depiction of the complex, highly specialist-driven care of the chronic disease patients in the typical primary care physician's practice. As you already know, the average multi-chronic disease patient, 20% of Medicare patients that drive half of the cost is seen each year by 14 providers who are mostly specialists, as shown here. In aggregate, all of the multi-chronic disease patients in a typical PCP's practice are seen by 86 providers. Again, mostly specialists. Keep in mind that each of these specialists mostly focuses only on the particular organ or body system they have special knowledge of, and are trained to treat. In total, the average PCP is involved with over 200 other providers. This situation is reminiscent of the network of specialized suppliers to a manufacturing company. One makes seats, but not radiators, while another makes dashboards, but not tires. But somehow it all comes together seamlessly to produce a great car, those industries long ago recognized the need for automation to help coordinate their supplier network starting with the automobile industry in the 1980s. Until very recently, and this really only started to change as a result of the federal programs we'll discuss in this lesson. The healthcare industry tried to operate its complex care network using paper records and fax machines. Name an example of a specific, non-healthcare, non-automotive corporate success story in using information technology to manage a complex logistics network. Well, good answers might be United Parcel Service or Federal Express in the package delivery business, Walmart in retailing, Boeing, a non-automotive manufacturer, or virtually any other large manufacturing retailing distribution or transportation company. Again quickly reviewing because this is so key to what follows. US healthcare is highly uncoordinated because so many specialized physicians are involved in the care of patients with multiple chronic diseases. One reason is that we have such a small percentage, only around 12% of primary care physicians. Other countries, such as Australia, France and Canada, have around 50% of primary care physicians. The resulting problems are not theoretical. The more physicians they see, 4 or more versus 1 to 2 physicians, more chronic disease patients report being seen by a physician with an incomplete or missing record. There's similar data for medical errors. Recall that the typical multi chronic disease patient, like Marla here, fills around 50 prescriptions a year, because for most chronic diseases medications are the main treatment. Misuse of medication, including in particular duplicate medications, because of poor coordination among providers, is a major problem that accounts for nearly a third of all hospitalizations in the US. Surveys by the Commonwealth Fund in 2008 and 2011 show progress. Here we see the United States improving. But still, 28% of patients surveyed report that no provider has reviewed their medications with them in the past year. Such review could help ensure that patients are taking only the medications they need, and are taken them properly. One reason for the absence of review, is that physicians may not even know the medications that other physicians have prescribed. We'll return to it later on, but this process called medication reconciliation, is a clear opportunity for health IT, once the underlying records are digital. Here's a screenshot from an actual commercial HIT system. Showing the medications in this physicians EMR on the right, and those out in other EMR's on the left. The system even highlights this medication,which the patient is taking, but which isn't in this provider's EMR. As we'll see later on, electronic prescribing or e-prescribing for short is a key requirement of the new federal EHR adoption programs. Because electronic prescriptions are clear and legible and an EMR can often spot potential problems as prescriptions are written. E-Prescribing has been shown to dramatically reduce medication errors within a year of being adopted. As you can see here they decline from 42.5 per 100 descriptions to 6.6 within a year of this practice, or the typical practice adopting e-prescribing. While of course they remain the same in those practices that continued to manually prescribe. Someday soon hopefully, each provider will have a full medication record, including whether patients are filling their prescriptions and then refilling them at the proper interval. Both are currently major problems. Currently patient privacy concerns, the topic of lesson four are an impediment to routinely and automatically providing this information. We just said that many patients either don't fill their prescriptions at all or take far fewer of the pills than prescribed. Why might this be the case? Well, good answers would be that they may not be able to afford their medications so they don't fill them. Or they fill them and try to stretch out the supply. They may not understand how to take them properly. They may not understand the benefits of taking them. Or they may not like the side effects of the medications. So they don't take them. Another benefit of e-prescribing is that more patients actually fill their prescriptions. Why do you think this is? Well, it's more convenient since the prescription is ready when they go to the pharmacy with no effort at all on their part. Since these problems have been around for a long time, why are we only now getting around to solving them? One reason is the increasing incidence of chronic disease for at least the two reasons illustrated here. As you probably know, people are living longer, as we can see by comparing the green bars, from 1999 to 2000, with the blue bars, only ten years later. Chronic disease rates increase with age. So, that's reason one. Moreover, in large part due to behavioral issues such as increase in obesity, the rates of chronic disease have increased significantly, even over this relatively short time period. This lesson is about the policies and programs the US Federal Government has created to encourage the adoption of electronic records and other health IT tools. And to provide incentives for their use in ways that will improve the quality and efficiency of care. Healthcare providers have had a financial incentive to do all available tests and procedures once someone is sick. And have had little or no incentive to keep their patients well. Until recently there was no incentive except for special situations like HMOs that we discussed in lesson one, to invest money in health IT systems that might actually reduce income by helping avoid unnecessary or duplicative tests and procedures. These incentives are often referred to as pay for performance, or P for P. In the typical P for P system, providers are rewarded for doing tests and procedures that scientific evidence suggests are beneficial for either managing or preventing chronic disease. For diabetes this might be an annual blood test called hemoglobin A1C that we'll discuss later on. It might also be screening their patients for obesity and smoking and counseling them to lose weight or quit. An advanced pay for performance system rewards providers who are able to provide higher quality care at a lower cost. There's evidence that these programs can work. Here are the results of the Physician Group Practice, or PGP, demo, a well known Medicare pilot conducted in ten medical practices. All of the groups achieved improvements in 25 of 27 quality metrics for the key diseases shown here, and you see the percentage of improvement as well. Four practices earned a bonus for outstanding performance. Marshfield Clinic earned half of the total bonus. And in explaining how they did it, their CEO cited a well developed electronic health record and went onto describe how it reduced unnecessary duplication of services by making information available to all providers caring for each patient. In fact, Marshfield developed the EHR system they were using, and in April 2015, at the largest health IT industry meeting held each year called HIMS, they introduced a very innovative new version of it that combines the traditional charting functions of an EHR with fully integrated population health tools to better manage chronic disease patients like Marla when they aren't in the office. They also added fully integrated analytics so that physicians could easily understand the overall quality of care being delivered by them to all their patients and a fully integrated patient portal so that Marla could engage with her physicians in her care. Accountable care organizations are a key feature of the new Affordable Care Act, whose design is in part derived from the PGP demo we just discussed. There are currently a few hundred ACOs being formed, or in operation. 32 select pioneer ACOs are piloting an even more advanced approach. Like the Marshville Clinic, pioneer ACOs must use advanced health IT to manage entire populations of patients in a coordinated manner to electronically exchange healthcare data. They must have the ability to share performance feedback. They must provide a portal providing access to patients to their own data. And they must have a demonstrated ability to coordinate care. So which of the following is the most advanced approach to pay-for-performance? The answer is Pioneer ACOs. In 2004, in his State of the Union address, President George W Bush made universal adoption of electronic records a ten-year national goal. He tasked a new Office of the National Coordinator for Health IT with achieving that goal. In 2009, the Obama administration's Health Information Technology for Economic and Clinical Health High Tech Act provided funding of from $20 to $30 billion, the exact amount depends on adoption levels. To reimburse hospitals and eligible providers for adopting an electronic health record system. Providers are eligible based on the amount of Medicare or Medicaid patients they manage. The adoption program has three co-dependent components. EHR certification specifies the minimal acceptable EHR capabilities that vendors must provide. So it's the vendors of the EHRs who obtain EHR certification. Meaningful use specifies the minimal provider usage of a certified EHR in order to qualify for incentive payments. And of course the incentive payments themself are specified and come from either the Medicare program or the Medicaid program to hospitals and eligible providers, but they can't be paid by both programs. As we just said, EHR certification specifies requirements for commercial EHR systems. This is a very important development, because for decades the grand challenge for health IT here in the U.S. has been interoperability, the ability of hundreds of commercial EMR products and tools to exchange and meaningfully share data. Even with universal adoption, without some degree of interoperability, we still wouldn't be able to coordinate care among the many providers who care for the same patients and who may well have different EMR systems. So the EHR certification process supports, among other things, a basic interoperability standard or capability. Systems must be able to record key specified demographic and clinical data. They must be able to provide tools to measure and improve care quality. And, of course, they must protect the confidentiality, integrity of the health care data that they record. We'll now look at each of these in some detail. Here's a list of the kinds of data that must be collected. The reason for some of these should be clear from our prior discussions. Smoking status, to help prevent the development of chronic disease. Maintaining an active medication list, to avoid duplicate or inappropriate medications. Performing a drug formulary check, to make sure the best available medication is chosen for each clinical situation. And maintaining a current problem list. So that each provider has a more complete overview of what's going on in their patients care. Keep this list in mind, as we later discuss meaningful use. The EHR usage requirements placed on eligible providers. Just recording data isn't sufficient. Certified EHRs must provide tools to use that data to improve care quality through functions like these. We earlier discussed the key role medications play in managing chronic disease and the many problems they currently create. So note that the first four of these all relate to improve medication management. Also note that the EMR must be able to calculate and submit clinical quality measures. Remember that as we discuss the key role that quality measures play in determining meaningful use. Finally, none of this can be done legally, or would be accepted unless EHRs can ensure privacy so that patient data is accessible only to people to whom the patient grants access. They must also provide security from authorized access. Other systems for information exchange must provide a means to establish trust to know for sure that the person or entities with whom information is being exchanged are who they claim to be. The technologies to help meet these challenges will be the topic of lesson four. Which of the following is not a key component of HITECH? The answer is, audit of provider records. Which of the following is not a key component of EHR Certification? The answer is audit of EHR vendor sales data. Vendors become certified through a formal testing process developed by the National Institute of Standards and Technology, or NIST for short. The actual testing is administered by one of several companies who have contracted with NIST to do it. We'll now take a closer look at that using one of the required items for data collection. You should recall that a certified EHR must maintain a current problem list. But what does that mean? Medical problems are usually coded in a global data standard called the international classification of disease, or ICD for short. A vendor can demonstrate that they meet this criteria by showing the EA chart can store test ICD test codes and supplement them with problem status and date diagnosed. The testing would include the asking the vendor change your problem status. For example, from active to resolved and demonstrating that this change is posted and displayed in appropriate places throughout their EHR. Quality reporting is a particularly interesting and challenging area generally done by reporting process and/or outcome measures. While outcome measures are usually preferable, we often don't have practical ones available. Hemoglobin A1c is a test done on a blood sample that can serve both as a process and an outcome measure for diabetes care. Hemoglobin, the red stuff in your blood cells, is the oxygen carrying molecule. The level of its A1c variant tracks with the amount of glucose out in the blood. Glucose is the molecule that is not properly regulated in diabetes. So the more that's out of the blood, the more of it that enters the red blood cells. The more glucose in the cells, the higher the A1c level. As we can see here, not much glucose, a low A1c level. Lots of glucose, an unhappy red blood cell because its A1c is very high. But this increase occurs over time, so hemoglobin A1c is proportional to the average blood glucose level over the prior couple of months. This is great, since the goal of diabetes therapy is to keep the same average glucose level within normal ranges over time. As you can see here, the blood glucose levels go up and down depending upon what's eaten, exercise and other factors. However, it may go up and down here the hemoglobin A1c level remains constant at 7, indicating good control. In fact, good control is defined as an A1c under 9 by most organizations, but the highly regarded Mayo Clinic uses this number 7 as its benchmark. Which of these would be an outcome measure for diabetes? Recording of key data, testing for HbA1c level on a regular basis, a HbA1c level under 9? The answer is a hemoglobin A1c level under nine, that's the outcome because it indicates that the diabetic is under good control. So, which of the following might be a process metric for the management of high blood pressure? The answer is regular measurements of blood pressure. The actual level of the blood pressure might be a good outcome measure. But the process metric is the fact that the blood pressure is regularly measured. That concludes EHR certification. The other two components of high tech are meaningful use and incentive payments. Before we discuss them, we'll have a basic overview of electronic medical records. Physicians are in the data business. They collect data. They make treatment decisions based on that data. And they follow patients through other data and make adjustments as needed. The data physicians record can be divided into subjective information, generally information collected from the patient, such as the chief complaint, the reason they visited the physician today, their medical history, and history of the particular problem that brought them to the doctor. Physicians may collect objective data by doing a physical exam, ordering X-rays or rather images, or doing lab tests. The resulting data can be in many forms. It can be free text. It can be structured codes, such as the ICD we referred to earlier. Could be continuous wave forms from an electrocardiogram, images, sound recordings, or even videos. Increasingly data from chronic disease and other patients may be coming from their smart phone or from physiologic measurement devices they use to track their own health at home. Storing and presenting this rich data set is an EMR design challenge, yet many EMRs still mimic the organization of the paper chart, even simulating the tabs that divide the sections where the data types are stored. Despite its key role in medicine, physicians have historically focused far more on collecting the right data than they have on recording it completely, accurately, and legibly. As we've seen, in a fragmented system of care where many physicians treat aspects of a single patient's care, this can lead to duplicate testing or even errors. Here's a particularly dramatic example. For which medication was this prescription written? What is Isordil, Plendil, or are you not sure? The pharmacist thought it was Plendil and dispensed that. The physician said he intended it to be Isordil. The patient died, leading to the first successful lawsuit in the US for illegible handwriting in a medical record. Which of the following is an example of objective patient data? The answer is the physician's recording of blood pressure. The other data mentioned here come from the patient and would be considered subjective. Meaningful use is a very important, highly visible and even controversial at times program, that's divided into the three stages that phase in over a period of years. Stage 1 is about data capture and sharing. Things like electronically capturing health information in the standardized format, specified by the EHR certification program. Stage 2 is about more advanced clinical processes, but is similar to Stage 1, just more ambitious and more sophisticated. For example, the increased requirements for E-Prescribing, and incorporating laboratory tests. We'll look at that in a moment. Stage 3 aims for improved clinical outcomes, which depends on more advanced EMR functionality, such as clinical decision support to guide providers. And tools to help patients become more involved in their own care. Stage 3 is ambitious, and has recently been pushed out to at least 2017. Today, a majority of providers participating in meaningful use have achieved stage 1, and are now focused on stage 2, but only a small percentage have achieved stage 2, to date. We earlier discussed quality measures, and now you'll see why. Providers demonstrate that they have achieved Meaningful Use Stage One by submitting measures in three categories, core measures, menu set measures, and clinical quality measures. There are 15 mandatory core measures. Providers must also submit five of ten menu set measures, as well as six clinical quality measures. Three of these are mandatory, and providers can select the other three from a list of 41. The core measures are divided into the four groups shown here. Improve quality, safety, and efficiency, and reduce health disparities. Engage patients and families. Improve care coordination and privacy and security. Menu set measures are similarly grouped as shown here. As with core measures, you should easily be able to see a close alignment with the coordinated management of chronic diseases, along with other key priorities, such as ensuring privacy and security, and improve population and public health. The three mandatory clinical quality measures are screening for weight, screening for and diagnosing hypertension, and preventing care and screening for smoking. All closely related to chronic disease. As we mentioned earlier, Stage 2 is similar to Stage 1, but raises the bar both qualitatively and quantitatively. It also introduces some striking requirements, with respect to patients participation in their care. Here we see the bar going up from 40% E-Prescribing to 50%. And the qualitative addition that prescriptions be compared to at least one drug formulary. A list of medications usually reviewed by experts, and felt to be cost effective treatments for their target condition. I feel that no aspect of MU2 is more interesting, and potentially more impactful to the management of chronic disease, than the requirement that more than 5% of all unique patients seen by an eligible provider during each reporting period either View, Download or Transmit to a third party their digital health information. This is referred to as VDT, and ONC has related specific guidance, as shown here, as to what electronic patient summaries should be made available to patients under various care scenarios. The first one listed here is Transitions of Care. This is, for example, when a patient goes from a hospital to a nursing home or back to their home from either of those facilities. Errors often occur at transition points because information is not passed on completely and accurately. In lesson five, we'll look in detail at the electronic clinical summary specified at transitions of care. But you may wonder why providers are measuring activities usually done by their patients. Research shows that patients are far more likely to embrace electronic self-care tools if their provider encourages them to do it. And even provide some help in getting there. As a result of VDT, many more provider practices will be offering encouragement and even the needed tools integrated with their EHR. We'll return to this and to VDT in lesson seven, when we discuss these patient facing tools. Later, you'll also obtain and use Marla's clinical summary in some activities. Which of the following is a key new goal of Stage 2 of Meaningful Use? The answer is patient engagement. A proposal for meaningful use stage three has been posted for public review and comment. What will happen next is not yet clear, but it is interesting to look briefly at what's been proposed. We'll begin with patient access to their own data, and we'll see how the bar gets raised as the stages of meaningful use progress. We've discussed the requirements in the first two stages. In Stage 3, as proposed, 80% of patients must be able to access their records either through the VDT function, or through an ONC certified API, or application programming interface. And 35% must have access to patient-specific educational materials. We mentioned the term API in our team project discussion near the end of lesson one, when we discussed FHIR. For all practical purposes, these two terms are probably going to be synonymous in healthcare, that is FHIR will be the API used in healthcare. Unsurprisingly, Stage 3 also raises the bar on actual patient engagement. 25% of patients must access their records, either through VDT or through an ONC-certified API. 35% of patients must receive a clinically relevant secure message. And of great interest with respect to patient phasing technologies such as those we'll discuss in lesson seven, providers must incorporate patient-generated data from non-clinical settings such as home health for 15% of their patients. This requirement clearly anticipates the increased use of mobile technologies and sensors in the home. Stage 3's proposed provider requirements also increase the percentage of e-prescriptions to 85% for eligible providers and 25% for eligible hospitals. They also require the use of 5 clinical decision support tools tied to 4 quality measures. And providers must use drug to drug and drug to allergy interaction alerts during the entire reporting period. Hospitals must receive at least 80% of medication orders, 60% of laboratory orders and 60% of diagnostic imaging orders via computer-based physician order entry. Although order entry by scribes, assistants to physicians who enter the information for them, does count toward these goals. Finally, Stage 3 has specific interoperability requirements. Providers must send an electronic summary for 50% of transitions of care and referrals. You recall that these are major sources of error in patient care. They must receive an electronic summary for 40% of transitions and referral. And providers must perform medication allergy problem list reconciliation for 80% of transitions of care and referrals. Finally, eligible providers must electronically submit three out of five of the following reporting requirements. Immunizations, syndromic surveillance, this would be for infectious diseases. Reportable conditions case reporting, certain conditions that need to be reported to public health. Public health registries to keep track of certain diseases and conditions and non public health registries, such as those for cancer. Eligible hospitals must submit four from the same list to which electronic lab reporting is added. As with most public policy issues, stage three is controversial with virtually every imaginable opinion being expressed. We'll need to wait to see what the final rules say. Beyond these programs there's a need to change incentives in at least three ways. The first is reimbursement for the expense of the electronic health record system. The second is some ongoing financial incentive to use the system properly. The Medicare and Medicaid incentive programs were designed to do the first. The Medicare payment program may do the second through penalties for providers in hospitals who don't achieve meaningful use. It's increasingly possible that private health insurance companies may start funneling patients to providers who have achieved meaningful use and the prospect of that is another incentive. The third needed incentive is changing reimbursement to some form of pay for performance to further reward providers for improving care efficiency and quality. The Medicare and Medicaid Incentive Payments are based on achieving the stages of Meaningful Use, shown here. The amount providers can earn is tied to the number of Medicare patients in their practice. For the Medicaid program, 30% of a practice is necessary to qualify, except for pediatricians where the threshold is 20%. The details for Medicare are shown here, and you can review them if you're interested. Basically the earlier provider starts, the more they can earn. Not shown here are the reductions in Medicare Payments for providers that haven't achieved meaningful use by 2015. But there are no penalties under the Medicaid program. Why do you think there are no penalties under the Medicaid Incentive payments program? My answer would be that these are providers caring for poor patients, often in rural underserved areas, for already low reimbursement levels. So penalties would be inappropriate. These programs have succeeded to a greater degree than many observers, including me, thought possible. As of March 2015, 86% of eligible hospitals and 73% of eligible professionals had received some incentive payments for achieving at least Stage 1 of Meaningful Use. Here you see the composition of these adopters. As you might expect, younger providers are more likely to adopt but the difference isn't striking. What is striking and has been in all prior studies, is the difficulty smaller practices have in adopting. Presumably due to the lack of financial as well as technical resources, since many of these are in rural, poor, under served areas. ONC funded a special Regional Extension Center Program to provide special help to these providers. Note, as well, that practice ownership can be a significant factor. Once they adopt, substantial majorities of providers report administrative, financial, and most important clinical benefits. In fact around 70% of non surgical specialists report improved care coordination. A key result given what you've learned about the fragmented approach to managing chronic disease. Another reason survey reports that despite shortcomings in their current EMR less than 20% of providers would revert to paper if that was an option. Which of the following are the most likely to be using an EMR? The answer, if you looked at the graph for practice ownership, was physicians working in an HMO, a group where EHR ownership or adoption is 100%. Which of the following best summarizes the attitudes of EHR adopters? As we said, they're both happy an unhappy. They like some aspects of their EMR, they have problems with others. Today I'm speaking with Dr. Doug Fridsma. Doug is an internal medicine physician and also has a PhD in Biomedical Informatics from Stanford. Starting in 2009, he joined the Office of the National Coordinator for Health IT, was there for some five years and is now the President of the American Medical Informatics Association. So, Doug, thank you for being here with me today. Thank you, I'm delighted to be here. So you were there from the beginning of high tech, effectively through the end, although ONC is still in operation. >From my point of view you guys accomplish a great deal. More maybe than I would have imagined, that many people would have imagined. So can you tell us what you feel the high points and the major accomplishments were? Well I think if you take a look at the adoption of electronic health records in a country. When we started in 2009 there was about a 17% adoption of electronic health records. Just last year it's at least larger medical centers, It's close to 80, 90%. And even among the physicians in the smaller practices, it's probably in the range of 60 to 70%. So in the course of 5 years we took 20% of the largest GDP in the world. And we convert it essentially from a paper-based system to an electronic system. And I think It's not the end. That actually is now the beginning as we begin to figure out how to connect the pieces and how we can refine and make them better for the doctors and patients that use them. So, as I speak about things today, I say that the first big challenge was adoption, and that has been largely overcome, not entirely overcome And predictably, the next big challenge is interoperability. And as you read about physician frustrations with EMRs, the difficulty of getting these systems to talk to each other is usually high on their list, if not at the top of their lists. So, where do you feel we are on that and what does your crystal ball tell us? Well, I think interoperability is probably the next big challenge. I think there are issues in usability. I think there are issues in safety and security. All of those things, I think, are going to be important now that we've got adoption. But, when it comes to interoperability,the thing that is so critical is to really define what it is that you want to do. So when somebody complains that their systems are not interoperable, my first question to them is well what were you trying to accomplish? The definition of interoperability is about the ability to exchange information, and then the ability to use the information that's been exchanged, so you can't separate interoperability from the thing you want to do. We think sometimes that interoperability is this green pasture that we're eventually going to get to. This state of being that everything is going to work. But in fact, interoperability is one of those things that's based on the things you want to do. It's incremental. It's iterative, and we will never probably ever get there entirely because there will always do a new thing that we want to do for which the capability doesn't exist. So this course in the informatics world in general there's a lot of excitement about FHIR, as maybe the solution to interoperabilities. What are your views on that? And do you see FHIR as the sort of tool that can evolve as people's requirements become more sophisticated over time? So, the great thing about FHIR is it solves a problem. And when you've got a standard, or if you've got a technology that solves a problem people are going to use it because it helps them. A lot of times we create standards in committees and have the expectation that if we build it, people will then use it. And we've got that wrong. What we need to do is find the problems for which we can develop standards that solve those solutions. HL7, which is sort of the place that FHIR has developed, it had their first set of standards really to help create interoperability within the enterprise, and many of the vendors adopted it, it was wildly successful, it was able to connect all of the pharmacy systems to the laboratory systems and things, using those kinds of messages. FHIR stands to transform the same thing between enterprises, and because it's aimed at the implementer and geared towards the person who's actually going to have to use it. It's simple, it's easy to use, it uses the best technologies that are out there. And a standard isn't a standard because you say it is, it's a standard because people use it, and this is a standard that people actually are using, so I think it's going to have tremendous impact. So shifting gears a bit, we've emphasized in this course, and are emphasizing, the chronic disease patient who's got multiple problems and multiple doctors. There's an obvious need for information sharing there, and the role that the patient might and arguably should play in their own care. As you look out to the future, how do you see the patient fitting into the whole spectrum of informatics and where the data is stored and how the data is accessed? You know, you can take a look at what's happened in other parts of a person's life, an individual's life, to get a sense for what may be happening in healthcare. So, it used to be that to get a airline ticket, you had to call up a travel agent who would then access this system that you had no way of knowing what was in there. Now, all of those things are available for individuals and they can make their own hotel reservations, and their own plane tickets. I think the EHRs right now are kind of in that same rule. You go there, there's information in there that the doctor tells you about, and then you'll use that information in your healthcare. As we begin to open that up, not only in the terms of having access for the patients, but for the patients to put their information into it. You suddenly have one more information, and you empower the patient to have a whole lot more knowledge about what's going on. So, to me, it's not a matter of if, it's a matter of when. Because the things that are happening in the rest of society are going to begin to impact healthcare in a substantive way. And as it becomes easier and cheaper for patients to have access to their own information, that's going to happen. So if adoption was the first big challenge, and interoperability is the second, and the least in my view of the world, the third is the analytics. Where do you think we are in analytics and where do you think we're going to go in the next few years. I think, again, there's a lot of good work that's been done in bioinformatics. I think we're starting to see it in other areas in which people are beginning to aggregate that data. Part of the challenge that we have in health care is that not only do we have knowledge about healthcare and medicine and the associations there. We need to be able to apply that to the data that we see. And so, it isn't simple mathematical relationships. It isn't necessarily, simple associations. But there's sort of a deep understanding of medicine and care delivery, that's embedded in the data there. So, it's an exciting time because as more and more of this data becomes available, we're going to be able to apply those informatics techniques to be able to extract information. But to do it in a way that is understanding of how the cure is delivered. And if we can leverage the best that's happening in the engineering field and computer science and marry that with what we understand about healthcare delivery. I think we're going to make some good progress. Are we there yet? No. I think there's still a lot that needs to be done. We're seeing some areas of excellence, sort of centers that are doing some of that, hospitals like Geisinger and Kaiser and others that are doing some analytic work. But I only see this accelerating at an incredibly rapid pace. Storage and the movement of the data is becoming less and less expensive. And as a result I think more and more analytics will be applied. Is the lack of structure in a great deal of medical record information a barrier to that? Do we need to keep pushing toward a goal that many of us had years and years ago of everything being structured, or Are we okay with where we are at, in terms of nature of the data? I actually think that you need structure when you need to make decisions or help for an individual patient, or a small group of patients. It's difficult to determine a drug-drug interaction through our data analytics on a single patient. So you need to be able to categorize that stuff so that rules can fire and alerts can be sent. But when you start to have hundreds of millions of patients. I think the need for structure actually goes down, even technologies like IBM Watson unstructures structured data to be able to do the analytics that they do. So, I think we need to be very comfortable with the ability to have unstructured data. Part of the challenge that we're going to have is that if what we do is we require structure as a barrier to exchange of information. We're going to sell ourselves short in our ability to do analytics. We need to get all the data out there so that we could apply those analytics. If you could rewind the clock and spend your five years at ONC all over again, what are one or two things you would do differently? You know, it was a remarkable time to be in the federal government, with the passage of the HITECH Act, and just the excitement that was going on. Obviously if all of the standards had been baled, and had been ready. That we could have just picked them off the shelf and put them out there. That would have made our job a lot easier. What we need is going to be a portfolio. We have the beginnings of a portfolio. I think we did a grand experiment that was different than any other country in terms of how we tried to approach this problem. We didn't engineer it from the top down, we created the building blocks and allowed the market to sort of drive that. Now, it's resulted in some innovative companies sort of getting lost. I wish there was a way that we could have created a playing field that allowed everybody to equally compete. But sometimes that's hard, and when you use a market-driven approach, there are some Winners and losers in all of that. The challenge that we have I think is that, as we have all of these products out there, we've reached a level of saturation and adoption in the market place. We're going to start to see some consolidation. What's important is that we don't lose that innovative and entrepreneurial edge as that happens. And I think things like FHIR, that produce the ability to create apps and to create other kinds of functionality are going to be really important as we go into that next level, where we take where we have now, and we begin to refine it and add the analytics that we need. The Jason report essentially paints a picture of EHRs as a database. And most of end-user work, particularly in the physicians and patients, is done through apps that access data,from that database using FHIR,and also, of course, can add data to it. Is that the world of the future that you see? I think increasingly the electronic health record is going to be a data store in the sky. I think that patients are going to be the principle health information exchange, in that they'll be the hub for all this information that's going to be exchanged. I hope that at the end, all of this technology doesn't create additional barriers for patients, but makes them first order participants in their care. I think when that happens, we are actually going to be able to have tremendous innovation. Perhaps the biggest underutilized innovative force in medicine is the patient. And I think their needs are going to probably drive more innovation that anything we could ever think of and think about with regard to physicians and nurses and others who are using the electronic health worker. So the exercise that students in this course are going through, and it aims to learn FHIR and develop fire apps toward the future world where that's the primary vehicle for use by physicians and patients makes sense to you, it sounds like. Yeah, it does. I think that's a great approach to kind of set the stage. I mean, if you want to know what the future is, you have to be there and try to invent it. Well, great. Thank you so much. Thank you. For being with me here today. Thank you. It's time to take a deep breath. We've covered a lot of territory in this lesson. We've learned about the core rationales for HIT adoption, and we deepened our understanding of the link between it and success in managing chronic disease. And how that link led to new federal policies to encourage adoption. We've learned that those polices are divided into three interrelated efforts to assure that EHRs have key capabilities. That they're used in a manner that is believed will lead to improved care coordination and ultimately to improved outcomes. And that financial incentives are there to reimburse providers and hospitals for the cost of these systems and to provide ongoing incentives to use them properly. We've seen that adoption levels are quite high, but that survey data reveals some dissatisfaction with current EHR systems. We'll look more deeply into why that is the case in lesson six, as we begin to look at real-world applications of health IT in the second half of the course. Next, we delve into the technologies that power real word HIT systems. You've now heard of one of them, data standards, when we looked at ICD codes and their role in the HR certification. You've also heard hints of the importance of the other two, technologies for assuring privacy, security, and trust, and for supporting health information exchange. We now begin a three lesson sequence in which we'll cover these in the reverse order to which I've just introduce them, beginning with health information exchange. We know that care coordination among the many providers who treat chronic disease patients like Marla is of paramount importance in improving the quality and efficiency of health care. In the age of the internet, where we're all used to information any time and any place, you would probably think that once health care records are digital, exchanging them would be easy. The reality is quite different. And health information exchange is yet another topic that illustrates the often skewed incentives within our complex health care system. Beyond care coordination, health information exchange is a key tool for population health. It can become a platform for patient engagement and it can be also used for data aggregation for public health and other purposes. Interoperability remains a major technical issue because of the multiplicity of EMRs in use here in the US. There are hundreds of certified EMRs and although a dozen or so represent 75% of all installations, the rest are spread among many vendors. As you can see from this graphic from the ONC's dashboard, a place I recommend you visit. These systems were not designed to record data in a standard fashion or to share data with other systems. They are not interoperable. EHR certification creates a very minimal degree of interoperability. As we'll see later on, building technologies to facilitate more extensive data sharing among these systems is a major focus of contemporary health IT. A closely related challenge is accurate patient identification across providers and health systems. Countries like France solved this problem by issuing a Special Health Smart card. Here in the US a National Identity Number or card for Healthcare is a controversial issue that is unlikely to be solved anytime soon. So, absent one knowing if John Smith at Hospital A, is the same patient as John A. Smith at Hospital B can be hard. Cross-enterprise health information exchange depends on master patient indices that use sophisticated algorithms designed to solve this problem as accurately as possible. Another challenge is figuring out where a specific patient's clinical data is stored. Marla, our multiple chronic disease patient seeing many different specialists, would have parts of her records scattered across many EMRs. Suppose one of them, or a PCP, should she get one, wants an integrated view of her care. Where does it go to get that information? Special document locator systems at some health information exchanges facilitate this type of query by indexing where a patient's images, lab tests, and other clinical data are stored. The new Meaningful Use Stage Two VDT requirements mean that Marla might create her own integrated record by obtaining electronic summaries from each of her providers. Which of these are reasons for establishing a health information exchange? All of these are reasons. Aggregating data for public health. Managing populations of patients. Coordinating care among multiple providers. What is the job of a Master Patient Index? Making sure the data being aggregated is all for the same patient. HIEs are classified in different ways. Scope. What is the geographic area they cover? Status. Where is each HIE in the process of going from an idea to full functionality? Architecture. How are the systems that support the exchange organized? And functionality. Of all the things that are possible, what does each HIE do? HIEs can vary substantially in scope. An HIE can serve a single health enterprise that may consist of more than one hospital, many physician practices, and other entities such as nursing homes or rehabilitation facilities. These entities may have electronic medical records from different vendors so interoperability can be an issue even within a single health system or enterprise. Some health systems extend their HIE to practices that refer patients to them in an effort to make that easier and thereby attract more referrals. This is often called a Service Area HIE. In an effort to better position the health system to contract under arrangements like an ACO, it is increasingly common for them to actually acquire these outlying medical practices, in which case they would definitely want them included in the enterprise HIE. There is a clear business case for HIE within a healthcare enterprise. Make it easy for physicians to refer patients to the enterprise and more convenient for patients to get all their care within it. In most business activities we assume bigger is better and more sustainable. With HIE, the business case typically gets weaker once it expands beyond a health enterprise. As a result, for the most part, regional, statewide and nationwide HIE remains an economic as well as a technical challenge. Can you think of any way wider scale HIE could be made more attractive to healthcare providers? My answer would be make it essentially free using light, web-based technologies. We'll be talking about some of those later on. The second HIE classification is status. E-health initiative, EHI for short, is a broadly based, collaborative, not-for-profit organization focused on improving healthcare quality and efficiency, including support of HIE, where it tracks the status of implementations across the country. E-health Initiative identifies the seven stages of HIE maturity shown here. Start up, getting organized, planning, piloting, operational, sustainable, and innovating. Note the rapid shift toward more mature HIEs, positive sign of progress. The third HIE classification is architecture. The main types are Centralized HIE which involves a master repository of patient data. Federated HIEs where all data remains at the source. And Hybrid HIEs where clinical data is either stored locally with centralized services to help locate it, or in an alternative approach there are data lockers, where any centrally stored clinical data is segregated by institution, and each contributing institution controls access to their data. The Office of the National Coordinator for HIT, ONC, defines a fourth way of classifying HIEs, by function. The first category is Directed Exchange, used to send and receive information between providers for care coordination. Later, Marla will return to help us see how that works. The second is Query-based HIE. This is data aggregation among providers. The third is Consumer Mediated Health Information Exchange, which refers to data aggregation by patients. A key concept in Meaningful Use Stage 2 VDT. eHeath Initiative classifies HIE's by. The answer is maturity. Which of these forms of HIE could help physicians coordinate care? The answer is all of these. Any form of HIE, is a step toward improved care coordination. The premier U.S. example of a large scale, in this case statewide, centralized health information exchange, is the Indiana Health Information Exchange, or IHIE. Its Indiana Network for Patient Care creates a single virtual community-wide health record and delivers more than a million transactions per day from a data warehouse that contains 5 billion pieces of clinical data for some 10 million patients. Docs 4 Docs provides a web portal to make it easy for physicians to obtain documents like lab test results or reports on images they've ordered. Quality Health First is a population-based quality reporting system we'll look at in Lesson 8. Image Zone is an image repository that allows physicians to see any image done on a patient anywhere in the exchange area from their office computer. ACO Services is the newest offering of IHIE that provides tailored management tools for that environment. Providing these services means aggregating data from many sources that are not inherently interoperable. The key component of this diagram is data governance, a sophisticated engine that bridges differences in data syntax and semantics across these sources to create a single database in a standardized format, as though everyone were using the same electronic records system. This screenshot from the Indiana Network for Patient Care illustrates that. Here, a single patient's data from diverse sources is presented to a provider as though it all came from a single record. It's even formatted similarly to a typical EMR. Next we're going to be speaking with John Kansky, the interim president and CEO of the Indiana Health Information Exchange. Well, John, I want to thank you for taking the time to be with us this morning and to help the students learn more about the Indiana Health Information Exchange. They're already aware of the challenges, the financial challenges, the sustainability challenges faced by health information exchanges. As you look around the country, where do you think we are on that? I, yeah thanks, Mark. The sustainability has long been discussed and debated as a key part of health information exchange and I think it's helpful to view that in terms of the three types of health information exchanges that have merged in the marketplace. There are a number of state run exchanges run by their respective state governments, largely funded by our recent era high tech funding. There are private or proprietary exchanges that are run by and controlled by single organization like a large health system for example. And then there's the type of exchange which I think most of us have been thinking about for a number of years, and the type of exchange that we are in Indiana which I call a regional inter-organization exchange. We have to sustain ourselves by delivering services that our customers value in exchange for fees, and making sure that those fees, the revenue from those fees exceed our expenses. The other two types of exchanges, the state run and the proprietary,in some cases have the opportunity to be sustained by their sort of parent organizations. If the state government chooses to levy attacks or generate or provide dollars from the general fund of the state government, they really don't have a sustainability challenge. And in the case of the proprietary or private exchange, they're sort of sustained at the whim of the organization that runs it, so I feel that question is most interesting for the region exchanges. And again, we really have to run ourselves like a business or despite the fact that many of us are not for profit organizations, you need to be very conscious of having revenue in excess of expenses. The way to do that is to focus on services that your customers value enough to pay for. It sounds fairly mundane and it sounds more like a business class than informatics, but it's very important for HIEs to remember to sustain themselves. The students are familiar with what IHIE does thanks to some examples that you sent before the class. Let's take a moment and look at what it takes to do those things. Can you tell us a bit about the IHIE organization? Sure, absolutely. And again, stressing that our, we're not affiliated with the state government, but certainly have a good relationship with them. We're a non-for-profit 501 C3 organization. We have a board of health care stakeholders from across the state including many of our large customers. We have a governance body which is extraordinarily important. Which governs the data use of our data and allows, it provides for confidence between competing organizations to share their data and have it used for our services. And the structure of our organization, we're about 54 people separated into three management stacks. One which is software and product development, one which is all the customer facing functions like support, implementation and customer relationship management and product strategy. And then finally a third stack which is largely IT infrastructure and administrative function. At the heart of all this, of course, is this normalized, curated database of clinical information that you've accumulated from the many different kinds and varieties and brands of EMRs that I'm sure in use across the state of Indiana. How do you do that? How do you bring all that together into something that for example can create the Indiana network for patient care. Which makes every patient look as though they have a single record even though that date may be, may exist in many different EMRs. So how we do that is through many real time interfaces. And an ongoing task of what we call data mapping, which a little bit more specifically is semantic normalization of the data. The best way to explain that, that I've found, is that we like to say that one organization will send us their data in Italian. Another organization sends it in Portuguese. Another organization will send it in German. We translate everything to Latin, so that we can combine it into that virtual patient, single patient record, that you referenced. That gives us the ability, which is really the key for our past, present and the future that we're going to be able to deliver. That gives you the ability to look at a patient's record at the point of care as if it came from one place, even though the data came from many places. It also gives you the opportunity to analyze across a population or to manage the health of a population using data, knowing that you have the complete view of that population, patient by patient. It is very expensive. I want to acknowledge that. To put the labor into normalizing the data and maintaining the interfaces which gets back to that question of sustainability and having to work very hard to make sure you're delivering value to customers to cover the cost of that semantically normalized system of repositories. Well, that's very interesting. I've used you as the primary example of centralized HIE in the course. Does IHIE see a role for some of the newer technologies designed for a federated environment? Technologies such as Direct, RESTful APIs and more recently Fire. Sure. Let me start by commenting that we describe ourselves as a centrally managed federated structure. Meaning our repositories, the data in our repositories, continues to be owned and legally controlled by the organizations that provide the data to us. It's not our data. But it is centrally managed in a single data center which gives us the best of both worlds. We have the performance of a centrally managed structure with the governance advantages and comfort of the providers knowing that it's federated. But that's not the question that you asked. So, let me comment on some of the newer, emerging standards. This is a very practical answer because HIEs have to be practical organizations. We can only base our services on what's available in the marketplace. So using Direct as an example, Direct is one of the newer standards that's finding its way driven by meaningful use into the marketplace. And to that extent, we are providing services based on Direct. Some of the other standards that are still evolving, we pay attention to. We study, we try to anticipate. We try and participate at a federal level in learning about and influencing those standards but we of necessity to sustain ourselves, base our services only on those that are in the marketplace and to an extent Direct is emerging as a useful standard. Well, thank you. And to conclude, as you look toward the future, all right. Can you tell us what additional services or directions you see IHIE going? Certainly, and this is an interesting area for us because you always have to be responsive to what your customers are looking for next and what In our business what the government is steering towards. So, I took a couple of notes of things that I thought might be worth highlighting. For example, one of the things we have done in the past, but we know we're going to be doing a lot more of. And, I wanted to stress, because, I don't believe there are many, if any, other HIEs doing this is that we are increasingly able to put structured data from our repository system into the EHR's in response to some clinical event. So for example, say a patient shows up at the emergency department of a hospital. We're able to take data from the repository corresponding to that patient, matching them with their identity on the fly and send that data, which comes from many, many other sources outside the enterprise where they just presented, and deposit that data electronically in the EHR at that facility. That's something we've only accomplished with a couple of different organizations in our customer base and it's been those that have been somewhat more sophisticated in terms of their IT. But I definitely see things going that way, if for no other reason than we don't want physicians, clinicians to be pulled out of their native EHR, but still need to be able to view data from outside the organization. One more example well, I'll give two quickly. Medication data is something that's in great demand and high value at the point of care. We're working hard to increase the amount and make it more comprehensive. In terms of the data that we can make available at the point of care, we've always done that but it's a very difficult thing to try to aggregate. It's worth noting that that even means working with the state legislature in Indiana to sometimes influence policy to make the data more available and remove obstacles. The last thing that is, I think, worth noting, is analytics is a word that is thrown around quite a bit these days, almost to the point that it loses its meaning without being defined. There's a lot of things in that category. We've already tried a couple of things in the analytics space, with mixed success and it's a great question I think for HIEs whether the marketplace is going to expect them to analyze the data that they are able to acquire and aggregate. Or whether the market is expecting them only to provide the data and not necessarily the analysis. I think that's an open question. Well John, this has been a fascinating conversation. IHIE is something that anyone interested in health informatics needs to know about and needs to follow and I appreciate your taking the time today to help the students understand it better. Happy to do it. Thank you. Other than HIEs operated by what are essentially a very large scale health enterprise, such as the Kaiser HMO or the VA Health System, I couldn't provide another example of centralized HIE at the scope and sophistication of IHIE. Why has Indiana been able to do it? A big part of the answer is the Regenstrief Foundation that supported the development of the technology and the HIE over a period of many years. Without that unusual support, we might not have an IHIE to be proud of and point to. There are a number of commercial technologies for centralized HIE. In addition, the federal government sponsored the development of CONNECT, an open-source solution for centralized or other models of health information exchange. CONNECT is a very robust, complex system that uses web services to facilitate connectivity with other systems. Its architecture is modular with some elements, as indicated by these colors, being required, others being optional or replaceable, and still others being customizeable. It's also a gateway to the proposed National Health Information Exchange, as shown here. Why would some CONNECT modules be replaceable or customizeable? Well, users may have their own versions installed, or may have special requirements. Connect adoption is not widespread, but has been growing. You can get more information on that by visiting the Connect website. Many adopters are federally supported health systems or activities, such as the CDC, VA, or Department of Defense. Others are state HIEs or private entities such as the Marshfield Clinic we mentioned in lesson two as the major success story from the PGP demo project. In federated exchange, all patient data typically stays at the source, facilitating provider participation, but creating technical challenges. Beginning in 2010, work began on defining an HIE approach for this environment using secure email over the public internet. The original used case, with scenarios around which HIT systems are developed, was replacing the fax machine as the way two physicians share data. We'll now go with Molly to her visit to Dr. Johnson to see how that works. You should recall that Marla has had diabetes for 15 years and is now showing early signs of lung disease because of her smoking. Today she's visiting Doctor Johnson the endocrinologist who manages her diabetes. Doctor Johnson examines her and checks her hemoglobin A1c level and her blood sugar. He reports that her diabetes is well controlled. So I'm always happy about that. But, when listening to her lungs, he's concerned and tells Marla he wants to send her to a pulmonary specialist, Doctor Jones, for a more careful evaluation. As they're talking, Marla notices that Dr. Johnson is recording what they say on a new tablet computer and asks about it. He explains that he recently installed an electronic medical records system as part of a federal program. He goes on to explain that he'll be able to use it to send an electronic record of her care to Dr. Jones, so she will have it well before Marla's scheduled visit. Marla is impressed. She doesn't want to remind Dr. Johnson, but she remembers that when she first visited him, he didn't have her records, and she ended up having some blood tests she had already had in a prior visit to one of her other doctors. Marla doesn't like needles, so she's glad this won't happen again. To send Marla's record to Dr. Jones, Dr. Johnson uses the direct service provided by his state HIE. This simplified and somewhat futuristic diagram explains how that could work. Here Marla's record has a share button. That's not quite available yet but some EMR vendors are working on integrating direct into their workflow so it may appear in the future. Doctor Johnson clicks it and selects Doctor Jones from a list of his usual referrals. If needed he could could look up a new physician in a directory provided by the direct server called a Health ISP or HISP. Here, for simplicity, we're assuming that Doctor Johnson and Doctor Jones are using the same HISP. Before physicians are listed, their identities and qualifications are verified, and they're assigned a special direct email address. This establishes trust so Dr. Johnson knows the email will only go to Dr. Jones. The form of the record can be whatever Dr. Johnson chooses. It could be a scan of a paper record or it could be a special XML formatted patient summary specified under VDT. In any case, the record is encrypted and attached to a secure email. This establishes security so the information can't be viewed in transit, a key requirement for health information exchange. Privacy was taken care of when Marla consented to sharing of her record with other physicians involved in her care. If you've ever signed a HIPAA release in your doctor's waiting room, you agreed to this. Dr. Jones gets the email in her direct inbox and can review the record if it's a PDF or paper scan. If it is in a special XML format called a Continuity of Care Document or CCD, the record could be parsed and put right into Marla's chart in Dr. Jones's office as shown here. Parsing it and merging it into an existing record is yet another futuristic component of this diagram. Note that the encryption and decryption of Marla's record is done using public key encryption. We'll look at how that works in lesson four. We just saw a simplified explanation of direct. Here we see essentially the same thing, but in the more likely and more complicated case where Dr. Johnson and Dr. Jones are connected to separate HISPs. Here, S/MIME is used to encrypt the attachment containing the patient record. SMTP is used to send secure email from an email client like Microsoft Outlook. HTTP/S is used for sending web mail, like Google Mail. The sender's HISP can discover or find the recipient provider's X.509 certificate, their public key, using DNS or LDAP, two technologies for managing distributed resources over a network. Once it has the recipient's public key, the sender's HISP uses it to encrypt the message. It them uses SMTP to send the message to the recipients HISP having used DNS or LDAP again to find that HISP based on the recipient's email address. The recipient's HISP decrypts the message using the recipient's private key, which is normally stored in the HISP as a service to its registered providers. The recipient then retrieves the message like any email, but using the secure versions of either Post Office Protocol, POP, or Internet Message Access Protocol, IMAP email retrieval. Which of these could be a direct attachment? A fax TIFF file, scan of a paper document, a printed version of a chart in PDF format, or all of the above? The answer is all of the above. A provider can choose to attach whatever they wish to a direct message. Direct currently has some technical limitations. The first is disposition notification. In certain situations, it's very important to know that a direct message has been received. Clinical laboratories, for example, are using Direct to send results back to ordering physicians. An abnormal result may require urgent attention. Confirmation of delivery is available, although currently not very robust. But it is accomplished using encrypted and signed message disposition notification. Efforts to make this more robust are discussed in the text. Trust among HISPs is another challenge. Earlier we said that the HISP established trust by verifying that Doctors Johnson and Jones are who they say they are and that they are licensed physicians in their practice state. That's fairly easy to do on a local basis. Suppose, however, that physicians in two states, or even two regions of the country, want to use direct. They will almost certainly be connected to different HISPs, each of which established its own policies for establishing trust. Nationwide there could be hundreds of HISPs. Is each of them going to have to investigate and contract with all the others to make sure they can trust each other? That gets unwieldy fast, as shown here. Direct Trust was established to provide a solution by establishing a trust community. As the number of HISPs increases, the number of trust relationships needed doubles as ten more HISPs are added? Triples as 30 more HISPs are added? Grows exponentially? Depends on where the HISPs are located? The answer is it grows exponentially because each HISP needs a relationship with all the other HISPs Another direct challenge is Pull, automating the retrieval of data. So far we've only discuss the fax machine replacement-use case where Dr. Johnson sends a patient chart to Dr. Jones when he refers Marla to her. Suppose Dr. Jones wants information from Dr. Johnson. Suppose Marla has started recording her blood glucose readings at home and Dr. Johnson wants to retrieve the most recent ones to see how she's doing. Suppose Marla has established a personal health record and wants it automatically updated whenever there's new information in any of her medical records. These are all examples of Pull, where the recipient, not the sender, initiates the message. Accomplishing this is still in the future, but a group of experts have proposed how it might be done, as shown here. Using some new standards within Direct for requesting information and sending back a response. This would be done using specially formatted XML documents, similar to constructs that already exist in quality reporting and that we'll see in lesson eight. Another interesting aspect of this proposal that's not specifically limited to Direct is that all of the patient's data would be stored in a repository that would receive and respond to these requests, presumably based on each patient's information sharing preferences. Many people propose something like this to simplify the privacy issues we'll discuss in lesson four. Can you think of two advantages of patient controlled data repositories? My answers are that more patients might participate since they're in control, and that patients could review and correct their data, thereby possibly avoiding medical errors. The final issue is incorporating direct into the physician's workflow. Earlier I showed you this and mentioned the share button, as well as automated import of the data into the recipient's EMR. These are examples of how HIT would ideally be more finely tuned to support efficient workflow. Absent them, the sending physician might have to export the patient chart from their EMR, start up their email program, find the exported chart in whatever folder it was placed, attach it, address the email, and send it. That's pretty much the way you would send an attachment using email. All of that, except for addressing email by selecting the intended recipient, could be automated. At the receiving end, a similar but essentially reverse process could be avoided. Moreover, were the attached patient summary in XML, as we suggested earlier, it could be parsed, so that, for example, Marla's blood pressure, pulse, heart rate, blood sugar level, and so on, could into the appropriate fields in the recipients EMR, saving still more time and eliminating potential sources of transcription error. Well we're with Laura Adams the CEO of Rhode Island Quality Institute, which operates the state of Rhode Island's Health Information Exchange, called Current Care. Laura, I really appreciate your taking the time to be with us today. The Current Care has a very interesting history, dating all the way back to 2001. If you could just explain that to us and maybe the role that you played in getting SureScripts launched briefly and walk us through today, that would be great. Certainly. We started in 2001 as a public private partnership. We're a 501, not for profit free standing entity, not a state agency, but deeply engaged with our state as a partner in all of this. Our goal was to dramatically improve quality, safety and value. We started out as a consortium of leaders basically, people with our hands on the levers of Rhode Island's healthcare system. And our first question was we'll great, we're all in a room, we're all agreeing to collaborate, what do we do? We settled on information technology. My background is quality improvement and I knew that I was stymied at every turn because of the paper based system, that if we were really going to improve quality we had to go for health information technology. So we became the place where SureScripts decided that they would do their first beta test of end to end true electronic prescribing. That pilot ended up being an incredible success and obviously SureScripts is nationwide now and robust and we're very proud of that opportunity to participate in that. >From that point on, we received the state on our behalf, the state government received a grant from the agency for health care research and quality called the State and Regional Demonstrations In Health Information Exchange. That came in 2004, and kicked off the funding for our privacy framework, our governance framework, and our stakeholder engagement piece of this. Along came HITECH in 2009. Our organization was the only one to hit the trifecta, to win all three major HITECH grants. That was the Beacon, the HIE, and the Regional Extension Center. We were able to land about $27 million in 90 days to build out everything we've done so far. So, that's sort of the history of how it all came about. Now, one of the reasons I'm here is because you're such an innovative and extensive user of Direct. So, how did that come about? We are a big believer of a centralized model in that what model you choose depends on what you're looking to do in the future, what your goals are. For ours we wanted a very robust, persistent database where we could do research, we could do quality improvement, population health, comparative public reporting, lots of things with that. But in 2004 we did a little survey and there were 60 plus different, EHR platforms in our state and we said, what are we going to integrate with all 60 of these? There's going to be 70 and there's going to be 80 and then we have to keep every interface up and running. The cost of that was beyond our reach and the time frame for doing that was beyond our comprehension, we want to move fast here. So suddenly we were at a national meeting when Direct was announced and then our Chief Operating Officer, Gary Christianson, turned to me and said, point to point. He said what if one of those points is in EHR and the other point is our HIE, Current Care? Couldn't we use that to lift a continuity of care document out of those EHR's? No interface required, no upkeep required, no millions of dollars to get our community connected. We were astonished at the simplicity of that solution and it has worked for us. So we have platforms in beta line of code. When a physician closes a record or some other trigger that they've put into their platform, we close the record, it sends out an automatic API call up to Direct messages formed. It attaches the current continuity of care document, ships it up to current care. So it's all automated, behind the scenes, no physician ever has to push a button, to remember to do something. It's automated, so for us, we were incredibly excited about it. So, I want the students to focus on what Laura just said. I've talked repeatedly about the need to integrate direct into workflow, and you just heard an example of that. That's right, that's right. So, next we're talking with Elaine Fontaine who's director of Data Quality and Analytics at the Rhode Island Quality Institute in CurrentCare. Thank you very much for being with us today. Thanks for having me. We all know, Elaine, that we need new financial models to make healthcare work the way we'd like it to work. The students are quite familiar with that from earlier lessons in the course. And they recognize that those are intimately tied to quality reporting and quality improvement. So, can you tell us the things that CurrentCare is doing to support providers here in Rhode Island to achieve those things? So I think that we are currently providing actually outside of CurrentCare data, quality data management on reporting services for the Chronic Care Sustainability Initiative, which is Rhode Island's patient-centered medical home project. And for that project, we're actually collecting and aggregating data on about 15 quality measures that are sort of related to meaningful use. We're also doing utilization reporting and patient experience through the patients under medical home caps survey which is what NCQA requires of those practices. And with those data we're really sort of looking, grouping them up and aggregating them in a variety of ways to help the practices see their performance over time. And assess their success with regard to improving the quality and reducing costs which is of course not that easy. But we think that the differences in the data sources associated with what's available to the HIE, relative to what's available to a EMR, relative to what's available in a claims data set. All of which are trying to be used for this same purpose of looking at cost and quality really can potentially result in different answers to the same questions because the populations are a little bit different. So for example, we're working on a chlamydia screening analyses. And I'll be presenting it, Amy, in November of this year, in 2014, to look at the differences in those rates between those three population groups. So commercial payer will have a different result than a Medicaid payer which will have a different result than what's coming out of the EMR and the HIE. So we're trying to understand those differences before we report them publicly from the perspective of here's how you do a better job taking good care of quality docs in the community. So, is it fair to assume that the HIE does serve to help collect and help aggregate the data so that you can do this quality report? I feel like we're really pretty nascent in that regard, and so these chlamydia screening rates are our first entry into it, but I'm pretty sensitive to the idea that we need to understand those differences. Is it that we as the HIE have a sub-selective population which might have a higher screening rate or a lower screening rate than another group. Are we including the uninsured, which are not really represented in anybody else's measures. So until we sort of dig into that and really are able to benchmark against known metrics, I'm reticent to sort of just say here's the real answer for the population of Rhode Island. And I think that's the goal and I feel confidant we're going to get there. But, it's one that we need to move cautiously in that regard because the worst possible thing to do is to report data without really understanding what it means. I think that you can do that in some situations where it's sort of marketing, but quality and cost analytics are not marketing. And when you're presenting information that would reflect how a provider is performing. You need to make sure you have their full engagement and input in how that measure is calculated, and they need to be able to be very, very confident of its accuracy before you report it Half the students remember these comments when we, later on in the course, we get into quality reporting, and I try to show that it sounds great, but it isn't so easy to do. Yes, it's really quite challenging without a doubt. But it sounds like your long goal is to have a unified statewide population in public health electronic reporting system for the whole state of Rhode Island. Is that where you're heading? It is absolutely where we're heading. And I believe, I have utmost confidence that we're going to be there, and I feel like we talk internally a lot about where we are today is light years behind where we're going to be in six months and it's light years ahead of where we were six months ago. I feel like the progress is really rapid, and I think that's in large part due to a high degree of interest. As coming full circle to the beginning of your question, the provider community is becoming more and more interested in these data because they are being held accountable for them. The Accountable Care Organization news releases that the cycle moving very rapidly. We've just seen in the last three weeks, three or four announcements in the news of payer provider arrangements that are outside of Medicare so that's pretty interesting. So, one quick final question. We look in the course at some of the public quality metrics that the Indiana Health Information Exchange posts through their Quality Health First Initiative. Are you doing that sort of thing here, and what's the reaction from providers to that idea if you are doing it? So, I think that we have not been so public yet. I think that too is a progression coming back to really needing provider buy in, and so we're presenting that data to providers on their groups. Not at the provider level yet even. And that's a real challenge with regard to N anyway, but we are presenting that to the group of practices that are participating in the patient-centered medical home initiative, where they can actually see their performance relative to their peers in other healthcare settings. But that's what reported publicly at this point is the aggregate of all of those programs relative to the non-patients center medical home data that's available in the state. So, it's a process. Well thank you very much for taking the time to be with us. My pleasure. So now I'm talking with Charles Hewitt who's the Director of HIE Program Management here at Rhode Island Quality Institute and Current Care. Thank you very much for taking time to be with us today. You're welcome. I'm glad to be here. We're mostly going to talk about direct and the direct services you provide, but before we do that, as the student's are going to learn in the next lesson, patient privacy is always a concern when health data is being utilized. Absolutely. And can you briefly tell us what patient privacy model you use? Students are going to learn about the various models later on. Sure. In Rhode Island, is a so called opt-in state. In our case, the opt-in is very strict. As an HIE, we cannot even see your data until you have given consent for us to see it, to collect it. When you do give your consent, in the Rhode Island model, you give consent for all of your data, including sensitive data like substance abuse treatment, for example. You know also when you give consent, you're consenting to who can see your data. And you basically have three choices. The first choice is anyone that's treating me, providing treatment or coordination of care. The second choice is only in an emergency, only in an emergency will I allow anybody to see my data. And the third choice is just for these providers. And you actually list them on the form. Well that's interesting. Do you know off the top of your head what percentage of patients fall into each bucket? Absolutely. About a little over 95%, 96%, that range, select the first option, all of my providers. About 2% to 3% select emergencies only, and about half of 1% select the third option, just these providers. Just skipping ahead for a second, what's being avoided here, which we'll discuss a lot in a lesson later on in the course, is the issue of allowing patients to select which data they want to share, which is very hard to do. Absolutely, and in fact it's a very challenging topic, this so-called segmentation. In Rhode Island, the law is all of your data. And that was basically the result of a compromise between the consumer advocates that wanted to be able to hold certain data back. And physicians that said, I need it all in order to provide effective treatment. Well, data segmentation will be a topic we look at in some detail, later. The second thing, and really, the primary focus, I hope, is for you to give us more detail. Laura and I talked briefly about direct, and the idea that it's used as a service. But can you give us more detail about the sorts of things you're doing with Direct. Sure, we're doing two things with Direct. One is on the inbound side, where we are collecting data from data-sharing partners. We use Direct to get feeds of continuity care documents, CCDs. Students know what those are. From the instance of the EHR that they happen to be using. So if they're using, for example, Allscripts, we worked with Allscripts so that Allscripts would generate a proper CCD, attach it to a direct message, and send it to the current care gateway where, it is checked to see whether that patient has consented or not, and if the patient is consented, it flows on into current care. Otherwise, it's blocked at the gateway. And I understood from chatting with Laura that that process is fairly automated. The provider doesn't have to do that every time. Very much so. In fact, it's automatic. Once it's set up, when the provider takes a certain action like lockup their record after an encounter. The system automatically does all this process of creating a CCD and sending it off to CurrentCare. It's all completely invisible to the practice. I also know from talking to Laura that you have a fairly large number of EHR systems implemented here in Rhode Island. Were the vendors pretty amenable to doing this? I would say some were, some weren't. [LAUGH] Part of the problem is that the vendors have a lot of priorities especially meaningful use. And so implementing this feed for the Rhode Island HIE, in many cases, has been way down the list. And we have to basically encourage the practices that want to send us data to go back to their EHR vendor and tell them. I need you to do this for me. So what about some of the services, go the other way. You've got the data now. I've got the data now. Now on the outbound side, the major service we provide using Direct are alerts. There are three kinds of alerts. One is a so called hospital alert. In that case, we receive ADT transaction from a hospital. You've been admitted say, to the ER. And in that transaction has been entered the identification of your primary care physician. We look up that primary care physician, and if they're subscribing to alerts, then we create a notification message and it's a very simple email type message. Very little information in the message itself. And send that off to the practice. Typically, that message is received in an inbox that's being monitored by a nurse case manager, and that person will decide what to do with the message. There's a link in the message to the CurrentCare viewer. So if they want, they can go in and review what the ADT was in detail, along with all the other information that's in CurrentCare. What is the adoption of these alerts? Right now we have, here in Rhode Island and we're a small state. Yes. But we have 150 practices that have subscribed and they represent about 400 to 500 subscriber providers, individual people. And we generate about between 5 and 6,000 alerts per month to that group. Well, this has been fascinating. Thank you very much for taking the time to do this. You're welcome, Mark. Nice to speak with you. Well Laura we've heard all about current care from you and others here at the Rhode Island Quality Institute, but I want to come back to you and let's look out in the future. What is the future of current care and of HIE and is there anything that you really want our students to take away from this interview? I think that HIEs have focused historically on providing information to physicians, hospitals, providers, long term care and so forth. By the time they see this, we're going to be turning the power of the HIE toward consumers and people. We know from McGuinness' research that in terms of the greatest predictors of morbidity and mortality, our health care system is only 10%. We're 85% of where the money is spent but it's only 10% of the impact. More and more care is migrating out of the hospital, out of primary care doctors offices into the community and into the home. So, we are busy building an opportunity for patients to look directly into their current care record, to upload their own information, be it biometrics that they're wearing an embedded to manage their blood pressure. Or their measure their glucose or their Fitbit, or whatever device they're wearing. We know that when we go to Appapalooza events, the inventors get up and say we have the coolest app since sliced bread, and the greatest thing about it, we're going to be able to send your data directly to your doctors. We know the doctors in the audience are going [SOUND] I can't take in data from every device. We can take in data, we can organize it and we can use direct to alert physicians when something is going on in the home that they need to know about. Where on, if they act upon that, then they can prevent an admission, they can prevent a patient with congestive heart failure picks up seven pounds in two days, we'll alert your care manager. That patient will home with their family on Friday night and not drowning in their own fluids in an ED. So I think it's that idea of beginning to look at the quantified self, mobile health, and how HIEs will begin to integrate that information into that record so it is all one comprehensive record. Problem with a lot of mobile health right now is it definitely collects your data and it sends it up to your mobile health platform, not integrated with anything else. So I think it's that, and the idea that we will use direct in the future because while I may want my 70-year-old mother to use the current care platform, my guess is that she won't be using that. I'll want a proxy message to come to me. And so I will want an alert from our hospitals that tell me, your mom just got admitted to an ED, because my mom lives in Colorado and I live in Massachusetts and Rhode Island. So, I'm going to want an alert. So it sounds like any physicians out there watching this interview and worried about meeting the VDT requirements of meaningful use stage two, should just pack their bags and move to Rhode Island. You're going to do it for them. We'd love it. Because with our portal we do have the view download and transmit capability. And so we're, give the ability for someone to get their complete longitudinal record and current care in the PDF form. They can put it on a USB drive, they can email it to their daughter, whatever. Well, thank you so much. My pleasure. I appreciate your allowing us to come up today, talk to you and some of your stuff. Thank you. It's been a pleasure. Thank you. Now that you're familiar with Direct, it's time for you to get involved. Later on we'll learn about personal health records. For now, I want you to go to HealthVault.com, the free public personal health record from Microsoft and create an account. Later on, I'll ask you to use that account in some other activities, but for now create it by putting in your demographic information and you'll be given a direct email address. Click on the question mark next to it and produce a letter for your doctor explaining how they can use it. The letter will refer to the CCDA, CCD, and CCR, three alternate formats for an electronic patients summary an EMR can produce is part of meaningful use. You'll learn about them in lesson five B. Submit that letter for credit for this activity. Next we'll speak with Dr. David Kibbe, president and CEO of Direct Trust, and a senior adviser to the American Academy of Family Physicians, the largest primary care physician organization in the United States. So good morning, David, and thank you very much for taking the time to be with us today. Good morning, Mark. It's a pleasure to be here. So at this point, the students are aware of Direct Trust and the role it plays in establishing trust across HISPs. But, of course, with health IT, adoption is always an issue. So, where is Direct Trust in terms of adoption at this point? Yeah, well, we've made a lot of progress, Mark, in the last year. Your students are I'm sure aware of direct as a protocol in a set of specifications. They're probably not aware of the extent to which direct service providers have been active in this sector of the industry. DirectTrust membership has grown from about 50 members this time last year to over 135 members. We now have 33 health ISPs or HISPs who are providing services to a range of customers and clients, including electronic health records. And of course the electronic health records are provided direct to their customers, who are provider organizations and individuals in those organizations. We also have HIEs, some federal agencies now, and a growing number of personal health records who are involved. So, right now that network serves approximately 6,000 to 7,000 healthcare organizations. There are over 250,000 direct addresses that have been provisioned for those parties in those organizations and those numbers have been doubling every three months. So we think that by the beginning of 2015, there will be approximately 200 electronic health records, depending on Direct Trust accredited service providers, and approximately a million direct addresses being supplied within the provider community nationwide. That's certainly impressive. In health IT right up there with adoption is use cases. The purposes to which technology is used to approve care. You've told me about a very interesting example in hospitals and transitions of care where patients are leaving the hospital and going to a long-term care facility. Could you tell us a bit about that? Yeah. Certainly. A couple things to keep in mind is that we're clearly seeing the network effect her. The hockey stick curve of adoption is growing very, very quickly, but we're also seeing that there's a long tail here in terms of use cases. The primary use case that's driving the adoption of Direct exchange right now is of course stage two meaningful use. As hospitals and medical practices seek to meet the transitions of care and patient engagement objectives in stage two meaningful use which require direct exchange, hospitals are probably the most active now in terms of using direct exchange to send referral documents, in particular transitions of care clinical summary, both to provider organizations on discharge of the patients or just before, but also to skilled nursing facilities. So we have a growing number of skilled nursing facilities and rehab centers that are, strictly speaking, not meaningful users, but they are endpoints for meaningful users and are starting to see the need for them to have direct exchange capabilities. I would add to that that claims attachment is a growing use case, again not really of meaningful use, and the federal agencies have referral needs to replace fax and e-fax mail with direct exchange, and that's also growing quickly. Another interesting example, I believe it was in Tennessee and Kentucky, is the use of direct by health departments to collect very patients specific data about those patients with sexually transmitted diseases. Can you tell us a bit about that? Yeah, that's a really exciting grassroots use case, and we're seeing several like this around the country, in rural areas of the country. This involves public health clinics in two states, Kentucky and Tennessee, using direct exchange for patients who they refer to one another or see in common and those are patients with STDs as you mentioned. They are also involving an army base, Fort Campbell, that has its own clinic. And just been contacted by the department of defense, we have a meeting next week to talk about how they can provide direct services at their installations within the coming months. Well, so, the use cases we've talked about so far are provider to provider. The students are familiar with view, download, and transmit, the VDT requirement meaningful use, too. That would seem like a role for direct. What can you tell us about that? The use of direct exchange by providers and patients to communicate protected health information is an enormous potential. We have about 15 companies in Direct Trust who are what I call next generation personal health records. These are personal health record companies like Healthy Circles that are contemplating or actually implementing now, direct services for their customers who are primarily patients and consumers, not providers. So that could grow very quickly but it requires first that the provider to provider infrastructure be in place. I don't expect to see large scale use of Direct exchange in the provider, excuse me, in the consumer space until 2015. Because if you want to communicate with your provider using Direct, which is a wonderful idea. It's secure messaging with attachments. Or you want devices to send direct messages to your provider organization. The provider organization has to be capable of receiving those messages and attachments first. And we're very much still in that building phase on the provider side. David, you mentioned something, that I hadn't really thought about before. Are the device manufacturers, the companies that are hoping to put devices into patient's homes so that the patients with chronic disease, for example, can be monitored more continuously, are they seeing direct as a technology they want to build into these devices? Yes, without question. There's one of the things that gives me real confidence that Direct is a disruptive innovation is this effect you're starting to see where of course we're talking about S/MIME over SMTP, email messages with attachments. But there's a lot of innovation occurring around the use of that very simple technology to replace interfaces of various kinds, including device to device interactions, or device to server Interactions and connections. That innovation is sort of quiet at this time, there's not a lot of noise around it. But there's significant interest in the part of device manufacturers and also the payers for example who believe that Direct can be a last mile from the device or from the data center to some end point, which could be the provider's inbox. Well, one final question. Of course there are always challenges, it sounds like exciting times for Direct and Direct Trust, but what do you see is the most significant challenges that have to be over come in the next few years for Direct to really achieve its potential. You know it, it always goes back to some payment or business rationale. Right now, the incentives for provider organizations to exchange information with one another easily and affordably using standards like Direct is not strong. It's growing with the movement of ACO's, but I think we still have a hesitancy on the part of providers to move data when they're getting paid for duplicating care, for example. I also think that we have to overcome the stigma of patients asking their providers and the provider organization for that information. There still is a hesitancy on the part of many institutions to make it easy for patients to get their information. Having standard in place, of course, helps. But it doesn't motivate people. Standards are not motivators. Standards are enablers. Well David, I thank you again. I particularly thank you for your final comments, which just emphasize again something I tried to get across in this course which is that without the right incentives, all the technologies in the world won't change healthcare. I would agree with that statement very strongly, Mark. Thank you for the opportunity to speak with your audience, with your class. And if there are individuals who want to contact me or learn more about direct trust, our website is directtrust.org. Health information exchange is essential for care coordination, managing populations of patients, public health and aggregating data for research. However, there are still many challenges that have proven hard to overcome, despite years of effort. Perhaps most importantly, it's difficult to establish a sustainable business model beyond the confines of a healthcare enterprise, which has its own proprietary business reasons for supporting an HIE. There's also the inevitable trade-off between functionality and cost. Centralized models like IHIE deal with data interoperability issues and can provide extensive services, but at a high cost. New virtually free models, like Direct, are not yet providing services nearly as rich as IHIE, but we've discussed how that might be changing. This was the first in the series of lessons on core HIT technologies. They're clearly interdependent. In discussing HIE, we've been drawn to the issues of privacy, security, and trust that we'll discuss in more detail in the next lesson. I'm sure you're curious about the details of what got sent from Dr. Johnson to Dr. Jones. And you'll learn about that in lessons 5A and 5B. By now it should be clear that successful management of chronic diseases in our highly specialized healthcare system requires digital data that can be shared as needed to achieve coordinated care. However, patient-specific healthcare data is highly sensitive, and is protected by a law called HIPAA, which calls for severe penalties, including prison time, for failure to properly secure what is called Protected Health Information or PHI. PHI is healthcare data that is linked to information that identifies the patient. You will hear the term de-identified data which refers to the same health information once it has been separated from any fields that could directly or indirectly be used to identify the patient. Everyone in healthcare is concerned about HIPAA. So the topic of this lesson are the critical technologies that secure PHI to make sure it is only shared for legitimate purposes and even then, the sharing has been approved by the patient or their legal designee. This sharing is not just with health providers. Patient engagement is the key strategy for preventing and managing diseases. Patients in 2010 survey by the California Health Foundation reported that access to their data improved their care. Those using a specific tool, a personal health record, reported they knew more about their health. They asked their physician a question they would not otherwise have asked. They felt more connected to their physician. And they actually did something to improve their health. However, the survey also showed that patient adoption is still low with only 1 in 14 having used a PHR. And adopters are predominantly young, highly educated, higher income patients. A big part of the reason seems to be patient concerns about privacy and security of their health data. 68% of the respondents said they are very or somewhat concerned about misuse of their digital health data. Keep in mind that PHRs are cloud-based web tools. So, it's likely that any well publicized problems with data misuse on the Internet translates into increased patient concerns about health data being stored there. At least partially as a result, a 2012 survey showed that only 26% of patients want their medical records to be digital, with 85% expressing privacy and security concerns. Protecting sensitive health data divides into three sub areas. Privacy in health care means that only individuals or entities authorized by the patient may have access to their data. So a patient's record may be stored digitally in the EHR of a hospital, but that doesn't mean anyone in the hospital can access it. It can only be accessed by people authorized for specific purposes. Security is preventing unauthorized access to the data by outsiders such as hackers. This is a particular concern where data is being shared. Just as with financial records, health data has value and must be protected at all times and in all places where it resides. Trust means knowing that the individual or entity with whom is being shared is who they say they are. A famous person is admitted to a hospital and a curious employee who is not involved in their care accesses their record. This is a violation of which of these? Check any and all that apply. The answer is privacy. The person accessing the data was not authorized to access it, even though they're in the hospital. A hospital employee brings a USB to work and inserts it in their computer in order to play a game. Malware on the stick now allows a hacker outside the hospital to access patient records. This is a violation of which of the following? Again, check any and all that apply. The answer is security. A failure to protect the data from entities outside of the health care organization. A physician is tricked into sending a digital health record to a hacker who the physician erroneously believes is a specialist for referring the patient. This is a violation of which of the following? Check any and all that apply. The answer here is Trust, not knowing for sure that the person to whom health data's being sent, or with whom health data is being shared, is who they say they are. Assuring privacy, and security and trust is a necessity for digital patient records and sharing these records over a health information exchange. As a result, it's an ONC Standards and Interoperability Framework workgroup priority area. We’ll be looking at many S&I workgroups as we proceed. So it's a good idea to first look at how they operate. The process is intended to be grounded in real-world issues, called use cases, that are first identified through an Environmental Scan. And then molded into a consensus document which drives the development of standards. Here's the workflow that surrounds the use case of a provider ordering a lab test, and the return of preliminary, final, and then corrected results back to the ordering provider. Privacy begins with the patient. Only they or their designees can consent to the use of their health information. There are a number of models for obtaining this consent. Using data without asking for consent is the No Consent model, but this isn't reasonable or even legal in most contexts. In the Opt-out Model, Consent is assumed, unless the patient specifically revokes it. Some HIEs are using that Model, but it would be very unlikely and probably not even legal for a healthcare provider to use it. The converse is the Opt-in Model, where no data is shared unless the patient consents. If you've ever visited a physician, you've probably consented to the use of your data under this Model. CurrentCare, the Rhode Island HIE we'll discuss later, uses this Model. Typically Opt-in is all or none. So patients can't share some but not all of their data. Despite this inflexibility, at least one study suggests as many as 90% of patients will consent under this Model. The final Model, Opt-in with patient specified restrictions, provides more flexibility and would probably be the preferred method if it can be managed. You probably agree that opt-in with patient specified restrictions is the most attractive privacy consent model. But how do you actually manage it? How do patients indicate what data they want to share or not share? Providing a mechanism is the data segmentation problem. Suppose for example a patient had hypertension and depression among their medical problems. Their hypertension is not well controlled and they have undesirable side effects from the drugs prescribed to treat it. So they're interested in joining a clinical trial for a new and promising medication. As a result, they want to share the hypertension part of their data to apply for the trial. But they are sensitive about their depression and wish to restrict access to that data. How do they specify that? The solution is easier if someone with clinical expertise and their trust, such as their physician, does it for them. But suppose the patient has their record in a PHR and wants to apply for the trial themselves. Presented with a list of their own medications, they may not recognize the names or know for sure which ones are for what condition. That form of sharing would be greatly facilitated if their PHR or EMR understood these clinical relationships. Here's a simple depiction of just such a record for our hypothetical patient from Dr. Jonathan Nebeker at the University of Utah in the Utah VA. Given this underlying representation of relationships, the patients could simply click on hypertension and see the related medications with the others greyed out, greatly facilitating the opt-in with patient-specified restrictions model. As with privacy consent, there are several data segmentation models. A completely patient controlled model is the norm for PHRs, where there is no obvious alternative other than a friend or family member who may be more familiar with medical terminology than the patient. We mentioned a Provider Assisted Model, but this does require that busy providers spend time on the task. And even if they are willing, this may not work well if the patient's data is, as is often the case, spread among a number of providers. There are other approaches, including Organization Controlled Models, Hybrid Models and Innovative Tools. That are typically aimed at providing patients with more support. All of these ultimately try to simplify the complexity in the underlying clinical data, which in turn relies on better structures than are typically found in today's EHRs. To get a hands on feel for data segmentation, go to the Microsoft HealthVault account you established and set your own information sharing preferences. For credit, email an invitation to an address we'll provide. Now that you've seen it, what is your opinion of HealthVault's approach to data segmentation? My answer is it seems very complex for a patient to deal with. There are over 80 categories of information, and some of their names, for example blood oxygen saturation, may not be clear to someone without a medical background. It seems to validate the idea that patients should be able to just pick a condition they have to share all related information. Can you think of an innovative approach to data segmentation for a patient at home who only has their PHR and the Internet to work with? My answer as I said earlier is the PHR presents the patient's problems. They select one, and it shows all the related information. They can then select or deselect those items they want to share. Security and trust are interrelated. Particularly with respect to the technology that is most commonly used to assure them, Public Key Infrastructure, or PKI. PKI can be confusing. For our purposes, you need to master three concepts, public and private keys, the difference between the message, the sensitive information and the digital signature derived from it and attached to it. And the function of two key organizations. The first organization confirms the identity of the people or entities involved in the exchange of information. This is the Registration Authority, or RA. Functionally separate, although one organization could do both is actually signing and issuing additional certificates that are bound to that identity. This is the Certificate Authority, or CA. We'll discuss each of this in some detail beginning with public and private keys. They're used together to facilitate the secure transfer of information and for other purposes. You can think of them simply as two numbers that are mathematically related but in a very complex way. One of them is public and is freely available. The other is private and must be secured, accessible only to its owner. Given the freely available public key, it is prohibitively expensive and time consuming to calculate the matching private key. PKI rests on that core assumption. If you've ever done banking, paid bills or purchased anything on the Internet, you've used these keys. Earlier in lesson three, we referred to PKI indirect. This is another highly simplified representation of direct designed specifically to show the dual function these keys play in security and trust. Along the top, you see that the message attachment, which contains protected information, is encrypted using the receiver's public key. This is a critical point. Anyone can discover their intended recipient's public key and can use it to secure information they wish to send to the owner of that key. Since only the owner should have the matching private key, only they can decrypt and view the message. That's the essence of PKI for security. Now here along the bottom a digital signal is created via a calculation performed on the data being sent. So it's unique to that message. It can then be used for two purposes, to validate that the message was unaltered in transit, and that it actually came from the purported sender. Remember, the attachment was encrypted using the recipient's public key. The signature is encrypted using the sender's private key. The recipient's public key for the message, sender's private key for the signature. Since anyone can obtain the sender's public key, anyone can decrypt the signature. Here's where people get confused. Keep two things in mind. The digital signature is not the message. It is derived from the message. So once decrypted, it can be used to assure that the message can be trusted and that it wasn't altered in transit. The fact that it can be decrypted using the sender's public key ensures that it was encrypted by the sender using their private key, thus validating the source of the message. Nowhere was the sender's private key or protected health information compromised. That's the essence of PKI for trust. Most people who aren't already familiar with PKI find this confusing, so here are some questions to make sure you're clear. To establish the source of a secure message the recipient needs their public key, their private key, the sender's private key, the sender's public key. And answer is the sender's public key. Remember, the digital signature, which assures that the message came from the source it purports to come from was encrypted using the sender's private key. To assure that the sender's message won't be read by anyone along its way to the intended recipient, the sender needs, their public key, their private key, the recipient's private key, or the recipient's public key? The answer is the recipient's public key, since only the recipient has the matching private key, only they can read the message. So you now know how the recipient can trust that the message actually came unaltered from its sender, but how do the sender and recipient each know that the other is who they say they are? Could a hacker be masquerading as a doctor in order to get access to health data and use it to file fraudulent claims? Assuring trust to prevent this is the job of the Registration Authority. Their job isn't particularly technically complex. It's driven by policies intended to assure that the background of people or entities is thoroughly verified before they're issued a pair of certificates. Once a registration authority has verified identity, certificates can be issued. That's the job of the certificate authority. They issue the certificates and typically encrypt public keys using their private keys. Can you think about what that does? Hint, we just discussed a similar concept under trust. Why does the certificate authority encrypt public keys with their private keys? The answer is, so people know and can trust that the certificate was actually issued by them. As we said earlier, you've probably used PKI on the web. An example is Google Mail. This also helps explain the value of the certificate authority. We're using Mozilla Firefox here, but you can, and should do the same with any browser. This lock icon, adjacent to the URL of the website, indicates that it's secure, using HTTPS. Clicking on it, as we see here, indicates the site name www.google.com, and that the connection is encrypted. Clicking here, brings up more information, including the name of the certificate authority. In this case, Thawte Consulting. This would be the entity that issued the certificate. You already know that browser can verify that by using the certificate authorities public key. So browsers are normally shipped with those keys to use for this purpose. Remember, it is a public key, so anyone can have it. Finally, clicking here, brings up Google's public key, showing that it really is public. The text provides more details about the key, and their references for the more technically inclined students, who want to know even more. What certificate authority issued the PKI keys to PayPal? Using what you've learned, you should have been able to verify that that is in fact, VeriSign. When does the current PayPal certificate expire each year? Again, using what you've learned, you should've been able to verify that it's April 2nd. What are the first six numbers in PayPal's public key? Once again, using what you've learned, you should be able to verify that it's 30 82 01. As we said earlier, PKI can be used for many other purposes than we've mentioned so far. These include securing software until you've entered the vendor's code indicating you've paid for it, securing wireless networks, digital rights management for movies and music. In reality, PKI is more complex than described here. The validation of the certificate should be more robust, something called extended validation. To reduce the computational load, there are so called hybrid approaches that create keys just for a specific session. But this is probably not sufficiently secure for health care. Finally, there is the concern about advances in computing. We've assumed that given the Public Key, it is too expensive and time-consuming to calculate the private key. Proposed quantum computers might change this. And could lead to the need to make major modifications for encryption to remain useful. For digital health data to help coordinate care, in our highly specialized and fragmented system, it must be shared. To support population health management, public health, and research, it must be aggregated for analysis. Whenever it is moving, there are inevitable concerns about respecting the patient's privacy preferences, securing it from unauthorized disclosure, and knowing that the persons or entities who send it and with whom it is being shared, are who they claimed to be. In this lesson, we've gained a basic understanding of the technologies and policies required to achieve all three of these important objectives. We've not yet dealt with a question that is increasingly being asked. Have we gone too far with this? Are the complexities we've created in the interest of privacy security, and trust now standing in the way of other important objectives? For that we talk with Don Detmer, one of the best known people in health informatics, and a member of the committee that wrote the landmark IOM report, we discussed in lesson one. Don, I want to start off by thanking you for taking the time to be with us today. I know the students are going to learn an awful lot by hearing from you, and I know you're very busy so I really appreciate it. My pleasure. I wish we could share a glass of wine at the same time, but anyway, we'll proceed. Well if we can figure out how to do that you and I will probably get very rich. Well, we've got a lot of wine up here in Charlotteville, so I always try to help the locals. Yes you do. So, Don you've been a thought leader in informatics for years. You've chaired most of the important IOM Reports that I've alluded to throughout the course, so you have a unique perspective. Where do you think we are today on transforming our health care system, particularly through the use of health IT? And what are your hopes for the future? Well, it's really been exciting to have been involved in this. I guess the only thing that's frustrating is that I'm not getting younger because I think it still has to play out. But we are really I think making terrific progress. When you consider that the 1991 report on a computer based patient record was called an essential technology for health care and that it took until just 2009 for the high tech legislation to really make this a national kind of development with meaningful use and so forth. So clearly, these have been really momentous times, and it's exciting to me to see that we are way past the early majority. We're into the late majority of people now doing this both in hospitals and practices. So having said that, I don't know if you saw, I can send you the reference. But I had a paper with a couple colleagues this years. The way it was finally broken, where are we now, that spoke to some of the issues. And there are a lot of issues out there. There's not the interoperability that we need. There's not the functionality that we need from the point of view of even physicians, let alone all the other members of the health team. So it's a glass half full, half empty kind of thing. But actually I think it's really in a good spot. Some of where we are though is, of course, we mostly have been using health IT in terms of say computer technology. And I think increasingly now we're using communications technology. And all these things are new, from the time the study was done and it moved forward. So, we're always going to be in the situation of trying to both keep up with where the technology and opportunities go. But a lot of progress has been made, and I think what I'm excited about, to talk about also, is where some of the pinch points are and where we can go next. Well, I will certainly post that paper for the students to read. As we look at a health care system that is increasingly using digital records instead of paper, the research community is very excited about the prospect of having access to huge amounts of digital data to study it, to help improve care, gain new knowledge, and maybe even improve the healthcare system itself. But I don't have to tell you, getting to that data is often a very difficult, time-consuming process, mostly because of concerns about patient privacy. Some people feel maybe the pendulum has swung too far on that, and we're actually harming our ability to use digital data as a result. What's your view? Well, in fact, I am one of those people who believe that. Some of the issues are not necessarily the HIPAA legislation per se. But clearly I think HHS has not in fact creatively tried to manage to this, to balance these competitive goods. Privacy's a good thing. But too much privacy and obviously you can't have knowledge, you can't run a government, you can't run research and so forth. And I think that the principle problem with HIPPA was that it was built in an era, and it was all pre-Internet. So the fact that you could do data analytics and do translational bioinfomatics and the human genome and proteomics and epigenetics with epidemiologic data as well. Let alone using it to improve health care systems with clinical alerts and even community health approaches using public health dimensions to this. All of that was well before HIPAA. So, candidly, I've been spending hours up on the hill with energy and commerce staff to try to see if we can, as part of their initiative to improve, really cures is where they're wanting to go, but care as well. See how we can hopefully make some reforms in these things because right now it's not only a problem for research it's also a problem for education too. I hear issues where pharmacy students are interested in trying to learn how to do data mining and looking at drug use and so forth can't get to the record data at some schools for some various reasons. As I say, the issue is not only the legislation which was built for a different era. Mostly clinical trials where the patient was at risk, to secure use of these data for data analytics where, honestly, the risks are extremely small. And I think, actually, if you look at hospitals around the country that are doing research, most patients actually overwhelmly want doctors to do, and other health professionals, to do this research, and in fact actually sort of assumed that we already were doing it. So, we really clearly have some issues here. And there've been cultural issues where not all scientists have eagerly wanted to share data, but I think the Clinical Translational Science Awards have really cracked into that. Plus, what has happened in the last 100 years is the human species has not only been very productive, but they've been very reproductive too. And as a result of that the human genome's gotten more complicated. So if we're going to really crack these things, which we can, we really need very big data sets. And you can't aggregate those, in fact, where you talk to a group of patients or patient advocates with one problem because a lot of these are multidimensional kinds of problems. In fact most of them are. They're not single, single chain or single issue, single dimensional issues. So anyway, tremendous opportunities, but also candidly we are leaving a huge amount of discovery on the table. Example, another quick example, HIPPA says you can use personal health data, identifiable data for doing quality studies in a hospital. But if you find something that really you'd like to share with your neighbors and colleagues and publish it, you gotta go all the way back to an IRB and start all over. Well, busy people may not want to mess with that now, and some big institutions, they spend a lot of money and extra effort to help facilitate people being able to deal with that. But, we can't afford to lose that wherever it is, smaller institutions or larger institutions. So, we've gotta work smarter. We can't work harder. I think the government's not going to throw more money at health care. That's, I think, becoming increasingly relevant for both research as well. So we've gotta get better data liquidity, as the term is called, and let it move around for care, for research, and then also ultimately then for population health. Well, you've mentioned patients a number of times. And I think most people would agree that patients are the ultimate decider when it comes to the use of their data. And a number of people have suggested that patients create their own data repositories that might be housed in a PHR or some other tool specifically for the purpose of mediating the sharing of their data. Do you think this is an inevitable development? Do you think it's a good development if it happens? Might that be the way to increase the accessibility of clinical data for research and other purposes? Well, if it were the only approach I'd be strongly against it mostly because that we know at least 25% of people really don't care if their data are used for anything. But they don't want to be bothered to be asked about it. So anytime you're carving out 25% of the population, and keep in mind the population may be split differently. For example, a number of years ago Minnesota passed a law that said you had to get people's permission to use their data, and it turned out Rochester was doing a lot of work also in breast cancer, particularly in younger women. Well, in Minnesota, it was very disproportionately young women who decided to opt out of having their data shared. Well as a result of that nobody could do research because their sample sizes weren't big enough. And obviously I think that's the challenge. It sounds kind of good from a patient autonomy point of view. But I don't think it's any way to get discovery in an era where big data where you really can try to get to the core of things and create real cures. Now, if you're talking about certain areas, I'm not saying that it's a uniformly bad idea, but if you only had one arrow you could shoot, frankly I think we need to decide that as a society, we care enough about each other that we're going to share our data to cure things so all of us can live longer and live better lives. And also, by the way, have a cheaper health care system, which we clearly need as well. So, I don't know if that's responsive, but that's a quick answer. No, that's a great answer. Well, Don, I think the students have really gotten some tremendous insights here from someone who's been obviously one of the great thinkers about this for many decades now, and I appreciate you sharing those insights with the students and taking the time with us. Thank you very much. Well you're very gracious. I enjoy when we can get together and obviously hope that it can happen more often in the future. If a particular student does have a question that they'd care to write to me about, I'm not saying how quickly I'd get back to them, but detmer@virginia.edu and I'd try to respond. And I'll also send a few references to you that might be potentially useful. All right, well thank you very much. Yeah, thank you. Have a good day. So we now know how health data is shared and how it is protected along the way. But what about the data itself? How is it represented? How is it packaged into documents? And how are those documents actually transmitted? For that, we now turn to lessons five A and B on data and interoperability standards. Data and interoperability standards are the virtually ubiquitous plumbing that underly all contemporary health informatic systems and tools. Given the complexity of health care, it should not be surprising that this is a complicated topic. In fact, we're dividing it into two lessons. In Lesson 5A, we'll focus on how health data is represented, Data Standards. In Lesson 5B on how it is packaged into documents, transported, and shared, Interoperability Standards. Why do we need data standards? This simple illustration helps to make that clear using a seemingly obvious data element: gender. It also helps explain the difference between syntax, which is structure, and semantics, which is meaning. Here, in system A, male is represented by a one, and female by a zero. Over here in system B, that's reversed, with zero representing male and one representing female. Should you mix data from these two systems without some intervening standard, gender would be impossible to determine accurately for reporting and other purposes. We say these two systems differs syntactically. They maybe using the same language of one's and zero's, but they can't interoperate without some intermediate translation process that maps one to the other, or both to some common syntax. Over in system C, M is used for male, and F for female. We say that system C differs semantically from systems A and B, it uses a different language to represent gender. Moreover, system C recognizes that gender may be ambiguous, and represent that with a U, a concept the other two systems don't deal with. Interoperability between system C and the other two systems would require translation from its syntax to theirs, or translations of both syntaxes to a common form, a standard. Since patients with gender U can't be represented in systems A and B, some accommodation for that would also have to be made. If something as seemingly simple and obvious as gender can lead to this much complexity, imagine what happens with concepts such as a patient's diagnosis, which is inherently somewhat subjective and can have thousands of possible values. That's why we need data standards. Diabetes Mellitus can be alternatively written as DM or just diabetes, and it can be coded using ICD as 250.00, or as 73211009 in SNOMED. What problem does this contribute to? Is it interoperability? Is it care coordination? Both? Or neither? The answer is both. This is an example of the inconsistency in the way clinical data is represented. It makes it difficult to aggregate data and communicate data among what we refer to as non-interoperable EHRs. And as a result of that, it makes it more challenging to coordinate care. Standards have evolved over many years to encompass more aspects of medicine, to cover them in more detail, and to adapt as technology changes. This can be divided into three dimensions, structure, purpose, and technology. Early data standards were a list, such as medical diagnoses, laboratory tests, or medications. We refer to standards, that is such a list as a classification. More recently, attention has been paid to describing more detail and even relationships among entities within a standard. We'll refer to a standard that can code for relationships as an ontology. Precomputing, all early standards were for data. Physicians would use ICD, a classification of diseases, in their notes. The clinical laboratory would have a classification for their tests. And the pharmacy would have a classification for the medications they dispense. As computers came into use in hospitals, the various departments needed to share information, but were using their own specialized computer systems. This led to early messaging standards. These meant a physician could order a lab test or medication at the nurse's station, and that order could be routed electronically to the appropriate department to do the test or send the medication. The next evolution was standards for clinical documents. A message might typically be an order for a lab test or medication, but what if you want a complete summary of a patient's care to send to the patient's physician once they're discharged from the hospital? This is the role of document standards. More recently, as computers have become more powerful, standards have been evolving to represent clinical workflows and processes. These have often been targeted at hospitals, but have proven to be hard to develop and implement. If this could be overcome, they could, at least in theory, lead to a reduction in unnecessary variations in the way patients are cared for. So far we've discussed the structure and purpose of standards. The final dimension is technology. And this is particularly applicable to messaging standards, where the messages must constructed from data standards in a manner that can be understood by the systems that receive them. We just discussed why. Departments in a hospital that want to share information with each other, so for example, an order can flow from the physician caring for the patient to the lab, and the result of the test can come back. Messaging standards serve that purpose, but now we're looking at the technology in which those standards are implemented and formatted. Early messaging standards were created using EDI/X12, a standard that evolved in other industries to automate business processes such as ordering, invoicing, and payment. EDI/X12 was developed in the early days of computing when memory and storage were still dear. So it is quite cryptic and compact as you can see here. Note that with some effort, we can tell that this involves a clinical lab and a glucose test. But most of the details are not obvious. In fact, to understand each specific field, typically requires a reference guide. More recently, messaging and document standards have been developed using XML, a modern syntax that is verbose, but has the advantages of being more human readable and easily rendered in a browser. This is the same lab test result we just looked at. While it's still not easy to read, we can much more easily tell that it was a blood glucose level, that its value was 182. It says value right there. And we can see the normal ranges for the test are 70 to 105. Data standards are the most widely used health standards and are a topic that is far too complex to completely cover in detail here. I'll provide an overview, and you should read the text and look at the many resources on the Internet for more details. The five key data standards are, the International Classification of Diseases, ICD. Current Procedural Terminology, CPT. Logical Observation Identifiers Names and Codes, LOINC. National Drug Code NDC. And Systematized Nomenclature for Medicine, or SNOMED. ICD and CPT are very widely used because they're required in most cases for medical billing. CPT and NDC are pretty much US specific. ICD, LOINC, and SNOMED are internationally used ontologies, capable of representing clinical relationships among their elements. NDC and CPT are classifications, although CPT increasingly has sub-codes to provide more details about a procedure for use in billing. Which of these would be most closely associated with a prescription a patient would take to a pharmacy? ICD, CPT, NDC, or LOINC? The answer is NDC, the National Drug Code, a classification of medications. Which of these would be most likely found on a claim for payment for a physician office visit? Choose any and all that apply. ICD, CPT, or NDC? The answer is ICD and CPT. NDC, codes for medications, would be found on a claim for payment for that medication, but ICD and CPT would be used to obtain payment for physician offices. The ICD states what problem was seen. The CPT states what was done. The International Classification of Disease is the oldest data standard dating back to the 1800s. And in a sense, even to earlier centuries when researchers first became interested in the causes of human mortality. Traditionally, it's been a list or classification of medical diagnoses. It's maintained by the World Health Organization, and is updated every ten years, with the latest version being ICD-10, which was adopted in 1994. Here in the US, where still using ICD-9, even though most other countries, are using 10. ICD-10 is a major expansion capable of representing many more clinical details. We can see that, here, by comparing the coding systems' capabilities for breast cancer. In ICD-9, we know what portion of the breast is involved. The central portion, here, but we don't know which breast. In ICD-10, we not only know which portion of the breast is involved, but we know whether it's the right or the left breast. So in ICD-10, laterality is represented, which greatly increases the number of codes, which is a big part of the objections to using it here in the U.S.. In fact, ICD-10 really becomes an ontology capable of representing clinical relationships. Here, we can see that the patient has gout, affecting their left shoulder. But they have not yet developed a uric acid deposit, called a tophus, in that shoulder. We can also see that the etiology, or the reason for their gout, is kidney failure. The current U.S. target date for ICD-10 adoption is October 1st, 2015. But given how close we are to ICD-11, there are some proposals to skip ICD-10 entirely. Current Procedural Terminology, CPT, is maintained and updated annually by the American Medical Association to classify all medical procedures. And it is required for virtually all billing and reimbursement. CPT codes are divided into three categories. Category I codes are widely performed procedures. And are 5 digits long divided into sections for anesthesiology, surgery, radiology, pathology, laboratory medicine, and medicine. Category II codes are for the collection of quality and performance metrics and are 4 digits. Category III codes are for new or experimental procedures and are also 4 digits. For each code there are full, medium, and short names or descriptions, as you can see here for flu vaccine, influenza vaccine. These are used for different purposes and provide varied levels of detail as needed. Given it's used for billing, a CPT code may need to provide details necessary to determine the proper charge. Here, we see a series of codes for a psychiatry visit. Different codes are used to indicate the visit length. The charge would be more for longer visits. These codes are used to indicate how much dead, damaged or infected tissue has been removed to promote healing. Removing more results in a larger charge. CPT codes are simple but given this critical role in billing and subtleties such as these, selection of the right code is important and billing personnel are extensively trained to code correctly. Typically, to ensure that the largest, but hopefully legitimate, bill is submitted. An investigational drug not yet approved by the FDA for general use, might have which of these CPT codes? Is the answer 12345, 1234T, 1234F, or 1234X? Which of these CPT codes might be used to track whether a physician has screened his patients for smoking? The answer is 1234F, remember that CPT codes ending in an F are used for quality and performance metrics. LOINC was developed and is maintained by the Regenstrief Institute in Indiana, but is used virtually worldwide. Each code is for a laboratory test or a clinical observation and is a number with up to seven digits. While the codes themselves are deceptively simple, they contain a lot of information detailed in their names which, as shown here, are divided into five or six main parts separated by colons. In this example of the first part the analyte, our substance of interest, is glucose. We can see that it was determined two hours after the patient consumed 100 grams of glucose because the first part of the name is divided into subparts, separated by a caret. Here there are two subparts, but there could be three. The first subpart, glucose, could also contain multiple levels of increasing specification separated by dots. The third and fourth parts, the time aspect, Pt, means a point in time rather than a time range. And the system sample, Ser/Plas, indicates that the test was performed on the serum or plasma components of a blood sample, can be modified by a separate subpart, again separated by a caret. This table is a more detailed explanation of each of the parts of the name. What does the Qn in the fifth (scale) subpart of a LOINC name indicate? Is it a number, a word from an ordered list or a word from a non-ordered list?. The answer is a number. Qn stands for quantity. The National Drug Code is a U.S. specific standard for medications maintained by the Food and Drug Administration. It consists of a simple 10-digit, three segment structure to indicate the Labeler or vendor, the Drug, and the Packaging. This systematized nomenclature of medicine SNOMED and SNOMED-CT, a subset for clinical medicine, began in NIH and was recently just for pathology. Interestingly, the original objective back in the 70s was machine coding of a pathologist's free text dictated note. It was always in ontology representing relationships among its concepts. Today it has expanded to all of medicine, and is maintained by the International Health Terminology Standards Development Organization. SNOMED is huge and complex. Even the SNOMED-CT subset has 311,000 concepts. And represents 1.3 million relationships among them. Concepts are a basic component of SNOMED-CT and have clinical meaning. They're identified by a unique nine digit numeric concept ID and a unique human-readable fully specified name. Concepts can be represented as a hierarchy based on level of detail as shown here where we go at least detail from the concept of a procedure at some site of the body all the way to a very specific procedure done at a very specific location. This illustration shows other basic components of SNOMED-CT. Relationship links, which are expressed using is a, and attribute relationships, which are a finding site or an associated morphology. So a fracture of the tarsal bone, is a sub type of a fracture of the foot. The tarsal bone is part of the foot, and has a finding site, and an associated morphology, the location of the fracture and the fact that it is in fact a fracture. The ability of SNOMED to represent medicine and medical concepts is illustrated here, where we see that the patient has a particular condition, Acute Post-Streptococcal Glomerulonephritis, a condition I actually had as a teenager. It's caused by group A Beta-hemolytic Streptococcus, which is an organism that affects a body structure, the kidneys, and it's a clinical finding or a medical problem. Using this hierarchy a computer could recognize these relationships. This is actually a simple example showing only three of the 18 top level components in the SNOMED hierarchy, Body Structure, Clinical Finding, and Organism. We've now had an overview of how standards have evolved over the years, in terms of their structure, purpose, and technology. You should think about the difference between simple classifications such as NDC, CPT, and ICD, prior to version 10, and a complex ontology such as snowman. The standards community has been divided for decades over the choice between perfection, standards that can represent medicine in all its detail, and practicality, standards that can actually be deployed and used in the real world. We'll look at this more specifically in lesson 6, when we review EMR design challenges. We'll be looking at messaging and document standards next. But both of these rest on the data standards we've just discussed. To see that, go to the URL in the instructor's notes, it will take you to the HealthVault Developer site to access a sample of a CCD, an XML formatted patient summary. Find the Problems section and use it to answer the questions that follow. Note that the problems are represented in three ways. First, in easily human-readable text, next in a table format that could readily be viewed as part of a web page, and third, the same problems are wrapped in a considerable amount of data standards nomenclature. Each of these begins with an entry type code and ends with /entry. Study what's between these to see how complex HL7 has become. After following the instructions for the CCD activity, answer this question. Which of the problems is still active for this patient? The answer is asthma. If you looked carefully, all of the other problems have been resolved or are not active. Which of these SNOMED-CT codes would tell a computer that your answer to the prior question is correct? Is it the first pair, the second pair, both of these or none of these? It's the first pair. This is a pair of SNOMED-CT concept ID codes. What subjective data is recorded about this patient's asthma? Its severity? Its frequency? Its cause? Its onset? The answer is its onset, which you can discover by examining the XML. So we know now how health data is represented. And we've had an introduction to how it's packaged into documents. We will explore that in more detail in the next lesson. In the last lesson we looked at the evolution of standards and examined data standards in particular. To be maximally useful in care coordination, the standardized data, typically along with other non-standardized data such as free-text notes, must be packaged into standard clinical documents and sent using standard message formats. We'll look at both of these in this lesson. Hospital health IT systems came into widespread use in the late 1970s and 80s. There were some early attempts to provide a single vendor fully integrated hospital wide solution, but most systems were developed for a particular department, such as pharmacy, clinical lab, or radiology. And it was common for each of these departments to select their own system. As a result, hospitals were faced with the problem of communicating among those systems. So, for example, patients could be uniformly registered and charges could be reliably collected. Health level seven, HL7, evolved to create these messaging standards. It used EDIX12 to format the messages. Its latest version, 3, migrates to XML, which, as we discussed earlier, is a technology developed for the internet in which tags and HTML formats are used to described data elements within web pages. While it is more verbose, memory and disk space are now inexpensive. It is more human readable, and of course, still machine readable, and it's easily rendered in a browser. Here's an example of an HL7 version two, EDI/X12 formatted message. A lab test result as it might be sent back to the ordering physician. The OBX line is the most interesting part since it contains the test results. Any idea what 1554-5 is? What about 182|mg/dl? Or 70_105? Here's the same message in XML. While it's still not as obvious as something designed just for human reading, it is much clearer that 1554-5 is a code LN is the abbreviation for loin, and 70 and 105 are the low and high values. And, at least by inference, 182 is the result for this patient. So H must mean the result is high. Over the years, HL7 has expanded greatly from messaging. Some would now argue that it's becoming overly complex and virtually impossible to fully implement. In what follows, I'll give you an overview of the many key components of HL7 with a particular emphasis on those that are key to data sharing for care coordination and patient engagement. Traditionally, the standards community often seeks standards for virtually everything. So it should not be surprising that there is an HL7 standard for developing HL7 standards. It's called the HL7 Development Framework, or HDF for short, and is diagrammatically shown here. For the purposes of this overview course, it is sufficient to know that it exists. And to be familiar with these linked circles showing the standards development process. Earlier we discussed syntax and semantics and said that syntax is structure while semantics is meaning. The HL7 Reference Implementation Model, or RIM, defines the semantics of a common set of administrative financial and clinical concepts in order to foster interoperability. It consists of five abstract concepts. Each happening is an Act. Examples include procedures, observations and medications. Acts are connected through Act Relationships. Participation defines the context for an Act author, performer, subject or location. The participants have Roles, such as patient, provider and so on. And Roles are played by Entities, which could be persons, organizations, material, places or even devices. Some of these are shown in this small example of an HL7 RIM example. As you can see, each of these is further specified in great detail, but finding the explanations can be tedious. The FHIR, F-H-I-R, Resources site is the best source of explanations I'm aware of, but it is intentionally not complete. To get a feel for this, let's focus on the special arrangements code in the patient encounter box. We see here that this is a leaf of the act box. Meaning, it's a particular example or instance. I picked it because most of the detailed names are self evident so you can easily see the level of detail that is specified. D set, indicates this is a discreet set of possible values and in fact, although they're not shown here. These are wheel for wheelchair. S-T-R-E-T for stretcher. I-N-T for interpreter. A-T-T for attendant and D-O-G for guide dog. All examples of things that would be specially arranged for were a patient to arrive at a hospital. Here's another detailed example where SBADM means substance administration. And RQO means request. Typically part of a medication order by a physician. Before leaving RIM and moving on to some of the things it is used for, it's important to emphasize, again, that the goal is interoperability. And in support of that, RIM can be used both for HL7 V3 messages, and for clinical documents constructed at least in part from the data in those messages. This was a key goal of the effort. The clinical document architecture, or CDA, defined HL7, version 3 rim based documents, assembled from administrative and clinical data for particular purposes. After a period of use, a consolidation effort removed inconsistencies, resulting in the current consolidated CDA or CCDA standard. Here's a particular example of some simple patient demographic data we'll used to show how RIM supports CCDA documents. While it's difficult to read, here's the same patient data depicted in a RIM Roll. And we can see, for example. The phone number here is displayed here. Here's the data incorporated into a CCDA document. In this case it's the continuity of care document or CCD. A key electronic document for exchange among providers at transitions of care. When, for example, patients are admitted to or discharged from a hospital, or referred by a primary care physician to a specialist. Transitions of care are a key risk point, so exchanging data at them is an important means of assuring quality and safety. The overriding goal of the RIM is to: introduce XML into HL7, help improve healthcare outcomes, reduce healthcare costs, support interoperability among healthcare systems and providers. The answer, as we said earlier, is to support interoperability. RIM can be utilized to create XML-based CCDA clinical documents, HL-7 v3 XML formatted messages, both a and b, or neither a or b. The answer is a and b. The RIM can be utilized to create either CCDA documents, or XML formatted messages. CCDA documents are assembled from templates, which are essentially reusable XML components. Templates can be constructed at the document, section, or data entry level. These correspond conceptually to the parts of a paper form, where the document as a whole consists of sections, each of which consists of fields where specific data items can be recorded. Here at the document level, sections are specified. And within those, data elements are specified. In this case, a clinical observation, which is recorded as free text. Here's an example of a CCDA section-level template. Note that it has a template ID. Also note this code and this designation for the coding system. It's actually an OID designation. Now copy the template ID and go to this URL to answer the next question. So did you find that this template is called the subjective section, the objective section, the medications section, or the lab test section? The answer is objective section. Now take the OID designation for that code and answer this question. Is it an NDC code, a LOINC code, an ICD code, or a date? The answer is a LOINC code. Of course, if you looked at the XML on the line below, it told you that. The point here is to understand that coding systems are assigned unique OID codes. The number you just searched for is an example of an OID, a globally unique identifier. These are not health care specific and are used to uniquely identify a diverse set of objects or other entities. As we said, the data entry areas within the sections of a CCDA document are also formatted using templates to assure consistency. Here's an example. And again we see a code with an OID designation. So what does this OID specify about the code that precedes it? It tells us that this is a SNOMED-CT concept for Age. So, how old is this patient? Is it specified or not specified? Are they over 60? Are they 57? Or is the age not clear? Of course the answer, as you can see right here, is that the patient is 57. I refer you to the Fire website to figure out what PQ and a mean. Earlier, we mentioned the key role that the CCD plays at transitions of care. In an earlier lesson, we saw this screenshot from an actual commercial health IT system. Now you know that it does medication reconciliation by comparing the medication section of a CCD from another EHR, here on the left, with the medications recorded in the patient's record in their physician's EHR, here on the right. This is a very good example of the power of standards incorporated into electronic clinical documents, as a tool to improve care. We've now seen the widely used HL7 standards in action. You now know what was in that document Dr. Johnson sent to Dr. Jones back in lesson three. In the next lesson, you'll obtain a copy and do some activities with it. First, we'll look at some advanced HL7 standards initiatives that point the way toward the future, and may help deal with many of the limitations of current non-interoperable systems, and the usability issues of EMRs. The goal of the Arden Syntax is to provide a standard approach to describing medical logic so that it can be shared across EMRs to support Clinical Decision Support or CDS. CDS is the idea that computers can suggest optimal therapy based on a patient's electronic record. Or spot potential mistakes before they happen. To do either, they not only need the electronic record, they need to understand at least a limited domain of medicine. That's where Arden comes in. Arden consists of medical logic modules, or MLMs, each supports one clinical decision. Here's a group of five MLMs used to provide clinical decision support for the use of Warfarin, a common but potentially dangerous blood thinner often given to patients with strokes or other potential blood clotting problems. Too little of the drug may mean another stroke. Too much can cause excessive, or even fatal, bleeding. Warfarin levels can be effected by many other drugs and even the patient's diet. So it's a very tricky drug to manage. A clinical test called PT/INR is used to assess the degree of blood thinning. You can see here that the modules, once they are fed clinical data about the patient, such as their PT/INR, can make Warfarin dosage recommendations to the physician. I refer you to the text for the internal details of the Ardent MLMs. Ardent was developed at Columbia University with support from IBM, with the idea that centers of excellence could develop MLMs and share them with other institutions. However there are a number of technical challenges to actually deploying Ardent, with the curly braces problem being the one we'll briefly discuss here. This part of an MLM helps with a radiologic study that involves injecting the patient with a contrast dye to better see the functioning parts of their internal organs. It deals with a lab test, the creatinine level, which measures kidney function. This is important, because the dye in the study is removed by the kidneys, but it can also cause kidney damage. So it's important to know the patient's kidney function before giving the dye. To check for that, Arden needs to know the creatinine level. And to get it, this expression must be interpreted by the EHR in a particular hospital to fetch that value from its proprietary database. So, even though the purpose of Arden is to share clinical decision support, because EHRs aren't readily interoperable, we run into roadblocks like this, and that's led to low Arden adoptions. Healthy Decisions was a nearly two year long standards effort sponsored by ONC, and has now been folded into a broader clinical quality framework. It proposed using parts of the Arden technology along with web services designed to make fetching data like this much easier. The Clinical Information Modeling Initiative, CIMI, C-I-M-I, is an attempt to enumerate the detailed models of hundreds of thousands of medical ideas to achieve consensus among clinicians. They can be used to define standard messages or structured documents as components of clinical rules, and to automate or facilitate constructing data entry or reporting templates. The CIMI group lists simplification of existing models, perhaps referring to RIM, as one of its key objectives. The Context Management Specification, CMS, began at Duke University as the Clinical Context Object Workshop, or CCOW. It seeks to facilitate the integration of diverse applications at the point of use. It does this by managing issues such as the identity of a patient the user wants to view or update via these applications. The identity of the user who wants to access the applications. A moment in time around which temporal data displays should be centered by the applications and a particular patient encounter that the user wants to review. The details are highly technical, but you can get the general idea from this illustration where several applications from different vendors are all centered around the same patient, at the same time, for the same clinician. This is an ambitious but potentially very important effort. Since clinicians commonly complain about the time and effort involved to use many different systems that work in different ways to manage their patients. Before moving on, I want to mention an interesting feature of the SMART on FHIR app platform. This is called launch parameters. These provide some of the same services as anticipated in CMS, as providers, patients, and other users move about among FHIR apps. As shown here, these include the identity of the patient, the current encounter being viewed, links to other resource instances of interest with this patient, and even the reason why the app is being launched. Here is an example of a JSON-formatted version of one of the simplest FHIR resources, the patient. It contains the basic demographic information, such as name, birthdate and the provider taking care of the patient. It also contains a patient ID that can be used to unambiguously retrieve other clinical information. Look a bit more carefully and you'll see that this patient is actually our old friend, Marla. Here's an example of the more complex FHIR Condition Resource, in this case for Marla's diabetes. Note here the reference to Marla's ID number from the patient resource. Note here a reference to the SNOMED-CT code for Type 2 Diabetes, the adult onset version that Marla has. This illustrates an important characteristic of FHIR. It isn't being build from scratch. Rather it builds upon the key existing standards we discussed earlier, such as SNOMED-CT. In an arguably seminal 2011 blog post, titled The Rise and Fall of HL7, Eliot Muir, founder and CEO of Interfaceware, a major Toronto-based HL7 solution provider wrote complicated standards can be pushed for a while, but ultimately markets reject them. Muir went on to recommend a web services approach. Australian standards guru Grahame Grieve added a simplified data model, based in part on HL7 RIM, and named it Fast Healthcare Interoperability Resources, or FHIR. It's now a part of HL7 and seems to be spreading like its homonym, fire, F-I-R-E. I introduced FHIR way back in lesson one as a universal health app platform and you're using it that way for your team projects. FHIR solutions are built from a set of modular components called resources. The FHIR resources are a subset of RIM, essentially the most important and most commonly used data constructs, organized and categorized as shown here. I suggest you visit the resources section of the FHIR website to further explore this key element of FHIR. These resources can easily be assembled into working systems that more quickly and easily solve real world clinical and administrative problems. FHIR is suitable for a wide variety of contexts as shown here. This might include mobile phone apps, cloud communications, EHR based data sharing, server communication in large and institutional health care provider settings and other environments. The key is the use of FHIR resource tags in the underlying data structures. The interaction between the systems and the databases is via restful APIs which are relatively easy and fast to implement, further facilitating innovation and development. As we discussed earlier, a similar web-services approach was specified by Healthy Decisions. The web-services paradigm is increasingly seen as the best vehicle for developing and more rapidly deploying interoperability solutions and even analytic routines, and services such as those required for clinical decision support. This is an important trend that I strongly encourage you to follow over time. I've presented a very basic introduction to FHIR which is still very much in development and testing. The standard encompasses many other issues including what values termed value sets are allowed to define structured data elements within resources. I encourage you to explore further and stay up to date as the standard develops. Recently for example, an article appeared proposing three new FHIR resources to represent patients genomic data. According to Grahame Grieve, a key challenge facing the FHIR standards development effort is achieving the right balance between functionality and complexity. FHIR is not intended to represent all of healthcare. Rather, it is intended to contain the key resources and details needed to represent the most common elements of healthcare. I've served on standard development teams, and it is entirely predictable that people whose favorite use case isn't perfectly covered will push for inclusion of more and more detail. Doing so clearly risks creating the very complexity that led to the idea of FHIR in the first place. FHIR is currently in its second edition as a draft standard for trial use or DSTU under the HL7 standards development process. According to Grahame Grieve, FHIR will contain some 100 to 150 resources when it is fully specified. But, as of the summer of 2015, it is anticipated that around a dozen of these will be in final form. You can explore all currently proposed FHIR resources using the link provided, in the instructor's notes. Today I'm talking with Graham Grieve the project leader of the global HL-7 Fire Standardization Effort. He's here all the way from Melbourne, Australia for an HL-7 meeting this week in Atlanta. Graham thank you very much for taking the time to be with me. I know this is a busy week for you. Perhaps we could begin by you discussing your background. So I qualify as a biochemist, worked in a clinical lab, did a bit of research. And then I ended up as a lab systems vendor, tech lead and got more involved in interoperability and HL-7 and eventually created my own consulting compan to do HL-7 consulting. So beginning in 2011, you invented the whole idea of fire can you tell us how that happened? We were getting to a point where the HL-7 was a crossroads. We had a lot of existing standards that no longer reflected what people wanted to do, and it was a lot of unhappiness in the user community. And we said well, what if we did something like this? And it grew into Fire and it was a bit of a surprise to me as much as everyone else that it was so popular. But it's been certainly keeping me busy ever since. By all accounts the growth of Fire adoption is nothing short of phenomenal. Could you give us an idea of how it's being adopted not just here in the US but around the world? Well, we expected that initially while it's still a trial, beta standard which it still is, that we would have a little bit of adoption particularly in companies doing health data exchange in the social web area. But it's just been crazy. We have government programs, large government programs and other insane programs, other national governments around the world. They have big vendors creating consortiums like Argonaut and HSPC. Lots of little companies and institutions creating solutions based on Fire. We have thousands of happening at the moment. So I believe the ballot on Fire as a second version of a draft standard for trial use was just conducted, or is being conducted. Where are we in the whole evolution of Fire to a full HL-7 standard. So we have balanced it to standard in May. And then we just published last week in September, the second, formal second version. It's an ongoing process but we started with a very initial draft. And then this is I think of this is the first fully usable version and we're calling it a trial version. So we can still introduce new content and refine the core standard but increasingly, we're going to treat the core as a normative standard. And I think the next version will represent a stable version for implementors to base the core of clinical stuff on. But it's an ongoing process and we'll be very driven by the feedback we get from the community about that, which is why we're not saying right now how it will unfold, we don't want to dictate to the community. Well, I've heard numbers in terms of the ultimate number of Fire resources, anywhere from 100 to 200, I suppose nobody actually knows at this point. Well, we have been a success off today. And we sort of said 200 was a nice the number. We didn't want to have thousands or hundreds of thousands, which were possible choices. We haven't got a fixed number that we're going to get to. So that leads directly to a topic you and I have discussed when we were together in the past. To me obviously the use of Web services and JSON objects these technologies that are so common in other domains is very interesting I think very important to the success of Fire. But perhaps even more interesting is your mission of not allowing Fire to become an overly complex standard like many of the predecessor standards in HL-7. How is that going? That must be a very tough thing to do. It's an interesting thing to try and do. We have constant pressures to try and simplify it and constant pressures to try and complicate it. And we've got to strike a balance between handling the things people need to do and not letting it get so complicated no one can do what they need to do. Most of the success in that area comes from the processes that we laid down at the start around solid engagement with the implementation community and and being responsive to them. And we grade the successive parts of the standard not by publication but by adoption and that's a very important metric that drives a wider consideration of the perspective than just how do we produce a standard? So the people who only have to use this standard and implement it have a big voice in what the standard looks like and it isn't just designed by I'll use the term standards geeks. I hope that isn't a pejorative term. It's a very common term. That's exactly what we're trying to achieve so that you can define on the standard as a committee. But until it's been adopted and proven in practice, it's always going to be draft. So as you look out toward the future after the final version of Fire is published, how do you see it continuing to evolve and adapt to an ever changing world? Any community goes through a phases of storming, norming, conforming. And clearly the core of the specification is moving into the norming phase and will become more and more stable and growth will happen at the margins. And then some point where you'll say to the standard has now got as big as it can be. It's no longer sufficient to keep growing and to keep pushing it along and it's time to start considering a new choice. I kind of thought just looking at the past record of standards that a development lifespan might be 15 years or so and we're sort of 5 of 15 now. And then at that point, you start to say maybe it's no longer worth keeping going. That's ten years out. We'll see how we go. But at some point, you go it's just not worth keeping going. So one final question. So what would you say to the students taking this cours about Fire and about how they might both think about it, use it, and look for opportunities that Fire creates? Fire's really different to all the other approaches we've had because it allows you to modularize your solutions and to create capabilities that can be used in all sorts of different ways. And so in the long term, it's going to represent decomposing large systems into more interoperable small systems. And there's going to be a lot of opportunity for changing the health care system, not IT but in healthcare itself driven by the capabilities of the decomposition and the flexibility that it offers. We didn't write Fire because we wanted to have a nice standard. We wrote Fire because we wanted the implementations to drive a better health care system. And that's going to be our main focus going forwards is we'll round out the standard and finalize the standard. But our main focus is going to be how do we drive that to get a health care system that's more nimble, more patient focused, more responsive to change, because the existing system's pretty pretty rigid. Well, I not only want to thank you. I want to tell you how much I admire you and what you're doing. I'm a lot older than you are. I'm not even sure you were born when I got involved in health IT. And I never thought I'd actually live to see what we're seeing, which I agree with you completely is an absolutely workable implementable deployable solution to and to actually making health data a tool for improving health care. So congratulations, thank you so much. Thanks. So today I'm talking with Andrei Pop, the Founder and Chief Executive of Human API. Andrei is my favorite kind of person. An entrepreneur trained in Political Science at the University of British Columbia. He is now starting his second technology company. Thanks so much for having me. So, the Human API is the luster of a lot of the things that we have talked about so far in the course. I am very excited to have you here, because it brings together many pieces in a tangible way, that's easy to understand and easy to demonstrate. So let's start by talking about the fundamentals of what you do. And for that, we've assembled a few toys here, right? Let's do it. So I'm a patient and this device is measuring my blood pressure, or my activity, or whatever. Tell me what you can do for me, starting at that device. And so the idea for us, really was, we wanted to make it really simple for this data to flow from this device into wherever it needed to go, whether that was for research, for a doctor, for a nurse practitioner, you name it. And so what this device would have, and sort of the other really important part of this system is somewhere over here in a totally different part is you have a, say personal health record system and it's floating away from me, but let's just use this for the time being. So you start with this, and what we said is, okay, this device will now, already has a connection to a phone of some kind, right? A phone. Let's put that there. Great. And then somewhere along the way, it finds its way into a cloud system of some kind, right? So now you've got blood pressure data that's flowing from this device to the phone into this cloud system. And then over here, you've got- And typically, this cloud system would be provided by the company that provides the device, that's the norm. That's right, that's right. But now, you're over here, and you're talking to your doctor. And you're trying to say okay, I've collected all of this blood pressure data, and I'd like to actually get it into the system, how do I do that? And so that's where we come in. So what we provide is, you essentially, and I'll show you how this works a little later on. But you essentially would click a button inside of this personal health record system. You know, just like you were logging into an app with Facebook. And what would then happen is we would say look, we're going to grab your data from this blood pressure device, we being Human API. We're going to pull it into our back end, we're going to structure and normalize it, and then we will send it onto there. And so I'll show you how that works. But essentially, we now sit right in the middle here, and this is us. And so, now the data's flowing from this device through the phone up to the cloud service to us, and to the person who holds the records. And of course, it wouldn't just be data from this device, it could be data from several devices? Of course. It could be data the patient has recorded in their PHR. Right. Could be any health data really, disassembled into this patient controlled database. That's right. So now, for example, let's say you have a different device over here and then you're trying to query data from that, and same sort of tree-like structure that would flow down from there. And now here's where it gets really interesting, so now that we have the central identity that you can query through an API, as long as the patient grants access. Let's say you're using a totally different personal health record system, right, so you've got some other application over here. We've already got your data here, so now, all you need to do is go inside of the system, click our button and the connection is made. And now any time you capture blood pressure on this device, it will automatically flow up the chain and into all of these different systems. So that's what Human API does. >From the point of view of the course, maybe the more interesting story is how you do it. So for that, we're going to clear the table and let's take a look. So I guess this is a PHR or some other tool that the patient might be using. Right, exactly. So for example, we mentioned that connect button that allows you to pull data from all of these different devices. So what we've actually done here is this would be some sort of personal health record system and somewhere inside of here, you'd have this connect your health data button. And now what you do when you click that button, is we serve up this pop-up that basically says look, this wellness portal, or PHR system wants permission to access your health data, and we're going to pull it through Human API. Now this is really important because central to our philosophy in how we approach this business is to put the patient in the driver's seat in this case. So what ends up happening is the patient grants access to the system. Now, what's happening on the back end here is we've created an identity for that patient in our system. And we're essentially saying look, now we have all of these different sources so, for this example Fitbit, Jawbone, Nike, and Moves, but it could be any one of these, right? And so might say, okay, let's go ahead and authenticate a Fitbit, or let's go ahead and authenticate a Moves application against Human API. And so this allows us to connect each of those different data sources, and it could be a blood pressure monitor, it could be another PHR system, you name it. And so Moves is an app that keeps track of people's activity. Right, that's right. And FitBit is another approach to keeping track of people's activity through a device. Right. That's right. One using the phone, one using the device. So that's actually a really, really important point as well, because what we do on our end that also sort of helps move this process along from the perspective of the application that's pulling in our data, is we actually normalize this data. So if you think about having steps, for example, as a metric that's being recorded, maybe by an app on your phone, maybe out by a pedometer you wear, maybe by a smart watch that has an accelerometer in it. All of the data from the perspective of the application is actually just steps. And so, what we do on our end is we structure and normalize that data before we send it on to the application, which makes it much easier for them to use it. This is a really key point. Right. As everyone taking the course knows interoperability is one of the overriding challenges in health informatics. And you sit in the middle and don't just aggregate the data. But you put it in a standard syntax so that it's much easier on the other side to access the data as we'll see. Right. So yeah, why don't I show you kind of how that works. So I will go to here and I will show you. You saw that authentication process sort of happen on the back end, right? So once that authentication process has happened, the data starts flowing from Human APIs or from the device to Human API. And into these various systems, right? Now, what does this actually look like? Well, if you have, and, you know, for to kind of geek out a little bit here but, we basically give an access token for a user to the application. So, in this case, we're using the demo access token but each user of the application would have their own unique token, and then it's a restful API that we can query in. So we say look, api.humanapi.co human blood pressure. And here you can see structured data, systolic and diastolic blood pressure that's come off of a Withings blood pressure monitor. And then one thing you can do here is actually change the date range. For example, you can sort of modify this query on the fly. So, if you're building an experience for a physician that wants to look at this data at some point, you as an application developer can choose how you interpret this data for that physician once it's in your system. If, for example, you wanted to actually visualize it, we can do that on the fly as well. So rather than using the api.humanapi.coendpoint, you would just use chart and then there it is. And if you wanted to change the dimensions of this or do any of that stuff, you can do that on the fly as well. So, it's important to look at the URI here, the restful API. And see how simple it is. And in fact, it's actually human readable. Right. So getting to this data becomes much, much simpler. And there's only a reference here to blood pressure. And the app developer or the query, the source that screened the data doesn't have to know what device is being used. Doesn't have to know anything about that device. Doesn't have to understand their data streams, or formats, or syntaxes. They're going to get it in the standard form through your system. Exactly. So, what we do is we abstract all of that away. So, if you actually look at our back end here, you'll see we have this tool we call the API explorer. And what you can essentially do in here is you can see all the different types of data that you can query, so blood glucose, blood pressure levels, BMI. So, really important to this idea of normalization is we're not actually taking Whithing's data, for example, and just passing it on to the developers. What we're actually doing is making sure that the data is very clean and very structured. And that enables the application developer to really focus on the experience that they want to create without worrying about where the data comes from and all that stuff. Well, this is great. This is an example, I think of the future of interoperability Thank you. So I'm glad that we're able to show the students tomorrow today. Thanks for coming. Thank you so much for having me. Next, we're going to speak with Dr. Josh Mandel who earned a B.S. in Computer Science at MIT and an M.D. degree at Tufts University School of Medicine. Currently, he's Chief Architect for the Smart Platform Project at the Boston Children's Hospital and Harvard Medical School. So Josh, I really appreciate you taking the time to be with the students today. I sometimes think of you as a younger version of me. So could you tell the students a little bit about your background? Sure. Well these days the main hat that I'm wearing is I'm the architect for a project called Smart Platforms at Boston Children's Hospital, Harvard Medical School. But I have a background in computer science and did undergrad in computer science at MIT. I spent a little bit of time working at a biotech startup in Cambridge, Mass doing gene synthesis and laboratory automation. And I want to medical school at Tufts. I graduated in 2010 and realized I was having a lot of fun trying to fix thecomputer systems in hospitals and clinics and I've been working hard since then in the health informatics space. That's great. You've been involved I guess since the beginning with the SHARP Project at Harvard Children's Hospital. Could you tell us a little bit about that? Yeah, that's right. Sure. So back in 2009, early 2010, the Office of the National Coordinator for Health IT Sponsored four research projects or put out proposals for four research projects. One in the area of security, one in the area of data normalization, one in the area of cognitive aspects of using EHRs and one in the area of building app platforms on top of electronic health record systems. And that last one is a project called SMART that I've been working on at Harvard since 2010. And it's all about trying to build app platforms so that doctors and patients and healthcare practitioners can use the applications that they want to to interact with health data. The idea was, we've got iPhones in our civilian lives and Android phones that let us use whatever apps that we want. And wouldn't it be nice in health care if we could bring the app that we wanted to the table to work with patient record data. This idea of app platforms is something that a number of people are interested in. Tell us why you feel it's important and what approach did Harvard take to developing one. Yeah, so we have a lot of health care data, some of which patients bring to the table, some of which are measured in a traditional clinical setting. And those data live inside of a variety of different databases inside of hospital and clinics. And, when it comes to making sense of an entire patient, to make a decision, to figure out a diagnosis, to decide on a kind of treatment, it's extremely helpful to be able to bring those data together in one place and to use specific tools that help to do a specific job. So, this is something that we're pretty used to in terms of an app platform on the phone, or you might use a specific tool just to manage your calendar and a different tool to manage your to do list. And if there's a better calendar application that comes along, or a better to do list that comes along, you can trade information with your friends and find the best one that works for you. And we'd like to be able to do the same thing with the way that we make a diagnosis, or make a treatment for a diabetic patient, or engage in communicating with patients in a clinical setting. We'd like to be able to encourage app developers to build the best tools to do these small kinds of jobs and give end users the freedom to pick the tools that really work for them. So there have been a number of approaches to these kinds of app platform development and what we're doing at Harvard and with smart platforms is to build an open set of specifications built on open standards that anyone can implement without any proprietary technology in the stacks. So we want to make sure that all the specifications we build are out there on the web for people to build on top of. Well along came HIMSS 2014. And I ran into you there. And to my surprise and delight you were showing a new version of SMART. Can you tell us about that? Sure. So we were very excited coming into the HIMSS conference which is the Health Information Management Systems Society conference in Orlando this past February where we were able to get three or four different EHR vendors that come together and implement support for the same set of apps. So we had some apps that we had built ourselves on the SMART platforms team and other apps that had been built by independent application developers. And we were able to get this same package of apps running on top of a commercial EHR system from a company called Cerner on top of an open source electronic health record called Vista, which this instance of Vista was maintained by Hewlett-Packard in their government innovation lab. We were also able to show these same apps running on top of a system called Help2 that runs at Intermountain Healthcare in Salt Lake City. So it was a really exciting time because we were able to take some new, emerging standards from HL7 which is the healthcare standards development organization and build this app platform on top of an open set of standards that EHR vendors and large healthcare providers organizations, and Hewlett-Packard and government innovation lab, were all able to integrate consistently. And more specifically, that new standard was FHIR. So yes, we've been doing a lot of work recently with FHIR, Fast Healthcare Interoperability Resources which provide a couple of key building blocks when you're thinking about building a health app platform. And so the first important building block that FHIR provides is a set of clinical data models. So how do you write down a medication or an allergy or a problem or a lab result or a set of patient demographics? And FHIR provides detailed data models, which are called resources for each of those things. And the second building block that FHIR provides is an API, an application programming interface. And this is one that's built on ADTP and REST, that lets you query for the specific kinds of discrete data elements that you're interested in. So if you're trying to build a lab Data viewing app you can write a restful query or pull back the lab results that you're interested in from a system that implements this FHIR API. So FHIR has been a really important building block for the current version of SMART which is entirely based on open standards. Well can you show us a little bit? Yeah I would be delighted to show you what the SMART and FHIR platform looks like. So, I mentioned that Smart is a platform where third party app developer's can build apps that hook into EHR's. And I'm going to start off by showing you, not a real EHR, but kind of the demo sandbox system that we host at Harvard. So, we won't be looking at any real patient data, but you'll see a set of synthetic and anonymized patient data. And anybody who wants to try this at home can go to fhir.smartplatforms.org and create an account and go through the same app and the same demo that I'll be showing you here. But this fake EHR that I'll start out with just does two very simple jobs. It lets you find a patient. And then it lets you run another app. So I'll show you an example of a growth charts app that we built at Boston Children's Hospital. And I'm going to pick a pediatric record here. I'm going to search for a patient called Amy Lee. And I'm going to open this record in the Growth Charts App. And what we'll see when this launches is two different views of a patient's growth history. We decided it was really important to build apps that made sense not just for clinicians but for patients as well. So we won't get deep into the features of this app. But I'll point out at a really high level this app is able to fetch the data that it needs from the SMART EHR that's running on the other end. And when it gets those data, it's able to plot the traditional kinds of growth curves that you see on a paper growth chart. So the initial view that we'll see here will show me the child's length or height, as well as their weight. And we can see how it's been tracking over time. And this is an app that built entirely using HTML and JavaScript. It runs entirely client side in the web browser, so the only server side component of this app is just a static web server that hosts this HTML and JavaScript, and the app itself is all just running in the browser. So this is a traditional kind of clinician-facing view that lets me see how growth has been tracking in terms of percentiles over time. But also built into this app is a communication tool, so we can have a conversation with parents as well that highlights a few of the key aspects of a child's growth. So in this example we see this is a patient with a healthy weight, and so rather than getting bogged down in a lot of curves and numbers that are changing over time, the real essential message for a conversation with Amy's parents here might be that Amy has a healthy weight for her height and for her age. And we can see that her weight has been trending in a healthy fashion since her last visit. So this an app that we built. It's available open source, so anybody can take the code for this application and run it, or extend it to fit their needs. And it's an application that you're seeing here running against what we call our sandbox server with 50 or 60 sample patients of data. And I believe the app is called Growth-Tastic!, just so the students have the name. Yeah, well, the official name is it's the SMART Pediatric Growth Chart App, but when we want to sound jazzy about it, we'll call it Growth-Tastic! And if it's interesting, Mark, I could show you a little bit about what the data looked like under the hood when this app runs. I think it'd be great. Okay, so let me actually open up the Chrome developer tools here and show you what happens when we load this app again. So what I want to show you is, as this app loads, the set of API calls that it makes in order to get data from the smart EHR. We won't talk about the details of authorization on authentication right now but I'll just say that under the hood the app is getting access tokens that it needs to fetch clinical data using oloft2. But once it's got those access tokens it's able to make a couple of very simple API calls in order to fetch clinical data. The first one here is a call to get patient demographics. And so the call itself is just /patient/880378, which the patient ID in this case, and what we get back is a FHIR response which is a JSON representation of patient demographics which has the patient's name, the patient's birthdate, medical record numbers, other identifiers. And we can see just at a glance, this is the kind of payload that a developer could look at, inspect and make sense of pretty quickly. And if there are any fields that you didn't understand the FHIR specification which is hosted on the web defines each one of these fields in great detail. So that's what the patient demographics look like. And I'll show you just one more example. This app only needs to make one other API call which is to fetch vital signs. So heights and weights and body mass indexes. And in fHIR all those things are called observations. So the app is able to fetch just the observations that it needs about a patient's growth over time. And I'll give you just a really quick look at what those observations look like when they come over the wire. So we're looking right now at the FHIR JSON representation of an observation. And in this case it's a body mass index observation. And we can see that this is a quantity of 16.9 kilograms per meter squared. And it was taken in 2008. So again, this is the JSON representation of a set of FHIR data. And this is over the wire the kind of raw information that's fueling this app. Once the app fetches patient demographics and a set of these observations from the EHR, it's able to draw the visualization that you see here on this screen based just on those data. Josh, I know in addition to what you've shown us, you're interested in the idea that many people refer to as Pull. Can you talk a bit about how you're using other technologies to enable patients to effectively subscribe to receive their own data. Sure. So the work that I've been doing with my SMART platform architect hat on has been really focused on building tools that let clinicians launch an app from an electronic health record system or that let patients launch an app from inside of a patient portal online. But I've also been very involved with some efforts at the level of federal work groups to define standards and specifications that let patients access their own data online. So right now in 2014 as part of meaningful use stage two patients have the ability to view their data online through a patient portal, and to download it and to transmit it to a third party app that they choose. And typically the way that transmission happens is through something called Push Messaging or the Direct Project, which works like secure email. I've been very interested in trying to define standards for a different kind of mechanism for apps to access data. So instead of basically emailing your data to the app, we can have a web API where apps ask for data and patients can authorize access to those data or not. And so we've defined a set of specifications called the Blue Button REST API that have enabled anyone with health data, so an EHR vendor or a healthcare provider organization, to expose patient data for patients who want to authorize access to third party applications. It's a very similar set of technology to what we've been doing with SMART. But a different set of use cases and a different set of end users. This is great. I want the students to reflect back on the interview with Doug Fridsma much earlier in the course, when he talked about the idea of using Internet to move health IT into a new era of interoperability. And I think that, Josh, you've just shown us a real live example of doing just that. And I really appreciate your taking the time to do it. I hope this is something the students find exciting and maybe some of them will decide to do what you and I did and actually go into this field. Thank you again. Thanks very much, Mark. It's been my pleasure. We've now completed our review of data and interoperability standards. This is an immense field, and we've only skimmed the surface in an effort to equip you with a working knowledge of the areas being standardized, the approaches and technologies being used, and the potential benefits. You should recognize that for the most part health informatics standards have been developed within the industry and have only recently begun to adopt approaches such as XML and web services. You should also appreciate the inevitable attention that exists between the need many feel to create very detailed standards for everything and the complexity that creates. Often this complexity gets in the way of the real objective, which is implementation and use of the standards in day-to-day patient care. And the increasing recognition of that seems to be leading to a new willingness to look outside of healthcare and to adopt more facile and easy-to-implement approaches, such as FHIR. We've also now completed our discussion of the core technologies of health IT. And with that knowledge, we're ready to see how it is being used in the real world. We'll begin with what is still a very challenging endeavor, even after decades of effort. The development of practical, efficient, and easy to use electronic medical record systems. This lesson is about the critical role that data plays in the practice of medicine, the benefits of introducing digital records into that practice, and the two key challenges in trying to do that. Efficiently and accurately collecting high quality, comprehensive clinical data from busy and often computer-averse providers, and visualizing EHR data in a way that supports the providers' mental model in order to improve the quality of care. As we discussed briefly in lesson two, physicians are in the data business. They collect data, analyze that data, make decisions based on it, and follow up using still more data, so they can make adjustments as needed. Despite this, medical records and record keeping often receive minimal attention in medical training. And most medical schools offer no training in health informatics beyond learning to use whatever electronic record systems are deployed in their hospitals and clinics. Physicians usually start collecting data by interviewing the patient, beginning with their chief complaint, the reason for their current visit. They ask about other problems and obtain the history of those. To further explore the chief complaint, they ask about symptoms, manifestations of the problem that the patient has noticed, and they physically examine the patient. Based on this data, they make a preliminary diagnosis if possible, and order laboratory imaging and other tests to obtain as much objective data as possible. >From this aggregated database of subjective information from the patient and objective data from their own observations and their tests and images, they assign the patient with one or more diagnoses, and develop a plan for managing them that usually involves prescribing medications. In subsequent visits, they follow up and make adjustments. The treatment plan may involve referral to specialists, requiring coordination with providers working at different care venues, and possibly in different communities and health systems. Research has shown that this process is subject to at least three common sources of error, an incomplete knowledge base. The physician can't know everything given the rapidly increasing store house of medical research and knowledge. And error in perception or judgement. Physicians are still human and are subject to human error. And a lapse an attention. Physicians are also very busy and may not sufficiently focus on each case. Way back in lesson one, we reviewed some of the problems of U.S. healthcare. Too many errors is another problem. In it's landmark 1999 publication, To Error is Human, Building a Safer Health System, the institute of medicine famously said that from 44,000 to 98,000 Americans, are killed each year in U.S. hospitals due to entirely preventable reasons. Many of those listed, such as adverse drug events, improper transfusions, wrong side surgery, and mistaken patient identities, have to do with mishandling of data. This leads to one of the key arguments for adoption of electronic medical records. However, electronic records themselves, can be a new source of error. We now turn to that. First we need to acknowledge the magnitude of the challenge. If you think back to lesson five, you'll recall how large and complex the data standards are. This is an arbitrary, human anatomy, physiology, and diseases are very complex. To be accurate, and electronic record must be able to capture the nuances of care. To be maximally useful it would ideally capture most of this as structured coded information. How do you do that? How can you provide busy physicians with a way to accurately and efficiently capture such a complex data set? That's the EHR data capture challenge, and it has been with us for decades. The most common approach is this, a template. Essentially an electronic version of the paper check-off forms physicians have used for years in their practices. When the list of choices is long, as illustrated here, it's far too easy to pick the wrong one. This is termed an active error, the entry of inaccurate information. It's one of the seven common sources of EHR data entry errors, cited at the 5th Annual MIT Information Quality Industry Symposium. Another source of error cited at that symposium was an incorrect system default. This can occur in at least two ways. A particular EHR could have been set up to default to a particular value, and the provider may overlook that while charting. Another, and far more insidious possibility is that EHR remembers the values from a patient's prior visit to save the physician time in charting this one. The obvious potential error is to document what may have been correct last time, but is wrong this time. Another source of error derives from the desire to save the provider time by fitting as much as possible onto each data entry screen. This can lead to a cluttered and/or illogical layout that can cause information to be overlooked. The next source of error, inconsistent data across locations, should not come as a surprise. This is the interoperability issue we've been discussing throughout the course. Where multiple physicians are involved, it can be further complicated by differences in the way they document even the same clinical issues. Particularly when text entry is involved. For example, they might use different terms for the same thing, or nonstandard abbreviations or acronyms. Similarly, physicians may not record all the needed data leading to misinterpretations of the past history, or even the current treatments. Finally, where multiple locations are involved in patient's care, it may not be clear which of several observations or events is definitive or even the most up-to-date The challenge should be clear. Physicians are busy. Their work depends on data taken from a huge and complex set of possible values. They have little, if any training in informatics. EMRs could be better. Isn't it possible to innovate and offer approaches to the computer physician interface that are more imaginative and hopefully more successful than mimicking the paper tools that have been used for decades? We turn to that now. Back in the 1970s, when I was involved in developing an early EMR, the data collection problem was already clear. In fact, for the most part, our physicians dictated notes that were transcribed into the EMR. I used to tell visitors that someday, voice recognition would make this far more efficient and accurate. Has that day arrived? To answer that question, we'll look at M*Modal, one of the most advanced medically specific voice recognition technologies. As shown here, it not only translates voice to text, but it recognizes clinical concepts in that text and can code them into ICD-9 or SNOMED. Here, 346.9 is the ICD-9 code for migraine headache. 62972009 is actually the SNOMED CT code for an extraction, showing that at least at the time this slide was made, which was a number of years ago, the system was not exactly perfect, because there is actually a better SNOMED CT code that is specific to wisdom tooth extraction. There is a publicly available SNOMED-CT browser at this URL. Use it to find the exact SNOMED-CT code for a wisdom tooth extraction and enter in the box below. You should've found 65546002. Next, we'll talk with Dr. Juergen Fritsch. After earning a Ph.D degree at Carnegie-Mellon University, he was a co-founder of M*Modal. Today, he's the company's chief scientist. Juergen, thank you very much for being with us today. Thanks for having me. You are the chief technology officer of M*Modal, I believe. Chief scientist, actually. And the company which, as you know, I've been interested in for years, was founded in 2001. But, it was quite a number of years before the technology was built into electronic medical records systems for direct use by physicians. Could you explain why that happened that way? And more specifically, how you went about training the system? Yes. So this was actually very intentional on our part. When we founded the company, we were looking for an application of speech technologies that would tolerate the error rate that's still present in today's technology. If you're familiar with telephony systems and other speech enabled dialogue systems, you're familiar with how annoying it can be to an end user to have to deal with misinterpretations and errors by the technology. So instead of doing that and going straight to the end users, we decided to go a different path. And we used the technology on the back end first. And we did this by enabling an existing workflow, based on dictation and transcription. So in the US, there is billions of lines of documentation created in healthcare every day or every year through dictation and transcription workflows. And we enable those dictation and transcription workflows, specifically the transcriptionists that were doing that, to be much more productive. In that we produced a draft document that they can review and correct, and thereby be twice as fast as if they had to type it all from scratch. So that was an immediate use case for the technology. In other words, an ROI attached to it and it allowed us to build up a business. And at the same time, collect huge amounts of data. We collected billions of audio recordings and got them transcribed, corrected if you will, by these transcriptionist as part of the process. So it was a fairly genius way to basically collect the data, including correct the transcripts, to then train the system to get better and better. And after doing this for five, six, seven years, we got it to the point where it was so accurate that we could enable end users to actually use it in the front end. And use it for the biggest paycheck commission. So, when I first became aware of M*Modal and traveled to your office in Pittsburgh to see it. One of the most striking aspects of the system to me was it's ability to go into the text that it had created from the verbal transcriptions. Find clinical concepts, and encode them into SNOMED. Now, the students are familiar with SNOMED as we have this interview. They know how complicated it is, or at least they have an idea of that. So how hard was it to actually do that? How technically challenging? It was fairly challenging. Apart from the usual challenges with any natural language processing system. We had to deal with the complexities of the ontology, of the SNOMED ontology, and with the medical terminology to begin with. So, in addition to dealing with things like syntax, semantics, pragmatics, and just the context of the entire dialog happening. We had to also deal with similar words, words that have different meanings. Or, it can have different meanings in different contexts, things like that in healthcare. Just a simple term like cold, it can have like three different meanings. It can be abbreviated COLD and it's a disease. It could be the common cold, or it just could be feeling cold. So there is all sorts of disambiguation that needed to happen. And needed to be accurate in order to codify these terms correctly to their SNOMED codes. So how accurate is it at this point? That's a tough question to answer because it really depends a lot on the use case. We are not attempting to codify to the entirety of SNOMED, to the hundreds of thousands or 300,000 concepts in SNOMED. But, we're focusing on specific subsets based on a use case. So one of the use cases we're pursuing is chronic medical conditions. Things like diabetes or heart diseases. And in that realm, if you focus on a particular subset of diseases and remodeling those with a data model that's not just including the disease. But, also the temporality and the specificity, and who is concerned with that disease. You can be extremely accurate. You can be up in the high 90s in discovering and detecting these things in all sorts of contexts. But if you would attempt to codify each and every concept in the SNOMED corpus and the SNOMED ontology, you would be ending up in a much lower accuracy rate. And it's just not happening today, that people would automatically codify to the entirety of SNOMED. I have not seen anybody doing that accurately. And I think you really don't need to, because at the end, you're always pursuing a particular use case. You're always trying to solve a particular problem. And for that, you really only need a subset of that corpus. So what can you tell us about the technology that's under the surface? What languages and databases, and approaches are you using? So let's start on the speech side. As your students might know, all the speech recognition, speech understanding systems that are out there today are based on statistical algorithms. So based on machine learning, statistically-trained machine learning algorithms. And there's no difference here in our company. We do use those, a variety of those, starting with hidden Markov models, neural networks, regression models. Even the more complex machine learning algorithms that are on the market today, or that have been used. But we combine them in a fairly unique way, in that we're not just seeing it as a pattern matching problem as most speech recognition systems do. But, we combine speech technologies with natural language processing technologies. The theory is that if you get a little bit of an understanding of what you're trying to recognize, you can do better at the acoustic identification of the sounds. And vice versa, right? So it's a constraint satisfaction problem that you're solving. And the more knowledge sources you bring in, the more accurate you can be. So we do combine syntactic power servers and annotators, and cross taggers with the actual hidden Markov models new and networks that do the speech recognition. And altogether by combing them, achieve better results that way. When it comes to identifying concepts and tagging them in a medical corpus such as SNOMED, we use a lot of classifiers. We use anything from a statistical classifier such as even the hidden Markov model, all the way to maximum entropy models and things like that. I know you've begun to incorporate M*Modal into commercial EMRs, I believe Greenway here locally was the first. How's that going? It's going really well. Initially, there was a little bit of a reluctance by the EMR community, I would say, to even incorporate speech technologies into their systems. There was this perception that physicians will eventually converge on structural data entry, and will accept the drop down menus and the choice lists that they have in their systems. But, the opposite did happen. The physicians were pushing back harder and harder, and saying we need to preserve the narrative. Not just from a productivity perspective, they don't just want to be more productive and faster in entering the data. But, they also want to tell a story. They want to be able to explain what they thought about, what the thought process was in the diagnosis. Why they tried certain things, and why they worked or didn't work. And that, you just simply cannot do by picking from a choice list. You have to be able to narrate a paragraph or two. And so, this need for, not just speech technologies, but just a narrative. A way to answer a narrative, whether it's typing, speech recognition, or other ways, was really something that they pushed hard for. The EMR community has come around. We're not aware of a single big vendor, whether it's Epic, Cerner, Greenway Athena, or Allscripts. All these companies have come around, and are now heavily investing into preserving a narrative part of the chart of the physician's documentation. And so as part of that, they are all including some sort of a speech technology for authoring these things. But also, they are more and more incorporating natural language processing technologies in order to search the narrative and find the relevant information. And combine it with the structure data that they have. When you look forward, beyond where we are today, at the future of healthcare, and the potential role and future of voice recognition technologies such as yours. What do you see? I wouldn't limit it to just the speech technologies. I think for me, again, it's the narrative. And what I find exciting in healthcare is that there is this renaissance almost, of the narrative in physician documentation. It started out with that. Since 100 years ago, physicians have been writing and scribbling. And really telling a story about their patients. And then, it was kind of on the verge of being abandoned with all these initial EMR systems, using drop down menus, pick lists. And now, you see the opposite happening. Again, physicians are creating much richer storyline and narrative around their patients. And the more these technologies mature, the more, the better the speech recognition gets, the better the natural language processing taggers and identifiers get. The more use cases you see appearing. And to me, the exciting thing is that we can get to real evidence-based medicine in the next few years. Where you can use these millions and millions of clinical documents that the physicians are creating in any given hospital. That we can use those. That we can scan those. That we can find trends. That we can tell exactly for this patient of that gender, of that age, with that disease and that outcome. This medication worked or this medication didn't work, and in what context. And we can use that information to derive better treatments, and more accurate diagnoses for other patients. And I think that's when you really have reached that evidence-based medicine realm that we're all talking about, and really haven't achieved yet. Yeah, I think there's a very exciting landscape that you portray. I hope the students remember these comments when in Lesson 1. We look at some of the contemporary research efforts in things like clinical student support. So thank you very much. Pleasure. I'm thrilled to see how far you guys have come since I first visited you, I don't remember how many years ago. And perhaps, we can do this interview again in two or three years, and see what you've done then. Any time, my pleasure. Thank you. Thank you, Mark. As impressive as it is M Modal is a tool it isn't an EMR. Once the company felt it was ready, they offered to integrate it into commercial EMRs. And the first to do this is was PrimeSUITE by Greenway Medical Technologies, which is shown here. As you can see, the voice recognition is built into the charting process the physicians use, and becomes an option for data entry in a number of clinical contexts. The introduction of Machine Learning as an EMR technology is I think, potentially a key development. If EMR's are to adapt to physicians, they must have some understanding of what they do. To explore that further, we'll now turn to a second commercial EMR, Praxis by Infor-Med Corporation. PRAXIS using machine learning and each physician user trains it as they see the common problems in their practice. Which it identifies based on the physicians entry of the clinical finding most specific for each problem. Over time PRAXIS learned how that physician usually treats each problem and as shown here. Presents a note based on the most similar past patients. Each phrase is what Praxis calls a clinical concept. But these are not encoded into SNOMED CT or any other data standard. They are text descriptions as defined by each physician. To avoid the erroneous default value problem we discussed earlier. The physician must click on each clinical concept to confirm and, or edited. This one has been clicked on and the physician can then edit the number of days to whatever is appropriate for that patient. If you were designing an EMR, how would you propose to use machine learning or some other artificial intelligence technology to facilitate data collection? Do physicians actually prefer EMRs with innovative approaches to data collection? This 2009 survey from the American Academy of Family Practice suggests the answer is yes. Here physician choices are analyzed based on practice size. The lighter colors are small practices. The darker colors are larger practices. It's important to recognize that smaller practices tend to independently make these decisions, while larger practices tend to have employed physicians and/or to be aligned with a health system that often makes an enterprise EHR choice for them. Grouped by practice size, as you can see, the choices are quite different. While I don’t want to generalize based on my own direct observations of the systems at the top, the physicians that chose EMRs on their own do, in fact, pick more innovative or well-designed solutions for data collection like those we’ve been discussing. The second major EMR challenge comes after the data is collected. How do you present it to physicians in a way that supports what an important national research committee report referred to as their mental model. Earlier in the discussion of data segmentation in Lesson 4, we looked at an EMR design proposal from Dr. Jonathan Nebeker to illustrate the added power that comes when software understands clinical relationships. Here's another illustration from the lab of my Georgia Tech colleague, Dr. John Stasko. His jigsaw text analysis tool shows the strength of the relationships between the staging of prostate cancer here and the specific medications and other treatments used by their physicians. So equipped with more powerful knowledge of clinical relationships like these. How my DMR is better support the physicians mental model. This is really an unsolved problem. But we'll look at some key concepts that might help lead to solutions. They all point to one major deficiency in most EMR's, they don't readily distinguish among fairly obvious differences in the way they're used. Here is Marla, we already know her story. Many chronic diseases leading to many medications and many encounters with the healthcare system. As a result, her chart is large and complex. And should she visit her PCP, if she had one, they would need a quick overview of it to adequately plan the visit and her comprehensive care. Her friend Doris on the left works out regularly and rarely goes to her physician. Today she's visiting because she overdid things a bit and sprained her right knee. Doris' chart is thin, and there's really not much else for her PCP to be concerned with, other than any routine preventive care that might be due. How might you approach the design of an EMR to present the data from these two very different patients most effectively to their physician? Now, remember that Marla actually receives most of her care from specialists. Each of them is focused on the one organ or body system they were trained in and they care for. Remember Doctor Johnson, the endocrinologist who cares for Marla's diabetes, and Doctor Jones, the lung specialist to whom she was referred. Each of them will probably be interested in different data from Marla's very large chart. This would include subjective information Marla has provided and objective information from her prior physical exams, lab tests, or other studies. Each of them will prescribe medications and other treatments that are specific to the disease that they're managing. Current EMRs generally don't understand this and don't automatically make this distinctions. How might you approach the design of an EMR to present the data to these two very different physicians? We conclude with some of the seven information-intensive aspects of the IOM's vision for 21st century healthcare. While these are certainly not limited to the data visualization challenge, nor will they be solved through it. They align well with what we've discussed. And their solutions certainly require improved EMR visualization capabilities. The Institute of Medicine calls for EMRs to store comprehensive patient data and to use it to provide cognitive support for providers and patients. Aspects of this are integration of patient-specific data where possible, in order to personalize care. And the integration of evidence-based guidelines to improve outcomes, reduce costs, and enhance the safety of care, along with the rapid integration of new knowledge and technology into clinical practice. The IOM also goes on to advocate for both practice-wide and population-level care management, something that's significantly scales up the data to be managed and a clear opportunity for innovation visualizations of the kind we'll see in lesson eight. The IOM specifically calls for recognizing the home as a new care setting. This aligns with more use of home-based technologies for monitoring and treatment and the use of the information technologies we'll discuss in lesson seven. Such as personal health records to promote patient engagement and communication with health professionals. We've now looked at the central challenges facing EMR designers. And have considered some innovative approaches, particularly to the data collection problems. There's some additional material in the text about data visualization you should review. In particular, view the medication reconciliation video from Dr. Shneiderman's group at the University of Maryland. New challenges arise as the reimbursement system moves from pay for procedure to pay for performance. Novel informatics tools are required to overcome these challenges. And, as you'll see later on, are usually provided by separate systems, since virtually all EMRs were designed for the historic payment model. The Marshfield Clinic, the organization whose exceptional performance in the PGP demo we discussed earlier, has historically developed its own EHR. The latest version introduced in HIMS 2015 features a fully integrated set of these tools. Now that we've considered the principle challenges of medical record systems for providers, we move on to informatics tools for patients. Years ago, my professor told me that as a practicing physician, I would probably see patients for a few minutes every few months, and think that what I did then would make all the difference. In fact, he said, it was what those patients did between the visits that would make the difference. Today we have a rich set of technologies and tools to empower patients to be more involved in maintaining their own wellness, preventing disease, and managing disease more effectively, should they develop it. Understanding these new technologies and the patient's point of view is the subject of this lesson. You're already familiar with what may well be the key tool for patients, the personal health record, or PHR. While the exact form of a PHR is evolving, the central ideas for a long time have been that it is patient controlled, patient's record data that could become part of their electronic health record. Patients control access to their data, and it's a life long record. In this lesson, we'll use Microsoft's HealthVault, but there are several widely used personal health records, and they mostly do the same things. You should already have a HealthVault account, but we haven't yet explored it to see how it stores clinical data and what it can do to help patients use that data, including sharing it with others. So here's an example of my health data I manually input in HealthVault. You can see key data such as patient allergies, immunizations, conditions, and lab test results. As shown here, users are assisted in inputting complex information such as their condition by smart lists that pop up as they type. More recently, as meaningful use has dictated the export of at least standardized clinical summaries, the burden on the patient to record all their data is largely alleviated. And the PHR can and should import these summaries from all providers caring for that patient. Note here that I have a direct email address provided to me by HealthVault to help facilitate that. With the growth of the wireless internet and mobile devices, the PHR's now accessible anytime, anywhere. And it can upload data from physiologic monitoring devices, from smartphone sensors, apps, and other sources. You can see here a few of them. I actually typed hypertension in, and it narrowed down to the three apps and 16 devices that are available to support patients with that condition. Up through now, you've only seen a limited range of the data that can be stored. There's actually a lot more and you see some of it here. If you view this screen in your own HealthAlt account, you'll see the full list of data categories. It's quite extensive. I want you to focus now on this item, the Continuity of Care Document, or CCD. You should know what it is from lesson five. By clicking here, a patient can upload their CCD into HealthAlt once they have it, something you're going to do yourself in the next class activity. You should recall that when we last saw Marla in Lesson 5B, she was seeing her new pulmonary specialist, Dr. Jones, who received Marla's XML-formatted CCD using Direct from Dr. Johnson. Under Meaningful Use Stage 2's VDT mandate, Doctors Johnson and Jones and all of Marla's other physicians are encouraged to provide electronic access to these clinical summaries to their patients. So Marla is able to access that information, and once she has it, she can do what I just showed you and load it into her HealthVault record. Marla's given us permission to provide you with her CCD. Since she's fictional this is not protected health information, so you can have it and freely use it. For this activity you should send this CCD as an attachment, and email it to your HealthVault direct email address. Before doing that, go to your message center settings, and make sure add incoming message attachments as files is checked, so the data in Marla's CCD will be added to your record. Once that's done, and you've imported Marla's CCD into to your record. Manually add the drug Spiriva to Marla's record. This is a drug Dr. Jones might prescribe to help her breathe better. Also manually add a new hemoglobin A1C, with a level of 9.2%. You can date them both the date you add them. Now go to health information and click more actions, and export Marla's record as a CCD. You'll need to submit it to get credit for this activity. Beyond PHRs, there are many other technologies and ideas about how to engage and empower patients. We'll turn now to one of the most prominent, PatientsLikeMe, a social networking site on steroids for patients who have a serious medical condition. So this morning I'm talking to Sally Okun who's the Vice President of Advocacy, Policy, and Public Safety at Patients Like Me here in Cambridge. Sally, thank you so much for taking the time to be with us today. Thank you for having me Mark. I really appreciate the chance to talk to you. So we've talked before, and I'm always fascinated by the story of how Patients Like Me came to be. So, would you tell us that story? Sure, absolutely. You know, it's not uncommon at all for a one patient or a one family story to actually be a driver behind innovation. Whether it's in healthcare or some other experience. And for our company, it was very much that. So, Stephen Haywood, who was 29 at the time he was diagnosed with ALS, had two brothers, Jamie and Ben Haywood. And a good friend Jeff Cole, who at the time of Steven's diagnosis decided to dive in deeply and figure out how they might help to stall the progression of his disease. Or, in fact, try to cure his disease. All of them coming out of MIT, engineer-minded and solution oriented, and so they really wanted to find some ways of being able to help Stephen through this. What they found was rather frustrating. A lot of the research wasn't available for them to be able get at. Some of the information that they were receiving was conflicting. And they ultimately decided that much of what they were actually learning was from other patients and families who were dealing with the same issues. And decided that they really needed to find a way of being able to bring those patients and families together to better learn and connect with each other. And ultimately find ways of being able to monitor track their own experiences. Bringing that together, you start to learn about the collective experience and the collective wisdom of everyone else. So Patients Like Me was actually born out of that kind of model. Bringing people together, helping them find different ways of being able to problem solve in their own times of living with illness, but also finding ways of being able to connect to the opportunities to contribute to research. And that was a big component of what we do. Well, there's a wonderful story about the ALS community on patients like me, contributing to research around a report out of Italy I think it was, that maybe lithium would benefit these patients. Can you tell us about that? Sure. You're exactly right. It was a small Italian study that was done and it indicated, or it claimed actually that the use of lithium carbonate was actually slowing the progression of ALS in a small subset of patients. What our patients on the site, who are already part of a community that was tracking and monitoring their condition, many of them went to their doctors and said I want to try Lithium. And in this particular population, the doctors will oftentimes try to work with their patients to try to determine an appropriate use for an off label indication. So in this case many of the patients started using Lithium. In fact, we had about 348 patients on the site, who were actively taking Lithium under their clinician's review and starting to track their experience with that. And they came to us, very activated, to say can you help us start to gather the data in such a way that we can actually start to determine whether or not this claim from this study is right or wrong. What we learned in a very short period of time, quite frankly, in nine months, we were able to demonstrate with this group of patients, that the claims that were made in the initial study were not holding up, that the progression was continuing. There was no indication that the progression was slowing as a result of using the product. So we took that opportunity to suggest that this was an environment within which patients who are activated and interested in participating in research could contribute their data in a very meaningful way. We took it to the next step, however. Because we understood that we have a biased population. So our research scientists and data scientists spent time developing a novel algorithm where they could match patients who were using Lithium with patients who actually had a similar disease course but were not on Lithium. So they were able to create a control group, that actually then was useful in being able to support the report that we ultimately showed, which was that Lithium really did not have an impact on the progression of ALS. In fact you know, for some patient's it could have been a concern because they were actually taking a drug that might have had adverse effects that they didn't necessarily need to incur. It was a great opportunity to show the power of the patient research network. So, you began with amyotrophic lateral sclerosis but you've certainly expanded well beyond that. Can you tell us about the other conditions or the number of conditions and the number of communities are now on Patients Like Me. Sure. We currently have about 270,000 patients on the site. We cover over 2,000 different conditions. Initially when we came out of the first few years with Patients Like Me, we were really relatively small communities that were focusing a lot on these very serious, neurological conditions similar to ALS. With MS and that sort of thing. And since then we've really expanded quite a bit. So today we have communities are focused on, Multiple Sclerosis is one of our largest communities still, about 40,000 patients. We have a very large community of fibromyalgia patients. Patients for whom the system isn't necessarily very helpful. Because it's such as complex condition with a lot of symptoms, and not a treatment in sight right now. Many mood conditions. So we have a lot of patients who are using the site for monitoring their depression or their anxiety disorders. But then other conditions such as epilepsy, rheumatoid arthritis, many of those are also in the thousands of patients. We're seeing more and more, some of the more rare conditions and often times, parents coming on to the sites to start to monitor and track their children who have some unusual and rare conditions as well. So, it's really expanded, just, exponentially, frankly, from where we began and it's very exciting to have the opportunity to have so many different conditions represented. Well this is a health informatics course, and I know from discussions we've had in the past that there is a lot of informatics sitting under this beautiful user friendly, non technical site. Can you tell us a bit about what it takes to actually make this thing work? Sure it does take a lot of engineering. Much of our company, we have about 80 staff now. Much of our company are engineers, software developers, designers, user experience specialists. Then we also have the other side of the house which is research scientists, data scientist and clinicians, who are trying to bring the two worlds together, very much in health IT perspective. So on the clinical side, for example, we actually take all of the data that comes in from patients, and we actually code that against the standardized terminologies that you would find in an electronic health record for example. Can you tell us which ones? Sure, we actually code against most of the ones that you would imagine. So ICD, SNOMED, we use mEDRA for side effect data, which is what the pharmaceutical industry as well as the FDA uses for monitoring adverse events. We use Multum database right now for our drug database. But we're moving more towards using RxNorm. Again another one of the standardized terminologies that's within the UMLS library. So we try very much to use existing standardized databases in order to be able to normalize our data in some way. So that it ultimately could be able to be interoperable with other systems. Right now that's not as transparent on the site as you might imagine, but when you go behind the scenes, we can then begin to look at how many ways have people talked about their symptom, against a particular specific clinical concept. And there could be 30 different ways that patients actually express one clinical concept. So, that's actually contributing to the development of a patient vocabulary that actually maps back to clinical concepts. And there's probably close to eight or 9,000 terms in there now that are in patient's voice that we've actually then mapped to clinical concepts. Super. I know you've recently, we talked earlier about clinical research, and you emphasized that the Lithium study really isn't a controlled trial, in the classic sense. But you guys are helping your users, your patients find and get involved in clinical trials, I understand. Can you tell us about that? I can. And we have a few ways of doing that. One is that we download every night, the data from cliinicaltrials.gov. And so a patient can look at their profile the next morning and see, based on their profile data, which trials they actually matched against, within 25 miles of their home. Gives them a place to begin. Then we've also just recently announced a service that we're going to be provided to the pharmaceutical industry. It's called Trial Access. We've learned from our patients through surveys that we've done with them, that many of them have been invited to participate in trials, and many of them have not been satisfied with the experience of those who actually did participate. Sometimes it's because there wasn't enough patient-oriented consideration into what it took to participate in a trial. So what we've learned is that by having this service called Trial Access, we're actually going to be helping patients participate in the development of trials, right from the very start. So they're better able to say, this particular intervention is going to be very inconvenient for me to be able to do. Had you considered that? Now another way of being able to state that could be, if I'm going to have to have a blood sample drawn, we need to determine how far away I'm going to need to go to get that, or can someone come to my home and do that? So there's different ways that patients can actually contribute practical advice to the child development [CROSS_TALK]. So patient-centered clinical trials. Absolutely. A new concept. All the way through the system. And actually even also to the point where they are participating in the evaluation of that trial. How did it go? And being able to get some findings back to them. Right now, you participate in a clinical trial and you never know what the outcomes of that trial were. And if you have access to a journal, you might be able to find the publication at some point, but much of that data isn't published. What we're saying, and we're suggesting, and we're hoping, that the industry really begins to embrace. Patients need to have some feedback. There needs to be a mechanism that if I've contributed my data and I'm participating in this study. I'd like to have some sense of how it all went and how it did contribute to new knowledge. Well great, so that sort of leads to my last question, which is, I'm going to ask you to get out your crystal ball. What do you see as the future of Patients Like Me, at least that you can discuss? And specifically, and I'm not sure I know the answer to this one, as I'll be interested. How do you see Patients Like Me being involved in VDT, the move for patients to be able to upload their electronic records and share their data with each other and with other clinicians? Well, we have a lot of exciting things going on. And some of the projects that we're currently working on are very much about that. We're doing some testing and piloting of sensors and mobile devices and wearables. Trying to figure out ways of being able to give patients the opportunity to have access to these devices, bringing that data into our platform, and then ultimately finding a way of bringing that data into the clinical records, so it can be useful. But there's a lot that you, as you might imagine, that goes along with that. We need to think about how do we standardize that data, how do we understand how to code that data so, we're just really beginning to learn and those projects are just launched. We've just actually sent out the packages to the patient participants, so they can start to do that active data collection. We see the platform as being an environment where patients can truly be a part of the changing healthcare system in an area where we've never really had access to their voice in such a systematic way before. And we believe that, actually, that contribution will really help us achieve the promise of a continuously learning health system. Without that voice, we're sort of operating in a bit of a vacuum. So we are continuing to build the platform with that in mind. We hope, within the next year or so, to really have the ability to have that interoperability data exchange, so that we might be able to pull in data from an EHR record, to unburden the patient from having to enter all that data in. But you know, there's another facet to that. The patient can also verify the data. Exactly. And really reconcile the medication list for example, as part of the helping to then achieve something that an electronic health record holder needs to do. They need to reconcile the medication record and this is a way of being able to do that. So there's a variety of ways that we see our ability to be able to take advantage of that kind of view, download and transmit type of technology. But we want it to be bidirectional. Let's get data going back and forth. So that ultimately each record is a reflection of the full 360 view of the patient. You raise a really interesting point. I was at a talk recently from someone at the Geisinger Clinic. Which is a very well known health system in Pennsylvania. And they have an open chart initiative, which allows patients to see their actual charts. And one of the benefits they've seen is that patients find and help them correct mistakes. Errors, exactly. You know, it's really very common for something to be on a record that isn't quite accurate, or for a patient to see a medication that they may have taken, ten years ago, but no longer take, and be able to say to the doctor, this is not accurate let's fix it together. One of the things I've been playing around with is this equation, and I've been talking about it a lot lately, and that's this notion of shared data. So my data as a patient, your data as a clinician, our data together. Actually can, when you add that into a shared decision making model, ultimately leads to a shared accountability for the outcomes. So that we actually all have the opportunity to share in the ways that I can improve my health. I can have my health be more reflective of what matters to me. But at the same time I'm understanding the kinds of things that are, a clinician needs to know in order to be able to take better care of the needs that I have so. Well as soon as I get back to Atlanta, I'm going to encourage our friend Marla to sign up. So if you see a woman with a lot of problems named Marla sign up, you should know that I sent her to you. Well the other thing to keep in mind with that in mind is that the site has levels of privacy that allow a patient to use a username or their real name. But I would want to be sure that people do know that we do take great strides to be sure that the experience for patients is safe, they can trust the site, they can feel comfortable about being able to exchange information. Students know who Marly is, you don't. I don't [LAUGH]. Thank you very much. Oh, thank you. Great talking to you. In addition to what you've seen how might social networking help improve health and healthcare? I can offer a couple of answers. First, researchers are already studying health related social networks for insights into patient behaviors and motivations. Second, social networks could help facilitate matching patients to research projects, such as clinical trials. Before PHRs and sites like Patients Like Me, hospitals and physicians were able to establish portals, essentially web pages where their patients could do some of the things we discussed under personal health records. The first of these is now part of Relay Health, itself a part of McKesson Technology Solutions For Healthcare. Here patients can get test results, they can request or sometimes even make appointments, and they can pay their provider, something you don't see on PHR sites, which emphasizes the key difference. Portals are generally provided to the patient, while PHRs are generally patient initiated and operated. Importantly as you can see here, patients can now Download their Health Data, in this case using Blue Button Plus, a technology we've not previously mentioned, that is intended for just this purpose. And one that is getting a lot of attention because of the VDT requirements of Meaningful Use 2. Which of these might be used by a healthcare provider to achieve the VDT requirement of meaningful use stage two? Choose any and all that apply. Is it Health Vault? Relay Health? PatientsLikeMe? The answer is the first two, HealthVault, a PHR, Relay Health, a patient portal. PatientsLikeMe is really a social networking site, not a place where patients, at least currently, would engage in the activities required by VDT. We just mentioned Blue Button, which began in the VA in 2010, and was adopted by ONC in 2012. Their SNI work group expanded the standard, added more structure, and dealt with direct integration to create Blue Button+. This graphic from the Blue Button+ site, shows how it can be used in conjunction with direct, to move the data from the providers, EMR, to the patients, PHR. This one shows that Blue Button+ can be used to bring data from multiple providers, into one patient manage technology, such as a PHR. The data itself is in an XML document, formatted according to CCDA. You should recall that CCDA documents are constructed from templates for the document, its sections, and the actually data entries. Here's a list of some of the sections of the Blue Button+ CCDA. More recently, the Blue Button+ rest API effort, is offering a restful API approach to using Blue Button+ for both its traditional role, push, where providers send the files to patients. Or their designated PHR app, as well as pull, where patients can in effect subscribe and receive updates, as new data comes into their providers EHR. You may be surprised to learn that collecting data in the home to help with care is actually quite an old idea. Here's a photo of Steve Kaufman, one of the early innovators in this space, with his HANC, home assisted nursing care robot, that offered a wide variety of voice controlled nursing services to patients at home in the mid 1990s. Among other things, it dispensed medications at the proper time and took physiologic measurements with its patient operated devices based on the technology of the day. We'll refer to this field as telehomecare, but that implies a more passive role for the patient than is appropriate and technologically possible today. What has also happened as telehomecare has evolved? Patients are less capable of sending data to their physicians. There's an increasing interest in consumers assuming more active roles in their care. Patient operated devices of today are more inexpensive, mobile, and user-friendly. A and B, B and C. The correct answer is B and C. There's definitely an increasing interest in consumers assuming more active roles in their care, as we've discussed a number of times in this course. And we're all familiar with the rapid growth of smartphones, sensors, and other far less expensive, and relatively mobile and hopefully more user-friendly devices. So given all of that, A is clearly wrong, if anything, patients are more capable of sending data to their physicians today than they were in the past. In our exploration of health fall, we saw that patients could upload data from a large number of devices. This is, of course, increasingly possible, inexpensive, and convenient with embedded smartphone motion and other sensors, being the latest technologies. With the introduction of Apple's new HealthKit, it may be that the phone itself, is the aggregation device. Since Hank, numerous commercial packaged telehomecare systems, including one I was involved in developing in the late 1990s, have sought to assist patients in recording subjective data about aspects of their care. Including their symptoms, their activity level. And very importantly their compliance with their treatment plan as well of course as objective data typically obtained by devices about their weight, blood pressure, pulse, temperature, and even their electrocardiogram. A key piece of data, medication compliance, has remained very difficult to obtain in a cost effective manner, despite a great deal of research in commercial activity. Devices and software are available to remind patients, to monitor when they open a special pill container, and to even dispense the proper medications at the right time. There are continuing challenges with usability, and with updating the more advanced devices when medication orders change, as they often do. The gold standard would be knowing that patients actually consume their medications at the proper time, and in the proper dose. Key objective data are increasingly available, from low costs wearable devices, like the Fitbit activity monitor, or even sensors embedded in smartphones. When combined with apps on the phone, they can create a patient self management system. That often includes the ability for family members or caregivers, to monitor the status of an elderly relative. They're becoming more sophisticated over time, and the US Food and Drug Administration is currently grappling with the degree to which they should be regulated as medical devices. AliveCor's smart phone case, that can monitor a patient's heartbeat is a good example of a sophisticated device that has been through the FDA process. This process is designed to assure things such as, the devices are safe, they won't injure the patient. They accurately measure what they claim, as compared to earlier devices. They're manufactured properly and that the manufacturer keeps records, so that patients with a potentially defective device can be readily identified. Technologies like AliveCor have potential uses beyond patients at home. A recent article in Circulation, a leading heart disease journal, reported on success in using the device to screen for atrial fibrillation, the most common cardiac arrhythmia. And one that can lead to strokes if not identified and properly treated. If you were the FDA, how and where would you draw the line between apps you regulate and those you don't? My answer would be that apps that manipulate or visualize the data they collect in order to give advice. Or influence the behavior or decisions of either patients or physicians should be regulated to make sure they do that accurately and safely. It's good to not only monitor patients but to provide them with help to manage their health and diseases more effectively. This is an area where PHR apps may have a role in part because they have access to information about the patient that is stored in the PHR. The American Heart Association's heart360 is an example that provides a suite of capabilities including a physician dashboard that brings together data for all the patients using the app and who are under their care. The final application of technology in the home that we'll discuss is virtual home visits. Once expensive, video conferencing is now available on virtually any computer device, including smartphones and tablets. So direct interaction between a patient at home and a professional or other caregiver can be inexpensive and relatively simple to achieve. However, this application is not without controversy. Some state medical boards take a dim view of physicians treating patients over the Internet, if they have not previously physically seen that patient in person in their office. When do you think it's okay for physicians to provide internet-based care to patients they've not previously seen? Always, never, or it depends? My answer is, it depends. This should probably be okay for simple conditions that could clearly be treated this way without first seeing the patient. In fact, doctors do this now when they're on-call. They may talk to patients they've never seen before and make decisions based on information the patient gives them. More complex problems requiring a physical exam to gather data probably should be initially seen in a traditional visit before remote technology is utilized. The challenges here are similar in many respects to those around EHRs. While efficiency may not be quite as important, usability is arguably even more important and difficult to achieve. Patients, including the elderly who may not have support at home, must be able to use these technologies. The technologies are obviously capable of generating vast amounts of data for millions of patients. No one has time to look at it all. So analytic and visualization tools that minimize false positives and appropriately alert providers are of critical importance. This is an area where machine learning and other tools of artificial intelligence are being applied. Despite the challenges, technology to empower and remotely monitor, educate and even treat patients is, without a doubt one of the most dynamic, innovative and rapidly growing domains within health informatics. Any students of this course with an entrepreneurial mindset, would do well to consider it. We've reviewed the many ways that technologies, in particular the Web, inexpensive measurement devices, and mobile computing devices, are empowering patients to become a participant in the healthcare team. However, for the most part, we've had a single patient at a time model in mind. How can physicians manage their entire patient population effectively and efficiently? That's the topic of Lesson 8. Up through now we've been focused on the traditional one patient at a time care paradigm. While this is, and should always be, the primary focus of healthcare, it should be clear to you that especially with patients who have chronic disease, or in managing public health problems across the population, a different approach is needed. Here, informatics has a key and essential role to play in aggregating and reporting data in ways not anticipated in the traditional EHR. Our case study for this will be PopHealth, an open source, quality measure reference implementation, supported by ONC. Here's the basic architecture of the system. The key concept is that queries are running its data in each provider's EMR, as shown here in the center of the diagram, without that data ever leaving the provider's control. The statistics are reported in a format called QRDA. What are the potential advantages and disadvantages of running each pop Health query, at the provider's site? My answers would be that the advantage is that providers are more likely to participate, since it's at their site. And securing the data is obviously simplified, since it doesn't leave their site. My disadvantage is the need to deal with the specifics of each EHR. This is, essentially, the curly braces problem, yet, once again. And the need to update the on site software should that EHR design change, which often happens. Can you propose an alternate architecture to running queries on site that might be equally acceptable but have other advantages? My answer would be data lockers. Centralize the data, but allow each provider to control their own data and decide what queries they will respond to and even possibly review their results before releasing them. As you can see here, QRDA statistics can be reported at one of three levels of detail called categories. These are all reported in a CDA-compliant XML format. As you can see here, the categories are the individual patient level, which could be a list of encounter dates for that patient, patient lists, which could be all the diabetic patients in a provider's practice, or aggregated data, which could be the average hemoglobin A1c for all the diabetics in a practice. We'll discuss QRDA and other related standards, such as the format for the queries themselves, in Lesson 9. For now it's just a standard vehicle for reporting the results of population or public health queries. >From these individual QRDA files, popHealth aggregates and summarizes quality metrics. Here, for example, are 500 patients from a 10 provider practice, in which, individual physicians could be using different EMRs. This practice might be under an outcome based contract, where their revenue is tied to meeting goals, for these quality metrics. Here, they're doing well on smoking screening, but not so well on weight screening, particularly for adults in the 18 to 64 group. This points out the need to identify the right group of patients for each metric. For example, you would not normally perform screening mammograms on men for breast cancer. And the recommendations for this screening are increasingly pointing to a specific age range for women. Note that in recognition of this key point, the numerator, typically, the number of patients who's care met criteria is in green for each metric, while the denominator, the applicable target patient group for the metric is in blue. In calculating a quality metric for smoking screening, what might be an appropriate denominator? Choose any and all that apply. Would it be all adult patients in the practice? All patients in the practice who smoke? All patients in the practice who quit smoking? All smoking patients in the practice who have been counseled to quit? The answer is all adult patients in the practice. This is screening for smoking, so the denominator, the group for which that screening should be performed, would be everyone who might be smoking, all adult patients in the practice. Where a practice level quality metric is of concern, the next step would typically be to drill down to the individual providers to see who might be the source of the problem. So PoP Health provides data at that level, as shown here. We see providers, and within them we see the actual individual persons. Here's someone who's doing very well on this screening metric for tobacco. Here's someone who's doing not nearly so well. Even though we're working at the population level, the results can be aggregated and reported for an individual patient as shown here. Such a report could be reviewed as part of a patient visit and makes the patient status easy to encompass at a glance. Care outside of guidelines is flagged here in red, to make that obvious. In lesson three, we studied the Indiana Health Information Exchange, as the premiere example of centralized health information exchange in the US. PopHealth is designed to deal with the reality in most other places, where there may not even be an HIE in place. You'd expect that IHIE could do more with their more facile access to data, and they do. As shown here, Quality Health First can support more sophisticated searches than would normally be found in a distributed framework using a simpler data model than the Indiana Health Information Exchange. A wide variety of patients subsets can be analyzed across all participating practice groups. Way back in lesson two, we discussed the difference between process and outcome measures using hemoglobin A1c as the exemplar. The process measure is whether it is done periodically according to guidelines and its actual value is a measure of the outcome of diabetes care. Here's a great example of the potential of quality reporting to add transparency to healthcare through public reporting of quality metrics. Suppose you have diabetes and want to make sure your provider cares for it properly. Where would you go to find that out? In general there's no good answer, but in Indiana many providers voluntarily report their data at the practice level. Here it is for the Hemoglobin A1c process metric. Remember, that means doing the test periodically as recommended. Three of the practices are below the standard for the state, as shown in red. Two are below their regional average, as shown in green. Here's the similar quality health first hemoglobin A1c outcome metric. Remember this means that the patient's hemoglobin A1c is below the threshold, in this case 9%. And again, you can see which practices are below the state or the regional average. What factor might be out of the control of a physician practice but could lead to their quality scores being either better or worse? My answer would be the mix of patients they see. Social, economic, and demographic factors, such as race or ethnicity, might influence the severity of the diseases they see, and their success in managing those diseases. With the growth and outcomes based contracting by insurance companies, Medicare, and even some major employers. Commercial companies now provide population health management tools. After our quick look at what they do, we'll interview Wellcentive based here in Atlanta to learn how they do it. Here, a report of overall practice performance for a list of practice-defined alerts illustrates the need to manage metrics specified in specific contracts. You see two metrics that are part of the United Healthcare contract. United Healthcare is a major national health plan. Here we see an analysis by provider for the use of lipid lowering drugs for patients at risk for coronary disease. This analysis is intended to help providers find patients whose treatment may fall outside of accepted standards of care. Here's a more detailed analysis. And an analysis of the relationship between the blood pressure of the patient and the use of lipid-lowering drugs illustrating the clinical detail that's possible when EMR data is made available in addition to the typical quality metrics used in population health. You know that Wellcentive collects clinical data in addition to the standard quality metrics we discussed and that are required for Meaningful use. What technical complexities does this create for them? I could provide two answers. One, increase privacy and security issues. And two, this clinical data won't be as standardized as quality metrics, necessitating more work to report it accurately in aggregate. Well Kirk, I appreciate you taking the time to be with us today. Definitely. You're the Chief Technology Officer at Wellcentive, a local company here in Atlanta. And the company has an interesting founding story. Could you briefly tell us about that? Yeah, absolutely. Two brother-in-laws were really talking over Christmas drinks. One of the brother-in-laws was Dr. Paul Taylor, and he was producing reports in Excel for payers, trying to summarize his own quality using claims data. And it was just way too cumbersome, very difficult, and inaccurate, and the other brother in law was working at Patient Care Technologies. Was a software designer and they realized, hey this is a trend that really needs to happen. Physicians need to be paid on quality long term. And there's absolutely a market here. They stayed small for four or five years, invested a lot in the product. Delivered real results to significant customers in the Michigan area. I joined about three years ago, and I had a unique history that followed along with their path, as well. I worked at MedQuist and M*Modal. There I saw 20 terabytes of rich text files of pure narrative, and we said, we have to tap into that information. But natural language processing and the different context that you might look at that narrative text in, offered many, many challenges that I felt would take a long time to really to get to the value that I wanted to get to. So, I then looked at a company called Outcomes Health Informations Solutions, and what they did, was they literally put nurses on site, picked up a paper medical record, scanned it, uploaded it to another bank of nurses that would read the paper, and then essentially abstract the information to answer a specific question that a payer would have. And payers would pay $80 a chart for that process to happen. It was a mess. An absolute nightmare of a workflow that led to lower data quality and higher cost. The whole time I was there, I said, can't we get this data out of the EHR? And lo and behold, three years ago, I found this company, Wellcentive, that was working on exactly that. Well that's a great story. In lesson ten we'll talk a bit more about feature selection out of natural language records. And I guess these nurses were the first generation of technology for doing that. Yep. Absolutely. So let's segue forward to today. Today you're gathering data from lots of doctor's practices and EMRs. Can you give us those numbers? Yeah, we have about 1,000 practices, 8,000 physicians. We have over 2,500 interfaces pumping data into our data center every day. We process about 200 million clinical data points every month. And one of my favorite stats is that we have sent 178 million transactions to payers that have resulted in a value based reimbursement payment. And it's all based on clinical quality. Okay, that sounds like a lot of stuff from a lot of sources. How do you do it? That's easier asked than answered, but it really comes down to being aggressive with respect to seeking out the data. So that means that for our customers, what we need to do is we need to go to them and say, you know what, I don't care what format of the data you have. Your system exports data somehow, some way, it's just a file format. People have been parsing files for a long, long time. So we just roll up our sleeves, we have a flexible platform that allows us to administer many different types of format parsing, many different types of protocols. And it creates a queue of processing and enterprise servers bus of processing that allows us to process and interpret that data as we hit our application, and put that data into a structured form in an application side. And I want to emphasize that you're not just collecting quality metrics, which the students should know are relatively standardized at this point. But you're collecting clinical data from EMRs. And of course, by now, the students have already seen that. They've seen some of the things you do. How do you normalize and standardize that clinical data when it comes from 200 different EMRs? The key to that is being what we say purpose driven. We don't try to boil the ocean and standardize every data point that we find in each document. We work with the provider organization to say what are your clinical quality goals? And how are you going to be paid based on this data? And we specifically look at all the file formats that they give us, and we pick out the pieces of data that relate to those clinical quality measures. And so that, first, simplifies the problem and allows us to really focus on data quality within that specific domain. Then we essentially maintain a dictionary of all code sets. And we have over 2.2 million mappings that we have curated over the past eight years that relate the customers custom dictionaries to all the national standard dictionaries that exist out there. Well, what about the patient in all of this? This is clinical data from the EMR. How do you manage the obvious privacy and security concerns? Well, it's very important. And I actually respect, a lot, the high tech act. When we went into our first HIPAA risk assessment, you get pretty nervous. But as you go through it you realize that it's a very healthy process. They have a long checklist of about 170 something regulations. That as we went through it, we just made sure that we hit on each of those and that we're diligent and respectful internally to our organization. Our people deal with clinical data all day long. They're helping our customers measure clinical quality. And so quite a bit of education within the company about how to handle that data responsibly is required. So, a patient like the make believe chronic, multi chronic disease patient, Marla, that we've created for this course. Whose going to lots of different specialists, becomes a real challenge to know where is her record? Whose caring for her? And how do you define quality in a patient like that? Yeah. That's very, very tricky. So one of the biggest problems that Marla can pose, is not going to the doctor at all. And how do you know if you're missing clinical data, maybe you just don't have that interface? Maybe you don't have that relationship with that doctor that she went to see? So, I think what's happening right now is provider organizations are a single legal structure or entity, they're starting to solve this problem internally. And it goes to some other topics like the roll out of direct and the roll of HIEs, is if you're solving this problem internally to that provider organization, that legal structure, how do the community structures that are outside of that organization participate? Well, you mentioned Direct, which is one of the technologies that I mentioned frequently here on the course. What do you see as the role of Direct in this complex mix of doctors, patients and quality metrics that you've laid out? Wellcentive is unique in that we're both in analytics and population health management, clinical quality, system but we're also meaningful you certified. We do that for a couple of reasons. One, because there's always a point of aspect to clinical quality. But, two, we have to play nicely with all of the standards that are out there with all the core point of care EHR. So that means, supporting direct and supporting standards like CCDA. The problem with the industry right now is that Direct is a very flexible technology once set up. There's quite a bit of setup, quite a bit of administration to getting it set up. And what we're seeing is that internally to that provider organization that we were talking about, it's much easier just to bring the data in directly from the EMRs and not worry so much about the protocols and the directed aspect of Direct, which means a provider can direct a chart to another provider. We see Direct as really being more the protocol for bringing data from external to that provider organization from the HIE, and those structures are really just getting adopted by the communities and by the provider organizations today. Well, one final question and then I'll let you go. Put on your crystal ball glasses, and tell us where you think we're going in the area of population health, quality measurement, and even outcome based incentives. Yeah, absolutely. This is a fascinating space. I'm very honored to be part of solving these problems in the industry. The most fundamental problem in the industry today is again saying how do we pay based on quality versus paying based on a fee for service action, a single action? And because the patients a consumer and they can go to many different places, you end up having data that contributes to the quality spread across multiple systems. So you then have to bring that data together to really assess the total quality. In this comes a lot of deep questions about who is responsible for each of the quality actions, but also, which record do I look at across these different EHRs or in the population of health management system to determine that quality action was taken. The FTC has created clinical integration network guidelines that essentially say that a provider organization cannot contract with payers unless they are clinically integrated. And what that means is that they, the organization has to behave as one unit from a clinical quality perspective. They have to be following the same standards, processes and procedures. That's easier said than done. And so workflows and health systems need to modify their internal structures and their internal investments over time to be able to even allow this work flow and the physician work flow to understand the entire spectrum of a longitudinal patient record. So what's happening there is that's getting refined. Once we achieve physician engagement in that process, and we have high data quality. I then expect patient engagement and the empowerment of the consumer in healthcare to really flourish. I think it's going to stagnate until then. But after that I really see patient engagement taking off and putting the power of the care but also the incentives into the patient's hand. Yeah I couldn't agree more. I think the whole role of the patient in patient engagement just keeps becoming more and more prominent as time goes on. Yes. Well I want to thank you again. Absolutely. You've not only given us an interesting technology story which weaves in many of the themes of the course. But a great entrepreneurial story. Two guys having a drink and now we have a major company in the population health space. So thank you very much. Absolutely. Thank you very much. >From a purely informatics perspective, many aspects of public health are similar to population health in that data is aggregated and reported. However, public health takes a broader view of the factors determining health and disease. To do this, as shown here, it uses disparate data sources to understand the impact of many factors and determinants of health and disease including the social, physical, environment, genetics and even governmental policies and programs. In the next lesson, we'll be looking in more detail at the technologies for aggregating data from diverse EHRs. A specific example is BioSense 2.0, a system developed at the Centers for Disease Control and Prevention, the CDC, here in Atlanta. An often cited example of the use of this technology is the Tarrant County Public Health Department in Fort Worth, Texas. Which uses it to collect and analyze data from 60 regional hospitals for a key and foundational role that public health plays. Syndromic surveillance, monitoring for disease outbreaks. Here you see as an example a time order display of the number of visits. Typically these would be emergency department visits for a particular clinical problem. The capability to produce this in a timely manner across an entire region is a powerful surveillance tool In this lesson, we've seen that data can be aggregated across EHRs to manage entire patient populations against the fine quality objectives. We've also seen that it can be used to support and enhance critical public health services, such as disease surveillance. Having seen what can be done, we now turn in lesson 9 to how it's done. The first is hQuery where the goal is simplicity, in part by working with a limited data set that is well standardized because of its key role in quality measurement and other common clinical activities. As you can see here, hQuery provides an attractive modern point-and-click interface called the hQuery composer to the query builder who might even be a nontechnical health care provider. At the same time, the possible queries that can be formulated are somewhat limited. HQuery uses a simple patient information model as shown here to facilitate query building by nontechnical users. Queries are forwarded to an hQuery gateway located at each source system which receives them in a standard format and forwards them to an adapter which knows how to translate the query for the source system, which might, for example, provide a CCDA document in response and can convert it into the standard data model for transmission back. You should watch a video to learn more. The URL is in the instructor's notes. HQuery presents its results in an attractive way that in this example includes a frequency distribution, a distribution over time, and a geographic distribution of the results. The hQuery adapter is solving a problem that should be familiar to you by now. What is it? The interoperability problem? The complex data standards problem? The curly braces problem? Or none of these? The answer is the curly braces problem. However, if you checked the interoperability problem, you're very close because effectively, the curly braces problem is a particular technical sub-issue within the interoperability challenge. Informatics for Integrating Biology and the Bedside, or i2b2 for short, is an NIH-funded effort based at Partners HealthCare System in Boston. Its mission is to enable clinical investigators to conduct research using state of the art genomics and biomedical informatics. As compared to hQuery, it's a far more sophisticated system to support complex research environments. But it provides an easily understood database schema that can be used to create a data warehouse, where clinical abstracts from an enterprise EHR can be stored for future analysis. This is important because the database scheme of commercial enterprise EHRs can be dauntingly complex, is usually proprietary, and it is not necessarily designed well to support ad hoc queries. i2b2 implementations are comprised of cells that communicate with each other via web services and together, are called a hive, as shown here. And no, Marla, it doesn't contain any honey. The role of most of these cells or web services should be clear from their names. Importantly, custom cells can be added to the hive. There's an organized i2b2 users group to share information and applications and to create the potential for translational research through collaboration in federated queries across institutions. What would you think is the role of the National Language Processing cell in the i2b2 hive you just saw? My answer would be extracting useful clinical concepts, these are often called features, from free text patient notes, and other free text information. We'll look at actual examples of this, in both, lessons six and ten. It's often desirable to bring together data from many sources and providers. But they may be cautious about participating because of concerns about how their data will be used and whether their institution might be portrayed negatively or look bad in some analyses. One solution, as depicted here for PopMedNet, is for each contributing institution to maintain control of its data in what is often called a data locker, but be capable of accepting and responding to queries that might go to many data sources. PopMedNet is intended to support medical product safety analysis and comparative effectiveness research of alternate treatments, both of which rest on obtaining data from multiple sources and other studies. For which of the following is simplicity the primary goal? Choose any and all that apply. Is it PopMedNet, i2b2, or hQuery? This was an easy one, of course the answer is hQuery. Which of these distributed query standards would likely be best suited for a research project aimed at discovering the phenotypic manifestations of genomic subtypes? You may have to look up some of these words. Would it be PopMedNet, i2b2, hQuery, or all of these? The answer of course, is i2b2. Which we described as being specifically designed for research around genomics, bioinformatics, and related data. This is another easy one, of course the answer is PopMedNet, which was described in exactly these terms. Earlier I mentioned Query Health as an umbrella effort. Here we can see a proposed architecture for that which utilizes all of the technologies we've just discussed. The goal is to facilitate cross-platform queries. The technical vehicle for that are distributed query standards we now turn to. There are four kinds of query standards, each of which specifies a different key element of distributed query. Envelope standards define the package for sending and receiving the queries. Format standards provide a declarative specification of the query. We'll explain what that means in the next exercise. Results standards such as QRDA we discussed in the last lesson specified the format and packaging of query results. Data model standards such as CEDD, Common Data Element Definitions specified the data model that will link data from the contributing systems. Before discussing the standards, we need to differentiate between a declarative versus a procedural specification. To do this, we'll use baking a cake. Well, you know how to do that, don't you? The declarative statement expresses the desired results, but does not say how to achieve it. The procedural statement provides a recipe for achieving the goal. Think back to the to the curly braces problem with the Arden Syntax. It's really the same thing. Arden provides a declarative standard but the procedure would be different depending on the system that is the source of the data. The hQuery Gateway similarly receives a standard query on one side, but knows how to get it done on the other. Which of the following is procedural, for a particular electronic device, such as a smartphone? Its quick start guide? Its technical specifications? A banner ad for it? None of these? Or all of these? The answer is a Quick Start guide, which provides the procedures for getting a device up and running, hopefully quickly. The Query Envelope standard serves to provide identification that is unique within the network. This includes the requester ID, their name, e-mail and organization. It also provides the purpose, priority and type of the query using seven purpose codes. For example, treat for treatment. Its priority from one to five, with one being the highest. And an alphanumeric type that can be as many as 20 characters. It identifies the level of PHI that's involved. Is this aggregated patient data? Limited data, de-identified patient data, or fully identified protected health information? And finally the timing, what's the submission date and time, and optionally, when should the query be executed. Queries are specified in a health quality measure format or HQMF for short. This is an XML standard for documenting the content and structure of a quality measure using an XML document formatted based on the HL7 REM. It consists of three levels of detail. Metadata, such as who wrote it, the dates over which it is valid, who validated it, and other details about how the measure works or is used. Human narrative, including measured description, data criteria, the measure population, and measure observations. Finally, computer instructions As to how to count and compute the results of the measure. This is a small part of the XML in a HQMF query specification. In this case, it provides instructions to the computer to only include patients who have had their weight measured. This might define the numerator of a weight screening quality metric. That as you recall from lesson two, is a requirement in meaningful use. We introduce QRDA, the standard for query results, in lesson eight. You should recall that the results can be reported at three levels of detail. The patient level, a list of patients, or an aggregated level across a group of patients. In this example of QRDA XML, two templates are utilized to specify NQF defined quality measures for inpatient asthma care and the use of systemic corticosteroids for inpatient asthma patients. Each of which, of course, has its own complex definition. The Clinical Element Data Dictionary, or CEDD, is a tool for implementers to use in setting up their source data in support of distributed queries within a larger query health solution. This is not intended as a new standard development effort, and began with the elements already specified in the CCD in which most MER's already support since they were required for stage one certification. Here's a simple example of the patient allergy data element showing how it ties to already-accepted standards used in the CCD. Which of these query standards would tell a computer how to calculate the percentage of a providers adult population that had been screened for smoking status? Choose any and all that apply. Would it be QRDA, HQMF, or CEDD? The answer, of course, is HQMF, which is the standard we just discussed for specifying how to calculate a quality query. In this lesson we examined in more detail exactly how data from multiple EHR systems can be queried and aggregated for diverse purposes, from quality reporting to advanced clinical research. All of these technologies serve the essential role of providing a framework over the many non inoperable EHR systems deployed today. Taken together they illustrate different technological solutions, each of which is optimized for a specific problem in a specific cross institutional context. Over the last few lessons, we've advanced from electronic record systems for providers and patients to technologies for querying those systems and aggregating data for potentially interesting purposes. In our final lesson, we'll see just how interesting some of those results can be as we delve into what is arguably the most dynamic area of Health Informatics today, big data and analytics.