Welcome to Graduate Introduction to Operating Systems. My name is Ada Gavrilovska, and I will be your instructor for this course. I've been a research faculty at Georgia Tech since 2004 when I finished my PhD here. And in these years, I've been involved in and led a number of research projects in the area of systems that specialize on topics related to operating systems, virtualization, or other aspects of system software. In these years, I've also taught a number of times at Georgia Tech. This particular class, I've taught it over a dozen times, and over the years, we've evolved it to update it to reflect any changes in technology or any changes in the student interest or the student population in general. I've also been teaching the advanced operating systems class and the special topics class in high performance communication. For additional information about me, you can look at my website, and the link is provided off of the instructor notes. Some of you may be asking yourselves why should you take this class. So, let me give you a little bit of context how this class came to be. Most of the masters and PhD students at Georgia Tech want to or need to take a, the Advanced Operating Systems class 60210, and likely, many of you will consider taking it in the future. But as the title of that class suggests, Advanced Operating Systems, that course deals with a number of advanced topics related to operating systems. However, over the years, the population of the graduate students at Georgia Tech has diversified a lot. And a lot of the students that come to us either don't come with formal operation systems training, so they may not have seen some of the more introductory topics related to operating systems, or maybe it's simply been a number of years since they've looked at that content. To address this gap, we offer this course, CS 8803 Graduate Introduction to Operating Systems. The intent with this course is to teach for the first time, or review, operating systems basics. In terms of metrics, once you finish this course, you should be able to answer every single one of the questions on the diagnostics test that's associated with the Advanced Operating Systems course. To begin talking about this course, let's look at the learning goals. In this course, you will learn what operating systems are, why are they needed, and how are they designed and implemented. This will include discussion of key mechanisms and abstractions that operating systems must support to provide the required functionality. In this course, we will cover a number of topics in depth. In particular, we will describe the design and implementation of operating systems' abstractions, mechanisms, and policies related to several of the key types of functionality provided by operating systems. This includes processes and process management, threads and concurrency, and in general, challenges related to multithreading. OS functionality for managing hardware resources like CPU and memory. OS services for communication and I/O, which includes file I/O. OS support for distributed services, including remote procedure calls. We call those RPCMs. Distributed file systems, distributed shared memory. And we will briefly review systems software that's common in data center and cloud environments. To fully understand and appreciate OS internals, we will supplement the theoretical part of the course material with a practical component, and that will be a sequence of programming projects. The goal of these projects is to give you a hands-on experience with operating systems. For instance, you will be put face-to-face with challenges related to multithreading, concurrency, and synchronization. You will have to use operating system services for interaction among multiple processes on a single node, like a server machine. You will also have to look beyond the single node and prototype distributed, or multi-node services, using some of the mechanisms that are supported at the operating system level, like remote procedure calls. Finally, one key issue with operating systems design and implementation is the performance that they can deliver. You will gain experience with designing and performing experimental evaluations. These experiments will ideally provide you with insight into the behavior of the operating system so you can better understand its performance capabilities, overheads, limitations, or other issues. Each of the projects will require you to program in the C programming language in a Linux environment, and you will have to use some of the standard Linux libraries, such as Pthreads, for threads. For the theoretical component of this course there's no required textbook. It is recommended, however, that you have access to some solid operating system textbook as an additional reference. There are a number of options out there from a variety of authors. And there are also many books that are available online, including with completely free content. My personal favorites are the Silberschatz operating systems books, often called the dinosaur books because of the dinosaur on the cover. There is one that includes more detail, Operating System Concepts, and then an abbreviated one, Operating System Concepts Essentials. I also like the Tanenbaum textbook called Modern Operating Systems. And to this one, I have personally contributed to the Linux case study chapter. If you look online, each of these books has a number of available online resources. There's some slides for the chapters. There's some free available text online. And there are also affordable Kindle editions that are for rent even. And there is also a lot of material that's freely available online. There's one recent title prepared by Andrea and Remzi Arpaci-Disseai. It's called Operating Systems Three Easy Pieces. This is a good book and its content matched fairly well with the content that we'll cover in this course. For certain lectures, I will also provide you with specific references from which the lecture content is drawn. Mostly these will be research papers, and some of them can be quite old. But they're still quite seminal works in computer science. Less often there will be some tutorials, manuals on specific OS technologies, or even technology surveys. In any case, all of these resources will be linked to in the Instructor Notes or in the Course Wiki online. Finally, regarding the programming aspect of this course, I said that you will be required to program in C in a Linux environment and that your programming assignments will require multi threaded programming. To help with these topics there are a number of resources online for C programming, for threading libraries such as PThreads. And we'll make sure to provide links to some such content with your programming assignment Next, I want to make sure you're aware of the online wiki for this course. It has information such as the class syllabus, lecture schedule, project schedule, various resource links that are useful for the class. It also has grading criteria and other policies that are valid for the course. Let me now explain the grading policy in this class. Your grade will roughly be split between the theoretical and the practical part. The theoretical part, you will have two exams, a mid-term and a final. The final will not be cumulative and these will be worth 25 and 30% of your grade. Your projects will be worth 40% of your grade, so these are your programming assignments. and you will have 5% of your grade come from class participation. Given that the class is online, for class participation, we expect you to participate in Piazza, the online forum that we'll use in this class. How well you answer to questions posted by other students, how often you raise an interesting and useful topics for discussion in the forum, and also how well you do on the quizzes on the Audacity platform that will be incorporated in the class lessons. You can always monitor your grades in T-Square. For the exams, we'll use an online service called ProcterU. As the name suggests, this is a service that proctors you during the examination and insures academic honesty. The tests are to be taken individually, and they will be closed notes, closed book exams. For the projects, you can work in teams. Across teams, you can have high level discussions about your design approach or implementation approach. But, you're not allowed to share code, results, text for the final report. You should have these discussions thru Piazza, so both the core staff, as well other students can help you. Before we get started, you may have noticed these cute, wooden blocks on my desk. They belong to my daughter, but I'm borrowing them for this course. And during each lesson, I will illustrate some more complex concepts related to operating systems using a metaphor that's based on toys and a toy shop example. To give you a high-level idea of how this will work, we will think of the hardware in computer system as the toy shop with its tools and work spaces and storage areas. Then we will think of all of the processes and applications that execute on hardware as all of the toys that are being produced in the toy shop. And then central to this course will be that we will drawing be analogies between operating systems and the toy shop manager that oversees everything that's happening in the toy shop. This will become much clearer in the following classes. For now, this concludes the introduction. I sincerely hope you will enjoy this class. Good luck and have fun. Here's your first quiz. We would like to know what your expectations are for this course. So please answer the following. What do you expect to learn in this course? Also please feel free to mention any of your prior experience, goals, or even fears about the class. In this introductory lecture, we will provide a very high-level view of operating systems. We will explain what is an operating system and what is the role that it plays in computer systems. Then we'll look at some of the key components of operating systems, design principles, and organizational structures. The following lectures in this class will provide you with a much more in-depth understanding of what any of the terms and mechanisms that come up during this lesson actually are. In simplest terms, an operating system is a piece of software that abstracts and arbitrates the underlying hardware system. And in this context, abstract means to, to simplify what the hardware actually looks like. And arbitrate, it means to manage, to oversee, to control the hardware use. We will see throughout this course that operating system support a number of abstractions and arbitration mechanisms for the various types of hardware components in computer systems. While this entire course will cover many elements of an operating system, it would be nice to have a simple illustration of what an operating system is like. So, to give a visual metaphor using our toy theme, we can say that an operating system is like a toy shop manager. At a high level, how is a toy shop manager like an operating system? First, a toy shop manager directs operational resources in the shop. Second, a toy shop manager enforces working policies. And finally, a toy shop manager mitigates the difficulty of complex tasks. For instance, for any toy shop manager, they must be in charge of their employees and direct their working efforts towards completing the work. Additionally, a toy shop manager is responsible for scheduling, for allocating, for distributing the resources in the shop, the parts and the tools, and for determining which of the employees will work on which orders and use which tools. Toy shop managers are also the ones that enforce working policies in the toy shop. This can be policies regarding fairness, safety, clean-up. For instance, how workers working on two different orders have to share one of the shared resources, shared parts and tools for instance. Another example is policy regarding how to clean up after yourselves once a worker is done with processing a toy order. Finally, a toy shop manager helps mitigate the effects of complex tasks. It does that by trying to make the overall operation of the toy shop more optimized and more simple. Without a manger, for instance, employers may not know how to establish order among themselves, how to decide, how to handle the workload. So the toy shop manager has to deal with this situation so as to avoid dealing with a much more complex task. So, the question then is, what parallels are there between a toy shop manager with an operating system? Operating systems do direct operating resources. In particular, they control the use of the hardware, CPU, memory, peripheral devices such as disk, network cards, etc. And they decide how these resources with be allocated to applications. Along those lines, operating systems actually enforce some policies. This can include polices regarding how these resources are used, for instance, to control fair access to the shared resources. Or it can be even to impose certain limits to allocate maximum amount of a certain resource that a particular application or process can use. Examples of such limits are, for instance, the number of open files that can be open per process, or some thresholds that need to be passed in order for some memory management daemons to kick in. There are numerous other examples of limiting resource usage. And finally, the operating system helps with the difficulty of complex tasks. And particularly important is that it simplifies the view of the underlying hardware that's observed by the applications that are running on that particular platform. Instead, applications interact via system calls with the operating system, and the operating system takes care of managing those difficult tasks for them. So, what is an operating system and what is the operating system's role in computing systems? Let's start taking a look at what a computing system looks like and how it's used. Computing systems consist of a number of hardware components. This includes one or more processing element. So, processors or CPUs. Today, the CPUs further consist of multiple cores. So, a single CPU will have more than one processing element. And there's also memory, and there are also devices like network interconnects for connecting to the Internet, like your Ethernet port or your Wifi card. Other components include graphics processing cards, like GPUs for instance. And also storage devices like hard disks, SSDs, flash devices like USB drives. Except for in some very specific environments like certain embedded platforms or sensors, all of these hardware components will be used by multiple applications. For instance, on your laptops or desktops, you may currently be running a browser, a text editor, perhaps Skype. You may have a number of other applications. In data centers, server machines also run also multiple applications, maybe a web server, a database, or some computation intensive simulation, many other options. An operating system is the layer of systems software that sits between the complex hardware and all of these applications. There isn't one formal definition of what an operating system is. So instead, we will look at the role that the operating system serves and the functionality that it must provide. And in that way, we will try to build our understanding of what an operating system is. First, the operating system hides the complexities of the underlying hardware from both the applications, as well as the application developers. You don't want to worry about disk sectors or blocks when you're saving the output of a computation in your program. For instance, the operating system hides the complexities of the various types of storage devices, so hard disks, SSDs, USB flash. And it manages a higher level abstraction of file, and it provides some number of operations, like reads and writes, that you can perform on that file. Similarly, for a web server application, when you're accepting and responding to end user requests, you don't want to think about the bits and the packets that you need to compose when performing that communication. Instead, the operating system abstracts the networking resource. It provides a higher level abstraction that is typically called a socket. And then it provides some services to send and receive packets or data from that socket. Furthermore, the operating system not only hides the hardware complexity, but it actually also manages the underlying hardware on behalf of the executing applications. For instance, the operating system decides how many and which one of these resources will the application use. For instance, the operating system allocates memory for these applications, and also is the one that schedules these applications on to the CPU that can execute them. It also controls access of these applications to the various devices in the system. So overall, it's responsible for all types of resource allocations and resource management tasks on behalf of these applications. Finally, when multiple applications are co-running on the same hardware, the operating system must ensure that each of them can make adequate progress and that they don't hurt each other. So, it will provide isolation and protection. For instance, the operating system will allocate to different applications different parts of the underlying physical memory. And it will also make sure that unless explicitly intended, applications don't access each other's memory. This is important both from a protection perspective so they don't read each other data, and it's also important from an isolation perspective so that they don't end up overwriting each other's memory. Note that these types of mechanisms are important even in environments that were traditionally considered embedded platforms like mobile phones, where hardware was typically running just one application. Think about it. How many applications do you currently have running on your smartphone? On my phone, I have a browser, I have Skype, I have Facebook. And I still have that one key application, and that is the actual phone, making phone calls and receiving phone calls. In summary, an operating system is a layer of systems software that resides between applications and hardware. It corresponds to the software that has privileges to access the underlying hardware and manipulates its state. In contrast, software that corresponds to the applications is not allowed to do that. Its role is to hide hardware complexity and to manage the hardware on behalf of one or more applications. And it has to do that according to some predefined policies. And finally, it has to ensure that applications are isolated and protected from one another. Now that we have been given a definition, let's take a quiz to see what types of components do we expect an operating system to have. The question is, which of the following are likely components of an operating system? The options are a file editor, a file system, a device driver, cache memory, a web browser, and a scheduler. You should check all that apply. Starting from the top, a file editor is likely not a part of an operating system because the users interact with it directly, and it's not involved directly in managing hardware. Next, the file system is likely a part of an operating system. It's directly responsible for hiding hardware complexity and for exporting a simpler, more intuitive abstractions. A file, as opposed to block of disk storage. Device drivers are also likely part of an operating system. A device driver is directly responsible for making decisions regarding the usage of the hardware devices. Cache memory is a little bit tricky. Although the operating system and the application software utilize cache memory for performance, the OS doesn't directly manage the cache. It's really the hardware that manages it itself. Web browsers are also not part of an operating system. Again, just like in the file editor case, it's an application that users interact with and does not have direct control over underlying hardware. And finally, the scheduler. This is indeed a part of the operating system because it's responsible for distributing the access to the processing element, the CPU, among all of the applications that share that platform. In the previous morsel, it was stated that an operating system abstracts and arbitrates the use of the computer system. For the following options, indicate if they're examples of an abstraction, where the operating system simplifies something about the underlying hardware, or arbitration, where the operating system manages the underlying hardware. Here are the options. Distributing memory between multiple processes. Supporting different types of speakers. Or, providing interchangeable access of hard disk or SSD. For the first option, distributing memory between processes, that's an arbitration. This is something an operating system does as a result of its effort to manage the memory and determine how multiple processes will share. The second option, supporting different types of speakers, that's an abstraction. It is because the operating system provides abstractions such as this one that you can plug in one set of speakers, and if they don't work, exchange them with something else. In some cases, drivers are required, which enables an operating system to control the hardware device without knowing details about that specific hardware, so the device driver will have the knowledge of the specific actual hardware element, like the specific speaker. And along similar lines, the ability to interchangeably access different types of storage devices like hard disks or SSDs is again an example of an abstraction just like the example above. Again, because of the use of the storage abstraction that operating systems support, they can underneath feel the different types of devices and hide that from the applications. Now that we understand what an operating system is, we can ask ourselves, what are some examples of actual operating systems? The examples of real systems out there differ based on the environment that they target. For instance, certain operating systems target more of the desktop environment o.r the server environment. Others target more of the embedded environment. Yet another set of operating systems target ultra high end machines like mainframes. But we'll focus on these environments, the desktop and embedded in our discussions just because they're most common. And also with these examples we'll really focus on more recent, more current really operating systems, as opposed to those that have been around or that have evolved over the last 60 plus years of computer science. For desktop operating systems one of the very popular ones is Microsoft Windows. It is been a well-known operating system since the early 1980s. Next there's a family of operating systems that extended from the operating system that originated at Bell Labs in the late 1960s, and these are all referred to as UNIX-based operating systems. This involves the Mac OS X operating system for Apple devices, and this extends the UNIX BSD kernel, and BSD here is really Berkley System Distribution of Unix. And Linux is another very popular UNIX like system, and it is open sourced, and it comes bundled with many popular software libraries. There are in fact many different versions of Linux. Ubuntu, CentOS, etc. On the embedded side, we've recently seen bigger proliferation of different operating systems, just because of the rising number of smartphones and user devices, like tablets, and now even smaller form factor devices. First you're probably very familiar with Android. It's an embedded form of Linux that runs on many of these types of devices. And its versions come with funny names like Ice Cream Sandwich and KitKat. Next we have iOS and that's the, Apple proprietary operating system for devices like iPhones and iPads. Then there is Symbian, and then there are other less population options. In each of these operating systems there are a number of unique choices in their design and implementation, and in this class we will particularly focus on Linux. So the majority of more illustrative, more in-depth examples will be given based on the Linux operating system. To achieve its goals, an operating system supports a number of higher-level abstractions, and then a number of key mechanisms that operate on top of these instructions. For instance, some of these abstractions, like process and thread, correspond to the applications that the operating system executes. Some corresponding mechanisms would be mechanisms to create, to launch an application to start executing, or to schedule it to actually run on the CPU. Other OS abstractions like file or socket that we've mentioned before or memory page, they may more closely correspond to the hardware resources that the operating systems need to manage. Storage device like disk, or a network card for the socket, or the actual memories, so, memory pages in abstraction abstract memory as a resource. To operate on these abstractions, the operating system may incorporate mechanisms to open gain access to a particular device or hardware component, to write to it, to update its state, to allocate to make sure that a particular application has access to that resource. These are some examples of mechanisms. Operating systems may also integrate specific policies that determine exactly how these mechanisms will be used to manage the underlying hardware. For instance, a policy can control what is the maximum number of sockets that a process can actually have access to. Or they may control which data will be removed from physical memory, for instance, based on some algorithm like least-recently used. Let's look at an example. And, for instance, we said one of the responsibilities of the operating system is to manage resources like memory. So, we'll look at a memory management based example. To do that, the operating system uses a memory page as an abstraction. And this abstraction corresponds to some addressable region of memory of some fixed size, for instance, four k. The operating system also integrates a number of mechanisms to operate on that page. It can allocate that page in DRAM, and it can map that page into the address piece of the process. By doing that it, allows the process to access the actual physical memory that corresponds to the contents of that page. In fact, over time, this page may be moved across different locations in physical memory. Or, it sometimes may even be stored on disk, if we need to make room for some other content in physical memory. This last one brings us to the third element, policies. Since it is faster to access data from memory then on disk, the operating system must have some policies to decide whether the contents of this page will be stored in physical memory or copied on disk. And, a common policy that operating systems incorporate is one that decides that the pages that have been least recently used over a period of time are the ones that will no longer be in physical memory, and instead will be copied on this. We refer to this also as swappings. So, we swap the pages. It's no longer in physical memory, it's in disk. The rational for that is that pages that have not been accessed in a while, so the least recently used ones, are likely not to be as important, or likely will not even be used any time in the near future. And, that's why we can afford to copy them on disk. The ones that have been accessed more frequently are likely more important, or likely recurrently working on that particular part of the content, so we will continue accessing them, and that's why we maintain them in memory. Let's look at some good guiding policies when thinking about how to design an operating system. The first one we call separation between mechanisms and policy. What this means is that we want to incorporate into the operating system a number of flexible mechanisms that can support a range of policies. For memory management, some useful policies would include least recently used, or least frequently used, or completely random. So what that means is that in the operating system, we'd like to have some mechanism to track the frequency or the, the time when memory locations have been accessed. This will help us keep track of when a page was last used or when a page was least frequently used. Or we can completely ignore that information. But the bottom line is we can implement any one of these policies in terms of how that memory management is going to operate. And the reason is that in different settings, different policies make more sense. This leads us to the second principle, which is optimize for the common case. What this means is that we need to understand a number of questions, how the operating system will, will be used, the, what it will need to provide in order to understand what the common case is. This includes understanding where will it be used, what kind of machine it will run on, how many processing elements does it have, how many CPUs, how much memory, what kinds of devices. And we also need to understand what are the common things that the end users will do on that machine. What are the applications that they will execute, and also what are the requirements of that workload? So how does that workload behave? We need to understand the common case, and then based on that common case, pick a specific policy that makes sense and that can be supported given the underlying mechanisms and abstractions that the operating system supports. To achieve its role of controlling and managing hardware resources on behalf of applications, the operating system must have special privileges, as the definition pointed out, to have direct access to the hardware. Computer platforms distinguish between at least two modes, privileged kernel mode, and unprivileged or user mode. Because an operating system must have direct hardware access, the operating system must operate in privileged kernel mode. Note the rectangle labeled Mm, this means main memory, and I will use this simplified drawing of memory and CPU throughout this course. The applications in turn operate in unprivileged or user mode. Hardware access can be performed only from kernel mode by the operating system kernel. Crossing from user level into kernel level and the other way around, or in general, distinguishing between the two is supported by the hardware on most modern platforms. For instance, when in kernel mode, a special bit is set in the CPU, and if this bit is set, any instruction that directly manipulates hardware is permitted to execute. When in user mode, this bit is not set, and such instructions that attempt to perform privileged operations will be forbidden. In fact, such attempts to perform a privileged operation when in user mode will cause a trap. The application will be interrupted, and the hardware will switch the control to the operating system at a specific location. At that point, the operating system will have a chance to check what caused that trap to occur, and then to verify if it should grant that access or if it should perhaps terminate the process if it was trying to perform something illegal. In addition to this trap method, the interaction between the applications and the operating system can be via these system call interface. The operating systems export a system call interface. So, the set of operations that the applications can explicitly invoke if they want the operating system to perform certain service and to perform certain privileged access on their behalf. Examples would be open to perform access to a file, or send to perform access to a socket, or malloc to allocate memory, many others. And operating systems also support signals, which is a mechanism for the operating system to pass notifications into the applications. And I will talk about these in a later lesson. Let's talk a little bit more about system calls. Using this diagram I'm going to trace through where control and data are exchanged during a system call. I will use this icon to denote where I am in the diagram. We will start by assuming we are currently in an executing user process. Then because the user process needs some hardware access, it makes a system call. On a system call, control is passed to the operating system, in privileged mode, and the operating system will perform the operation and then it will return the results to the process. Executing a system call involves changing the execution context from the user process to that of the OS kernel, also passing arguments, whatever necessary for the system cooperation. And then jumping somewhere in the memory of the kernel so that you can go through the instruction sequence that corresponds to that system call. With the system call, control is passed to the operating system. The operating system operates in privileged mode. And it's allowed to perform whatever operation was specified in the system call. Once the system call completes, it returns the result and the control back to calling process. And this again will involve changing the execution context from, now from kernel mode into the user mode, passing any arguments back into the user address space, and then, jumping to the exact same location in the execution of the user process where the system call was being made from. But the entire process involved changing the execution context from user to kernel mode and back, passing arguments, jumping around in memory to locations where the code to be executed is. So, it's not necessarily a cheap operation. To make a system call, an application must write arguments, save all relevant data at a well-defined location, make the actual system call using this specific system call number. The well-defined location is necessary so that the operating system kernel, based on the system call number, can determine which, how many arguments it should retrieve and where are they, at this well-defined location? The arguments can either be passed directly between the user program and the operating system, or they can be passed indirectly by specifying their address. In synchronous mode, the process will wait until the system call completes. I will talk about an alternative where we can issue asynchronous system calls, but that we will leave for a later discussion in this course. For now, you must understand that there are some basic steps involved in calling an operating system service and obtaining the results. In summary, user/kernel transitions are a necessary step during application execution. Applications may need to perform access to certain types of hardware. Or, may need to request change in the allocations of hardware resources that have been made to them. Only the operating system, the kernel, are allowed to perform those types of operations. The hardware provides support for performing user/kernel transitions. We explain that the hardware will cause a trap if the application from unprivileged mode tries to perform some instruction, or a memory access for which it doesn't have permissions. For instance, the application cannot change the contents of certain registers, and give itself more CPU, or more memory. Only the operating system can do that. The result of this trap is that the hardware initiates transfer of the control to the operating system, to the kernel, and marks this by that special privilege bit that we mentioned. At that point, once control is passed over to the operating system, the operating system can check what caused the trap, and determine what's the appropriate thing to do. Whether to grant or deny the specific request that caused the trap to occur in the first place. This will, of course, depend on the policies that are supported by the operating system. Performing all of this, despite of the fact that hardware provides support, still takes a number of instructions. For instance, on a two gigahertz machine running Linux,it can take 50 to 100 nanoseconds to perform all the operations that are necessary around a user/kernel transition. This is real time, real overhead for the system. The other problem with these transitions is they affect the hardware cache usage. The application performance is very dependent on the ability to use the hardware cache. If accessing cache is order a few cycles, accessing memory can be order of hundreds of cycles. When we perform a system call, or in general when we cross into the operating system, the operating system, while executing, will likely bring content that it needs in the cache. This will replace some of the application content that was in the hardware cache before that transition was performed. And, so this will have some impact on the application performance, because it will no longer be able to access its data in cache, it will have to go to memory. In summary, these user/kernel transitions, they're not cheap. An operating system provides applications with access to the underlying hardware. It does so by exporting a number of services. At the most basic level, these services are directly linked to some of the components of the hardware. For instance, there is a scheduling component that's responsible for controlling the access to the CPU, or maybe there are even multiple CPUs. The memory manager is responsible for allocating the underlying physical memory to one or more co-running applications. And it also has to make sure that multiple applications don't overwrite each other's accesses to memory. A block device driver is responsible for access to a block device like disk. In addition, the operating system also exports higher-level services that are linked with higher-level abstractions, as opposed to those that are linked with abstractions that really map to the hardware. For instance, the file is a useful abstraction that's supported by virtually all operating systems. And in principle, operating systems integrate file system as a service. In summary, the operating system will have to incorporate a number of services in order to provide applications and application developers with a number of useful types of functionality. This includes process management, file management, device management, and so forth. Operating systems make all of these services available via system calls. For example, here are some system calls in two popular operating systems, Windows and Unix. I will not read through this list, but notice although these are two very different operating systems, the types of system calls and the abstractions around those systems calls these two OSes provide are very similar. But process control, creating a process, exiting a process, waiting for object, creating files, etc. Because we have been discussing system calls, I would like for you to take a quiz. You will need to fill in the following statement. On a 64 bit Linux-based operating system, which system call is used to send a signal to a process? To set the group identity of a process? To mount a file system? Or to read/write system parameters? Please use only single word answers. For instance, just reboot, and also feel free to use the Internet. The answers to this quiz are as follows. To send a signal to a process, there is a system called kill. To set the group identity of a process, there is a system called SETGID. This is valid on 64 bit operating systems, on 16 or 32 bit systems, there is a variant SETGID 16, or SETGID 32. Mounting a file system is done via the mount system call. And finally reading or writing system parameters is done via the system control system call, SYSCTL. We saw so far some rough indications how an operating system is laid out. But let's now more explicitly look at different types of operating system organizations, and we will start with what we call a monolithic operating system. Historically, the operating system had a monolithic design. That's when every possible service that any one of the applications can require or that any type of hardware will demand is already part of the operating system. For instance, such a monolithic operating system may include several possible file systems, where one is specialized for, of sequential workloads where the workload is sequentially accessing files when reading and writing them. And then maybe other file system that's optimized for random I/O. For instance, this is common with databases. There isn't necessarily a sequential access there. Rather, each database operation can randomly access any portion of the backing file. This would clearly make the operating system potentially really, really large. The benefit of this approach is that everything is included in the operating system. The abstractions, all the services, and everything is packaged at the same time. And because of that, there's some possibilities for some compile-time optimizations. The downside is that there is too much state, too much code that's hard to maintain, debug, upgrade. And then its large size also poses large memory requirements, and that can always impact the performance that the applications are able to observe. A more common approach today is the modular approach, as with the Linux operating system. This kind of operating system has a number of basic services and EPIs already part of it, but more importantly, as the name suggests, everything can be added as a module. With this approach, you can easily customize which particular file system or scheduler the operating system uses. This is possible because the operating system specifies certain interfaces that any module must implement in order to be part of the operating system. And then dynamically, depending on the workload, we can install a module that implements this interface in a way that makes sense for this workload. Like, if these are database applications, we may run the file system that's optimized for random file access. And if these are some other types of computations, we may run the file system that's optimized for sequential access. And most importantly, we can dynamically install new modules in the operating system. The benefits of this approach is that it's easier to maintain an upgrade. It also has a smaller code base and it's less resource intensive, which means that it will leave more resources more memory for the applications. This can lead to better performance as well. The downside of this approach is that all the modularity may be good for maintainability. The level of interaction that it requires, because we have to go through this interface specification before we actually go into the implementation of a particular service. This can reduce some opportunities for optimizations. Ultimately, this can have some impact on performance, though, typically, not very significant. Maintenance, however, can still be an issue given that these modules may come from completely disparate code bases and can be a source of bugs. But overall, this approach delivers significant improvements over the monolithic approach, and it's the one that's more commonly used today. Another example of OS design is what we call a microkernel. Microkernels only require the most basic primitives at the operating system level. For instance, at the OS level, the microkernel can support some basic services such as to represent an executing application, its address space, and its context, so a thread. Everything else, all other software components, applications like databases, as well as software that we typically think of as an operating system component, like file systems, device drivers, everything else will run outside of the operating system kernel at user level, at unprivileged level. For this reason, this microkernel-based organization of operating systems requires lots of inter-process interactions. So typically, the microkernel itself will support inter-process communications as one of its core abstractions and mechanisms, along with address spaces and threads. The benefits of a microkernel is that it's, it's very small. This can not only lead to lower overheads and better performance, but it may be very easy to, to verify, to test that the code exactly behaves as it should. And this makes microkernels valuable in some environments where it's particularly critical for the operating systems to behave properly, like some embedded devices or certain control systems. These are some examples where microkernels are common. The downsides of the microkernel design are that although it is small, its portability is sort of questionable because it is typically very specialized, very customized to the underlying hardware. The fact that there may be more one-off versions of a microkernel specialized for different platforms makes it maybe harder to find common components of software, and that leads to software complexity as well. And finally, the fact that we have these very frequent interactions between different processes, these different user-level applications, means that there is a need for frequent user/kernel crossings. And we said already that these can get costly. Let's look at some popular operating systems, starting with the Linux architecture. This is what the Linux environment looks like. Starting at the bottom, we have the hardware, and the Linux kernel abstracts and manages that hardware by supporting a number of abstractions and the associated mechanisms. Then come a number of standard libraries, such as those that implement the system call interfaces, followed by a number of utility programs that make it easier for users and developers to interact with the operating system. And, finally, at the very top, you have the user developed applications. The kernel, itself, consists of several logical components, like all of the the I/O management, memory management, process management. And, these have well defined functionality, as well as interfaces. Each of these separate components can be independently modified or replaced. And, this makes the modular approach in Linux possible. The Mac OSX operating system, from Apple, uses a different organization. At the core is the Mac micro kernel and this implements key primitives like memory management, thread scheduling and interprocess communication mechanisms including for, what we call RPC. The BSD component provides Unix interoperability via a BSD command line interface, POSIX API support as well as network I/O. All application environments sit above this layer. The bottom two modules are environments for development of drivers, and also for kernel modules that can be dynamically loaded into the kernel. In this lesson, we answered the big question, what is an operating system? And we saw that it's important because it helps abstract and arbitrate the use of the underlying hardware system. We explained that to achieve this, an operating system relies on a number of abstractions, like processes and threads, a number of mechanisms that allow it to manipulate those abstractions, and a number of policies that specify how those abstractions can be modified. We saw that operating systems support a system call interface that allows applications to interact with them. We looked at several alternatives in organizational structures for operating systems. Then very briefly, we looked at some specific examples of operating systems, Windows, Linux, and Mac OS to see some examples of their system call interfaces or their organization. As the final quiz, please tell us what you learned in this lesson. Also, we'd love to hear your feedback on how we might improve this lesson in the future. One of the key abstractions that operating systems support is that of a process. In this lecture, I will explain what is a process, how an operating system represents a process, and also what an operating system must do in order to manage one or more processes, particularly when multiple processes share a single physical platform. Before we begin, let's define what a process is. In the simplest terms, a process is an instance of an executing program. Sometimes it makes sense to use the terms task or job interchangeably with a process. We will use again a visual metaphor to describe what a process is. Continuing with a toy shop as an example, you can think of a process as an order of toys. An order of toys has its state of execution, it requires some parts, and a temporary holding area, and even may require some special hardware. For instance, it's state of execution, may include the completed toys, the toys that are waiting to be built, that are part of that order, and other things. Building the toys may require various parts, like plastic pieces, wooden pieces, and these come in different containers, or we may require some other temporary holding area for the pieces. And, finally, to actually build a toy, we may need some special hardware. We may need sewing machines, glue guns, or other types of tools. So, how does all of this then compare to a process in an operating system? Well, a process also has a state of execution described with the program counter, the stack pointer. All this information is used by the operating system to decide how to schedule the process, how to swap between multiple processes, and for other management tasks. In order to execute, the process needs some data. There's some state in registers. And, it also has some temporary holding area. For instance, it occupies state in memory. Finally, executing a process may require some special hardware like I/O devices like discs, or network devices. The operating system has to manage these devices and control which of the processes that are perhaps executing concurrently at the same time gets access to which hardware components. This is similar to what would happen in a toy shop where the toy shop manager has to control how the special hardware, like the sewing machine or the glue gun, are used. Which particular order of toys will get to be assigned the usage of these more designated hardware components. Let's talk now, more specifically, about processes. And, we'll start first by understanding, what is a process? To do this, recall that one of the roles of the operating system is to manage the hardware on behalf of applications. An application is a program that's on disk, in flash memory, even in the cloud. But it's not executing, it's a static entity. Here, for instance, in this picture, we have some application that's stored on disk. Once an application is launched, it's loaded in memory here, and it starts executing. Then it becomes a process. So a process is an active entity. If the same program is launched more than once, then multiple processes will be created. These processes will executing the same program, but potentially will have very different state. In fact, very likely they will have very different state. For instance, a process can be one instance of the word editor program. Here, you're displaying some notes from a previous lecture. And perhaps you're just reviewing it, you're not really modifying this. And then you can have a second process, another instance of the exact same word editor program to take notes from this lecture. Given that we just started, this probably doesn't have many notes, so it has relatively small state, and it may have some unsaved edits. So, process therefore represents the execution state of an active application. It doesn't mean necessarily that it's running. It may be waiting on input like user input to type in certain notes. Or it may be waiting for another process that's currently running on the CPU, in case there's just one CPU in the system. So what does a process look like? A process encapsulates all of this data for running application. This includes the code, the data, all of the variables that that application needs to allocate. Every single element of the process state has to be uniquely identified by its address. So an OS abstraction used to encapsulate all of the process state is an address space. This is what we have here. The address space is defined by a range of addresses from V0 to some Vmax, and different types of process state will appear in different regions in this address space. What are the different types of state in a process? First we have the code, the text, and the data that is available when the process is first initialized. So all of this is static state that's available when the process first loads. Then during the execution, the process dynamically creates some state, allocates memory, stores them per our results, reads data from files. This part of the address space we call a heap. In this picture here, the heap is shown as contiguous portion of the address space starting immediately after the data, but in reality there may be holes in this space. It may not be contiguous. There may be portions of it that don't have any meaning for that particular process and, in fact, the process isn't even allowed to access them. I will talk in a little bit how the operating system knows what's okay for the process to access versus what isn't. Another very important part of the address space is what we call a stack. It's a dynamic part of the address space state, in that it grows and shrinks during execution, but it does so in a last-in, first-out order. Whatever you put on the stack will be the very first item to be returned when you're trying to read from the stack. Consider for instance we're executing a particular portion of the process. And now we need to call some procedure to jump to a different part of the address space. We want to make sure that the state that we were in at this point of the execution, before we called this other procedure, is saved, and then that it will be restored once we come back from the execution. We can place the state on the stack and jump to execute this portion of the code. So the procedure y. When we're finished with y, the state x will be popped from the stack and we can continue the execution in the same state that we were in before the call to y was made. There are lots of points during a process execution where the last-in, first-out behavior is very useful. So the stack is a very useful data structure. As a whole, we refer to this process representation as an address space. We said earlier that the potential range of addresses from v0 to vmax represents the maximum size of the process address space. And we call these addresses virtual addresses. So these, between v0 and vmax are the addresses that are used by the process to reference some of its state. We call these addresses virtual, because they don't have to correspond to actual locations in the physical memory. Instead, the memory management hardware and operating system components responsible for memory management, like page tables maintain a mapping between the virtual addresses and the physical addresses. By using this type of mapping, we decouple the layout of the data in the virtual address space, which may be complex and it may depend on the specifics of the application or the tools that we used, like how the compiler chose to lay that data out. That's completely decoupled with how that data is laid out in physical memory. And that will allow us to maintain physical memory management simple and not in any way dictate it by the data layout or processes that are executing. For instance, the variable x may be at a location 03c5 in the virtual address space. And this may correspond to a completely different address, 0f0f in physical memory. The way this happens is when the process requests some memory to be allocated to it at a particular virtual address. The address of the physical memory that the operating system actually allocates may be completely different, and instead of notifying the process about the details of where exactly in memory that variable really is. The operating system will create a mapping between this virtual address, 03c5, and the physical address, 0f0f, where x actually is. So then whenever the process tries to access x, this mapping is referenced, and in reality the exact physical location where x is will be accessed. As long as the mapping between 03c5 and 0f0f is present in this mapping table, this is a page table and this is a page table entry, any access of the process to x will, in fact, access the correct physical location where x is stored. We said already not all processes require the entire address space from V0 to VMax. There may be portions of this address space that are not allocated. Also, we may simply not have enough physical memory to store all this state even if we do need it. For instance, if we have virtual addresses that are 32 bits long, this address space can have up to 4 gigabytes. And if we have several such processes running at the same time, even in a pretty expensive machine, we will quickly run out of physical memory. To deal with this, the operating system dynamically decides which portion of which address space will be present where in physical memory. For instance, inside a system with processes P1 and P2, they may share the physical memory in this manner. So, the regions marked with yellow belong to P1, and the regions marked with pink belong to process P2. Both P1 and P2 may have some portions of their address space not present in memory but rather swapped temporarily on disk. And this portion of the address space will be brought in whenever it's needed. And perhaps that will cause some other parts of either P1's or P2's address space to be swapped to disk to make room. So the operating system must maintain information where these virtual addresses actually are in memory, on disk since it maintains the mapping between the virtual addresses and the physical location of every part of the process address space. I will talk about memory management in a later lesson, but at the very least, you must understand that for each process, the operating system must maintain some information regarding the process address space. We mentioned the page tables for instance. And then the operating system uses this information to both maintain mappings between the virtual addresses and the physical location where the state is actually stored. And also to check the validity of accesses of memory to make sure that a process is actually allowed to perform a memory access. To review this, let's take a quiz. If two processes, P1 and P2, are running at the same time, what are the ranges of their virtual address space that they will have? The first choice is P1 has address ranges from 0 to 32,000, and P2 from 32,001 until 64,000. The second choice is that both P1 and P2 have address ranges from 0 to 64,000. And the last choice, P1 has an address space range from 32,001 to 64,000, and P2 has address ranges from 0 to 32,000. So the reverse from the first one. So go ahead and mark all the ones that you think are correct answers. The correct answer is the second one. Both P1 and P2 can have the exact same virtual address space range from 0 to 64,000 in this case. The operating system underneath will map P1's virtual addresses to some physical locations, and P2's virtual addresses to other physical locations. The fact that we have decoupled the virtual addresses that are used by the processes from the physical addresses where data actually is makes it possible for different processes to have the exact same address space range and to use the exact same addresses. The OS will make sure that they point to distinct physical memory locations if that's what's required. For an operating system to manage processes, it has to have some kind of idea of what they are doing. If the operating system stops a process, it must know what it was doing when it was stopped so that it can restart it from the exact same point. So how does an operating system know what a process is doing? Let's think about the underlying hardware, the CPU, and think how it executes applications. Applications, before they can execute, their source code must be compiled, and a binary is produced. The binary is a sequence of instructions, and they're not necessarily executed sequentially. There may be some jumps, loops, or even there may be interrupts that will interrupt the execution of the binary. At any given point of time, the CPU needs to know where in this instruction sequence the process currently is. So we call this the program counter, PC. The program counter is actually maintained on the CPU while the process is executing in a register. And there are other registers that are maintained on the CPU. This whole value is necessary during the execution. They may have information like addresses for data. Or they may have some status information that somehow affects the execution of the sequence. So these are also part of the state of the process. Another piece of state that defines what a process is doing is the process stack. And the top of the stack is defined by the stack pointer. We need to know the top of the stack because we said the stack exhibits this last in, first out behavior, so whatever item was the last one to come on top of the stack needs to be the very first item that we can retrieve from the stack. But the stack pointer maintains this information. And similarly, there are other bits and pieces of information that help the operating system know what a process is actually doing at a particular point of time. To maintain all of this useful information for every single process, the operating system maintains what we call a process control block, or a PCB. Let's see now what is a Process Control Block. A Process Control Block is a data structure that the operating system maintains for every one of the processes that it manages. >From what we saw so far, the Process Control Block must contain process state like the program counter, the stack pointer, really, all of the CPU registers, their values,uh, as they relate to the particular process, various memory mappings that are necessary for the virtual to physical address translation for the process, and other things. Some of the other useful information includes a list of open files, for instance, information that's useful for scheduling, like how much time this particular process had executed in a CPU, how much time it should be allocated in the future. This depends on the process priority, etc. The Process Control Block data structure, or PCB as we call it, is created when the process is initially created itself. And it's also initialized at that time. For instance, the program counter will be set to point to the very first instruction in that process. Certain fields of the process are updated whenever the process state changes. For instance, when a process requests more memory, the operating system will allocate more memory and will establish new valid virtual to physical memory mappings for this process. This will reflect the memory limits information as well as the information regarding the valid virtual address regions for this process. And this perhaps doesn't happen too often. Other fields of this PCB structure change pretty frequently. For instance, during the execution of the program, the program counter changes on every single instruction. We certainly don't want the operating system for every instruction that the process executes to have to spend time to write this new PCB value for the program counter. The way this is handled is that the CPU has a dedicated register, which it uses to track the current program counter for the currently executing process. This PC register will get automatically updated by the CPU on every new instruction. It is the operating system's job, however, to make sure to collect and save all the information that the CPU maintains for a process, and to store it in the Process Control Block structure whenever that particular process is no longer running on the CPU. Let's see what we mean by this. Let's assume the operating system manages two processes, P1 and P2. It has already created them and their Process Control Blocks, and these Process Control Blocks are stored somewhere in memory. Let's say P1 is currently running on the CPU, and P2 is idle. What this means, that P1 is running, is that the CPU registers currently hold a value that correspond to the state of P1. So, they will ultimately need to be stored in PCB of P1. Then at some point, the operating system decides to interrupt P1, so P1 will become idle. Now, what the operating system has to do, it has to save all the state information regarding P1, including the CPU registers, into the Process Control Block for P1. Next, the operating system must restore the state about process 2 so that process 2 can execute. What that means is that it has to update the CPU registers with values that correspond to those of the Program Control Block for process 2. If at some point during its execution, P2 needs more physical memory, it will make a request via the malloc call, for instance. And the operating system will allocate that memory and establish new virtual to physical address mappings for P2, and update as appropriate the control block data structure for process P2. When P2 is done executing, or when the operating system decides to interrupt P2, it will save all the information regarding P2 state in the Process Control Block for P2, and then it will restore the Process Control Block for P1. P1 will now be running, and the CPU registers will reflect the state of P1. Given that the value of the Process Control Block for P1 corresponds exactly to the values it had when we interrupted P1 earlier, that means that P1 will resume its execution at the exact same point where it was interrupted earlier by the operating system. Each time the swapping between processes is performed, the operating system performs what we call context switch. Recall our illustration that shows how the operating system swaps between P1 and P2 for them to share the CPU. In this illustration, the process control blocks for P1 and P2 reside in memory. And the values of the CPU will change depending on which process is currently executing. Now we can more formally state that a context switch is the mechanism used by the operating system to switch the execution from the context of one process to the context of another process. In our diagram, this is happening both when the operating system switches from the execution of P1 to the execution of P2. And then again a second time when the OS switches from the execution of P2 back to the execution of P1. This operation can be expensive, and that's for two reasons. First, there are direct costs, and this is basically the number of cycles that have to be executed to simply load and store all the values of the process control blocks to and from memory. There are also indirect costs. When process 1 is running on the CPU, a lot of its data is going to be stored into the processor cache. As long as P1 is executing, a lot of its data is likely going to be present somewhere in the processor cache hierarchy. In the picture, we show a single processor cache, but in practice, modern CPUs have a hierarchy of caches from L1 to L2, down to the last level cache. And each one is larger, but potentially slower than the previous one. More importantly, however, accessing this cache is much, much faster than accessing the memory. For instance, the accesses along the processor cache hierarchy will be on the order of cycles, whereas the accesses to memory will be on the order of hundreds of cycles, for instance. When the data we need is present in the cache, in this case, that's P1's data, we call this that the cache is hot. But when we context switch to P2, some, or even all, of the data belonging to P1 in the cache will be replaced to make room for the data needed by P2. So, the next time P1 is scheduled to execute, its data will not be in the cache. It will have to spend much more time to read data from memory, so it will incur cache misses. We call this the cold cache. Running with a cold cache is clearly bad because every single data access requires much longer latency to memory and it slows down the execution of the process. As a result, we clearly want to limit the frequency with which content switching is done. Here's a quick quiz about the processor cache. For the following sentence, check all options that correctly complete it. The sentence start says, when a cache is hot, and here are the choices. When a cache is hot, it can malfunction, so we must context switch to another process. When a cache is hot most process data is in the cache, so the process performance will be at its best. Or, the last choice, when a cache is hot sometimes we must context switch The first option implies that the hot cache means that the cache is physically getting hot, then it will malfunction. However, the term hot cache has nothing to do with the actual temperature of the cache. It merely refers that many of the cache accesses will actually resolve in a cache hit. The data will be found and cached. So in this context, the more cache hits means that the cache is hot. Now coincidentally, this also will lead to a rise in temperature. However, the effects of that aren't going to be that the operating system will context switch to another process. Modern systems and platforms do have a lot of mechanisms to deal with temperature rises, but that's beyond the scope of this lecture. Let's look at the second option. The second option is actually the most correct one. If data is present in the cache, it will be accessed much faster than if data is accessed from memory. So, executing with a hot cache actually corresponds to the state when the process performance is at its best. And unfortunately, three is correct as well. Although hot cache means best performance, sometimes we must context switch although the process cache is hot. And that's because there is another process that maybe has higher priority that needs to execute. Or maybe we have a policy where we have to timeshare the CPU between two processes and P1's time has expired, so we have to context switch and give the CPU to P2. During the context switch discussion, we said that P1 and P2 were going back and forth between running and idling. So they were in two states. They were either running or idling. When a process is running, it can be interrupted and context-switched. At this point, the process is idle, but it's in what we call a ready state. It is ready to execute, except it is not the current process that is running from the CPU. At some later point, the scheduler would schedule that process again, and it will start executing on the CPU, so it will move into the running state. What other states can a process be in? And how is that determined? To answer that question, let's look at a general illustration of the states that a process is going through throughout its life cycle. Initially, when a process is created, it enters the new state. This is when the OS will perform admission control, and if it's determined that it's okay, the operating system will allocate and initiate a process control block and some initial resources for this process. Provided that there are some minimum available resources, the process is admitted, and at that point, it is ready to start executing. It is ready to start executing, but it isn't actually executing on the CPU. It will have to wait in this ready state until the scheduler is ready to move it into a running state when it schedules it on the CPU. So, once the scheduler gives the CPU to a ready process, that ready process is in the running state. And from here, a number of things can happen. First, the running process can be interrupted so that a context switch is performed. This would move the running process back into the ready state. Another possibility is that a running process may need to initiate some longer operation, like reading data from disk or to wait on some event like a timer or input from a keyboard. At that point, the process enters a waiting state. When the event occurs or the I/O operation completes, the process will become ready again. Finally, when a running process finishes all operations in the program or when it encounters some kind of error, it will exit. It will return the appropriate exit code, either success or error, and at that point, the process is terminated. Let's take a quiz now. Using the process life cycle diagram, let's answer the following question. The CPU is able to execute a process when the process is in which of the following states? You'll need to check all that apply and here are the choices. Running, ready, waiting, or new. A running process is already executing, so it should be marked as a correct answer. Any of the processes that are in ready state, the CPU is able to execute them. They're just waiting for the operating system's scheduler to schedule them on the CPU. You should remember that as soon as a ready process is scheduled on the CPU, it will continue its execution from the very first instruction that's pointed by the process program counter. It is possible that this is the very first instruction in the process, if the process entered the ready queue for the first time after being newly created. And the other option is that it's some other random instruction in the process binary, depending on when the process was interrupted last time. Either when it was interrupted by the scheduler or because it had to stop executing since it had to wait on an I/O or some kind of external event. You may be asking yourself, how are processes created? What came first? In operating systems, a process can create child processes. In this diagram here, you see that all processes will come from a single root, and they will have some relationship to one another where the creating process is the parent and the created process is the child of that parent. Some of these will be privileged processes. They will be root processes. In fact, this is how most operating systems work. Once the initial boot process is done and the operating system is loaded on the machine, it will create some number of initial processes. When a user logs into a system, a user shell process is created. And then when the user types in commands, like list or emacs, then new processes get spawned from that shell parent process. So the final relationship looks like this tree. Most operating systems support two basic mechanisms for process creation, fork and exec. A process can create a child via either one of these mechanisms. With the fork mechanism, the operating system will create a new Process Control Block for the child. And then it will copy the exact same values from the parent Process Control into the child Process Control Block. At that point, both the parent and the child will continue their execution at the instruction that's immediately after the fork. And this is because both the parent and the child have the exact same values in their Process Control Block, and this includes the value of the program counter. So, after the operating system completes the fork, both of these processes will start their execution at the exact same point. Exec behaves differently. It will take a Process Control Block structure created via fork, but it will not leave its values to match the parent's values like with fork. Instead, the operating system place the child's image. It will load a new program. And the child's PCB will now point to values or describe values that describe this new program. In particular, the program counter for the child will now point to the first instruction of the new program. Now, the behavior of actually creating a new program is like, you call a fork, where the fork creates the initial process. And then you call an exec, which replaces the child's image, the image that was created in the fork, with the image of this new program. Since we have been talking about process creation, let's take a quiz about some special parent processes. The first question is, on UNIX-based operating systems, which process is often regarded as the parent of all processes? And the second question, which is not required but it's extra credit, on the Android OS, which process is regarded as the parent of all App processes? Feel free to use the Internet to find the answer for these questions On UNIX-based systems, init is the first process that starts after the system boots. And because all other processes can ultimately be traced to init, it's referred to as the parent of all processes. On the Android OS, Zygote is a daemon process which has the single purpose of launching app processes. The OS accomplishes this by forking the Zygote process every time a new app needs to be created, so the Zygote process is the parent of all of the apps. Let's talk about process scheduling next. For the CPU to start executing a process, the process must be ready first. The problem is, however, there will be multiple ready processes waiting in the ready queue. How do we pick what is the right process that should be given the CPU next, that should be scheduled on the CPU? This is a simple diagram where we have our ready queue with several processes waiting in it. Here's the CPU which has currently one process scheduled on it. So the question is, which process do we run next? This is determined by a component called a CPU scheduler. The CPU scheduler is an operating system component that manages how processes use the CPU resources. It decides which one of the currently ready processes will be dispatched to the CPU so that it can start running, start using the CPU. And it also determines how long this process should be allowed to run for. Over time this means that in order to manage the CPU, the operating system must be able to preempt, to interrupt the executing process and save its current context. This operation is called preemption. Then the operating system must run the scheduling algorithm, in order to choose one of the ready processes that should be run next. And finally, once the process is chosen, the OS must dispatch this process on to the CPU and switch into its context so that process can finally start executing. Given that the CPU resources are precious, the operating system needs to make sure that CPU time is spent running processes and not executing scheduling algorithms and other operating system operations. So, it should minimize the amount of time that it takes to perform these tasks. The operating system must be efficient in that respect. What that means is that it is important to have both efficient designs as well as sufficient implementations of the various algorithms that are used, for instance in scheduling. As well as efficient data structures that are used to represent things like the waiting processes or any information that's relevant to make scheduling decisions. This includes information about the priority of the processes, about their history, like how long that they ran in the past, other information may be also useful. Another issue to consider is how often do we run the scheduler? The more frequently we run it the more CPU time is wasted on running the scheduler versus running application processes. So, another way to ask this same question is how long should a process run? The longer we run a process, the less frequently we are invoking the scheduler to execute. Consider this scenario in which we are running processes for amount of time Tp, and the scheduler takes some amount of time Tsched to execute. If you want to understand how well the CPU was utilized, we have to divide the total processing time that was performed during an interval. So during this interval that was 2 times Tp and then divide that by the total duration of the interval. So the total duration of the interval is 2 times Tp plus 2 times the scheduling interval. If the processing time and the scheduling time are equal as in this picture, that means that only 50% of the CPU time is spent on useful work. Half of the time during this interval, the CPU was basically doing systems processing work, scheduling, and that time should be considered overhead. Let's now look at the second interval, where the processing time Tp is much larger than the scheduling time. And let's assume that it's actually 10 times the scheduling time, not to scale. So if we work out the math here, we will find out that almost 91% of the CPU time was spent on actually doing useful work. So we're doing much better in this interval in terms of the efficiency of the CPU. How much of it is used for useful application processing versus in this previous time. In these examples, Tp refers to the time that's allocated to a process that has been scheduled to run. And so the time that that process can consume on the CPU. We refer to this time as the timeslice. As you can see there are a lot of decisions and tradeoffs that we must make when we're considering how to design a scheduler. Some of these include deciding what are appropriate timeslice values for instance, or deciding what would be good metrics that are useful when the scheduler is choosing what's the next process it should run. I will discuss these design issues in a later lesson. But for now you need to be aware that some decisions need to be made. Before we move forward, we need to consider how I/O operations affect scheduling. So far, we know the operating system manages how processes access resources on the hardware platform. And this in addition to the CPU and memory will include I/O devices, peripherals like keyboards, network cards, disks, et cetera. So in this diagram, imagine a process had made an I/O request. The operating system delivered that request. For instance, it was a read request to disk. And then plays the process on the I/O queue that's associated with that particular disk device. So the process is now waiting in the I/O queue. The process will remain waiting in the queue until the device completes the operations, so the I/O event is complete, and responds to that particular request. So once the I/O request is met, the process is ready to run again, and depending on the current load in the system, it may be placed in the ready queue. Or it may be actually scheduled on the CPU if there's nothing else waiting in the ready queue before it. So to summarize, a process can make its way into the ready queue in a number of ways. A process which was waiting on an I/O event ultimately found its way into the ready queue. A process which was running on the CPU, but its time slice expired goes back on the ready queue. When a new process is created via the fork call, it ultimately ends its way on the ready queue. Or a process which is waiting for an interrupt, once the interrupt occurs, it will also be placed on the ready queue. To make sure you understand the responsibilities of a CPU scheduler, let's take a quiz. The question is, which of the following are not a responsibility of the CPU scheduler? The options are, maintaining the I/O queue, maintaining the ready queue, deciding when to context switch, or deciding when to generate an event that a process is waiting on. You should pick all that apply. Let's see what the correct answers are. So which of the following are not a responsibility of the scheduler? First, the scheduler has no control over when I/O operations occur. So clearly the first choice should be marked. One exception are the timer interrupts. Depending on the scheduling algorithm, the scheduler chooses when a process will be interrupted, so when it will context switch, so clearly it has some influence over when events based on the timer interrupt will be generated. This also answers the third question. It is the scheduler, based on the scheduling algorithm, that decides when a process should be context switched, so this clearly is responsibility of the scheduler. Similarly, it is the scheduler that's in charge of maintaining the ready queue. It is the one that decides which one of the processes in the ready queue will be scheduled to execute next. And finally, the scheduler really has no control over when external events can be generated, other than the timer interrupt as we discussed. So it has no control over events that a process may be waiting on. So this choice should be marked as well. Another natural question can be, can processes interact? And the simple answer to this is yes. An operating system must provide mechanisms to allow processes to interact with one another. And today in fact, more and more of the applications we see are actually structured as multiple processes. So these multiple processes have to be able to interact to contribute to a common goal of a more complex multi-process application. For instance, here's an example of a web application consisting of two processes on the same machine. The first one is the web server, the front-end, that accepts the customer request. And the second one is the backend, the database that stores customer profiles and other information. This is a very common case in many enterprise and web applications. So, how may these processes interact? Now, before we answer that, remember that the operating systems go through a great deal to protect and isolate processes from one another. Each of them is a separate address space. They control the amount of CPU each process gets, which memory is allocated, and accessible to each process. So these communication mechanisms that we will talk about somehow have to be built around those protection mechanisms. These kinds of mechanisms are called inter-process communication mechanisms, or we refer to them as IPC. The IPC mechanisms help transfer data and information from one address space to another, while continuing to maintain the protection and isolation that operating systems are trying to enforce. Also, different types of interactions between processes may exhibit different properties. Periodic data exchanges, continuous stream of data flowing between the processes or coordinated at the, to some shared single piece of information. Because of all these differences, IPC mechanisms need to provide flexibility as well as clearly performance. One mechanism that operating systems support is message passing IPC. The operating system establishes a communication channel, like a shared buffer for instance, and the processes interact with it by writing or sending a message into that buffer. Or, reading or receiving a message from that shared communication channel. So, it's message passing because every process has to put the information that it wants to send to the other process, explicitly in a message and then to send it to this dedicated communication channel. The benefits of this approach is that it's really the operating system who will manage this channel, and it's the operating system that provides the exact same APIs, the exact same system calls for writing or sending data, and the reading or receiving data from this communication channel. The downside is the overhead. Every single piece of information that we want to pass between these two processes we have to copy from the user space of the first process into this channel that's sitting in the OS, in the kernel memory. And then back into the address space of the second process. The other type of IPC mechanism is what we call shared memory IPC. The way this works is the operating system establishes the shared memory channel, and then it maps it into the address space of both processes. The processes are then allowed to directly read and write from this memory, as if they would to any memory location that's part of their virtual address space. So the operating system is completely out of the way in this case. That in fact is the main advantage of this type of IPC. That the operating system is not in the fast path of the communication. So the processes, while they're communicating are not going to incur any kind of overheads from the operating system. The disadvantage of this approach is because the operating system is out of the way it no longer supports fixed and well defined APIs how this particular shared memory region is used. For that reason, its usage sometimes becomes more error prone, or developers simply have to re-implement code to use this shared memory region in a correct way. Let's provide a little bit of more information through this shared memory quiz. Let's look at this statement. Shared memory-based communication performs better than message passing communication. So, you think this statement is true? It is false? Or, whether it depends on something The correct answer to this is, it depends. With shared memory based communication, the individual data exchange may be cheap, because they don't require that the data is copied in and out of the kernel. However, the actual operation of mapping memory between two processes, that operation itself is expensive. So, it only makes sense to do shared memory-based communication if that cost, the setup cost, can be amortized across a sufficiently large number of messages. That's why the real answer is, it depends. In this lesson, we'll learned how a process is represented by operating systems. We learned how process is laid out in memory, how operating systems use the process control block structure to maintain information about a process during its lifetime. We looked at some of the key mechanisms that operating systems support to manage processes, like process creation and process scheduling. And then finally, we reviewed some aspects of memory management that are necessary for your understanding of some of the decisions and overheads that are associated with process management. As the final quiz, please tell us what you learned in this lesson. Also, we'd love to hear your feedback on how we might improve this lesson in the future. During the last lecture, we talked about processes and process management. We said that a process is represented with its address space and its execution context. And this has registers and stack and stack pointer. This type of process, the process that's represented in this way, can only execute at one CPU at a given point of time. If we want a process to be able to execute on multiple CPUs, to take advantage of multi-CPU systems, of multi-core systems today, that process has to have multiple execution contexts. We call such execution context within a single process, threads. In this lecture, we will explain what threads are, and how they differ from processes. We will talk about some of the basic data structures and mechanisms that are needed to implement threads, and to manage and coordinate their execution. We will use Birrell's paper, An Introduction to Programming with Threads, to explain some of these concepts. And this is an excellent paper that describes fundamentals of multithreading, synchronization, and concurrency. In a later lesson, we will talk about a concrete multithreading system called Pthreads. In this lesson we're talking about threads and concurrency. So how am I going to visualize these concepts? Well, one way you may think of threads is that each thread is like a worker in a toy shop. But what qualities make a worker in a toy shop similar to a thread? The first quality is that a worker in a toy shop is an active entity. Secondly, a worker in a toy shop works simultaneously with others. And finally, a worker in a toy shop requires coordination. Especially when multiple workers are working at the same time. And perhaps even contributing to the same toy order. Let's elaborate on these points now. A worker is an active entity in the sense that it's executing a unit of work that is necessary for a given toy order. Many such workers can contribute to the entire effort required for an actual toy to be built. Next the worker can simultaneously work with others, this is pretty straightforward. You can imagine a shop floor with many workers, all simultaneously are concurrently hammering, sewing, building toys at the same time. They are working on the same order or others. And finally, while the workers can work simultaneously, this comes with some restrictions. Workers must coordinate their efforts in order to operate efficiently. For instance, workers may have to share tools, they may have to share some working areas, their workstations, or even parts while they're in the process of making toys and executing the toy orders. Now that we know about workers in a toy shop, what about threads? How do they fit into this analogy? First, threads are also active entities. Except in this case, threads execute a unit of work on behalf of a process. Next, threads can also work simultaneously with others, and this is where the term concurrency really applies. For instance, in modern systems that have multiple processors, multiple cores, you can have multiple threads really at the exact same time executing concurrently. But this obviously will require some level of coordination. And specifically when we talk about coordination, we're really mainly talking about coordinating access to the underlying platform resources. Sharing of I/O devices, the CPU course, memory, all of these and other systems resources must be carefully controlled and scheduled by the operating system. This begs the question, how do we determine which thread gets access to these resources? And as you will see in this lesson, the answer to the question is very important, designed decision for both operating systems, as well as software developers in general. Let's try to understand better the differences between a process and a thread. To recap from the previous lessons, a single thread of process is represented by its address space. The address space will contain all of the virtual to physical address mappings for the process, for its code, its data. Keep section files for everything. The process is also represented by its execution context that contains information about the values of the registers, the stack pointer, program counter, etc. The operating system represents all this information in a process control block. Threads, we said, represent multiple, independent execution contexts. They're part of the same virtual address space, which means that they will share all of the virtual to physical address mappings. They will share all the code, data, files. However, they will potentially execute different instructions, access different portions of that address space, operate on different portions of the input, and differ in other ways. This means that each thread will need to have a different program counter, stack pointer, stack, thread-specific registers. So we will have, for each and every thread, we will have to have separate data structures to represent this per-thread information. The operating system representation of such a multithreaded process will be a more complex process control block structure than what we saw before. This will contain all of the information that's shared among all the threads. So, the virtual address mappings, description about the code and data, etc. And it will also have separate information about every single one of the execution contexts that are part of that process Lets now discuss why are threads useful. We will do that by looking at an example on a multiprocessor or a multicore system. At any given point of time when running a single process, there may be multiple threads belonging to the process, each running concurrently on a different processor. One possibility is that each thread executes the same code, but for a different subset of the input. For instance for a different portion of input array or an input matrix. For instance, T1 processes this portion of the input matrix. T2 processes the next one and so forth. Now, although the threads execute the exact same code, they're not necessarily executing the exact same instruction at a single point in time. So, every single one of them will still need to have its own private copy of the stack, registers, program counters, et cetera. By parallelizing the program in this manner we achieve speed up. We can process the input much faster than if only a single thread on a single CPU had to process the entire matrix. Next, threads may execute completely different portions of the program. For instance you may designate certain threads for certain input/output tasks like input processing or display rendering. Or another option is to have different threads operate on different portions of the code that correspond to specific functions. For instance in a large web service application different threads can handle different types of customer requests. By specializing different threads to run different tasks or different portions of the program, we can differentiate how we manage those threads. So, for instance, we can give higher priority to those threads that handle more important tasks or more important customers. Another important benefit of partitioning what exactly are the operations executed by each thread, and on each CPU, comes from the fact that performance is dependent on how much state can be present in the processor cache. In this picture, each thread running on a different processor has access to its own processor cache. If the thread repeatedly executes a smaller portion of the code, so just one task, more of that state than of that program will be actually present in the cache. So one benefit from specialization is that we end up executing with a hotter cache. And that translates to gains in performance. You may ask, why not just write a multi-process application where every single processor runs a separate process? If we do that, since the processes do not share an address space we have to allocate for every single one of these contacts address space and execution context. So, the memory requirements, if this were a multi-process implementation, would be that we have to have four address space allocations and four execution context allocations. A multi-threaded implementation results in threads sharing an address space so we don't need to allocate memory for all of the address space information for these remaining execution contexts. This implies that a multi-threaded application is more memory efficient. It has lower memory requirements than its multiprocessor alternative. As a result of that, the application is more likely to fit in memory and not required as many swaps from disk compared to a multi-processed alternative. Another issue is that communicating data, passing data among processes or among processes requires interprocess communication mechanisms that are more costly. As we'll see later in this lecture, communication and synchronization among threads in a single process is performed via shared variables in that same process address spaced. So it does not require that same kind of costly interprocess communication. In summary, multithreaded programs are more efficient in their resource requirements than multiprocess programs and incur lower overheads for their inter thread communication then the corresponding interprocess alternatives. One question that you can ask is also, are threads useful on a single CPU? Or even more generally, are threads useful when the number of threads is greater than the number of CPUs? To answer this question, let's consider a situation where a single thread, T1, makes a disk request. As soon as the request comes in, the disk needs some amount of time to move the disk spindle to get to the appropriate data and respond to the request. Let's call this time t_idle. During this time, thread 1 has nothing to do but wait. So the CPU is idle and does nothing. If this idle time is sufficiently longer than the time it takes to make a context switch, then it starts making sense to perhaps context switch from thread one to some other thread, T2, and have that thread do something useful. Or specifically rather, we need the execution context that's waiting on some event to be waiting for an amount of time that's longer than, really, two context switches. So that it would make sense to switch to another thread. Have that thread perform some operation. And then switch back. So basically as long the time to context switch, t context switch, is such that t idle is greater than twice the time to context switch, it makes sense to context switch to another thread and hide the idling time. Now this is true for both processes and threads. However, recall from the last lesson, we said that one of the most costly steps during the context switch is the time that's required to create the new virtual to physical address mappings for the new process that, that will be scheduled. Given that threads share an address space. When we're context switching among threads, it is not necessary to recreate new virtual to physical address mappings. So in the threads case, this costly step is avoided. For this reason, the time to contact switch among threads is less to contact switch among processes. The shorter the contact switching time is there will be more of these t_idle situations when a thread is idling where it will make sense to contact switch to another thread and hide the wasted, the idling time. Therefore, multithreading is especially useful because it allow us to hide more of the latency that's associated with IO operations and this is useful even in a single CPU. There are benefits of multithreading both to applications when we have multithreaded applications, but also to the operating system, itself. By multithreading the operating system's kernel, we allow the operating system to support multiple execution contexts. And this is particularly useful when there are, in fact, multiple CPUs, so that the operating system context can execute concurrently on different CPUs in a multiprocessor or multicore platform. The operating system's threads may run on behalf of certain applications. Or, they may also run some operating system level services, like certain daemons or device drivers. To make sure we understand some of the basic differences between a process and a thread, let's take a look at a quiz. You'll need to answer if the following statements apply to processes, threads, or both. And please mark your answers in the text boxes. The first statement is, can share a virtual address space. Next, take longer to context switch. The third one, have an execution context. The fourth one, usually result in hotter caches when multiple such entities exist. And then the last statement, make use of some communication mechanisms. The first statement applies to threads. Each thread belonging to a process shares the virtual address space with other threads in that process. And because threads share an address space, the context switch among them happens faster than processes. So, processes take longer to context switch. Both threads and processes have their execution context described with stack and registers. Because threads share the virtual address space, it is more likely that when multiple threads execute concurrently, the data that's needed by one thread is already in the cache, brought in by another thread. So, they typically result in hotter caches. Among processes, such sharing is really not possible. And then the last answer is B. We already saw that for processes, it makes sense for the operating system to support certain interprocess communication mechanisms. And we'll see that there are mechanisms for threads to communicate and coordinate and synchronize amongst each other. Now that we know what threads are, what is it that we need in order to support them? First, we must have some data structure that will allow us to distinguish a thread from a process. This data structure should allow us to identify a specific thread and to keep track of their resource usage. Then we must have some mechanisms to create and to manage threads. In addition, we also need mechanisms to allow threads to coordinate amongst each other. Especially when there are certain dependencies between their execution when these threads are executing concurrently. For instance, we need to make sure that threads that execute concurrently don't overwrite each other's inputs or each other's results. Or we need mechanisms that allow one thread to wait on results that should be produced by some other thread. Well, when thinking about the type of coordination that's needed between threads, we must first think about the issues associated with concurrent execution. Let's first start by looking at processes. When processes run concurrently, they each operate within their own address space. The operating system together with the underlying hardware will make sure that no access from one address space is allowed to be performed on memory that belongs to another. Memory is, or state that belongs to the other address space. For instance, consider a physical address x that belongs to the process one address space. In that case, the mapping between the virtual address for p1 in process one and the physical address of x will be valid. Since the operating system is the one that establishes these mappings, the operating system will not have a valid mapping for any address from the address space of p2 to x. So from the p2 address space, we simply will not be able to perform a valid access to this physical location. Threads, on the other hand, share the same virtual-to-physical address mappings. So both T1 and T2, concurrently running as part of an address space, can both legally perform access to the same physical memory. And using the same virtual address on top of that. But this introduces some problems. If both T1 and T2 are allowed to access the data at the same time and modify it at the same time, then this could end up with some inconsistencies. One thread may try to read the data, while the other one is modifying it, so we just read some garbage. Or both threads are trying to update the data at the same time and their updates sort of overlap. This type of data race problem where multiple threads are accessing the same data at the same time is common in multithreaded environments, where threads execute concurrently. To deal with these concurrency issues, we need mechanisms for threads to execute in an exclusive manner. We call this mutual exclusion. Mutual exclusion is a mechanism where only one thread at a time is allowed to perform an operation. The remaining threads, if they want to perform the same operation, must wait their turn. The actual operation that must be performed in mutual exclusion may include some update to state or, in general, access to some data structure that's shared among all these threads. For this, Birrell and other threading systems, use what, what's called mutexes. In addition, it is also useful for threads to have a mechanism to wait on one another. And to exactly specify what are they waiting for. For instance a thread that's dealing with shipment processing must wait on all the items in a certain order to be processed before that order can be shipped. So it doesn't make sense to repeatedly check whether the remaining threads are done filling out the order. The thread just might as well wait until it's explicitly notified that the order is finalized so that it can at that point get up, pick up the order, and ship the package. Birrell talks about using so-called condition variables to handle this type of inter-thread coordination. We refer to both of these mechanisms as synchronization mechanisms. For completeness, Birrell also talks about mechanisms for waking up other threads from a wait state, but in this lesson, we will focus mostly on thread creation and these two synchronization mechanisms, mutexes and condition variables. We will discuss this issue a little bit more in following lessons. Let's first look at how threads should be represented by an operating system or a system library that provides multithreading support. And also what is necessary for thread creation. Remember, during this lesson we will base our discussion on the primitives that are described and used in Burrell's paper. These don't necessarily correspond to some interfaces that are available in the real threading systems or, or programming languages. And in our next lesson we will talk about Pthreads which is an example of a threading interface supported by most modern operating systems. So that will make the discussion a little bit more concrete. You can think of this lesson and the content of Burrell's paper as explaining this content at a more fundamental level. But first we need some data structure to represent a thread actually. The thread type proposed by Burrell is the data structure that contains all information that's specific to a thread and that can describe a thread. This includes the thread identifier that the threading system will use to identify a specific thread. Register values, in particular the program counter and the stack pointer, the stack of the thread, and any other thread specific data or attributes. These additional attributes for instance could be used by the thread management systems so that it can better decide how to schedule threads or how to debug, errors with threads or other aspects of thread management. For thread creation, Burrell proposes a fork call with two parameters, a proc argument, and that is the procedure that the created thread will start executing. And then args, which are the arguments for this procedure. This fork should not be confused with the Unix system called fork that we previously discussed. The Unix system call creates a new process and is an exact copy of the calling process. And here fork creates a new thread that will execute this procedure with these arguments. When a thread T0 calls a fork a new thread T1 is created. That means that new thread data structure of this type is created and its fields are initialized such that its program counter will point to first instruction of the procedure proc, and these arguments will be available on this stack of the thread. After the fork operation completes, the process as a whole has two threads. T0, the parent thread, and T1, and these can both execute concurrently. T0 will execute the next operation after the fork call, and T1 will start executing with the first instruction in proc, and with the specified arguments. So what happens when T1 finishes? Let's say it computed some result, as a result of proc, and now somehow it needs to return that result. Or, it may be just some, some status of the computation like success or error. One programming practice would be to store the results of the computation in some well-defined location in the address space that's accessible to all the threads. And they have some mechanism to notify either the parent or some other thread that the result is now available. More generally, however, we need some mechanism to determine that a thread is done, and if necessary, to retrieve its result, or at least to determine the status of the computation, the success or the error of its processing. For instance, we can have an application where the parent thread does nothing but create a bunch of child threads that process different portion of an input array. And the parent thread will still have to wait until all of its children finish the processing before it can exit so as not to force their early termination, for instance. To deal with this issue, Burrell proposes a mechanism he calls Join. It has the following semantic. When the parent thread calls join with the thread ID of the child thread, it will be blocked until the child completes. Join will return to the parent the result of the child's computation. At that point the child, for real, exits the system, any allocated data structures saved for the child, all of the resources that were allocated for its execution will be freed and the child is at that point terminated. You should note that other than this mechanism where the parent is the one that's joining the child in all other aspects, the parent and the child thread are completely equivalent. They can all access all resources that are available to the process as a whole and share them. And this is true both, with respect to the hardware resources, CPU memory, or actual state within the process. Here's a code snippet illustrating thread creation. Two threads are involved in this system. The parent thread that's executing this code and the child that gets created via this fork. Both threads perform this operations safe_insert which manipulates some shared list that's initially empty, let's say. Let's assume initially the process begins with one parent thread, T0 in this case. At some point thread 0 calls fork and it creates a child T1. Now, T1 will need to execute safe_insert with an argument 4. As soon as T1 is created, the parent thread continues its execution and at some point it will reach a point where it calls safe_insert of 6, in this case. So it's trying to insert the element 6 into the list. Because these threads are running concurrently, and are constantly being switched when executing on the CPU, the order in which these safe_insert operations on the parent and the child thread is not clear. It is not guaranteed that when this fork operation completes the execution will actually switch to T1 and will allow T1 to perform its safe_insert before T0 does. Or, if after the fork, although the thread is created, T0 will continue and the safe_insert for argument 6 that T0 performs will be the first one to be performed. So as a result, both the list may have a state where the child completes its safe_insert before the parent or the other way around, the parent completes before the child does. Both of these are possible executions. Finally, the last operation in this code snippet is this join. So we're calling join with T1. If this join is called when T1 has actually completed, it will return immediately. If this join occurs while T1 is still executing, the parent thread will be blocked here until T1 finishes the end of the safe_insert operation. In this particular example, the results of the child processing are available through this shared list. So really the join isn't a necessary part of the code. We will be able to access the results of the child thread regardless. >From the previous example where the threads were inserting elements in the list, how does this list actually get updated? An implementation make look as follows. Each list element has two fields, a value field and then a pointer that points to the next element in the list. We call this one p_next. The first element, we call it the list head. It can be accessed by reading the value of the shared variable list. So in this code sample this is where this is happening, read list, as well as it's list pointer element. Then, each thread that needs to insert a new element in the list, will first create an element and set it's value. And we'll then have to read the value of the head of the list, so this element list. And then it will have to set its pointer field to point to whatever is still in the list. And it will have to set the head of the list to point to the newly created element. What that means is for instance when we are creating this element value X, we will first create this data structure. And then we will first read the pointer of the list, originally this pointed to value Y. Set the new elements pointer to point to value Y. And then set the head pointer to actual point to the newly created element. And in this way, new elements are basically inserted at the head of the list. The end of pointing to the rest of the list and the head of the list points to the newly created element. Clearly there is a problem if two threads that are running on two different CPUs at the same time try to update the pointer field of the first element in the list. We don't know what will be the outcome of this operation if two threads are executing it at the same time and trying to set different values in the p_next field. There is also a problem if two threads are running on the CPU at the same time because their operations are randomly interleaved. For instance, they may both read the initial value of the list. So this is where they're both reading the value of the list. And it's pointer to the next element of the list. In this case null. They were both set the pointers p_next in their elements to be null. And then they will take turns setting the actual list pointer to point to them. Only one element will ultimately be successfully linked to the list. And the other one will simply be lost. There is a danger in the previous example that the parent and the child thread will try to update the shared list at the same time, potentially overriding the list elements. This illustrates a key challenge with multi-threading that there is a need for a mechanism to enable mutual exclusion among the execution of concurrent threads. To do this, operating systems and threading libraries in general support a construct called mutex. A mutex is like a lock that should be used whenever accessing data or stayed that's shared among threads. When a thread locks a mutex, it has exclusive access to the shared resource. Other threads attempting to lock the same mutex are not going to be successful. We also use the term, acquire the lock or acquire the mutex, to refer to this operation. So these unsuccessful threads, they will be what we call blocked on the lock operation, meaning they'll be suspended here, they will not be able to proceed until the mutex owner, the lock holder, releases it. This means that as a data structure the mutex should at least contain information about its status. Is it locked or free? And it will have to have some type of list that doesn't necessarily have to be guaranteed to be ordered, but it has to be a some sort of list of all the threads that are blocked on the mutex and that are waiting for it to be free. Another common element that's part of this mutex data structure is to maintain some information about the owner of the mutex, who currently has the lock. A thread that has successfully locked the mutex, has exclusive rights to it, and can proceed with its execution. In this example, T1 gets access to the mutex and thus continues executing. Two other threads, T2 and T3, that are also trying to log the mutex will not be successful with that operation. They will be blocked on the mutex, and they will have to wait until thread one releases the mutex. The portion of the code protected by the mutex, is called critical section. In paper, this is any code within the curly brackets of the lock operating that he proposes to be used with mutexes. The critical section code should correspond with any kind of operation that requires that only one thread at a time performs this operation. For instance, it can be updated to shared variable like the list or increment a counter. Or performing any type of operation that requires mutual execution between the threads. Other than the critical section code, the rest of the code in the program, the threads execution concurrently. So using the same example, imagine that the three threads need to execute a sequence of code among block A fold by the critical section. And then some other portions of code, blocks B, C, and D. All the code blocks with a letter can be executed concurrently. The critical sections, however, can be executed only by one thread at a time. So thread two cannot start the critical section until after thread one finishes them. Thread three cannot start its critical section until any prior thread exits the critical section, or releases the lock. All of the remaining portions of the code can be executed concurrently. So in summary, the threads are mutually exclusive with one another, with respect to their execution of the critical section code. In the lock construct proposed by Burrell, again, the critical section is the code between the curly brackets. And the semantics are such that upon acquiring a mutex, a thread enters the locked block. And then when exiting the block with the closing of the curly bracket the owner of the thread releases this mutex, frees the lock. So the critical section code in this example, that's the code between the curly brackets and the closing of the curly bracket implicitly frees the lock. When a lock is freed, at this point any one of the threads that are waiting on the log, or even a brand new thread that's just reaching the lock construct, can start executing the lock operation. For instance, if T3's lock statement coincides with the release of the lock, of thread one. T3 may be the one, to be the first one to execute, to acquire the lock and execute the critical section although T2 was already waiting on in it. Who will use Burrell's lock construct throughout this lecture. However, you should know that most common APIs have two separate calls, lock and unlock. So, in contrast to Burrell's lock API, in this other interface, the lock/unlock interface, both the lock and the unlock operations must be used both explicitly and carefully when requesting a mutex, and when accessing a critical section. What that means is that we must explicitly lock a mutex before entering a critical section. And then we also must explicitly unlock the mutex when we finish the critical session. Otherwise nobody else will be able to proceed. Returning to the previous safe_insert example, let's demonstrate how to use new techniques making the operation safe_insert actually safe. Just like in the threads creation code we have threads T0 and T1 and they both want to perform the safe_insert operation. The parent thread T0 wants to perform safe_insert with an element 6, and the child thread wants to perform safe_insert with a value of 4. Let's assume that once the parent created the child T1 it continued executing and was the first one to reach safe_insert with value 6. It will acquire the log and start inserting the element 6 on the list. What that means, T0 has the log and when T1, the child, reaches the safe_insert operation it will try to acquire the lock as well and it will not be successful it will be blocked. At some later point, T0 will release the lock, T1 will acquire the lock, and then T1 will be able to insert its element onto the front of the list. So in this case, the final ordering of the list will be 4 followed by 6. Since we're always inserting in the front of the list. That's how we describe this operation. So, the parent inserted its element first, and then the child thread was the second one to insert the value 4 into the list. Let's do a quiz now. Let's take a look at what happens when threads contend for a mutex. For this quiz, we will use the following diagram. We have five threads, T1 through T5, who want access to a shared resource, and the mutex m is used to ensure mutual exclusion to that resource. T1 is the first one to get access to the mutex and the dotted line corresponds to the time when T1 finishes the execution of the critical section and frees m. The time when the remaining threads issue their mutex requests correspond to the lock(m) positions along this time axis. For the remaining threads, which thread will be the one to get access to the mutex after T1 releases control? Your choices are T2, T3, T4, and T5, and you should mark all that apply. Looking at the diagram, both T2 and T4 have attempted to get the lock before it was released. So, their requests will definitely be in the queue that's associated with the mutex, the queue of pending requests. Any one of these two requests could be one of the requests that can get to execute first. The specification of the mutex doesn't make any guarantees regarding the ordering of the lock operations. So, it doesn't really matter that the thread 4 issued the lock operation, the lock request before T2. We don't have a guarantee that these requests will be granted in order. But, regardless, since both T2s and T4s requests are pending on the mutex, then either one of these two threads will be viable candidates of who gets to execute next after T1 releases control. Thread T3 is definitely not a likely candidate, since it doesn't get to issue the lock operation until after T1 released it and their already pending requests. So it's not going to be one of the next threads to execute. For T5 it's a little tricky. >From the diagram we see that the lock is released just as T5 is starting to perform the lock operation. So, what can happen is T1 releases the lock and then we see that both T2 and T4 are actually pending on it. But just before we give the lock to one of these two threads, on another CPU, say T5 arrives, makes a lock request. The lock is still free at that particular time, and so T5 is the one that actually gets the lock. So, it is the one that gets to execute next. So, either one of these T2, T4, or T5 is a viable candidate of which one of the threads is going to get to execute after T1 releases the lock. For threads, the first construct that Birrell advocates is mutual exclusion. And that's a binary operation. A resource is either free or you can access it, or it's locked and busy and you have to wait. Once the resource becomes free, you get a chance to try to access the resource. However, what if the processing that you wish to perform needs to occur only under certain circumstances, under certain conditions. For instance, what if we have a number of threads that are inserting data to a list. These are producers of list items. And then we have one special consumer thread that has to print out and then clear the contents of this list once it reaches a limit. Once, for instance, it is full. We'd like to make sure that this consumer thread only really gets to execute its operation under these certain conditions when the list is actually full. Let's look at this producer/consumer pseudocode example. In the main code, we create several producer threads and then one consumer thread. All of the producer threads will perform the safe_insert operation, and the consumer thread will print, will perform an operation print and clear the list. And this operation, as we said, needs to happen once the list is full only. For producers, the safe_insert operation is slightly modified from what we saw before. Here, we don't specify the argument to safe_insert when the producer thread is created. Instead, every single one of the threads needs to insert an element that has a value of the thread identifier. For the consumer thread here, it continuously waits on the lock, and when the mutex is free, it goes and checks if the list is full. If so, it prints and clears up the element of the list. Otherwise, it immediately releases the lock and then tries to reacquire it again. Operating this way is clearly wasteful, and it would be much more efficient if we could just tell the consumer when the list is actually full, so that it can at that point go ahead and process the list. Burrell recognizes that this can be a common situation in multithreaded environments. And argues for a new construct, a condition variable. He says that such condition variables should be used in conjunction with Mutexes to control the behavior of concurrent threads. In this modified version of the producer consumer code. The consumer locks the mutex and checks and if the list is not full, it then suspends itself and waits for the list to become full. The producers on the other hand, once they insert an element into the list, they check to see whether that insertion resulted in the list becoming full. And only if that is the case, if the list actually is full will they signal that the list is full. This signal operation is clearly intended for whomever is waiting on this particular list_full notification, in this case, the consumers. Not that while the consumer is waiting, the only way that this predicate that its waiting on can change, so the only way that the list can become full, is if a producer thread actually obtains the mutex and inserts an element onto the list. What that means, is that, the semantics of the wait operation must be such that this mutex that was acquired when we got to this point, has to be automatically released when we go into the wait statement. And then automatically reacquired once we come out of the wait statement. Because we see, after we come out of the wait statement, we actually need to modify the list and this mutex m was protecting our list. So when a consumer sees that it must wait it specifies the condition variable that it must wait on. And the wait operation takes its parameters, both the condition variables as well as this mutex. Internally the implementation of the weight must ensure the proper semantics. It must ensure that the mutex is released. And then, when we're actually removed from the wait operation, when this notification is received, we have to reacquire the mutex. At that point, the consumer, so it has the mutex, it's allowed to modify the list, so to print its contents and remove its contents. And then the consumer will reach this curly bracket, this unlock operation, and at that point the mutex will indeed be released To summarize a common condition variable API, we'll look as follows. First, we must be able to create these data structures that correspond to the condition variable, so there must be some type that corresponds to condition variables. Then there is the wait construct that takes as arguments the mutex and the condition variable. Where a mutex is automatically released and re-acquired if we have to wait for this condition to occur. When a notification for a particular condition needs to be issued, it would be useful to be able to wake up threads one at a time. And for this, Birrell proposes a signal API. Or it could also be useful to wake up all of the threads that are waiting on a particular condition, and for this, Birrell proposes a broadcast API. The condition variable as a data structure, it must have basically a list of the waiting threads that should be notified in the event the condition is met. It also must have a reference to the mutex that's associated with the condition so that this wait operation can be implemented correctly so that this mutex can be both released and re-acquired as necessary. As a reference, the way the wait operation would be implemented in a operation system or in a threading library that supports threads and that support this condition variable. It would mean that, in the implementation, the very first thing that happened is that the mutex that's associated with the condition variable is released and this thread is placed on this wait queue of waiting threads. And then at some point when a notification is received, what would have to happen is that the thread is removed from the queue and this mutex is reacquired and then only then do we exit the wait, so only then will a thread return from this wait operation. One thing to note here is that, you know, when on a signal or broadcast we remove a thread from the queue, and then the first thing that this thread needs to do is to re-acquire the mutex. So what that really also means is that on broadcast, although we are able to wake up all the threads at the same time, the mutex, it requires mutual exclusion. So they will be able to acquire the mutex only one thread at a time. So only one thread at a time will be re-acquiring the mutex and exiting this wait operation. So it's a little bit unclear, that it's always useful to broadcast, if you're really not going to be able to do much work once you wake up with more than one thread at a time. For this quiz, let's recall the consumer code from the previous example for condition variables. Instead of while, why didn't we simply use if? Is it because, while can support multiple consumer threads? We cannot guarantee access to m once the condition is signaled? The list can change before the consumer gets access again? Or, all of the above? The correct answer is, all of the above. For instance, when we have multiple consumer threads, one consumer thread is waiting. It wakes up from the Wait statement because the list is full. However, before it gets to actually process that list, we acquired the mutex and processed that list. Newly arriving other consumers, they will reacquire the mutex, see that the list is actually full, and print and remove its contents. So, the one consumer that was waiting on the list when the signal or broadcast notification arrived, when it comes out of the wait, the list, its state has already changed. So, we need the while in order to deal with situations where there are multiple of these consumer threads, multiple threads that are waiting on the same condition. This is related, also, to the second statement in that when we signal a condition, we have no way to guarantee the order in which the access to the mutex will be granted. So, there may be other threads that will require this mutex before you get to respond on the fact that a condition has been signaled. And so in that regard, if you cannot control the access to the mutex and guarantee it, you have no way to guarantee that the value of this list variable will not change before the consumer gets to have access to it again. So, all of these three factors contribute to the fact that we have to use while in order to make sure that when we wake up from a while statement, indeed this condition that we were waiting on, it is met. Now let's see how mutexes and condition variables can be combined for use in a scenario that's common in multithreading systems, multithreading applications, as well as at the operating system level. And this is a scenario where there're multiple threads. But of two different kinds. Some subset of threads that want to perform read operation to access the shared state for reading. And other set of threads that want to perform write operations or that want to access that same shared variable, shared state, and modify it. We call these types of problems the readers/writer problem. The readers/writer problem is such that at any given point of time, zero or more of the reader's threads can access this resource, but only zero or one writer threads can access the resource concurrently at the same time. And clearly we cannot have a situation in which a reader and a writer thread are accessing the shared resource at the same time. One naive approach to solving this problem would be to simply protect the entire resource, let's say this is a file, with a mutex. And put a lock and unlock operations around it. So whenever anyone of these types of threads is accessing the resource, they will lock the mutex, perform their respective access, and then release the mutex. This is, however, too restrictive for the readers/writer problem. Mutexes allow only one thread at a time to access the critical section. So they have a binary state, either zero and the resource is free and accessible, or one and the resource is locked and you have to wait. This is perfectly fine for the writers, since that's exactly the kind of semantic that we want to achieve. But for readers, with mutexes, we cannot express that multiple readers can be performing access to the shared resource at the same time. So how can we solve this problem? Well, we can start by trying to come up with the situations in which it is okay versus it is not okay to perform certain type of access to the resource. So we'll start enumerating these things and we'll try to express this based on the number of readers and writers that are also performing an operation. In the simple case when there are no readers and no writers accessing this resource, both a read operation can be granted, so an upcoming read operation, as well as a upcoming or pending write operation can be granted. As we describe the problem, if the read counter is greater than zero, so if there are more readers already accessing this file, it's perfectly fine to allow another reader to access the file concurrently. Since readers are not modifying it, so it's perfectly okay to grant that request. And then finally, if there is a writer that's performing an access to the resource, then we cannot allow neither the reads nor the writes. So, given these statements, we can express the state in which the shared file or the shared resource is as follows. If the resource is free, then some resource_counter variable, we can say well that will be zero in that case. If the resource is accessed for reading, then this resource_counter variable will be greater than zero. And so it will be actually equal to the number of readers that are currently reading this file. And let's say we can encode the fact that currently there is a writer, somebody's writing to this resource, by choosing that, in that case, the resource_counter variable should take the value negative 1. And of course, this will indicate that there is exactly one writer currently accessing the resource. So there is a saying in computer science that, all problems can be solved with one level of indirection. And here, basically, we produce a proxy resource, a helper variable or a helper expression. In this case, it is this resource_counter variable. This resource_counter variable reflects the state in which the current resource already is. But what we will do, as opposed to controlling the updates to the file, so controlling, with a mutex, who gets to access the file, we will control who gets to update this proxy variable, this proxy expression. So as long as any access to the file is first reflected via an update to this proxy expression, we can use a mutex to control how this counter is accessed. And in that way, basically monitor, control and coordinate among the different accesses that we want to allow to the actual shared file. So let's explain all of this with an actual reader writer example. The actual shared file is accessed via these read data and write data operations. As we see, these read data and write data operations are outside of any lock construct both in the case of the readers, as well as of the writers. So what that means is that the actual access to the file is not really controlled. Instead, what we do is we introduce this helper variable, resource_counter. And then we make sure that in our code, on both the readers and the writer side, before we perform the read operation, we first perform a controlled operation in which the resource counter is updated. Similarly on the writer side. Before we write, we have to make sure that first the resource counter is set to negative 1. Now that, the changes to this proxy variable, that will be what will be controlled, that will be what will be protected within the lock operations. Once we are done with the actual shared resource access, with reading the file or writing the file, we again go into these lock blocks. This is where we update the resource counter value to reflect that the resource is now free. That this one reader finished accessing it, or that no writer is currently accessing that resource. So it will need, basically in our, in our program, we will need a mutex. That, that's the counter mutex. This is the mutex that will always have to be acquired whenever the resource counter variable is accessed. And then we will also need two variables, read_phase and write_phase, which will tell us whether the readers or the writers need to go next. So lets explain this. Let's see what happens when the very first reader tries to access the file. The reader will come in, it will log the mutex, it will check what the resource counter value is, and it will be zeroed. That's what the resource counter was initialized at. So it is not negative 1, perfect. We continue to the next operation, we increment resource_counter. Resource_counter will now have a value of 1. And then we unlock the mutex and we proceed accessing the file. A subsequent reader comes in, and let's say, while it's executing this operation, before it came to the unlock statement, the next reader comes in. The next reader, when it comes in, it will see that it can not proceed with that lock operation. So it will be blocked on the lock operation. So this way, basically, we are protecting how resource_counter gets updated. So only one thread at a time both on the readers and the writer side will be able to update to access this resource counter. However, let's say when that second reader came to the unlock operation, it will be able to join the first reader in this read data portion of the code. So we will have two threads at the same reading the file. Now let's say a writer thread now comes in. So the writer locks the mutex, let's say the mutex is now free, and it checks the resource_counter value. The resource_counter value will have some positive number. We already allowed some number of readers to be accessing it, so it will be, let's say, two. So clearly the writer has to wait. Now, it will wait, in the wait operation it specifies the counter mutex, and it says it's going to wait for the write phase. What will happen at this point, remember, we are performing this wait operation within the lock construct, so the writer has the counter mutex, it's the owner of the counter mutex. However when it enters the wait operation, the mutex is automatically released. So the writer is somewhere suspended on a queue that's associated with the write_phase condition variable, and the mutex is at this point free. So let's say the readers, our readers start finishing the accesses. So as they finish the access they will first log the counter_mutex and this is why it was important that the writer release the mutex, because otherwise none of the readers would have been able to basically exit the real critical section, the reading of the files. So to perform these updates to the proxy variable, and to reflect the fact that nobody's currently reading the file. So reader exists the read phase. So it will lock the mutex. It will decrement the counter. And it will check the value of the resource_counter. So, once the reader decrements resource_counter, it checks to see whether it's the last reader. So, this really should be it checks whether resource_counter is, has reached the value 0. The very last reader that is exiting this read phase will see that resource_counter has reached 0, it's the last reader. And then it will signal the write_phase condition variable. It makes sense to generate this signal and to notify a potential writer that currently there are no readers performing read operations. And given that only one writer can go at a time, it really doesn't make sense to use a broadcast. So, this write_phase will be received ultimately over here. And the one writer that was waiting on that write_phase condition of the area above will be woken up. What will happen internally, that writer will be removed from the wait queue that's associated with the write_phase condition variable. And the counter mutex will be reacquired before we come out of the wait operation. Now as we explained earlier, the very first thing that we have to do is, we have to go ahead and check the statement resource_counter one more time. So we, we go out of the wait statement, but we're still in the while loop. We have to check whether resource_counter is indeed still zero. The reason for this is that internally in the implementation of this wait operation, removing the thread from the queue that's associated with the write_phase condition variable, and acquiring this counter_mutex mutex, are two different operations. And in between them, it is possible that another, either another writer or another reader has basically beat the writer that was waiting to the mutex. And so when we come out, yes we have the mutex, but somebody else has already acquired it, changed the resource_counter value to reflect the phase that maybe there is a reader or a writer currently accessing the file, and then released the mutex. But it, actually, there is another thread that's currently in one of these code blocks that we wanted to protect in the first place. So, while the writer is executing in this write_phase, let's say we have another writer that came in. So this writer is now pending on the read_phase variable. So, we currently have basically one writer here, one writer pending on this write_phase. And let's say we have another writer, reader that came in and it's pending on this read_phase variable. So we have two threads that are waiting on two different variables, and then a writer that's actually in the critical section. When this writer completes, it starts exiting the critical section. So it will reset the resource_counter value, doesn't make sense to decrement, only one writer at a time could, could be in there so it's either negative 1 or 0. And then here in this code, we did two things. We broadcast to the read_phase condition variable. And we'll signal to the write_phase condition variable. We signal to the write_phase condition variable again because only one thread at a time is able to proceed with our write operation. We broadcast to those threads that are reading on the read_phase. So potentially multiple threads will be woken up. Because it makes sense, we allow multiple readers, we allow multiple threads to be in a read_phase, it makes sense to use the broadcast. So, let's say that we have multiple readers waiting on the read_phase when we issued this broadcast. Now, this phase here requires a mutex. So, when these threads that were waiting on the read_phase are waking up from the wait, they will one at a time acquire the mutex, check is resource_counter negative 1? No, it is not negative 1, right? We just reset it to be 0. So, I'll increment the counter. So, incrementing the counter, when the first thread wakes up, the first, the resource_counter will be 1. And then the first thread of the waiting ones will release the mutex and start reading the data. The remaining threads that were waiting on the read_phase will also one at a time come out of wait statement, check to see whether resource_counter is negative 1, and now it will have some positive value. And so, they too will increment the resource_counter and will come out. So, the waiting threads which were woken up from the broadcast statement will be coming out of this wait operation one at a time, but ultimately, we will have multiple threads in the read_phase at the same time. So, this is why broadcast is useful. Yes, indeed, only one thread at a time can really be woken, can really execute this piece of code. But we do want multiple threads to be woken up so that multiple threads can ultimately reach this read_phase once this writer completes. The other thing that's worth noting is here we use both broadcast and signal. Whether the reader's will be really the first ones to execute, or the writers, that we really don't have control over. It really depends on how the scheduler ends up scheduling these various operations. So, the fact that we first called broadcast versus signal, this is really just implicit that the readers are given some kind of priority over the writers. We have no control over that. Looking in the code from the previous morsel, it may seem complex, but you will soon become experts at turning on these critical sections. To make things more simple, let's generalize the structure of each of the code blocks corresponding to the entry points and the exit points in the actual, the real critical section code that the readers and writers are performing. So these code segments here correspond to the enter critical section and the exit critical section codes. When we consider read data as the operation that the real critical section, given that what we really wanted to protect was the file. So this is what we want to control, and then we structure these code blocks to, to limit the entry phase into this code and then the exit phase. Internally, each of these clearly, it represents a critical section code as well. Since there is a lock operation and then there is a shared resource. The resource counter that is updated in both of these cases. If we closely examine each of the blocks that we highlighted in the previous example. We will see that they have the form as follows. First we must lock the mutex. Then we have to check on a predicate to determine whether it's okay to actually perform the access. If the predicate is not met, we must wait. The wait itself is associated with the condition variable and a mutex, and this is the exact same mutex that we locked. When we come out of the wait statement, we have to go back to check the predicate. So we have to have the while loop. The wait statement has to be in a while loop. When we ultimately come out of the while loop, because the predicate has been made. We potentially perform some type of update to the shared state. These updates potentially impact something about the predicates that other threads are waiting on, so we should need to notify them by basically notifying the appropriate condition variables. We do that via a signal and a broadcast. And ultimately we must unlock the mutex so here the lock construct we set in Burrell's paper, the unlock operation is implicit. In some threading environments there is an explicit unlock api that, that we must call. Returning to the readers writers example the real critical sections in this case, are the read operation and the write operation, right? These are the operations that we want to control. And we want to protect, the very least we want to make sure that there isn't a situation where there is a concurrently a write operation where, while others are reading. As well as that there are no concurrent writes. So the code blocks that precede and follow each of these critical sections both on the reader's side that we outlined before and the corresponding ones on the writer's side. These we call the Enter Critical Section blocks and the Exit Critical Section blocks. Every single one of these blocks uses the same mutex, counter_mutex. And so only one thread at a time will be able to execute within these code blocks, except that in these code blocks we only manipulate the resource_counter variable. So only one one thread at a time can manipulate the resource_counter variable. However potentially multiple threads at a time can be performing concurrently a read file operation. So it looks like these enter critical section blocks are sort of like a lock operation that we have to perform before we want to access a resource that we want to protect. That we want to control what kinds of accesses to that shared resource we allow. And then similarly when we are done with manipulating that shared resource, these exit critical sections are sort of for like are, unlock. In the plain mutex case, we have to unlock a mutex in order to allow some other thread to proceed, otherwise the threads will be blocked indefinitely. Same here, when we finish this critical section we have to execute this portion of the code in order to allow other threads, writers in this case, to actually go ahead and gain access to the file. So for examples like the reader writer example we have basically this common building blocks. The actual operations to the shared resource, the shared file, so the reads and writes of the shared file in this case, have to be protected with these code blocks enter critical section and exit critical section. Each of these code blocks internally follows basically the critical section structure that we outlined just couple of screens before, where they lock a mutex, they check for a predicate. If the predicate isn't met, they wait. The wait is in a while loop, and if the while is okay, a predicate potentially gets updated. When we're done with the actual operation, the exit critical section code. Again, in one of these lock constructs, we have to update the predicate, so up here we really have nothing to wait on. We just want to update the predicate and then signal and broadcast the appropriate condition variable. The mutex is actually held online within these enter critical section and exit critical section codes. So if you see it is unlocked on the end and this allows us to then basically control the access to the proxy variable, but to allow more than one thread to be in the critical section at a given point of time. This lets us benefit from mutexes, because mutexes allow us to control how a shared resource is accessed. However, this type of structure also allows us to deal with a limitation that mutexes present because they allow only one thread at a time to access that resource, so with this structure, we will be able to implement more complex sharing scenarios. So in this case the policies that either multiple threads of type reader can access the file, or one thread of type writer can access the file. The default behavior of mutexes alone doesn't allow us to directly enforce this kind of policy. Mutexes only allowed mutual exclusion policy. Now let's look at some frequent problems that come up when writing multi-threaded applications. First, make sure to keep track of the mutex and condition variables that are specifically used with a given shared resource. What that means, for instance, is that when defining these variables make sure to write immediately a comment, which shared resource, which operation, which other piece of shared state, do you want this synchronization variable to be used with. For instance, you're creating a mutex m1 and you want to use it to protect the state of a file, file 1. Next, make sure that if a variable or a piece of code is protected with a mutex in one portion of your code, that you're always consistently protecting that same variable, or that same type of operation with the same mutex everywhere else in your count. Basically, a common mistake is that sometimes we simply forget to use the lock/unlock construct. And therefore, sometimes access the variable in a safe way. And if we don't use the lock and unlock, then it won't be accessed in a safe way period. Some compilers will sometimes generate warnings or, or even errors, to tell us that there is a potentially dangerous situation, where shared variable is and isn't used with a mutex in different places in the code. Or, maybe they will generate a warning to tell us that there is a lock construct that's not followed by the appropriate unlock construct. So certainly you can rely on compilers and tools to help avoid mistakes but it's just easier not to make them in the first place. Another common mistakes that's just as bad as not locking a resource, is to use different mutexes for a single resource. So, some threads read the same file by locking mutex m1, and other threads write to the same file by locking mutex m2. At the same time, different threads can hold different mutexes and they can perform concurrently operations on this file, which is not what we want to be happening. So this scenario can lead to these undesirable situation, actually dangerous situations where different types of accesses happen concurrently. Also it's important to make sure that when you're using a signal or a broadcast you're actually signaling the correct condition. That's the only way that you can make sure that the correct set of threads are potentially going to be notified. Again, using comments when you are declaring these conditions can be helpful. Also make sure that you're not using signal when you actually need to use broadcast. Note that the opposite is actually safe. If you need to use signal but use broadcast, that's fine. You will still end up waking up one thread or more. And you will not affect the correctness of the program. You may just end up affecting its performance. But that's not as dangerous. Remember that with a signal only one thread will be woken up to proceed. And if, when the condition occurred we had more than one thread waiting on the condition. The remaining threads will continue to wait. And in fact they may continue to wait possibly indefinitely. Using a signal instead of a broadcast can also possibly cause deadlocks. And we'll talk about that shortly. You also have to remember that the use of signal or broadcast or rather, the order of signal or broadcast. Doesn't do anything about making any kind of priority guarantees as far as which one of the threads will execute next. As we explained in the previous example, the execution of the threads is not directly controlled by the order in which we issue signals to a condition variables. Two other common pitfalls spurious wake ups and dead locks, deserve special attention and we will discuss these two in more detail next. One pitfall that doesn't necessarily affect correctness, but may impact performance, is what we call spurious or unnecessary wake-ups. Let's look at this code for a writer and readers. Let's say currently there is a writer that's performing a write operation, so it is the one that has the lock counter mutex, so this is the shared lock. And then elsewhere in the program, readers for instance, are waiting on a condition variable, read_phase. So there are a number of readers that are associated with the wait queue that's part of that condition variable. So what can happen when this writer issues the broadcast operation, this broadcast can start removing threads from the wait queue that's associated with the read phase condition variable, and that can start happening, perhaps in another core, before the writer has completed the rest of the operations in the lock construct. Now, if that's the case, we have the writer on one core. It holds still the lock, and it's executing basically this portion of the code. And, at another core, on another CPU, the threads that are waking up from this queue that's associated with the condition variable that's part of the wait statement, they have to, the very first thing they do is, they have to reacquire the mutex. We explained this before. So that means the very first thing that these threads will do will try to reacquire the mutex. The mutex is still held by the writer thread. The writer thread still has the mutex. So none of these threads will be able to proceed. They'll be woken up from one queue that's associated with the condition variable, and they'll have to be placed on the queue that's associated with the mutex. So we will end up with this type of situation as a result of this. This is what we call spurious wake-up. We signaled we woke up the threads. And that wake-up was unnecessary. They have to now wait again. The program will still execute correctly. However, we will waste cycles by basically context switching these threads to run on the CPU and then back again to wait on the wait queue. The problem is that when we unlock only after we've issued the broadcast or the signal operation, no other thread will be able to get the lock. So spurious wake-ups is this situation when we're waking threads up, we're issuing the broadcast or the signal, and we know that it is possible that some of the threads may not be able to proceed. It will really depend on the ordering of the different operations. So, would this always work, though? Can we always unlock the mutex before we actually broadcast our signal? For instance by using this trick, we can transform the old writer code into this code where, we first unlock, and then we perform the broadcast and signal operations. This clearly will work just fine. The resulting code will avoid the problem of spurious wake-ups, and the program remains correct. In other cases, however, this would not be possible. We cannot restructure the program in this way. So if we look at what's happening at the readers, the signal operation is embedded in this if clause. And the if statement relies on the value of resource_counter. Now, resource_counter was the shared resource that this mutex was protecting in the first place. So we cannot unlock and then continue accessing the shared resource. That will affect the correctness of the program. Therefore, this technique of unlocking before we perform the broadcast or signal doesn't work in this particular case or in similar cases. One of the scariest problems related to multithreading is deadlocks. An informal definition of a deadlock is that, it is a situation in which competing threads, at least two or more, they're each waiting on each other to complete. However, none of them ever do because each waits on the other one. So thus, the execution of the process overall of all of these threads is stuck and it cannot continue. The threads are, we call, deadlocked. We can use the visual example to help explain a deadlock using our toy shop example. So, imagine that two workers in the toy shop are finishing toy orders that involve a train, and each worker will need a soldering iron and a solder wire to finish their toy. The problem is, there's only one of each of those. So, let's say the first worker grabs the soldering iron first, and the second worker grabs the solder wire. And because both workers are stubborn, they're unwilling to give up either one of the items that they've grabbed, so none of the toys will ever get made. The workers remain continuously stuck in a deadlock. In practice, the deadlock example can be described with the following situation. So we have two threads, T1 and T2. They need to perform some operation involving some variables, A and B. And, in fact, the two threads don't even need to perform the same operation on A and B. But let's say they both need to access these shared variables, A and B. Before performing these operations, they must lock mutexes that protect the shared variables A and B. And let's say T1 first locks the mutex for A and then locks the mutex for B. And in the case of T2, T2 first locks the mutex for B and then locks the mutex for A. And this is where the problem is. The two threads are waiting on each other in a cycle. Neither one of them will be able to get to the foo operation. Neither one of them, in fact, will be able to execute this second lock operation. They'll keep waiting on each other. And we'll have a dead lock. So how can we avoid these situations? One way to avoid this situation would be to unlock the mutex for A before locking the mutex for B, or the other way around. We would call this fine-grained locking. The problem with this is that, it won't work in this case since the threads need both A and B. So, they have to have both locks. They need both variables, after all. Another possibility would be to get all the locks up front and then release them at the very end. Or maybe we just end up using one mega-lock, some mutex m sub A,B. This solution may work for some applications. However, in other cases it can be very restrictive because it limits the level of parallelism that can exist in the system, and the last and the really most accepted solution is to maintain a lock order. If we force everyone to first get the mutex for A and then the lock for B, the problem will not occur. If we investigate this a little bit more, we see that the problem is that T1 is waiting on something that T2 has. And T2's waiting on something that T1 has. So we have a cycle basically. And in this case, we have two threads, so it's easy to reason about the situation and to determine that the order in which these locks are being acquired can result potentially in a cycle. In principle, this type of analysis is a little bit more difficult to illustrate than to determine, but what we're trying to really show is that if there is a cycle in this kind of wave graph in which we draw a line between two threads. If one thread is waiting on a resource that the other thread has, then a cycle indicates a dead lock. Maintaining a lock order will prevent such cycles from occurring. So will ensure that there will be no deadlocks in the code. So to enforce this kind of maintain lock order principle in the example before, T2 would have to get the mutexes, the mutex for A first and then the mutex for B, and there no way that in that, in this code a cycle would occur. There's no way that a deadlock can happen. So, consider this. If thread 1 is waiting on the lock, is about to execute the operation lock m of B, it means it already has acquire the mutex m of A. If thread 1 already has this mutex then thread two cannot have it. So, thread two must be somewhere before this lock operation and its execution. That also means that thread 2 could not have acquired the m_b mutex, it doesn't have it, and therefore there's no cycle. Thread 1 is not going to end up in a situation in which it has to wait on thread 2 for that particular lock. So it will be able to acquire it and continue. So this type of technique will always work. The only potential challenge is that in complex programs that use many shared variables and potentially many synchronization variables, so lots of mutexes, take some effort to make sure that these are indeed ordered. And everywhere used in the same order. But as a technique this is foolproof. And it guarantees that it will prevent deadlocks from happening. There is more that goes into dealing with deadlocks. Detecting them, avoiding them, and recovering from them. But for the sake of this class, remember that maintaining order of the deadlocks will give you a deadlock-proof solution. So in summary, a cycle in the wait graph is necessary and sufficient for a deadlock to occur. And this graph itself is one where the edges are from the thread that's waiting on a resource to the thread that owns a resource. So what can we do about this? First we can try to prevent deadlocks. Every single time a thread is about to issue a request for a lock, we first have to see if that operation will cause a cycle in this graph. If that is the case, then we must somehow delay that operation. And in fact, we may even need to change our code so that the thread first releases some resource and only afterwards it attempts to perform that lock request. Clearly this can be very expensive. The alternative of completely preventing deadlocks is to detect deadlocks, and if you detect that they have occurred, to have mechanisms to recover from them. We do these kinds of things based on, basically, analysis of the graph to determine whether at any given point of time, any cycles have occurred. This maybe isn't as bad as monitoring and analyzing every single lock request to see whether it will cause future deadlocks. But it's still expensive in that it requires us to have ability to roll back the execution so that we can recover. And the only way that that can be possible is if we have maintained enough state during the execution so that we can figure out how to recover. And in some cases, these rollback mechanisms are essentially impossible to perform. If we have inputs and outputs that came from external sources, we don't have ways to roll back their execution. And finally we have the option to apply the Ostrich Algorithm. And as sophisticated as that sounds, the Ostrich Algorithm is that, will basically just hide like an ostrich with his head in the sand, and do nothing. This is even more optimistic than this rollback based mechanism, in which we were letting the system execute, allowing it to cause a deadlock, and then recovering from him. Here we're essentially just hoping the system will never deadlock, and if we're wrong, then we'll just reboot. The truth is that although we know that lock order will help remove any cycles in the graph, when we have complex codes, when we have codes from different sources, it's really hard to guarantee that all of the mutexes will be acquired in exactly the same order across multiple source files. And so deadlocks are a reality. These types of techniques are possible, exist, and they can help us deal with deadlocks, however they're rather expensive in terms of the overheads they impose on the execution time of the system. So typically they're applied only in really performance-critical systems. Let's take a quiz in which we'll take a look at an example of a critical section. The critical section that we will look at corresponds to a critical section in a toy shop similar to the toy shop examples that we looked at. In the toy shop, there will be new orders that will be coming in. As well as there will be orders for repairs of toys that have already been process, so, like, old orders that need to be revisited. Only a certain number of threads, a certain number of workers, will be able to operate in the toy shop at any given point of time. So there will be a mutex, orders_mutex, that controls which workers have access to the toy shop. Basically, which orders can be processed. The toy shop has the following policy. At any given point of time, there can be up to three new orders processed in the toy shop. In addition, if there is up to only one new order being processed, then any number of requests to service old orders can be handled in the toy shop. The code shown in this box describes the critical section entry code that's executed by the workers performing new orders. As expected, we first lock the mutex and then check a condition, and this condition must correspond to this policy. Depending on this condition, on this predicate, we determine whether the thread, whether the new order, can go ahead and be processed in the toy shop, or if we must wait. With the wait statement, we use a condition variable new_cond as well as we include the mutex. Because as we mentioned, a mutex must be associated with a wait statement so that it can be atomically released. The predicate statement that we must check on before determining whether a thread will wait or can proceed, is missing. For this quiz, you need to select the appropriate check that needs to be made in order to enter the critical section. There are four choices given here. You should select all that apply. The first code snippet is correct because it perfectly aligns with the policy. If new_order is equal to 3, clearly an incoming thread will not be able to proceed. So this will guarantee that there cannot be more than three new orders processed in the toy shop. Note, by the way, that new_ order cannot be larger than 3, given that the only way that it will get updated is once we come out of this wait statement. So, the maximum value that new_order can receive in this code is 3. The second part of this statement also perfectly aligns with the second part of the policy. If we have a situation in which there are some number of old_orders in the system, and one request for a new_order has already entered the toy shop, then any incoming new_order will have to be blocked at the wait statement. So, this piece of code, this answer, is correct. The second code snippet is almost identical to the first one, however, it uses if as opposed to while. We explained that if creates a problem, in that, when we come out of the statement, when we thought that this condition was satisfied. It is possible that, in the meantime, another thread has come in and executed this particular lock operation or even the critical section entry for the old_order, and has therefore changed the value of this predicate. If we don't go back to reevaluate the predicate, it is possible that we will miss such a case and therefore enter the actual critical section. So, enter the toy shop in a way that violates this policy. So this answer is not correct because it uses this if as opposed to a while. The third statement is incorrect because it checks whether old_order is greater than or equal to 0. So, what this means means if that a new incoming order will be blocked if there is already one new_order in the system and old_order is equal to 0. And basically no other orders for toy repairs, no other old_orders are in the system. That is clearly not the desired behavior. We want to allow up to three new orders to be processed in the system. So this statement is incorrect. The fourth statement is basically identical to the first one except for the fact that it uses greater than or equal to 3. As we already pointed out, new_order will really not even receive a value greater than 3 the way it's updated here. So this statement will result in the identical behavior as the first statement. So both of these are correct. We said earlier that threads can exist at both the kernel and the user level. Let's take a look more at what we mean by this distinction. Kernel level threads imply that the operating system itself is multithreaded. Kernel level threads are visible to the kernel, and are managed by kernel level components like the kernel level scheduler. So, it is the operating system scheduler that will decide how these kernel level threads will be mapped onto the underlined physical CPUs, and which one of them will execute at any given point of time. Some of these kernel level threads may be there to directly support some of the processes. So, they may execute some of the user-level threads and other kernel level threads may be there just to run certain OS level services like daemons, for instance. At the user level, the processes themselves, are multi-threaded, and these are the user level threads. For a user level thread to actually execute, first, it must be associated with a kernel level thread. And then, the OS level scheduler must schedule that kernel level thread onto a CPU. So let's investigate a little more, what is the relationship that exists between user level threads and kernel level threads. In fact, there's several possible relationships between the user level threads and the kernel level threads. And we will now look at three such models. The first model is a One-to-One Model. Here each user-level thread has a kernel-level thread associated with it. When the user process creates a new user-level thread, there is a kernel-level thread that either is created or there is available kernel-level thread, then a kernel-level thread is associated with user-level thread. This means that the operating system can see all of the user-level threads. It understands that the process is multithreaded, and it also understands what those threads need in terms of synchronization, scheduling, blocking. So as the operating system already supports these mechanisms in order to manage its threads, then the user-level processes can directly benefit from the threading support that's available in the kernel. The downside of this approach is that, for every operation we have to go to the kernel, so we have to pay the cost of a system call, of crossing the user to kernel boundary. This we discussed already could be expensive. This model also means that since we're relying on the kernel to do the thread-level management synchronization, scheduling et cetera, we are limited by the policies that are already supported at the kernel level. So for instance if the kernel doesn't support a particular scheduling policy or if the kernel has a limit on the number of threads that can be supported, the process is restricted to operate within those bounds. This basically affects the portability, so if a particular process, a particular application, has certain needs about how its threads should be managed, we're limited to running that kind of process only on the kernels that provide exactly that kind of support. The second model is the Many-to-One Model. Here all of the user-level threads are supported, are mapped onto a single kernel-level thread. What this means is that at the user level there is a thread management library that decides which one of the user-level thread will be mapped onto the kernel-level thread at any given point of time. That user-level thread, of course, will run only once the kernel-level thread is actually scheduled on the CPU. The benefit of this approach is that it's totally portable. Everything will be done at the user-level thread library, scheduling, synchronization, et cetera, and so we don't rely on any specific kernel-level support. Similarly we're not limited by the specific limits and policies that are available in the kernel. Also because all of the thread management is done at the user level by the user-level threading library, we don't have to make system calls, we don't have to rely on user kernel transitions, in order to make decisions regarding scheduling, synchronization, blocking, et cetera. So this is very different than what we saw in the one-to-one model. The problem with this approach is, however, that the operating system has really no insight into the application needs, it doesn't even know that the process is multithreaded. What the OS sees is just a kernel-level thread, so the real danger is that when the user-level library schedules one user-level thread onto the kernel-level thread, and let's say this user-level thread makes a request for an I operation that's blocking. The kernel level scheduler will see that the kernel-level thread block, and it will basically block the entire process. So the fact that there may be other user-level threads that have useful work to do and the process overall can make some progress, that's hidden from the operating system, from the kernel, and the whole process is forced to wait. This is obviously going to have some implication on performance. Finally there is the Many-to-Many Model. The Many-to-Many Model allows some user-level threads to be associated with one kernel-level process, others perhaps to have a one-to-one mapping with a process, so sort of the best of both worlds. The kernel knows that the process is multithreaded since it has assigned multiple kernel level threads to it. And also, if one user-level thread blocks an I/O, and as a result the kernel-level thread is blocked as well, the process overall will have other kernel-level threads onto which the remaining user-level threads will be scheduled. So the process overall can make progress. The user-level threads may be scheduled onto any of the underlying kernel-level threads, so they're unbound. Or we can have a certain user-level thread that's basically mapped one-to-one permanently onto a kernel-level thread. We called this the bound mapping. The nice thing about this is that if we have certain user-level threads that somehow even the kernel should be able to treat differently, to ensure that they have better priority or are more responsive to certain events that are happening in the kernel, we have a mechanisms to do this by these bound threads. For instance, if the kernel sees that there is some user input, and we have a particular user level thread that's designated to run the user interface for this process, the kernel has a way to immediately schedule the corresponding kernel-level thread as a result of that, also the corresponding user-level thread. So this ability to provide this kind of one-to-one, permanent mapping in the many-to-many model is another benefit of the model. There are still some cons with this model. And that's, in particular, because now we require some coordination between the kernel-level thread management and the user-level thread management, which we didn't see in the other cases. In the one-to-one model, pretty much everything goes up to the kernel-level manager, and in the many-to-one model, pretty much everything goes up to the user-level thread manager that's part of the thread's library. In the many-to-many model there's often the case where we require certain coordination between the kernel and user-level managers, mostly in order to take advantage of some performance opportunities. We will discuss later some of the implications on implementation that are there because of the interactions between the user-level threads and the kernel-level threads. But for now, you need to understand that there are different levels at which multi-threading is supported, at the entire system or within a process. And that each level affects the scope of the thread management system. At the kernel level we have system-wide thread management that's supported by the operating system-level thread managers. What this means is, that the operating system thread managers will look at the entire platform we're making decisions, as to how to allocate resources to the threads. This is the system scope. On the other end at user level, a user-level thread library that's linked to the process manages all of the threads that are within that single process only. So the management scope is process wide. Different processes will be managed by different instances of the same library. Or even different processes may link entirely different user-level libraries. To illustrate the effects of having a different scope, let's take a look at the following situation. Let's say the web server has twice as many threads as the database. If the user-level threads have a process scope, the operating system doesn't see all of them. So at the operating system level, the available resources will be maybe managed 50/50 among the two different processes. That means that both the web server and the database will be allocated equal share of the the kernel level threads, so two each. And then the OS level scheduler will manage these threads by splitting the underlying CPUs amongst them. The end result of that however, is that the webserver's user level threads, will have half of the amount of the CPU cycles that's allocated to the database threads. Now if we have a System Scope, the user-level threads all of them, will be visible at the kernel level. So the kernel will allocate to every one of its kernel-level threads. And therefore, to every one of the user-level threads across the two applications, in equal portion of the CPU. If that happens to be the policy that the kernel implements. As a result, if we have a situation in which one process has more user-level threads than the other, this process will end up receiving a larger share of the underlying physical resources. Since very one of its user level threads will get equal share of the physical resources as the user level threads in the other process. Before we conclude this lesson, let's discuss some useful multithreading patterns for structuring applications that use threads. We will look at the boss-workers pattern, the pipeline pattern, and the layered pattern. Before we start, let's take a look at the toy shop application. We will describe these pattern in the context of this application. In this application, for each toy order we receive, and let's say we're sticking to wooden toy orders, we have to perform the following steps. First, we have to accept the order from the customer. Then, we have to parse the order to see what it's for. Then, we have to start cutting the wooden parts for the toy. Then, we need to paint and add decorations for the toy parts. Then, all those parts need to be put together to assemble the wooden toy. And finally, we need to ship the order. Depending on the multithreading pattern, these steps will be assigned differently to the workers in the workshop. We will first look at the boss-workers pattern. This is a popular pattern that's characterized by one boss thread and then some number of worker threads. The boss is in charge of assigning work to the workers, and the workers are responsible for performing the entire task that's assigned to them. Concerning our toy shop example, that means that the very first step, the step where we accept an order will be performed by the boss. The boss will accept an order and then immediately pass it on to one of the workers. Each of the workers will perform steps two through six, so we'll parse the order, cut the pieces, stain the pieces, and assemble the wooden toy and ultimately ship the order. Since we only have one boss thread that must execute on every single piece of work that arrives in the system, it means that the throughput of the system overall is limited by the boss' performance. Specifically, the throughput of the system is inversely proportional to the amount of time the boss spends on each order. So, clearly, that means that we must keep the boss efficient if we want to make sure that the system overall is performing well. In our toy shop example, the boss thread just picks up an order from the customer and immediately passes it to the workers. It doesn't really look to see what it's for. That's why each of the workers starts with step two. So in that way we're trying to limit the amount of operation that's required from the boss on each order. So how does the boss pass work to one of the workers? One way is for the boss to keep track of exactly which workers are free, and then hand off work to those workers. So, it's specifically signalling one particular worker. This means that now the boss will have to do more for each order, because now it has to keep track of which of the workers are available. And will also have to wait for that particular worker to accept the order from the boss when, when its being passed, sort of like a handshake. The positive of this approach is that the workers don't need to synchronize amongst each other in any way. The boss will tell them what they need to do, and they don't have to care about what the other workers do. The downside, however, is that given that the boss now has to keep track of what the workers are doing, the throughput of the system will go down. Another option is to establish a queue between the boss and the workers. This could be similar to a producer/consumer queue, where the boss is the only producer that produces work requests, so toy orders for the workers. And then the workers are the consumers that are picking up work from this queue, picking up orders from this queue and then proceeding with the steps that they need to perform. The upside of this approach is that the boss now doesn't really need to know about what each worker is doing and whether it's free. It also doesn't have to wait for a worker to explicitly do, a handshake when it's passing off a work item to one of them. The boss just accepts an order, so performs steps one, places the order on the shared queue, and can go back to picking up the next order. Whenever one of the worker becomes free, it looks into the queue, at the front of the queue ideally, and picks up any pending work requests. The downside is that now the workers, as well as the workers and the boss amongst each other, have to synchronize their accesses to the shared queue. All of the worker threads may contend to gain access to the front of the queue, and any one work item can only be assigned to one worker thread. And also the workers and the boss may need to synchronize when they need to compare the front and the end pointer of this queue, for instance, when they need to determine that a queue is full or that a queue is empty. Despite of this downside, this approach of using a shared queue among the boss and the workers when passing work among them, still results in lower time per order that the boss needs to spend. So it results in better throughput of the system overall. So that's why we tend to pick this particular model when a building multithread applications using this pattern So if we use this queue structure, the performance of the system overall will depend on whether or not the boss thread has to wait when inserting work requests, toy orders into this queue. If the queue is full, the boss will have to wait, the time that it spends per order will increase, and overall, the throughput of the system will go down. Clearly, if we have more threads, it's less likely that the queue will be full, but arbitrarily increasing the number of threads will add some other overheads in this system. So the question is, how many workers is enough? Well, we can add more workers dynamically on demand. Whenever we have yet another order, we go and call yet another worker to join the crew. This clearly can be very inefficient if we have to wait a long time for a worker to arrive. A more common model, therefore, is to have a pool of workers that's created up front. With such a pool of workers, or pool of threads since a worker is directly supported by a thread, we don't have to wait for a new thread to be created or a new worker to arrive every single time we start seeing that order start piling up on the queue. The question is, though, how do we know how many workers, how many thread to pre-create in this pool? A common technique is to use this pool of workers or pool of threads model, but as opposed to statically deciding what the size of the pool should be, is to allow the pool to be dynamically increased in size. Unlike the purely on demand approach, these increases won't happen one thread at a time, rather we'll create several threads whenever we determine that the pool size needs to be adjusted. So this tends to be the most effective approach of managing the number of threads in the boss-worker pattern. So to summarize, so far we saw that the boss-workers model has these features. A boss assigns work to the workers. The workers, every one of them, performs the entire task. The boss and the workers communicate via shared producer consumer queue, and we use a worker pool based approach to manage the number of threads in the system where we potentially adjust the size of the pool dynamically. The benefit of this approach is in its overall simplicity. One thread assigns work to all others. All other threads perform the exact same task. The negatives of this approach include the overhead related to the management of the thread pool, including synchronization for the shared buffer. Another negative of this approach is the fact that it ignores locality. The boss doesn't keep track of what any one of the workers was doing last. If we have a situation in which a worker just completed performing a similar type of task or identical type of task, it is more likely that that particular worker will be more efficient at performing that exact same task in the future. Or maybe it already has some of the tools that are required for building that particular type of toy nearby on its desk. But if the boss isn't paying attention to what the workers are doing, it has no way of making that kind of optimization. An alternative to having all workers in the system perform the exact same task, so all workers be equal, is to have different workers specialized for different sets of tasks. In the context of the toy shop example, this may mean that we have workers specialized for different types of toys. Or it could mean that we have workers specialized for repairs versus for brand new orders, or even we can specialize the workers to deal with different types of customers. One added stipulation in this case is that now the boss has to do a little bit of more work for each order. Because in addition to just accepting the order it has to take a look at it, and decide which set of workers it should actually be passed to. Now the fact that the boss has to do a little bit of more work per order is likely offset by the fact that each of the worker threads will now be more efficient, because they're specialized for the task. So overall we can achieve better performance. The real benefit of this approach is that it exploits locality. Each of the threads, by having to do a subset of the tasks, it ends up probably accessing only a subset of the state, and therefore that state is more likely present in the cache. And we already talked about the benefits of having state present in cache. It is much faster to access than if we have to go to memory. Also, by being able to assign different workers to different types of tasks or different types of customers, we can do better quality of service management. So we can assign more threads to those tasks or those customers that we need to give higher quality of service to. This main challenge in this variant of the boss-workers model comes from the fact that it is now much more complicated to do the load balancing. How many threads should we assign for the different tasks? This is not necessarily a question that has a unique answer regardless of the amount of flow, the number of requests that are coming in the system, the kind of hardware, so the kind of tools that these threads use. So it ends up being a more complicated type of question. A different way to assign work to threads in a multithreaded system is using this pipeline approach. In this pipeline approach, the overall task, the processing of the toy shop, is divided into subtasks, and each of the subtasks is performed by a separate thread. So threads are assigned subtask in the system, and then the entire applications, so the entire complex task is executed as a pipeline of threads. In the context of our toy shop example, for instance, what this would mean is that given that we have six steps in the toy order processing, we can have six workers and every single one of the workers will be assigned one steps to process. At any given point of time we can have multiple tasks concurrently in the system. So, multiple toy shop orders being processed by different workers. It just every single one of those orders, every single one of those tasks, will be in a different stage in the pipeline. So, one worker can be accepting one toy order, another worker can be performing the parsing of the order for another toy, a third worker can be cutting the wooden pieces for yet another toy and so forth. The throughput of the system overall will clearly be dependant on the weakest link, the longest stage in the pipeline. Ideally, we would love it if every single stage of the pipeline takes approximately the same amount of time, but sometimes it just may not be possible. So the way to deal with this is using the same thread pull technique that we saw before. If one of the pipeline stages for instance, the wood cutting stage, takes more amount of time longer. Then we can assign multiple threads to this particular stage. Let's say it takes three times longer to perform this stage in the pipeline compared to all the other stages in the pipeline. If we assign three different threads, three different workers, to this stage, then overall, every single stage in the pipeline will approximately be able to process the same number of subtasks, the same number of toy orders over a period of time. So the system overall will still be balanced. The best pay to pass work among these different pipeline stages is using a shared-buffer based mechanism similar to the producer consumer shared buffer that we saw in the previous pattern. The alternative to that would be to require some explicit communication between threads in the pipeline, and this will mean that a thread in an earlier stage will potentially have to wait until a thread in the next stage is free. The shared-buffer based approached, the queue based approach, helps correct for any kind of small imbalances in the pipeline. For instance, with direct communication in the context of our toy shop, it would require workers directly to hand off the output of their processing, so cut pieces etcetera to the next worker, to the next thread. Where as in the shared buffer base communication you can think of a worker leaves the output of its processing, so the piece is on the table and the next worker picks them up whenever he or she's ready. In summary, a pipeline is a sequence of stages where a thread performs a stage in the pipeline and that's equivalent to some subtask in the end-to-end processing. To keep the pipeline balanced a stage can be executed by more than one thread. And we can use the same thread pool management technique that we described in the Boss-Workers model to determine what is the right number of threads per stage. Passing partial work products or results across the stages in the pipeline should be done via a shared buffer based communication. This provides for some elasticity in the implementation and avoids stalls due to temporary pipeline imbalances. A key benefit of the approach is the fact that it allows for highly specialized threads and this leads to improved efficiency. Like what we saw in the variant of the boss-worker model, when threads perform a more specialized task, it's more likely that the state that they require for their processing is present in the processor cache and that kind of locality can ultimately lead to improve performance. A negative of the approach is the fact that it is fairly complex to maintain the pipeline balanced over time. When the work load pattern changes, so we have more toys arriving or when the resources of the pipeline change, so a worker slows down or takes breaks, we'll have to rebalance the entire pipeline to determine how many workers to assign to each stage. In addition to that, there is more synchronisation, since there are synchronisation points at multiple points in the end to end execution. Another multithreading pattern is what we call a layered pattern. Let's return to the toy shop example. Steps one, two and six, all of them deal with order processing, accept the order, parse the order, ship the order. Steps three and five deal with cutting and assembling the wooden pieces for the toy. Step four deals with decorating or painting the toy. A layered model is one in which each layer is assigned a group of related tasks, and the threads that are assigned to a layer can perform any one of the subtasks that correspond to it. End to end, though, a task must pass up and down through all the layers. So, unlike in the pipeline pattern, we must be able to go in both directions across the stages. The benefit of the approach is that it provides for specialization and locality, just like what we saw in the pipeline approach. But it's less fine-grained than the pipeline, so it may become a little bit easier to decide, how many threads should you allocate per layer? The downsides are that this approach, this pattern, may not be suitable for all applications since, in some cases, it may not make sense for the first and the last step in the processing to be grouped together, to be assigned to the same thread. You may not be able to get any benefits from specialization in that case. The other potential issue with the approach is that a synchronization it requires is a little more complex than what we saw before, since every single layer needs to coordinate with both the layers above and below, to both receive inputs, as well as pass results. Let's take a quiz now in which we will compare the performance of some of the multithreading patterns that we saw. For this quiz we will look at an extremely simplified performance calculation for the toy order application. And we will compare two solutions. One, which is implemented via the boss-workers pattern. And then the second one that's implemented via the pipeline pattern. For both solutions, we will use six threads. We'll assume, also, that in the boss-workers solution, a worker takes 120 milliseconds to process a toy. For the pipeline solution, we will assume that each of the six stages, where a stage is a step from this application, take 20 milliseconds. The question then is, how long will it take for each of these solutions to complete ten toy orders? You should ignore any time that's spent waiting in the shared queues in order to pass orders from the boss to the workers or across the pipeline stages. And assume infinite processing resources, like tools or work areas. Then, you should answer, what if there were 11 toy orders? How long will it take each of these solutions to process the 11 orders? You should write your answers below, expressed in milliseconds. Okay. Let's take a look at what's happening in the system. In the first case, we have ten toy orders. Both solutions have six threads each. For the boss-workers case, that means that one of the threads will be the boss, and then the remaining five threads will be the workers. For the pipeline model, each of the six threads will perform one stage, one step in the toy order application. So, for the boss-workers case, because we have five worker threads, at any given point of time, these workers will be able to process up to five toy orders. So, if we have ten toy orders, for the boss-worker model, the workers will process the first five orders, given that we have five workers, at the same time. And every single one of them will take 120 milliseconds, so the first five toy orders will be processed in 120 milliseconds. The next five orders will take additional 120 milliseconds for a total of 240 milliseconds. For the pipeline case, the very first toy order will take 120 milliseconds to go through the six stages of the pipeline. So 6 times 20 milliseconds. Then, once the first toy order exits the pipeline, that means that the second toy order is already in the last stage of the pipeline. So we'll take another 20 seconds to finish. And then the third toy order will be immediately afterwards. It will take additional 20 milliseconds to finish. So given that we have nine remaining orders after the first one, the total processing time for the pipeline case when we have ten toy orders is as follows. 120 for the first one, and then 9 times 20 to complete the last stage of every single one of the remaining nine toys. That's 300 milliseconds. Now, if we have 11 toy orders, we will process the first ten in the exact same manner as before. We have five worker threads. They can only process five toy orders at the same time. So the first ten out of these 11 will be processed in 240 milliseconds. Then, the 11th order will take another 120 milliseconds. Only one of the workers will be busy. Only one of the workers will be processing that toy order. However, it will take an additional 120 milliseconds to complete all of the 11 toy orders for a total of 360 milliseconds. For the pipeline case, applying the exact same reasoning as before, when we have 11 toys, it will take 120 to process the first one. And then for the remaining ten, it will take another 20 milliseconds for every single one of them to finish the last stage of the pipeline. So we'll take a total of 320 millisecond for the pipeline approach to process 11 toys. If we look at these results, we see that the boss-worker model is better in one case, when there are only ten toy orders in the system. And then the pipeline approach is better in the other case, when there are 11 toy orders in the system. This illustrates the fact that there isn't a single way to say that one pattern or the other is better. As this example illustrates, the answer to that can depend very much on the input that that application receives. For one input, an input of ten toy orders, one implementation is better, whereas for another input of 11 toy orders, the other implementation is better. And finally, you should note that we really simplified the calculation of the execution times of the different models because we ignored overheads due to synchronization, overheads due to passing data among the threads through the shared memory queues. In reality, you'd actually have to perform a little bit more complex experimental analysis to come up with these answers and draw conclusions as to which pattern is better suited for a particular application. In this lesson, we talked about threads, how operating systems represent threads, how threads differ from processes, and why they're useful in the first place. We spent some time talking about several mechanisms related to multithreading, and in particular, about mutexes and condition variables which are needed for synchronization. We also spent some time, at the end, talking about certain challenges, solutions, and design approaches that are related to threads and multithreading. As the final quiz, please tell us what you learned in this lesson. Also, we'd love to hear your feedback on how we might improve this lesson in the future. The last paper talked about threads in a generic way. It described multithreading concurrency synchronization in more generic terms. In this lecture, we will talk about PThreads, which is a very concrete multithreading system, and it's the defacto standard in UNIX systems. First off, PThreads stands for POSIX Threads. POSIX stands for Portable Operating Systems Interface, and it basically describes the interface, the system call interface, that operating systems need to support. And its intent is to increase interoperability among OSes. Within POSIX, PThreads describes the thread and correlated API that operating systems need to support, in order for creating and usage and management of threads. And this includes both the threads themselves, as well as the synchronization and concurrency related construct such as new texts and condition variables. First, let's look at the Pthread's thread abstraction and the thread creation mechanism that corresponds to the mechanisms that were proposed by Birrell. Birrell proposed the Thread abstraction and the Fork and join mechanisms that work with that abstraction. To represent threads, Pthread supports a pthread_t data type. That's the thread data type. Like Birrell thread data type, variables of this type will uniquely be identified with an identifier and will describe a thread, in this case a Pthreads thread. They'll have an ID, execution state, any other information that's relevant to the thread. Most of this is not something that you will directly see as a developer. Instead this is information that's used and maintained by the Pthread's library. The equivalent to the Fork that Birrell proposes is a more intuitive called pthread create. It takes as arguments a start routine. That's the equivalent to proc and Fork, as well as arguments equivalent to the arguments in Fork. It creates as a result and returns a new data structure that's of the type pthread type. And it's populated with all the relevant information so that this thread can start executing. It also takes as an input, a variable of type pthread attribute type. And this is a data structure that you can use to specify certain things about the thread that the pthreads library will need to take into consideration when managing the thread. We'll look at this in more detail shortly. Pthread_create also returns status information that indicates whether the creation was successful or a failure. Last, the alternative to Join is pthread_join. It takes two parameters, the thread structure, that's the thread that need to be joined, as well as the status variable. The status variable will capture all of the relevant return information, as well as the results that are returned from the Thread. The overall operation also returns a status that indicates whether the Join was successful or it failed. As you can see, these operations are fairly analogous to the operations that were proposed by Birrell. I would now like to talk, in a little more detail about Pthread Attributes. As we previously saw, there is a Pthread attribute-type argument to the pthread_create function. This argument gives us flexibility to specify certain features of the newly created thread. For instance, we can specify the stack size, scheduling policy or priority of the new thread. We can set the scope of the new thread, whether it's system or process, like what we explained in the previous lesson. Whether the threads should inherit attributes from the calling thread, whether it's joinable. Pthread establishes some default values for all of these parameters. And if you pass NULL as an argument to the the pthread_create function, you will achieve that default behavior. There's several calls that support operations on Pthread Attributes. Pthread attribute init or destroy allow us to create and initialize the attribute data structure. Or to destroy it and free that data structure from memory. Pthread set/get allows us to either set the value of an attribute field or to read that value. One of these attributes requires particular attention, and that's joinable. To explain this, I will first need to explain some mechanisms that are supported by Pthreads, but were not considered by Birrell. The mechanism not considered by Birrell is detachable threads. In Pthreads, the default behavior of thread creation is just like what Birrell described. The threads are joinable. With joinable threads the parent thread creates children threads, and can join them at a later time. The parent thread should not terminate until the children threads have completed their execution and have been joined via the explicit join operation. If the parent thread exits early, the children threads can turn into zombies, because they may have completed or died, but not exited or have not been reaped properly. In Pthreads there is a possibility for the children threads to be detached from the parent. Once detached, these threads cannot be joined. If a parent exits, these children are free to continue their execution. This really makes the parent and the children equivalent to one another, with the exception that the parent threads have some additional information on the children that they have created. To detach threads, Pthread provides a pthread_detach operation. That takes the thread data structure, the thread that needs to be detached as an argument. Threads also can be created as detached threads using the attribute DETACHED state. The value of this attribute first needs to be set to PTHREAD_CREATE_DETACHED, because otherwise the default behavior for Pthreads is for threads to be created as joinable threads. So with DETACHED thread since the parent thread doesn't need to stick around to wait for the children threads to complete, it can simply exit using pthread_exit. Here is an example of using pthread attributes. First the pthread attribute data structure must be created and initialized. This will allocate the data structure with sufficient memory, and it will set its values to the default pthread parameters. We can also adjust the values of the attributes using calls like pthread_attr_setdetachstate or setscope, or which ever other attributes we want to adjust. For instance, here we are setting the detach state to be PTHREAD_CREATE_DETACHED like what we described just a minute ago. And we're setting the scope, the scheduling scope of the threading system, to be the system scope. This means that the newly created thread will share resources equally with all other threads in the system. Once the attributes have been initialized and set to the desired values, the resulting data structure is passed to the call pthread_create Before we look at some examples, there are few things that we need to consider when compiling threads. First make sure to include the pthread header file, pthread.h, in your main file that contains the pthreads code, otherwise your program will not compile. Second, make sure to link your program with the pthreads library by passing the lpthread flag at compile time. On certain platforms, the better option is to use a pthread instead as a flag, which tells the compiler to link the pthreads library as well as to configure the compilation for threads. If you don't link the library your program may not report certain compilation errors at compile time, but it will still fail. And finally, it's always a good idea to check the return values on common functions like when you're creating threads, creating variables, initializing certain data structures. This is a good programming practice in general, but it's extra useful when dealing with, all the [INAUDIBLE] of writing multithreaded programs. Here is a simple example of creating threads with pthreads, let's look at the main function first. We see that in a loop we create pthread_create a number of times or, and we create four threads where each of the threads executes the function hello. All this function does is print Hello Thread, it doesn't take any arguments and that's why we pass NULL as a argument to the pthread creation function. We also pass NULL in the attributes field because we're okay with just using the default pthread's behavior. That also means that these threads will be joinable, so then we have to, in a loop, join every single one of these threads. So, as a quick quiz, what do you think will be the output of this program? Type your answer in this text box. The end result here is straightforward. The string hello thread, will appear four times, will be printed four times once by every one of the threads we've created. Let's look at a slightly different example. Here the threads need to execute a function, thread function. That's the function that's past the pthread create that takes in one argument. This is an integer argument and the function, what it does, it prints out thread number and then the number, the integer that was provided as an argument. The variables p and myNum are private to every one of the threads, so they are only valid in the scope of the thread function. Since we have multiple threads executing, four, every one of them will have its own private copies of these two variables, and they will potentially and in fact, likely be set to different values. When a thread is created, we see that the very first thing that happen are that it sets these private variables to values that depend on the input parameter. If you look at where the threads were created, we see that this input parameter, this argument is identical, that is, the index that's used in this loop. So once the thread sets these private variables, every one of them will print out a line, pthread number, and the value of the private variable, my number. For this slightly modified example, what are the possible outputs? Instead of typing in your answers, here's some possible outputs, and you should check all that apply. The first output with sequential thread number 0, 1, 2, 3, is possible since I, whose values past this an argument to the thread creation function, has values that reach from 0 to 3. The next output, the print out, is a little bit arbitrary thread number 0, 2, 1, 3. But this is still possible because as we said earlier. We don't have control over how these newly created threads will be actually scheduled. So, it's possible that just the order in which their execution was scheduled, so the order in which every one of them performed the printf operation was slightly different that the order in which they were created. Now the last output that's actually also possible. Now, you may be asking yourself how since the print out thread number one, which appeared in the previous two options, doesn't even appear in this case. Is that an indication that that thread wasn't even created? If we look at this loop in main, we see that we must have really executed the printout operation for every one of the four created threads. So we really would expect that one of them would have printed out thread number one, when we pass the argument i equals 1. Let's explain what happened to that line in the next morsel. >From the previous quiz the problem is that the variable i that's used in this thread creation operation is a globally visible variable that's defined in main. When its value changes in one thread, every one of the other threads will see the new value. In this particular case the second thread that was created in pthread_create was created with i equal 1. In the thread function, p will become equivalent to the address of i and myNum will then become equivalent to the actual value of i, so that's presumably 1. However, it is possible that before this thread had a chance to execute these operations and set the value of myNum to be 1, the main thread went into the next iteration of this for loop. And there it incremented i. So i is now 2. Since we passes an argument the address of i, p will also correspond to the address of i. So it will point to the same i and then myNum will actually take as a value the new value of i so it will take as a value 2. So it's not like we lost the print out from that second thread that we were expecting with print out thread number 1, it's just that both the second and the third thread ended up seeing that the value of i is 2 and that's why then printing out thread number 2. We call this situation a data race, or a race condition. It occurs when one thread tries to read a variable that another thread is modifying. In this example the second thread that we created was trying to read the variable i, and we were expecting it that it would read i equal 1, however at the same time the main thread was modifying i, was incrementing it, and it became 2. To correct the problem lets look at a slightly modified code here. We see that in the for looping main the value of i is first copied into an array. Into an element of an array tNum. The array has s many elements as there are threads and when we are creating a thread we pass as an argument the address of the particular element of the array that corresponds to that thread number. By creating this array then, it's like as if we created local storage, or private storage, for the arguments of every single one of the threads that we create. Now we don't have to worry about the ordering of how the new threads will execute the operations, because every one of them will have their own private copy of the input arguments that won't change. Now that we have fixed the error, we have one more quiz question. What are the possible outputs for this program? Here are your three choices. You should check all that apply. Now that we have fixed the error, and every one of the threads has its own private storage area to store the argument i, we expect to see the, line thread number, with the numbers 0, 1, 2, and 3 appear in the output. Given that, this first insert is not correct, and both of these two outputs, the second and third output, are correct answers to this question. To deal with the mutual exclusion problem, pthread supports mutexes. As we explained when discussing Birrell's paper, mutexes provide a mechanism to solve the mutual exclusion problems among concurrent threads. Mutual exclusion lets us ensure that threads access shared state in a controlled manner. So that only one thread at a time can perform modifications or otherwise access that shared variable. Birrell proposed the use of the mutex itself and an operation to lock mutexes. In pthreads, the mutex data structure is represented via pi pthread mutex type. For the log operation, remember that Birrell used the block construct where the critical section was protected by these curly brackets. Where the open curly bracket meant that the mutex was being locked, and the closed curly bracket meant that the mutex was unlocked or free. In pthreads, this concept is supported explicitly, there is a separate pthread mutex lock operation and a separate pthread mutex unlock operation. Whatever code appears between these two statements will correspond to the critical section. As an example, remember that in the thread introductory lecture, we implemented the safe_insert operation using Birrell's construct in the following way. With pthreads, the same safe_insert operation would be implemented as follows, we would be explicitly be locking and unlocking the mutex around the insert operation in the shared list, my_list, and also note that the mutex is of appropriate type, pthread_mutex type. Pthread supports a number of other mutex related operations. Several of them are worth highlighting. First, mutexes must be explicitly initialized. This operation allocates a mutex data structure and also specifies its behavior. It takes as an argument a mutex attribute variable, and this is how we specify the mutex behavior. By passing now as this argument, we have an option to specify the default behavior from mutexes, or we can set one or more attributes that are associated with mutexes. For instance, pthreads permits mutexes and condition variables in general to be shared among processes. The default behavior would make a mutex private to a process, so only visible among the threads within a process, whereas we can explicitly modify that behavior and make sure that the mutex can be shared with other processes. Another interesting operation is pthread_mutex_trylock. Unlike the lock operation which will block the calling thread if the mutex is in use, what trylock will end up doing is it will check the mutex, and if it is in use, it will actually return immediately, and it will notify the calling thread that the mutex is not available. If the mutex is free, trylock will result in the mutex successfully being clocked. But if the mutex is locked, trylock will not block the calling thread. This gives the calling thread an option to go and do something else and perhaps come back a little bit later to check if the mutex is free. Also, you should make sure that you free up any pthread related data structures, and for mutex, for instance, you have the mutex destroy operation. These are just some of the operations pthread support from mutexes. The ones we described here are enough to get your started with pthreads, and you can always refer to the pthreads documentation for information on the others. In the previous lesson, we mentioned a number of common pitfalls where it comes to writing multithreaded programs. A few that are worth mentioning in the context of pthread mutexes include the following. Shared data should always be accessed through a single mutex. This is such a frequent error that it's worth reiterating. Next, the mutex scope must be visible to all threads. Remember, a mutex cannot be defined as a private variable to a single thread. Including main, you must declare all of your mutexes as global variables. Another important tip is to globally order the locks. Once we establish an order between the locks, basically between the mutexes in the pthreads program, then for all threads we have to make sure that the mutexes are locked in that particular order. Remember, we said that this is a way to ensure that dead locks don't happen. Finally, remember to always unlock a mutex. Moreover, make sure that you always unlock the correct mutex. Given that pthreads has separate lock and unlock operations, it can be easy to forget the unlock, and compilers will not necessarily tell you that there is a problem with your code. So you have to make sure that you keep track of your locks and unlocks. As with described in Birrell condition variables are synchronization constructs which allow block threads to be notified once a specific condition occurs. Birrell proposed the condition as condition variable abstraction as well as three operations. Weight, signal, and broadcast that can be performed on conditioned variables. In pthreads condition variables are represented via the designated condition variable data type. The remaining operations align really well with Birrell's mechanisms. For instance, for weight, pthread has a pthread condition weight that takes two arguments, a condition variable and a mutex, just like what we saw in Birrell's weight. The semantics of this operation is also identical to Birrell's wait. A thread that's entering the wait operation, a thread that must wait, will automatically release the mutex and place itself on the wait queue that's associated with the condition variable. When the thread is woken up, it will automatically re-acquire the mutex before actually exiting the wait operation. This is identical to the behavior we saw in Birrell's wait. Identical to the signal and broadcast mechanisms in Birrell, PThreads has. Pthread condition signal and pthread condition broadcast, that we can use to either notify one thread that's waiting on a condition variable using the signal operation, or to notify all threads that are waiting on a condition variable using the pthread condition broadcast operation. There are also some other common operations that are used in conjunction with condition variables. These include the init and destroy functions. Pthread_condition_init is pretty straight forward, you have to use this operation in order to allocate the data structure for the condition and in order to initialize it's attributes. Like what we saw with mutexes. The attributes can further specify the behavior that pthreads provides with conditions. For instance an example is whether or not the conditions variable will be used only within threads that belong to a single process or also shared across processes. And similar to what we saw with the mutex and threads attributes data structures. Passing null in this call will result in the default behavior that's supported by pthreads. That happens to be that the condition variable is private to a process. Just like threads condition variables should be explicitly freed and reallocated, we use the condition destroy call for that. And finally, a few pieces of advice regarding the use of condition variables. First make sure you don't forget to notify the waiting threads. Whenever any aspect off a predicate that some threads are waiting on change, make sure that you signal or broadcast the correct condition variables that these threads are waiting on. Next, if you're ever in doubt whether you should use signal or broadcast, use broadcast untill you figure out what the desired behaviour is. Note that with broadcast you will lose performance. So, make sure you use the correct notification mechanism, signal or broadcast, when you need to wake up threads from a condition variable. Remember, since you don't actually need the mutex to signal and broadcast, it may be appropriate for you to remove that signal and broadcast operation. Until after you've unlocked the mutex, just like what we saw in the introductory lecture about threads. We will point out some of these options during the discussion of an actual pthreads example that we'll do next. Now to tie everything together, we will look at an implementation of the classic producer-consumer problem that uses the pthreads library. We will look at the source code section by section. In this first, for instance, section this is the global scope where all of the different variables are defined. If you happen to get lost as I trace through this code, then please reference the source code link thats provided in the instructors notes. So lets take a look at this. In this producer-consumer example, we have a shared buffer of size buffer size, and it happens to be three. There are three also shared variables where num refers to the number of elements in the buffer, and then add and rem refer to the indices that point to the element in this buffer where we need to add the next element or to remove the next element from. For instance if this is our shared buffer initially all of these variables would be zero. When we add one element, that means that the total number of elements is one. Adding new elements will have to happen in the next field in the buffer array. And removing an element still remains to be zero, because this is the element we need to remove. Adding a second element changes the values of these shared variables as follows. So the total number will be two. And then new additions should be placed in the buffers of two element. And we still haven't removed anything. When we remove one element from the buffer, the x, that means that the total number of elements in the buffer is now one. Still, the next available slot in the buffer, that we can use to add an element, is two. And the slot that contains the next valid entry in the buffer that we should be removing next is one, the y. So, this illustrates how this buffer and these three shared variables are used to manage the producer consumer data. The shared variables are used in conjunction with a mutex, and we use this mutex initializer statement, that this basically automatically initializes the mutex. So it does the function of attribute init, essentially. And we're going to use two condition variables. One, c_cons, which will be used by the consumers, so the consumers will be waiting on this condition variable. And then the other one, c_prod, and this one will be used by the producers. The producers will be waiting on this variable. There are also two functions, two procedures, the producer operation that will be executed by our producer threads, and the consumer operation and that will be executed by our consumer threads. Now let's look at the main portion of the code of this producer consumer implementation. We'll be creating two threads, thread id1 and thread id2. The first thread will be created to execute the producer function, and then the second thread will be created to execute the consumer function. We're using the default behavior for these, so we will have to join them later in the main function, in the main thread. And we will look at the producer and consumer functions next, but they don't take any input so we're pasisng NULL as arguments. Note how we're checking for the return code of the pthread_create operation in order to help with debugging. So the thread, the main thread, the one that's executing command, will just create the producer and consumer threads, and then it will do nothing. It will wait for them to exit. The producer functions of the producer thread will try to execute for 20 times a loop in which it tries to produce an element for the shared buffer. During each pass through this loop, the producer thread will be trying to modify the shared buffer to add an element in that buffer, and then also to change the values of the shared variables, like add a num. Therefore, all of this has to happen within a mutex_lock, mutex_unlock operation. We first do some error checking to make sure that we don't have buffer overflow. Now we're trying to insert, we're trying to produce data for the shared buffer. If the number of elements that are currently in the buffer is equal to the buffer size, that means that the buffer is full. So we have to wait, we have to wait on the condition variable that associated with the producers, and this wait operation we have to use the mutex as part of it. Remember, this mutex has to be used as an argument of the wait call so that the pthreads library knows which particular mutex needs to be freed and then reacquired after we complete the wait. Now, when we ultimately come out of the wait, so when the producer indeed comes out of the wait operation because the buffer is no longer full, so a consumer must have consumed some of the items in this buffer, then what do we do? Then the producer adds an element in the buffer, so it copies the value of i, this index i, copies the value of i into the element of the buffer that is indexed by the value of add, and then increments both the add, a variable, as well as the num, the total number of elements in the shared buffer. Note we may have a wraparound situation. So, given that the buffer is a fixed size, buffer size, this add, we need to wrap around in case it becomes greater than buffer size. Once we perform this, then we're done. We've inserted an element. We've updated the variables in such a way that it reflects that now there is a new element in the buffer. And so, we can unlock the mutex. Now, the one thing that we didn't do is we didn't do any kind of signaling or broadcasting while we were performing this. It is possible that the buffer was empty when we performed this insert operation, and because of that, that currently there is some consumer threat that's waiting on a condition. We've inserted just one element. Therefore, only one consumer thread can proceed. So what we'll do is we will notify a thread that's waiting on the condition variable, and we will use for this the signal operation pthread condition signal because again, we inserted one element, no point waking up seven consumer threads. For sanity, I've inserted here some printout statements that will help you keep track of what's going on. These are not critical to the behavior of the multithreaded program. Now if we look at the consumer code, so this is the code that's executed by every one of the consumer threads. There, what every one of the threads needs to do, it's going to, in a loop, so in a continuous loop, it will try to remove elements from the shared buffer. In every pass through this loop, the consumer thread will try to remove an element from the buffer and update the rem and num variables accordingly. To these kinds of modifications we have to lock the mutex, and then once we're done we have to unlock the mutex. Again, like in the producer case, we do some sanity error checking to make sure that our buffer doesn't have a negative number of elements. And then before we actually start removing elements from the buffer, we have to make sure that there are any elements in the buffer to begin with, that the buffer isn't empty. If the buffer is empty, so if this variable num that indicates the total number of elements in the buffer is 0, the consumer thread has to wait. The wait is associated with the consumer condition variable. Remember this is the condition variable that the producer thread was signaling to, and also with this wait we have to use the mutex that protects this piece of code. Once we have successfully completed the wait part, so a producer has generated more data, the consumer has been notified, it has come out of the conditional wait, and yes indeed, re-verified that now there is data in the buffer that can be consumed. Then we can safely move on to execute this portion of the code. To remove an element from this buffer, what we really do is we read out the element of the buffer that's pointed by the rem variable. So rem points to the next valid field in the buffer where there is valid data that could be removed. We also make sure to print out that value i, just for sanity, to make sure that we're doing the correct thing. Now that that element that was pointed by rem is no longer valid, we have to make sure we increment rem to point to the next entry in the buffer. Just like in the case with the add index, we have to do some modular computation to deal with wrap-arounds around the end of the buffer. And we have to also make sure we decrement the total number of elements in the array. Now in the producer code, the producer was checking the value of this variable num against buffer size to determine whether or not the buffer is full. Whenever you determine that these are identical so that the buffer is full, the producer was waiting to be notified on a condition variable that's associated with the c producer. Therefore, in the consumer code, now that we've decremented this, variable num, now that we have consumed an element from the buffer and it's no longer full. We need to go ahead and notify a producer thread that there is now room in the buffer to insert other elements, to produce more data. The consumer consumed one element of this buffer, so it makes no sense to broadcast. Only one producer can insert one element in the buffer. Therefore, we will just use pthread_cond_signal. And note how here we are using this pthread_cond_signal outside of the lock-unlock operation. Given that we will always signal once we complete this code that this operation is not conditional upon some specific values of any of the shared data structure, we can release the mutex and then signal, and this will avoid the spurious wakeups issue that we talked about before. Again, the printf's are for debugging purposes. In this lesson we took an in-depth look at the PThreads library. We talked about PThreads Creation, Mutexes, Condition Variables, and we compared these constructs to the constructs that were proposed in Birrell's paper. We spent some time talking about common practices when it comes to PThreads, as well as some safety tips. And then we looked at how to compile PThread programs. And then walk through several programming examples. As the final quiz, please tell us what you learned in this lesson. Also, we'd love to hear your feedback on how we might improve this lesson in the future. In our introductory lecture on threads and concurrency, we talked about the fact that threads can be implemented at the kernel level, at the user level, or both. In this lesson we will re-visit that statement and see what it is that is necessary in terms of data structures and mechanisms from the operating system. In order for us to be able to implement threads at both the kernel and the user level. During this discussion, we will also look at two notification mechanisms that are supported by OSs. And that includes interrupts and signals. To make the discussion in this lecture more concrete, we will use two older papers, the Eykholt paper Beyond Multiprocessing: Multithreading the Sun OS Kernel and the Stein and Shah paper on implementing lightweight threads. These are older papers, but they provide us with some historic information on how threading systems evolved over time. There are links to both of these papers in the Instructor's Notes. We will end this lecture with a brief summary of the current threading model in the Linux operating system. Let's start by revisiting the illustration we used in the threads and concurrency lecture. There we explained that threads can be supported at user level, at kernel level, or both. Supporting threads at the kernel level means, that the OS kernel itself is multithreaded. To do this, the OS kernel maintains some abstraction, for our threads of data structure to represent threads, and it performs all of the operations like synchronizations, scheduling, et cetera, in order to allow these threads to share the physical resources. Supporting threads at the user level means that there is a user-level library, that is linked with the application, and this library provides all of the management in the runtime support for threads. It will support a data structure that's needed to implement the thread abstraction and provide all the scheduling synchronization and other mechanisms, that are needed to make resource management decisions for these threads. In fact, different processes may use entirely different user-level libraries that have different ways to represent threads that support the different scheduling mechanisms, et cetera. We also discussed several mechanisms, how user-level threads can be mapped onto the underlying kernel level-threads, and we said these include a one-to-one, many-to-one, and a many-to-many mapping, and briefly touched upon some of the pros and cons of each approach. Now, we'll take a more detailed look at about, what exactly is needed to describe kernel versus user-level threads, and to support all of these types of models. Let's start by looking at what happens in a single threaded process. The process is described with all of the process-related state, that includes the address space or the virtual to physical address mappings, its stack, its register since it has a single thread. Whenever this process makes a system call, it tracks into the kernel, executes in the context of a kernel thread. All of the information about the state of this process we said is contained in its process control block. And let's, for now, assume that we're dealing with just one CPU. Let's now make the process multithreaded, so this will look like our many-to-one model, where many user-level threads are supported by one kernel level thread, and that there is a user level threading library that manages these threads. This user-level library will need some way to represent threads so that it can track their resource use and make decisions regarding scheduling and synchronization. So it will have some user-level thread data structure. If we wanted it to be multiple kernel level threads associated with this process, we see that currently we have the process control block by containing the, all the virtual address mappings, as well as the execution state of the process thread. We don't want to have to replicate this entire data structure with all this information, just so as to represent different stack and register values for the kernel-level entities. So we will start splitting up this process control block structure to separate the information that's specifically useful to represent the execution state of the kernel-level threads, so their stack and register pointer. And this will still have, the process control block will still have the virtual address mappings, as well as some additional information that's relevant for the entire process, so for all of the kernel-level threads. Note that from the perspective of the user-level threading library, the underlying kernel-level threads look sort of like virtual CPUs. So the threading library looks at the user-level threads and decides which one of the user-level threads will be scheduled onto the underlying kernel-level threads. Unix-based systems, for instance, have certain operations called set jump and long jump which are useful when we need to save and restore the context of the user-level thread. Now, let's say we have multiple such processes. Clearly we will need copies of these user-level thread structures, process control blocks, and kernel-level thread structures to represent every single aspect of every one of these processes. So, we'll need to start maintaining certain relationships among them. With respect to the user-level threads within the threading library, it keeps track of all of the user-level threads that represent the single process. So, there is a relationship between them and the process control block that represents that address space. For each process we need to keep track of what are the kernel-level threads that execute on behalf of this process, and vice versa, for each kernel-level thread we have to make sure we know what is the address space within which that thread executes. If the system has multiple CPUs, we need to have a data structure to represent the CPU. And then we need to maintain a relationship between the kernel-level threads and the CPU. So what is the CPU that a kernel level thread has affinity to, last strand it was scheduled on, and then for a CPU, a pointer to its current thread or a pointer to the threads that typically run there, and similar information. When the kernel itself is multithreaded, we said we can have multiple kernel-level threads supporting a single user-level process. When the kernel needs to schedule, or context switch, among kernel-level threads that belong to different processes, it can quickly determine that they point to a different process control block. So they will have different virtual address mappings, and therefore can easily decide that it needs to completely invalidate the existing address mappings and restore new ones. In the process, it will save the entire process control block structure of the first kernel-level thread, and then if it's context switching to the second one, it will restore the entire process control block structure of the second one. Now when two kernel level threads belong to the same address space there's actually information in the process control blog that is relevant for the entire process. However there is information here that is specific to just one kernel level thread. For instance this includes information about signals or system call arguments. So when we're contact switching among these two kernel level threads, there is a portion of this process control block information that we want to preserve, like all of the virtual address mappings. But then there is portion of it, that's really specific to the particular kernel level thread and it depends on what is the user level thread that's currently executing. So it's something that the threading library directly impacts. So we will split up the information that was contained originally in the process control block, and we will separate it into the hard process state that's relevant for all of the user level threads that execute within that process. And then, the more light weight process state that is only relevant for a subset of the user level threads that are currently associated with a particular kernel-level thread. So we started off with one large contiguous process control block structure, and then we divided the information that was contained in it across a number of different data structures. When we had just one single process control block, it was a large contiguous data structure. We have to maintain separate copies for every single thread, even though they may share some information. Whenever we need to context switch we need to save and restore this entire data structure, and it is large we said. And finally, it's just this one data structure that's used for so many different operations. For scheduling, for memory management, for synchronization. If we want to customize any aspect of that, we are potentially going to affect multiple OS services, and so it makes updates a little bit challenges. So all in all, there are multiple limitations to this approach of using a single process control block structure to represent all aspects of the execution state of a process. Scalability is limited due to the size. Overheads are limited because they need to have private copies. Performance is affected because everything has to be saved and restored. And then flexibility is affected by the fact that updates are a little bit more difficult. In contrast, when we have multiple data structures, we actually end up with mu, multiple small data structures. The information that was contained in the original process control block, is now maintained via pointers by pointing to much smaller data elements. Then it becomes easy to share portions of that information. We will point to the same data structure for those components of the state, which are identical across threads or processes. And we will create new elements when we need to have different information. On a context switch, only that portion of the state that actually needs to change will be saved and restored. And then both, any kinds of modifications will impact only subset of the data elements. And then the interactions between the user-level library and the system will also be carried out through a much smaller more, more confined interfaces. All in all, this trend to use multiple data structure leads to improvements across the board. We gain on scalability, on overheads because we don't have to have separate copies for, for everyone. We have improvements in performance because context which time can be reduced, and we have more flexibility. As a result, operating systems today typically adapt this type of approach for organizing information about their execution contexts. Now that we have discussed how thread structures are separated let's take a look at an actual Linux kernel implementation in this quiz. For each of the questions in this quiz we will be referencing version 3.17 of the Linux kernel. The first one is, what is the name of the kernel thread structure that's used in Linux? We're looking for the name of a C structure, basically. The second question is, what is the name of the data structure, that's actually contained in the above data structure, that describes the process that the kernel thread is running? Again, we're looking for a name of a C structure. Provide your answers in these text boxes and refer to the instructor notes that reference the 3.17 version of the Linux kernel. If you browse the kthread.H header file, you will see in line 66 that there is a structure ktread_worker. This data structure, as well as the various functions that are defined in this file, provide a simple interface for creating and stopping kernel threads. You can see that within the kthread_worker data structure, there are four members. The stem lock data structure is definitely not the one that's used to describe a process, nor is the list head that points to a list of kthread_workers. If you click on the next one, task_struct, you will see that it's a holding place for tons of important information regarding a process. So our answer now is at task_struct. Let's look now at the data structures that are described in the two reference papers of this lesson. The two papers describe the kernel and user-level implementations of threads in the SunOS 5.0 kernel of Solaris 2.0. So Solaris is the operating system. Sun, where this work was done, no longer exists; it was bought by Oracle in 2010. But it was very well known for the quality and stability of its UNIX distributions. It was also one of the leader in introducing new and revolutionary features into the kernel. And this is why we are looking at its threading model. This is a diagram from figure one in the Stein and Shah paper, Implementing Lightweight Threads. And it illustrates quickly the threading model supported in the operating system. Going from the bottom up, the OS is intended for multi-processor systems, with multiple CPUs and the kernel itself is multi-threaded. There are multiple kernel-level threads. At user level, the processes can be single or multithreaded. Both many-to-many as well as one-to-one mappings are supported. Each kernel-level thread that's executing a user-level thread, has a lightweight process data structure associated with it. >From the user-level libraries perspective, these lightweight processes represent the virtual CPUs onto which it's going to be scheduling the user-level threads. At the kernel level, there will be a kernel-level scheduler that will be managing the kernel-level threads and scheduling them onto the physical CPUs. We will now look a little more closely at the user-level thread data structures. They are described in the implementing lightweight threads paper by Stein & Shah. This does not describe pthreads, the POSIX threads, but it's a similar type of user-level threading library. When a thread is created, the library returns a thread ID. And this is not a direct pointer to the actual thread data structure like we've implied before. Instead, it's an index in a table of pointers. It is the table pointers that in turn point to the actual thread data structure. The nice thing about this is that if there is a problem with the thread, if the thread ID were a pointer, then that pointer would just point to some corrupt memory. And we can't really figure out what's going on. Whereas here, by having the thread ID index into a table entry, we can encode some information into the table entry that can provide some meaningful feedback or an error message. The thread data structure, we said, contains a number of fields, registers, signal mask, priority. There's also the stack pointer, of course, that points to the stack, and then there is the thread local storage area. This area, this includes the, variables that are defined in the thread functions that are known at compile time, so the compiler can allocate private storage on a per-thread basis for each of them. The stack itself, its size, it may be defined based on some library defaults or the user can provide a stack. But basically the size of a lot of this information, is known up front at compile time, so we can create these thread data structures and sort of layer them in a continuous way, and that can help us achieve locality. It can make it easy for the scheduler to find the next thread. It just has to basically multiply the thread integs with the size of the data structure. The problem however is that the threading library doesn't really control the stack growth, so it doesn't in, inject itself between any kind of update and what gets written on the stack. And then the operating system itself, it doesn't know that there are multiple user-level threads. So it's possible that as the stack is growing, that one thread will end up overwriting the data structure of another thread. If this happens, the tricky part is that the error, the problem will be detected when that other thread gets to run. However, the cause of the problem is a completely different thread. So, so this makes debugging a little bit tricky. The solution that was introduced in this paper was to separate the information about different threads with a so-called red zone. This really refers to a portion of the virtual address space that's not allocated. So if a thread, it's running, and its stack is increasing, if it tries to write to an address that basically falls in this red zone region, then the operating system will cause a fault. Now it's however much easier to reason about what happened because the fault, the problem, was directly caused by the thread that was executing. So it's easier to do root cause analysis and to fix the problem. Let's move now to the kernel level data structures. First for each process we maintain information about that process. What are all the kernel level threads that execute within that process address space? So what are the mappings that are valid between the virtual and physical memory? What are the user credentials? For instance, if this process is trying to access a file, we have to make sure that that particular user has access to that file. And then, information like, what are the signal handlers that are valid for this process. We'll talk about this a little bit later, but for now, know that this is information about how to respond to certain events that can occur in the operating system. Next, we have the lightweight process data structure and this contains information for a sub subset of the process. For instance, it can have information that's relevant to one or more of the user level threads that are executing in the context of the process. And keep track of their user-level registers and the system call arguments. The information that's maintained in a light-weight process data structure is in some ways similar to what we maintain at the user level in the user-level thread data structure. But this is what's visible to the kernel, so when the OS-level schedulers need to make scheduling decisions they can see this information and act upon it. Also note that we track resource usage information in this data structure. At the operating system level, the kernel tracks resource uses on a per kernel thread basis. And this is maintained in the data structure for the lightweight process that corresponds to that kernel level thread. So if we want to find out the aggregate resource usage for the entire process, we need to basically walk through all of the lightweight processes that are associated with it. The kernel level data structure includes the kernel level information, like registers, stack pointers, scheduling class. And it also has pointers to the various data structures that are associated with this kernel. So what is the lightweight process? What is the actual address space? What is the CPU where this is running? One thing to note about these two data structures is that the kernel-level thread structure, it has information about a kernel-level thread, about an execution context, that is always needed. They're operating system level services that need to access some information even when a thread is not active. Like, for instance, scheduling information if they need to decide whether they need to activate that thread. So this information is basically not swappable. It always has to be present in memory. Whereas in contrast the light weight process data structure, the information that it maintains does not always have to be present in memory so. If we're running under memory pressure, it is possible to swap out this content. This also potentially allows the system to support larger number of threads in a smaller memory footprint than what would've been the case if everything needed to be constantly memory. Next is the CPU data structure. It has information like the current thread that's currently scheduled, list of the other kernel level threads that ran there. Some information how to actually execute the procedure for dispatching a thread, or how to respond to various interrupts on the referral devices. Note that if we have information about the CP and a given CPU once we know the current thread through it we can find that information about all of the different data structures that are needed to rebuild the entire process state. On the SPARC architecture that is used in the Solaris papers, there are extra registers, so there are lots of registers. And the implementation is such that there is one dedicated register that is used to point to the current thread at any given point of time. So you're in context which this register is updated. But what it implies is that it's easy to just access that register and then immediately be able to start tracking through these pointers to find the right information. That's in contrast to perhaps having to access memory to read the CPU structure to then read the current thread information, et cetera. Here's how the Eykholt paper on multithreading the SunOS kernel describes the relationship between all of these data structures. This is figure two in this paper. A process data structure has information about the user, for instance the address space, and then points to a list of kernel-level thread structures. Each of the kernel-level thread structures points to the likely process that it corresponds to, to it's stack, and to other information. The lightweight processing stack. This portion of the state is actually swappable. What's not shown in this figure that was showed in the previous image is any information about the CPU. And there is some other information, some other pointers that are not shown here so as not to clutter everything, like from the thread going back to the process, et cetera. So we have threads at the user level, we have threads at the kernel level. We will now see what are some of the interactions that are necessary in order to efficiently manage threads. Consider we have a multithreaded process. And let's say that process has four user-level threads. However, the process is such that, at any given point of time, the actual level of concurrency is just two. Basically, if you look at the process, it always happens that two of its user-level threads are waiting on I/O, and then some other two are actually executing. So, if our operating system has a limit on the number of kernel threads that it can support, it would be nice if the user-level process actually said, I just really need two threads. So when the process starts, the kernel will first give it, let's say, a default number of kernel-level threads and the accompanying lightweight threads. And let's say that is one. Then the process will request additional kernel-level threads, and the way it's done is that the kernel now supports a system call called set_concurrency. In response to this system call the kernel will create additional threads and it will allocate those to this process. Now lets consider this scenario in which the two user-level threads that were mapped on the underlying kernel-level threads block. They needed to perform some I/O operation and then they were basically moved on the wait queue that's associated with that particular I/O event. So the kernel level threads are blocked as well. Now let's say we have a situation in which the two user-level threads that were running on kernel level threads issued an I/O request, and now have to wait for that to complete. So it's a blocking I/O. What that means is that the kernel-level threads themselves, they're also blocked on that I/O operation. They're waiting in a queue somewhere in the kernel for that I/O event to occur. Now we have a situation where the process as a whole is blocked, because it only had two kernel-level threads, both of them are blocked, and there are user-level threads that are ready to run and make progress. The reason why this is happening is because the user-level library doesn't know what is happening in the kernel, it doesn't know that the kernel threads are about to block. What would have really been useful is if the kernel had notified the user-level library before it blocks the kernel-level threads. And then the user-level library can look at its run queue, it can see that it has multiple runnable user-level threads, and, in response, can let the kernel know, so, call a system call to request more kernel-level threads or lightweight processes. Now in response to this call, the kernel can allocate an extra kernel-level thread, and the library can start scheduling the remaining user-level threads onto the associated lightweight process. At a later time when the I/O operation completes, at some point the kernel will notice that one of the kernel-level threads is pretty much constantly idle, because we said that that's the natural state of this particular application. So maybe the kernel can tell the kernel-level library that, you no longer have access to this kernel-level thread, so you can't schedule on it. By going through these examples you realize that both the user-level library doesn't know what's happening in the kernel, but also the kernel doesn't know what's happening at the user level. Both of these fact cause for some problems. To correct for these issues, we saw how in the Solaris threading implementation, they introduced certain system calls and special signals that can be used to pass or request certain things among these two layers. And basically this is how the kernel-level and the user-level thread management interact and coordinate. Let's take a quiz and look at an example of how the pthreads threading library can interact with a kernel to manage the level of concurrency that a process gets. The first question is, in the pthreads library, which function sets the concurrency level? We're looking for a function name here. For the second question, given the above function, what is the concurrency value that instructs the underlying implementation to manage concurrency as it finds appropriate? And we're looking for an integer value here. And please feel free to use the Internet as a resource to understand the answer to this question. The answer to the first question is a very straightforward pthread_setconcurrency function. You can see that you can specify an exact value or you can pass a 0 which will mean that the underlying manager should decide how to manage the concurrency level for the particular process. In the previous morsel, we talked about the fact that the kernel and the user-level library, don't have insight into each other's activities, and let's talk about that a little bit more now. In the kernel-level, the kernel sees all of the kernel-level threads, the CPUs, and, the kernel-level scheduler is the one that's making decisions. At the user-level, the user-level library sees the user-level threads that are part of that process, and the kernel-level threads that are assigned to that process. If the user-level threads and the kernel-level threads are using the one-to-one model, then every user-level thread will have a kernel-level thread associated with it, so, the user-level library will also essentially see as many, kernel-level threads, but it will be the kernel that will actually manage those. Even if it's not a one-to-one model, the user-level library can request that one of its, user-level threads be bound to a kernel-level thread. This is similar of what we would want to perhaps to in a multi-CPU system, if a particular kernel-level thread, is to be permanently associated with a CPU, except in that case we call it thread pinning, and the term that was introduced with the Solaris threads was that a user-level thread is bound to a kernel-level thread. And clearly, in a one-to-one model, every user-level thread is bound to a kernel-level thread. Now let's have the situation in which, one of the user-level threads has a lock, and so that, basically the kernel-level thread is now supporting the execution of that critical section code. Now let's say, the kernel preempted this kernel-level thread from the CPU to schedule the other one, so that means that the execution of this user-level thread, the execution of this critical section cannot continue. As the user-level library scheduler cycles through the rest of the user-level threads, if they need the lock, none of them will be able to continue. So only after the kernel-level, the schedule of this thread again, will the critical section complete, the lock will be released, and so subsequently the rest of the user-level threads will be able to execute. So to reiterate, this problem that there is lack of visibility between the kernel and the user level-thread management, is because at the user-level, the library, makes scheduling decisions that the kernel is not aware of, and that will change the user to kernel-level mappings. And also data structures, like mutexes and wait queues, that's also invisible to the kernel. So the fact that this lack of visibility causes situations such as the one that we described really leads us to the conclusion that we should look at one-to-one models, to address some of these issues. Since the user-level library plays such an important role in how the user-level threads are managed, we need to understand exactly, when does it get involved in the execution loop. The user-level library is part of user process, part of its address space, and occasionally the execution basically jumps to the appropriate program counter into this address space. There are multiple reasons why the control should be passed to the user-level library scheduler, a user-level thread may explicitly yield, a timer that's set by the user-level threading library may expire, also we jump into the user-level library scheduler whenever some kind of synchronization operation takes place, like, when we call a lock, clearly that thread may not be able to run if it needs to be blocked, when we call an unlock operation, then we need to evaluate what is then new runnable thread that the scheduler should allocate on the CPU. And in general, whenever we have a situation where a blocking user-level thread becomes runnable, we jump into the scheduler code, this is part of the library implementation, this is not something that you will explicitly see. In addition in being invoked on certain operations that are triggered by the user-level threads, the library scheduler is also triggered in response on certain events, certain signals that come either from timer or directly from the kernel. The next morsel should give you an illustration of these interactions. Other interesting management interactions between the user level threading library and the kernel level thread management occur when we have a situation where we have multiple CPUs. In all of the previous cases we've discussed, we only had a single CPU. So all of the user level threads ran on top of that CPU, and then whatever changes, in terms of which of the user-level threads will be scheduled, were made made by user-level threading library were immediately reflect on that particular CPU. In a multi-CPU system, the kernel level threads that support a single process may be running on multiple CPUs, even concurrently. So we may have a situation when the user-level library that's operating in the context of one thread on one CPU needs to somehow impact what is running on another thread on another CPU. Let's consider the following situation. Let's say we have three user-level threads that are running, T1, T2, and T3. And their priorities are such, so that T3 has the highest priority followed by T2, and then T1 has the lowest priority. Let's say the situation is such that T2 is running in the context of one of the kernel-level threads and currently holds a mutex. T3, the highest priority thread, is waiting on that mutex, and so it's blocked. It's not executing. And therefore, the other user-level thread, T1, is the one that's running on the other kernel-level thread on the other CPU. Now, at some later point, T2 releases that mutex, it unlocks it. And as a result of that ,T3 becomes runnable. Now, in all three threads, T1, T2, and T3, are runnable, and so we have to make sure that the ones with highest priority are the ones that actually get to execute. What needs to happen is T1 needs to be preempted, since that's the one with the lowest priority among the three. And we're making this realization while running in the context of the T2 thread. When T2 performed the unlock operation, that's when we invoked the user-level threading library. And that's when we determined that we need to schedule T3 on top of the other context. We need to context switch T1. However, T1 is running on another CPU, and so we somehow need to notify this other CPU to do something to update its registers and its program counters. We cannot directly modify registers of one CPU when executing on another CPU. What we need to do, instead, is to send some kind of signal, some kind of interrupt from the context of one thread and one CPU to the other thread on the other CPU. And to basically tell this other CPU to go ahead and execute the library code locally because the library needs to make some kind of scheduling decision and change who's executing. Once that signal happens, the user-level library on the second CPU will determine that it needs to schedule the highest priority user-level thread, T3. And thread T1, which has lowest priority, will be the one that's blocked. So basically, once we start adding multiple CPUs and have multiple kernel and user-level threads in the process, the interactions between the management and the kernel and the user-level becomes a little bit more complex than what the situation is when there's only one CPU. Another interesting case when we have multi CPU systems and threading support at the user and the kernel level is related to synchronization. Consider the following situation. We have one user level thread T1 running on top of one kernel level thread on one CPU. And this thread currently has a mutex. A number of user level threads may be blocked, but then on another CPU, currently a user level thread T4 is scheduled. Let's say this thread T4 actually needs to log the same mutex that's currently held by T1. Now the normal behavior would be to place T4 in that case on the queue that's associated with this mutex. That's what we saw during our earlier discussion about threads and concurrency. However, on a multi-CPU system, it's possible to have this situation. The owner of the mutex, the one that's currently executing the critical section, is running on one CPU. And when we request that same mutex from the other CPU, it is possible that by the time we take this thread, T4, and context switch it and place it on the queue that's associated with this mutex. In that amount of cycles maybe the critical section here is very short and T1 will actually complete its execution. If that is the case if the critical section is short then we are better off if the thread that needs the mutex. Actually just ends up spinning on this CPU. Just burning a few cycles, waiting a little bit until T1 actually releases the mutex. If it takes less time for T1 to release the mutex. Were better off spinning than actually picking a thread, context switching it, and queueing it up on a mutex queue. Super short critical sections don't block spin. For long critical sections we will have the default behavior where a thread is actually properly blocked placed on a queue that's associated with a mutex until the mutex is freed. We call these kinds of mutexes which sometimes result in the thread to spin and other times it result in the thread to block adaptive mutexes. Clearly these only make sense on multi-CPU systems, since whether or not we spin is going to depend on whether the owner of the mutex, like in this case, is actually running on the other CPU. In a single CPU system, that definitely won't be the case, so then it doesn't make sence to consider the use of adaptive mutexes. Early on, when we first introduced mutexes, we said that it is useful to maintain some information about the owner of the mutex. These adaptive mutexes are one example of how such information can be useful. When we try to lock a mutex. If the mutex is currently busy, we can look quickly who the owner of the mutex is, and then verify whether that other thread is running on another cpu. That will tell us whether or not we should spin or block. Clearly we'll also need to have some idea about the kinds of critical sections that are used with this mutex, so as to determine whether it's likely that the owner of the mutex will release it quickly so we can spin, or not, and in that case we need to block. And at the end, I want to make some final points about destroying threads. Once a thread is no longer needed, so once it actually exits, it should be destroyed and its data structure, stack, etc., should be freed. However, since thread creation takes some time, like data structures need to be created and initialized, it makes sense to reuse these data structures, essentially as if we're reusing the actual threads. The way this is done is when a thread exits it's not immediately destroyed, the data structures are not immediately freed. Instead the thread is marked as it's on a death row. And periodically a special reaper thread will perform garbage collection which means that it will actually go ahead and free up all of the data structures that are associated with the threads on the death row. If a request for a thread comes in before the thread has been properly destroyed from the death row then its data structure and stack can be reused. And this will lead to performance gains since we don't have to wait for all the allocations. As we saw so far, the interactions between the kernel and the user-level library involve requesting, allocating, and scheduling threads. And this you may assume there's some number of threads allocated at startup to get the operating system to boot. So as a quick quiz, answer the following questions. First, in the Linux kernel's codebase, what is the minimum number of threads that are needed to allow a system to boot? Second, what is the name of the variable that's used to set this limit? Each of these questions can be answered by examining the source code of the Linux kernel. And please refer to the Instructors Notes for some useful pointers. The answer to the first question is 20 threads. If you look through the source code for fork.c, you will see that in the, in it fork function, there is a place among lines 278 and 282 that ensures that at least 20 threads are going to be created to get the system to boot. And if you found the answer to this question, then you will know that the variable that holds this value is referred to as max_threads. In the earlier description of data structures, we mentioned two terms that we have not yet talked about, interrupts and signals. Let's take a moment now to explain these concepts in a little more detail. Interrupts are events that are generated externally to a CPU by components that are other than the CPU where the interrupt is delivered. Interrupts represent, basically, some type of notification to the CPU that some external event has occurred. This can be from I/O devices like a network device delivering an interrupt that a network packet arrived or from timers notifying the CPU that a timeout has occurred or from other CPUs. Which particular interrupts can occur on a given platform depends on the specific configuration of the platform, like the types of devices that it has, for instance. Or the details about the hardware architecture and similar features. Another important characteristic about interrupts is they appear asynchronously. That's to say that they're not in the direct response to some specific action that's taking place on the CPU. Signals, on the other hand, are events that are triggered basically by the software that's running on the CPU. They're either for real generated by software, sort of like software interrupt, or the CPU hardware itself triggers certain events that are basically interpreted as signals. Which signals can occur on a given platform depends very much on the operating system. So two identical platforms will have the same interrupts, but if they're running a different operating system they will have different signals. Unlike hardware interrupts, signals can appear both synchronously and asynchronously. By synchronous here we mean that they occur in response to a specific action that took place on the CPU, and in response to that action, a synchronous signal is generated. For instance if a process is trying to touch memory that has not been allocated to it, then this will result in a synchronous signal. There's some aspects of interrupts and signals that are similar. Both interrupts and signals have a unique identifier. And its value will depend either on the hardware in the case of interrupts. Or on the operating system in the case of signals. Both interrupts and signals can be masked. For this, we use either a per CPU mask for the interrupt. Or a per process mask for the signals to disable or to suspend the notification that an interrupt or a signal is delivering. The interrupt mask is associated with a CPU because interrupts are delivered to the CPU as a whole. Whereas the signal mask is associated with a process, because signals are delivered to individual processes. If the mask indicates that the signal, or the interrupt, is enabled, then that will result in invoking the corresponding handler. The interrupt handlers are specified for the entire system by the operating system. For the signal handlers however, the operating system allows processes to specify their per process handling operations Now that we have compared and contrasted interrupts and signals, let's see how we can visualize these concepts. We'll use again an illustration within a toy shop, where we will try to make an analogy between an interrupt and a snowstorm warning, and a signal and a battery is low warning. The reason for these two choices is to make it a little bit more similar with the interrupt being generated by an event that's external to the CPU. So, an event that's external to the toy shop. Whereas the signal is more generated from within, so the battery is low is directly caused by the toy shop worker fixing a toy. First, each of these types of warnings need to be handled in specific ways. Second, both of them can be ignored. And last, we can think about both of them as being expected or unexpected. In a toy shop, handling these types of events may be specified via safety protocols or certain hazard plans. This is not uncommon. There may be, however, situations in which it's appropriate to just continue working. And finally, situations like the fact that the battery died are pretty frequent. They happen regularly, so they're expected. Whether or not it is expected for a snowstorm to occur, that will really depend on where the toy shop actually is. If we think about interrupts or signals, well, both of them are handled in a specific way and that's defined by the signal handler. Next, both interrupts and signals can be masked, as we said. And in that way, we can ignore them. And finally, as we previously discussed, these types of events can appear synchronously or asynchronously. So we have some analogy between these two contexts again. Now lets talk a little bit more in depth about interruption signal handling. Lets start with interrupts. When a device like disk for instance wants to send the notification to the CPU it sends an interrupt by basically sending a signal through the interconnect that connects the device to the CPU complex. In the past for this we used dedicated wires, but most modern there's a special message called a message signal interrupter, MSI, that can be carried on the same interconnect that connects the devices to the CPU complex. So PC Express, for instance. Based on the pins where the interrupt occurs or based on the MSI message. The interrupt can be uniquely identified so we know, based on this information, exactly which one of the devices generated the interrupt. Okay, so now the interrupt interrupts the execution of the thread that was executing on top of the CPU. And now what? Now, if the interrupt is enabled only, based on the interrupt number a table is referenced. For all the interrupt supported in this system, this table specifies what is the starting address of the interrupt handling routines? So, this is the interrupt handler table. Based on the interrupt number, for instance, interrupt-N in this case. We look up the starting address of the handler co. And then the program counter is set to that starting address. And the execution of interrupt handling link routine starts. All of this happens in the context of the thread, which was interrupted. Remember again that which exact interrupts can occur on a platform depends on the hardware and how they're handled is specified by the operating system. The situations with signals differs because signals are not generated by an external entity. For instance, if this thread is trying to access a memory location that hasn't been allocated to it, so it's basically performing an illegal memory access. That will result in the signal being generated, that's called SIGSEGV. So once the OS generates this fault, then the rest of the processing is similar to what was happening in interrupts. The OS maintains a signal handler for every process in the system. For each signal in the system, this table will specify the starting address of a handling routine. So the signal would discuss SIGSEGV. That's number 11 in Linux, the access illegal memory. And for that signal, there will be a handling routine whose starting address will be specified in this table. And as a reminder, again, the signals that can occur on a particular platform are really defined by the operating system that executes there. And how they're handled can be specified by the process. A little more on signals now. The reason we said that a process may specify how a signal should be handled. Is because the operating system actually specifies some default actions for handling signals. For instance, a default action for a signal could be that when that signal occurs a process should be terminated. Or maybe that the signal should simply be ignored. An example of what could happen when the SIGSEGV signal occurs is to terminate at and core dump. So that one can inspect the core dump and determine the reason for the crash of the process. Other common default actions in UNIX like systems include to stop a process or to continue a stopped process. For most signals however, a process is also allowed to install its own custom handling routine. And there are system calls or library calls that allow a process to do this. There are certain signals which are exception to this. These refer to them as signals that cannot be caught. For instance, that would always kill the process. Here are a few examples of synchronous signals. For instance as a result of an attempt at access to illegal memory location to protect that memory location. This signal SIGSEGV would occur. Or that we have a signal that occurs synchronously as the result of an attempt to divide by 0. An example of a synchronous signal is also the one that can be directed from one process to another. So there is an API how to send a directed signal to a specific thread. And this is really asynchronous event. There are also asynchronous signals. For instance, this same command kill that's here used to send a directed signal can also be used to cause a process to terminate. And from the process perspective, this is generated asynchronously. Similarly, a timeout that's generated as a result of a time expiring, is another example of an asynchronous signal. There is a problem with both interrupts and signals, in that they're executed in the context of the thread that was interrupted. This means that they're handled on the thread stack and can cause certain issues that will lead us to the answer of why we should sometimes disable interrupts and signals. To demonstrate this problem, let's assume we have some arbitrary thread that's executing, and this is its program counter and its stack pointer. At some point in the execution, an interrupt occurs, or a signal, and as a result of that, the program counter will change, and it will start pointing to the first instruction of the handler. However, notice that the stack pointer will remain the same. And in fact, this can be nested. There may be multiple interrupts or multiple signals. And in a nested fashion, they will keep executing on the stack of the thread, which was interrupted. If the handling code, the handling routine, if it needs to access some state that perhaps other threads in the system would be accessing, then we have to use mutexes. However, if the thread which was interrupted already had that exact same mutex that's needed in the handling routine, we have a deadlock situation. The interrupted thread will not release the mutex until the handling routine completes the execution on its stack and returns. And we know that that clearly wont hap'pen because this one is locked on this mutex. To prevent from these issues, one possibility we have is to keep the handler code simple. What this means in this context is we can prohibit the handling code to use mutexes. Even if there is no possibility for the handler code to lock on some mutex operation, then the deadlock will not occur. The problem with this is that it's too restrictive. It limits what a handler can do. So instead of enforcing that a handler has to be simple and avoid the use of mutexes, we introduce masks. These masks allow us to dynamically enable or disable whether the handling code can interrupt the executing mutex. We call these interrupt or signal masks. The mask is a sequence of bits where each bit corresponds to a specific interrupt or signal, and the value of the bit, zero or one, will indicate whether the specific interrupter signal is disabled or enabled. When an event occurs, first, the mask is checked, and if the event is enabled, then we proceed with the actual handler invocation and interrupt or signal handling. If the event is disabled, then the signal or the interrupt remains standing, and it will be handled at a later time when the mask value changes. To solve the deadlock situation that we described, the thread, prior to acquiring the mutex, it would have disabled the interrupt. So then even if the interrupt occurs, it will be disabled, and it will not interrupt the execution of the threads. It will not interrupt this critical section. If the mask indicates that an interrupt is disabled, then it will remain pending until a later time. Once the lock is freed, once we perform an unlock operation on the mutex, the thread will then reset the appropriate field in the mask. As a result, the interrupt becomes enabled. And at that point, the operating system will allow the execution of the handler code. Know that at this point, it is okay to execute this code because we no longer hold the mutex. So the thread that would be interrupted when the handler code is called doesn't hold this mutex. As a result, the deadlock will be avoided. We should point out that while an interrupt or a signal is pending, then other instances may occur, and they will remain pending as well. Once the event is enabled over here, the handling routine will typically be executed only once, so if we want to ensure that a signal handling routine is executed more then once, it is not just sufficient to generate the signal more then once. Here are a few more things that you should know about masks. Interrupt masks are maintained on per CPU basis. What this means is that if the interrupt mask disables a particular interrupt, the hardware support for routing interrupts will just not deliver that interrupt to the CPU. The signal mask, however, that depends on what exactly is the user-level process, for instance, the user-level thread, doing at a particular moment, so we say that the signal masks are per execution context. If a signal mask is disabled, the kernel sees that, and in that case, it will not interrupt the corresponding thread. So it will not interrupt this execution context. There are many details related to interrupt handling that we'll not discuss in this class. I would like to make some final notes about interrupts. And specifically, interrupts in the presence of multi-core Systems. Actually, this applies not just to multi-core Systems, but to multi-CPU systems in general. On the multi-CPU systems, the interrupt routing logic will direct the interrupt to any one of the CPUs. That, at a particular point of time, has that Interrupt enabled. The reason we put this crown here is, what we can do in these multi-CPU systems. We can specify that only one of the CPUs, only one of the cores, is designated for handling the interrupts. That one will be the only CPU that has the interrupts enabled. And so, what that will allow us to do is we'll be able to avoid any overheads or perturbations related to interrupt handling from any of the other cores. The net effect will be in proof performance. And finally, one more point regarding signal handling. We have two types of signals. The first type are the so-called one-shot signals. One property of these signals is that we know that if they're multiple instances of the same signal that will occur, they will be handled at least once. So it is possible that if we have a situation in which only one signal of that kind occurred versus n signals of that same kind occurred, that only one execution of the actual signal handler is performed. The other thing about the one-shot signals is that the handling routine must be re-enabled every single time. So, if the process wants to install some custom handler for a particular signal, then invoking the operation will mean that once when the signal occurs, the process specific handling routine will be invoked. However, any future instances of that signal will be handled by the default operating system action. Or, if the operating system chooses to ignore such signals then they will be lost. Another type of signals are so-called real time signals that are supported in an operating system like Linux, for instance. And their behavior is such that if a signal is raised n times, then the handler is guaranteed to be called n times as well. So, they have sort of a queuing behavior as opposed to an overriding behavior, as is the case with the one-shot signals. In the previous morsel we mentioned several signals. For this quiz, I will ask you to look at the most recent POSIX standard, and then indicate the correct signal names for the following events. The events are terminal interrupt signal, second, high bandwidth data is available on a socket. Next background process attempting to write. And the last event to look at is file size limit exceeded. Note that a link to the most recent POSIX standard is provided in the instructor notes. So hopefully you found the link for the signals.h header file. On that reference page, you will the table describing the signals and their default actions and descriptions. If you did not find that page, try searching for signal.h online. Using that information as a reference, you can see that the terminal interrupt signal is sickened. For high bandwidth data is available on a socket, SIGURG is used. For background process attempting write, SIGTTOU. And for file size limit exceeded, SIGXFSZ. So, now next time you need to see a signal reference, you will know where to look. Now that we have a basic understanding of how interrupts are typically handled, let's look at the relationship between interrupts and threads. Recall from the previous example that when an interrupt occurred there was a possibility of a deadlock. And this was happening because the interrupt handling routine was waiting on something, was trying to lock a mutex that was already held by the thread that was interrupted by that interrupt routine. A similar situation could have happened for the signal handling routine. So how can we solve this? One way that's illustrated in the SunOS paper is to allow interrupts to become full-fledged threads. And that this should be happening every time they're potentially performing blocking operations. In this case, although the interrupt handler is blocked at this particular point, it has its own context, its own stack, and therefore it can remain blocked. So at that point, the thread scheduler can schedule the original thread back on the CPU. And that one will continue executing. Eventually the original thread will unlock the mutex and at that point, the thread that corresponds to the interrupt handling routine will be free to actually execute. The way this happens looks as follows. Whenever an interrupt or a signal occurs, it interrupts the execution of a thread. And by default, that handling routine should start executing in the context of the interrupted thread using its stack and its registers. If the handling routine is going to be performing synchronization operations, in that case, that handler code will execute in the context of a separate thread. When the locking operation is reached, if it turns out that this one blocks, then the handler code and its thread will be placed in a wait queue associated with the mutex, and instead the original thread will be scheduled. When the unlock operation happens, we go back and we unschedule, we de-queue the handler code from the queue that's associated with the mutex and the handling routine can complete. This sounds like it makes sense. However, one concern is that the dynamic thread creation is expensive. The decision that needs to be made dynamically is whether or not the handler should be handled on the stack of the interrupted thread or as a real thread. The rule that's described in the SunOS paper that's used in the Solaris system is that if the handler routine doesn't include locks, then it's definitely not going to block and so it's safe to execute it on the stack of the interrupted thread. However if there is a possibility of the handler to block because it tries to lock mutexes, then we turn it into a real thread. In order to eliminate the need to dynamically create threads, whenever it's determined that a handler can potentially lock, the kernel precreates and preinitializes a number of threads for the various interrupt routines that it can support. What this means is that the kernel will precreate the number of threads and their associated thread data structures. It will initialize those data structures too so that they point to the appropriate place in the interrupt handling routine, so that any interrupt internal data is appropriately allocated, and similar types of activities. As a result, the creation of a thread is removed from the fast path of the interrupt processing. So, we don't pay that cost when an interrupt actually occurs, and therefore the interrupt handling time can be significantly sped up. Furthermore, when an interrupt first occurs and we're in this initial, in this top part of the interrupt handler, it may be necessary to disable certain interrupts. We said that's one way to prevent the deadlock situation. But then, when the interrupt handling is passed to a separate thread, then we can enable any interrupts that we had disabled originally. Because now this is a separate thread, so interrupts occurring can be handled in the same way as it would for any other thread in the system. So there isn't any danger of some additional deadlock situations, because we are executing in an interrupt-handling routine. So basically, this much safer in terms of having external interrupts occur when we are executing in this bottom part of the handling code. I intentionally chose the words top and bottom to describe what's happening in this situation. Because this description of how Solaris uses threads to handle interrupt, is a very common technique how we allow the interrupt-handling routine to potentially have arbitrary complexity, and not be worried about deadlocks. In Linux, these two parts of the interrupt processing are referred to as the top half and the bottom half. So, this is really what we illustrate with this portion of the lesson. The top half will perform a minimum amount of processing, and it's required to be non-blocking. It will be fast basically. The bottom half is pretty much allowed to perform arbitrary types of processing operations. The top half executes immediately when an interrupt occurs. Whereas the bottom half, like any other thread, can be scheduled for a later time. It can block. And so, other than perhaps because of certain timeouts that are associated with the device, we're not really restricted when it actually gets to execute. The paper goes into further detail to describe a specific policy as to how to interpret the priority levels that are associated with the threads when they're being interrupted. Also priority levels associated with the devices. And then use these priority levels in deciding when and how a thread should be used to handle the particular interrupt. And we will skip that discussion. But the takeaway is that if you want to permit arbitrary functionality to be incorporated in the interrupt-handling operations. Then you really need to make sure that, that handling routine is executed by another thread that you can potentially synchronize with. And that thread potentially is allowed to block Now the reason that this paper described this exercise of creating threads to handle interrupts was really motivated by performance. The operations that are necessary to perform the appropriate checks and if necessary, create a thread to handle an interrupt, add about 40 instructions to each interrupt handling operation. However, as a result of that, it is not necessary to repeatedly change the interrupt mask whenever a mutex is locked, and then, switch it back again whenever the mutex is unlocked. This saves about 12 instructions for every mutex operation. Now because there are way fewer interrupts in the system than mutex lock and unlock operations, then clearly this ends up being a winning situation. We end up saving much more than the actual cost that we end up paying on each interrupt. This observation is also one of the most important lessons in system design, and that is, optimize for the common case. The common case here where the mutex lock/unlock operations. And so, we wanted to make those as efficient as possible. We saved 12 instructions there. Yes, we end up paying somewhere else. We can not sacrifice the safety and the correctness of the system. So we have to make sure we use some other technique to compensate for the fact that we added this optimization. But as long as the net effect is a positive one, this is a very good practice. Lets now look at some of the interplay between threads and the way signals need to be handled. In the selerious threads implementation as described in the papers there's a signal mask that's associated with each user level done. And that's part of the user level process. It's visible at the user level library level. There is also signal mask that's associated with the kernel level thread or rather the likely process that it's attached to and that kernel level mask is only visible at the kernel level. Now in when a user level thread wants to disable a signal it clears the appropriate bit in the signal mask and this is happening at user level. This mask is not visible to the kernel. Now when a signal occurs, the kernel needs to know what should it do with the signal. It is possible that the kernel visible signal mask has that bit still set at one, so the kernel thinks that the signal is enabled as far as this particular process, this particular thread is concerned. If we don't want to have to make a system call and cross from user into kernel level each time a user-level thread modifies the signal mask, then we need to come up with some kind of policy. The SunOS paper that describes the lightweight user-level threading library proposes a solution of how to handle this situation. To explain what's happening, let's consider a sequence of different situations. The first case we'll look at, both the user-level signal mask and the kernel-level signal mask had the signal enabled. Let's say this is the user-level thread that's currently actually executing on top of this kernel-level thread. Now when the signal occurs, no problem. The kernel sees that the signal is enabled, so it will interrupt the currently running user-level thread that's running on top of this kernel-level thread. And there's absolutely no problem because that user-level thread had the signal enabled as well, so the processing will be safe. Now let's consider a second case. Here, the kernel level mask is one, so the kernel thinks that the process overall can handle the signal. However, the user level thread that's currently running on top of the kernel level thread has the signal disabled. So its mask has the bit zero in the signal, in the appropriate place for the signal. But there is another user level thread that's currently in the run queue. So it's not executing. It's runnable but it's not executing at this particular point of time. That one has the mask enabled. The threading library, that manages both of these user level threads, will know about this thread. So now when a signal occurs at the kernel level, the kernel sees that the process overall knows how to handle this particular signal. So it has the bit set as one. But, it should be appropriate for it to interrupt this user level thread, because this particular user level thread, the one that's currently active, has a signal disabled bed. We should figure out a way how to get to the threading library because that's the one that knows about this other runnable thread that would be capable of handling a signal. We said that the way signals are handled is that when they interrupt the process or either a thread that's running in the process. The handling routine that needs to be executed is specified in the signal handler stable. So then one easy thing that we can do is for all the signals in the system, we can have a special library routine that will basically wrap the signal handling routine. So when a signal occurs, we start executing the library provided handler. That library provided handler can see all of the masks of the user level threads. If we have a situation like here, where the currently scheduled user level thread cannot handle the signal. But there is another runnable user level thread that can, the library handling routine can invoke the library scheduler and can now make this user level thread running on the kernel level thread. So then the signal can be handled. Now lets look at yet another case. This is case three now. So here we have a similar thing in that the user level thread thats currently executing on top the kernel level thread where the signal actually occurs, this user level thread has the signal disabled. In the process overall, there is another user level thread that has the signal enabled. And unlike in the previous case when this user level thread was just on the run queue, now in this case this user level thread is currently running on another kernel level thread on another CPU. So when the signal is delivered in the context of this kernel level thread, the library handling routine will kick in. The library handling routine knows that there is a user level thread in the system that can handle this particular signal. And it sees that this user level thread is currently associated, it's executing on top of a kernel level thread, or rather a lightweight process that's managed by the threading library. Since the library knows this, the library will then generate a directed signal to the other kernel level thread to the lightweight process where the user level thread currently is executing. When the OS delivers the signal to this particular kernel level thread, it sees signal mask enabled. Great. And it moves up. Now technically it will still go into the library handling routine first, right? Because that's a wrapper for all of the signal handlers. Here the library handling routine will see grade at a current user level thread that's running in the context of this kernel level thread. Can handle the particular signal of the signal and so it finally will allow the execution of the signal handler. And now let's consider one final case in which every single one of the user-level threads has that particular signal disabled. So all of the user-level thread masks are zero. The kernel-level masks are still one, the kernel still thinks that the process can handle this particular signal. When the signal occurs here, the kernel sees that the signal mask is one, and so it will interrupt whoever is executing in the context of this kernel-level thread. The library handling routine kicks in. It sees that this particular thread has the mask zero. And it sees that it doesn't have any other user-level threads that can handle that particular signal. Now what will happen is the threading library, at that point, will perform a system call, and it will request that the signal mask off the underlying kernel-level thread be changed. So this signal mask will become zero. Now, from the execution of one thread, we can go ahead and affect the state of the kernel-level masks that are associated with other threads. They may be executing on other CPUs. So here, we can only change the mask that's associated with this kernel-level thread, and then what the threading library will do, it will basically reissue the signal for the entire process again. The OS will now find another thread in that process. So in this case, this other kernel-level thread. It had originally the mask. The OS will, in that case, find another kernel-level thread. In this case, this other kernel-level thread that has the mask enable. So this one has its zero. And we'll try to do the same thing. We'll try to deliver the signal in the context of this kernel-level thread. It will interrupt this user-level thread via the library handler. As a result, this particular kernel-level mask will be changed as well via system call, and the process will continue until all of the kernel-level signal masks don't show that the signal is disabled for this process. Now another possibility is that one of the user-level threads finishes whatever they were doing and are ready to enable the signal mask for that signal again. Because the threading library knows that it has already disabled all of the kernel-level signal masks. It will, at that point, have to perform a system call to go into the kernel and update one of the signal masks, so it appropriately reflects that now the process is capable of handling the signal. The solution for how signal handling is managed and what kind of interactions happen between then kernel and the user-level library is in the same spirit of optimizing the common case. Signals, actual signals, occur much less frequently then the need to be safe and update the signal mask. So whenever we would have a certain critical portion of the code, we would first disable and then again enable the signal. And in most of those cases, a signal doesn't really occur. So then we tried to make the common case cheap, so the updates of the signal mask, we just apply them to the user-level signal mask, and the actual system call that's necessary to reflect that change in the kernel is avoided. As a result of that, we have to make the actual signal handling a little bit more complex. But hey, that's the less frequent of the two, and we want to optimize the common case. Finally, let's look at some aspects of the threading support in Linux as well. Note that the current threading support in Linux has a lot of lessons learned based in large part on earlier experiences with threads. Such as, the experiences that are presented in the two Solaris papers were described. Like all operating systems, Linux has an abstraction to represent processes. However, the main abstraction that it uses to represent an execution context is called a task. And it's represented via corresponding task structure. A task is essentially the execution context of a kernel level thread. A single-threaded process will have one task, and a multi-threaded process will have many tasks. One per thread. Some of the key elements in a task structure are shown here. Each task is identified by a task identifier, however for historic reasons we call this a pid like a process ID. It's a little bit misleading. What this means is that, if we have a single thread of process that has one task, then basically the task ID and the process ID are the same. If we have a multi-threaded process, then we will have multiple tasks. Each will be identified by its own identifier for the task, and that will be held in the process ID. Now the process as a whole, basically the entire group of tasks. Will be identified by the process ID of the very first task that was created when the process was first created. This information is also stored in the task group ID field. In addition a task structure maintains a list of tasks. So this basically links all of the tasks that are part of a single process it's all of threads of the process. And so one can figure out what the process ID for a group of tasks is also by walking through this list. Having learned from implementation efforts like the Solaris threads implementation Linux never had one contiguous process control block like what we described at the start of this course. Instead the process state was always represented through a collection of references to data structures, like the memory management, file management. These are all referenced via pointers. So this makes it easy for tasks in a single process to share some portions of the address space, like the virtual address mappings or files. And in that case these pointers would simply point to the same memory-management structure or file-structure. There are a number of other fields in the task structure. It's a data structure that's approximately 1.7 kilobytes large so there's quite a lot of information in it. To create a new task, Linux supports an operation clone, and this is similar to this. It takes a function pointer and arguments similar to like what we saw when we were creating a new thread. But it also has this one argument that's called sharing flags. The flags parameter is a bit map that specifies which portion of the state of a task will be shared between the parent and the child task. As you can see the values of these flags can have different effects when they're set versus when they're cleared. For instance, when all of the flag bits are set, then we're really creating a new thread that shares everything, the entire address space and everything else with the parent thread. They're part of the same address space. If all of the sharing flags are clear, then we're really not sharing anything between the child and the parent. And this is more similar to what we saw happens when we're forking a new process. And in some cases various combinations make sense. For instance you may want to share the files or something else between the parent task and the child task. Speaking of fork you should know a couple of things. First of all fork in Linux is internally implemented via clone by basically having these flags cleared. And also fork has a very different semantics in Linux and compliant OS's in general for multithreaded processes versus single threaded processes. So for a single threaded process when we're forking we're really expecting that the created, that the child process will be a full replica of the parent process. Where as with mult-threaded processes the child will be a single threaded process. So we're really going to create a replica of the portion of the address space that's visible from the parent thread. >From the parent task in the process that called the fork. This has a lot of implications on some issues related to synchronization, to what happens with mutexs. It's beyond the scope o f the class, but I should just make sure that, that you're aware of this. So that's why I'm bringing it up at this point. The current implementation of Linux Threads is called Native POSIX Threads Library, NPTL. And this is a one to one model where there is a kernel level task for each user-level thread. This implementation replaced an earlier implementation, Linux threads, which was very similar to the many to many model that was described in the Solaris papers. And it suffered from the same kind of complexity regarding signal management, etc. In NPTL because of the one to one model the kernel sees every user level thread. Sees all of its information, whether it's block synchronization, what is its signal mask, everything. This is made possible for two reasons. First, the kernel traps have become much cheaper. So the user to kernel level crossing that we've been trying to avoid in part with this mini to mini model has become much faster and we can afford to go through the kernel and update the kernel level signal map. Also modern platforms have more memory so there really isn't some constraint to keep the number of kernel level threads as small as possible. So we can create as many kernel level threads and, as the process needs. There aren't restrictions on the range of IDs that are too stressing for most of the common processes. So these sorts of things eliminate some of the main reasons for going to the many to many model. Still, however, when we start thinking about extremely large number of threads, and as a community this is something that comes up in the context of exascale computing. Or when we are thinking about thread management in platforms that are really complex, that maybe have different kinds of processors, heterogeneity, et cetera. Then it makes sense to start thinking again about user-level library support, about providing more custom policies for how threads are managed, how threads, how many threads are there going to be in the system, or similar issues. But for most practical purposes, the one to one model that's supported by the current Linux threading model is completely sufficient. In this lesson, we reviewed two older papers that gave us some historic perspective and some insights into the challenges related to supporting threads. A major takeaway from these papers is that now we have a better understanding as to why current operating systems like Linux have their present day threading model. In addition, in this lesson, we also introduced interrupt and signals to important notification mechanisms supported in operating systems today. As the final quiz, please tell us what you learned in this lesson. Also, we'd love to hear your feedback on how we might improve this lesson in the future. In this lecture, we will contrast several approaches for structuring applications that require concurrency. This will include and comparison between multi-process and multi-threaded approaches, and we talked about both of these already in this course, and in addition, we will present an alternative, a so-called event driven approach. We will base the discussion of the event driven model on Vivek Pai's paper, Flash: An Efficient and Portable Web Server. This paper describes the event-driven architecture, and it also includes detailed performance comparisons between a multi-process, multi-threaded, and an event driven implementation of a web server application. In addition, these results are put in contrast with Apache, which is a popular open-source web server. We will end this lecture with a more generic discussion on how to structure good experiments. During the threads and concurrency lecture, recall that we had a quiz in which we were comparing the boss-worker model with the pipeline model. And we did that specifically in one example for six worker threads in both cases, and for a toy order that consisted of 11 toys. For the boss-worker model, we said that it takes 120 milliseconds for a worker to process a toy order. And then for the pipeline model we said that it takes 20 milliseconds to complete each of the pipeline stages, and a full toy order had six pipeline stages. Let's compare these two models to see which one is better. The computation that we performed during that homework showed us that regarding execution time, the boss-worker models took 360 milliseconds for the 11 toy orders, and the pipeline model took 320 milliseconds for the same 11 toy orders. Now let's consider something else. Now let's compare these two models with respect to the average time they take to complete an order. To find the average time we have to sum up the times that it took to complete every single one of the 11 orders, and then divide by 11. The first five orders took 120 milliseconds to complete. They were executed by the first group of five threads. The second five orders were scheduled in the second batch, so they took twice as long, they took 240 milliseconds to complete. And then the 11th toy order took 360 milliseconds to complete. It had to wait, it could only be started until the previous ten orders were completed in the groups of five plus five threads, each. So if we compute this, the average time to complete an order for the boss-workers model is 196 milliseconds. If we take a look at the pipeline model, the first order took 120 milliseconds to complete. Six pipeline stages times 20 milliseconds. The next one was already in the pipeline and once the first order completed, it had to finish the last stage of the pipeline. So its completion time will be 20 milliseconds longer, so 140 milliseconds. The one that came after that, another 20 milliseconds longer for 160. And so on until the very last order, which will take 320 milliseconds. So the average completion time for the pipeline model is 220 milliseconds. So basically what this shows us is that if we consider execution time, if that's what's important, then we should pick for this particular configuration, for 11 toy orders and six workers, then we should pick the pipeline model. It's better. It leads to shorter execution time. However, if what we care for is average time to complete the order, because that's what makes our customers happy, then the boss-worker model is better. Its average completion time is 196 milliseconds compared to 220, for the exact same work load, 11 toy orders, and the exact same number of workers working in the toy shop. If you slightly modify this problem where you look at a situation with different number of workers, or different number of orders, then you may end up coming up with completely different answers and drawing different conclusions as to which one of these models is better. Now the important thing is, though, that when we look at a specific configuration, if we are the toy shop manager and it's concerned with completing as many orders as possible in a fixed amount of time. Then maybe we'll choose, under these circumstances, the pipeline model, because it gives us shorter completion time. However, if we're the customers of that toy store, then we probably would prefer if this was the model that the toy shop implemented, because the orders will be completed in shorter amount of time. Important conclusion of this is that, when we're comparing two different implementations of a particular problem, or two different solution design points, then it's very important to think about, what are the metrics that are used to evaluate those different solutions, those different implementations? Who cares about that? At the start of the threads and concurrency lecture, we asked ourselves whether threads are useful, and we mentioned there are a number of reasons why they are, they allow us to gain speed up because we can parallelize problems, they allow us to benefit from a hot cache because we can, specialized what a particular thread is doing on a given CPU. The implementations that have lower memory requirements and where it's cheaper to synchronize compared to, multiprocess implementations of the same problem. We said that threads are useful even on a single CPU because they let us hide the latency of I/O operations. However, how did we draw these conclusions, what were the workloads, what where the, kinds of resources that were available in the system. And ultimately, what were the different metrics that we were using when comparing different implementations with and without threads. And the way that we would measure whether something is useful or not, would differ. For instance, for a matrix multiply application, we want to think about the execution time of an implementation or a solution. Or for a web service application, maybe what we care for is the number of requests per unit of time that we can handle. Now in the context of that same application, if we think about things from the client's perspective, maybe truly the response time though can be used to evaluate whether something is better or more useful than the alternative. For these kinds of properties of the system, maybe I want to know their average values, or whether they're maximum or minimum values, so their best and worst case values. But also perhaps I'm concerned with just, what is the number of request per time that I can service, or what is the response time that I deliver to clients, and most of the time, 95% of the time or 99% of the time, so yes, there maybe few outliers, few situations in which my, request trade drops, but as long as, 95% of the time it's exactly where I want it to be, that's a solution that's good for me, so, because of the fact that these outliers, these remaining 5%, may have very different behavior than the rest of the requests or the rest of the time that the service is operating, then, when you're using the average numbers for these values, the evaluation may look very different than when we're using the 95 percentile values. Or maybe we're designing some hardware chip, and in that case really, from the hardware prospective the thing that we're really concerned with, is, whether or not the overall utilization of the hardware of the CPU is better. What these examples illustrate is that, to evaluate some solution, and to determine whether it's useful or not, it is important to determine what are the properties that we really care for, what are the properties that capture the utility of that particular solution. We call such properties metrics, so basically the evaluation, and the answer to whether something is useful or not, will depend on the relevant metrics. Lets consider a visual metaphor in our discussion about metrics. We will do this by comparing metrics that exist in a toy shop, to metrics that exist in operating systems. For instance, from the perspective of the toy shop manager, a number of properties of how the workers operate, how the toy shop is being run, may be relevant. One example is throughput. The toy shop manager would want to make sure that this is as high as possible. Other things that may be important for the toy shop manager include how long does it take to react to a new order on average? Or what is the percentage of the workbenches that are used over a period of time? There can clearly be many more properties of the toy shop and how it's run that are relevant to the toy shop manager. Metrics such as throughput, response time, utilization and others are also relevant from the operating systems perspective. For instance, it's important to understand how many processes can be completed over a period of time on a particular platform. It's important to know how responsive the system is. So when we click the mouse, does something happen immediately or we have to wait some noticeable amount of time? Does the operating system design lead to a solution that utilizes the CPU, devices, memory well, or does it leave a lot of unused resources? So metrics exist in any type of system, and it's important to have them well-defined when you're trying to analyze how a system behaves and how it compares to other solutions. If you have not noticed yet, performance considerations are really all about the metrics that we choose. Ideally, metrics should be represented with values that we can measure and quantify. The definition of the term metrics, according to Webster, for instance, is that it's a measurement standard. In our analysis of systems, a metrics should be measurable. It should allow us to quantify a property of a system, so that we can evaluate the system's behavior or at least compare it to other systems. For instance, let's say we are concerned with the execution time of the system. That's a metric. We can measure it. We can quantify exactly what is the execution time of a system, so it's a quantifiable property as well. A metric is associated in some way with some system that we're interested in. For instance, that can be the implementation of a particular problem, the software implementation of a problem. And that's what we want to measure the execution time of. And a metric should tell us something about the behavior of the system we're interested in. For instance, it can tell us whether it's an improvement over other implementations of the same problem. For the later, in order to perform this kind of evaluation and comparisons, we really should explore the values of this metrics over some range of meaningful parameters. By varying the workload that this implementation needs to handle, or by varying the resources that are allocated to it, or other dimensions. So far in this lesson we mentioned several useful metrics. For instance, we talked about execution time and throughput, response time, CPU utilization. But there are many other useful metrics to consider. For instance, user may not just care when they will get an answer, but they may also care when their job will actually start being executed. We call this metric wait time. The job is interactive, so the user needs to interact with this. Obviously the sooner he starts, the sooner the user will be able to do something about it. If the job is a long running job and the sooner it starts, the user has a chance to find out maybe that something's going wrong. So It can reconfigure the task, it can stop it and then reconfigure and launch it again. So wait time could be an important metric in some contexts. Then let's think about throughput for instance. We know throughput helps evaluate the utility of a platform. So how many tasks will it complete over a period of time? How many processes, how many jobs will we complete at over a period of time? This can be relevant in the context of a single machine, a single server. Or in the context of an entire data center for instance. Now, if I'm the owner of the data center, throughput is not the only thing that I care for. I'm probably more concerned about some other type of metric that we can call platform efficiency. And this says some combination of how well I utilize my resources to deliver this throughput. So it's not just a matter of having higher throughput, but also being able to utilize the resources that are available in my data center more efficiently. The reason for this is that as a data center operator, I make money when I complete jobs. So the higher the throughput, the greater the income for me. However, I also spend money to run the machines, to buy more servers. So it's important to have a good ratio. So platform efficiency would for instance, capture that. If it's really just the dollars that I'm concerned about, then a metric like performance per dollars would capture that. So if I'm considering buying the next greatest hardware platform. Then I can think about whether the cost that I will pay extra for that new piece of hardware, will basically be compensated with some impact on the performance that I see. Or maybe I'm concerned about the amount of power, the watts, that can be delivered to a particular platform. Or the energy that will be consumed during the execution. So then defining some metrics that capture performance per watt, or performance per joule will be useful ones. You may have heard of the term SLA. It stands for Service Level Agreement. Enterprise applications will give typically SLAs to their customers. One example, for instance will be that you will get a response within three seconds. Or, it may be even more subtle than that. For instance, a service like Expedia perhaps, has an SLA with it's customers. And it's customers would be like Delta Airlines and American Airlines, that it will provide most accurate quote for 95% of the flights that are being returned to customers. So then for that enterprise application, one important thing would be whether there any SLAs that are violated. Whether there are any customer requests that took longer than three seconds, or that did not provide quotes for airfare that were all 100% accurate. A metric-like percentage of SLA violations would capture that information. For some applications, there is some natural opportunity for a slack in the application. For instance, if you think about a regular video application, humans can't perceive more than 30 frames per second. So being so focused on the frames per second, and trying to maximize that frames per second rate, that's not the goal. However, making sure that there's at least 30 frames per second. So that users don't start seeing some random commercials during the video that they're watching on YouTube, that's something that's important. So it's not so much about this raw request rate or wait time, but rather it's a metric that really is concerned whether the client perceives the service as performing well or not. You may be concerned with the performance metric of an individual application. Or you may need to try to come up with some kind of aggregate performance metric that tries to average the execution time for all tasks, or average the wait time for all tasks. Or maybe even this would be a weighted average based on the priorities of the tasks. Also in addition to just being constrained with CPu utilization, there are a number of other resources that we may be concerned about. Memory, file systems, the storage subsystem. So some metrics that are concerned with the average resource usage are also useful In summary a metric is some measurable quantity that we can use to reason about the behavior of the system. Ideally we will obtain these metrics. We will gather these measurements running experiments using real software deployment on the real machines using real workloads. However sometimes that's really not an option. We cannot wait to actually deploy the software before we start measuring something about it or analyzing its behavior. In those cases we have to resort to experimentation with some representative configurations that in some way mimic as much as possible the aspects of the real system. The key here is that such a toy experiment must be representative of this real environments so we must use workloads that have similar access patterns, similar types of machines. So as closely mimics the behavior of the real system as possible. And possibly we will have to supplement those toys experiments with simulation. So that we can perhaps create an environment that somehow mimics up a larger system that was possible with a small experiment. Any of these methods represent viable settings where one can evaluate a system and gather some performance metrics about t. We refer to these experimental settings as a testbed. So the testbed that tells us where were the experiments carried out and what were the relevant metrics that were measured? So if we go back now to our question, are threads useful? We realize that the answer is not so simple. We cannot simply say, yes, threads are useful. We know that the answer of the question will depend on the metrics that we're interested in. Also, it will depend on the workload. We saw in the toy shop example where we compared the boss worker and the pipeline model that the answer as to which model is better dependent on the number of toys that need to be processed to the number of orders. So in the toy shop example, depending on the workload, the toy orders, and metrics we were concerned in, it lead us to conclusion that a different implementation of the toy shop, a different way to organize its workers was a better one. If you look at other domains, for instance, if we think about graphs and graph processing. Depending on the kind of graph, how well connected it is, it may be suitable to choose different type of shortest path algorithm. Some shortest path algorithms are known to work well on densely connected graphs whereas others work better for sparsely connected graphs. So again, the workload is something that we're interested in. When comparing file systems, maybe what's important to consider is the, the patterns. The file, some file systems may be better for predominantly read accesses whereas others are better for more of a mixed workload, where files are both read and updated. The point of looking at all of these is that across the board, both for the first question as well as in these other cases, the answer of whether something is better than an alternative implementation or an algorithm, it's pretty much always it depends. Depending on the file pattern, depending on the graph, depending on the number of toy orders. So similarly, the answer to, are threads useful isn't really going to be a straightforward yes and no one. It's really going to depend on the context in which we're trying to answer this question. And while we are at this, it depends, answer, you should know that it's pretty much always the correct answer to a question in systems. However, it's never going to be an accepted one. I will not take it as accepted answer in this course either. For the remainder of this lecture, we will to answer a specifically, whether threads are useful. And when are threads more or less useful when comparing a multithreaded-based implementation of a problem to some alternatives. I will also provide you with some guidance on how to define some useful metrics, and how to structure experimental evaluations, so that you can correctly measure such metrics. To understand Winter threats useful, let's start to think about what are the different ways to provide concurrency and what are the trade offs among those implementation. So far we've talked about multi threaded applications. But an application can be implemented by having multiple concurrently running processes. We mentioned this in the earlier lecture on Threads and Concurrency. So let's start by comparing these two models. To make the discussion concrete we will do this analysis in the context of a web server. And for a web server it's important to be able to concurrently process client requests. So that is the concurrency that we care for there. Before we continue let's talk for a second about what are the steps involved in the operation of a simple web server. At the very first, the client or the browser needs to send a request that the web server will accept. So let's say this is a request to www.contact.edu and the web server at Georgia Tech needs to accept that request. After the request is accepted, there are a number of processing steps that the web server needs to perform before finally responding with the file. Now, we will talk about a simple web server. So if we take a look at what these steps are, so we accept the connection, we read the request that there is an HTTP request that's received and we need to parse that request. We need to then find the file in the local file system, that's at the server side. Once we have extracted the file, we need to compute the header, send out the header and then also send out the file or potentially send out an error message along with the header that the file is not found. So for the rest of this lesson we'll really focus on this simple web server processing. One of the things that's worth pointing out is that there's some differences among these steps. Some of them are more computational intensive, so it's mostly, the work is done by the CPU. For instance, parsing the request or computing the header. This is mostly done by the CPU. Other steps may require some interaction with the network, like accepting connection, reading request, or sending the data. Or the disk, for instance, when finding the file and then reading the file from the disk. These steps may potentially block, but whether or not they block will really depend on what is the state of the system at a particular point of time. So for instance, the connection may already be pending or the data for the file may already be cached in memory because of the previous request that serviced that file. So in those cases, these will not result in an actual call to the device, so an actual implication of the disk or the network and will be serviced much more quickly. Once the file or potentially the error message are sent out to the client, then the processing is complete. This, then, clearly represents a single threaded process. One easy way to achieve concurrency is to have multiple instances of the same process. And that way we have a multi-process implementation. This illustration is adapted from Vivek Pai's paper, Flash, An Efficient and Portable Web Server, and it appears as figure two in the paper. The benefits of this approach is that it is simple. Once we have correctly developed the sequence of steps for one process, we just spawn multiple processes. There are some downsides, however, with running multiple processes in a platform. We'll have to allocate memory for every one of them and this will ultimately put high load on the memory subsystem and it will hurt performance. Given that these are processes, we already talked about the cost of context switch among processes. Also it can be rather expensive to maintain shared state across processes because the communication mechanisms and the synchronization mechanisms that are available across processes, those are little bit higher overhead. And in some cases it may even be a little bit tricky to do certain things like, for instance, forcing multiple processes to be able to respond to a single address and to share an actual socket port. An alternative to the multi-process model is to develop the web server as a multi-threaded application. So here we have multiple execution context, multiple threads within the same address space and every single one of them is processing a request. Again, this illustration is taken from Pai's Flash paper, and this is figure three there. In this figure, every single one of the threads executes all the steps, starting from the accept connection call all the way down to actually sending the file. Another possibility is to have the web server implemented as a boss-workers model where a single boss thread performs the accept connection operation. And every single one of the workers performs the remaining operations from the reading of the HTTP request that comes in on that connection until actually sending the file. The benefits of this approach is that the threads share the address space, so they will share everything that's within it. They don't have to perform system calls in order to coordinate with other threads, like what's the case in the multi-threaded execution. Also context switching between these threads is cheap. It can be done at the user level, threading library level. Because a lot of the per thread state is shared among them, then we don't have to allocate memory for everything that's required for each of these execution contexts. They share the address space, so the memory requirements are also lower for the multi-threaded application. The downside of the approach is that it is not simple and straightforward to implement the multi-threaded program. You have to explicitly deal with synchronization when threads are accessing and updating the shared state. And we also rely for the underlying operating system to have support for threads. This is not so much of an issue today. Operating systems are regularly multi-threaded. But it was at the time of the writing of the Flash paper, so we will make sure that we address this argument as well in our explanations. Now let's talk about an alternative model for structuring server applications that perform concurrent processing. The model we'll talk about is called event-driven model. An event-driven application can be characterized as follows. The application is implemented in a single address space, there is basically only a single process. And a single thread of control. Here is the illustration of this model and this is taken from the read pies flash paper as well. The main part of the process is the event dispatcher that continuously in a loop looks for incoming events and then based on those events invokes one or more of the registered handlers. Here events correspond to some of the following things. We see that the request from the client browsers, that message that's received from the network, that's an event. Completion of the send, so once the server responds to the client request, the fact that the send completed, that's another event, as far as the system is concerned. Completion of a disk read operation. That's another event that the system will need to know how to handle. The dispatcher has the ability to accept any of these types of notifications, and then based on the notification type to invoke the appropriate handler. So in that sense, it operates very much like a state machine. Since we're talking about a single credit process, invoking a handler simply means that we will jump to the appropriate location in the processes address space where the handler is implemented. At that point the handler execution can start. For instance, if the process is notified that there is a pending connection request on the network port that it uses, the dispatcher will pass that event to the accept connection handler. If the event is a receipt of a data of message on an already established connection, then the event dispatcher will pass that to the read request handler. Once the filename is extracted from the request and it's confirmed that the file is present, the process will send out chunks of the file. And then once there is a confirmation that that chunk of the file portion of the file has been successfully sent and it will continue iterating over the handler that's dealing with the send operation. If the file is not there, then some sort of error message will be sent to the client. So whenever an event occurs the handlers are the sequence of code that executes in response to these events. The key feature of the handlers is that they run to completion. If a handler needs to perform a blocking operation, it will initiate the blocking operation and then it will immediately pass control back to the event dispatcher, so it will no longer be in the handler. At that point, the dispatcher is free to service other events or call other handlers. You're probably asking yourselves, if the event-driven model has just one thread, then how did it achieve concurrency? In the multi-process and the multi-threaded models, we had each execution context, whether it's a process or a thread, handle only one request at a time. To achieve concurrency, we would simply add multiple execution context, so multiple processes or multiple threads. And then, if necessar,y if we have fewer CPUs than contexts, then we would have to context-switch among them. The way the event-driven model achieves concurrency is by interleaving the processing of multiple requests, within a same execution context. Here in the event-driven model, we have a single thread, and the single thread switches its execution among the processing that's required for different requests. Let's say we have a client request coming into the system, so it's a request for client C1. And we receive a request for a connection that gets dispatched, the accept operation gets processed. Then, we receive the actual request. So it's an HTTP message that gets processed, the message gets parsed, we extract the files. So now we actually need to read the file and we initiate I/O from the reading file handler. So at that point, the request for client one has been processed through several of these steps and it's waiting on the disk I/O to complete. Let's say, in the meantime, two more requests have come in. So client two and client three have sent a request for a connection. Let's say the client two request was picked up first, the connection was accepted, and now for the processing of client two, we need to wait for the actual HTTP message to be received. So the processing of client two is waiting on an event from the network that will have the HTTP message that needs to be received. And let's say client three, its request has been accepted and it's currently being handled, so the client three request is in the accept connection handler. Some amount of time later, the processing of all of these three requests has moved a little bit further along. So the request for C3, the accept connection was completed, and now that request is waiting on an event with the HTTP message. The request for client two, that one, perhaps, we're waiting on the disk I/O, in order to read the file that needs to be sent out. And maybe the request for client C1, already started sending the file in chunks at a time, so blocks of some number of bytes at a time. So, it's waiting in one of those iterations. So, although we have only one execution context, only one thread, if we take a look, we have concurrent execution of multiple client requests. It just happens to be interleaved, given that there's one execution context. However, they're multiple, at the same time, multiple client requests being handled. The immediate question is why does this work. What is the benefit of having a single thread that's just going to be switching among the processing of different requests compared to simply assigning different requests to different execution contexts, to different threads or even to different processings. Recall our introductory lecture about threads, in which we said that on a single CPU threads can be useful because they help hide latency. The main takeaway from that discussion was that, if a thread is going to wait more than twice the amount of time it takes to perform a contact switch, then it makes sense to go ahead and context switch it to another thread that will do some useful work. And in that way we hide this waiting latency. If there really isn't any idle time. So if the processing of a request doesn't resolve in some type of blocking idle operation on which it has to wait, then there are no idle periods. It doesn't make sense to context switch. The context switching time will be just cycles that are spent on copying and restoring a thread or a process information, and those cycles could have been much better spent actually performing request processing. So in the event driven model, a request will be processed in the context of a single thread, as long as it doesn't have to wait. Whenever a wait needs to happen, then the execution thread will switch to servicing another request. If we have multiple CPUs, the event driven model still makes sense, especially when we need to handle more concurrent requests than the number of CPUs. For instance, each CPU could host a single event-driven process, and then handle multiple concurrent requests within that one context. And this could be done with less overhead than if each of the CPUs had to context-switch among multiple processes or multiple threads where each of those is handling a separate request. There is one gotcha, though, here. It is important to have mechanisms that will steer, that will direct the right set of events to the appropriate CPU, at the appropriate instance of the event-driven process. And there are mechanisms to do this, and there's current support, a networking hardware to do these sorts of things, but I'm not going to go into this in any further detail. So just know that overall in the model, this is how the event-driven model would be applied a multi-CPU environment. Now let's see how can this be implemented. So at the lowest level, we need to be receiving some events, some messages from the network or from the disk. So information about completed requests to read a portion of the file, write the file, etc. The operating systems use these two abstractions to typically represent networks or disks. So sockets are typically used to represent interface to the network. And then files are what's really stored on disks. So these are the main abstractions when it comes to storage. Now although they are called differently, sockets and files, it is quite fortunate that internally, the actual data structure that's used to represent these two different obstructions, is actually identical. It's called the file descriptor. So then an event in the context of this web server is an input on any of the files descriptors that are associated with it. So in any of the sockets. Or any of the files that are being accessed by the connections that these sockets carry. To determine which file descriptor has input, so to determine that there is an event that has arrived in this system. The flash talks about using the select call. The select call takes a range of file descriptors and then returns the very first one that has some kind of input on it. And that is regardless is whether that file descriptor is a socket or a file ultimately. Another alternative to this is to use a poll API. So this is another system call that's provided by current operating systems. The problem with both of these, is that they really have to scan through potentially really large list of file descriptors, until they find one. And, it is very likely that along that long list of file descriptors, there going to be only very few that have inputs. So, a lot of that search time will be wasted. An alternative to these is a more recent type of API that's supported by, for instance, the Linux kernel and that's e poll so this eliminates some of the problems that select and poll have. And a lot of the high performance servers that require high data rates and low latency use this kind of mechanism today. The benefits of the event driven model really come from its design. It's a single address space, single flow of control. As a result, the overheads are lower. There's no need for context switching. Overall, it's a much more compact process so it has smaller memory requirements. And the programming is simpler. We don't need to worry about use of synchronization primitives, about shared access to variables, etc. Now, in the context of this single thread, we are switching among multiple connections, so we are jumping all over the code base of this process and executing different handlers, accessing different states. That will have some effect on basically loss of localities and cache pollution effects. However, that will be significantly lower than would have been happening if we were doing a full blown context switching. So the overheads and some of the elimination of the synchronization, these are some of the things that really make this an attractive approach. The event-driven model doesn't come without any challenges. Recall that when we talked about the many to one multithreading model, we said that a single blocking I/O call that's coming from one of the user level threads can block the entire process, although there may be other user level threads that are ready to execute. A similar problem can occur here as well. If one of the handlers issues a blocking I/O call to read data from the network or from disk, the entire event-driven process can be blocked. One way to circumvent this problem, is to use asynchronous I/O operations. Asynchronous calls have the property that when the system call is made, the kernel captures enough information about the caller and where and how the data should be returned once it becomes available. Async calls also provide the caller with an opportunity to precede executing something, and then come back at a later time to check if the results of the asynchronous operation are already available. For instance, the process or the thread can come back later to check if a file has already been read and the data is available in the buffer in memory. One thing that makes asynchronous calls possible is that the OS kernel is multithreaded. So while the caller thread continues execution, another kernel thread does all the necessary work and all the waiting that's needed to perform the I/O operation, to get the I/O data, and then, to also make sure that the results become available to the appropriate user level context. Also, asynchronous operations can benefit by the actual I/O devices. For instance, the caller thread can simply pass some request data structure to the device itself, and then the device performs the operation, and the thread at a later time can come and check to see whether device has completed the operation. We will return to a synchronous I/O operations in a later lecture. What you need to know for now is that when we're using asynchronous I/O operations, our process will not be blocked in the kernel when performing I/O. In the event-driven model, if the handler initiates an asynchronous I/O operation for network or for disk, the operating system can simply use the mechanism like select or poll or epoll like we've mentioned before to catch such events. Since summary asynchronous I/O operations fit very nicely with the event-driven model. The problem with asynchronous I/O calls is that they weren't ubiquitously available in the past. And even today, they may not be available for all types of devices. In a general case, maybe the processing that needs to be performed by our server isn't to read data from a file, where there are asynchronous system calls. But instead maybe to call processing some accelerator, some device that only the server has access to. To deal with this problem, paper proposed the use of helpers. But a handler needs to issue an I/O operation that can block, it passes it to the helper, and returns to the event dispatcher. The helper will be the one that will handle the blocking I/O operation, and interact with the dispatcher as necessary. The communication with the helper can be via socket based interface, or via another type of messaging interface that's available in operating systems called pipes. And both of these present a file descriptor-like interface. So the same kind of select or poll mechanism that we mentioned can be used for the event dispatcher to keep track of various events that are occurring in the system. This interface can be used to track whether the helpers are providing any kind of events to the event dispatcher. In doing this, the synchronous I/O call is handled by the helper. The helper will be the one that will block, and the main event dispatcher in the main process will continue uninterrupted. So this way although we don't have asynchronous I/O calls, through the use of helpers, we achieve the same kind of behavior as if we had asynchronous calls. At the time of the writing of the paper, another limitation was that not all kernels were multi-threaded. So basically, not all kernels supported the one to one model that we talked about. In order to deal with this limitation, the decision in the paper was to make these helper entities processes. Therefore, they call this model AMPED, Asymmetric Multi-Process Event-Driven model. It's an event-driven model. It has multiple processes. And these processes are asymmetric. The helper ones only deal with blocking I/O operation. And then, the main one performs everything else. In principle, the same kind of idea could have applied to the multi-threaded scenario where the helpers are threads, not processes, so asymmetric multi-threaded event-driven model. And in fact, there is a follow-on on the Flash work that actually does this exact thing, the AMTED model. The key benefits of the symmetric model that we described is that it resolved some of the limitations of the pure event-driven model in terms of what is required from the operating system, the dependence on asynchronous I/O calls and threading support. In addition, this motto lets us achieve concurrency with a smaller memory footprint than either the multi-process or the multi-threading model. In the multi-process or multi-threading model, a worker has to perform everything for a full request. So its memory requirements will be much more significant than the memory requirements of a helper entity. In addition, with the AMPED model, we will have a helper entity only for the number of concurrent blocking I/O operations. Whereas, in the multi-threaded or multi-process models, we will have as many current entities, as many processes, or as many threads as there are concurrent requests regardless of whether they block or not. The downside is that audit works well with the server pipe applications. It is not necessarily as generally applicable to arbitrary applications. In addition, there are also some complexities with the routing of events in multi CPU systems. Here is a quick quiz analyzing the memory requirements of the three concurrency models we talked about so far. The question is, of the three models mentioned so far, which model likely requires least amount of memory? The choices are the Boss-Worker Model, the Pipeline Model and the Even-Driven Model. Also answer why you think that this model requires the least amount of memory to see if your reasoning matches ours. The correct answer is that likely the event-driven model will consume least resources. Recall that in the other models, we had a separate thread for each of the requests or for each of the pipeline stages. In the event-driven model, we have handlers which are just procedures in that address space, and the helper threads only occur for blocking I operations. For the event-driven model, extra memory is required only for the helper threads that are associated with concurrent blocking I/O calls. In the boss-worker model, extra memory will be required for threads for all concurrent requests, and similarly, even in the pipeline model, concurrent requests will demand multiple threads to be available in a stage of the pipeline if the level of concurrency is beyond the number of pipeline stages. As a result, the event-driven model will likely have the smallest memory footprint. With all this background on the event-driven model, we will now talk about the Flash paper. Flash is an event-driven webserver that follows the AMPED model, so basically it has asymmetric helper processes to deal with the blocking guy operations. In the discussion so far, we really described the architecture of Flash. So it uses helper processes for blocking I/O operations. And then everything else is implemented as an event dispatcher with handlers performing different portions of the web servicing tasks. Given that we are talking about a web server, and this is the old fashioned Web 1.0 technology where basically the web server just returns static files. The blocking I operations that are happening an the system are really just disk reads, so the server just reads files that the client requests. The communication from the helpers to the event dispatcher is performed via pipes. The helper reads the file in memory via the mmap call, and then the dispatcher checks the in-operation mincore, if the pages of the file are in memory. And it then uses this information to decide if it should just call one of the local handlers, or if it should pass the request to the helper. As long as the file is in memory, reading it won't result in a blocking I/O operation, and so passing it to the local handlers is perfectly okay. Although this is an extra check that has to be performed before we read any file, it actually results in big savings because it prevents the full process from being blocked if it turns out that a blocking I/O operation is necessary. Now we will outline some additional detail regarding some of the optimization that Flash applies. And this will help us later understand some of the performance comparisons. The important thing is that these optimizations are really relevant to any web server. First of all, Flash performs application-level caching at multiple levels. And it does this on both data and computation. What we mean by this is, it's common to cache files. This is what we call data caching. However, in some cases it makes sense to cache computation. So in the case of the web server, the requests are requests for some files. These files need to be repeatedly looked up. So you need to find the file, traverse the directory, look up some of the directory data structures. That processing will compute some results. So some location, some pathname for the file. And we will just cache that. We don't have to recompute that and look up the same information next time a request for that same file comes in. Similarly in the context of web processing, the HTTP header that files have that are returned to the browser, it's really going to depend on the file itself. So a lot of the fields in there are file dependent given that the file doesn't change. The header doesn't have to change so this is another type of application level caching that we can perform and Flash does this. Also Flash does some optimizations that take advantage of the networking hardware and the network interface card. For instance all of the data structures are aligned so that it's easy to perform DMA operations without copying data. Similarly, they use DMA operations that have scatter-gather support, and that really means that the header and the actual data don't have to be aligned one next to another in memory. They can be sent from different memory locations, so there's a copy that's avoided. All of these are very useful techniques, and are now fairly common optimizations. However, at the time the paper was written, they were pretty novel, and in fact, some of the systems they compare against did not have some of these things included. Before we continue I would like to briefly describe the Apache Web Server. It's a popular open source web server, and it's one of the technologies that in the flash paper the author's compare against. My intent is not to give a detailed lecture on Apache. That's beyond the scope of the course, but instead I wanted to give you enough about the architecture of Apache, and how it compares to the models that we discussed in the class. And also the other way around, to understand how these discussions in class, are reflected in real world designs. >From a very high level, the software architecture of Apache looks like this. The core component provides the basic server-like capability, so this is accepting connections and managing concurrency. The various modules correspond to different types of functionality that is executed on each request. The specific Apache deployment can be configured to include different types of modules. For instance, you can have certain security features, some management of dynamic content, or even some of the modules are really responsible for more basic HTP request processing. The flow of control is sort of similar to the event driven model that we saw, in the sense that each request passes through all of the modules. Like in the event driven module each request ultimately passed through all the handlers. However, Apache's a combination of a multiprocess and a multithreaded model. In Apache, a single process, a single instance, is internally a multithreaded, boss/worker process that has dynamic management of the number of threads. There's some configurable thresholds that can be used to dynamically track when to increase or decrease the number of threads in the pool. The total number of processes, so the MP part of the model, can also be dynamically adjusted, and for these, it's information such as number of outstanding connections, number of pending requests, CPU usage, a number of factors can drive how the number of the threads per process and the total number of processes are adjusted. It is now time to discuss the experimental approach in the Flash paper. In the paper, the experiments are designed so that they can make stronger arguments about the contributions that the authors claim about Flash. And this is something that you should always consider when designing experiments. That they should help you with the arguments that you're trying to make. To do this, to achieve a good experimental design, you need to answer a number of questions. For instance, you should ask yourself, what is it that you're actually comparing? Are you comparing two software implementations? The hardware the same. Are you comparing two hardware platforms? Make sure then the software is the same. You need to outline the workloads that will be used for evaluation. What are the inputs in the system? Are you going to be able to run data that resembles what's seen in the real world or are you going to generate some synthetic traces? These are all important questions you need to resolve. Not to forget the metrics, we talked about them earlier in this lesson is that execution time or throughput or response time. What is it that you care for and who are you designing this system for? Is it the manager? Is it resource usage in the system? Or is it ultimately the customer's? So let's see now how these questions were treated in the Flash paper. Let's see what were the systems that they were comparing, what were the comparison points? First they include a comparison with a multiprocess version of the same kind of Flash processing. So a web server with the exact same optimizations that were applied in Flash however, in a multiprocess, single-threaded configuration. Then again, using the same optimizations as Flash, they put together a multithreaded web server that follows the boss-worker model. Then they compare Flash with a Single Process Event-Driven model, so this is like the basic event-driven model that we discussed first. And then they also use as a comparison, two existing web server implementations. One was a more research implementation that followed the SPED model, however it used two processes and this was to deal with the blocking I/O situation. And then another one was Apache and this is the open-source Apache web server. And this was at the time when this was then an older version obviously than what's available today and at the time Apache was a multiprocess configuration. Except for Apache, every single one of these implementations integrated some of the optimizations that Flash already introduced. And then, every single one of these implementations is compared against Flash. So this basically means, is that they're comparing the different models, multiprocess, multithreaded SPED against the AMPED, the asymmetric multiprocess event-driven model. Given that all of these really implement, otherwise the exact same code with the same optimizations. Next let's see what are the workloads they chose to use for the evaluations. To define useful inputs, they wanted workloads that represent a realistic sequence of requests. Because that's what will capture our distribution of web page accesses. But they wanted to be able to reproduce, to repeat the experiment with the same pattern of accesses. Therefore, they used a trace-based approach where they gathered traces from real web servers. And then they replayed those traces so as to be able to repeat the experiment with the different implementations. So that every single one of the implementations can be evaluated against the same trace. What they ended up with were two real world traces, they were both gathered at Rice University where the authors are from, actually were from. Some of them are no longer there. The first one was the CS web trace, and the second one was the so-called Owlnet trace. The CS trace represents the Rice University Web Server for the Computer Science Department. And it includes a large number of files and it doesn't really set in memory. The Owlnet trace, that one was from a web server that hosted the number of student webpages. And it was much smaller, so would typically fit in the memory of common server. In addition to these two traces, they also use the synthetic workload generator. And with the synthetic workload generator, as opposed to replaying these traces of real world page access distributions. They would perform some best or worst type of analysis, or run some what if questions. Like what if the distribution of the web pages accesses had a certain pattern, would something change about their observations? And finally, let's look at what are the relevant metrics that the authors picked in order to perform their comparisons. First, when we talk about web servers, a common metric is clearly bandwidth. So what is the total amount of useful bytes or the bytes transferred from files, over the time that it took to make that transfer? And the unit is clearly bytes per second, megabytes per second and similar. Second, because they were particularly concerned with Flash's ability to deal with concurrent processing. They wanted to see the impact on connection rate as a metric. And that was defined as the total number of client connections that are serviced over a period of time. Both of these metrics were evaluated as a function of the file size, so the understanding they were trying to gain was. How does the workload property of requests that are made for different file sizes impact either one of these metrics? The intuition is that with a larger file size, the connection cost can be ammortize. And that you can at the same time push out more bytes, so you can basically obtain higher bandwidth. However, at the same time the larger the file, the more work that the server will have to do for each connection. Because it will have to read and send out more bytes from that larger file. So that will potentially negatively impact the connection rate. So this is why they chose that file size was a useful parameter to vary. And then understand it's impact on these metrics for the different implementations. Let's now look at the experimental results. We will start with the best case numbers. To gather the best case numbers, they used a synthetic load in which they varied the number of requests that are issued against the web server, and every single one of the requests is for the exact same file. Like for instance, every single one of the requests is trying to get index.html. This is the best case because really in reality clients will likely be asking for different files, and in this pathological best case it's likely basically the file will be in cash so every one of these requests will be serviced as fast as possible. There definitely won't be any need for any kind of disk IO. So for the best case experiments, they measure bandwidth and they do that, they vary the file size of zero to 200 kilobytes and they measure bandwidth as the n, the number of requests, times the file size over the time that it takes to process the n number of requests for this file. By varying the file size, they varied the work that both the web server performs on each request but also the amount of bytes that are generated on a request. You sort of assume that as we increase the file size that the bandwidth will start increasing. So let's look at the results now. The results show the curves for every one of the cases that they compare. The flash results are the green bar, SPED is the single process event driven model, MT, multi-threaded, MP, multi-process, Apache, this bottom curve, corresponds to the Apache implementation And Zeus, that corresponds to the darker blue. This is the SPED module that has two instances of SPED so the dual process event driven model. We can make the following observations. First, for all of the curves, initially when the file size is small, bandwidth is slow, and as the file size increases, the bandwidth increases. We see that all of the implementations have very similar results. SPED is really the best. That's the single process event driven, and that's expected because it doesn't have any threads or processes among which it needs to context switch. Flash is similar but it performs that extra check for the memory presence. In this case, because this is the single file tree. So every single one of the requests is for the single file, there's no need for blocking I/O. So none of the helper processes will be invoked, but nonetheless, this check is performed. So that's why we see a little bit lower performance for flash. Zeus has an anomaly. Its performance drops here a little bit, and that has to do with some misalignment for some of the DMA operations. So not all of the optimizations are bug-proof in the Zeus implementation. For the multi-thread and multi-process models, the performance is slower because of the context switching and extra synchronization. And the performance of Apache is the worst, because it doesn't have any optimizations that the others implement. Now, since real clients don't behave like the synthetic workload, we need to look at what happens with some of the realistic traces, the Owlnet and the CS trace. Let's take a look at the Owlnet trace first. First we see that for the Owlnet trace, the performance is very similar to the best case with SPED and Flash being the best and then Multi-thread and Multi-process and Apache dropping down. Note that we're not including the Zeus performance. The reason for this trend is because the Owlnet trace is the small trace, so most of it will fit in the cache and we'll have a similar behavior like what we had in the best case, where all the requests are serviced from the cache. Sometimes, however, blocking I/O is required. It mostly fits in the cache. Given this, given the blocking I/O possibility, SPED will occasionally block. Where as in Flash their helper processes will help resolve the problem. And that's why we see here that the performance of Flash is slightly higher than the performance of the SPED. Now if we take a look at what's happening with the CS trace, this, remember, is a larger trace. So it will mostly require I/O. It's not going to fed in the cache, in memory in the system. Since the system does not support asynchronous I/O operations, the performance of SPED will drop significantly. So relative to where it was, close to Flash, now it's significantly below Flash and, in fact, it's below the multi-process and the multi-threaded implementations. Considering the multi-thread and the multi-process, we see that the multi-threaded is better than the multi-process, and the main reason for that is that the multi-threaded implementation has a smaller memory footprint. The smaller memory footprint means that there will be more memory available to cache files, in turn that will lead to less I/O, so this is a better implementation. In addition, the synchronization and coordination and contact switching between threads in a multi-thread implementation is cheaper, it happens faster than long processes in a multi-process implementation. In all cases, Flash performs best. Again, it has the smaller memory footprint compared to multi-threaded and the multi-process, and that results in more memory available for caching files or caching headers. As a result of that, fewer requests will lead to a blocking I Operation which further speeds things up. And finally, given that everything happens in the same address space, there isn't a need for explicit synchronization like with the multi-threaded or multi-process model. And this is what makes Flash perform best, in this case. In both of those cases, Apache performed worse, so let's try to understand if there's really an impact of the optimizations performed in Flash. And here the results represent the different optimizations. The performance that's scattered with Flash without any optimizations that's the bottom line. Then Flash with the path only optimizations, so the path only, that's the directory lookup caching, so that's like the computation caching part. Then the red line here, the path and maps, so this includes caching of the directory lookup plus caching of the file. And then the final bar, so the final line, the black line, that includes all of the optimization. So this is the directory lookup, the file caching as well as the header computations of the file. And we see that as we add some of the optimizations, this impacts the connection rates of the performance that can be achieved by the web server significantly improves. We're able to sustain a higher connection rate as we add these optimizations. This tells us two things. First, that these optimizations are indeed very important. And second, they tell us that the performance of Apache would have been also impacted, if it had integrated some of these same optimizations as the other implementations. To summarize, the performance results for Flash show the following. When the data is in cache, the basic SPED model performs much better than the AMPED Flash, because it doesn't require the test for memory presence, which was necessary in the AMPED Flash. Both SPED and the AMPED Flash are better than the multi-threaded or multi-process models, because they don't incur any of the synchronization or context switching overheads that are necessary with these models. When the workload is disk-bound, however, AMPED performs much better than the single-process event-driven model, because the single process model blocks, since there's no support for asynchronous I/O. AMPED Flash performs better than both the multi-threaded and the multi-process model, because it has much more memory efficient implementation, and it doesn't require the same level of context switching as in these models. Again, only the number of concurrent I/O bound requests result in concurrent processes or concurrent threads in this model. The model is not necessarily suitable for every single type of server process. There are certain challenges with event-driven architecture. We said, some of these can come from the fact that we need to take advantage of multiple cores and we need to be able to route events to the appropriate core. In other cases, perhaps the processing itself, is not as suitable for this type of architecture. But if you look at some of the high performance server implementations that are in use today, you will see that a lot of them do in fact use a event-driven model, combined with a synchronous I/O support. Let's take one last look at the experimental results from the flash paper as a quiz this time. Here's another graph from the Flash paper and focus on the green and the red bars that correspond to the Single-Process Event-Driven model and the Flash-AMPED model. You see that about 100 megabytes, the performance of Flash becomes better than the performance of the SPED model. Explain why, and you should check all that apply from the answers below. Flash can handle I/O operations without blocking. At that particular time, SPED starts receiving more requests. The workload becomes I/O bound. Or, Flash can cache more files. The first answer is correct, yes. Flash has the helper processes, so it can handle I operations without blocking. The second answer really makes no sense. Both processes continue receiving the same number of requests in these experiments. The third answer is correct as well. At 100 megabytes, the workload, it's size increases. It cannot fit in the cache as much as before, and so it becomes more I/O bound. There are more I/O requests that are needed beyond this point. For a SPED, at this point, once the workload starts becoming more O/I bound the problem is that a single blocking i operation will block the entire process. None of the other requests can make progress, and that's why its performance significantly drops at that point. And finally, the last answer, that flash can handle more files. That's really not correct. SPED and Flash have comparable memory footprints. And so, it is not that one can handle more files than the other in the memory cache. If anything, Flash has the helper processing so if those are created, they are going to interfere with the other available memory, and will impact the number of available cache in the negative sense. So if anything, it will have less available memory for caching files than SPED, so this is not an answer that explains why the Flash performance is better than the SPED performance. Before we conclude this lesson I'd like to spend a little more time to talk about designing experiments. It sounds like it's easy, we just need to run bunch of test cases, gather the metrics, and show the results. Not so fast actually, you running tests, gathering metrics and plotting the results. It's not as straightforward as it might seem. There is actually a lot of thought and planning that should go into designing relevant experiments. By relevant experiment, I'm referring to an experiment that will lead to certain statements about a solution. That are credible, that others will believe in, and that are also relevant that they will care for. For example, the paper we discussed is full of relevant experiments. There the authors provided the detailed descriptions of each of the experiments. So that we could understand them and then we could believe that those results are seen. And then we were also able to make well founded statements about flash and the ambit model versus all of the other implementations. Let's continue talking about the web server as an example for which we'll try to justify what makes some experiments relevant. Well, the clients using the Web Server. They care for the response time. How quickly do they get a web page back? The operators, for instance, running that Web Server, that website. We care about throughput, how many total client requests can see that webpage over a period of time? So this illustrates that you will likely need to justify your solution, using some criteria that's relevent to the stakeholders. For instance, if you can show that your solution improves both response time and throughput, everybody is positively impacted, so that's great. If you can show that your solution only improves response time but doesn't really affect throughput, well okay. I'll buy that too. It serves me some benefit. If I see a solution that improves response time and actually degrades throughput, that still could be useful. Perhaps for this improved response time. I can end up charging clients more that ultimately will give me the revenue that I'm losing due to the negative throughput. Or maybe I need to define some experiments in which I'm trying to understand how is the response time that the client see, how is it effected when the overload of the Web Server increases, when the request rate increases? So by understanding the stakeholders and the goals that I want to meet with respect to these stakeholders. I'm able to define what are some metrics that I need to pay attention to. And that will give me insight into useful configurations of the experiments. When you're picking metrics, a rule of thumb should be, what are some of the standard metrics that are popular in the target domain? For instance, for Web Servers, it makes sense to talk about the client request rate or the client response time. This will let you have a broader audience. More people will be able to understand the results and to relate to them, even if those particular results don't give you the best punchline. Then you absolutely have to include metrics that really provide answers to questions such as, why am I doing this work? What is it that I want to improve or understand by doing these experiments? Who is it that cares for this? Answering these questions implies what are the metrics that you need to keep track of. For instance, if you're interested in client performance. Probably the things that you need to keep track of are things like response time, or number of requests that have timed out. Or if you're interested in improving the operator costs, then you worry about things like throughput, or power costs, and similar. Once you understand the relevant metrics, you need to think about the system factors that affect those metrics. One aspect will be things like system resources. This will include hardware resources such as the number and type of CPUs or amount of memory that's available on the server machines, and also the software specific resources like number of threads or the size of certain queues or buffer structures that are available in the program. Then there are a number of configuration parameters that define the workload. Things that make sense for Web Server include the request rate, the file size, the access pattern, things that were varied also in the flesh experiments. And now that you understand the configuration space a little bit better, make some choices. Choose a subset of the configuration parameters that probably are most impactful when it comes to changes in the metrics that you're observing. Pick some ranges for these variable parameters. These ranges must also be relevent. Don't show that your server runs well with one, two, and three threads, so don't vary the number of threads in your server configuration. If you look out and then you see that real world deployments, they have servers with thread counts in the hundreds. Or don't go and vary the file sizes. To have sizes of 10000 and one kilobytes. If you look at what's happening in the real world, file sizes range from maybe from tens of bytes up to tens of megabytes and hundreds of megabytes and beyond. So make sure that the ranges are representative of reality. Again, these ranges must somehow correspond to some realistic scenario that's relevant. Otherwise, nobody will care for your hypothetical results. That is, unless your hypothetical results are concerned with demonstrating the best or the worst case scenarios. Best and worst case scenarios do bring some value, because. They, in a way they demonstrate certain limitations, or certain opportunities that are there, because of the system that you've proposed, because of the solution you have proposed. So these are the only times where picking a non realistic workload makes sense. Like for instance, in the flash paper case. They had an example in which every single one of the requests was accessing one, single file. And there was some value in the results that were obtained through that experiment. For the various factors that you're considering, pick some useful combinations. There will be a lot of experiments where the results simply reiterate the same point. It really doesn't make sense to make endless such results. Few are good, it's good to confirm that some observation is valid, but including tens of them it really doesn't make any sense. A very important point, compare apples to apples. For instance let's look at one bad example. We have one combination in which we run an experiment with a large workload. And a small size of resources. And then a second experiment, second run of the experiment in which we've changed the workload so now we have a small workload and then we have also allocated more resources. So, for instance, more threads. And then we look at these results and we see that In the second case, for the second experimental run. The performance is better, so then we may draw a conclusion, well I've increased the resource size, it added more threads. And therefore, my performance has improved, so I must be able to conclude that performance improves when I increase the resources. That's clearly wrong, I have no idea whether performance improved because I've added more resources. Or because I have changed the workload. So, I'm using a much smaller workload in the second case. This is what we mean by, make sure that you're comparing apples to apples. There's no way you can draw a conclusion between these two experiments. And what about the competition. What is the baseline for the system that you're evaluating? You should think about experiments that are able to demonstrate that the system you're designing, the solution you're proposing, in some way improves the state of the art. Otherwise it's not clear why use yours. And if it's not really the state-of-the-art then at least what's the most common practice, that should be improved. And perhaps there's some other benefits over the state-of-the-art that are valuable. Or at least think about evaluating your system by comparing with some extreme conditions in terms of the workload or resource assignment, so some of the best or worst case scenarios. That will provide insight into some properties of your solution. Like, how does it scale as the workload increases, for instance. Okay, so at this point we have designed the experiments and now what? And now it actually becomes easy. Now that you have the experiments, you need to run the test cases a number of times using the [INAUDIBLE] ranges of the experimental factors. Compute the metrics, the averages over of those n times and then represent the results. When it comes to representing the results, I'm not going to go into further discussion in terms of best practices and how to do that. But just keep in mind that the visual representation can really help strengthen your arguments. And there are a lot of papers that will be discussed during this course that use different techniques on how to represent results. So you can draw some ideas from there. Or there are other documentations online, there are also courses that are taught at Georgia Tech or also in the Audacity's platform that talk about information visualization. So you can benefit from such content in terms of how to really visualize your results. And make sure that you don't just show the results. Actually make a conclusion, spell out what is it that these experimental results support as far as your claims are concerned. Let's now take a quiz in which we will look at a hypothetical experiment, and we'll try to determine if the experiments we're planning to conduct will allow us to make meaningful conclusions. A toy shop manager wants to determine how many workers he should hire in order to be able to handle the worst case scenario in terms of orders that are coming into the shop. The orders range in difficulty starting from blocks, which are the simplest, to teddy bears, to trains, which are the most complex ones. The shop has 3 separate working areas, and in each working area there are tools that are necessary for any kind of toy. These working areas can be shared among multiple workers. Which of the following experiments that represented as a table of types of order that's being processed and number of workers that processes this order will allow us to make meaningful conclusions about the manager's question? The first configuration has three trials. In each trial, we use trains as the work load, so order of trains, and we vary the number of workers, 3, 4, and 5. In the second configuration, again, we have three trials. The first trial consist order of blocks with 3 workers. The second trial is an order of bears with 6 workers. And the third trial is an order of trains with 9 workers. In the third configuration in each of the trials, we have a mixed set of orders of all the different kinds, and we vary the number of workers from 3 to 6 to 9. And in the fourth configuration in each of the trials, we use a set of train orders, and we vary the number of workers from 3 to 6 to 9. Let's quickly talk about what the toy shop manager should want to evaluate. It should be something like this. Given that the most complex case of toy orders includes trains, then we should have in each of the trials a set of orders that are really for trains. Second, the toy shop has three working areas. We can perform any kind of toy order in each of the working area and multiple workers can share an area. So, as we're trying to see how many more workers can we add in the system, how many more toys can we process, we really should be trying to get as many more workers per working area. Now, if we take a look at configuration one, configuration one has correctly in each trial order of trains, the sequence of train orders. That corresponds to our worst case scenario. However, the way the workers are varied in the first case, there are a total of three workers, so there is one in each working area. In the second case, there are a total of four workers, so the first working area has one extra worker. So the number of resources in that case, is larger for the first working area, and then lower for the next two. Similarly, in the third trial We have in two working areas, two workers and in the last one just one. It's really hard to draw any conclusions. The amount of resources that's available in each of these for handling the toys is not equal, therefore, it doesn't really tell us anything about the worst case capacity of the system. If we take a look at the second configuration here, we have the first trial is an order of blocks. The second trial is an order of bears. The third trial is an order of trains. Again, it doesn't tell us anything about the worst case capacity of the system. This could tell us something, but it really is not the question that the manager is asking. The third configuration similarly, it could provide some information. In every single one of these, the workload is mixed. So this could correspond to the average number of toys that can be processed with different number of workers. So how is the average throughput impacted by adding more workers in the store? Again, however, this doesn't address the question of how is the worst case impacted by adding more workers to the store? So that basically gives us the answer to the final configuration. The last configuration is identical to configuration 3 in the number of workers, but it uses the worst case scenario, so it's just orders of trains. So this tells us how much better will I be able to handle the worst case, of just receiving trains, if I add more and more workers, and then really adding an even amount of workers per working area. This is a meaningful set of experiments that will let me draw some conclusions. It will also likely ultimately demonstrate what is the capacity of the individual working area. So, let's say, if I tried maybe to add another trial where I'm running train orders with 12 workers, so four workers per working area. Likely, I will, at some, point no longer start seeing any kind of improvement, simply because I cannot squeeze in more workers per working area. So performing this type of experiment will actually be useful. In this lesson we introduce the Event-driven model for achieving concurrency in applications. We performed comparisons between multi-process, multi-threaded, and an Event-driven approach for implementing a web server application. And in addition, we discussed in more general terms how to properly structure experiments. As the final quiz, please tell us what you learned in this lesson. Also, we'd love to hear your feedback on how we might improve this lesson in the future. In the next set of lessons, we will discuss in more detail some of the key research management components in operating systems. First we will look at how the operating system manages CPUs, and how it decides how processes and their threads will get to execute on those CPUs. This is done by the scheduler. We will review some of the basic scheduling mechanisms, some of the scheduling algorithms and data structures that they use, and we will look in more detail at some of the scheduling algorithms used in the Linux operating system, the completely fair scheduler and the O(1) scheduler. We will also look at certain aspects of scheduling that are common for multi-CPU platforms. This includes multicore platforms and also platforms with hardware level multithreading. For this we will use the paper chip multithreaded processors need a new operating system scheduler in order to demonstrate some of the more advanced features that modern scheduler should incorporate. In this lesson, we will be talking at length about scheduling and OS schedulers. So continuing with our visual metaphor, we'll try to visualize what are some of the issues when it comes to OS scheduling. Using our toy shop analogies, we'll think of an OS scheduler and we'll compare it with a toy shop manager. like an OS scheduler, the toy shop manager schedules work in the toy shop. For a toy shop manager, there are multiple ways how it can choose to schedule the toy shop workers and dispatch them two workers in the toy shop. First, the toy shop owner can dispatch orders immediately, as soon as they arrive in the toy shop. Or the manager may decide to dispatch the simple orders first. Or, even conversely, the manager can decide to dispatch the complex orders as soon as they arrive in the shop. The motivation for each of these choices is based on some of the high level goals of the manager and how he wants to manage the shop and utilize the resources in the shop. Let's investigate these choices a little more. If the manager wants low scheduling overhead, not to have to analyze too much what he needs to do when an order comes, he can choose to dispatch the orders immediately. The scheduling in this case is very simple, we can think of it as a FIFO. First order that comes in, is the first one that will be served. If the manager is concerned with the total number of orders that get processed over a period of time, it would be good to dispatch the simple orders as soon as they arrive. They will be the ones that will be completing more quickly, and that will maximize this particular metric. Obviously, in order to do this, the manager has to spend a little bit more time whenever an order comes, because he needs to check what kind of order that it is, whether it's going to be a simple one or not, and then decide what to do with it. So basically, it will be important for the manager to be able to assess the complexity of the orders. On the other hand, on each of the work benches in the shop, there may be different kinds of tools, and these simple orders, these may not really exercise all of these tools, they may not require the use of all aspects of the work benches. So if the manager wishes to keep all of the resources that are available on the work benches as busy as possible, he may choose to schedule complex orders as soon as they arrive. And then whenever simple ones come in, perhaps the workers can pause the processing of the complex order to get a simple one in and to finish it as soon as possible. In comparison, the scheduling options for a toy shop manager are surprisingly similar to those of an OS scheduler. For an OS scheduler, these choices are analyzed in terms of tasks, which is either a thread or a process. And these are being scheduled onto the CPUs that are managed by the OS scheduler, like the work benches in the case of the toy shop manager. For an OS scheduler, we can choose a simple approach to schedule tasks in a first come, first serve manner. The benefit is that the scheduling is simple and that we don't spend a lot of overhead in the OS scheduler itself. Using a similar analogy to what the toy shop manager was thinking of when he chose to dispatch the simple orders first, an OS scheduler can assign, can dispatch simple tasks first. The outcome of this kind of scheduling can be that the throughput of the system overall is maximized. And there are schedulers that actually follow this kind of algorithm and those are called shortest job first. So simple in terms of the task is equal to its running time, so shortest job first. And finally, the scheduling logic behind assigning complex task first is similar to the case in the toy shop. Here, the scheduler's calls are to maximize all aspects of the platform. So to utilize well both the CPUs as well as any other devices, memory, or other resources that are present on that platform. As we move through this lesson, we will discuss some of the options that need to be considered when designing algorithms such as these. And, in general, we will discuss various aspects of the design and implementation of OS schedulers. We talked briefly about CPU scheduling in the introductory lesson on processes and process management. We said then, the CPU scheduler decides how and when processes access the shared CPUs in the system. In this lesson, we'll use the term task to interchangeably mean either processes or threads, since the same kinds of mechanisms are valid in all context. The scheduler concerns the scheduling of both user-level processes or threads, as well as the kernel-level threads. Recall this figure that we used when we talked originally about processes and process scheduling. The responsibility of the CPU scheduler is to choose one of the tasks in the ready queue, a process or a thread. We said we'll just use the term task to refer to both, and then to schedule that particular task onto the CPU. Threads may become ready so they may enter the ready queue after an I/O operation they have been waiting on has completed or after they have been woken up from a wait on an interrupt, for instance. A thread will enter the ready queue when it's created, so when somebody has forked a new thread. And also a thread that, maybe, was interrupted while executing on the CPU because the CPU has to be shared with other threads. That thread, it was ready, it was executing on the CPU. So, after it has been interrupted, it continues to be ready so it will immediately go into the ready queue. So to schedule something, the scheduler will have to look at all of the tasks in the ready queue and decide which is the one that it will dispatch to run on the CPU. Whenever the CPU becomes idle, we have to run the scheduler. For instance, if a thread that was executing on the CPU makes an I/O request and now it has to wait in the I/O queue for that particular device, the CPU's idle, what happens at that moment, we run the OS scheduler. The goal is to pick another task to run on the CPU as soon as possible, and not to keep the CPU idle for too long. Whenever a new task becomes ready, so a task that was waiting on an I/O operation, or was waiting on an interrupt, or some kind of signal. Or whenever a brand new task was created for the first time. For all of these reasons, again, we need to run the scheduler. The goal is to check whether any of these tasks are of higher importance and therefore should interrupt the task that's currently executing on the CPU. A common way how schedulers share the CPU is to give each of the tasks in the system some amount of time on the CPU. So, when a time slice expires, when a running thread has used up its time on the CPU, then, that's another time when we need to run the scheduler so as to pick the next task to be scheduled on the CPU. Once the scheduler selects a thread to be scheduled, the thread is then dispatched onto the CPU. What really happens is we perform a context switch to enter the context of the newly selected thread. And then we have to enter user mode, then set the program counter appropriately to point to the next instruction that needs to be executed from the newly selected thread. And then we're ready to go. The new thread starts executing on the CPU. So in summary, the objective of the OS scheduler is to choose the next task to run from the queue of ready tasks in the system. The first question that comes to mind is, which task should be selected? How do we know this? The answer to that will depend on the scheduling policy or the scheduling algorithm that is executed by the OS scheduler. We will discuss several such algorithms next. Another immediate question is, how does the scheduler accomplish this? How does it perform whatever algorithm it needs to execute? The details of the implementation of the scheduler will very much depend on the runqueue on the data structure that we use to implement this ready queue. That's called the runqueue. The design of the runqueue and the scheduling algorithm are tightly coupled. We will see that certain scheduling algorithms, they demand a different type of runqueue, different data structure. And also that, the design of the runqueue, it may limit in certain ways what kinds of scheduling algorithms we can support efficiently. So, the rest of the lecture will focus on reviewing different scheduling algorithms and runqueue data structures. For this first discussion of scheduling algorithms I want to focus on what we call run-to-completion scheduling. This type of scheduling assumes that as soon as a task is assigned to a CPU, it will run until it finishes or until it completes. Let me list first some of the assumptions we will make. We will consider that we have a group of tasks that we need to schedule. I'll refer to these also as threads and jobs and similar terms. Let me also, to start with, assume that we will know exactly how much time these threads need to execute. So there will be no preemption in the system. Once a task starts running it will run to completion, it will not be interrupted or preempted to start executing some other task. And also to start with, let me assume that we only have a single CPU. We will relax these requirements further as we go through this lesson. Now since we will be talking about different scheduling algorithms, it will be important for us to be able to compare them, so we're going to think about some useful metrics. When it comes to comparing schedulers, some of the metrics that can give meaningful answers regarding those comparisons include throughput. The average time it took for tasks to complete, the average time the tasks spent waiting before they were scheduled, overall CPU utilization. We will use some of these metrics to compare some of the algorithms that we will talk about. The first and the simplest algorithm we'll talk about is First-Come First-Serve. In this algorithm, tasks are scheduled on the cpu in the same order in which they arrive. Regarding of their execution time, of loading the system, or anything else. When a task completes, the schedule will pick the next task that arrived, in that order. Clearly, a useful way to organize these tasks, would be in a queue structure, so that tasks can be picked up from it in a FIFO manner. Whenever a new task becomes ready, it will be placed at the end of the queue. And then whenever the scheduler needs to pick the next task to execute, it will pick from the front of the queue. To make this decision, all the scheduler will need to know will be the head of the queue structure, and how to dequeue tasks from this queue. So basically for first come first serve scheduling some FIFO like queue would be a great run queue data structure. Now let's take a look at this area in which these three tasks T1 T2 and T3 have the following execution times. T1 is one second, T2 is ten seconds, and T3 is also one second. And let's assume they arrive in this order. So T1 followed by T2 followed by T3. So this is how they'll be placed in the runqueue. Now let's look at through put asymmetric for this kind of system that uses the first come first serve scheduler. We have three tasks. To execute them one after the other we will take total of 12 seconds, 1 plus 10 plus 1. So the scheduler on average achieves a quarter of a task per second. So 0.25 tasks per second. If we are interested in the average completion time of these tasks, the first task will complete in one second since it will start immediately. The second task, it will complete at time T11. It will have to wait one second for the first task to complete, and then it will execute for ten seconds. And then the third task it will complete at time T12, because it will have to wait for the 11 tasks for T1 and T2 to execute until it starts and executes for one second. So, if we compute the average completion time in the system we see that it is 8 seconds. If we're interested in the average wait time for the three tasks in the system, then the first task started immediately. The second task started a second later, because it had to wait for T1 to complete, and then the third task had to wait for 11 seconds before it started executing. So, the average wait time for these three tasks is four seconds. So we have our simple scheduling algorithm, however, probably we can do a little bit better in terms of the various metrics that you're able to achieve with this algorithm when we try something else. So we see that first come first serve is simple, but the wait time for the tasks is poor even if there is just one long task in the system that has arrived ahead of some shorter tasks. So, to deal with this we can look at another algorithm that's called shortest job first, and the goal of this algorithm is to schedule the tasks in the order of their execution time. Given the previous example with tasks T1, T2 and T3, the order in which we want to execute them would be T1 followed by T3 and then T2, the longest task, at the end. And for tasks that take the same amount of time to execute, perhaps we break ties arbitrarily. Now if we organize our run queue the same way as before, adding new tasks to the run queue will be easy, since it will just mean that we have to add a task at the tail of the queue. However when we need to schedule a new task, we'll need to traverse the entire queue until we find the one with the shortest execution time. So run queue won't really be a FIFO run queue anymore, since we will need to remove tasks from the queue in a very specific order based on execution time. One thing we can do is we can maintain the run queue as an ordered queue so that tasks, when they're inserted into the queue, are placed in the queue in a specific order. It will make the insertion of tasks into the queue a little bit more complex, but it will keep the selection of a new task as short as it was before. We just need to know the head of the queue. Or, our run queue doesn't really have to be a queue. It can be some treelike data structure, in which the nodes in this tree are ordered based on their execution time. When inserting new nodes in the tree, new tasks in this tree, the tree may need to be rebalanced. However, for the scheduler, it will be easy, since it will always have to select the left most note in the stream, if the tree is ordered, the left most note will have the smallest execution time. So we a queue, a tree, this illustrates really that the run queue doesn't really have to be a queue. It can be other type of data structure, and we'll see that it often is, based on whatever is appropriate for the scheduling algorithm Let's do a quiz in which we analyze the performance metrics for shortest job first. To do this, let's assume first that shortest job first is used to schedule three tasks, T1, T2, and T3. Also, let's make the following assumptions. The scheduler will not preempt the task, so this will be the run to completion type of model that we discussed so far. We'll assume that we know the execution times of these tasks, and we'll use the same values as we used before. So, T1 and T3 are 1 second and T2 is ten seconds. And, let's assume that they're all arrive at the same time. So, we will start analyzing this system from some time, T0, when all of these tasks are actually in the system already. Per this system, calculate the throughput, the average completion time, and the average wait time, using the shortest job first algorithm. Let's look at the answer to this question. So first, given that we have a shortest job first algorithm and these are the execution times of the tasks, what is going to be the execution order of these tasks? Clearly they'll have to execute in this order. So T1, followed by T3 or the other way around, and then followed by T2. Now that we know the order of the tasks, we can actually compute these three metrics. To compute the throughput, again we have three tasks in the system, all of the three tasks get processed in 12 seconds. So 1 plus 10 plus 1. So we have a total throughput of 0.25 tasks per second. So nothing has really changed compared to the first come, first serve algorithm here. Now if we look at the average completion time, the first task T1, completes in 1 second. The second task T3, that one completes in another 1 second so in 2 seconds. And then the third task, T2, well that one will need another 10 seconds, so it will complete at time T12. So the average completion time is 1 plus 2 plus 12 over 3. That's five seconds. That's already way better than the eight seconds that we saw in the first come first serve algorithm. If we look at the average wait time, the first task, T1, it then waited, always started executing immediately. T3 had to wait one second, T2 had to only wait two seconds, since both T1 and T3 are short. So, the average wait time in this system is only one second. That is way better than the four seconds that first come first serve resulted in. So if we care about metrics such as these. Clearly shortest job first would be a better algorithm than first come, first served. So far in our discussion, we assume that the task that's executing on the CPU cannot be interrupted, cannot be preempted. Let's now relax that requirement, and start talking about preemptive scheduling, scheduling in which the tasks don't just get the CPU and hog it until they're completed. So we'll consider preemption. First in the context of the shortest job first algorithm. And for this, we will also make another assumption or we will modify another assumption that we made earlier. Tasks don't all have to arrive at the same time. So we're going to look at the system that has three tasks. T1, T2, and T3. We know their execution time, so that assumption still holds. And we will assume that they arrive now at arbitrary times. In this particular case, T2 arrives first. When T2 arrives, it's the only task in the system. T1 and T3 haven't arrived yet. So the scheduler will clearly schedule it and it will start executing. When the tasks T1 and T3 show up, then T2 should be preempted. We're using shortest job first, and T1 and T3 have shortest jobs compared to T2. The execution of the rest of the scenario will look something as follows. Let's say at T2 when the tasks T1 and T3 show up. T1 is the one that's scheduled next. Then once it completes, T3 is the next one that has the shortest running time. So T3 will execute. And once these two have completed, then T2 can take the remaining of its time to execute. So basically, what would need to happen in order for us to achieve this kind of behavior is that, whenever tasks enter the run queue, like T1 and T3, the scheduler needs to be involved, so that the scheduler can inspect their execution times, and then decide whether to preempt the currently running task, task T2, and schedule one of the newly readied tasks. Now, so far we talked as if we know the execution time of a task. But it's principle, that's, that's hard to tell. It's really hard to claim that you know what is the execution time of a task. There are a number of factors that depend on the inputs of the task, on whether the data is present in the cache or not. Which other tasks are running in the system. So in principle, we have to use some kind of heuristics in order to estimate, or rather guesstimate what the execution time of a task will be. When it comes to the execution time, so the future execution time of the task, it's probably useful to consider, what was the past execution time of that task, or that job. So in a sense, history is a good predictor of what will happen, so we will use the past execution time to predict the future. For instance, we can think about how long a task ran the very last time it was executed. Or maybe we can take an average over a period of time or over a number of past runs of that particular task. We call this scenario in which we compute the averages over a period of the past, a windowed average, so we compute some kind of metric based on a window of values from the past, and the window of historic values. And then use that average for prediction of the behavior of the task in the future. Shorter's job considers the execution time of the tasks in order to decide how to schedule tasks, and when to preempt a particular task. Another criteria for driving those decisions may be that the tasks have different priority. Tasks that have different priority levels, that's a pretty common situation. For instance, we have certain operating system level threads that run OS tasks, that have higher priority than other threads that support, maybe user-level processes, or even within a user-level process, maybe certain threads that are intended to run when there is user input. Such threads may have higher priority compared to other threads that just do some background processing for long running simulations. In such scenarios the scheduler will have to be able to run the highest priority task next. So clearly it will have to support preemption. It will need to be able to stop a low priority task and preempt it, so that the high priority one can run next. So, let's look at now at the same example from before, except now we're going to use priority scheduling. And we need to know the task's priorities. And in this particular example, the priorities P1, P2, and P3 are such that the first thread, T1, has the lowest priority. Followed by the second thread, so it's Priority, P2. And then finally, the third thread, P3, has the highest Priority, P3. Again, we start with the execution of T2, since it's the only thread in the system. Now however, when T1 and T3 become ready at this particular point in time, when time is doom, we'll have a very definite execution compared to the shortest job first with preemption scheduler. So when we look at the priorities, we see that T3 has the highest priority, P3. So when threads one and three become ready and they arrive at this particular moment, thread two is first going to be preempted and the execution of the thread three will start. So thread three has the highest priority. When thread three completes, at that point thread two has the lower priority. So we'll have to give the CPU to thread two. And now it's pretty unfortunate that for thread 1 we'll have to wait for all this time before it can execute. But a priority based scheduling is only going to look at the priorities of the threads. So thread 1 is not going to really start running until the 11th second in this time graph and then it will complete at time T12 as the entire schedule will complete at that time as well. In this example we were looking at this table, but in principal, our scheduler if it needs to be a priority based scheduler, it will somehow need to quickly be able to assess not just what are the runnable threads in this system that are ready to execute, but also what are their priorities? And it will need to select the one that has the highest priority to execute next. We can achieve this by having multiple run queue structures. Different run queue structures for each priority level. And then have the scheduler select a task from the runqueue that corresponds to the highest priority level. In this case that was the P3. Other than having per priority queues, another option would be to have some kind of ordered data structure. Like, for instance, the tree that resolve with the shortest job first. However, in this case, with priority scheduling, this tree would need to be ordered based on the priority of the tasks. One danger with priority-based scheduling is starvation. We can have a situation in which a low priority tasks is basically infinitely start in a run queue just because they're constantly higher priority tasks that show up in some of the other parts of the run queue. One mechanism that we use to protect against starvation is so called priority aging. What that means is that, the priority of the task isn't just a fixed priority, instead, it's some kind of function of the actual priority of the thread. Plus one other factor, and that is the time that the thread or the task spent in the run queue. The idea is that the longer a task spends in a run queue, the higher it priority should become. So eventually the task will become the highest priority task in the system, using this priority aging mechanism, and it will run. And in this manner, starvation will be prevented. Let's take a quiz now in which we will compute some of the performance metrics for a preemptive scheduler. An OS scheduler uses a priority-based algorithm with preemption to schedule tasks. Given the values that are shown in this table, complete the finishing times for each of the three tasks. Also assume that the priorities are such that P3 is slower than P2 is, and then is slower than P1. Write the finish times of each of the tasks in the boxed areas provided here. To answer this question, let's look at the execution scenario in this system. T3 is the first one to arrive in the system, and it will be the only task for a while. So, T3 will be the first one to be scheduled. At time, T3 it will be preempted, and task T2 will start executing, because it has higher priority than T3, at time T5. Task 1 will arrive, and that one will execute for three seconds. T1 will preempt the execution of T2, because T1 has higher priority than T2, according to this assumption. And then, we have two more seconds to finish the execution of T2, followed by the execution of the lowest priority task, T3. So, that one, we had one more second remaining. So, the finishing times of the three tasks according to this diagram are going to be 8 seconds, 10 seconds and 11 seconds, respectively. An interesting phenomenon called priority inversion happens once we introduce priorities into the scheduling. Consider the following situation and we'll use the shortest job for scheduling to illustrate what happens. We have three threads, the first one has the highest priority. And P3 is the lowest priority, so T1 is the highest priority,` and T3 is the lowest one. For this example we have left out the execution time. Just assume they take some longer amount of time all these tasks and that the time graph continues into the future. Initially T3 is the only task in the system. So T3 executes, and lets say that T3 also acquired a lock. Now T2 arrives, T2 has higher priority than T3, so T3 will be preempted and T2 gets to execute. Now time 5, T1 arrives, it has higher priority than T2, so T2 will be preempted. And then T1 executes for two time units. And at that point, it needs to acquire a lock, and this happens to be the exact same lock that's already held by T3. Unfortunately that won't happen. T1 cannot acquire the lock so T1 at that point is put on a wait queue associated with the lock. We schedule the next highest priority task that's runable, that's going to be T2. And then T2 will execute as long as it needs. And let's say in this case, we're locked out. T2 really only needed to execute for two more units, at that point, T2 completes. We schedule the next highest priority runnable task. The only runnable task in the system is T3. So T3 will get to execute. In fact, T3 will execute for as long as it needs to, until it's releases the lock. Only after T3 re, release the lock will T1 become runnable and then once T1 becomes runnable again. It is the highest priority thread. So T1 will preempt T3 and T1 will continue it's execution. So it will acquire the lock and continue executing. So based on the priority in the system, we were expecting that T1 will complete before T2, and then T3, the lowest priority thread, will be the last one to complete. However, that's not what happened and the actual order of execution was as follows: T2, the medium priority thread; followed by T3, the lowest priority thread; followed by finally T1, the highest priority thread last in the system. So the priorities of these tasks were inverted. This is what we call Priority Inversion. A solution to this problem would've been to temporarily boost the priority of the mutex owner. What that means is that, at this particular point when the highest priority thread needs to acquire a lock that's owned by a low priority thread, the priority of T3 is temporarily boosted to be, basically at the same level as T1. Then T1 could not have proceeded just as before given that the log is down by somebody else, however, instead of scheduling T2, we would have scheduled T3. Priority would have been temporarily boosted to that of T1, so it would have been higher than T2. And then T2 would have completed, and so at least we would have been able to go ahead and start executing T1 at this particular point. And then we would not have had to wait for T2 to complete for the medium priority threat to get in the middle. This technique of boosting the priority of the mutex owner, this is why, for instance, it is useful to keep track of who is the current owner of the mutex. This is why we said we want to have this kind of information in the mutex data structure. And obviously if we're temporarily boosting the priority of the mutex owner we should lower its priority once it release the mutex. The only reason why we were boosting its priority was so as to be able to schedule the highest priority threat to run as soon as possible. So wanted to make sure that the mutex is released as soon as possible. This is a common technique that's currently present in many operating systems. Now, when it comes to running tasks that have the same priority, we have other options in addition to the first-come first-serve or shortest job first that we discussed so far. A popular option is so-called a round robin scheduling. So let's say we have the following scenario. We have three tasks, they all show up at the same time, and they're all in the queue. With round robin scheduling, we'll pick up the first task from the queue just like with the first-come, first-serve scheduling. Now, let's say we pick up T1, that's the first in the queue, so T1 starts executing. Now, if they're executing for one time unit, the task stopped because it now needs to wait on an I/O operation. So it will yield the CPU and be blocked on that I/O operation. This is unlike what we saw in first-come first-serve, where we were assuming that each of the tasks executes until it completes. If that's the case, we'll schedule T2. T3 will move to the front of the queue. Now, potentially, T1 will complete its I/O operation and will be placed at the end of the queue behind T3. Then when T2 completes, we will schedule T3. The execution time here we assume is only two time units. And then when that one completes, we'll finally pick up T1 from the queue and complete T1. If T1 had not been waiting on I/O, then the execution based on T1, T2, T3, the order in which they were placed in the queue, would've looked something like this. So each of their tasks executes one by one in a round robin manner, and the queue is traversed in a round robin manner one by one. We can further generalize round robin to include priorities as well. Let's assume that the tasks don't arrive at the same time, T2, and T3 arrive a little later, and that their priorities are as follows, T1's priority is the lowest and T3's is the highest. So in that case, what happens is that when a higher priority task arrives, the lower priority task will be preempted. If T2 and T3, however, have the same priorities, then the scheduler will just go round robin between them until they complete. So basically, in order to incorporate priorities, we have to include preemption, but otherwise the tasks will be scheduled from the queue like in first-come first-serve. So, the first task from the queue. However, we will release the requirement that they have to control the CPU, that they have to execute on the CPU until they complete. Instead, they may explicitly yield. And we will just round robin among the tasks on the queue. A further modification that makes sense for round robin is not to wait for the tasks to yield explicitly, but instead to interrupt them so as to mix in all of the tasks that are in the system at the same time. We call such mechanism time slicing. So let's say we can give each of the tasks a time slice of one time unit. And then, after a time unit, we will interrupt them, we will preempt them, and we will schedule the next task in the queue in a round robin manner. So we will cycle through them, T1, T2, T3, and then again. We will focus our discussion next on timeslicing to better explain this mechanism. We mentioned timeslices very briefly in the introductory lesson on processes. To define it more specifically, a timeslice is the maximum amount of uninterrupted time that can be given to a task. It is also referred to as a time quantum. The timeslice determines the maximum amount, that means that a task can actually run a less amount of time than what the timeslice specifies. For instance the task may need to wait on an I/O operation or to synchronize with some other tasks in the system, on a mute tag that's locked. In that case the task will be placed on a queue, will no longer be running on the CPU. The task will run less amount of time, once it's placed on the queue the scheduler will pick the next task to execute. Also if we have a priority-based scheduling, a higher priority task will preempt a lower priority one, which means that the lower priority task will run less amount of time than the timeslice. Regardless of what exactly happens, the use of timeslices allows us to achieve for the tasks to be interleaved. They will be timesharing the CPU. For I/O bound tasks, this is not so critical since they're constantly releasing the CPU to wait on some I/O event. But for CPU bound tasks, timeslices is the only event that we can achieve time-sharing. They will be preempted after some amount of time as specified by the timeslice. And we will schedule the next CPU bound task. Let's look at some examples now, consider for an instance the simple first-come-first-serve and shortest job first scheduler that we saw earlier in this lesson, they both had a same mix of task with same arrival times but led to different metrics. And, note that the metrics that we computed for first-come-first-serve also apply to a round-robin scheduler that doesn't use timeslices. Given that these tasks don't perform any I/O, they just execute for some fixed, specified amount of time. Then, round-robin would have scheduled them one and after another the way they showed up in the queue. And, that would have been identical to first-come-first-serve. Now let's calculate the metrics for a round-robin scheduler with timeslices, and let's say we'll first look at a timeslice of 1. The execution of these tasks will look as follows. T1 will execute for 1 second, that happens to be exactly the time that T1 requires, so T1 will complete. Then, with timeslicing, we would have preempted the execution of T1 anyways. We will execute T2. Now, T2 needs more time to execute, 10 seconds. So, it will actually be preempted. T3 will run for 1 second. At the time when we're about to preempt it, the T3 will anyways complete, and the only runnable task in the system is T2, and we will execute T2. So T2 will run for the remainder of the time. Now if we look at some of the metrics for throughput, we'll see we have the exact same thing. It's still takes us 12 seconds to complete these three tasks, so the throughput will again be the same as in the previous two systems. Looking at the wait time, we have a wait time of, 0, 1, and 2 for each of the tasks respectively. So wait time is 1 second. If we look at the average completion time, the tasks complete at 1, at 12, and at 3 seconds respectively. So the average completion time is 5.33 seconds. So without even knowing what are the execution times of these tasks, with a timeslice of 1, we were able to achieve a schedule that's really close to this best one that we saw before, the shortest job-first one. This is good. We keep some of the simplicity that we had in first-come-first-serve. We don't need to worry about the going out, what is the execution time of the task. And yet we're able to achieve good wait time and good completion time for all of the tasks. Some of the benefits of this timeslice-based method, particularly when the timeslice is relatively short like in this case, is that we end up with a situation where the short tasks finish sooner. So T3 was able to complete much sooner than in the first come, first serve case. And we're also able to achieve a schedule of that is more responsive and any I/O operations can be executed and initiated as soon as possible. So for instance, consider if T2 is a task that the users interact with, it will be able to start as soon as possible, only 1 second into the execution. The users will see that it is running. And yet it will be preempted to squeeze in T3 at this point. If T2 needs to initiate any I/O operations, those will be initiated during this interval that it's running at this point. That would not have been the case with the shortest job first scheduler, and because T2 would have only been scheduled after all of the shorter jobs complete, so T1 and T3 in that case. The downside is that we exaggerated so far a little in that we had these tasks immediately starting their execution after the previous one was interrupted. However, there's some real overheads. We have to interrupt a running task. We have to run the scheduler in order to pick which task to run next. And then we actually need to perform the context which when we're scheduling from the context of task of one task to another. All of these times are pure overhead. There's no useful application processing that's done during those intervals. Know that these overheads, so if we have a timeslice of one these set time-outs for the timer will appear during the execution of task T2 except at that point since there are no other runable tasks in the system we're really not going to be scheduling or contact switching. And that's the dominant part of the overhead. And exactly how these sticks are handled, we're not really going to discuss further in this class. The dominant sources of these overheads will impact the total execution of this timeline, and increasing the time will cause the throughput to go down. Each of the tasks will also start just a little bit later, so the wait time will actually increase a little bit. And the completion time for each of the tasks will be delayed by a little bit so the average completion time will increase as well. The exact impact on these metrics will depend on the length of the timeslice and how it relates to the actual time to perform these context switching and scheduling actions, so as long as the timeslice value significantly larger than the context switch-time, we should be able to minimize these overheads. In general, we should consider both the nature of the tasks as well as the overheads in the system when determining meaningful values for the timeslice. We saw in the previous morsel that the use of timeslices delivers certain benefits. We're able to start the execution of tasks sooner, and therefore, we are able to achieve an overall schedule of the task that's more responsive. But that came with certain overheads. So the balance between these two is going to imply something about the length of the timeslice. We will see that to answer this question, how long should a timeslice be, the balance between these two differs if we're considering I/O-bound tasks, so tasks that mostly perform I/O operations, versus CPU-bound tasks, so tasks, these are tasks that are mostly executing on the CPU and don't do any I/O or do very little I/O. We will look at these two different scenarios next. Let's first look at an example in which we will consider two CPU bound tasks. So, these are tasks that are mostly just running on the CPU and don't perform any I/O. Let's assume their execution time is ten seconds, and in this system let's assume that the time to context switch from one context to another takes just 0.1 seconds. For this scenario now let's take a look at what will happen when we consider two different timeslice values, timeslice of one second, and timeslice of five seconds. With a timeslice of one second, the execution of these two tasks will look something as follows, and let's assume that the thicker vertical bars are trying to capture this context, which overhead in this case. For the timeslice value of five seconds, the schedule will look as follows. In the context switch overhead is only paid at this particular points. That's in contrast with having to context switch at every single one of these vertical bars here. Now if we compute the metrics for throughput, average wait time, and average completion time for these tasks, we will obtain the following results. To complete the throughput, we calculate the total time that it took to execute these tasks, and divide it by two, by the number of tasks. To complete the average wait time, we look at the start time of each of the tasks, and divide that by two, the number of tasks. And to complete the average completion time, we'll look at when each of the two tasks completed, and then we average that. And the detailed calculations for both of these are in the instructors notes. Looking at these metrics, we see that for throughput, we are better off choosing a higher timeslice value. For completion time, also, we're better off choosing a higher timeslice value. And then for the average wait time of the task, we are actually better off choosing a lower timeslice value. However, these are CPU bound tasks. We don't really care about their responsiveness and the user is not necessarily going to perceive when they started. The user really cares about when they complete, and overall when all of the tasks submitted to the system complete. So this really means that CPU bound tasks we're really better off with choosing a larger timeslice. This is the winning combination for us. In fact, for CPU bound tasks, if we didn't know timeslicing at all, so like the timeslice value is infinite, we'd end up with absolutely the best performance in terms of the throughput and the average completion time, so the metrics that we care for when we run CPU bound tasks. Yes, of course, the average wait time of the task will be worse in that case, but we don't care about that. It's CPU bound task. We won't notice that. In summary, a CPU bound task prefers a larger timeslice. Now, let's try to see what will happen if we're considering two I/O bound tasks. And again, we'll think of two tasks that have execution time of ten seconds, and in a system that has a context switch overhead of 0.1 second. And let's also assume that the nature in which these I/O calls are performed is that a task issues an I/O operation every one second. And also let's assume that every single one of those I/O operations complete in exactly half a second. If we look at the timeline, it looks identical as what we saw in the case for the CPU-bound jobs with a time slice of one second. This makes sense, because exactly after one second, the tasks are, in this case, not exactly preempted. They actually end up issuing an I/O operation, so yielding voluntarily, regardless of the fact that the time slice is 1. So if we look at the performance metric for this case, they will be identical to the previous case in the case of the CPU bound tasks. Now if we look at the second case that has a time slice value of five seconds, we see that the timelines, the schedules of the two tasks, are identical to the case when we had a much smaller time slice value of one second. Similarly, obviously, if we compute the matrix, they will be identical, it's for two identical schedules. The reason for this is that at this particular moment, we're not exactly time slicing. We're not interrupting the tasks in either one of these cases. The I/O operation, again, is issued every one second. So regardless of the fact that in this scenario, the time slice value is much longer, we still end up issuing an I/O operation at the end of the first second. And therefore, the CPU is at this point released from T1, T1 yields, and T2 is free to be scheduled and to start executing. So one conclusion that you can make from this is that for I/O bound tasks, the value of the time slice is not really relevant. Well, let's not draw that conclusion that fast. Let's look first at a scenario where only T2 is an I/O bound task. T1 is a CPU bound task, as what we saw in the previous morsel. In that case, the execution for the two tasks T1 and T2 when the timeslice is 1 will look the same. The one difference is that in the event of T1, we preempted after one second, where as in the case of T2, the I/O bound task, after one second, it voluntarily yields since it has to go and wait for an I/O operation. In the case of five seconds, the execution of T1 and T2 will look something as follows. T1 will run for five seconds, and at that point, its time slice will expire, so it will be preempted. T2 will be scheduled, and as an I/O bound task, T2 will yield after one second because of reading on an I/O operation. At that point, T1 will be scheduled again. Now, at this point, T1 is actually complete, so T2 is the only runnable task in the system, and that's why we have this illustrated in this manner. If we work out the performance metrics for this last case, the numbers will look as follows. And again, for all of these, the calculations are posted in the instructor notes. We see that both with respect to throughput and the average wait time, the use of a smaller time slice results in better performance. The only reason why, in this case, the average completion time is so low, then, if you look at the calculations, there is a huge variance between the completion time of T1, which is at 11, and then T2, which is way later. We see from this that for I/O bound tasks, using a smaller time slice is better. The I/O bound task with a smaller time slice has a chance to run more quickly, to issue an I/O request, or to respond to a user. And with a smaller time slice, we're able to keep both the CPU as well as the I/O devices busy, which makes, obviously, the system operator quite happy. Let's summarize quickly on our question: how long should a timeslice be? CPU bound tasks prefer longer timeslices. The longer the timeslice, the fewer context switches we'll have to perform, so that basically limits the context switching overheads that the scheduling will introduce. To perform useful work, the useful application processing to as slow as possible. And as a result, both the CPU utilization, as well as the system throughput as metrics will be as high as possible. On the other hand, I/O bound tasks prefer shorter timeslices. This allows I/O bound tasks to issue I/O operations as soon as possible. And as a result, we achieve both higher CPU and device utilization, as well as the performance that the user perceives that the use, the system is responsive and that it responds to its commands and displace output. Let's take a quiz that looks at a problem involving timeslices and their values. On a single CPU system, consider the following workload and conditions. We have ten I/O bound tasks in the system and one CPU bound task. The I/O bound tasks issue one I/O operation every one milliseconds of CPU compute time. I/O operations always take 10 milliseconds to complete. The context switching overhead is 0.1 millisecond. And at the end also all tasks are long running. So we're not reporting any kind of execution time for the tasks. Given this, answer first, what is the CPU utilization for a round robin scheduler with a timeslice of one millisecond. How about for a 10 millisecond timeslice? Provide the two answers here, and round them up to the nearest percent value. The formula for computing CPU utilization is provided in the Instructor's Notes. In the case when the time slice is one millisecond, every one millisecond either we will be preempting the CPU bound task or the I/O bound tasks on their own will be stopping because they need to perform an I operation every one millisecond. So that means for every one millisecond of useful work, we have total of one millisecond of the useful work, plus 0.1 millisecond of the context switching overhead. So, total useful CPU utilization is 91%. For the case when we have a round robin scheduler with a 10 millisecond timeslice, as we're going through the 10 I/O bound tasks, every single one of them will run just for one millisecond and then we will have to stop because of issuing an I/O request, so we'll have them context switch in that case. So we will perform for the I/O bound tasks, 10 times 1 millisecond of useful work, and 10 times 1 millisecond plus 10 times 0.1 millisecond for the context switch. So this is the total, amount of total work that has been performed. And then finally the CPU bound tasks will be scheduled and that one will run for full 10 milliseconds because the timeslice value is 10 milliseconds. So that will complete 10 milliseconds of useful work and then it will complete total of 10 milliseconds plus a context switch time of a total time. If we compute this, this comes out too close to 95%. So, what this example shows us is that from the CPU's perspective, from the CPU utilization perspective, having a large timeslice that favors the CPU bound task is better. We didn't ask the question of what is the I/O utilization for both of these cases. Likely if we work out the math for that case, we will see that from the perspective of the I/O device, it is better to have a smaller timeslice. And that will be because the I/O device really cannot do anything during this entire period, when the CPU bound tasks is running for 10 milliseconds with the long time slice. We said earlier that the runqueue is only logically a queue. It could also be represented as multiple queues, like when dealing with different priorities, or it could be a tree, or some other type of data structure. Regardless of the data structure, what is important is that it should be easy for the scheduler to find the next thread to run, given the scheduling criteria. If we want the I/O and CPU bound tasks in the system to have different timeslice values, we have two options. The first option is to maintain a single runqueue structure, but to make it easy for the scheduler to figure out easy what type of task is being scheduled, so that it can apply the different policy. Another option is to completely separate I/O and CPU bound tasks into two different data structures, two different runqueues, and then with each runqueue associate a different kind of policy that's suitable for CPU versus for I/O bound tasks. One solution for this, is this type of data structure that we will explain now. It is a multi queue data structure, that internally it has multiple separate cues. Let's say in the first round of queue we'll place the most I/O intensive tasks, and we will assign with this round queue a timeslice of 8 milliseconds. Let's say for the tasks that are of medium I/O intensity, so they have a mix of I/O and CPU processing, we have a separate queue in this multi-queue data structure. And here we assign with this queue, a timeslice of 16 milliseconds. And then for all of our CPU intensive tasks, we'll use another queue in the multi-queue data structure. And here we'll associate with this timeslice, basically that's infinite. So this will be like the first come, first serve policy. >From the scheduler's perspective, the I/O intensive tasks have the highest priority, which means that the scheduler will always check on, on this queue first. The queue that's associated with the I/O Intensive Tasks. And the CPU-bound tasks will be treated as tasks with lowest priority. So, this queue will be the last one to be checked when trying to figure out what is the next task to be scheduled. So, depending on the type of task that we have, we place it in the appropriate queue. And on the other side, the scheduler selects which task to run next. Based on highest priority, medium, and then lowest. So in this way we both provide the time slicing benefits for those tasks that benefit for the I/O bound tasks, and avoid the time slicing overhead for the CPU bound tasks. But how do we know if a task is CPU or I/O intensive? How do we know how I/O intensive is the task? Now we can use for that some history based heuristics, like slide the task run and then decide what to do with it. Sort of like what we explained with the shortest job first algorithm. But, what about new tasks, or what about tasks that have dynamic changes in their behavior? To deal with those problems, we will treat these three queues not as three separate runqueues, but as one single multi-queue data structure. This is how this data structure will be used. When a task first enters the system, so a newly created task will enter it in the topmost queue. The one that has the lowest timeslice associated with it. It's like we're expecting that it's the most demanding task. When it comes to these scheduling operations that it will need to be context which most often. If the task stops executing before these 8 milliseconds, so whether it yields voluntarily or it stops, because it needs to wait an I/O operation. That means we made a good choice. The task is indeed fairly I/O interactive, and so we want to keep the task in this level. So next time around, when it becomes runnable, after that I/O operation completes, it will be placed in this exact same queue. However, if the task ends up using up its entire timeslice. That means that it was more CPU intensive than we originally thought. So we will push it down the next level. It will be preempted from over here, but then the next time it needs to run, it will actually be pushed into this queue, so it will be scheduled from this queue. If the task ends up getting preempted when it was scheduled from this queue, so it used up its entire 60 millisecond time slice. That means that it's even more CPU intensive. So in that case it will even get pushed down to the bottom queue. So we basically have a mechanism to push the task down these levels based on its historic information. Although we didn't know if a task is I/O CPU intensive to start with. We made an assumption, and then we were able to correct it. So we assume that it's I/O intensive. And we were able to correct and push it down these levels, down to the lowest most level, in case it's CPU intensive. Now if a task that's in one of the lower queues all of a sudden starts getting repeatedly releasing in the CPU earlier, then whatever the timeslice specifies, because it is waiting on our operation. There will be a hintto the scheduler to say, oh, well, this task is more I/O intensive than I originally thought. And it can push it up at one of the queues that are on the higher levels. This resulting data structure is called the multilevel feedback queue. And for the design of this data structure, along with other work on time sharing system, Ferdando Corbato received the Turing Award, which is like the highest award in computer science. It's the equivalent of the Nobel Prize for computing. I want to make sure you don't trivialize the contribution of this data structure, and say that it's equivalent to priority queues. First of all, there are different scheduling policies that are associated with each of the different levels that are part of this data structure. More uniquely, however, this data structure incorporates this feedback mechanism, that allows us over time to adjust which one of these levels will be place a task, and when we're trying to figure out what is the best time sharing schedule for the subtask in the system. The Linux, so called O of one scheduler, that we will talk about next, that uses some of the mechanism borrowed from this data structure as well. And we won't describe the Solari scheduling mechanism. But I just want to mention that that's pretty much a multi-level feedback queue with 60 levels. So 60 subqueues. And also some fancy feedback rules that determine how and when a thread gets pushed up and down these different levels Let's now look at couple of concrete examples of schedulers that are part of an actual operating system. First, we will look at the so called O(1) scheduler in Linux. The O(1) scheduler receives it's name because it is able to perform task management operations, such as selecting a task from the run queue, or adding a task to it, in constant time. Regardless of the total number of active tasks in the system. It's a preemptive and priority-based scheduler, which has total of 140 priority levels, with zero being the highest and then 139 the lowest. These priority levels are organized into two different classes, the tasks, the priority levels from zero to 99 fall into a class of real time tasks, and then all others fall into a so called time sharing class. All user processes have one of the time sharing priority levels. Their default priority is 120 but it can be adjusted with a so called nice value. There's a system call that can be used to do this. And the nice values can be between negative 20 and 19, so as to span the entire set of time sharing priorities. The O(1) scheduler borrows from the multilevel feedback queue scheduler, in that it associates different timeslice values with different priority levels. And it also uses some kind of feedback from how the tasks behaved in the past, to determine how to adjust their priority levels in the future. It differs, however, in how it assigns the timeslice values to priorities and how it uses the feedback. It assigns timeslice values based on the priority level of the task, similar to what we saw in the multilevel feedback queues in the scheduling. However, it assigns smallest timeslice values to the low priority, CPU bound tasks and it assigns high timeslice values to the more interactive high priority tasks. The feedback it uses for the time sharing tasks is based on the time that the task spends sleeping, the time that it was waiting for something or idling. Longer sleep times indicate that the task is interactive, it's spent more time waiting, for instance, on user input or in some type of events. Therefore, when longer sleeps are detected, we need to increase the priority of the task and we do that by actually subtracting five, in particular from the priority level of the task. In this way, we're essentially boosting the priority, so next time around, this interactive task will execute bit higher priority. Smaller sleep times are indicative of the fact that the task is compute intensive. Therefore, we want to lower its priority and we do that by incrementing it by adding the number five to it up to a maximum, and essentially, the task next time around will execute in a lower priority class. The runqueue in the O(1) scheduler is organized as two arrays of task queues. Each array element points to the first runnable task at the corresponding priority level. These two arrays are called Active and Expired. The active list is the primary one that the scheduler uses to pick the next task to run. It takes constant time to add a task since you simply need to index into this array based on the priority level of the task and then follow the pointer to the end of the task list to enqueue the task there. It takes constant time to select a task because the scheduler relies on certain instructions that return the position of the first set bit in a sequence of bits. So, if the sequence of bits corresponds to the priority levels and a bit value of one indicates that there are tasks at that priority level. Then, it will take a constant amount of time to run those instructions to detect what is the first priority level that has certain tasks on it. And then, once that position is known, it also takes a constant time to index in to this array and select the first task from the runqueue that's associated with that level. If tasks yield the CPU to wait on an event or are preempted due to higher priority task becoming runnable. The time they spent on the CPU is subtracted from the total amount of time, and if it is less than the timeslice, they're still placed on the corresponding queue in the active list. Only after a task consumes its entire timeslice will it be removed from the active list and placed on the appropriate queue in the expired array. The expired array that contains the tasks that are not currently active in the sense that the scheduler will not select tasks from the expired array as long as there are still tasks on any of the queues in the active array. When there are no more tasks left in the active array, at that point, the pointers of these two list will be swapped and the expired array will become the new active one and vice versa. The active array will start holding all of the tasks that are removed from the active array and are becoming inactive. This also explains the rationale why in the O(1) scheduler, the low priority tasks are given low timeslices, and the high priority tasks are given high timeslices. As long as the high priority tasks have any time left in their timeslice, they will keep getting scheduled, they will remain in the one of the queues in the active array. Once they get placed on the expired array, they will not be scheduled. And therefore, we want the low priority tasks to have a low timeslice value so that, yes they will get a chance to run, however they won't disrupt the higher priority tasks, they won't delay them by too much. Also note that the fact that we have these two arrays also serves like an aging mechanism so these high priority tasks will ultimately consume their timeslice be placed on the expired array and ultimately, the low priority tasks will get a chance to run for their small time amount. The O(1) scheduler was introduced in the Linux kernel 2.5 by Ingo Molnar. In spite of its really nice property of being able to operate in constant time, the O(1) scheduler really affected the performance of interactive tasks significantly. And as the work loads changed as typical applications in the Linux environment were becoming more time sensitive, think Skype, movie streaming, gaming. The jitter that was introduced by the O(1) scheduler was becoming unacceptable. For that reason, the O(1) scheduler was replaced with the completely fair scheduler, and the CFS scheduler became the default scheduler starting in the Linus 2.6.23 kernel. Ironically, both of these scheduler's are developed by the same person. You should note that both the O(1) and the CFS scheduler are part of the standard Linux distribution. This one is the default, however, if you wish, you can switch and choose the Linux O(1) scheduler to execute your tasks. As we said, when problem with the O of 1 scheduler in Linux is that once tasks are placed on the expired list, they wouldn't be scheduled until all remaining tasks from the active list have a chance to execute for whatever their timeslice amount of time is. As a result, the performance of interactive tasks is affected. There is a lot of jitter. In addition, the scheduler in general doesn't make any fairness guarantees. There are multiple formal definitions of fairness, but intuitively you can think of it that in a given time interval, all of the tasks should be able to run for an amount of time that is proportional to their priority. And for the O of 1 scheduler, it's really hard to make any claims that it makes some kind of fairness guarantees. As we said, Ingo Molnar proposed the completely fair scheduler, CFS, to address the problems with the O of 1 scheduler. And CFS is the default scheduler in Linux since the 2.6.23 kernel. It's the default scheduler for all of the non-real time tasks. The real time tasks are scheduled by a real time scheduler. The main idea behind the Completely Fair Scheduler is that it uses a so-called a Red-Black Tree as a Runqueue structure. Red-black trees belong to this family of dynamic tree structures that have a property that as nodes are added or removed from the tree. The tree will self balance itself, so that all the paths from the root to the leaves of the tree are approximately of the same size. You can look at the instructor notes for a link for more information about this type of data structure. Tasks are ordered in the tree based on the amount of time that they spend running on the CPU, and that's called virtual runtime. CFS tracks this virtual runtime in a nanosecond granularity. As we can see in this figure, each of the internal nodes in the tree corresponds to a task. And the nodes to the left of the task correspond to those tasks which had less time on the CPU. They had spent less virtual time. And therefore, they need to be scheduled sooner. The children to the right of a node are those that have consumed more virtual time, more CPU time. And therefore, they don't have to be scheduled as quickly as the other ones. The leaves in the tree, really don't play any role in the scheduler. The CFS scheduling algorithm can be summarized as follows. CFS always schedules the task which had the least amount of time on the CPU, so that typically would be the left most node in the tree. Periodically CFS will increment the vruntime of the task that's currently executing on the CPU. And at that point, it will compare the virtual runtime of the currently running task to the vruntime of the leftmost task in the tree. If the currently running task has a smaller vruntime compared to the one that's in the leftmost node in the tree, the currently running task will continue executing. Otherwise, it will be preempted, and it will be placed in the appropriate location in the tree. Obviously, the task that's corresponding to the leftmost node in the tree will be the one that will be selected to run next. To account for differences in the task priorities or in their niceness value. CFS changes the effective rate at which the task's virtual time progresses. For lower priority tasks, time passes more quickly. Their virtual run time value progresses faster. And therefore, they will likely lose their CPU more quickly, because their virtual run time will increase, compared to other tasks in the system. On the contrary, for high priority tasks, time passes more slowly. Their virtual runtime value will progress at a much slower rate, and therefore, they will get to execute on the CPU longer. You should take note of the fact that CFS uses really one run queue structure for all of the priority levels, unlike what we saw with some of the other scheduler examples. In summary, selecting a task from this run queue to execute takes O of 1 time. Takes constant amount of time since it's typically just a matter of selecting the leftmost node in the tree. At the same time, adding a task to the run queue takes logarithmic time relative to the total number of tasks in the system. Given the typical levels of load in current system, this log of n time is acceptable. However, as the computer capacity of the nodes continues to increase and systems are able to support more and more tasks. It is possible that at some point the CFS scheduler will be replaced by something else that will be able to perform better when it comes to this second performance criteria. As a review, I would like to ask a question about the two Linux schedulers that we just discussed. What was the main reason the Linux O of 1 scheduler was replaced by the CFS scheduler? Was it because scheduling task under high loads took unpredictable amount of time? Low priority tasks could wait indefinitely and starve? Or because interactive tasks could wait unpredictable amounts of time to be scheduled to run? Select the appropriate answer. Let's take a look at each of the choices that are given. The first statement is not correct. The Linux O of 1 scheduler was O of 1 because it took constant amount of time to select and schedule a task regardless of the load. The second statement is sort of correct in the sense that as long as there were continuously arriving higher priority tasks, it was possible for the low priority tasks to keep waiting an unpredictable amount of time and possibly indefinitely and therefore, starve. But this was really not the main reason why the scheduler was replaced. The final choice was the main reason. Recall that we said that the common work, workloads were becoming much more and more interactive and were demanding high predictability. In the 01 scheduler with the active and expired list, once the task was moved to the expired list, it had to wait there until all of the low priority tasks consumed their entire time quantum. For a very long time Linus Torvalds resisted integrating a scheduler that would address the needs of the small interactive tasks in the Linux kernel. His rational was that Linux was supposed to be a general purpose operating system and should not necessarily be addressing any of the needs of some more real time or more interactive tasks. And therefore he liked the simplicity of the offline scheduler. However, as the general purpose work loads began to change, then a general purpose operating system like Linux, have to really incorporate a scheduler that would address the needs of those general purpose workloads and CFS was really meeting those needs. Let's now look at scheduling on multi-CPU systems. However, before we start talking about scheduling, let's look a little bit at some architecture detail. First we will look at shared memory multiprocessors. And then we will also take a look at how this compares to multi-core architectures. In a shared memory multiprocessors, or SMPs, there are multiple CPUs. Each of them have their maybe own private caches, like L1 and L2. There are last level caches that may or may not be shared among the CPUs. And there is a system memory, DRAM, that is shared across all of the CPUs. Here we show just one memory component, but it is possible that there would be multiple memory components. But the point is that all of the memory in the system is shared among all of the CPUs. In the current multicore world, each of these CPUs can have multiple internal cores, so multiple CPUs internally. Each of those cores will have private caches, and then overall the entire multicore CPU will have some shared last level cache. And then again, there will be some shared system memory. Here in this picture, we have a CPU with two cores, so that's a dual-core processor, and this is more common for client devices like laptops, for instance, or even cell phones today can have two CPUs. Whereas on the server and platforms, it's more common to have CPUs that have six or eight cores and to also have multiple such CPUs, so we'll have multiple multicore CPUs. As far as the operating system is concerned, it sees all of these CPUs as well as the cores in the CPU as entities onto which it can schedule all execution context, so threads. All of these are, as far as the operating system is concerned, possible CPUs for it can schedule some of its workload. So to make our discussion more concrete, we will first start talking about scheduling on multi-CPU systems in the context of SMP systems, and a lot of these things will apply to the multicore world because again, the scheduler just sees the cores as CPUs. And we'll make some comments that are more exclusively applied to multicores towards the end of this lesson. We said in our earlier lectures that the performance of threads and processes is highly dependent on whether the state that the thread needs is in the cache or in memory. Let's say a thread was executing on CPU one first. Over time this thread was slightly able to bring a lot of the state that it needs both into the last level of cache that's associated with this CPU, as well as in the private caches that are available on the CPU. And in this case, when the caches are hot, this helps with the performance. Now, the next time around, if the thread is scheduled to execute on the other CPU, none of its state will be there so the thread will operate with a cold cache. We'll have to bring all of the state back in and that will affect performance. Therefore, what we want to achieve with a scheduling on multi-CPU systems is to try to schedule the thread back on the same CPU where it executed before because it is more likely that its cache will be hot. We call this cache affinity and that is clearly important. To achieve this, we basically want the scheduler to keep a task on the same CPU as much as possible. To achieve this, we can maintain a hierarchical scheduler architecture, where at the top level, a load balancing component divides the tasks among CPUs. And then a per-CPU scheduler with a per CPU runqueue repeatedly schedules those tasks on a given CPU as much as possible. To balance the load across the different CPUs and their per-CPU runqueue, the top level entity in the scheduler can look at information such as the length of ea, of each of these queues to decide how to balance tasks across them. Or potentially when a CPU is idle, it can at that point start looking at the other CPUs and try to get some more work from them. In addition to having multiple processors, it is possible to also have multiple memory nodes. The CPUs and the memory nodes will be interconnected via some type of interconnect. For instance, on modern Intel platforms, there is a interconnect that's called QuickPath Interconnect, or QPI. One way in which these memory nodes can be configured is that a memory node can be technically connected to some subset of the CPU, so for instance, to a socket that has multiple processors. If that is the case, then the access from that set of CPUs to the memory node will be faster versus from that particular processor to a memory node that's associated with another set of CPUs. Both types of accesses will be made possible because of the interconnect that's connecting all of these components. However, they will take different amount of time. We call these types of platforms non-uniform memory access platforms, or NUMA platforms. So then clearly, from a scheduling perspective, what would make sense is for the scheduler to divide tasks in such a way that tasks are bound to those CPUs that are closer to the memory node where the state of those tasks is. We call this type of scheduling NUMA-aware scheduling. The reason why we have to contact switch among threads is because the CPU has one set of registers to describe the active execution context, the thread that's currently running on the CPU. These include the stack pointer and program counter, in particular. Over time, however, hardware architects have recognized that they can do certain things to help hide some of the overheads associated with contact switching. One way this has been achieved, is to have CPUs that have multiple set of registers, that each set of register can describe the context of a separate thread, of a separate execution entity. One term that's used to refer to this is hyper threading. So, hyper threading refers to multiple hardware-supported execution contexts, so hyper threads. There's still just one CPU, so on this CPU only one of these threads can execute at a particular moment of time. However, the context switching between these threads is very fast. And just basically the CPU needs to switch from using this set of registers to another set of registers. Nothing has to be saved or restored. This mechanism is really referred to by multiple names. So in addition to hyperthreading a common term is also to refer to this is hardware multithreading, or chip multithreading or simultaneous multithreading, SMTs. So, we will used basically these two terms, hyperthreading and SMTs, more dominantly than the others. Hardware today frequently supports two hardware threads. However, there are multiple higher end server designs to support up to eight hardware threads. And one of the features of today's hardware is that you can enable or disable this hardware multithreading at boot time, given that there's some trade-offs associated with this as always. If it is enabled, as far as the operating system is concerned, each of these hardware contexts appears to the operating system's scheduler as a separate context, a separate virtual CPU, onto which. Which it can schedule threads given that it can load the registers with the thread context concurrently. So for instance in this figure the scheduler will think that it has two CPUs and it will load these registers with the context of these two threads. So one of the decisions that the scheduler will need to make is which two threads to schedule at the same time to run on these hardware contexts. To answer that question, let's remind ourselves of what we talked about when we talked about the context switching time. We said that it, the time that a thread is idling, that a thread has to wait on something to happen is greater than twice the time to perform a context switch, then it makes sense to actually do the context switch in order to hide this waiting, this idling latency. In SNT systems, the time to perform a context switch among the two hardware threads is in the order of cycles. And the time to perform a memory access operation, a memory log, remains in the order of hundreds of cycles, so it's much greater. So given this, then it means that it does make sense to context switch to the other hardware thread. And in that way, this technology hyperthreading will help us even hide the memory access latency that threads are experiencing. Hyperthreading does have implication and scheduling, in that it raises some other requirements when we're trying to decide what kinds of threads should we co-schedule on the hardware threads in the CPU. We will discuss this question in the context of the paper Chip Multithreaded Processors Need a New Operating System Scheduler by Sasha Fedorova and others. To understand what's required from a scheduler in a simultaneous multi threading system, let's make some assumptions first. Since we will base our discussions on Federova's paper, we will use the same assumptions that she has made, and the figures that we will use to illustrate those assumptions will be reproductions from her paper and her presentation. The first assumption that we will make is that a thread can issue an instruction on every single cycle. So that means that a CPU bound thread, a thread that just issues instructions that need to run on the CPU, will be able to achieve a maximum metric in terms of instructions per cycle. And that would be instructions per cycle equal one. Given that we have just one CPU, we cannot have a IPC that is greater than one. The second assumption that we will make is that a memory access takes four cycles. What this means is that a memory-bound thread will experience some idle cycles while it's waiting for the memory access to return. We will also assume that the time it takes to contact switch among the different hardware threads is instantaneous, so we won't take any overheads over that into consideration. And also let's to start with for the sake of our discussion, let's assume that we have a SNT with two hardware threads. let's take a look first at what would happen if we chose to co-schedule on the two hardware contacts. Two threads that are both compute-bound, so compute intensive, or CPU bound. What that means is both of the threads are ready to issue a CPU instruction on every single cycle. However given that there is only one CPU pipeline, so one CPU fetch decode issue ALU logic, only one of them can execute at any given point of time. As a result these two threads will interfere with each other. They will be contending for the CPU pipeline resources. And best case, every one of them will basically spend one cycle idling while the other thread issues its instruction. As a result, for each of the threads, its performance will degrade by a factor of two. Furthermore, looking at the entire platform, we will notice that in this particular case our memory component, the memory controller, they're idle. There's nothing that's scheduled that performs any kinds of memory accesses. Well that's not good either. Well another option is to co-schedule to memory-bound threads. In this case we see however, that we end up with some idle cycles because both of the threads end up issuing co-memory operation. And then they need to wait four cycles until it returns. Therefore, we have two of the cycles that are unused. So, the strategy then to co-schedule memory bound threads leads to wasted CPU cycles. So then our final option is to consider mixing some CPU and memory intensive threads, and then if we see, we end up with the desired schedule. It's a bingo. We end up fully utilizing each processor cycle, and then, whenever there is a thread that needs to perform a memory reference, we context switch to that thread in hardware. The thread issues the memory reference, and then we context switch back to the CP-intensive thread. Until the memory reference completes. Scheduling, a mix of CPU and memory-intensive threads, allows us to avoid or at least limit the contention on the processor pipeline. And then also, all of the components, both the CPU and the memory will be well utilized. Note that this still will lead to some level of degradation due to the interference between these two threads. For instance, the compute bound thread can only execute three out of every four cycles, compared to when it ran alone. However, this level of the degradation will be minimal given the properties of the particular system While the previous example gave us a good idea what type of scheduler to use, that the scheduler should mix CPU and memory bound tasks. The question that is open at this point is how do we know if a thread is CPU bound versus a memory bound. To answer this question we will use historic information. We will look at the thread's past behavior. And this is similar to what we said was useful when we were trying to determine whether a thread is interactive or I/O bound versus CPU bound. However, previously we used sleep time for this type of differentiation of I/O versus CPU bound. And that won't work in this case. First of all, the thread is not really sleeping when it's waiting on a memory reference, the thread is active and it's just waiting in some stage in the processor pipeline and not on some type of software queue. Second, to keep track of the sleep time we were using some software methods, and that's not acceptable. We cannot execute in software some computations to decide whether a thread is CPU bound or memory-bound. Given the fact that the context switch takes order of cycles, so the decision what to execute should be very, very fast. Therefore, we somehow need some kind of hardware support, some information from the hardware in order to be able to answer this question. Fortunately, modern hardware has lots of so called hardware counters that get updated as the processor is executing and keep information about various aspects of the execution. These include information about the cash usage for instance such as the L1, L2, or the last level cash misses or information about the number of instructions that were retired. So that we can compute the IPC or in your platforms there's also information regarding the power or energy usage of the CPU or particular components of the system. There are a number of interfaces and tools that can be used to access these hardware counters via software. For instance, oprofiler or the Linux perf tool are available in Linux, and one of the things that's useful is that if you look at the oprofile website, it actually has a list of all of the hardware counters that are available for different architectures. Because the hardware counters are not uniform on every single platform. So, then how can hardware counters help a scheduler make any kinds of scheduling decision? Many practical as well as research based scheduling techniques rely on the use of hardware counters to understand something about the requirements of the threads in terms of the kinds of resources that they need, CPUs or memory. So then, the scheduler can use that information to pick a good mix of the threads that are available in the run queue to schedule on the system so that all of the components of the system are well utilized or that, so that the threads don't interfere with one another. Or, whatever other scheduling policy needs to be achieved. For instance, a scheduler can look at a counter like the last level cash misses and using this counter, a scheduler can decide that a thread is memory bound, so its footprint doesn't fit in the cash. Or the same counter can also tell the scheduler that something changed in the execution of the threat so that now it's executing with some different data in a different phase of its execution, and it's running with the cold cash. What this tells us is that, that one counter can tell us different things about a thread. So, given that there isn't a unique way to interpret the information provided from hardware counters, we really sort of guesstimate what it is that they're telling us about the thread's resource use. That doesn't mean that the use of hardware counters is not good, in fact schedulers can use hardware counters to make informed decisions regarding the workload mix that they need to select. They would typically use some combination of the counters that are available on the CPU, not just one, in order to build a more accurate picture of the threads resource needs. And they would also rely on some models that have been built for a specific hardware platform and that have been trained using some well understood workloads. So we ran a workload that we know is memory intensive and we made some observations regarding the values of those counters and therefore, we now know how to interpret them for other types of workloads. These types of techniques really fall into much more advanced research problems, which are a little bit out of the scope of this particular introductory course. However, I really wanted to make sure that you're aware of the existence of these hardware counters and how they can be used. And how they can be really useful when it comes to resource management in general not just regarding CPU scheduling. As a more concrete example, Fedorava speculates that a useful counter to use to detect the thread CPU NS versus memory NS is cycles for instruction. She observes that memory bound threads take a lot of cycles to complete an instruction therefore it has a high CPI. Where is the CPU-bound thread, it will complete an instruction every cycle or near that and therefore, it will have a CPI of 1, or a low CPI. So this speculates then, it would be useful to gather this kind of information, this counter about the cycles per instruction. And use that as a metric in scheduling threats on hyper-threaded platforms. Given that there isn't exactly a CPI counter on the processors that Fedorova uses in her work, and computing something like one over IPC would require software computations so that wouldn't be acceptable. For that reason Fedorapa uses a simulator, that supposedly the CPU does have a CPI counter. And then she looks at a better scheduler can take that information and make good decisions. Her hope is that if she can demonstrate that CPI is a useful metric, then hardware engineers will add this particular type of counter in future architectures. To explore this question she simulates a system that has four cores where every one of the cores is four way multi threaded. So, there's a total of 16 hardware contexts in her experimental test bed. Now, she wants to bury the threads that get assigned to these hardware contexts based on their CPI. So, she creates a synthetic workload, where her threads have a CPI of one, six, 11 and 16. Clearly the threat with the CPI of one will be the most CP intensive, and then the threats with a CPI of 16 will be the most, memoring threads. And the overall work load mix has four threads of each kind. And then what she wants to evaluate is what is the overall performance when a specific mix of threads gets assigned to each of these 16 hardware contexts. To understand the performance in tact of such potentially different scheduling positions, she uses a metric, the instructions per cycle. Given that the system has four cores in total, the maximum IPC that could be achieved is going to be four. So, four instructions per cycle will be the best case scenario for where every single one of the cores complete one instruction in each cycle. And then she conducts several experiments as shown in this figure. In every one of the experiments she manually and statically changes how the workload is distributed across the course. So in the first experiment on core one, the four hardware threads will be assigned threads that have, software threads that have a CPI of one, six, 11, and 16. The four hardware threads on Core 2 will be assigned software threads and tasks that have CPI of one, six, 11, and 16, and so forth. In the first experiment every one of the Cores runs identical mix, where. Each hardware thread runs a task with a different CPI. And then in the last experiment, each of the cores runs a very different kinds of mix, where on Core 0, all of the tasks are CPU intensive, they have a CPI of 1. Where as on Core 3, all of the tasks are memory intensive, because they have a CPI of 16. And then the second and the third round of the experiments falls somewhere between these two extremes. So what she's trying to do, she's trying to make some static decisions that a scheduler would have made. And in doing that she's trying to understand whether it even makes sense to build a scheduler that will use CPU as a metric. Instead of a typical quiz, I would like for you to do a self check of your analytical skills. Here is a diagram that's summarizing the performance results that Fedorova gathered from running the experiments that we showed before. What do you think these results tell us about the use of a metric-like cycles per instruction for scheduling? Again, this is not really a quiz. This is more of a self check of your analytical skills. Try answering this question and then see our summary of the results of using CPI as a scheduling metric in the next video. Here are the conclusions that we can draw from these results. If we look at the cases for a and b, if we remember from the table that we saw with the actual experiments, in these cases we had a fairly nice mix of tasks with different CPI values on each of the cores. So in these cases, the processor pipeline was well utilized and we're obtaining a high IPC. It is not the maximum IPC of 4, so the maximum performance that one can obtain on the processor that they will be simulating, but it's fairly high. If we'll look at the cases for c and d, in these experiments each of the cores was assigned tasks that have much more similar CPI values. In fact, in the case of d, every single one of the cores ran tasks that had the exact same CPI value. So if we take a look at these results, we see that the total IPC is much lower than what we saw for the case a and b. The reason for that is that on some cores, like on core zero in particular, but also on core one, there is a lot of contention for the CPU pipeline. So these are the cores that mostly had tasks with a low CPI, so mostly the compute intensive tasks were here. On the other hand, cores two and three, they contribute very little to the aggregate IPC metrics. So basically, they really execute only very few instructions per cycle or a few, small percentage of an instruction per cycle rather, given that a maximum is one. The reason for that is that we mostly have memory intensive tasks on these two cores and that leads to wasted cycles on them. So by running this experiment, Fedorova confirmed her hypothesis that a running tasks that have mixed CPI value is a good thing, that that will lead to an overall high performance of the system. So the conclusion from these results is that CPI's a great metric. And therefore, we should go ahead and build hardware that has a CPI hardware counter and tracks this value so that we can then go ahead and also build operating system schedulers that use this value in order to schedule the right workload mix. Not so fast. In our discussions so far in the experimental analysis, we used a workloads that had CPI values of one, six, 11 and 16. And the results showed that if we have such a hypothetical workload that has such distinct and widely spread out CPI values, the scheduler can be effective. But the question is, do realistic workloads have CPI values that exhibit anything that we used in the synthetic workload? To answer this, Fedorova profiled a number of applications from several benchmark suites. And these benchmark suites are widely recognized in industry and in academia as well that they include workloads that are representative of real world, relevant applications. And let's look at the CPI values for all of these benchmarks. We see that they're all sort of cluttered together. They are in such distinct CPI values like one, six, 11, and 16 as what she used in her experimental analysis. What this tells us is that although in theory it seems like a great idea to use cycles for instruction as a scheduling metric for hyperthread of platforms, in practice, real workloads don't have behavior that exhibit significant differences in their CPI value, and therefore CPI really won't be a useful metric. So I showed you a paper about something that doesn't work. There's still some very important takeaways from this paper. First, you learn about SMTs and some of the resource contention issues there, specifically regarding the processor pipeline as a resource. Next, you learn how to think about the use of hardware counters to establish some kind of characteristics about the workload, to understand it better so that you can better inform the operating system level resource management. In addition, you learn that it is important to design schedulers that will also think about resource contention, not just about load balancing. For instance, a scheduler should think about choosing a set of tasks that are not going to cause a resource contention with respect to the processor pipeline, or the hardware cache, or the memory controller, or some type of I/O device. So these principles generalize to other types of resources, not just to the processor pipeline in hardware multithreaded platforms. And by the way, in Fedorova's follow-on work, as well as several other efforts, it's been established that particularly important contributor to performance degradation when you're running multiple tasks on a single hardware, multithreaded or multi-core platform, is the use of the cache resource, in particular the last level cache. So what that has told us is to, for instance, keep track of how a set of threads is using the cache as a resource and pick a mix that doesn't cause contention on the last level cache usage. And this is just for your information. We're not going to look in any additional papers that really further explore this issue, not in this course at least. In summary, you should now know how scheduling works in an operating system. We discussed several scheduling algorithms and the corresponding runqueue data structures that they use. We described two of the schedulers that are default in the Linux Kernel, the Completely Fair Scheduler and its predecessor, the Linux Cell One Scheduler. And also, we discuss some of the issues that come up in scheduling when considering multiple CPU platforms. This we said includes platforms with multiple CPUs that's you memory, multi-core platforms as well as platforms with hardware level multithreading. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In this lesson, we will review the memory management mechanisms used in operating systems. We will describe how operating systems manage physical memory, and also how they provide processes with the illusion of virtual memory. Let me remind you that the intent of this course is for us to review some of the basic mechanisms in operating systems. And also to serve as a refresher for those of you who may have forgotten some of this content from your undergraduate classes. My goal is for us to review some of the key concepts, and to make it easier for you to use additional references like the books that I mentioned or some online content, in case you need to fill in any blanks Before we get started, let's look at memory management from a high level with another visual metaphor. Here is an idea. Operating systems and toy shops each have some sorts of mechanisms to manage the state that's required for their processing actions. In the case of the operating systems, this state is captured in the system memory. In the case of the toy shops, this state is captured in the various parts that are needed for the toy production process. Beginning with the toy shop, here are few elements required for its part management system. First, the process uses intelligently sized containers. Second, not all parts are needed at the same time. And finally, the process is optimized for achieving high performance. So what do we mean by intelligently sized containers? Well, for storage purposes, it's convenient to make sure that every container is of the same size so that it's easier to manage those containers in and out of the storage room. Regarding the next point, you can imagine how certain types of toys, like teddy bears, would require fabric and threads, whereas other types of toys may require wooden parts or something else. Given that toy orders are completed in stages and not every single one of the toy orders is processed at the exact same time, these parts can be brought in and out of these containers as necessary, for instance. How that's done should be optimized for performance because if we can reduce the wait time for bringing the parts in and out of the containers, ultimately we can make more toys. The memory management subsystems that are part of operating systems have some similar types of goals. In an operating system, memories typically manage the granularity of pages or segments. We will discuss this more later on, but the size of these containers can be an important design consideration. Like with the analogy of the toy orders and the parts needed for the toy orders, similarly processes that execute in a computing system don't require all of the memory at the same time. Some subsets of the state that's needed for the computation of the task can be brought in and out of memory as needed, depending on what the task is performing. And finally, how the state that's required for these tasks is brought in and out of memory and into memory pages or segments is optimized so as to reduced the time that's require to access that state. The end result again is improve performance for the system. For instance, to achieve some of the performance related optimizations, memory management subsystems rely on hardware support like, for instance, translation lookaside buffers, or TLBs. They rely on caches and also in software algorithms such as for page replacement or for memory allocation. In the introductory lecture on processes and process management, we discussed, briefly, a few basic mechanisms related to Memory Management. The goal of this lesson is to complete our discussion with a more detailed description of OS-level Memory Management components. Let's remind ourselves one of the roles of the operating system is to manage the physical resources. In this case, the physical memory (DRAM). On behalf of one or more executing processes. In order not to post any limits on the size and the layout of an address space based on the amount of physical memory or how it shared with other processes, we said that we will decouple the notion of Physical memory from the Virtual memory that used by the address space. In fact, pretty much everything uses virtual addresses, and these are translated to the actual, physical addresses where the particular state is stored. The range of the virtual addresses, from V0 to Vmax here, establishes the amount of virtual memory that's visible in the system. And this can be much larger than the actual amount of physical memory. In order to manage the physical memory, the operating system must then be able to allocate Physical memory and also arbitrate how it's being accessed. Allocation requires that the operating system incorporates certain mechanisms or an algorithm as well as data structures so that it can track how Physical memory is used and also what is free among the Physical memory. In addition is the Physical memory smaller than this Virtual memory. It is likely that some of the contents that are needed in the virtual address space are not present in Physical memory. They may be stored on some secondary storage like on disk. So, the operating system must have mechanisms to decide how to basically replace the contents that are currently in Physical memory. With needed content that's on some temporary storage. So, there is basically some dynamic component to the memory management process that determines when content should be brought in from disk and then which contents from memory should be stored on disk, depending on the kinds of processes that are running. The second task, Arbitration, requires that the operating system is quickly able to. Interpret and verify a process memory axis. That means that, when looking at a virtual address, the OS should quickly be able to translate that virtual address into a physical address. And to validate it, to verify that that is, indeed, a legal axis. For this current operating systems rely on a combination of hardware support as well as smartly designed data structures that are used in the process of address translation and validation. The figure here illustrates that the virtual address space is subdivided into fixed sized segments that are called pages. The Virtual memory not to scale these two. Is divided into page frames of the same size. In terms of allocation then the role of the operating system is to map pages from the Virtual memory into page frames of the Physical memory. In this type of Page-based memory management system the arbitration of the axis is done via page tables. Paging is not the only way to decouple the Virtual and the Physical memories. Another approach is segmentations, so that would be a Segment-based memory management approach. With segmentation, the allocation process doesn't use thick-sized pages. Instead it uses more flexibly-sized segments that can then be mapped to some regions in Physical memory as well as swapped in and out of Physical memory. Arbitration of accesses in order to either translate or validate the appropriate access uses segment registers that are typically supported on modern hardware. Paging is the dominant method used in current operating systems and we'll primarily focus our discussion on Page-based memory management. That will mention segments a little bit more later in this lesson again. As I already hinted memory management isn't purely done by the operating system alone. In order to make these tasks efficient over the last decades the hardware has evolved to integrate a number of mechanism that make it easier, faster, or more reliable to perform allocation and arbitration tasks regarding the memory management. First every CPU package is equipped with a Memory Management Unit. The CPU issues virtual addresses to the Memory Management Unit, and it's responsible for translating them into the appropriate physical address. Or potentially the MMU can generate a fault. The faults are an exception or signal that's generated by the MMU, that can indicate one of several things. For instance, it can say that the access is illegal, like for instance that the memory address that's requested hasn't been allocated at all. Or it can indicate that there inadequate permissions to perform a particular access. For instance, the memory reference may be part of a store instruction, so the process is trying to override a particular memory location. However, it doesn't have a right permissions for that particular access. That page is what we call right protected. Or another type of fault may be an indication that the particular. Page that's being referenced isn't present in memory and must be fetched from disk. Another way hardware supports memory management is by using designated registers during the address translation process. For instance in a Page-based system there are registers that are used to point to the currently active page table or in a segment based memory management the registers that are used to indicate the base address of the segment potentially its limit, so its overall size of the segment. Maybe the total number of segments and similar information. Since the memory address translation happens on pretty much every memory reference, most memory management units would integrate a small cache of valid virtual to physical address translations. This is called the translation lookaside buffer or TLB. The presence of a TLB will make the entire translation process much faster. Since if this translation is present in this cache then there's no need to perform any additional operations to access the page table or the segment and to interpret. the validity of the axis. And finally, the actual generation of the physical address from the virtual address, so the translation process, that's done by the hardware. The operating system will maintain certain data structures such as the page table, to maintain certain information that's necessary for the translation process, however, the actual translation, the hardware performs it. This also implies that the hardware will dictate what type of memory management modes are supported. Can you support paging? Can you support segmentation or both? So basically, are there any kinds of registers of this sort? It will also potentially imply what kinds of pages can there be. What is the virtual address format as well as the physical address format since the hardware needs to understand both of these? There are other aspects of memory management that are more flexible in terms of their design since they are performed in software. For instance, the actual allocation basically determining which portions of the Main Memory will be used by which process that's done by software or the replacement. Policies that determine which portions of state will be in main memory versus on disk. So we will focus our discussion on those software aspects of memory management, since that's more relevant from an operating systems course perspective. As I already hinted, memory management isn't currently done by the operating system alone, in order to make these tasks efficient, over the last decades the hardware has evolved to integrate a number of mechanisms that make it easier, faster, or more reliable to perform allocation and arbitration tasks regarding the memory management. First every CPU package is equipped with a memory management unit. This CPU is used virtual address as to the memory management unit. And it's responsible for translating them into the appropriate physical address. Or potentially the MMU can generate a fault. The faults are an exception or signal that's generated by the MMU that can indicate one of several things. For instance, it can say that the access is illegal. Like, for instance, that the memory address that's requested hasn't been allocated at all. Or it can indicate that they're inadequate permissions to perform a particular access. For instance, the memory reference may be part of a store instruction, so the process is trying to override a particular memory allocation. However, it doesn't have a write permission for that particular access. That page is what we call write protected. Or another type of fault may be an indication that the particular page that's being referenced, isn't present in memory, and must be fetched from disk. Another way hardware supports memory management is by using designated registers during the address translation process. For instance in a page based system, there are registers that are used to point to the currently active page table, or in a segment based memory management the registers that are used to indicate the base address of the segment potentially it's limit, so its overall size of the segment, maybe the total number of segments and similar information. Since the memory address translation happens on pretty much every memory reference, most memory management units would integrate a small cache of valid virtual to physical address translations. This is called the Translation Lookaside Buffer or TLB. The presence of a TLB will make the entire translation process much faster. Since, if this translation is present in this cache then, there's no need to perform any additional operations to access the page table or the segment and to interpret the validity of the access. And finally, the actual generation of the physical address from the virtual address. So the translation process, that's done by the hardware. The operating system will maintain certain data structures, such as the page tables, to maintain certain information that's necessary for the translation process. However, the actual translation, the hardware performs it. This also implies that the hardware will dictate what type of memory management modes are supported. Can you support paging? Can you support segmentation, or both? So basically, are there any kinds of registers of this sort? It will also potentially imply what kinds of pages can there be? What is the virtual address format, as well as the physical address format? Since the hardware needs to understand both of these. There are other aspects of memory management that are more flexible in terms of their design since they're performed in software. For instance, the actual allocation. Basically determining which portions of the main memory will be used by which process. That's done by software or the replacement policies that determine which portions of state will be in main memory versus on disk. So we will focus our discussion on those software aspects of memory management, since that's more relevant from an operating systems course perspective. As we said, pages are the more popular method for memory management. Now, let's take a look at one of the major components that enables page based memory management, and that's page tables. As the component that's used to translate the virtual memory addresses into physical memory addresses. So, here's a page table. For each virtual address, and entry in the page table is used to determine the actual physical location that corresponds to that virtual address. So in this way, the page table is like a map that tells the operating system and the hardware itself where to find specific virtual memory references. All of the sizes in this drawing are a little bit off. The sizes of the pages of the virtual memor,y and the corresponding page frames in physical memory are identical. By keeping the size of these two the same, we don't have to keep track of the translation of every single individual virtual address. Instead, we can only translate the first virtual address in a page to the first virtual address in a page frame in physical memory. And then the remaining addresses in the virtual memory page will map to the corresponding offsets in the physical memory page frame. As a result, we can reduce the number of entries we have to maintain in the page table. What that means is that only the first portion of the virtual address is used to index into the page table. We call this part of the virtual address the virtual page number, and the rest of the virtual address is the actual offset. The virtual page number is used as an offset into the page table. And that will produce the physical frame number. And that is the physical address off the physical frame in DRAM. Now, to complete the full translation of the virtual address. That physical frame number needs to be sent with the offset that's specified in the later part of the physical address to produce the actual virtual address. That resulting physical address can ultimately be used to reference the appropriate location in physical memory. Let's look at an example now. Let's say we want to access some data structure, some array, for instance, to initialize it for the very first time. However, we have already allocated the memory for that array into the virtual address space of the process, we've just never accessed it before. So since this portion of the address base has not been accessed before, the OS has not yet allocated memory for it. What will happen the first time we access this memory is that the operating system will realize that there isn't physical memory that corresponds to this range of virtual memory addresses. So it will take a page of physical memory, P2 in this case, a page that is free, obviously. And it will establish a mapping between this virtual address, so this is the V-K and the offset address, where the array is placed in virtual memory, and the physical address of page 2 in physical memory. Note that I said, that the physical memory for this array is only allocated when the process is first trying to access it, during this initialization routine. We refer to this as allocation on first touch. The reason for this is that we want to make sure that physical memory is allocated only when it is really needed, because sometimes programmer may create data structures that they don't really use. If a process hasn't used some of its memory pages for a long time, and it's likely that those pages will be reclaimed. So the contents will no longer be present in physical memory. They will be reclaimed, they will be pushed on desks and probably some other content will find its way into the physical memory. In order to detect this, page table entries don't just consist of the physical frame number. Instead they also have a number of bits that tell the memory management system something about the validity of the access. For instance, if the page is in memory and the mapping is valid, then this bit is one. If the page is not in memory then this bit is zero, and if the hardware MMU see's that this is a bit zero in the page table entry it will raise a fault. It will trap to the operating system. If the hardware determines that the mapping is invalid and false, then control gets passed to the operating system. The OS at that point gets to decide a number of questions. Should the access be permitted. Where exactly is the page located. Where should it be brought into DRAM. So long as a valid address is being accessed. Ultimately in fault, there will be a mapping that will be re-established between a valid virtual address and the valid location in physical memory. It is likely however, if the page was pushed in disk and now it's being brought back into memory, that it will be placed in a completely different memory location. So for instance, here. This page is now placed in P3, and it use to be in P2, as a result clearly the entry in the page table needs to be correctly updated. So as a final note to summarize, the operating system creates a page table for every process that it runs. As a summary, the operating system will maintain a page table on every single process that exists. That means that whenever a context switch is performed, the operating system has to make sure that it switches to the page table of the newly context switch process. We said that hardware assist with page table accesses by maintaining a register that points to the active page table. On X86 Platforms there's a register CR3. And so basically, on a context which we will have to change the contents of the CR3 register with the address of the new page table. We see that every page table entry will have the physical page frame number and that it will also have at least a valid bit. This is also called the present bit since it indicates whether the contents of the virtual memory are actually present in physical memory or not. There are a number of other fields that are part of each page table entry that the operating system uses during memory management operations, and also that the hardware understands and knows how to interpret. For instance, most hardware supports a dirty bit, which gets set whenever a page is written to. This is useful, for instance, in file systems, where files are cached in memory. And then we can detect using this dirty bit which files have been written to and need to be updated on disk. Also useful is to keep track of an accessed bit. This can keep track of in general whether the page has been accessed, period, for read or for write. Other useful information that can be maintained as part of the page table entry also would include certain protection bits. Whether a page can be only read or also written to, or maybe some other operation is permissible. So that was a generic discussion of a page table entry. Here is the specifics of the Pentium x86 page table entry. The flags present, dirty, and accessed, have identical meaning as in the generic page table entry we just discussed. The bit read/write, it's a single bit that indicates permission. If its value's 0, that means that, that particular page can be accessed for read only, whereas if it's 1, that means that both read and write accesses are permissible. U/S is another type of permission bit which indicates whether the page can be accessed from user mode or only from supervisor mode, from when you're in the kernel, basically. Some of the other flags here dictate some things regarding the behavior of the caching subsystem that the hardware has. So, for instance, you can specify things like whether or not caching is disabled. That's an operation that's supported on modern architectures. And also there are some parts and bits in the page table entry that are unused, and hopefully in the future we'll have good uses for these bits as well. The MMU uses the page table entries not just to perform the address translation, but also relies on these bits to establish the validity of the access. If the hardware determines that a physical memory access cannot be performed, it causes a page fault. If this happens, then the CPU will place an error code on the stack of the kernel, and then it will generate a trap into the OS kernel. That will in turn generate a page fault handler. And the page fault handler, it will determine what is the action that needs to be taken depending on the error code as well as the address that caused the fault. Key pieces of information in this error code will include whether or not the fault was caused because the page was not present, it needs to be brought in from disk. Or because there is some sort of permission protection that was violated, and that's why the page access is forbidden. On x86 platforms, the error code information is generated from some of the flags in the page table entry. And the faulting address that's also needed during the page fault handler, that one is stored in a register, CR2. To calculate the size of the page table, we know that a page table has number of entries that is equal to the number of virtual page numbers that exist in a virtual address space. For every one of these entries, we know that the page table needs to hold the physical frame number, as well as some other information like the permission bits. Here is something that would make sense on a 32-bit architecture. So, each of the page table entries is 4 bytes and that includes the page frame number as well as the flags. The total number of page table entries, that will depend on the total number of VPNs. And how many VPNs we can have, that's going to depend on the size of the addresses, of the virtual addresses, and of the page size itself. So let's say in this example, we have a 32-bit, both physical memory as well as 32-bit virtual addresses. So that will be 2 to the 32nd and that will have to be divided by the actual page size. Different hardware platforms support different page sizes, but let's say we pick a common 4 kilobyte page size for this example. In that case, if you do the math, you will see that the page table will be 4 megabytes and it will be 4 megabytes for every single process. With many active processes in an operating system today, this can get to be quite large. If we try to work through the same kind of example for a 64-bit architecture that, say, has a page table entry size of 8 bytes, and let's say we use also the same 4 kilobyte page size, we come up with a really scary number of 3 petabytes per process. So where does one store all of this? Before we answer that question, it's important to know that a process likely will not use all of the theoretical available virtual memory. Even on 32-bit architecture, it's not all of the 4 gigabytes of virtual address space is used by every single type of process. The problem is that the page table as we described it so far, it assumes that there is an entry for every single VPN. And that is regardless of whether the corresponding virtual memory region is needed by the process or not. So this page table design really explodes the requirements of the page table size. And what we'll do next, we'll look at some alternatives of how to represent a page table. The answer to our storage issues relies on the fact that we don't really design page tables in this flat manner anymore. Instead, page tables have evolved from a flat page map to a more hierarchical multi-level structure. This figure here shows a two level page table. The outer level here is referred to as a page table directory. Its elements are not pointers to actual pages, as in here. Instead, they're pointers to page tables. The internal page table has proper page tables as its components that actually point to page tables. Their entries have the page frame number and all the protection that's for the physical addresses that are referenced by the corresponding virtual address. An important thing to note is that the internal page tables exist only for those virtual memory regions that are actually valid. So any kinds of holes in the virtual memory space will result in lack of internal page tables, so for those holes, there won't be internal page tables allocated for them. If a process requests via malloc additional virtual memory to be allocated to it, the OS will check and if necessary it will allocate an additional internal page table element and set the appropriate page table directory to correspond to that entry. That new internal page table entry will correspond to some portion of the newly allocated virtual memory region that the process has requested. To find the right element in this page table structure, the virtual address is split into yet another component. Using this address format, this is what we need to perform to determine the correct physical address. First the last portion of the address is still the offset so that's going to be used to compute the offset within the actual page, within the actual physical page. The first two components of the address are used as indices into the page tables, into the different levels of the page table hierarchy. And they're ultimately going to produce the physical frame number that's the starting address of the physical region. The first portion is used as in index into the outer page table. So, that will determine the page table directory entry that points to the actual page table. And then the second index is used as an index into this page table, into the internal page table. This will produce the page table entry that consists of the physical frame number and then we add that with the offset just like before, and compute the physical address. In this page the address format is such that it uses ten bits for the internal page table offset. That means that this internal page table can address two to the tenth elements. So two to the tenth pages can be addressed in the internal page table. Since we used ten bits as the offset into the actual page that means that the page itself is also two to the tenth in size. Therefore, if we do the math, we see that every single internal page table can address two to the tenth, the number of entries, times the page size, that's another two to the tenth, so one megabyte of memory. What that means is that whenever there is a gap in the virtual memory that's one megabyte size. We don't need to allocate that internal page table, so that will reduce the overall size of the page table that's required for a particular process. This is in contrast with the single leve page table design where the page table has to be able to translate every single virtual address and it has entries for every single virtual page number. So clearly the hierarchical page table model helps in reducing the space requirements for a page table. The skin can be further extended to use additional layers using the same principle. For instance, we can add another, third level that can consist of pointers to page table directories. Adding yet another fourth level to this, which consists of a map of pointers to page table directories. This technique is particularly important on 64 bit architectures. There, not only that the page table requirements are larger, it's also the fact, is that the virtual address spaces of processes on these 64 bit architectures tend to be more sparse. If it's more sparse, that means that it will have larger gaps in the virtual address space region. And the larger the gaps, the larger the number of internal page table components that won't be necessary as a result of that gap. In fact, with a four level addressing, we may end up saving entire page table directories as a result of certain gaps in the virtual address space. Let's look at a quick example. These two figures show how a 64-bit virtual address can be interpreted to determine which indices are used into the different levels of the page table hierarchy. The top figure has two page table layers, whereas the bottom one has three page table layers. In both of these figures, the offset field is the actual intext into the actual physical page table. There is a trade-off in supporting multiple levels in the page table hierarchy. As we add multiple levels, the internal page tables and page table directories end up covering smaller regions of the virtual address space. As a result, it is more likely that the virtual address space will have gaps that will match that granularity and we will be able to reduce the size of the page table. The down side of adding multiple levels in the page table is that there will be more memory accesses that will be required for translation since we'll have to access each of the page table components before we ultimately produce the physical address. Therefore, the translation latency will be increased. Let's check your understanding of how multi level page tables work using a simple quiz. A process which uses 12bit addresses, has an address base for only the first two kilobytes, and the last one kilobyte are allocated and used. How many total page table entries are there in a single level page table that uses the first address format? As a second question, how many entries are needed for the inner page tables of the 2-level page table that uses the second address format? Write your answers in the boxes here. In both formats, the page offset is six bits. That means that each of the pages is 2 to the 6, that's 64 bytes. In the first address format in the case of the single-level page table, six bits are used for the virtual page number. That means that there will be a total of 2 to the 6, so 64 different virtual pages and in a single-level page table design, we have to have an entry for every single virtual page number, so there will be a total of 64 elements. In the second address format, the first two bits are used as an index into the outer page table, so the page table directory, and the inner four bits are used as an index into the inner page tables. But, take a look at this address format, these two bits, so the outer page table entries, will address 2 to the 10, 4 plus 6 virtual addresses from the virtual address space. That means that every single element of the outer page table can be used to hold the translations for one kilobyte of the virtual addresses. Given that the process is such that only the first two and the last one kilobyte of the virtual address space are allocated. That means that one of the entries in the outer page table will not really need to be populated with a corresponding inner page table. So, we can save the memory that's required for that inner page table. Now, the inner page table is the reuse of four bit index to index into the inner page table, that means that, that will have 16 entries, every single one of the inner page tables will hold 16 entries. So therefore, the total number of entries that are needed across the remaining inner page tables will be 48. So, we reduce the page table size by 25% by choosing this multi-level page table format in this particular example Now we know that adding levels to our address translation process will reduce the size of the page tables. But it will add some overheads to the address translation process. In the simple, single level page table design, a memory reference will actually require two memory references. One to access the page table entries so that we can determine the physical frame number, and the second one to actually perform the proper memory access at the correct physical address. In the four level page table, however, we will need to perform four memory accesses to read the page table entries at each level of the memory hierarchy, before we can produce the physical frame number. And only afterwards are we able to actually perform the proper access to the correct physical memory location. Obviously this can be very costly and can lead to a slowdown. The standard technique to avoid these repeated accesses to memory is to use a page table cache. On most architectures, the MMU hardware integrates a hardware cache that's dedicated for caching address translations, and this cache is called the Translation Look Aside Buffer or TLB. On each address translation first the TLB cache is quickly referenced and if the resulting address can be generated from the TLB contents then we have a TLB hit and we can bypass all of the other required memory accesses to perform the translation. Of course, if we have a TLB miss, so the address isn't present in the TLB cache, then we have to perform all of the address translation steps by accessing the page tables from memory. In addition to the proper address translation, the TLB entries will contain all of the necessary protection and validity bits to verify that the access is correct or, if necessary, to generate a fault. It turns out that even a small number of entries in the TLB can result in a high TLB rate and this is because we have typically a high temporal and spatial locality in the memory references. On recent x86 platforms, for instance, there is a separate TLB for data and instruction. And each of those has a modest number of entries. 64 for the data and 128 for the instruction TLB. These are per core and in addition to these two, there is also another shared second level TLB that's shared across all cores and that's, that one is a little bit larger. It has 512 entries. So this is for the I7 in platforms. And this was determined to be sufficiently effective to address the typical memory access needs of processes today. A completely different way to organize the address translation process is to create so-called inverted page tables. Here, the page table entries contain information, one for each element of the physical memory. So, for instance, if we're thinking about physical frame numbers, each of the page table elements will correspond to one physical frame number. Today on the most high-end platforms, we have physical memory that's on the order of tens of terabytes, whereas the virtual memory of an address space can reach petabytes and beyond. Clearly, it would be much more efficient to have a page table structure that's on the order of the available physical memory versus something that's on the order of the virtual memory that a process can have. To find the translation, the page table is searched base on the process ID and the first part of the virtual address, similar to what we saw before. When the appropriate pid and p entry is found into this page table. The index, the element where this information is stored, that will denote the physical frame number of the memory location that's indexed by this logical address. So then, that is combined with the actual offset to produce the physical address that's being co-reference from the CPU. The problem with inverted page tables is that we have to perform a linear search of the page table to see which one of its entry matches the pidp information that's part of the logical address that was presented by the CPU. Since the physical memory can be arbitrarily assigned to different processes, the table isn't really ordered. There may be two consecutive entries that represent memory allocated to two different processes, and there really isn't some clever search technique to speed up this process. In practice, the TLB will catch a lot of these memory references so this detailed search is not performed very frequently. However, we still have to perform it periodically, so we have to do something to make it a little bit more efficient. To address this issue, inverted page tables are supplemented with so-called Hashing Page Tables. In most general terms, a hashing page table looks something as follows. A hash is computed on a part of the address and that is an entry into the hash table that points to a linked list of possible matches for this part of the address. So, that allows us to basically speed up the process of the linear search to narrow it down to few possible entries into the inverted page table, as a result, we speed up the address translation. In addition to paging, we said that virtual to physical memory mappings can be performed using segments. So the process is referred to as segmentation. With segments the address space is divided into components of arbitrary granularity, of arbitrary size, and typically the different segments will correspond to some logically meaningful components of the address space, like the code, the heap data, etc. A virtual address in the segmented memory mode includes a segment descriptor, and an actual offset. The segment descriptor is used in combination with a descriptor table, to produce information regarding the physical address of the segment and the two are combined. That information along with the offset, they're combined to produce the linear address of the memory reference. In its pure form, a segment could be represented with a contiguous portion of physical memory. In that case, the segment would be defined by its base address and its limit registers, which implies also the segment size. So we basically can have segments with different size using this method. In practice segmentation and paging are used together. What this means is that the address that's produced using this process and that one we call the linear address, is then passed to the paging unit so it will be passed to a multilevel, hierarchical page table. To ultimately compute the actual physical address that is used to reference the appropriate memory location. The type of address translation that's possible on a particular platform, that's determined by the hardware. For instance, if you look at the Intel platforms, the x86 platforms, on 32 bit hardware both segmentation and paging are supported. For these platforms on Linux allows up to 8000 segments to be available per process and then another 8000 global segments. At the same time, on 64-bit Intel platforms, segmentation and paging are supported for backward compatibility, however the default mode is to use just paging. So far we glossed over any discussion of what is the appropriate page size or how large is a page. In the examples that we showed so far, regarding the address formats, we use 10-bit for the offset or 12-bit for the offset. Well, this offset determined what is the total amount of addresses in the page. And therefore, it determined the page size that we were discussing in those examples. So in the examples in which we had a 10-bit offset in the address field, that meant that these 10 bits could be used to address 2 to the 10th bytes in the page. And therefore it meant that the page size is 1 kilobyte. Similarly, the examples that had a 12-bit offset for the address format. That means that they could have addressed 4 kilobyte pages, 2 to the 12th. But what are the page sizes in real systems? These are some examples that we cooked up. In practice, systems support different page sizes. For Linux and x86 platform, there's several common page sizes. 4 kilobyte page size is pretty popular, and that's the default in the Linux x86 environment. However, page sizes can be much larger, 2 megabytes, 1 gigabyte. The 2 megabyte pages are referred to as large pages, as opposed to the regular 4 kilobyte ones. In addition, x86 supports huge pages and these are 1 gigabyte in size. In the first case, to address 2 megabyte of content in a page, we need 21 bit for the page offset. And in the case of a huge page, we need 30 bits as an offset to compute that physical address. So one benefit of using these larger page sizes is that more bits in the address are used for these offset bits. And therefore fewer bids are used to represent the virtual page number, so there will be fewer entries that are needed in the page table. In fact, use of these large page sizes will significantly reduce the size of the page table. Compared to the page table size that's needed when we're working with 4 kilobyte pages. Large pages will reduce the page table size by a factor of 512, and then switching to huge page sizes will reduce the page table size by another 512. So in summary, the benefits of larger page sizes are the fact that they require smaller page tables, due to the fact that there are few page table entries that are needed. And we can have additional benefits, often such as, for instance, increased number of TLB hits, just because we'll be able to translate more of the physical memory using the TLB cache. The down side of the larger pages is the actual page size. If this large virtual memory page is not densely populated, there will be a larger unused gaps within the page itself, and that will lead to, to what we call internal fragmentation. There will be basically wasted memory in these allocated regions of memory, depending on the page size. So because of this issue, smaller page sizes of 4 kilobytes are commonly used. There are some settings like databases or in memory data stores, where these large or huge page sizes are absolutely necessary and make most sense. I should note that on different systems, depending on the operating system and the hardware architecture, different page sizes may be supported. So, for instance, on Solaris 10 on SPARC architecture, the page size options are 8 kilobytes, 4 megabytes, or 2 gigabytes. Our first quiz about page tables looked at the address formats. And in the second quiz we will look at page tables again, but by looking at the page sizes. On a 12-bit architecture, what is the number of entries that is required in the page table, if the page size is 32 bytes? Also, think about what is the answer to this question, if the page size is 512 bytes. You should assume that the page table is a single-level page table. Write your answers in the text boxes. If the architecture is 12-bit, that means that the addresses are 12-bit long. If the page size is 32 bytes, then we need 5 bits for the offset into that page. That will leave 7 bits for the virtual page number, and therefore, we will need 128 to do the seven total number of entries in the page table. Using the same logic for the 512 byte pages, we will need 9 bits out of the total 12 bits for the offset into the page. And therefore, we will be left with 3 bits for the virtual page number. As a result, the page table will need to have entries for all of the 2 to the 3rd number of virtual pages. So it will have total of 8 entries. As you can see, this example illustrates the impact of using a larger page sizes on the requirements of the page table size. So far, we have described how the operating system controls the processes' access to physical memory. But what we didn't explain was how the operating system decides how to allocate a particular portion of the memory to a process in the first place. This is the job of the memory allocation mechanisms that are part of the memory management subsystem of an operating system. Memory allocation incorporates mechanisms that decide what are the physical pages that will be allocated to particular virtual memory regions. So what are the physical addresses that will correspond to a specific virtual address. Once the memory allocator establishes a mapping, the mechanisms that we discussed so far, like the address translation, use of page tables, et cetera. They're simply used to determine a physical address from a virtual address that the process presents to the CPU. And also to perform all necessary checks regarding the validity of the access or the access permissions. Memory allocators can exist at the kernel level, as well as the user level. Kernel-level allocators are responsible for allocating memory regions, such as pages for the kernel, so for various components of the kernel state. And also these are used for certain static states, for the processes when they're created, like for their codes, stack, or initialized datum. In addition, the kernel-level allocators are responsible for keeping track of the free memory that's available in the system. The user-level allocators are used for dynamic process state, for instance, for the heap. So this is state that's dynamically allocated during the process execution. The basic interface for these allocators includes malloc and free. What these calls do is that they request from the kernel some amount of memory from its free pages, and then ultimately release it when they're done. Once the kernel allocates some memory to a malloc call, the kernel is no longer involved in the management of that space. That memory will at that point be used by whatever user-level allocator is being used, and there are a number of options out there right now. That have certain different trade-offs in terms of their cache efficiency or friendliness with respect to how they behaved in a multithreaded environment or other aspects. We will not discuss the internals of these user-level allocators in this course. Instead, we will briefly describe some of the basic mechanisms that are used in the kernel-level allocators. And the same kinds of design principles are used in the design of some of the user-level allocators that are out there today. Before we talk about the kernel-level allocators, I want to describe a particular memory allocation challenge that needs to be addressed. Consider a page-based memory manager that needs to manage these 16 physical page frames. Let's say this memory manager takes requests of sizes two or four page frames, and let's say it's facing the following sequence of memory requests. The first memory allocation is for the request of two page frames and then the rest of the requests are for four pages. So let's say the memory allocator allocates these requests in order, and the end result of this will be that this will be the memory allocation, how the physical memory is used to satisfy these requests, and their two free page frames. Let's say next the two pages that were initially allocated or freed. So now you likely can already imagine what the problem is. If at this point a next request comes to allocate four pages, there are four free pages in the system. However, this particular allocator cannot satisfy this request since these pages are not contiguous. Let's say the requirement with these allocation requests was for these memory pages to be contiguous. So in that case, this allocator cannot meet this requirement. This example illustrates a problem that's called external fragmentation. This occurs where we have multiple interleaved allocate and free operations, and as a result of them, we have basically holes of free memory that's not contiguous. And therefore, requests for largest contiguous memory allocations cannot be satisfied. In the previous example, the allocator had a policy in which the free memory was handed out to consecutive requests in a sort of first come, first served manner. Let's consider an alternative in which the allocator probably knows something about the requests that are coming. It knows that they will be coming for consecutive regions of two and four page frames. In the second case when the second request for an, allocating four pages comes, the memory allocator isn't allocating it immediately after the first allocation but instead is leaving some gap. The second allocation for four pages comes in at a granularity of four pages, and then the rest of the allocations are satisfied further below. Now when the free request comes in, these two first pages are freed. The system again has four free pages. However, they're consecutive. Therefore, this next request for four pages can actually be satisfied in the system. What we see in this example is that when these pages are freed, there was an opportunity for the allocator to coalesce, to aggregate these adjacent areas of free pages into one larger free area. That way, it was more likely for the allocator to satisfy these future larger requests. This example illustrates some of the issues that an allocation algorithm needs to be concerned with to avoid or to limit the extent of fragmentation and to allow for quick coalescing and aggregation of freed areas. To address the free space fragmentation and aggregation issues we mentioned in the previous morsel, the Linux kernel relies on two basic allocation mechanisms. The first one is called the buddy allocator and the second one is called the slab allocator. The buddy allocator starts with some consecutive memory region that's free that's of a size that's a power of two. Whenever a request comes in, the allocator will subdivide this large area into smaller chunks such that every one of them is also a power of two. And it will continue subdividing until it finds the smallest chunk that's of a size that's a power of two that can satisfy the request. For instance, in this figure, when the first request of eight pages came in, the buddy allocator subdivided the region that was 64, the original area, first into two chunks of 32 pages. Then it subdivided one of these 32 page chunks into chunks that were 16 pages in each. Then it subdivided this 16 page chunk into chunks that were eight pages each. And it turned out that this eight page chunk satisfied the request that was for eight pages. So that was great. Subsequently, another request for eight pages came in, and then a request for four pages came in, and for that reason, this chunk of eight pages was subdivided into two chunks of four. Now when this eight page region is freed, there will be some fragmentation here. However, when the next eight page region is freed, the algorithm will quickly be able to combine these two to produce a 16 page free space. So fragmentation still exists in the buddy allocator, but its benefits are that when a request is freed, it has a way to quickly find out how to aggregate data. When this allocation of eight pages was freed, with the buddy allocator, it was very easy to figure out what is the start of the adjacent allocation. Where does the buddy of this eight page region start? If we didn't have this information, if we didn't know that the adjacent region is also an eight page region, we would have had to potentially scan all of these pages to determine which one is free and which one isn't. So as to figure out whether we can increase this free space to nine, ten, 11, 12, or some other number of pages. So the benefit of the buddy algorithm is that the aggregation of the free areas can be performed really well and really fast. The checking of what are the free areas in the system can further be propagated up the tree to check the buddies of this 16 page free area, and then the buddy of the 32 page free area, and so forth. The reason why these areas are the power of two is so that the addresses of each of the buddies differ only by 1 bit. This makes it easier to perform the necessary checks when combining or splitting chunks. Define that allocations using the buddy algorithm have to be made in a granularity of a power of two, means that there will be some internal fragmentation using the buddy allocator. This is particularly a problem because there are a lot of data structures that are common in the Linux kernel that are not of a size that's close to a power of two. For instance, the task data structure, task_struct, is 1.7k. To fix this issue, Linux also uses the slab allocator in the kernel. The allocator builds custom object caches on top of slabs. The slabs themselves represent contiguously allocated physical memory. When the kernel starts, it will pre-create caches for the different object types. For instance, it will have a cache for a task_struct or for the directory entry objects. Therein, when an allocation comes from a particular object type, then it will go straight to the cache and it will use one of the elements in this cache. If none of the entries is available, then the kernel will create another slab and it will preallocate an additional portion of contiguous physical memory to be managed by this slab allocator. The benefit of this slab allocator is that it avoids internal fragmentation. These entities that are allocated in the slab, they're of the exact same size as the common kernel objects. Also, external fragmentation is not really an issue. Even if we free objects in this object cache, future requests will be of a matching size and then they can be made to fit in these gaps. So the combination of the slab allocator and the buddy allocator that are used in the Linux kernel, these are really effective methods to deal with both the fragmentation and also the free memory management challenges that are present regarding memory management in operating systems. Since the physical memory is much smaller than the addressable virtual memory, allocated pages don't always have to be present in physical memory, in theorem. Instead, the backing physical page frame can be repeatedly saved and restored to and from some secondary storage, like disks, for instance. And this process is referred to as paging or demand paging, and traditionally with demand paging, pages are moved between main memory and a storage device such as disk, where a swap partition resides. In addition to disk, the swap partition can be on another type of storage medium like a flash device, or it could even sit in the memory of another node. Let's see how paging works. When a page is not present in memory, it has its present bit in the page table entry that's set to zero. When there is a reference to that page, then the memory management unit will raise an exception, and that will cause a trap into the operating system kernel. On an access, the memory management unit will raise an exception, that's called the page fault, and this will be pushed into the operating system. So it will trap into the operating system kernel. At that point, the OS kernel can determine that this exception is a page fault. It can determine that it had previously swapped out this memory page onto disk. It can establish what is the correct disk access that needs to be performed. And it will issue an I operation to retrieve this page. Once this page is brought into memory, the OS will determine a free frame where this page can be placed. And it can use the page frame number for that page to appropriately update the page table entry that corresponds to the virtual address of that page. At that point, control is pushed back into the process that caused this reference, and the program counter will be restarted with the same instructions, so that this reference will now be made again. Except at this point, the page table will find a valid entry with a reference to this particular physical location. Note the the original physical address of this page will very likely be different from its physical address that was established after this demand paging process was over. If, for whatever reason, we require a page to be constantly present in memory, or if we require it to maintain the same physical address during its lifetime, then we will have to pin the page, and at that point we basically disable the swapping. This is, for instance, useful when the CPU is interacting with devices that support direct memory access support, or DMA. Moving pages between physical memory and secondary storage raises some obvious questions. When should pages be swapped out of physical memory and onto disk? And also, which particular pages should be swapped out? The first part is easier. Periodically when the amount of occupied memory reaches a particular threshold the operating system will run some page out daemon that will look for pages that can be freed. So something that would make sense as an answer to this question would be that the pages should be swapped out when the memory usage in the system reaches some level, some high water mark. And that this paging out should be performed also when the CPU usage is below a certain threshold so as not to disrupt the execution of some applications too much. To answer the second question, one obvious answer would be that the pages that will not be used in the future are the ones that should be swapped out. The problem is, how do we know which pages will versus won't be used in the future? To make some predictions regarding the page usage, operating systems use some historic information. For instance, one common set of algorithms is to look at how recently or how frequently has a page been used, and use that to inform a prediction regarding the page's future use. The intuition here is that a page that has been used most recently is more likely to be needed in the immediate future, whereas a page that hasn't been accessed in a very long time is less likely to be needed. This policy is referred to as the LRU policy, least recently used, and it uses the access bit that's available on modern hardware to keep track of the information whether or not the page is referenced or not. Other useful candidates for pages that should be freed up from physical memory are the pages that don't need to be written out to disk, to secondary storage. And that is because the process of writing pages out to the secondary storage takes some time, consumes cycles, so we'd like to avoid the overhead of the memory management. To assist with making this decision which pages don't need to be written out, the operating system can rely on the dirty bit that's maintained by the MMU hardware that keeps track of which particular page has been modified. So not just accessed and referenced however, modified during a particular period of time. In addition there may be certain pages, particularly certain pages containing important kernel state or used for I operations that should never be swapped out. Then making sure that these pages are not considered by whatever replacement algorithms are executed in the operating system is going to be important. In Linux and most OS's, a number of parameters are available to allow the system administrator to configure the swapping nature of the system. This would include thresholds such as the ones that we mentioned earlier that control when our page is swapped out, but also other parameters such as how many pages should be replaced during a period of time. Also Linux categorizes the pages into different types and then that helps narrow down the decision process when it's trying to decide which pages should be replaced. Finally, the default replacement algorithm in Linux is a variation of the LRU policy we described, and it gives a second chance. It basically performs two scans of a set of pages before determining which ones are really the ones that should be swapped out and reclaimed. And similar types of decisions can be made in other operating systems as well. We briefly discussed the Least Recently Used policy that is often used for determining which pages to swap in and out of physical memory. Consider the following problem. Suppose you have an array with 11 page-sized entries, and that all of these entries are accessed continuously in a loop, one after another. So they're accessed one by one. Also suppose that you have a system that has ten pages of physical memory. For the following system, answer this question. What is the percentage of pages that will need to be demand paged using the LRU policy? You should round up your answer to the nearest percent. In this example, initially the first ten pages will be loaded into memory one at a time, as they're being accessed one-by-one. First page 9, then page 2, then page 10. Now at this point, page 11 needs to be accessed. And that will mean that the first page, which is the one that's least recently used, needs to be swapped out of memory, given that the physical memory only has 10 pages. Now the really unfortunate thing is that just as we swapped page 1 out of memory, given that the pages are accessed one-by-one in a loop, that exact same page, page 1, is the very next page that's needed. We will have to demand page that in. And given that our physical memory has 10 pages, we need to pick out another page to swap out to replace. And that's going to be page 2, given that that's the least recently used page right now. And guess what, the next page that will be needed will be exactly page 2 that we just swapped out. So the process will continue for all of the remaining pages during the execution of the program. And therefore, the nearest percentage of the number of pages that need to be demand paged using the LRU policy is 100. This is clearly a very pathological scenario. But what it's trying to demonstrate is that an intuitive policy such as LRU can result in really poor behavior under certain conditions. For that reasons, operating systems can be configured to support different kinds of replacement policies that are used to manage their physical memory. In our discussion about memory management so far, we saw that our operating systems rely on the hardware, on the memory management unit hardware, to perform address translations and to also validate the accesses to enforce protection in similar mechanisms. But the same hardware can also be used to build a number of other useful services and optimizations, beyond just the address translation. One such mechanism is called Copy-on-Write, or COW. Let's consider what happens during process creation. When we need to create a new process we need to recreate the entire parent process by copying its entire address space. However, many of the pages are static, they don't change. So it's not clear why we should keep multiple copies. In order to avoid unnecessary copying, on creation the virtual address space of the new process or portions of it at least, will point, will be mapped to the original page that had the original address space content. The same physical address of the physical memory may be referred to by two completely different virtual addresses from the two processes. We also have to make sure to write protect the physical memory so that we can track concurrent accesses to it. If the contents of this page are indeed going to be read only, then we're going to save both on memory requirements, as well as on the time that would have otherwise been necessary to perform the copy. However, if a write request is issued for this memory area via either one of these virtual addresses, then the MMU will detect that the page is write protected and will generate a page fault. At that point the operating system will see what is the reason for this page fault. We'll create the actual copy. So the copy will only be performed then. We'll update the page tables of the two processes as necessary. So basically the page table of the faulting process. And will in this manner, copy only those pages that need to be updated. Only those pages for which the copy cost is necessary. We call this mechanism Copy-on-Write because the copy cost will only be paid when we need to perform a write operation. There may be other references to this write protected feature so whether or not the write protection will be removed once this one copy is performed will depend on who else is this page is shared with. Another useful operating system service, that can benefit from the hardware support from memory management is checkpointing. Checkpointing is a technique that's used as part of the failure and recovery management that operating systems or systems software, in general, supports. The idea behind checkpointing is to periodically save the entire process state. The failure may be unavoidable however with checkpointing, the process doesn't have to be restarted from the beginning. It can be restarted from the nearest checkpoint point. And so the recovery will be much faster. A simple approach to checkpointing would be to pause the execution of the process and copy its entire state. A better approach will take advantage of the hardware support for memory management and will try to optimize the disruption the checkpointing will cause on the execution of the process. Using the hardware support, we can write protect the entire address space of the process and try to copy everything at once. However, since the process will continue executing, we won't pause it. It will continue dirtying pages. So, then we can track the dirtied pages, again using the hardware MMU support, and we will copy only the diffs, only those pages that have been modified. That will allow us to provide for an incremental check point. If we check point using these partial diffs of just dirtied pages, we will somewhat make the recovery process more complex since we will have to rebuild the image of the process using multiple such diffs, potentially. Or also, in the background, these diffs can be aggregated to produce more complete checkpoints of the process. The basic mechanisms used in checkpointing can also be used in other services. For instance, debugging relies often on a technique called Rewind-Replay. Here rewind means that we will restart the execution of the same process from some earlier point. So we will restart it from a checkpoint, and then we will move forward and see whether we can establish what is the error, what is the bug in our program. We can gradually go back to older and older checkpoints until we find the error. Migration is another service that can benefit from similar kinds of memory management mechanisms that we described are useful for checkpointing. With migration, it's like we checkpoint the process to another machine and then we restart it on that other machine. It will continue its execution on the other location. This is useful in scenarios such as disaster recoveries, so as to continue the process on another machine that will not crash. Or, in consolidation that is common in today's data centers, when we try to migrate processes and migrate load onto as few machines as possible so that we can save on power and energy or utilize resources better. One way in which migration can be implemented is as if we are performing repeated checkpoints in a fast loop until ultimately, there is so few dirtied state from the process that something like the pause and copy approach becomes acceptable. Or maybe at that point, simply we really don't have another choice. The process keeps dirtying enough pages that we have to stop it in order to copy the remaining contents. And to wrap up our discussion about memory management, let's take a quiz about checkpointing. Which one of these endings correctly completes the following statement? The more frequently you checkpoint, the more state you will checkpoint, the higher the overheads of the checkpointing process, the faster you will be able to recover from a fault, or all of the above. The correct answer is all of the above. The more frequently you checkpoint, the faster you will be able to recover from a fault. This is true because, with a frequent checkpoint you will have a recent checkpoint compared to the point of execution when the fault occurred. So you will have to replay or re-execute a less amount of time of the execution of the process. Clearly the more frequently you checkpoint, the higher the overheads of that will be. And furthermore, with frequent checkpoint, it's more likely that you will end up catching every single write to your particular page. If you spread out the checkpoints, it's possible that a single page will be written multiple times, so dirtied multiple times. And also, the more frequently you checkpoint, you will end up transmitting more state, checkpointing more state. And the reason for this is that with a frequent checkpoint, it's more likely that you will end up catching every single one of the references of the write updates to a particular page. If you spread out the checkpoints over time, it's possible that there will be repeated writes to a particular page that you will observe as a single dirty page and so you will amortize the checkpoint costs over multiple writes. With a frequent checkpoint, both the amount of the state that will be checkpoint, and in general the overheads of the process will be higher than if you do the checkpoint less recently. So this is just one of those tradeoffs where you end up gaining something, but that's going to cost you something else. To summarize, in this lesson we look at virtual and physical memory management mechanisms in operating systems. You should now understand what are the data structures, the mechanisms, and the hardware level support. That the operating system relies on when it tries to map the process' address space that uses virtual memory onto the underlying physical memory. We talked about pages and segmentation, address translation, page allocation, page replacement algorithms. We also looked at how these memory management mechanisms, that are part of the operating system, can be used by some higher level services, like checkpointing. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In this lesson we will talk about inter-process communications, or IPC. We will primarily talk about shared memory and describe some of the APIs for shared memory based IPC. In addition we will describe some of the other IPC mechanisms that are common in operating systems today. In an earlier lesson, we described the process like an order of toys in a toy shop. In this lesson, we will see how processes can communicate with each other during their execution. But first let's see how inter-process communication is achieved in a toy shop illustration. IPC is like working together in the toy shop. First, the workers can share the work areas. The workers can call each other. And finally, the interactions among the workers requires some synchronization. Looking at this list, it is fairly obvious that the workers have many options in terms of how they can interact with one another. When sharing a work area, the workers can communicate amongst each other by leaving common parts and tools on the table to be shared among them. Second, the workers can directly communicate by explicitly requesting something from one another, and then getting the required response. And finally, good communication using either one of these methods requires some synchronization so that a worker knows when the other one has finished talking or the other one has finished with a particular tool. One way of thinking about this is that a worker may say I will start a step once you finish yours. As we will see, processes can also interact amongst each other and work together in similar ways. First, processes can have a portion of physically shared memory, and any data they both need to access will be placed in such shared memory. We will discuss this further in this lesson. Second, processes can explicitly exchange messages, requests, and responses via message passing mechanisms that are supported through certain APIs like like sockets. For both of these methods, processes may need to wait on one another and may need to rely on some synchronization mechanism like mutexes to make sure that the communication proceeds in a correct manner. Inter-process communication refers to a set of mechanisms that the operating system must support in order to permit multiple processes to interact amongst each other. That means to synchronize, to coordinate, to communicate all of those aspects of interaction. IPC mechanisms are broadly categorized as message-based or memory-based. Examples of message passing based IPC mechanisms include sockets, that most of you are familiar with already, as well as other OS supported constructs like pipes or message queues. The most common memory based mechanism is for the operating system to provide processes with access to some shared memory. This may be in the form of completely unstructured set of pages of physical memory or also may be in the form of memory mapped files. Speaking of files, these two could be perceived as a method for IPC, multiple processes read and write from the same file. We will talk about file systems in a separate lecture. Also, another mechanism that provides higher-level semantics when it comes to the IPC among processes is what's referred to as remote procedure calls, or RPC. Here, by higher-level semantics, we mean that it's a mechanism that supports more than simply a channel for two processes to coordinate or communicate amongst each other. Instead these methods prescribe some additional detail on the protocols that will be used, how will the data be formatted, how will the data be exchanged, et cetera. RPC too will be discussed in a later lesson in this class. Finally, communication and coordination also implies synchronization. When processes send and receive to each other messages, they in a way synchronize as well. Similarly, when processes synchronize, for instance, using some mutex like data structure, they also communicate something about the point in their execution. So from that perspective, synchronization primitives also fall under the category of IPC mechanisms. However, we will spend a separate lesson talking specifically about synchronization. For that reason this lesson will focus on the first two bullets, and we will talk about these remaining topics later. One mode of IPC that operating system support is called message passing. As the name implies, processes create messages and then send or receive them. The operating system is responsible for creating and maintaining the channel that will be used to pass messages among processes. This can be thought of as some sort of buffer or FIFO queue. Other type of data structure. The operating system also provides some interface to the processes so that they can pass messages via this channel. The processes then send or write messages to this port. And on the other end the processes receive or read messages from this port. The channel is responsible for passing the message from one port to the other. The OS Kernel is required to both establish the communication channel, as well as to perform every single IPC operation. What that means is that both the send and receive operation require a system call, and a data copy as well. In the case of send, from the process address base into the communication channel. And in the case of receive, from this channel into the receiving process address base. What this means is that a simple request response interaction among two processes will require four user kernel crossings, and four data copies. In message passing IPC, these overheads of crossing in and out of the kernel and copying data in and out of the kernel, are one of the negatives of this approach. A positive of this approach is it's relative simplicity. The operating system kernel will take care of all of the operations, regarding the channel management, regarding the synchronization. It will make sure that data is not overwritten or corrupt in some way, as processes are trying to send or receive it, potentially at the same time. So that's a plus. In practice, there are several methods of message passing based IPC. The first and most simple form of message passing IPC that's also part of the POSIX standard is called pipes. Pipes are characterized by two end points, so only two processes can communicate. There's no notion of a message per se with pipes. Instead, there's just a stream of bytes that pushed into the pipe from one process and then received, but into another. And one popular use of pipes is to connect the output from one process to the input of another process. So, the entire byte stream that's produced by P1 would be delivered as input to P2 instead of somebody typing it in, for instance. A more complex form of message passing IPC is message queues. As the name suggests, message queues understand the notion of messages that they transfer. So a sending process must submit a properly formatted message to the channel, and then the channel will deliver a properly formatted message to the receiving process. The OS level functionality regarding message queues also includes things like understanding priorities of messages or scheduling the way messages are being delivered. The use of message queues is supported through different APIs. In Unix-based systems, these include the POSIX API and the System V API. The message passing API that most of you are familiar with is the socket API. With the socket form of IPC, the notion of ports that's required in message passing IPC mechanisms, that is the socket abstraction that's supported by the operating system. Which sockets processes send messages or receive messages via an API that looks like this, and then receive. The socket API supports send and receive operations that allow processes to send messages buffers in and out of the in kernel communication buffer. The socket call itself creates a kernel-level socket buffer. In addition, it will associate any necessary kernel-level processing that needs to be performed along with the message movement. For instance, the socket may be a TCP/IP socket, which will mean that the entire TCP/IP protocol stack is associated with the data movement in the kernel. Sockets, as you probably know, don't have to be used for processes that are on a single machine. If the two processes are on different machines, then this channel is essentially between a process and a network device that will actually send the data. In addition, the operating system is sufficiently smart to figure out that if two processes are on the same machine, it doesn't really need to execute the full protocol stack to send the data out on the network, and then just to receive it back and push it into the process. Instead, a lot of that will be bypassed. This remains completely hidden from the programmer, but you could likely detect it if you perform certain performance measurements. In shared memory IPC, processes read and write into a shared memory region. The operating system is involved in establishing the shared memory channel between the processes. What this means is that the operating system will map certain physical pages of memory into the virtual address spaces of both processes, the virtual addresses in P1 and the virtual addresses in P2 will map to the same physical addresses. At the same time, the virtual address regions that correspond to that shared memory buffer. in the two processes, they don't need to have the same virtual addresses. Also the physical memory that's backing the shared memory buffer does not have to be a contiguous portion of physical memory. All of this leverages the memory management support that's available in operating systems in our modern hardware. The big benefit of this approach is that once the physical memory is mapped into both address spaces, the operating system is out of the way. The system calls are used only in the setup phase. Now, data copies are potentially reduced, but not necessarily completely avoided. Note that for data to be visible to both processes, it actually must explicitly be allocated from the virtual addresses that belong to the shared memory region. So if that's not the case, then data within the same address space has to be copied in and out of the shared memory region. In some cases however, the number of required copies can be reduced. For instance, if P2 needs to compute the sum of two arguments, that were passed to it from P1 via the shared memory region, then P2 can only read these arguments, it doesn't actually need to copy them into other portions of its address space, compute the sum, and then pass it back. However, there are some drawbacks. Since the shared memory area can be concurrently accessed by both processes, this means that the processes must explicitly synchronize their shared memory operations. Just as what you would have with threads operating within a single address space. Also, it is the developer's responsibility to determine any communication protocol related issues such as, how are messages going to be formatted? How will they be delimited? What are their headers going to look like? And also, how this shared memory buffer will be allocated? When which process, we'll be able to use a portion of this buffer for its needs. So this adds some complexity, obviously. Unix based systems, including Linux, support two popular shared memory APIs. One of these was originally developed as part of System V and the other one is the official POSIX shared memory API. In addition, shared memory based communication can be established between processes using a file based interface. So the memory wrapped files in both address spaces. This API's essentially analogous to the POSIX shared memory API. Also the Android operating system uses a form of shared memory IPC that's called Ashmem. There are a number of differences in the details of how Ashmem behaves compared to the system files POSIX APIs, but I'm just providing it here as a reference only. For the remainder of this lesson, we will focus on briefly describing the Unix space shared memory APIs We saw two major ways to implement IPC using a message-passing or a memory-based API. Which one of the two do you think will perform better? The message-passing? The shared memory-based API? Or neither, it depends? Mark your answer from the following choices. The answer to this question is the it depends answer that's common in many systems questions. Here is why. We mentioned that in message passing multiple copies of the data must be made between the processes that communicate and the kernel. That leads to overhead, clearly. For shared memory IPC, there are a lot of costs that are associated with the kernel establishing valid mappings among the processes' address spaces and the shared memory pages. Again, these are overheads. So there are drawbacks, basically, on the both sides. And the correct answer will be, it depends. In the next video, we will explain the trade-offs that exists among these two types of IPC mechanisms. Before I continue I want to make one important comment to contrast the message-based and the shared memory-based approaches to IPC. The end result of both of these approaches is that some data is transferred from one address space into the target address space. In message passing, this requires that the CPU is involved in copying the data. This takes some number of CPU cycles to copy the data into the channel via the port and then from the port and into the target address space. In the shared memory-based case, at the minimum, there's CPU cycles that are spent to map the physical memory into the appropriate address spaces. The CPU is also used to copy the data into the channel when necessary. However, note that, in this case, there are no user to kernel level switches required. The memory mapping operation itself is a costly operation. However, if the channel is set up once and used many times, then it will result in good payoff. However, even for 1-time use, the memory mapped approach can perform well. In particular, when we need to move large amounts of data from one address space into another space, the CPU time that's required to perform the copy can greatly exceed the CPU time that's required to perform the map operation. In fact, Windows systems internally in the communication mechanisms they support between processes, leverage the fact that there exists this difference. So if the the data that needs to be transferred among address spaces is smaller than a certain threshold, then the data is copied in and out of a communication channel via a port like interface. Otherwise, the data is potentially copied once to make sure that it's in a page aligned area. And then that areas is mapped into the address space of the target process. This mechanism that the Windows kernel supports is called Local Procedure Calls, or LPC. Now that we've described the shared memory mechanisms in a general way, let's look at the specific details of the system five Unix API. First the operating system supports segments of shared memory, that don't necessarily have to correspond to contiguous physical pages. Also, the operating system treats shared memory as a system-wide resource using system-wide policies. That means that there is a limit on the total number of segments of the total size of the shared memory. Presently, that's not so much of an issue as, for instance, currently in Linux, that limit is 4,000 segments. However, in the past it used to be much less and in certain OSs, it was as few as six segments. More recent versions of Linux had a limit of 128 segments. The operating system may also impose other limits as far as the system wide shared memory. When a process requests that a shared memory segment is created, the operating system allocates the required amount of physical memory, provided that certain limits are met. And then it assigns to it, a unique key. This key is used to uniquely identify the segment within the operating system. Any other process can refer to this particular segment, using this key. If the creating process wants to communicate with other processes using shared memory, then it will make sure that they learn this key in some way. By using either some other form of IPC, or just by passing it through a file, or as a command line argument, or maybe other options. Using the key, the shared memory segment can be attached by a process. This means that the operating system establishes valid mappings between the virtual addresses, that are part of that process virtual address space, and the physical memory that backs the segment. Multiple processes can attach to the same shared memory segment, and in this manner, each process ends up sharing access to the same physical pages. Reads and writes to these pages will be visible across the processes just like when threads share. Access to memory that's part of the same address space. And also, the shared memory segment can be mapped to different virtual address in different processes. Detaching a segment means invalidating the address mappings for the virtual address region that corresponded to that segment within the process. In other words the page table entries for those virtual addresses will no longer be valid. However, a segment isn't really destroyed once it's detached. In fact, a segment maybe attached and detached then reattached multiple times by different processes during it's life time. What this means is that once a segment is created it's like a persistent entity until there is an explicit request for it to be destroyed. This is similar to what would happen to a file. We create a file and then the file persists until it is explicitly deleted. In the mean time, we can open it and close it and read it and write it, but the file will still be there. This property of shared memory, to be removed only when it's explicitly deleted or when there is a system reboot, makes it very different than regular non-shared memory, that is Malloced and then it will disappear as soon as the process exits. SysV uses the following shared memory API for the high-level operations we just discussed. Shmget is used to create or open a segment of the appropriate size. And the flags include the areas options like permissions. This unique identifier is the key and that is not actually magically created by the operating system. Instead it is explicitly passed to the OS by the application. To generate a unique identifier the API relies on another operation ftok which generates a token based on its arguments. If you pass to this operation the same arguments, you will always get the same keys. That's like a hash function. This is how different processes can agree upon how they will obtain a unique key for the shared memory segment they will be using to communicate. The following call attaches the shared memory segments into the virtual address space of the process. So we'll map them into the user address space. The programmer has an option to provide the specific virtual addresses where the segment should be mapped, or if NULL is passed then the operating system will choose and return some arbitrary suitable addresses that are available in the processes address space. The returned virtual memory can be interpreted in arbitrary ways. So, it is the programmer's responsibility to cast that address to that memory region to the appropriate type. The following operation detaches the segment identified by this identifier, so the virtual to physical memory mappings are no longer valid. And then finally the control operation that the shared memory API supports is used to pass certain commands related to the shared memory segment management to the operating system. Including the command to remove a particular segment. And that command is IPC.RMID. There is also the POSIX API for shared memory. On Linux systems, it has been supported since the 2.4 kernel. Although it's supposed to be the standard, the POSIX API is not as widely supported as, for instance, the System V API. Here is the API. The most notable difference is that the POSIX shared memory standard doesn't use segments. Instead, it uses files. Now, these are not the real files that exist in some file system that used otherwise by the operating system. Instead, these are files that only exist in the so called tmpfs file system, which is really intended to look and feel like a file system. So, you can always reuse the same type of mechanisms that is used for file systems. But, in essence, is just a bunch of state that's present in physical and volatile memory. The I/O simply uses the same representation and the same data structures that used for representing a file to represent bunch of pages in memory that correspond to a shared memory region. For this reason, there is no longer a need for the awkward key generation process. Instead, shared memory segments can be referenced by the file descriptor that corresponds to the file. And, then the rest of the operations are analogous to what you'd expect to exist for files. A segment is opened, or closed. So, they're explicit, shared memory, open and close operations. But, in fact, it can really only call the regular open and close operations, since you will anyways pass a file. And, the operating system will manage to figure out which file system this file sits in. To attach or detach shared memory, the POSIX shared memory API relies on the mmap and unmap calls that are used to map, or unmap files into the address space of a process. To destroy a shared memory segment, there is an explicit unlink operation. There is also a shared memory close, and this will remove the file descriptor from the address space of the process. But, in order to tell the operating system to delete all of the shared memory-related data structures, and to free up that memory segment, you must call the explicit unlink operation. I have provided a link to the reference of the POSIX Shared Memory API in the instructor notes. When data is placed in shared memory, it can be concurrently accessed by all processes that have access to that shared memory region. Therefore such accesses must be synchronized in order to avoid race conditions. This is analogous to the manner in which we synchronize threads when they're sharing an address space, however it needs to be done for processes as well. So we still must use certain synchronization constructs, such as mute accessor condition variables, for processes to synchronize when they're accessing shared data. There are a couple of options in how this interprocess synchronization can be handled. First one can rely on the exact same mechanisms that are supported by the threading libraries that can be used within processes. So for instance two pthreads processes can synchronize amongst each other using pthreads mute access and condition variables that have been appropriately set. In addition, the operating system itself supports certain mechanisms for synchronization that are available for interprocess interactions. Regardless of the method that is chosen, there must be mechanisms to coordinate the number of concurrent accesses to the shared memory region. For instance, for support for mutual exclusion, mutexes provide this functionality. And also must, to coordinate, when is data available in the shared memory segment and ready to be consumed by the peer processes. This is some sort of notification or signaling mechanism. And condition variables are an example of a construct that provides this functionality. When we talked about PThreads we said that one of the attributes that's used to specify the properties of the mutex or the condition variable when they're created, is whether or not that synchronization construct is private to your process or shared among processes. The keyword for this is PTHREAD PROCESS SHARED. So when synchronizing the shared memory accesses of two pthreads multithreaded processes, we can use mutexes and condition variables that have been correctly initialized with pthread process shared styles. One important thing, however, is that the synchronization variables themselves have to be also shared. Remember, in multithreaded programs, the mutex or condition variables have to be global and visible to all threads. That's the only way they can be shared among them. So it's the same rationale here. In order to achieve this, we have to make sure that the data structures for the synchronization construct are allocated from the shared memory region that's visible to both processes. For instance, let's look at this code snippet. Let's look here at how the shared memory segment is created. Here we are using the system VAPI. In the gap operation, the segment id, the shared memory identifier, is uniquely created from the token operation where we use argument zero from the command line. So the path name for the program executable, and then some integer parameter, so in this case this is 120. We're also requesting that we create a segment size of 1 kilobyte, and then we specify the areas permissions for that segment. Then using that segment identifier that's returned from the get operation. We are attaching this segment and that will provide us with a shared memory address. So this is the virtual memory address in this instance of the process. In the execution of this particular process in its address space. That points to the physically shared memory. Now, we are casting that address to point to something that's of the following data type. If we take a look at this data type, this is the data structure of the shared memory area that's shared among processes. It has two components. One component is the actual byte stream that corresponds to the data. The other component is actually the synchronization variable, the mutex that will be used among processes when they're accessing the shared memory area, when they're accessing the data that they care for. So as to avoid concurrent writes, race conditions, and similar issues. So this is how we will interpret what is laid out in the shared memory area. Now, let's see how this mutex here is created and initialized. First of all, we said that before creating a mutex, we must create its attributes, and then initialize the mutex with those attributes. Now concerning the mutex attributes, we see that we have here set the, the pthread process shared attribute for this particular attribute data structure. Then, we initialize the mutex with that attribute data structure so it will have that property. Furthermore, notice that the location of the mutex we pass to this initialization call is not just some arbitrary mutex in the process address piece. It is this particular mutex element that is part of the data structure in shared memory. This set of operations will properly allocate, and initialize a mutex that's shared among processes. And a similar set of operations should be used, also, to allocate and initialize any condition variables that are intended for shared use among processes. Once you have properly created and allocated these data structures, then you can use them just as regular mutexes and condition variables in a multi threaded PThreads process. So there's no difference in their actual usage, given that they're used across processes. The key, again, let me reiterate, is to make sure that the synchronization variable is allocated within the shared memory region that's shared among processes. In addition shared memory accesses can by synchronized operating system provided mechanisms for inter process interactions. This is particularly important because the process shared option for the mutex condition variables with p threads, isn't necessarily supported on every single platform. Instead, we rely on other forms of IPC for synchronization, such as message queues or semaphores. With message queues for instance, we can implement mutual exclusion via send/receive operations. Here is an example of protocol how this can be achieved. Two processes are communicating via shared memory and they're using message cues to synchronize. The first process writes to the data that's in shared memory and then it sends a ready message on the message queue. The second process receives that ready message, knows that it's okay to read the data from the shared memory. And then it sends another type of response, an OK message back to P1. Another option is to use Semaphores. Semaphores are an operating system supported synchronisation contract and a binary semaphore can have two values, zero one. And it can be achieved, the similar type of behavior like what is achieved with a mutex. Depending on the value of semaphore, a process is either allowed to proceed or it will be stopped at the semaphore and it will have to wait for something to change. For instance, a binary semaphore with value zero and one, we use it in a following way. If its value's zero, the process will be blocked. And if its value is one, the semantics of the semaphore construct is such that a process will automatically decrement that values. It will turn it to zero, and it will proceed. So this decrement operation is equivalent to obtained a lock. In the instructor's notes, I'm providing a code example that uses shared memory and message queues and semaphores for synchronization. And the example uses the System V, or the System five API as a reference. The system five APIs for these two IPC mechanisms is really somewhat similar to those that we saw for shared memory in terms of how you create and close, et cetera, message queues or semaphores. For both of these constructs are also posex equivalent to APIs., Now, let's take a treasure hunt type of quiz concerning the Message Queue construct. The question has four parts. For a message queues, what are the Linux system calls that are used for? Send a message to a message queue? Receive messages from a message queue? Perform a message control operation? Or, to get a message identifier? Provide answers for each of the following questions. Remember to use only single word answers, like just reboot or just recv and feel free to use the Internet. The answers to these questions are as follows. Sending messages to a message queue uses the following command, msgsnd, message send. Receiving a message uses the following msgrcv. Performing a control operation on the message can be done using the following command, msgctl, control. And finally obtaining an identifier for a message can be done using the msgget call. As you start using IPC methods, it is useful to know that Linux provides some command line utilities for using IPC and shared memory in general. Ipcs will list all of the IPC facilities that exist in the system. This will include all types of IPC, message queues, semaphores. Passing the -m flag will display only the shared memory IPC. There is also a utility to remove an IPC construct. For shared memory you use the m flag. And you specify the shared memory identifier. Look at the man pages for both of these commands for a full set of options. When using shared memory, the operating system doesn't restrict you how the memory will be used. However, the choice of the API or the specific mechanisms that will be used for synchronization are not the only decisions that you need to make. Remember, with shared memory, the operating system. Provides the shared memory area and then it's out of the way, all of the data passing and synchronization protocols are up to the programmer. So in the upcoming more we will mention a few things that you can consider to assist with your design process To make things concrete, let's consider two multi threaded processes in which the threads need to communicate via shared memory. First consider how many segments will your processes need to communicate. Will they use one large segment? In that case you will have to implement some type of management of this shared memory. You'll have to have some memory manager that will allocate and free this memory for the threads from the different processes. Or you can use multiple segments, smaller ones, one for each pair-wise communication. If you choose to do this, it's probably a good idea to prealloacate, ahead of time, a pool of segments. So you don't have to slow down, that way, every individual communication with the segment creation overhead. So, in that case, you will have to create how will threads pick up which of the available segments they will end up using for their inter process communication? So, using some type of queue of segment identifiers will be, probably, a good idea for that. The tricky part here, if you are using a queue of segment identifiers, that means that a thread doesn't know up front which particular segment it's going to use for a communication with a peer thread in the other process. If that's important for the type of application that you're developing, you can consider communicating the segment identifier from one process to another via some other type of communication mechanism, like via message queue. Another design question is how large should a segment be? That will work really well if the size of the data is known up front and static. It doesn't change. However, in addition to the fact that data sizes may not be static, that they may be dynamic, the other problem with this is that it will limit what is the maximum data that could be transferred between processes because typically an operating system will have a limit on the maximum segment size. If you want to potentially support arbitrary message sizes that are much larger than the segment size, then one option can be that you can transfer the data in rounds. Portion of the data gets written into the segment, and then once P2 picks it, up P2's ready to move in the next round of that data item. However, in this case, the programmer will have to include some protocol to track the progress of the data movement through the rounds. In this case, you will likely end up casting the shared memory area as some data structure that has the actual data buffer, some synchronization construct, as well as some additional flags to track the progress. In this lesson we talked about inter-process communication, or IPC. We described several IPC mechanisms that are common in operating systems today. We spent a little bit more time on use of shared memory as an IPC mechanism. And we contrasted this also with use of message-based IPC mechanisms. Based on this lesson you should have enough information on how to start using inter-process communication mechanisms in your projects. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. Up to this point, we mentioned synchronization multiple times while discussing other operating system concepts. In this lesson, we will primarily focus on synchronization. We will discuss several synchronization constructs, what are the benefits of using those constructs. And also we will discuss what is the hardware level support that's necessary in order for those synchronization primitives to be implemented. As we cover these concepts, we will use the paper, The Performance of Spin Lock Alternatives by Thomas Anderson. This paper will give you a deeper understanding of how are synchronization constructs implemented on top of the underlying hardware, and why they exhibits certain performance strengths. There is a link to the paper available in the instructors notes. Let's illustrate synchronization and synchronization mechanisms using our toy shop example, synchronization in operating systems is like waiting for a co-worker in the toy shop to finish so that you can continue working. When an elf is waiting on a co-worker to finish, several things can happen, the elf can continuously check whether the other elf is done or they can ask the co-worker to signal them once they are done. At any rate, the time that an elf spends waiting will hurt the performance of the toy shop, for instance the elf may repeatedly check the other elf whether it's completed the work, asking questions like, are you done now? What about now? Are you still working? This can have a negative impact on the elf that is working because it will delay its processing, he will be annoyed. The second approach of hey, I'm done you can come and do your work now is maybe a little bit more laid back but this other guy may have gone out for lunch in the meantime so it may take it longer to actually come back and precede with the execution with the processing of the toy. So regardless, in both of these cases the workers are wasting some productive time during this wait period, similar analogies to these exist in synchronization in operating systems. Processes may repeatedly check whether it's okay for them to continue using a construct called spinlocks that's supported in operating systems. We already talked about mutexes and condition variables and we saw that they can be used in order to implement this behavior where a process waits for a signal from another process before it can continue working. Regardless of how we wait, and which synchronization mechanism we use, this will affect performance, this can result in CPU cycles that are wasted when we're performing this checking or performance may be lost due to certain cache effects when we are signalling another process that was periodically blocked to come back and start executing again. We spend already quite a bit of time talking about dealing with concurrency in multi-threaded programs. We explained mutexes and condition variables, both in generic terms based on Birrell's paper, and also their specific APIs and usage in the context of Pthreads. You're probably all safe and well underway with writing your Pthread programs using these constructs. So you're probably asking yourselves now, why do we have to talk more about synchronization? We spent quite a bit of time already talking about it and gained experience with it. Well, as we discussed mutexes and condition variables we mentioned a number of common pitfalls. This included things like forgetting to unlock the mutex, or signalling the wrong condition variables. These issues are an indication that the use of mutexes and condition variables is not an error-proof process. What that means is that these errors may affect the correctness of programs that use mutexes and condition variables. And in general it will affect the ease of use of these two synchronization constructs. Also, in the examples that we discussed, we had to introduce some helper variables when we needed to express invariance to control a reader's writer access to a shared file. We also need helper variables to deal with certain priority restrictions, given that mutexes or condition variables don't inherently allow us to specify anything regarding priority. This implies that these constructs lack expressive power. We cannot easily express with them arbitrary synchronization conditions. Finally, mutexes and condition variables and any other software synchronization construct requires a lower level support from the hardware in order to guarantee the correctness of the synchronization construct. Hardware provides this type of low level support via atomic instructions. So these are some of the reasons why it makes sense to spend more time talking about synchronization. What we will do, we will look at few other constructs, some of which eliminate some of the issues with mutexes and condition variables. And also we will talk about different types of views the underlying atomic instructions, to achieve efficient implementations of certain synchronization constructs. One of the most basic synchronization constructs that's commonly supported in an operating system is spinlock. In some ways, spinlocks are similar to a mutex. The lock is used to protect the critical section to provide mutual exclusion among potentially multiple threads or processes that are trying to perform this critical section. And it has certain constructs that are equivalent to the lock and unlock constructs that we saw with mutexes. The use of these lock and unlock operations is similar to the case of the mutexes. If the lock is free, then we can acquire it and proceed with the execution of the critical section. And if the lock is not free, we will be suspended at this particular point and unable to go on. The way spinlocks differ than mutexes, however, is that when the lock is busy, the thread that's suspended in its execution at this particular point of time isn't blocked, like in the case of mutexes, but instead, it is spinning. It is running on the CPU and repeatedly checking to see whether this lock became free. With mutexes, the thread would have relinquished the CPU and allowed for another thread to run. With spinlocks, the thread will spin. It will burn CPU cycles until the lock becomes free or until the thread gets preempted for some reason. For instance, maybe it was spinning until ultimately its time slice expired, or potentially a higher priority thread became runable. Because of their simplicities, spinlocks are a basic synchronization primitive, and they can be used to implement more complex, more sophisticated synchronization constructs. Because they're a basic construct, we will revisit spinlocks a little bit later in this lesson, and we will spend some time discussing different implementation strategies for spinlocks. The next construct we will talk about is semaphores, and these are a common synchronization constructs that have been part of operating system kernels for a while. As a first approximation, a semaphore acts like a traffic semaphore. It either allows threads to go or it will block them. It will stop them from proceeding any further. This is in some ways similar to what we saw in, with the mutex. It either allows a thread to obtain the lock and proceed with the critical section, or the thread is blocked and has to wait for the mutex to become free. However, semaphores are more general than just this behavior that can be obtained with a mutex, and we will take a look at how, what exactly a semaphore is a bit more formally next. Semaphores are represented via an integer number, some positive integer value. When they're initialized, they're assigned some maximum value, some positive integer. Threads arriving at the semaphore will try it out. If the semaphore's value is non-zero, they will decrement it and proceed. If it is zero, then they will have to wait. This means that the number of threads that will be allowed to proceed will equal this maximum value that was used when the semaphore was first initialized. So as a synchronization construct, one of the benefits of semaphores is that they'll allow us to express count-related synchronization requirements. For instance, five producers may be able to produce an item at the same time. The semaphore will be initialized with the number five, and then it will count and make sure that exactly five producers are allowed to proceed. It a semaphore is initialized with one, then its behavior will be equivalent to a mutex. It will be a binary semaphore. All threads that are leaving the critical section will post to the semaphore or signal the semaphore. What that will do, it will increment the semaphore's counter. For a binary semaphore, this is equivalent to unlocking a mutex. Finally, some historic tidbits. Semaphores were originally designed by Dijkstra. He was a Dutch computer scientist and also a Turing Award winner. And the wait and post operations that are used with semaphores were referred to as P and V, and P and V come from the Dutch words proberon, which means to test out, to try, and verhogen, which means to increase. So if you ever see a semaphore used with operations P and V, you will now know what that means. Here's some of the operations that are part of the Posix Semaphore API. Semaphore.h defines the sem_t semaphore type. Initializing a semaphore is done with the sem_init code. This takes as a parameter of semaphore beta type variable and also it takes the initialization count and a flag. This flag will indicate whether the semaphore is shared by threads within a single process or across processes. The sem_wait and sem_post operations take as a parameter the semaphore variable that was previously initialized. As a quick quiz, complete the arguments in the initialization routine for a semaphore. So that the use of the semaphore is identical in its behavior to that of a mutex that is used by threads within a single process. A reference link to the semaphore API has been included in the instructor notes. To use a semaphore in this manner, you should initialize the semaphore so that it is a non-process shared semaphore. So, this argument to the initialization call should be 0. And so that its initial count is 1. So this argument will be 1. Then, when the semaphore wait operation is called. It will decrement this counter and it will allow exactly one thread at a time to enter the critical section and posting to a semaphore will increment the counter and it will be identical to a mutex being freed. Note that most operating systems textbooks will include some examples on how to implement one synchronization construct with another including mutexes or condition variables with semaphores. And there are many other examples. So you're welcome to experiment on your own with these kinds of examples. When specifying synchronization requirements, it is sometimes useful to distinguish among the different types of accesses that a resource can be accessed with. For instance, we commonly want to distinguish those accesses that don't modify a shared resource, like reading, versus those accesses that do modify a shared resource. Like writing. For the first type of accesses, the resource can be shared concurrently. For the second type of accesses, we require exclusive access. For this reason, operating systems and language run times as well supports so called Reader/Writer Locks. You can define Reader/Writer Locks and use them in a way that's similar to a mutex. How ever you always specify the type of access, read versus write that you want to perform. And then underneath the lock behaves accordingly. In Linux, you can define a reader writer lock by using the provided data type for reader writer locks. To access a shared resource using this reader writer lock, you use the appropriate interface, read lock or write lock. Which one you use will clearly depend on the kind of operation that you want to perform in the shared resource. The reader writer API also provides the corresponding unlock counterparts for both read and write. A few other operations are supported on reader writer locks, but these are the primary ones. And if you would like to explore more, then take a look at the .h file. We are providing a link to it in the instructor's notes. Reader writer locks are supported in many operating systems and language run times. In some of these contexts, the reader writer operations are referred to as shared locks and exclusive locks. However, certain aspects of the behavior of the reader writer locks are different in terms of their semantics. For instance, it makes sense to permit a recursive read_lock operations to be invoked. But then, it differs across implementations, and exactly what happens on read_unlock. In some cases, a single read_unlock may unlock every single one of the read_lock operations that have recursively been invoked from within the same thread. Whereas in other implementations, a separate read_unlock is required for every single read_lock operation. Another way in which implementations of reader writer locks differ is in their treatment of priorities. For instance in some cases, a reader, so an owner of a shared lock, may be given a priority to upgrade the lock. So from a reader lock to convert to a writer lock. Compared to a newly arriving request for a write lock, or an exclusive lock. In other implementations, that's not the case. So the owner of a read lock will first release it and then try to re-acquire it with write access permissions. And contend with any other thread that's trying to perform the same operation. Another priority related difference across reader writer lock implementations is what kind of interaction is there between the state of the lock, the priority of the threads, and the scheduling policy in the system overall. For instance it could block a reader so a thread that otherwise would have been allowed to proceed. If there is already a writer that has higher priority and that is waiting on that lock. In this case the writer is waiting because there are other threads that already have read access to the lock. And if there is a coupling between the scheduling policy and the synchronization mechanisms, it's possible that a newly arriving reader will be blocked. It will not be allowed to join the other readers in the critical section. Because of the fact that the waiting writer has higher priority. One of the problems with the constructs that we saw so far is that they require developers to pay attention to the use of the peer-wise operations, lock/unlock, wait signal and others. And this, as you can imagine, is one of the important causes of errors. Monitors, on the other hand, are a higher-level synchronization construct that helps with this problem. In an abstract way, the monitors will explicitly specify what is the resource that's being protected by the synchronization construct. What are all the possible entry procedures to that resource, like, for instance, if we have to differentiate between readers and writers? And also, it would explicitly specify any condition variables that could potentially be used to wake up different types of waiting threads. When performing certain types of access on entry, all the necessary locking and checking will take place when the thread is entering the monitor. Similarly, when the thread is done with the shared resource and it's exiting the monitor, all of the necessary unlock operations, checks, any of the signaling that's necessary for the condition variables, all of that will happen automatically, will be hidden from the programmer. Because of all of this, monitors are referred to as a high-level synchronization construct. Historically, monitors were included in the MESA language runtime developed by Xerox PARC. Today, Java supports monitors too. Every single object in Java has an internal lock, and methods that are declared to be synchronized are entry points into this monitor. When complied, the resulting code will include all of the appropriate locking and checking. The only thing is that notify has to be done explicitly. Monitors also refer to the programming style that uses mutexes and condition variables to describe the entry and exit codes from the critical section. And this is what we described in the threads and concurrency lesson with the enter critical section and exit critical section section sub code. In addition to the multiple synchronization constructs that we already saw, there are many other options available out there. Some, like serializers, make it easier to define priorities and then also hide the need for explicit signaling and explicit use of condition variables from the programmers. Others, like path expressions, require that a programmer specify the regular expression that captures the correct synchronization behavior. So as opposed to using locks or other constructs, the programmer would specify something like many reads or a single write. And the runtime will make sure that the way the operations are interleaved that are accessing the shared resource satisfy that particular regular expression. Another useful construct are barriers, and these are almost like a reverse from a semaphore in that if a semaphore will allow n threads to proceed before it blocks, a barrier will block all threads until n threads arrive at this particular point. Rendezvous points is also a synchronization construct that waits for multiple threads to meet that particular point in execution. Also for scalability and efficiency, there are efforts to achieve concurrency without explicitly locking and waiting. These approaches all fall in a category that we refer to as wait-free synchronization. And they're optimistic in the sense that they bet on the fact that there won't be any conflicts due to concurrent writes and it's safe to allow reads to proceed concurrently. An example that falls into this category is this so-called read-copy update log, RCU log, that's part of the Linux kernel. One thing that all of these methods have in common is that at the lowest level, they all require some support from the underlying hardware to atomically make updates to a memory location. This is the only way they can actually guarantee that the lock is properly required, and that the state change is performed in a way that is safe. And that it doesn't lead to any race conditions, and that all threads in the system are in an agreement of what exactly is the current state of the synchronization construct. We will spend the remainder of this lesson discussing how synchronization construct can be built using directly the hardware support that's available from the underlying platform. And we will specifically focus on spinlocks as the simplest construct out there. We said that spinlocks are one of the most basic synchronization primitives. And that they're also used in creating some more complex synchronization constructs. For that reason it makes sense to focus the remainder of this lesson on understanding how exactly spinlocks can be implemented. And what types of opportunities are available for their efficient implementation. To do this, we will follow the paper The Performance of Spin Lock Alternatives on Shared Memory Multiprocessors by Tom Anderson. The paper discusses different implementations of spinlocks and this is relevant also for other synchronization constructs that use internally spinlocks. Also some of the techniques that are described in this paper that concern the use of atomic instructions generalize to other constructs and other situations. As a quiz, let's look at a possible spinlock implementation. Here is a possible pseudo code. To be clear, the lock needs to be initialized as free and that will be 0. And 1 will indicate that the lock is busy. To lock the spinlock, we first need to check to make sure that the lock is free. And if so, then we can change its state and block it. So change its state to busy. Otherwise, if the lock isn't free, we have to keep spinning. So we have to keep repeating this check and this operation. Finally, releasing the lock means that we set the value of the lock to free. These steps are listed in the Instructor Notes. The questions in this quiz are does the implementation of the lock correctly guarantee mutual exclusion? Also, is this implementation efficient? Mark your answers here. Before we talk about the correctness of the mutual exclusion, let's take a look at the efficiency factor. So this goto spin statement, as long as the lock is not free, this means that this cycle will repeatedly be executed and this will waste CPU resources. Therefore, from efficiency standpoint, this is not an efficient implementation. It's inefficient. But efficiency aside, this solution is also incorrect. In an environment where we have multiple threads that execute concurrently, or multiple processes, it is possible that more than one thread will see at the same time that lock is free. And they will move on to perform this lock equal busy operation at the same time. Only one thread will actually set the lock value to busy correctly. The other one will simply override it and will then proceed. It will think that it has correctly acquired the lock. So as a result, both processes or both threads can end up in the critical section and that clearly is incorrect. Here's another slightly different version of the same implementation that avoids the go to statement. As long as the lock is busy, the thread will keep spinning, it will remain in this while loop. At some point, when the log becomes free, the thread will exit from this while loop, and it will set the lock value to busy, so as to acquire it. Now answer the same question as in the previous quiz. Is this implementation of a spinlock correct in terms of its ability to guarantee mutual exclusion, and also, is this an efficient implementation? Again, as in the previous case, this implementation will be both inefficient and incorrect. The inefficiency comes from the fact that again we have continuous loop that is spinning as long as the lock is busy. Now, the implementation is incorrect because although we did put this while check here, multiple threads again will see that the lock is free once it becomes free. They will exit this while loop and will move on here and try to set the lock value to be busy. If these threads are allowed to execute concurrently, there's absolutely no way purely in software to guarantee that there won't be some interleaving of exactly how these threads perform this check in these set operations and that a race condition will not occur here. We can try to come up with multiple purely software-based implementations of a spinlock. And we'll ultimately come to the same conclusion that we need some kind of support from the hardware in order to make sure that some of this checking and setting of the lock value happens atomically via hardware support. So we need to get some help from the hardware, looking at this spinlock example from the previous video. We somehow needed the checking of the lock value and the setting of the lock value to happen indivisibly, atomically, so that we can guarantee that only one thread at a time can successfully obtain the lock. The problem with this implementation is that it takes multiple cycles to perform the check in this setting and during these multiple cycles, threads can be interleaved in arbitrary ways. If they're running on multiple processors, their execution can completely overlap in time. To make this work, we have to rely on support from hardware-supported atomic instructions Each type of hardware or hardware architecture will support a number of atomic instructions. Some examples include test and set, or read and increment, or compare and swap. Different instructions may be supported on different hardware platforms. Not every architecture has to support every single one of the available atomic instructions out there. As you can tell from the names of these instructions, they all perform some multi-step, multi-cycle operation. But, because they're atomic instructions, the hardware makes guarantees that the set of operations that are included in these instructions will happen atomically, so not just halfway, the entire operation or none of it. That it will happen in mutual exclusion, meaning that only one instruction at a time will be allowed to perform the appropriate operation. And that the remaining ones will be queued up, will have to wait. So if we think about this, what this means is that the atomic instructions specify some operation, and this operation is the critical section. And the hardware supports all of the synchronization-related mechanisms that are required for that operation. If you look at our spin lock example, using the first atomic test and set up operation, the spin lock implementation can look as follows. Here test is set to automatically returns or tests the original value of the memory locations that's past this parameter, lock in this case. And sets the new value of this memory location to be one. This happens automatically. Remember, one, to us, indicates that the lock is busy. When we have multiple threads that are contending for this lock, when they are trying to execute this spinlock operation, only one needs to successfully wire the lock. The very first thread that comes to execute the test and set. For that thread, test and set will return zero because the original value of the lock was zero. Originally, the lock was free. That thread will therefore exit the Y loop because test_and_set will return zero, read, and that is different than busy. Why? That is the only thread that will acquire the log and proceed with the execution. All of the remaining threads that try to execute test_and_set, or then test_and_set will return one, because this first thread already set the value of lock to be one. Therefore, those remaining threads will just continue spinning into this wild loop. Notice that in the process, these other threads, they're repeatedly resetting the value of the lock to one. So as they're spinning through the while loop, every single time they try to execute this test_and_set operation, this sets the value of lock field to be one again and to be busy. However, that's okay. The very first thread, when it executed test_and_set, they already set the value of the log to be busy. And these other threads are not really changing the fact that the log is indeed locked. Which specific atomic instructions are available on a given platform varies from hardware to hardware. Some operations like test and set, others like read and increment may not be available on all platforms. And in fact we may have versions of this, where in some cases there is availability of an atomic operation that anatomically increments something that does not necessarily return the old value. In other cases, there may be atomics that support read_and_decrement as opposed to read_and_increment. In addition, there may be differences in efficiencies with which different atomic operations execute on different architectures. For this reason, software such as synchronization constructs that are built using certain atomic constructions has to be ported. We have to make sure that the implementation actually uses one of the atomic constructions that's available on the target platform. Also we have to make sure that the implementation of software of these synchronizations constructs is optimized so that use the most efficient atomicity on our target platform. And so that it uses them in an efficient way to begin with. Anderson's paper presents several alternatives in how spinlocks can be implemented using available hardware atomics, and we will discuss these in the remainder of this lecture. Before moving on with the discussion of the spin lock alternatives that are presented in Anderson's paper, let's do a refresh on multiprocessor systems and our cache coherence mechanisms. This is necessary in order to understand the design trade-offs and the performance trends that will be discussed in this paper. A multiprocessor system consists of more than one CPU. In some memory that is accessible to all of these CPUs. The shared memory may be a single memory component that's equidistant from all of the CPUs. Or there will be multiple memory components. Regardless of the number of memory components, they are somehow interconnected to the CPUs. And in this figure, I'm showing an interconnect based connection. And this is more common in current systems. Or a bus-based connection, which was really more common in the past. Also note, here I'm drawing the bus-based connection to apply to a configuration where there is only a single memory module. However, the bus-based configuration can apply to both of these situations and vice versa. The one difference is that in the interconnect base configuration, I can have multiple memory references in flight. Where one memory reference is applied to this memory module and another one to the other memory module. Whereas if I have a bus-based configuration in this case, the shared bus, only one memory reference at a time can be in flight. Regardless of whether these references are addressing a single memory module or are spread out across multiple memory modules. So the bus is basically shared across all the modules. Because of this property that the memory's accessible to all CPUs, these systems are called shared memory multiprocessors. Other terms used to refer to shared memory multiprocessors are also symmetric multiprocessors, or for short, SMPs. In addition, each of the CPUs in these kinds of systems can have caches. Access to the cached data is faster so caches are useful to hide the memory latency. The memory latency is even more of an issue in shared memory systems because there is contention on the memory module. Because of this contention, certain memory references have to be delayed. And that adds to the memory latency even more so than before. So it is as if the memory is further away from the CPUs because of this contention effect. So when data is present in the cache, the CPUs will read data from the cache instead of memory, and that will have an impact on performance. Now when CPUs perform a write, several things can happen. First, you may not even allow a write to happen to the cache. A write will directly go to memory and any cached copy of that particular memory location will be invalidated. Second, the CPU write may be applied both to the cached location as well as directly in memory. So this technique is called write-through. We write both in the cache, as well as in memory. And finally, on some architectures the write can be applied in cache. But the actual update to the appropriate memory location can be delayed and applied later. For instance, when that particular cache line is evicted. We call this the write-back. One challenge is, what happens when multiple CPUs reference the same data, x in this case? The data could appear in multiple caches. Or in this case when we have multiple memory modules, x is actually present in one of the memory modules, but both of the CPUs referenced it, and so it appears in both of their caches. On some architectures, this is a problem that has to be dealt with purely in software, otherwise the caches will be not coherent. For instance, if on one CPU we update x to be 3, the hardware is not going to do anything about the fact that the value of x in the cache of another CPU is 4. This will have to be fixed in software. These are called non-cache-coherent architectures. On other platforms, however, the hardware will take care of all of the necessary steps to make sure that the caches are coherent. So, when one CPU updates x to be 3, the hardware will make sure that the cache on the other CPU is also updated. These are called cache-coherent platforms. The basic methods that are used in managing the cache coherence are called write-invalidate and write-update. Let's see what happens with each of these methods when we have a situation where a certain value is present in all of the caches on these two different platforms. In the red invalidate case, if one CPU changes the value of x, then the hardware will make sure that if any other cache has cashed that particular variable x, that value will be invalidated. Future references on this CPU to that same value x will result in a cache miss and will be pushed over to memory. The memory location clearly has to be updated based on one of the methods write-through or write-back. In the write-update case, when one CPU changes the value of x to x prime, then the hardware makes sure that if any other cache has cached that same memory location, its value gets updated as well, as the name of this method suggests. Subsequent accesses to this memorial occasion from the other CPU will result in a cache hit and will return the correctly updated new value. The trade off is that with write-invalidate, we actually post lower bandwidth requirements on the shared interconnecting the system. This is because we don't actually have to send the full value of x, just its address so that it can be invalidated in other caches. Plus once the cache line is invalidated, future modifications to the same memory location will not result in subsequent invalidations, that location is already invalidated. So if the data isn't needed on any of the other CPUs anytime soon, it is possible to amortize the cost of the coherence traffic over multiple changes. So basically, x will change multiple times over here, before it's needed on the other CPU. And it's only going to be invalidated once. That's what we mean by this amortized cost. For write-update architectures, the benefit is that the data will be available immediately on the other CPUs that need to access it. We will not have to pay the cost to perform another memory access in order to retrieve the latest value of x. So then, clearly programs that will need to access the value of x immediately after it has been updated on another CPU will benefit from support for a write-update. Note that you as a programmer, you don't really have a choice in whether you will use write-update or write-invalidate. This is a property of the hardware architecture, and whichever policy the hardware uses, you will be stuck with it. One thing that's important to explain is what exactly happens with cache coherence when atomic instructions are used. Recall that the purpose of atomic instructions is to deal with issues that are related to the arbiter intereleaving of threads that are sharing the CPU, as well as threads that are concurrently executing across multiple CPUs. Let's consider the following situation. We have two CPUs. On both of these CPUs we'll need to perform some atomic instruction that involves the memory location of x. And this x has been cached in both of the CPUs. The problem, then, is how to prevent multiple threads on these different CPUs to concurrently access the cashed values of x. If we allow the atomic constructions to read and update the cash value of the memory reference, that's the the target of the atomic construction. There can be multiple problems. We have multiple CPUs with caches and we don't know where that value has been cached. We have write update versus write invalidate protocols, we have latency on the chip. Given all these issues, it's really challenging if a particular atomic is applied to the cache on one CPU. To know whether or not on another CPU another atomic is attempted against the cash value in that CPU. For that reason, atomic operations bypass the caches, and they always directly access the memory location where the particular target variable is stored. By forcing all atomics to go directly to the memory controller, this is going to create a central entry point where all of these references can be ordered and synchronized in a unique matter. So none of the rates conditions that could have occurred if we allowed atomics to access the cache, that just won't occur in this situation. This will solve the correctness problem, but it will raise another issue. Atomics will take longer than other types of instructions. We will always have to access memory and they will also content on memory. In addition in order to guarantee atomic behavior, we have to generate the coherence traffic and either update or invalidate all of the cached copies of this memory reference. Even if the value of this memory location doesn't change with the atomic operation. We still have to perform this step on enforcing coherence traffic, so either invalidating or forcing the same value to be reapplied to the cache, regardless of whether or not this location changes. This is necessary in order to stay on the side of safety and to be able to guarantee correctness of the atomic operations. In summary, atomic instructions on SMP systems are more expensive compared to on a single CPU system, because there will be some contention for the shared bus or the shared interconnect. In addition, atomics in general are more expensive, because they will bypass the cache, in these kinds of environments. And they will trigger all the coherence traffic, regardless to what happens with the memory allocation that's the target of the atomic instruction With this background on SMPs, cash coherence, and atomics, we're now finally ready to discuss the design and performance trends of spinlock implementations. The only one thing that's left to decide are what are the performance metrics that are useful when reasoning about different implementations of spinlocks. To determine this, we should ask ourselves, what are our objectives? First we want the spinlock to have low latency. By latency, we are referring to how long does it take for a thread to. Acquire a lock when it's free, ideally we want the thread to be able to acquire a free lock immediately with a single instruction, and we already established that spinlocks require atomic instructions. So the ideal case will be that, we just want to be able to execute a single atomic instructions and be done. Next we want the spinlock to have low delay, or to have low waiting time. What that means is that whenever it is spinning and a lock becomes free, is we want to reduce that time that it takes from the thread to stop spinning and to acquire that lock that has just been freed. Again ideally, we would like for the thread to the able to do that, as soon as possible, as soon as this lock is freed, for a thread to be able to acquire it. So to execute an atomic. And finally, we need a design that will non generate contention. On the shared bus or the shared network interconnect. By contention, we mean both the contention that's due to the actual atomic memory references as well as contention that's generated due to the coherence traffic. Contention is bad because it will delay any other CPU in the system that's trying to access the memory, but more importantly it will also delay the owner of the spinlock and. That is the thread, that is the process that's trying to quickly complete a critical section and then release the spinlock. So if we have a contention situation, we may potentially even be delaying the unlock operation for this spinlock. That will clearly impact performance even more. So these are the three objectives that we want to achieve in a good spinlock design. And the different alternatives that we will discuss in this lesson will be evaluated based on these criteria. As a quiz, let me ask a question regarding the spinlock performance objectives and accompanying performance metrics, which is discussed. Among the performance metrics that we just discussed, are there any conflicting goals? In other words, does one of these goals counteract any of the other ones? I'm giving you three possible answers, that one conflicts with two, that one conflicts with three, and that two conflicts with three. You should check all the ones that apply. The goal number one is to reduce latency, and this implies that we want to try to execute the atomic operation as soon as possible. As a result, the locking operation will immediately incur an atomic operation and that can potentially create some additional contention on the network. Therefore one conflicts with three is one of the correct answers. Similarly with two, if we want to reduce the waiting time to delay, then we have to make sure we're continuously spinning on the lock as long as it is not available, so that we can detect as soon as possible that the lock is freed and we can try to acquire it. Again, this will create contention and so two is conflicting with three. As for any conflicts between reducing latency and reducing waiting time or delay, it is hard to answer this in a general case. It will really depend on from one algorithm to another. So we're not going to mark this answer as a conflicting one. Let's look at this simple spinlock implementation we showed earlier in this lesson that uses the test-and-set instruction. In this one and in all of the other examples that we will show, we will assume that the lock initialization step sets the lock to be free, and that 0 indicates free and 1 indicates busy, that the lock is locked. The nice thing about this lock is that the test-and-set instruction is a very common atomic construction that most hardware platforms support it. So, as code, it will be very portable. We will be able to have the exact same code run on different hardware platforms. >From a latency perspective, this spinlock implementation performs as good as it gets. We only execute the atomic operation and there's no way we can do any better than this. Note the lock was originally free. Say it was 0. And then as soon as we execute this spinlock_lock operation, we make an attempt to execute test_and_set. The lock is free, so this will return 0. As a result of that, we will come out of this while loop. And also the test_and_set will change the value of lock to 1, so the lock is busy. So at that point, we have clearly come out of the while loop. We're not spinning, and we have the lock, and we have marked that the lock is busy. Regarding delay, this particular implementation potentially could perform well because we see that we're continuously just spinning on the atomic instruction. As long as the lock is busy, test_and_set will return a 1, will return busy, so we will remain in this while loop. However, whenever the lock does become freed, this test_and_set will, or at least one of them, will immediately detect that and will come out of the while loop. That single successful test_and_set operation also will again set the value of lock to 1, so any other test_and_set attempts will result in spinning again. Now, we already said there is a conflict between latency and delay and contention, so clearly this lock will not perform well from a contention perspective. As long as they're spinning, every single processor will repeatedly go on the shared interconnect, on the shared bus to the memory location where the log is stored, given that it's repeatedly trying to execute the atomic instruction. This will create contention, delay the processing that's carried out on other CPUs. It will delay the processing that the lock owner needs to perform, who's trying to execute the critical selection. And so, therefore, it will also delay the time when the lock actually becomes freed. So it's just bad all over. The real problem with this implementation is that it continuously spins on the atomic construction. If we don't have cache coherence, we will really have to go to memory in order to check what is the value of the lock. But with this implementation, even if we do have coherent caches, they will be bypassed because we're using an atomic instruction. So in every single spin, we will go to memory regardless of the cache coherence. That clearly is not the most efficient use of atomics or of hardware that does have support for caches and cache coherence. Let's see how we can fix the problem with the previous implementation. If the problem is that all of the CPUs are repeatedly spinning on the atomic operation, let's try to separate the test part which is checking the value of the lock from the atomic. The intuition is that, for the testing, we can potentially use our caches and test the cached copy of the lock. And only if that cached copy of the lock is indicating that the lock has changed its value, that it's cleaned, only then do we try to actually execute the atomic operation. So, here is what the resulting spin lock, lock operation will look like. The first part checks if the lock is busy. This checking is performed in the cache value, so this is the not involving any atomic operation, we're just checking whether a particular memory address is set to one or zero, so it's busy or free. On a system with caches, this will clearly hit the cache. As long as the lock is busy, we will stay in this while loop, it will not evaluate the second part of this predicate. So if the lock is busy, this is one, this is true, the entire predicate is true, two will go back in the while loop. What that also means is that as long as the lock is busy, as long as this part is true, the test and set, so the atomic operation, will not be a valued at all. We will not attempt to execute it. Now, only when the lock becomes free, so when this part of the predicate lock equals busy, when this is not true. Only then do we try to evaluate the second part of this predicate, at that point the test to set operation, the atomic instruction will be executed or will be attempted at least, and then we will see what happens whether we acquire the lock or not. So what this also means is we will try to make a memory reference since the test and set performs a memory reference only when the lock becomes free. This spin lock is referred to as the test and test_and_set spinlock. It is also reference as a spin and read spinlock, since we're spinning on the red value that's in cache, or spin on the cached values, so because this is the behavior of the lock. The Anderson paper uses the spin and reach term to refer to this lock. >From a latency and delay time point, this lock is okay. It's slightly worse than the test_and_set lock, because we have to perform this extra check, whether the lock is busy or not that hits the cache, but in principle, it's good. But, if we start thinking whether or not we really solved the contention problem by allowing to spin on the cache, we realize that this is not quite the case. First if we don't have a cached coherent architecture then every single memory reference will go to memory, just like with test_and_set. So there's no difference what so ever. If we have cache coherence with right update then it's not too bad. The one problem is that all of the processors with right update will see that the lock becomes free. So, regarding the delay. And so every single one of them will at the same time try to execute the test_and_set operation, so that can potentially be an issue. Now the worst situation of using this particular spinlock implementation is when we have a right invalidate based architecture. In this case, every single attempt to acquire the lock not only that it will generate contention for the memory module, but it will also create invalidation traffic. Now when we talked about the atomics, we said that a one outcome of executing an atomic instruction, is that we will trigger the cache coherence, so the right update or right invalidate traffic are regardless of what the situation is. If we have a right update situation, that coherence traffic will update the value of the other caches with the new value of the lock. If the lock was busy before the write update event and if the lock remains busy after the right update event. Great. No change whatsoever. That particular CPU can continue spinning from the cached copy. However, with the right invalidate, we will simply invalidate the cached copy. So it is possible that the lock was busy before the cache coherence event, before somebody executed an atomic instruction. And if the atomic construction was not successful, we're basically continuing to have a situation in which the lock is busy. However, as far as the caches in the system are concerned, that atomic instruction just invalidated their cache copy of the lock. The outcome of that is they will have to go out to memory, in order to fetch this copy of lock, that they want to be spinning on. So they will not be able to just spin on the cache copy, they will have to go ahead and fetch this lock value from memory, every single time somebody attempts to perform an atomic instruction. So this type of behavior will simply just compound the contention effects and will make performance worse. The reason basically for the poor performance of this lock is that, at the same time, everybody will see that the lock has changed it state so that it has been freed. And, everyone will also at the same time, try to acquire this lock. Let's look some more into the implications of the test and test_and_set spinlock implementation. Here is the question. In an SMP system with N processors, what is the complexity of the memory contentions, relative to the number of processors, that will result from the test and test_and_set spinlock when the lock is freed? I would like for you to write the O of n complexity for a system that has cache coherence with a write-update and also cache coherence with write-invalidate If caches are write-updated, then all of the processors will be able to see when the lock is released immediately, and they will issue a test_and_set operation. So, we'll have as many memory references as there will be test_and_set operations, so the complexity of the contention is going to be order of O of N. If the caches are write-invalidated, then all of the processor's caches will be invalidated after that initial lock release. For some processors, by the time they reread the lock value from memory in order to execute this part of the predicate, the lock will already have been set to busy by another processor. So those processors will try to spin on the newly read cached copy. So back in this portion of the while loop. Other processors, however, when they reread the value of lock from memory, that will happen before any test_and_set has executed. So they will see the value of lock is free. As a result, they will try to execute that test_and_set operation. Now, only one of these test_and_set operations will succeed. However, every single one of them will go ahead and invalidate everybody's caches. That means that that will also invalidate the caches on those processors that did see that the lock was busy. For this reason, the complexity of the, the input that gets created, of the contention that gets created when the lock is freed using that test _and_set spin lock is O of N squared. A simple way to deal with the problems of the test and test and set lock is to introduce a delay. Here's a simple implementation which introduces a delay every single time a thread notices that the lock is freed. So the delay happens after release. When the thread sees that the lock is freed, we'll come out of this while loop. And then before going back to recheck what the value of the lock is and if it's indeed free to try to execute the atomic operation, the test and set, the thread will wait a little bit, will delay. The rationale of this is yes, everybody will see that the lock is free at the same time. However, with this delay, not everybody will try to issue the atomic operation at the same time. As a result, the contention in the system will be significantly improved. When the delay expires, the delayed threads will try to recheck the value of the lock. And it's possible that if somebody else in the meantime came and executed the test and set, it's possible they will see that the lock is busy. And then they will go in this inner while loop and continue spinning. If the lock is free, the delayed thread will execute the atomic operation. However with this delay it's more likely that not all the threads will try to execute the atomic operation at the same time. And also that we're not going to have these situations where threads are repeatedly invalidated while they're attempting to spin on the cached value. That is because after the delay in the second check, a lot of the threads will see that this lock has become busy already. And they will not even attempt to execute the test and and set instructions. So, it'll be fewer cases where the lock is busy and somebody's attempting to execute the test and set instruction. And that was what caused one of the issues with the contention effects in the previous examples. >From a latency perspective, this spinlock is still okay. Yes, we do have to perform one memory reference to get the lock first into the cache, and then perform the atomic instruction. But that's similar to what we saw with the test and test and set. >From a delay perspective, clearly, this lock will be much worse, because once the lock is freed, then we have to delay for some amount of time. And if there's no contention for the lock, then that delay is just waste of time. Another variant of the delay-based spinlocks is to introduce a delay after every single lock reference. So every time we go through this while loop, we include a delay. The main benefit of this is that it works in non-cache coherent architectures. Because basically we're not going to be spinning constantly. In every single spin loop, we will include a delay. So if we don't have cache coherence and we have to go to memory, using this delay will help with the reduction of contention of the memory network. The downside of this is clearly that it will hurt the delay much more. Because we're basically building up the delay even when there is no contention on the network. One issue with the delay based spinlocks is how to pick a good delay value. Two basic strategies make sense. Static delay and a dynamic delay. For static delay, we can use some fixed information like for instance the CPU ID, where the process is running. In order to determine a delay that will be used for any single process that ends up running on that particular CPU. The benefits of this is that it's a simple approach, and under high loads, likely this static delays will sort of nicely spread out all of the atomic references so that there's no contention. Clearly, the delay will have to be a combination of something that combines this fixed information and the length of the critical section. So that one process is delayed one times the critical section, another process is delayed twice the critical section and so forth. The problem is that this will create unnecessary delay under low contention. So what if we have two processes? The first process is running on CPU with an ID one. The second process is running on a CPU with an ID 32, and so they're the only two that are contending for the spin lock, and yet the second process that's running on the CPU with ID 32 will be delayed substantially, regardless of the fact that there is no contention for the lock. So to avoid this issue, a more popular approach is use dynamic delay and that they named the delay such that each process will pick a random delay value based on the current perception of the contention in the system. The idea is that the system is operating in a mode of low contention then it will choose dynamic delay value that's within a smaller range. So it will back off just for a little bit, it will delay just for a little bit. And if the system is opening in a mode of large contention, then this delay range will be much larger. So with the random selection, some processes will back off a little bit. They'll delay a little bit. Whereas other processes will back off, they will delay for quite a bit of time. Theoretically both of these approaches under high load will result into the same kind of behaviors. So dynamic will at high load will tend to be equivalent to the static delay based approach. The key question is, however, in this Dynamic Delay is how do we know how much contention is there in the system? That's why we put here the term perceived. We don't know exactly what is the contention in the system. So each of the processes, the implementation of the spinlock. Somehow has to infer whether the system is operating in low or high contention, so that it can pick an appropriate delay. So a good metric to estimate the contention, is to track the number of failed test and set operations. If a test and set operation fails, the more likely it is that there is a higher degree of contention. The problem with this, however, is if we're delaying after every single log reference, then our delay will keep growing based on both whether there is indeed contention in the system, or if simply the owner of the critical section is delayed or if it's executing a long, critical section. So then we may end up with the same situation as before. Just because somebody was executing a long, critical section while holding the spin log, that doesn't mean that we need to bump up the delay. So we need to be able to guard against this case. The delay alternatives in the spinlock implementations that we saw in the last morsel address the problem that everybody tries to acquire a spinlock at the same time when that lock is freed. In this paper, Anderson proposes a new lock called a queuing lock, and that lock is trying to solve the problem that everybody sees that the lock is free at the same time. If we solve this problem, if not everybody sees that the lock is free, then we're essentially also solving the second problem, because then not everybody will try to acquire the lock at the same time. So let's take a look at what this lock looks like. The queuing lock looks as follows. It uses an array of flags, with up to N elements where N is the number of processors in the system. And every single one of the elements will have one of two values, either has-lock or must-wait. In addition, we will have two pointers. They will indicate the current lock holder, so that one clearly will have a value of has-lock. And they will also indicate the index into this array that has the last element on the queue. When a new thread arrives at the lock, it will get a ticket and that will be the current position of the thread in the lock. This will be done by adding it after the existing last element in the queue. So basically the queue last value will be incremented and the new thread will be assigned the next position in the array. Since multiple threads may be arriving at the lock at the same time, we will clearly have to make sure that the way that this queue last appointer is incremented is done atomically. So basically this queuing clock depends on some support for an atomic read and increments. It will rely on that kind of hardware atomic operation. This is not as common as test-and-set that we used in the previous spinlock implementations. For each of the threads that are arriving at this queuing spinlock, the assigned element of this flags array, the queue[ticket], that acts like a private lock. What that means is that as long as the value of queue[ticket] is must-wait, then the thread will have to spin just like with a spinlock. When the value of this element becomes has-lock, that will be an indication that the lock is free and you can go ahead and proceed and enter the critical section. When a thread completes the critical section and needs to release the lock, it needs to signal the next flag in this queuing array that it currently has the lock. So these are the steps that control who gets to execute and what needs to happen when a critical sections is complete and a lock needs to be released. Other than the fact that this lock requires some support for some read and increment to be performed atomically, clearly it's going to require a size that is much larger than the other spinlock implementations. All other locks we saw needed only one memory location to keep the spinlock information whether the spinlock is free or busy. And here we need N such locations to keep the values has-lock or must-wait for each of the elements in this array. Here is the implementation of the queuing lock or the so called Anderson lock. The lock is an array and the array elements have values has lock or must wait. Initially the first element of the array has the value has lock and all the other elements have the value must wait. Also part of the lock structure is the global variable queuelast. To obtain a ticket into this queueing lock a process must first perform a read and increment atomic operation. That will return the myplace so the index into that particular process into the queue. The process will then continue to spin on the corresponding element of the array as long as its value is must-wait. For the very first thread that arrives at this lock, the value of flags of zero will be has-lock, so that one will successfully acquire the lock and proceed in the critical section. Every subsequent thread, as long as the first thread is in the critical section, will come and will get tickets that will point to some elements of the flags array that are from zero to p minus one. And they will not be able to proceed, their value will be must-wait. When the owner of the lock is done with the critical section, it will reset its element in the array, you must wait in order to get this build ready for the next threads and next processes that try to acquire this lock. Notice that we're using some modular math in order to wrap around this index read in increment and just continue increasing the value queuelast where as our array's of limited size. Releasing the lock means that we need to change the value of the next element in the array. So myplace+1, we will change its value from must-wait, it will become has-lock. That means that that thread, that process, will now come out of the spin loop that it's in. Notice that the atomic in this spin lock implementation involves a read and increment on a variable, queuelast. All of this spinning disc implementation happens on completely different variables. So, the elements of the flags array. For this reason, issuing the atomic operation, read an increment. Any kind of invalidation traffic is not going to concern any of the spinning on the elements of the flags array. These are two different memory locations, two different variables. >From a latency perspective, this lock is not very efficient, because it performs a more complex atomic operation, the read and increment. This read and increment operation takes more cycles than an atomic test and set. In addition, it needs to perform this modular shift in order to find the right index into the array. All of that needs to happen before it can determine whether or not it has to spin or be in the critical section. So the latency is not good. The delays really good however when a lock is freed, the next processor to run is directly signaled by changing the value of its flag. Since we're spinning on different locations, we can afford to spin all the time, and therefore we can notice that the value has changed immediately. >From a contention perspective, this lock is much better than any of the other alternatives that we discussed. Since the atomic is only executed once up front and it's not part of the spinning code. Plus, the atomic instructions and the spinning are done on different variables, so the invalidations that are triggered by the atomic will not affect the processor stability to spin on local caches. However, in order for us to achieve this, we have to make sure that first we have a cache coherent architecture. If we don't have cache coherent architecture, this spinning has to happen on potentially remote memory refer. Second we also have to make sure that every element is in a separate cache line. Otherwise when we change the value of one element of the array, we will invalidate the entire cache line so we will invalidate potentially the caches of the other elements in the array of the processors that are spinning on other elements. And that's clearly not something we want to achieve. To summarize the benefits of this lock, come from the fact that it addresses the key problem that we mentioned with the other spin lock implementations. In that everyone saw hat the log was free at the same time and everyone tried to acquire the lock at the same time. The queue in lock solves that problem. By having a separate, essentially a private, lock in this array of locks, only one thread at a time sees that the lock is free, and only one thread at a time attempts to acquire the lock. Assume we are using the Anderson's queueing spinlock implementation where each array element can have one of two values: has-lock, and let's say that's zero, and must-wait, let's say we use one for that. Now if a system has 32 CPUs, how large is the array data structure that's used in the queueing spinlock? Your choices are 32 bits, 32 bytes or neither. Choose the correct answer. The correct answer is neither. Remember that for the queuing implementation to work correctly each of the elements of the array has to be in a different cache line. So the size of the data structure depends on the size of the cache line. For example, on my machine, the cache line is 64 bytes so, the size of the data structure will be 32 by 64 byte. But in practice, there may be other cache line sizes that are used on the architectures that you use. Then finally, let's take a look at one of the results from the performance measurements that are shown in the Anderson's paper. This is Figure 3 from that paper if you're following along. This figure shows measurements that were gathered from executing a program with multiple processes. And each process executed a critical section, the critical section was executed in a loop 1 million times. The number of processes in the system was varied such that there is only one process per processor. In the platform that was used, was a Sequence Symmetry that had 20 processors. So that's why the maximum number of processes was also kept to 20. Also this platform was cache coherent with write and validate. The metric that was computed based on the experiments was the overhead that was measured compared to in a case of ideal performance. An ideal performance corresponded in these measurements to a theoretical limit. How long it takes to execute that fixed number of critical sections. So basically there is no contention, no effects due to the fact that each of these critical sections needs to be locked and unlocked. Then how long would it take to run these number of critical sections. The measured difference between that theoretical limit and whatever it actually took to perform this experiment was considered the overhead. And this is what these graphs represent. These experiments were performed for every one of the different spinlock implementations we discussed. >From the spin on read, the test_and_test_and_set, all the way to the queuing implementation. These results don't include that the basic test and set when we're spinning on the atomic construction. Basically that implementation would result in something that would just completely be off the charts. It would be the highest overhead the worst performance measurement. And then, notice how we have for the different variations of the delay both the static and the dynamic delay. So let's see what happens under high load. This is where we have lots of processors and lots of processes running on those processors that are contending for the log. We see from these measurements that the queuing log, it's the best one. It is the most scalable and as we add more and more processors, more and more load, it performs best. The test_and_test_and_set log, that's this line here, that one will perform worst. That isn't particular the case, because here we have an architecture that's cache coherent with right invalidate. And we said that in that case, on release of the test_and_test_and_set log, we have an order of n squared memory references. So a very high contention on the shared bus and that's going to really hurt performance. After delay based alternatives, we see that the static implementations are a little bit better than their dynamic counterparts. Since under high loads with static, we end up nicely balancing out the atomic constructions. First with dynamic, we still end up with some randomness, with some number of collisions. Those are avoided in the static case. Also note that delaying after every single memory reference is slightly better than delaying after the log is freed only, it's only on release. Because when we avoid after every reference, we end up avoiding some additional invalidations that come from the fact that sequence is a right invalidate architecture. Under light loads, when we have few processes then few processors as well, we can make couple of observations. First we see that test_and_set performs really pretty well in this case. And that's because this implementation of a spinlock has low latency. We were just performing a check, if lock equal busy and then we were moving on to the atomic test_and_set. We also see that in terms of the delay alternatives, the dynamic delay alternatives. The backup delay alternatives perform better than the static ones. And that is because with the dynamic alternatives, we end up with lower delay. But we said that static alternatives can lead to situations in which the two processors that have the extreme, the smallest and the largest delay. Are the only ones that are contending on the lock and so we have wasted cycles. We also see that under light loads, the queueing lock performs the worst. This is the performance of the queueing lock. And the reason for that is we explain that with the queuing clock, we have pretty high latency. Because we need to implement the read and increment and the modular processing, etc. So this is what hurts the performance of the queueing lock under light loads. One final comment regarding the performance results that we just discussed is that this points to the fact. That with system design, there isn't really a single good answer that the design points should be driven based on the expected workload. Light loads, high loads, architectural features, number of processors, write invalidate, write update, etc. The paper includes additional results that point to some of these trade offs in more detail.spi In this lesson, we talked about other synchronization constructs beyond just mute excess and condition variables. And described some of the synchronization problems that these constructs are well suited for. In addition, we talked about the spinlock alternatives that are described in Anderson's paper. And learned how hardware support, specifically how atomic instructions are used when implementing constructs like spinlocks. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. We said that a main role of operating systems is to manage the underlying hardware. But, so far, we talked primarily about CPUs and memory. In this lesson, we will focus on the mechanisms that operating systems use to represent and manage I/O devices. In particular, we will look at the operating system stack for block devices, using storage as an example. So, in this context, we will also discuss file systems, since files are the key operating system abstraction that's used by processes to interact with storage devices. We will describe the Linux file system architecture as a concrete example of this. I/O management is all about managing the inputs and the outputs of a system, to illustrate some of the tasks involved in I/O management, we will draw a parallel between I/O in computer systems and a shipping department in a toy shop. Let's see what are some of the similarities we can find between how operating system manages I/O and how the shipping department is managed in a toy shop. For instance, both in the toy shop and in the operating system there will be protocols, in terms of how the needs to be managed, in both cases there will be dedicated handlers or some dedicated staff that will oversee these protocols. And in both environments the details about the I operations are the cop out from the core processing actions, in a toy shop the I/O protocols determine how and what parts exactly come from a storage facility into the toy shop and which toys and toy orders are being shipped out in which order and how. To make sure that these protocols are enforced in the toy shop will have dedicated staff that handle all aspects related to shipping and in the toy shop, the process of making toys is completely separate from any details regarding the shipping process. What are the carriers that are being used? What are their protocols? Are there any slaves that are being used? It doesn't matter. This has analogies in the context of the I/O management in operating systems in multiple ways. For instance, operating systems incorporate interfaces for different types of I/O devices and how these interfaces are used determines the protocols that are used for accessing those types of devices. Similarly, operating systems have dedicated handlers, dedicated operating system components that are responsible for the I/O management their device drivers, their interrupt handlers, these are used in order to access and interact with devices. And finally, by specifying these interfaces, and using this device driver model operating systems are able to achieve this last bullet, they're able to abstract the details of the I/O device and hide them from applications or upper levels of the system software stack from other system software components. Here is the illustration of a computer system we used in our introductory lesson in operating systems. As you can see, the execution of applications doesn't rely only on the CPU and memory, but relies on many other different types of hardware components. Some of these components are specifically tied to providing inputs or directing outputs. And these are referred to as I/O devices. Examples include keyboards and microphones, displays, speakers, mice, also network interface hard disk. So there are a number of different types of input/output devices that operating systems integrate into the overall computing system. Here's a quick quiz, for each device indicate whether it is typically used for input, for output, or for both? The devices mentioned are keyboard, speaker, display, hard disk drive, microphone, network interface card, or NIC, flash cards. Your answer should be as follows. Keyboards and microphones are input devices. Speakers and displays are both output devices. And all of the remaining devices can be used both for input and output. In addition to these types of devices, there are many other types of devices. And then within each of these categories there are many concrete examples. Many different types of microphones. Many different types of speakers or network interface cards. Operating systems must be designed in a way that can handle all of these different types of devices efficiently and in a straightforward manner. As this figure suggests, the device space is extremely vaporous. Device spaces come in all sorts of shapes and sizes, with a lot of variability in their hardware architecture, in the type of functionality that they provide, in the interfaces that applications use to interact with them. So in order to simply our discussion in this lesson, we'll point out the key features of a device that enable the integration of devices into a system. Any device can be abstracted to have the following set of features. Any device will have a set of control registers that can be accessed by the CPU and that permit the CPU device interactions. These are typically divided into command registers that the CPU uses to control what exactly the device will be doing. Data registers that are in some way used for the CPU to control the data transfers in and out of the device. And then also some status registers that are used by the CPU to find out exactly what's happening on the device. Internally, the device will incorporate all other device-specific logic. This will include a microcontroller, and that's like the device CPU. This is what controls all of the operations that actually take place on the device. It may be influenced by the CPU, but the microcontroller will make sure what things happen versus not. Then there's some amount of on device memory, any other types of processing clauses, special chips, special hardware that's needed on the device. Like for instance, you convert analog to digital signals to actually interact with the network medium, with the physical medium. Whether it's optics or, or copper, whatever else, so all of those chips will be part of the device. Devices interface with the rest of the system via controller that's typically integrated as part of the device packaging, that's used to connect the device with the rest of the CPU complex via some CPU device interconnect. Whatever off-chip interconnect is supported by the CPU that different devices can connect to. This figure that's adapted from the Silver Sheds book shows a number of different devices that are interconnected to the CPU complex via PCI bus. PCI stands for peripheral component interconnect and it's one of the standard methods for connecting devices to the CPU. Today's platforms typically support PCI Express interconnect, which are technologically more advanced than the original PCI or PCI-X bus. So, PCI Express has more bandwidth and it's faster, has lower access latency, supports more devices. In all aspects, it is better than PCI-X which was the follow on on the original PCI standard. For compatibility reasons, though, today's platform will also include some of these older technologies, typically PCI-X because it's compatible with PCI, PCI-X stands for PCI extended. Note that the PCI bus is not the only interconnect that's shown in this figure alone. There is also a SCSI bus that connect SCSI disks, there's a peripheral bus here shown that connects devices like keyboards, and there may be other types of buses. The controllers that are part of the device hardware, they determine what type of interconnect can a device directly attach to. And, there are also bridge and controllers that can handle any differences between different types of interconnects Operating systems support devices via device drivers. Here's a chart that shows where device drivers sit with respect to the rest of the operating system and the actual hardware they manage. Device drivers are device-specific software components. And so, the operating system has to include a device driver for every type of different hardware devices that are incorporated in the system. Device drivers are responsible for all aspects of device access, management, and control. This includes logic that determines how can requests be passed from the higher levels of the system software applications to the actual device, how can the system respond to device-level events like errors, or response notifications, or other status change information. Or in general, any device-specific details regarding the device configuration or operation. The manufacturers or the designers of the actual hardware devices are the ones that are responsible for making sure that their device drivers available for the different operating systems or versions of operating systems where that particular type of device needs to be available. For instance you may have had to download drivers for printers from a manufacturer like HP. Operating systems in turn standardize their interfaces to a device driver's. Typically this is done by providing some device driver framework so that device manufacturers can develop the specific device drivers within that framework, given specific interfaces that that operating system supports. In this way both device driver developers know exactly what is expected from the ways the rest of the operating system will interact with their device. And also the operating system does not depend on one particular device, one specific device if there are multiple options for devices that provide the same functionality. For instance, for storage you may have different types of hard disk devices from different manufacturers. Or from networks, you may have different types of network interconnect cards. And switching the hardware components will require switching the device driver but the rest of the operating system, the rest of the applications will not be affected. So there's standardized interfaces both in terms of the interaction with those devices as well as in terms of the development and integration of the device drivers. In this way we achieve both device independence. The operating system does not have to be specialized or to integrate this particular type of functionality for each specific device. And also device diversity. It's an easy way for the operating system to support arbitrarily different types of devices. We just need another device driver. To deal with this great device diversity, devices are roughly grouped in several categories. This includes block devices like disks. These are devices that operate at granularity of entire blocks of data that's delivered in and out of the device and in and out of the CPU complex in the other end. A key property is that individual blocks can be directly accessed. For instance if you have ten blocks of data on the disk, you can request to directly access the ninth one. That's what makes it a block device. Then there are character devices like keyboards that work with a serial sequence of characters and support something like a get-a-character, put-a-character type of interface. And then there are network devices that are somewhat of a special case somewhere in-between. Since they deliver more than a character at a time, but their granularity is not necessarily a fixed block size. It can be more flexible. So this looks more like a stream of data chunks of potentially different sizes. In this manner the interfaces from the operating system to the devices are standardized based on the type of device. For instance for a block devices, the driver should support operations to read/write block of data, for character devices, a driver should support operations to put/get a character from, into the device. So, in that sense standard. Internally, the operating system maintains some representation for each of the devices that are available on the platform. This is typically done by using a file abstraction to represent the different devices. And in that way what really the operating system achieves is that it can use any of the other mechanisms that are already part of the operating system to otherwise manipulate files. Too, now think mean to refer to access different types of devices. Given ultimately that these files will correspond to devices and not real files then things like read and write operations or actual operations that manipulate this file will be handled in some device-specific manner. On Unix-like systems all devices appear as files underneath this /dev directory. As special files, they're also treated by a special file system. They're not really stored on the real file system. In Linux versions, these are tmpfs and devfs. As we just stated, Linux represents devices as special files and the operations on those files have some meaning that's device specific. As a quiz, look at the following three Linux commands which perform the same operation with an I/O device. What operation do they perform? Type your answer in the text box and feel free to use the internet as a resource. If we take a look at these commands, we see that in all of these cases, there's some content, whether the content of file, or the string "Hello, world" that's being written into a file, /dev/lp0. 'lp' stands for line printer and 0 stands for the first line printer that's identified by the Linux system. What these commands mean that we're printing something. So the correct answer is that each of the commands are printing something to the lp0 printer device. To go in a little bit deeper into the special device files, Linux also supports what are called pseudo or virtual devices. These devices do not represent an actual hardware and are not critical in our understanding of I/O management, but are useful nonetheless. As a quiz, I will ask you to look at the following function descriptions and name the pseudo device that provides that function. First, what is a device that accepts and then discards all output without actually producing anything? Second, what is a device that produces a variable-length string of pseudo-random numbers? For your answers, please fill them in, in these boxes, and also use the full path to the device to specify what is the correct answer. For instance to answer that the devices line printer is here up, use path /dev, /lp0. The answer to the first question is /dev/null. For instance, if you just want to discard the output of a process you can just send it to /dev/null. The answer to the second question is the device /dev/random. It also has a more robust counterpart, /dev/urandom, and one use of this device is to create files that contain pseudo-random bytes. As an exploratory quiz, try running the following command in a Linux environment. This command lists the contents off a directory, in this case, the /dev directory. The question to you, then, is what are some of the device names that you see when you run this command? Enter at least five of those device names in a comma separated list in this text box. And, if you don't have access to some other Linux environment, you may use the Ubuntu VMs that are provided this course. The instructor notes have a download link. Here are some of the possible answers to this question. You may have noticed hda or sda devices, these are drives, like hard drives, SSDs, or even CD-ROMs. Also, you may have noticed some tty devices. These are special devices representing terminal stations. They can actually be used to pipe output to and from terminals. There are many other devices, the null device that we already mentioned, lp for printer, and many more. The main way in which PCI interconnects devices to the CPUs is by making devices accessible in a manner that's similar to how CPUs access memory. The device registers appear to the CPU as memory locations at a specific physical address. So then when the CPU writes to these locations, the integrated memory PCI controller realizes that this access, the access to this physical location, should be routed to the appropriate device. What this means is that a portion of the physical memory on that computing system is dedicated for interactions with the device. We call this memory-mapped I/O. And the portion of the memory that's reserved for these interactions is controlled by a specific set of registers, the Base Address Registers. So how much memory in starting it, which address, will be used by a particular device. This gets configured during the boot process and it's determined how exactly it's done by the PCI configuration protocol. In addition, the CPU can access devices via special instructions. For instance, in x86 platforms, there are special in/out instructions that are used for accessing devices. Each of the instructions has to specify the target device, so the I/O port, and some value that's going to be stored in registers. That's the value that needs to be written out to device or the value that will be read out of the device. This model is called the I/O port model. The path from the device to the CPUs complex can take two routes. Devices can generate interrupts to the CPU, or the other option is for the CPU to poll the device by reading its status register in order to determine, does the device have some data for the CPU? Does the device have a response to a request that was sent to the CPU? Or some other information. There are overheads associated with both of these methods, and as always, there are trade-offs between the two. With interrupts, the problem is due to the handling of the interrupt routine, the interrupt handler. There are the actual steps involved in the handling of the interrupt routine. There's several operations like setting and resetting the interrupt mask depending on what kinds of interrupts are allowed to interrupt the interrupt handling routine or not. And also some indirect effects due to cache pollution that's related to the execution of this handler. So all of these are overheads, but on the flip side, it is possible to trigger an interrupt as soon as the device has something to do, some kind of notification, some kind of data for the CPU. For polling, the operating system has a possibility to choose when it will poll at some convenient times, when at least some of the cache pollution effect won't be too bad. If that is the case, then that's great. Some of those overheads will be removed, but potentially this will introduce some delays in the way that event is observed and in the way that the event is handled. The opposite of just continuously polling will clearly introduce some CPU overheads that may simply not be affordable if there aren't enough CPUs in the system or the system is busy otherwise doing other things. But our interrupt or polling mechanism should be selected will really depend on the kind of device that we're dealing with on the objectives, whether we want to maximize things like throughput or latency, on the complexity of the interrupt handling routine, and the characteristics of the load of the device. So, what is the input data rate, the output data rate that needs to met, and number of other factors. With just basic support from an interconnect like PCI and the corresponding PCI controllers on the device, a system can access or request an operation from a device using a method called programmed I/O. This method requires no additional hardware support. It involves the CPU issuing constructions by writing into the command registers of the device and also controlling the data movement by accessing the data registers of the device. So either the data will be written to these data registers or read from them. For instance, let's look at a network interface card or a NIC as a simple device that's accessed via programmed I/O or PIO. Here in the NIC case, when a process that's running from the CPU wants to transmit some data that has been formatted into an network packet, the following steps need to take place. First, the CPU needs to write to a command register on the device. And this command needs to instruct the device that it needs to perform a transmission of the data that the CPU will provide. The CPU also needs to copy this packet into the data registers and then the whole thing will be repeated as many times as it's necessary for the entire packets to be sent out. Given that the size of this register space that's available on the device may be smaller than the network packet. For instance if they have a 1500-byte packet and the device data registers or the bus that connects the device with the PCU are 8 bytes long, then the whole operation of performing programmed I/O will require one CPU access to the device registers to write out the command, and then another 188 accesses, so 1500 divided by 8, approximately 188. In total, there will be 189 CPU accesses to the device-specific registers. And, we said that these look like CPU store instruction. This gives us some idea about the costs associated with programmed I/O. Let's now look at the alternative. An alternative to programmed IO is to use DMA-supported devices. DMA stands for direct memory access, and it is a technique that relies on special hardware support in the form of a DMA controller. For devices that have DMA support, the way the CPU interacts with them is that it would still write commands in the command registers on the device, however, the data movement will be controlled by configuring the DMA controller which data to be moved from CPU memory into the device. This requires interactions between the CPU and the DMA controller, and, in fact, the same method can be used to move data in the opposite direction, from the device to the CPU, so the device would have as DMA controller that it interacts with to enable that interaction. Let's look again at the network interface card, the NIC example, to illustrate how exactly DMA-based CPU-device interactions are carried out. Again, here, the data that needs to be moved from the CPU to the device is already formed as a network packet. And the very first step requires that the CPU write a command into the device command register to request the packet transmission. This, however, needs to be accompanied with an operation that configures the DMA controller with the information about the in-memory address and size of the buffer that's holding this packet that we want transmitted. So that's basically the location of the data we need to move,and the total amount of data to be moved. Once this is done, the device can perform the desired operation. >From the CPU perspective, performing this transmission requires that we perform one store instruction in order to write out the contents in the command register and one operation to configure the DMA controller. All of this looks much better than the alternative of performing 189 store operations that we saw with programmed IO. The second step, configuring the DMA controller, is not a trivial operation, and it takes many more cycles then a memory store. Therefore, for smaller transfers, programmed IO will still be better than DMA, because the DMA step itself is more complex. One important thing is that in order to make DMA work is, we have to make sure that the data buffer that we want moved or where we want data to be written must be present in physical memory until the transfer completes. It cannot be swapped out to disk, since this DMA controller can only read and write data to and from physical memory, so the contents have to be there. This means that the memory regions involved in DMAs are pinned, they're not swappable, they have to remain in physical memory. As a quiz, I would like for you to consider DMA and programmed IO in a hypothetical system. For a hypothetical system, assume the following, it costs 1 cycle to perform a store instruction to a device register, and it costs 5 cycles to configure a DMA controller. Also, the PCI-bus is 8 bytes wide. And also assume that all devices in the system support both DMA and programmed IO based operations. With this in mind, which device access method is best for the following devices? Keyboards or NICs? The options to consider are programmed IO, DMA, or depends. These are the three answers you need to provide for each of these devices. The answer to each question will be depend heavily on the size of the data transfers. For a keyboard which likely will not transfer much data for each keystroke, a programmed I/O approach is better since configuring the DMA may be more complex than to perform one or two extra store instructions. For the network card the answer is the popular, it depends, answer. If we're sending out small packets, that require that we perform a less than five store instructions to the device data registers, given that the difference between the store instruction and the DMA controller in this hypothetical example is one is to five. Then it's better to perform program PIO. If we need to perform larger data transfers, then the DMA option will be a better one since we just need to configure the DMA controller and then issue the request. Typical ways in which user processes interact with a device is as follows. A process needs to perform an operation that requires access from a hardware device, for instance, to perform a send operation to transmit a network packet or to read a file from disk. The process will perform a system call in which it will specify the appropriate operation. With this, the process is requesting this operation from the operating system, from the kernel. The operating system then will run the in-kernel stack related to this particular device. Like, for instance, the TCPIP stack to form the appropriate packet before the data is sent out to the network or the file system that's needed to determine the particular disk block that stores the file data. And the operating system will invoke the appropriate device driver for the network or for the block device for the disk, and then the device driver will actually perform the configuration of the request to the device. On the network side, this means that the device driver will write out a record that configures the device to perform a transmission of the appropriate pack of data. Or on the disk side, this means that the device driver will issue certain commands to the disk that configure the disk head movement or where the data should be read from, et cetera. The device drivers will issue these commands using the appropriate programmed IO or DMA operations, and this will be that in a device-specific manner. So that driver sort of wants that are understand the available memory registers on the device. They understand the other pending requests. So the driver is the one that will need to make sure that the requests aren't somehow overwritten or undelivered to the device. So, all of this configuration and control will be performed at this level, at the driver level. And finally, once the device is configured, it will perform the actual request. In the case of the network card, this device will perform the transmission of the data. In the case of the disk device, the device will perform the block read operation from disk. And finally, any results from the request or in general any events that are originating on the devices will traverse this coal chain in a reverse manner. It is not actually necessary to go through the kernel to get to a device. For some devices, it is possible to configure them to be directly accessed from user level. This method is called operating system bypass. We're bypassing the operating system and from user level, directly accessing the device. That means that the device registers or any memory that is assigned for use for this device is directly accessible to the user process. The operating system is involved in configuring this. So, making sure that any memory for register share data corresponding to the device are mapped to the user process. But then after that's performed, the operating system is out of the way. It's not involved in it, and we go from the user process all the way down to the device. Since we don't want the user process to go into the operating system, this driver has to be some user level driver. It's like a library that the user process has to be linked with in order to perform the device specific operations regarding the access to registers or device configurations that are typically performed by the kernel level drivers. Like the kernel level drivers, this code, this user level drivers, will typically be provided by the actual manufacturers of the devices. When bypassing the operating system, the operating system has to make sure that it still has some kind of coarse-grain control. Like for instance, enabling or disabling a device, adding permissions to add more processes to use the device, etc. This means that the operating system relies on some type of device features, like for instance, the device has to have sufficient registers so that the operating system can map some registers to the user process. So the user process can perform the default device functionality like send/receive if it's a network device or read/write if it's a disk device. But still retain access to whatever registers are used for configuring and controlling the device that are needed for these coarse-grain control operations. If the device has too few registers and it's reusing the same registers for both the core data movement or the core functionality, as well as these control operations that are needed to be performed by the operating system, we can't do this. We need to be able to share the same device across potentially multiple user processes. So assign some subset of the registers to different user level processes to be controlled by different user level drivers and libraries. And still make sure that the operating system has some coarse-grain control over exactly how the device is used. And whether something needs to be changed. Another thing that happens when multiple processes use the device at the same time is that when the device needs to pass some data to one of the processes. It is now the device that needs to be able to figure out how exactly to pass data to the address space of process one versus process two. For instance, when receiving network packets, the device itself has to determine which process is the target of the packet. If you think about what that means with respect to the networking protocols, that means the device has to peek inside of the packet in order to see what is the port number that this packet is intended to do. And then also it has to know which are the socket port numbers that these processes are using in their communication. So what that means is that the device has to perform some protocol functionality in order to be able to demultiplex the different packets that belong to these different processes. In general, it needs to have demultiplexing capabilities so that data that's arriving in this device can be correctly passed through the appropriate process. In the regular device stack, where the operating system kernel is involved, it is the kernel that performs these operations. The kernel is aware of the resources that are allocated to each process. And the mappings that each process has with respect to the physical resources in the system. When the operating system is bypassed, those types of checks have to be performed by the device itself. When an I/O request is made the user process typically requires some type of a response from the device even if it's just a status that, yes I got it I'll do this for you, so what happens with the user process or the user thread once the I/O call is made? What will happen to the thread will depend on whether the I/O operations are synchronous or asynchronous, for synchronous I/O operations, the process or the calling thread at least will be blocked. The OS kernel will place that thread on the wait queue that's associated with the corresponding device and then the thread will eventually become runable when the response from this request becomes available so in the mean time it will be blocked, it will be waiting, it will not be able to perform anything else. But asynchronous operations the user process is allowed to continue as soon as it issues the I/O call, at some later time the user process can be allowed to come in and check are the results ready for this operation and at that point it will retrieve the results. Or perhaps at a later point the process itself will be notified by the device, or by the operating system, that the operation has completed and that any results are ready and are available at the particular location. The benefit of this is that the process doesn't have to go and periodically check to see whether the results are available, this is somewhat analogous to the polling versus interrupt base interface that we talked about earlier. Remember that we talked about the asynchronous I operations when we talked about the flash paper in the lesson on thread performance consideration. In there, the solution was for the kernel to avoid blocking the user process by creating separate threads that will perform I/O operations, in that case synchronous operations that will block. Here, we're really talking about asynchronous I/O operations truly being supported within the operating system. Let's look closer at how block devices are used, using the similar diagram. Block devices, like disks, are typically used for storage. And the typical storage-related abstraction, used by applications, is a file. The file is a logical storage unit, and it is mapped to some underlying physical storage location. What that means is that at the top-most level, applications don't think about the disks. They don't issue operations on disks for seeking blocks and sectors, etc. Instead, they think about files, and they request operations to be performed on files. Below this file-based interface used by applications will be the file system. The file system will have the information how to take these reads and writes that are coming from the application, and to then determine where exactly is the file, how to access it, what is the particular portion of that file that needs to be accessed, what are any permission checks that need to be performed, and to ultimately initiate the actual access. One thing that's useful to have is for operating systems provide some flexibility in the actual details that a file system has in terms of how it lays out files in disk, how it performs these access operations. In order to do that, operating systems allow for a file system to be modified or completely replaced with a different file system. To make this easy, operating systems specify something about the file system interfaces. This includes both the interfaces that are used by applications to interact with the file system, and there, the norm is the POSIX API that includes the read and write and open file system calls that we've mentioned so far. Also, standardizing the file system interfaces means that there would be certain standard APIs in terms of how these file systems interact with the underlying storage devices, as well as with other operating system components they need to interact with. If the files are indeed stored on block devices, clearly at the lowest level, the file system will need to interact with these block devices near their device drivers. We can have different types of block devices where the files could be stored, SCSI ID disk, hard disk, USB drive, solid state disks. And the actual interaction with them will require certain protocol specific APIs. Now, in spite of the fact that all of these may be block devices, there still may be certain differences among them. For instance, what are the types of errors they report or how they report the errors. So, in order to mask all of that, the block device stack introduces another layer, and that is the generic block layer. The intent of this layer is to provide a standard for a particular operating system to all types of block devices. The full device features are still available and unaccessible through the device driver. However, if used by the file system stack some of these will be abstracted underneath this generic block device interface. So then what happens when the user process wants to perform an access, a read or write operation in the file, it invokes that read/write operation for the POSIX API and the kernel level file system will then, based on the information that it maintains, will determine what is the exact device that needs to be accessed, and what is the exact block on that device that supports that particular region of the file. That in turn will result in some generic read block, write block operations that are passed to the generic block layer and this layer will know how exactly to interact with a particular driver, so how to pass those operations to the particular driver and how to interpret the error messages, or notifications, or responses that are generated by that driver. Any lower-level differences among the devices, like the protocols that they use, et cetera, will be handled by the driver. It will speak the specific protocol that's necessary for the particular device. As we mentioned in a previous morsel, system software can access devices directly. And in Linux, the command ioctl is used to directly access and manipulate devices. It stands for io control, and this is essentially a way that the operating system can access the device's control registers. For this quiz you need to do the following. Again, in Linux the ioctl command can be used to manipulate devices. What you need to do is complete the code snippet using ioctl so as the determine the size of a block device. The correct argument for the ioctl operation will be BLKGETSIZE. B-L-K GETSIZE. And this is specified in the Linux fs.h header file. When this function is executed the memory location that's pointed by the numblocks variable will be filled out with the returned value. Here is the block device stack figure from the previous morsel, as we said, it has well-defined interfaces among the applications and the kernel level file system and this is what makes it easy to change the implementation of the file system that we want to use without making any changes to the applications. In the same way we can also change what is the particular type of a device that a file system uses without really making changes further up the stack. However, what if we want to make sure that the user applications can seamlessly, as a single file system, see files that are distributed across multiple devices? And, if on top of that, different types of devices work better with different file system implementations, or what if the files aren't even local on that particular machine, on some of the devices? What if they need to be accessed via the network? To deal with these differences, operating systems like Linux use a virtual file system layer, the virtual file system will hide from applications all details regarding the underlying file system. Whether there is one or more, whether they use one or more local devices or a file systems they reference remote servers and use remote storage. Applications continue to interact with this virtual filesystem using the same type of API, for instance the POSIX API and the virtual file system will specify a more detailed set of all system related abstractions that every single one of the underlined file systems must implement, so that it can perform the necessary translations. The virtual file system supports supports several key extractions. First, there's obviously the file extraction. These are the elements on which the virtual operating system operates. The OS represents files via file descriptors. A file descriptor is created when the file is first opened, and there are a number of operation that can be supported on files by using the file descriptor to identify the specific file. These include read, write, lock a file, send file, also close a file, ultimately. For each file VFS maintains a persistent data structure called an inode. One of the information that's maintaining those inode is the list of all the data blocks that correspond to this file. This is how the inode derives it's name, it's like an index node for that file. The inode also contains other pieces of information for that file like permissions, the size of the file, whether the file is locked, et cetera. The inode is a standard data structure in Unix-based systems, and again it's important because the file does not need to be stored contiguously on disk. Its blocks may be all over the storage media, and therefore, it's important to maintain this index. Now, we know that files are organized in directories. But, from the virtual file systems perspective and in general from unique space systems perspective, a directory is really just a file, except, its contents include information about files and their inodes. So that we can find where the data blocks for these files and so the virtual file system will interpret the contents of the directory a little bit differently. To help with certain operations on directories Linux maintains a data structure called dentry, stands for directory entry. And each dentry object corresponds to a single path component that's being traversed as we are trying to reach a particular file. For instance, if we're trying to reach a file that's in my directory, in the directory named ada. We would have to traverse this path /users/ada. In the process, the virtual file system will create a dentry element for every path component. Forward slash for /users. That's the second directory in this path. And then finally, for the third directory in this path, /users/ada. Now this first slash, that corresponds to the root directory in the file system structure. The reason that this is useful is that when we need to find another file that's also stored in this directory ada, we don't have to go through the entire path and try to reread. The files that correspond to all of these directories in order to get to the directory ada, and then ultimately to find the file, the next file that we are searching. So the file system will maintain a cache of all of the directory entries that have been visited, and we call that the dentry cache. Note that this is soft-state, \there isn't some persistent on-disk representation of the dentry objects. This is only in memory maintained by the operating system. Finally, there is the superblock abstraction that's required by the virtual file system, so that it can provide some information about how a particular file system. It's laid out on some storage device. This is like a map that the file system maintains so that it can figure out how has it organized on disk the various persistent data elements, like the inodes or the data blocks that belong to the print files. Each file system also maintains some additional metadata in the superblock structure that helps to during its operation. Exactly what information will be stored in a superblock, and how it will be stored differs among file systems. So that's why we say it's file system specific information. The virtual file system data structures are software entities. And they're created and maintained by the operating system file system component. But other than the entries, the remaining components will actually correspond to blocks that are present on disk. The files, of course, are written out to disk, and they will occupy some blocks. And here we have two files, the green file and the blue file. And they occupy multiple blocks that don't have to be contiguous. The inodes, we said, will track all of the blocks that correspond to a file. And they do have to be persistent data structures. So they will live somewhere in disk. So let's say these two blocks here correspond to the inodes for these two different files, for simplicity. To make sense of all of this and to be able to tell what is an inode, what is a data block, what is a free block, the super block maintains an overall map of all of the disks on a particular device. This is used for allocation, when we need to find some free blocks to allocate to a new file creation request or file write request. And it is also used for lookup when we need to find the particular portion of a particular file. To make things concrete, let's look at a file system that's supported on disk devices. We will look at ext2, which stands for Extended Filesystem version 2. It was a default file system in several versions of Linux until it was replaced by ext3 and then ext4 more recently, that are the default versions in more current versions of Linux. It is also available for other operating systems, it's not just Linux specific. A disk partition that is used as an ext2 Linux file system, will be organized in the following way. The first block, block 0 is not used by Linux and it often contains the code to boot the computer. The rest of this partition is divided into groups and exactly what are the sizes of the groups that has nothing to do with the physics of the disk. What are the disk cylinders or sectors or anything like that. Each of the Block Groups in this partition will be organized as follows. The first Block in a Block Group is the Super Block. And this contains information about the overall Block Group. It will have information about the number of inodes, about the number of disk Blocks in this Block. And it will also have information about the start of the free Blocks. The overall state of the Block Group is further described in the group descriptor. And this will have information about the location of the bitmaps. We'll describe what they mean, next. About the total number of free nodes. About the total number of directories in the system. This information is useful when files are being allocated because the ext 2 tries to balance the over all allocation of directories and files across the determined Block Groups. The bitmaps are used to quickly find a free Block or a free inodes. And for every single inode in this particular group and every single Data Block, the bitmap will be able to tell the upper layer allocators. Letter that inode component or the Data Block are free or they're used by some other file or directory. Then come the inodes. They're numbered from one up to some maximum number and every one of the inodes is in ext 2 128 byte long data structure that describes exactly one file. It will have information like, what is the owner of the file, some accounting information that system calls like stat would return, and also some information like how to locate the actual Data Blocks. So these are the Blocks that hold the file data. Again, a reminder, a directory will really be just the file except that in the upper levels of the file system software stack there will be these entry data structures created for each particular component for the particular directory. We said that inodes play a key role in keeping track how file is organized on disk, because they essentially integrate an index of all of the disk blocks that correspond to a particular file. First a file is uniquely identified by its inode. In the virtual file system, inodes are uniquely numbered, so to identify a file we use the number identifier of the corresponding inode. This inode itself will contain a list of all of the blocks that correspond to the actual file. So it will be like an index into the blocks of the actual storage device. That when stitched together give us the actual file. In addition to this list of blocks, the inode also has some other metadata information. And this is useful to keep track of whether or not certain certain file accesses are legal, or to correctly updated the status of the file if it's locked or not locked. Or other things. One simple way in which this can be used this as follows. A file name is mapped to a single inode. And an inode, let's say here that it corresponds to a block. So the file jeep points to the block 19. That is the inode for this particular file. The contents of this inode block are all of the other blocks that constitute the contents of the file. If you look here we see that this file has five blocks allocated to it, and if we turn aside that we need more storage for these files, as per, pending per writing more data into the file. The file system will allocate a free block, lets say in this case, this block 30. It will correctly update the inode data structure, the list of blocks to say that the next node in the system is this node 30. And so the actual representation of the file on disk will now look as follows. The benefit of this approach is that it's easy to perform both sequential or random accesses to the file. That for sequential we just need to get the next index into this list of blocks. For random access we just, based on the block size need to compute. Which particular block reference do we need to find. And so it's fairly straightforward to do this. The downside is that this limits the size of the file to the total number of blocks that can be indexed using this linear data structure. Let's take a look at this example. Let's say we have 128 byte inodes. And let's say they only contain these indexes to the blocks on desk. Supposedly, we have 4 bytes to address individual block on this. So that means that the maximum number of block pointers, of block addresses, that can be included in this inode is 32 of those. That's if we don't have any metadata in the inode. If we assume that a single block is 1 kilobyte, that means that the maximum number of a file that can be addressed using this inode data structure. That's represented in this way, is 32 kilobytes. That clearly is very restrictive. One way to solve this is to use so called indirect pointers. Here is an inode structure that uses these indirect pointers. Just like before, the inode contains metadata and pointers to blocks of data. The inode is organized in a way that first, it has all of the metadata, owner, when is the file last accessed? Then it starts with the pointers to blocks. The first part is a list of pointers that directly point to a block on disk that stores the file data. Using the same example like before, where we had blocks that are 1 kilobyte large, and we used 4 bytes to address an individual block, these direct pointers will point to 1KB of data per entry. To extend the number of disk blocks that can be addressed via single inode element, while keeping the size of the inode small, we use so-called indirect pointers. And indirect pointer, as opposed to pointing to an actual data block will point to a block that's full of pointers. Given that our blocks are 1 kilobyte large and our pointers are 4 byte large, that means that a single indirect pointer can point to 256 kilobytes of file content. So just using a single element of the inode data structure as this indirect pointer can significantly increase the overall size of the files that can be supported in this file system. Now, if we need even larger files, we can use double indirect pointers. A double indirect pointer, points to a block which contains pointers to blocks of data. If every block has 256 pointers, then this double indirect pointer can help us address 256 times 256 times 1 kilobyte blocks for a total of 64 megabytes of data. We can apply the same idea to triple indirect addressing and so forth. The benefits of using indirect pointers is that it allows us to continue using relatively small inodes, while at the same time addressing really large files. The downside is that this has an implication on slowing down the file access with direct pointer when we need to perform an access to a portion of the file. We need to first access the inode itself and that may be stored somewhere on disk, and then we will find out what is the pointer to a particular data block, and we will access the data block. So we mean the performing 2 disk accesses per file access and that's at most. With the double indirect pointers, the situation is very different. We need one disk access to get to the block that contains the inode. Then we may need another disk access to get to the first addressing block. Then, from there, a second disk access to get to the second level addressing block, and then finally we get to the block that contains the data. So for a single file operation, we may end up performing up to 4 disk accesses. That's a 2x performance degradation. And inode has the following structure and every single one of the block pointers, both the direct and indirect ones, is 4 bytes long. If in the system a block on disk is 1 kilobyte, what is the maximum file size that can be supported with this inode structure? You should write your answer, rounded up to the nearest gigabyte. Also answer, what is the maximum file size if a block on disk is now 8 kilobytes. In this case, provide the answer to the nearest terabyte. To answer this question we will need to add up the sizes that can be addressed with every single type of the different pointers that are included in the inode data structure. This includes the 12th direct disk block pointers, and then the one single indirect pointer, the one double indirect pointer, and the one triple indirect pointer. The answer to the first question is 16 gigabytes. Remember, we have 1 kilobyte blocks, and every single block pointer is 4 bytes. So, with a single block, again, we address 256 pointers. Now, if we think about what is the total number of file blocks that are addressed, their direct pointers will address 12 file blocks. Then the single indirect will address another 256 of those. The double indirect will address 256 square. Triple indirect 256 cube. And all of that needs to be multiplied by the actual size of the blocks. So that's 1 kilobyte. That will produce a maximum file size of 16 gigabytes. The answer to the second question is 64 terabytes. Remember here we have 8 kilobyte block sizes, and given that the pointer size again is 4 bytes, each one of the blocks can contain 2k pointers. Now if we do the math to compute what is the total number of blocks that can be addressed with this inode, we will come up with this calculation and multiply that by 8 kilobytes again, that is the block size will produce the answer of 64 terabytes. So, although we've only increased the block size from 1 kilobyte to 8 kilobyte, because of this non-linear data structure that's used to address the blocks, we're able to achieve much larger file sizes that can be supported by these classes. File systems use several techniques to try to minimize the access to disk and to improve the file access overheads. Much like hardware caches are used to temporarily hold recently used data and avoid accessing main memory, file systems rely on buffer caches, except these buffer caches are present in main memory and they're used in order to reduce the number of necessary disk accesses. With caching the file will be read or written to from the cache and periodically any changes to the file that have not been backed up on permanent storage will be flushed from memory to disc, by forcing these flushes to happen only periodically, we can amortize the cost of performing a disc access over multiple intermittent rights that will hit the cache. File systems support this operation using the fsync system call, another component that helps reduce the file access overheads is what we call I/O scheduling, this is the component that orders how disk access operations will be scheduled. The intent is to reduce the diskette movement that's a slow operation and we can do that by maximizing the number of sequential accesses, which are needed, so, for sequential access, this kind of movement is not expensive It's expensive for these random accesses, so that's what we want to avoid. What that means is, let's say we have two operations that have been issued, write block 25 followed by write block 17 and if the disk head is at position, let's say 15, these operations will be reordered by the I/O scheduler so that they're issues as write first block 17 and then block 25, and this will achieve this objective of maximizing sequential and minimizing random accesses. Another useful technique is prefetching, since for many workloads there is a lot of locality in how the file is accessed, it is likely that if one data block is accessed, the subsequent blocks will be accessed as well. File systems can take advantage of this feature by prefetching more than one block of a file whenever a single block is accessed, this does use up more disk bandwidth to move larger, in this case, three x worth of data from disk into main memory. But it can significantly impact, or reduce, the access latency, by increasing the cache hit rate because more of the accesses will be served out of cache, potentially. Finally, another useful technique is journaling, I/O scheduling reduces the random access, but it still keeps the data in memory, so, these blocks, 17 and 25, are still in memory waiting for the I/O scheduler to interleave them in the right way. That means that, if something happens and the system crashes, these data blocks will be lost, so, we want to make sure that the data ends up on disk, but we still want to make sure that we reduce the level of random access that's required. This is where journaling helps, as opposed to writing out the data in the proper disk location, which will require a lot of random access we write updates in a log, so, the log will contain some description of the write that's supposed to take place. If we specify the block, the offset, and the value, that essentially describes an individual write, now, I'm over trivializing this there is a little bit more that goes into it, but this is overall, the nature of the journal link or the logging base systems. The following file systems to execute called the ext3 and the ext4 they're also part of current Linux versions, they use journaling as well as many other file systems. Note that a journal has to be periodically updated into a proper disk location, otherwise it will just grow indefinitely and it will be really hard to find anything. So if we look at these four techniques and summary, every single one of them contributes to reducing the file system overheads and latencies. This is done by increasing the likelihood of accessing data from memory by not having to wait on slow disk head movements, by reducing the overall number of accesses to disk and definitely the number of random accesses to disk. These techniques are commonly used in current file system solutions. In this lesson, we learned how operating systems manage I/O devices. In particular, we talked about the operating system stack for block-based storage devices. And in this context, we also talked about file systems, and how they manage how files are stored on such block devices. For this, I gave as an example, some details of the Linux file system architecture. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In this lesson, we will talk about system virtualization. It's another important component of the system software stack that's involved in the management of the physical resources. We will explain what is virtualization and why it is important. And we will discuss what are some of the key technical approaches that are part of popular virtualization solutions, such Xen and KVM and VMware CSX. In addition, at the end of the lesson we will discuss what are some of the hardware advances that have occurred over the last decade in response to virtualization trends. Specifically in the context of the x86 architecture. A lot of the content in this lecture is also summarized in a paper by Rosenblum and Garfunkel. Virtual Machine Monitors, Current Technologies and Future Trends. Note that this is a paper from 2005, so much more has happened in the virtualization space than what this paper what this paper describes. But it gives, does serve as a useful reference. Virtualization is an old idea. It originated in the 60s at IBM when the norm of computing was that there were a few large mainframe computers that were shared by many users and many business services. In order to concurrently run a very diverse workloads on the same physical hardware, without requiring that a single operating system be used for all of the o- applications for all of the possible purposes, it was necessary to come up with a model where multiple operating systems can concurrently be deployed on the same hardware platform. With virtualization, each of the operating systems that are deployed on the same physical platform has an illusion that it actually owns the underlying hardware resources. Or at least some smaller portion of them. For instance, the operating system may think that it owns some smaller amount of memory compared to the memory that's actually available on the physical machine. Each operating system together with its applications as well as the virtual resources that it pings at us is called a virtual machine, or VM for short. This represents a virtual machine because it is distinct from the physical machine that is natively managed by some software stack. Virtual machines are often referred to as guests such as guest VMs or also referred to as domains, like a guest domain for instance. To support the coexistence of multiple virtual machines on the same physical machine, require some underlying functionality in order to deal with the allocation and the management of the real, physical hardware resources. In addition, it's necessary to provide some isolation guarantees across these VMs. This is provided by the virtualization layer, that is refered to as virtual machine monitor or hypervisor, also. There are multiple ways in which the functionality, that's required by the virtualization layer, can be designed and implemented. And in this lesson we will describe some of the key virtualization related techniques. A formal explanation of Virtualization is given in an old paper by Povek and Goldberg from 1974. There is a link to that paper in the instructor's notes. We'll go through the ideas that are presented in that paper and we will explain how to define Virtualization. The definition states that a virtual machine is taken to be an efficient isolated duplicate of the real machine. Virtualization is enabled through a virtual machine monitor, or VMM. This is the layer that enables virtual machines to exist. As a piece of software, the virtual machine monitor has three essential characteristics. The virtual machine monitor provides an environment for programs that's essentially identical with the original machine. As I said the capacity may differ, however it would be the same kind of CPU that the virtual machine thinks it has. The same types of devices, etc. What this means is that as the, one of the goals of the virtual machine monitor is that it must provide some fidelity that the representation of the hardware that's visible to the, in the virtual machine matches the hardware that's available on the physical platform. Second, programs that run in this environment show at worst only minor decrease in speed. Now, clearly we have a situation where the operating system and its processes in the virtual machine are given only two CPUs as opposed to the four that are available on the physical machine. This won't execute exactly at the same level, it wouldn't be able to complete the programs as fast as if they were in natively. However the goal of the virtual machine monitor is that if the virtual machine were given the exact amount of resources as the physical machine, then the operating system, and the processes, would be able to perform at the same speeds. So another goal of the virtual machine monitor, is to deliver performance to the VMs that's as close to the need of performance as possible. And last, the VMM is in complete control of the system resources. This means that the virtual machine monitor has full control to make decisions, who accesses which resources and when, and it can be relied upon to ensure safety and isolation among the VMs. This doesn't mean that every single hardware access has to be inspected by the VMM layer. Instead, what this means is that the virtual machine monitor determines if a particular VM is to be given direct hardware access. And also that, once those decisions are put in place, a virtual machine can not just change those policies, and potentially hurt other collocated VMs. So one of the goals for the virtual machine monitor is that it has to provide for safety and isolation guarantees. Based on the classical definition of virtualization that's provided by Popec and Goldberg, which of the following do you think are virtualization technologies? The choices are Virtual Box, the Java Virtual Machine, and Virtual GameBoy, which allows you to run Nintendo GameBoy games on your computer. we have included links in the instructor notes in case you want to find out more about these choices. You should check all the answers that applied that are correct for this question. The only correct answer is Virtual Box, but let's talk about why. First you must recognize that the goals of the system or platform virtualization are different than what's intended by the 'ava Virtual Machine or hardware emulator, as in the case of Virtual GameBoy. Remember that according to Popek and Goldberg, a virtual machine is an efficient isolated duplicate of the real machine. GDM is a language run time which provides system services and portability to Java applications, that it is very different than the underlying physical machine. Also, emulators like virtual GameBoy, they emulate some hardware platform, the GameBoy platform, but again, this is very different than the hardware platform where this emulator is running. With system virtualization, this is, in the case of Virtual Box, the physical hardware that's visible to the virtual machine is identical, or at least very, very similar, to the physical platform that actually is supporting the execution of the virtual box, yes. So why do we care about Virtualization? Well, first it enables consolidation. And consolidation is this ability to run multiple virtual machines, with their operating systems and applications, on a single physical platform. Consolidation then leads to improved cost efficiency. With pure machines, with less space, potentially with fewer admins, and with fewer electrical bills, we will be able to run the same kind of workload. So consolidation delivers benefits because it allows us to decrease costs, and also to improve the manageability of the system. In addition with Virtualization, once the operating system in its applications are nicely encapsulated in a VM, and decoupled from the actual physical hardware. It becomes more easy to migrate the OS in the applications from one physical machine to another physical machine. Or even to copy and clone them onto multiple physical machines at the same time. As a result, Virtualization leads to mechanisms that are useful in order to provide greater availability of the services. Like, for instance, if we have an increase in the load, we can just, create multiple VMs and address that issue. So, increase the availability of the system. And also to provide solutions that improve the reliability. For instance, if we detect that a particular hardware node is getting hot and likely will fail, we can just migrate those VMs onto some other physical nodes. There are other benefits to Virtualization. For instance because the OS and the applications are nicely encapsulated in a virtual machine. It becomes more easy to contain any kinds of bugs, or any kinds of malicious behavior, to those resources that are available to the virtual machine only, and not to potentially affect the entire hardware system. Speaking of debugging, in particular, Virtualization has delivered some other benefits, that it has become a very important platform for operating systems research. It lets systems researchers quickly introduce new operating system feature and has them in the OS that's encapsulated in the VM. And then they have ability to more quickly view the effects of that and debug it as opposed to a more traditional cycle, which would have included hardware restarts of the machines and then searches through the log files or the error files, etc. Virtualization is also useful because it provides affordable support for legacy operating systems. With Virtualization it's no longer necessary to designate some hardware resources for some older operating system just because it's needed to run one particular application, for instance. Instead, that legacy OS and applications can be packaged in a virtual machine and then can be co-allocated, consolidated, on the same hardware resources that support other VMs and other applications. And there are many other benefits to Virtualization. These are some of the key ones, though. Let me ask a question now, if virtualization has been around since the 60s, why do you think it hasn't been used ubiquitously since then. Here are the possible choices. Virtualization was not efficient. Everyone used Microsoft Windows. Mainframes were not ubiquitous. Or other hardware was cheap. You should check all that apply. The correct answers to this question are that, the fact that virtualization wasn't dominant technology all these years is that first mainframes weren't ubiquitous, and then other hardware was cheap. Majority of the companies did not necessarily run mainframe computers. They ran servers that were based on x86 architecture mostly. This was affordable, and it was always much simpler to just add new pieces of hardware than to try to figure out how to make multiple applications and multiple operating systems coexist on that same hardware platform. This trend of just buying more machines if you need to run a different kind of operating system to support different applications continued for a few decades actually. Let me ask a second question now. If virtualization was not widely adopted in the past, what changed? Why did we start to care about virtualization all of a sudden? The choices are, servers were under utilized, data centers were becoming too large, companies had to hire more system admins, or companies were paying high utility bills to run and cool the servers. You should check again all of the choices that apply. Using the model of just buying new hardware whenever there is a need to run a slightly different operating system or to support slightly different applications, in that process, data centers became too large. At the same time, some of these servers were really very underutilized. In fact, on average, the utilization rates in data centers were around 10, 20% tops. So as a result, companies, now that they had to manage these large data centers with lots of machines, they had to hire more system admins. So this is a correct choice. And at the same time, they had to spend more money to host all of those machines, to power them, so they have higher electric bills, to cool the data centers since the machines need to operate within certain temperature variability. So this was basically burning through their operating budget. The fact that all of these choices are correct also translated at that time of something like companies spending 70% of their IT budget on operating expenses versus on capital expenses, like actually buying new hardware or new software services. So then it became apparent that it was important to revisit virtualization technology as a mechanism for consolidating some of these workloads on fewer hardware resources that will be easier to manage them and more cost-effective to actually run them. And this is why the industry and the community overall revisited these solutions that were in existence for certain types of platforms for decades at the time. Before describing the technical requirements for virtualization, let's take a look at the Two Main Virtualization Models. The two popular models for virtualization are called Bare-metal or Hypervisor-based and Hosted. They are also often referred to as (type 1) for the Hypervisor-based model and (type 2) for the Hosted model for virtualization solutions. The Bare-metal model is like what we illustrated before. A virtual machine monitor or hypervisor is responsible for the management of the physical resources, and it supports execution of entire virtual machine. One issue with this model are devices. According to the model, the hypervisor must manage all possible devices, or stated differently, device manufacturers now have to provide the device drivers, not just for the different operating systems, but also for the different hypervisors that are out there. To illuminate this, the hypervisor model typically integrates a special virtual machine. Like a service VM that runs a standard operating system and has full hardware privileges to access and perform any kind of hardware manipulation just like if you were in need of BMV hardware. It is this privilege VM then that would run all of the device drivers and would have control over how the devices on the platform are used. The service VM also runs some other management tasks and configuration tasks that specifies exactly how the hypervisor would share the resources across the guest VMs, for instance. This model is adapted by the Xen virtualization solution, as, and also by the VMware's hypervisor, the ESX hypervisor. Regarding Xen, both when it comes to the open source version, as well as the version that's supported by Citrix, the Xen server. The VMs that are run in the virtualized environment are referred to as domains. The privileged domain is called dom 0, and the guest VMs are referred to as domUs. Xen is the actual hypervisor and all of the drivers are running in the privileged domain, in dom 0. So the management of all of the devices has to involve the execution of the drivers that are part of dom 0. Given that VMware and its hypervisors were first to market, VMware still owns the largest percentage of virtualized server cores. So these server cores run the ESX hypervisor. Even this fact, viewers in a position to actually mandate from the vendors that they do provide the drivers for the different devices. That are going to be part of the hypervisor. Now, this is not as bad because this is really targeting the server. Part of the market space and in servers in data centers there not going to be that many devices or there are going to be relatively a fewer devices compared to what you would see on your laptop or desktops or in general the client platforms. To support a third party community of developers VMware actually also exports a number of APIs. So this not just for the sake of the developers, but also for users when they want to configure exactly the kinds of policies that will be enforced by the hypervisor. And in the past the ESX architecture was such that there was a control core, a core domain, if you will, that was based on a regular operating system, it was based on Linux. But the right now, all of the configuration related tasks are configured via remote APIs. The second model is called the Hosted model. In this model, at the lowest level, there is a full fledged host operating system that manages all of the hardware resources. The Host OS integrates a virtual machine monitor module, that's responsible for providing the virtual machines with their virtual platform interface and for managing all of the context switching scheduling, etc. As necessary, this VMM module will invoke drivers or other components of the host operating system as needed. This one benefit of this model is that it can leverage all of the services and mechanisms that are already developed for the host operating system. Much less functionality needs to be redeveloped for the VMM Module in this manner. Also, note that on this host operating system, you may run Guest VM's through the Virtual Machine Module, but you can also run Native Applications directly on the host operating system as you would in general. One example of the Hosted model is KVM, which stands for kernel-based VM. That's based on the Linux operating system. The Linux host provides all aspects of the physical hardware management and just like any regular OS, it can run directly regular Linux applications. The support for running Guest Virtual Machines is through a combination of our kernel module, that's the KVM module and a hardware emulator called QEMU. We said that the goal of virtualization is to provide identical hardware. So here, this emulator is used as a virtualizer. It really matches the underlying hardware resources, the X86 Intel or AMD. The support for running SVMs in KVM is through a combination of a kernel module KVM and a hardware emulator, QEMU. We said that virtualization the intent is to provide identical hardware, so this QEMU emulator isn't emulating some bizarre hardware platform. Instead, it's used in what's called a virtualizer mode. So the resources that are available to the Guest VM are actually the exact hardware resources from the physical platform, except that this virtualizer intervenes during certain types of critical operations or specific instructions or re, relative to pass control to the KVM Module and the Host OS. One example of that would be any aspect of IO management, because all of the support for devices, the device drivers are handled as part of the Linux operating system the Host OS. A huge benefit for KVM has been that it's able to really leverage all of the advances that are continuously being contributed to the large Linux open-source community. Because of this KVM can quickly adapt to new hardware features, new devices, new security, bugs or similar things. In fact, the KVM Module was originally developed as a Linux module in order to allow regular use of Linux applications to take advantage of some of the virtualization related hardware that started appearing in commodity platforms. All of the sudden, users realized that this can be useful to actually run guest operating system and regular virtual machines. And so three months later, KVM was an actual virtualization solution that was part of the mainstream Linux kernel. For a quiz, I would like you to do a quick survey of some virtualization products. The question that you need to answer is, do you think that the following virtualization products are based on bare-metal or hypervisor-based virtualization, or host-OS-based virtualization? The product you need to look at are KVM, Fustion, VirtualBox, VMware Player, VMware ESX, Citrix XenServer, and Microsoft Hyper-V. And you should mark HV for hypervisor-based solutions or OS if you think the solution is host-OS-based. As we stated earlier, VMware ESX and the Citrix XenServer are both hypervisor- based solutions. So is Microsoft's Hyper-V. All of the other products are hosted. However, it's important to note that at KVM, the host OS switches to a, a mode, a module in order to assume a hypervisor-like role. So, the rest of the operating system really plays a secondary supporting role like, like a privileged partition. So before we go any further, let me ask you one question based on what we've learned so far. Which of the following do you think are virtualization requirements? Here are some possible choices. Present the virtual platform interface to the guest VMs. Provide isolation across the guest VMs. Protect the guest operating system from the applications that are running in the VM. Protect the hypervisor or the virtual machine monitor from the guest operating system. Among these choices, you should check all of the ones that apply. If you've marked all of these as correct answers, then you're correct, and I'll explain why. First, at the lowest level, we said that the virtual machine monitor must provide guest VMs with a virtual platform interface to all of the hardware resources, the CPU, the memory, the devices. So this clearly is a requirement. Obviously, the virtual machine monitor will have to isolate guest VMs from one another. And this actually can be pretty easily achieved using the similar kinds of mechanisms that are used by operating systems to provide isolation across the guest VMs. So, the hypervisor will use techniques or the virtual machine monitor will use techniques like preemption, will take advantage of hardware support in the memory management unit so that it can perform validations and translations of memory references pretty quickly. So, there are opportunities to achieve this requirement efficiently using the existing methods and the existing hardware support. Also, within the virtual machine at the topmost level of the stack, the virtualization solution must continue to provide the ability to protect the guest operating system from faulty or malicious applications. We don't want a single application, when it crashes, to take the entire guest OS down. What this means is that somehow we have to have separate protection levels for the applications and for the guest OS. So these expectations that exist when the guest OS is executing natively on the physical platform, they must continue to be valid in the virtualized environment. At the same time, the virtualization solution has to have mechanisms to protect the virtual machine monitor from the guest operating systems. We don't want a single faulty or malicious guest OS to bring down the hypervisor in the entire machine. What this means is that we cannot have a solution in which the guest operating system and the virtual machine monitor run at the same protection level. They have to be separated. When thinking about how to address the virtualization requirements that we just mentioned in the previous quiz, it is fortunate to observe that commodity hardware actually has more than two protection levels. Looking at the architecture that's, at least in the server space, most dominant, the x86 architecture, there are four protection levels called rings. Ring 0 has the highest privilege and can access all of the resources and execute all hardware-supported instructions. And this is where in a, a native model, the operating system would reside. So, when the OS is in control of all the hardware resources, it sits in ring 0. In contrast, ring 3 has the least level of privilege, so this is where the applications would reside. And whenever the applications try to perform something, some operation for which they don't have the appropriate privileges, then a trap would be caused, and control would be switched to the ring 0, to the lowest privileged level. One way in which these protection levels can be used is to put the hypervisor now in ring 0, so that's the one that has full control over the hardware, to leave the applications to execute at ring 3 level, and then the operating system would execute at ring 1 level. We'll explain how this actually works in the following video. More recent x86 architectures also introduce two different protection modes called root and non-root. Within each of these modes, the four protection levels exist, so there are like two times these protection rings. Now, when running in root mode, all of the operations are permitted, all hardware is accessible, all instructions can be executed. So, this is the highest privilege level. And this is, the ring 0 of the root mode, is where we would run the hypervisor. In contrast, in non-root mode, certain types of operations are not permitted. So then, the guest VMs would execute in this non-root mode. And they would have, as they did in the native execution, their applications running in ring 3 and the operating system running at ring 0 privilege level. Attempts by the guest operating system to perform privileged operations cause traps that are called VMexits. And these trigger a switch to this root mode and pass control to the hypervisor. When the hypervisor completes its operation, it passes control back to the virtual machine by performing a VMentry which switches the mode into non-root mode, to ring 0, so that the execution continues Now that we understand how hardware-supported protection levels can be used, we can start explaining how virtualization techniques can be developed that achieved our goal to efficiently at near native speeds allow execution of virtual machines on top of these basically identical virtual platforms. First, guest instructions are executed directly by the hardware. That's important thing to know. The virtual machine monitor does not interfere with every single instruction that is issued by the guest operating system, or its applications for that matter. What this means is that just like the OS doesn't interfere on every single instruction and memory access, here the hypervisor does not interpose itself on every single operation and every single memory access performed by the guest. As long as the guest operating system and its applications operate within the resources that were allocated to them by the hypervisor, then everything is safe. The instructions in those cases will operate at hardware speeds and this will lead to efficiency of the virtualization solution. Whenever a privileged instruction gets accessed, then the processor causes a trap, and control is automatically switched to the most privileged level, so to the hypervisor. At this point the hypervisor can determine whether the operation is to be an allowed or not. If the operation is an illegal operation and it shouldn't be allowed, then the hypervisor can perform some action like to terminate the VM. If the operation is, should be allowed, however, it's a legal operation, in that case, the hypervisor should perform the necessary emulation so that the guest operating system is under the impression that it actually does have control over the hardware. So from the guest perspective, it should seem as if the hardware did exactly what it was expected to do given the instruction. In reality, however, it was the hypervisor that intervened, that potentially executed slightly different set of operations in order to achieve that emulation. This trap-and-emulate mechanism is a key method in which virtualization solutions rely in order to achieve efficient CPU virtualization. Although the trap-and-emulate model seems like it will solve all problems and it worked beautifully on the mainframes, when in the 90s, the need to reapply virtualization solutions to the prevalent x86 architecture came up, it turned out that there were certain problems with this model. At the time, x86 platforms had just the 4 rings. There wasn't any support for root or non-root mode. And so the way to virtualize them would be to run the hypervisor in ring 0 and the guest OS in ring 1. However, it turned out that there were exactly 17 instructions, that were privileged in that hardware would not allow them to be executed if they're not issued from the most privileged ring 0. However, they did not cause a trap. Issuing them from another protection level from ring 1 or above wouldn't pass control to the hypervisor and instead would just fail silently. For instance enabling or disabling interrupts requires manipulating of a bit in a privileged flags register and this can be done via the POPF/PUSHF instructions. However, when these instructions are issued from ring1 in the Pre 2005 architecture, they just fail and the instructions pipeline is allowed to continue to the next instruction. The problem with the situation that there is no trap is that since control isn't passed to the hypervisor, the hypervisor has no idea that the OS wanted to change the interrupt status. So, the hypervisor will not do anything to change these settings, will not emulate the behavior that was required, that was intended with this instruction. At the same time because the failure of the instruction was silent, the operating system, the guest OS, doesn't know that anything wrong happened, so the OS will continue its execution, assuming that correctly the interrupts were enabled or disabled as intend. So the OS will then go ahead and perform operations that, for instance, if interrupted can leave it in corrupt or in deadlock state. Which was intended to be avoided by some manipulation of this flags register to disable interrupt. So clearly this is a major problem and makes this trap-and-emulate approach not applicable for these architectures. Let's explain the problem with some of these problematic instructions a little more via quiz. So we said that in the earlier x86 architectures, the CPU flags privileged register was accessed via instructions POPF and PUSHF. And these instructions failed silently if they're not accessed from the most privileged ring, ring zero. This is where the hypervisor would reside. What do you think can occur as a result of this situation? The options are, the guest VM could not request interrupts to be enabled, the guest VM could not request interrupts to be disabled, the guest VM could not find out what is the state of the interrupts enabled/disabled bit, or all of the above. The correct answer is all of the above. To perform any of these operations requires access to this privileged register and requires execution of these instructions. When these fail silently, the guest will assume that the request completed, and may end up interpreting some other information that's on the stack incorrectly, as if that's information that's provided by that register. So none of these will be successful. One approach that was taken to solve the problem with the 17 instructions, was to rewrite the binary of the guest VM so that it never really issues any one of these operations. This process is called binary translation. This approach was pioneered by research at Stanford University, by a group led by Professor Mendel Rosenblum. And subsequently, this was commercialized as VMware. Now some 15 plus years and 30, $40 billion later, VMware still owns by far the largest share of the virtualized cores in the server market. Rosenblum later received the ACM Fellow reward, and in the recognition he was specifically credited for reinventing virtualization. He served as VMware's chief scientist for about ten years and now is back full time at Stanford. Let me give you now a brief description of what binary translation actually is. A key thing to note is that the goal that's pursued by VMware is to run unmodified guest operating systems. Meaning that we don't need to install any special drivers, or policies or otherwise to change the guest OS in order to run in a virtualized environment. As a startup, they clearly couldn't tell Microsoft to modify Windows so that VMware can improve it's, it's success rate. So, this type of virtualization where the guest OS is not modified is called full virtualization. The basic approach consists of the following. Instruction sequences that are about to be executed are dynamically captured from the VM binary, and this is typically done at some meaningful granularity like a basic block such as a loop or a function. Now the reason that this is done dynamically versus statically, so up front before any code is actually run, is because the exact execution sequence may depend on the parameters that are available at runtime. So it's input dependent. So you cannot really do all of this in an efficient way statically up front. Or in some cases you just cannot do it all because you don't have the input parameters. So then you dynamically capture these code blocks and then inspect them to see whether any of these 17 infamous instructions is about to be issued. If it turns out that the code block doesn't have any of these bad instructions, it's marked as save and allowed to execute natively at hardware speeds. However, if one of the bad instructions is found in the code block, then that particular instruction is translated into some other instruction sequence that avoids the undesired instruction and in some way, emulates the desired behavior. This can possibly be achieved, even by bypassing a trap to the hypervisor. Certainly, binary translation adds overheads, and the number of mechanisms are incorporated specifically in the viewer solutions, in order to improve the efficiency of the process. These things include mechanisms such as, caching code fragments that correspond to the translated basic blocks. So that the translation process can be avoided in the future. Also, the steps like distinguishing which portions of the binary should be analyzed. For instance, distinguishing between the kernel and the application code and making sure that the kernel code is the one that's analyzed and various other optimizations. A very different approach is to give up on the goal of running unmodified operating systems. Instead, the primary goal is to offer a virtualization solution that offers performance and avoids some of the overheads that may be associated with any of the complexities that are necessary to support unmodified guests. In contrast to full virtualization, this is called paravirtualization. With paravirtualization, the guest OS is modified so that it now knows that it's running in a virtualized environment on top of a hypervisor as opposed to on top of native physical resources. A paravirtualized guest OS will not necessarily try to directly perform operations, which it knows that they will fail. And instead, it will make explicit calls to the hypervisor to request the desired behavior. Or specifically, the desired hardware manipulations. These calls to the hypervisor are called hypercalls. And they behave in a way that's similar to the way system calls behave in an operating system. So the unprivileged guest OS here that's modified will package all relevant information about its context, its current state. And it will specify the desired hypercall. And at that point, it will issue the hypercall and that will trap to the virtual machine monitor. When the hypervisor completes the operation, control will be pass back to the virtual machine, to the guest, and any data, if appropriate, will be made available to it. This approach of paravirtualization was originally adapted and popularized by the Xen hypervisor. This was a popular virtualization solution and originally was an open source hypervisor that started as a research project at University of Cambridge in the UK. This was later commercialized as XenSource and XenSource is now owned by Citrix. But there still remains a lot of activity in the open source Xen project, including at our own research group here. One thing to note, however, is that the open source Xen version as, and the Citrix Xen version have diverged perhaps substantially over time Let me ask a question now, which of the following do you think will cause a trap and will exit the VM and pass control to the hypervisor for both binary translation and for paravirtualized VM's? The options are, access to a page that's been swapped, or update to a page table entry. The first option is correct. If a page is not present, it will be the hardware MMU that will fall, and it will pass control to the hypervisor regardless of the virtualization approach. For the second option, update to a page table entry, this is not always true. It really depends on whether the OS has write permissions for the page tables that it uses or not. We'll see in the next videos how this can be handle. So far we've focused on explaining the basics of how to virtualize efficiently the CPU, but let's now look at the other types of resources looking at memory first. We will explain how memory virtualization can be achieved for the two basic virtualization approaches, whether it's based on full virtualization, or requires guest modification and we will talk about full virtualization first. For full virtualization a key requirement is that the guest operating system continues to observe a contiguous linear physical address space that starts from physical address zero, this is what an operating system with c if it actually own the physical memory and run natively on physical hardware. To achieve this we distinguish among three types of addresses, virtual addresses, so these are the ones that are used by the applications in the guest. Physical addresses, these are the ones that the guest thinks are the addresses of the physical resource and the machine addresses, these are the actual machine addresses with the actual physical addresses on the underlying platform. The similar distinction of virtual verses physical verses machine will also apply to the page numbers and the page frame numbers. So given this, the guest operating system can continue make mappings of virtual addresses to of the physical addresses that it thinks it owns, and then underneath that the hypervisor will then pick these physical addresses that the guests believes are the real ones and map them to the real machine addresses. So in a sense they're two page tables, one that's maintained by the guest operating system, and another one that's maintained by the hypervisor. Now remember that at the hardware level, we have a number of mechanisms, the memory management unit, the TLB caching of the address translations, that these mechanisms help with the address translation process, make it much more efficient, and don't require us, in software, to repeatedly perform address translations and validations. Now this option that we discussed so far will require that every single memory access goes through two separate translation, the first one which will be done in software, and then the second one potentially can take advantage of hardware resources like TLB because the hardware will understand only this page table. Clearly this will be too expensive since this will add over hands on every single memory reference, it will slow down the ability to run at near native hardware speeds. The second option is for the hypervisor to maintain a shadow page table, in which it actually looks at what are the virtual addresses that the guests has mapped to these physical addresses. And then in the shadow page table it directly establishes a mapping between the virtual addresses that are used by the guest, and the machine addresses that are used by the hypervisor by the physical hardware. Then if the hardware MMU uses this page table, the guest operating system is allowed to execute natively using the applications will use virtual addresses, and these will be directly translated to the machine addresses that are, used by the physical hardware. The hypervisor will clearly have to be responsible to maintain consistence between these two page tables and it will have to employ mechanism that for instance invalidate, what is the currently valid page tables, shadow page table whenever there is a, context wedge or to write protect the guest page table, in order to keep track of new mappings that the guest operating system install since similar mechanism. This write protection is necessary so that whenever the guest OS tries to install new virtual to physical address mapping in the page tables that are used by the guest, this will cause a trap to the hypervisor, and then the hypervisor will be able to pick up that virtual address and then associate the corresponding machine address and insert this mapping into the page table that is used by the hardware MMU. This can be done completely transparently to the guest operating system. In contrast, in paravirtualized systems, the operating system knows that it's executing in a virtualized environment. Because of this, there is no longer a strict requirement for the guest OS to use contiguous physical memory that starts at zero. And the guest OS can explicitly register the page tables that it uses with the hypervisor so there is no need for maintaining dual page tables, one of the guest and then another shadow one at the hypervisor level. Now, the guest still doesn't have write permissions to this page table that's now used by the hardware because otherwise the guest potentially can establish any mapping and corrupt other VMs that are running on the same system. So, because of that, every update to the page table would cause a trap and pass control to the hypervisor. But because the guest is paravirtualized, and we can modify the guest and do tricks like batch a number of page table updates and then issue a single hypercall in this case to tell the hypervisor to install all of these mappings. So this can amortize the cost of the exit across multiple operations. There can be other optimizations that are useful. For instance, optimizations related to how the memory's managed so that it's more friendly to execution in a virtualized environment or so that it's more cooperative with respect to other VMs in the system and other things. One thing to note that the two mechanisms that I described with respect to memory virtualization for both full as well as para, paravirtualized VMs have substantially been improved given advances in the new hardware architectures. So, some of these overheads have completely been eliminated or at least substantially reduced if we take a look at what's happening at the newer generation of x86 platforms. And we will talk about that shortly in this lesson. When we talk about Virtualization. When we look at CPUs in memory. Certain things are relatively less complicated, in spite of everything that we said, so far. Because there is a significant level of standardization at the instruction set architecture, across different platforms. So then from a Virtualization standpoint, we know that we have to support a specific ISA. And then we don't care if there are lower level differences between the hardware because it's up to the hardware manufacturers to be standardized at the ISA level. This clearly is the case for a particular ISA for instance, for x86 things will be different across x86 and for instance MIPS platforms. When we look at devices however, there is a much greater diversity in the types of devices, if we compare that to the types of CPU instruction sets. And also there is lack of standardization when it comes to the specifics of the device interface and the semantics of that interface, so what is the behaviour when a particular call is invoked. How a particular device should respond to a call to send the packet for instance. To deal with this diversity, Virtualization solutions adopt one of three key models, to virtualize devices. We will describe these models, next. And note that they were developed before any Virtualization friendly hardware extensions were made to the, CP architectures and the chip sets. And these new modifications of the hardware make some aspects of device Virtualization much simpler than than what originally it was when these models were first introduced. One model is the so-called Passthrough Model. The way the pass through model works is that the VMM level driver is responsible for configuring the access permissions for device. For instance, it will allow a Guest VM to have access to the memory where the control registers for the device are. There are clearly benefits to this approach, one is for instance, that the guest VM has exclusive access to a particular device. It's the only one that can manipulate its state, it can control it and it's the only one that will use it. Also, the VM's accesses to the device completely bypass the hypervisor, so there are direct accesses to the device. This model is also called the VMM bypass model. Now clearly, once we start providing VMs with exclusive access, figuring out a way to share devices will become difficult. Well, basically have to continuously reassign, which particular VM it can access a particular device over time. But the sharing will not happen simultaneously, concurrently. That in some cases is, is really not doable, because of the limitations of the device. In other cases, it can be done, but it will be very high overhead operations. So, in practice really device sharing with this model is not feasible. Now because the hypervisor is completely out of the way, it means that the Guest VM and the Device Driver that's in the Guest VM, directly operates and controls the particular physical device. So that means that when we're launching this Guest VM, there better be a device of the exact same type as expected by the Guest OS on the physical machine. In some cases, maybe in the server space that's not as critical of a requirement just because there are fewer types of devices that are commonly present there. But in other environments, this is really not a practical constraint. Remember, we're not talking about the fact that there needs to be a network interface, or a disk device or hard disk device. We're talking about the exact same, particular type of network card or hard disk drive that the Guest VM expects depending on the device drivers that it has. Also, we mentioned that one of the benefits of virtualization is that the Guest VM started decouples from the physical hardware. And therefore, we can migrate them easily to other nodes in the system. Well, this pass through really breaks that decoupling, because it directly binds a device to a VM. This makes migration difficult in particular, because there may be some device specific state and potentially even device resident state that would also need to be copied and migrated and then properly configured at the destination mount and then basically that turns VM migration not in a hyperviser and VM specific operation. But it needs to be implemented in a way that knows how to deal with the device specifics of all of the particular devices that are of interest. The second model for providing virtualization of physical devices is to allow the hypervisor to first intercept every single possible device access that's performed by a guest VM. Once the hypervisor has the information about the device operation that the guest VM wanted to perform, it doesn't have a requirement that the device that the guest VM wants and the physical device match. So the hypervisor would first translate that device access to some generic representation of an I/O operation for that particular family of devices, whether they are networked devices or disk devices. And then it will traverse the hypervisor resident, the VMM resident I/O stack. So the bottom of that stack is the actual real device driver, and the hypervisor will invoke that device driver and perform the I/O operation on behalf of the guest VM. Clearly, a key benefit of this approach is that now the virtual machine is again decoupled from the physical device. Any translation, any emulation, will be performed by the hypervisor layer. And because of that, operations such as sharing and migration, or the requirements of how we need to deal with device specifics, all of that becomes simpler in a sense. This is a model that's originally adapted by the VMware ESX hypervisor. The downside of the model is that it clearly adds latency on device accesses because of this emulation step. And then also it requires that the device driver ecosystem is in certain ways integrated with the hypervisor. Now the hypervisor need to support all of the drivers so that it can perform the necessary operations. And the hypervisor is then exposed to all of the complexities of, and bugs of various device drivers. Again, we said earlier in the case of VMware, because of its market share, this model was a reasonable model, and it made sense and it's been sustained because of that. A third model to device virtualization is the so-called S0plit-Device Driver Model. This model is called split, because all of the device accesses are controlled in a way that involves both a component that resides in the Guest Virtual Machine and also a component that resides in the hypervisor layer. Specifically, devices accesses are controlled using a device driver that sits in the Guest VM called the front-end device driver. And then the actual driver for the physical device that is the regular device driver that's used by operating systems when they run natively. In this back-end driver, referred to as back-end resides in the service VM in the case where we have a virtualization model that involves a service VM or the host operating system in the case of type two virtualization. The point is that this back-end should really be just the regular device driver that the service, the Linux OS for instance and the Service VM would just be able to install and use, even if it's not running in a virtualized environment. Now although this back-end driver does not necessarily have to be modified, the front-end driver has to be modified, because it explicitly needs to take the device operations that are made by the applications in the guest and then put them together in something that looks like a, a special message that will directly be passed to this back-end component that's in the service VM. So this approach, essentially applies only to paravirtualized guests that will be able to explicitly install these special front-end device drivers. What these really are, they look like wrappers for the actual device API. So the applications potentially, they don't have to be modified. They will continue making the same kinds of requests for device operations. But this front-end device driver will treat these operations, especially will not attempt to actually shoot off access to the physical device, instead will create messages that will get passed to the service VM. One benefit of the approach is that it can eliminate the overheads that are associated with device simulation that the previous model required. Now, we don't have to reverse engineer exactly what the Guest OS is trying to do. The Guest OS via its front end device driver explicitly tells the virtualization layer, so overall these two components. What exactly is it that the guest VM requires. Another more subtle benefit of the approach is that now that we have this back-end component that's centralized in that it will re, accept requests from all the Guest VM's in the system and then try to decide which one of those gets to execute the physical device access. There's some opportunities for some better management of the shared devices accesses. So to enforce some finer grained policies regarding fairness, regarding priorities and those sorts of things. The solutions that we described on how to virtualize memory in inode clearly indicate that there is some degree of complexity and overhead that have to be incurred by the due to virtualization. Given the wide recognition that virtualization delivers important benefits and it's been pointed out earlier in this lesson in that it presented an important path to address some of the issues related to rising operating costs in the IT industry. The hardware companies responded and they modified their architectures in a way that makes them more appropriate for virtualization. In the x86 world these virtualization friendly architectures started appearing around 2005. Read AMD Pacifica and Intel Vanderpool Technology or LIntel-VT for short. With respect to x86, so one of the first things that was fixed was to close the holes with respect to those 17 non-virtualizable instructions, so that they will cause a trap and pass control over in a privileged mode. Also, the new protection mode was introduced. So, as opposed to having just one protection mode with four ranks, now there are two protection modes, so root and non-root. Also referred to as host because this is the route is the mode in which the host operating system, the hypervisor would run and the non route, that's also referred to as guest which is where the guest VM would run. Also, a support was added for the processor, the hardware processor to understand and to be able to interpret information that describes the state of the virtual processors called VCPUs. This information is captured in a VM Control Structure or also called a VM control block in the AMD x86 architectures. The fact that the hypervisor understands how to interpret this data, so it can walk this data structure is the term that's commonly used. Means that it can specify whether or not a system call should trap. So, it's easy for the hypervisor to know that a particular type of operation should not cause a trap into root mode and instead should be handled by the privilege layer in the non-root mode, so the privilege layer in the non-root mode is the operating system. Then other pieces of information, then that in a certain way can help reduce the virtualization overhats. The next step in terms of virtualization related advances was to make it easier to manage memory. Since hardware was already able to understand the presence of different VMs, the next step here involved tagging the memory structures used by the hypervisor with the corresponding VM identifiers. So this led to support for extended page tables where the page table entries now include information about the VMI team and also tagged TLBs. What this means is that if there is a context switch am, among VMs that's also called the world switch, when we're switching from one VM to another. We don't have to flush or invalidate those entries that are in the TLB that belong to the previous VM. This is because the MMU, when it performs a check against the TLB entries will try to match both the virtual address that is causing the access request as well as the VM identifier. And if they both match, then it will proceed with the address that's specified in the TLB entry. Otherwise, it will deal with the page fault failures. As a result, context switches are now much more efficient. Hardware was also extended to add better support for IO virtualization and this included modifications both to the processor and the chipset side. And also device and system interconnect capabilities that were introduced in order to support this. Some examples of these features include things like multiqueue capabilities on the device and you can think of this as the device having multiple logical interfaces where each interface can be used by a separate VM. And also better support of interrupt routing, so that when a device needs to deliver an interrupt to a specific VM, it actually interrupts the core where that VM is executing and not some other CPUs. Additional virtualization related hardware features were also included for stronger security guarantees that now can be made to the VMs and also to protect VMs from one another as well as from the hypervisor. And also, features for better management support or for more efficiently to be able to perform various management operations in virtualized environments. You can think of this as more virtualization friendly management interfaces. Also, a number of new instructions were added to x86 in order to actually exercise all of these new features. For instance, a new instruction was introduced to transition from one mode to another. Basically, to transition from root mode or to return control to non-root mode. Or a new instructions to manipulate in certain ways state that's in the per VM control data structure, etc. Let me ask a question now. With hardware support for virtualization, guest VMs can now run unmodified and can have access to the underlying devices. Given this what do you think is the split device driver model still relevant? Answer either yes or no. The answer to this is yes, because with the split device driver model we are consolidating all of the request for device access to the surface VM, where we can make better decisions and enforce stronger policies in terms of how the device is share. Without perhaps relying to specific support for desired behavior, for desired sharing behavior on the actual physical device. And finally I'd like to show you this illustration from Intel that summarizes the advances in the Intel VT architecture over the last decade or so. The lowest set of boxes summarize the impact of the virtualization related hardware modifications that have been added along the three different dimensions. The three dimensions are as follows, vector 1, refers to any CPU related modifications and this includes things like fixing issues with non-virtualizable instructions or adding support to the CPU for extended page tables. Vector 2, refers to modifications pertaining to the entire chipset, such as chipset site support for technologies like what's called SR-IOV, that technologies that help with IO virtualization, or for IO routing and mechanisms for direct device access, or support rather for direct device access. And then the third vector, the third dimension in which Intel has contributed to the evolution of virtualization friendly technology, is to actually push advances in the device level technology. So this is what's happening on the device as opposed to on the chip you, on the chipset or the CPU side. And with this draws devices are more easily integrated in a virtualized environment. This includes things like support on the device for DMA remapping so that it can appropriately DMA into the correct set of memory depending on the virtual machine that it targets. And to also support the multiple queue, the multiple logical interfaces such that different logical interface can be given to a different VM. The architecture versions that encapsulate these modifications are referred to as VT-x for the modifications along the first vector. With a VT-x2, VT-x3 occurring in subsequent generations of processors. Then VT-d for the advances along the second dimension with VT-d following and so forth, and then the architecture modifications along the third vector are encapsulated into what's referred to as IOVs or IO virtualization technology. And VMDq what stands for virtual machine device queue's, so this is device residence support. In this lesson we explained what is virtualization, and what are some of the main approaches that are used to virtualize physical platforms. We specifically described mechanisms for processor and memory, or device virtualization that either were or still are part of the dominant virtualization solutions, such as Zen and KVM or the VM Ware products. In addition, we discussed some of the hardware advances that have occurred as a result of the need to efficiently virtualize the popular x86 platform. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In the previous lessons, we discussed several mechanisms for Inter-process Communication. But we said that these were fairly low-level mechanisms because they focused on providing the basic capability for moving data among address spaces. And didn't really specify anything about the semantics of those operations or the protocols that are involved. In this lesson we will talk about Remote Procedure Calls, or RPC. This is an IPC mechanism that specifies that the processes interact via procedure call interface. For the general discussion of RPCs, we will roughly follow Birrell and Nelson's paper, Implementing Remote Procedure Calls. This is an older paper, but it discusses very nicely the general design space of RPC. Then, we will discuss in more detail Sun RPC. It's a concrete implementation of an RPC system, that's common in operating systems today. To understand why we need RPC, let's look at two example applications. The first one is an application, where a client requests a file from a server. And uses a simple get file protocol that's like HTTP request, but less complex. In this application the client and the server interact using a socket based API. And as a developer you would have to explicitly create and initialize the sockets, then allocate any buffers that are going to be sent via those sockets, and populate them with anything that includes protocol related information. Like for instance, this protocol will have something like, get file directives. And you have to specify the size of the buffer. And also you'll have to explicitly copy the data in and out of these buffers. So copy the file name, string or the actual file in and out of these buffers. Now imagine another application that's also a client server application in which the client interacts with a server to upload some images and that it requests then from the server for these images to be modified. To create a gray scale version of an image, to create a low resolution version of an image, to apply some phase detection algorithm. So it's in some sense similar to get file, but there are some additional functionalities, some additional processing that needs to be performed for every image. The steps that are required from the developer of this particular application are very similar, in fact some of them are identical to the steps that are required in the get file application. One difference is that the protocol related information that would have to be included in the buffers would have to specify things like the algorithm that the client is requesting from the server to be performed. Like whether it's grace key link or whether its some face detection algorithm along with any parameters are relevant for that algorithm. And also the data that is being sent between the client and the server, we said in this case the client uploads an image to the server and then the server returns that image back to the client after this particular function has been performed. That's different than the file name, the string that's being sent from the client to the server, and the actual file that's returned in response. But a lot of the steps end up being identical in both cases. In the 80s, as networks are becoming faster, and more and more of distributed applications were being developed. It became obvious that these kinds of steps are really very common in a related interprocess communications, and need to be repeatedly reimplemented for a majority of these kinds of applications. It was obvious that we need some system solution that will simplify this process that will capture all the common steps that are related to remote interprocess communications. And this key do Remote Procedure Calls or RPC. RPC is intended to simplify the development of cross-address space and/or cross-machine interactions. So what are the benefits? RPC offers a higher-level interface that captures all aspects of data movement and communications, including communication establishment, requests, responses, acknowledgements, et cetera. What this also allows it permits for RPC's to capture a lot of the error handling and automated, and the programmer doesn't really have to worry about that. Or at least the programmer doesn't have to explicitly reimplement the handling of all types of errors. And finally, another benefit from RPC is that it hides the complexities of cross-machines interactions, so the fact that machines may be of different types, that the network between them may fail, that the machines themselves may fail. That will be hidden from the developer. So as a programmer, when using RPC, we don't have to worry about those differences. Let's see what's required from the system software that provides support for RPCs. First, the model of inter-process interactions that the RPC model is intended for needs to manage client/server interactions. A server supports some potentially complex service. Maybe it's running a complex computation but really fast, or maybe it's a file service that services a remote file content. The clients do not need to have the same capabilities or they don't have to be able to perform accesses to the same data. They just need to be able to issue requests to the server for whatever they need. The second requirement has to do with the fact that when RPC was first developed, the state-of-the-art programming languages for procedural languages including Basic and Pascal, and Fortran and C. So this is what programmers were familiar with. One goal of the RPC systems was to simplify the development of distributed applications underneath a procedure called interface. This is why the term remote procedure calls came to be. As a result, RPCs are intended to have similar synchronous semantics. Just like regular procedure calls. What that means is that when a process makes our remote procedure calls, the calling process or at least the calling thread, will block, and wait until the procedure completes, and then returns the result. This is the exact same thing that happens when we call a procedure in a single address base. The execution of the thread will reach the point when the procedure call is made. At that point, we jump somewhere in the address base where the procedure is implemented. The actual original thread of execution will not advance beyond that procedure called point until we get the results from the procedure. And then when we move on to the next step, we act, actually already have the results. So this is what we mean by the synchronous call semantics. And this is what we require from the RPC systems as well. RPCs have other useful features that are similar to regular procedure calls, and one is type checking. If you pass to a procedure an argument of the wrong type, you'll receive some kind of error. This is one useful reason why RPC systems would incorporate type checking mechanisms. In addition, type checking mechanisms are useful because they allow us, in certain ways, to optimize the implementation of the RPC run-time. When packets are being sent among two machines, it's just a bunch of bytes that reach from one endpoint to another. And some notion about the types of the data that's packaged into those bytes can be useful when the RPC run-time is trying to interpret what do these bytes mean. Are they integers or they're file. Do I need to put them together so as to create some kind of image or some kind of array. This is what the type information can be used for. Since the client and the server may run on different machines, there may be differences in how they represent certain data types. For instance, machines may differ in the way they use big endian or little endian format to represent integers. This determines whether the most significant byte of that integer is in the first or the last position in the sequence of bytes that corresponds to the integers. Or machines may differ in their representation of floating point numbers, may use different representations for negative numbers. The RPC system should hide all of these differences from the programmer, and should make sure that data is correctly transported, and it must perform any of the necessary conversions, any of the necessary translations among the two machines. One way to deal with this conversion is for the RPC run-time in both endpoints to agree upon a single data representation for the data types. For instance, it can agree of that everything will be represented in the network format. Then there is no need for the two endpoints to negotiate exactly how data should be encoded, exactly how data should be represented. Finally, RPCs intended to be more than just a transport-level protocol like TCP and UDP that worries about sending packets from one endpoint to another in an ordered reliable way. RPC should support underneath different kinds of protocols, so we should be able to carry out the same types of client-server interactions, regardless of whether the two machines use UDP or TCP, or some other protocol to communicate. But RPC should also incorporate some higher level mechanisms like access control, or authentication, or fault tolerance. For instance, if a server is not responding, a client can retry and reissue their same request to either the same server or it can make an attempt to contact the replica of that original server that it was trying to contact. To illustrate the structure of the RPC system I will walk you through an example. Consider a client and server system. The client wants to perform some arithmetic operations, let's say addition, subtraction, multiplication, but doesn't know how to. The server is the calculator process, and it knows how to perform all of these operations. In this scenario, whenever the client needs to perform some arithmetic operation. It needs to send the message over to the server that specifies what is the operation it wants performed, as well as the arguments. The server is the one that has the implementation of that operation. So it will take those arguments, perform the operation, and then return the results. To simplify all the communications related aspects of the programming, like creating sockets, allocating managing the buffers, for the arguments and for the results, and all the other detail, this communication pattern will use RPC. Let's consider in this example the client wants to perform an addition. It wants to add i and j and it wants to obtain the results of this computation in k. The client doesn't have the implementation of the addition process, only the server knows how to do it. However, with RPC the client is still allowed to call something that looks just like a regular procedure k equals add of i and j. In a regular program, when a procedure call is made the execution jumps to some other point in the address space where the implementation of that procedure is actually stored. So the program counter will be set to some value in that address space that corresponds to the first instruction of the procedure. In this example, when the RPC add is called the execution of the program will also jump to another location in the address space. But it won't be where the real implementation of add is, instead it will be in a stub implementation. >From the rest of the client's process it will look just like the real add, but internally what this stuff does is something entirely different. The responsibility of the client's stub is to create a buffer and populate that buffer with all of the appropriate information. In this case, it's the descriptor of the function that the client wants the server to perform, the add, as well as its arguments, the integers i and j. The stop code itself is automatically generated via some tools that are part of the RPC package so the programmer doesn't have to write this code. So when the client makes the call at here the call takes the execution of the client process into a portion of the RPC run time, and by that we mean the system software that implements all of the RPC functionality. In the first step here is that stub implementation. After the buffer is created, the RPC run time will send a message to the server process. This may be the TCP/IP sockets or some other transport protocol. What we're not showing in this figure is that there is some information about the server machine like the IP address and the port number where this server process is running. That is available to the client. And that information is used by the RPC run time to establish the connection, and to carry out all of the communication. On the server side when the packets are received for disconnection they will be handed off to the server stub. This is a code that will know how to parse and interpret all the received bytes in the packets that were delivered to the stub and it will also know how to determine that this is an RPC request for the procedure add with arguments i and j. The servers stop once it sees that it needs to perform this act. It will know that the remaining bytes need to be interpreted like two integers i and j. So it will know how many bytes to copy from the packet stream, how to allocate data structures for these particular integer variables to be created in the address piece of the server process. Once all this information is extracted on the server side these local variables are created in the address space. The stub is ready to make a call in the user level server process that has the actual implementation of all of the operations, including the add. Only at that point did the actual implementation of the add procedure will be called and the results of the edition of i and j will be computed and stored in a variable in the server process address space at that point. Once the result is computed, it will take the reverse path. It will go through the server step that will first create a buffer for that result and then it will send the response back via the appropriate client connection. That will arrive on the client side into the RPC run time. The packets will be received. The result will be extracted from those packets by the client side stub, be placed somewhere in memory in the client address space, and then ultimately the procedure will return to the client process. For the entire time while this is happening, the client process will be blocked on this add operation will be suspended here. It will not be able to continue. Which is exactly what happens when a client process makes a local procedure call. The execution of the client process will continue only once the results of that procedure call are available. To generalize from the example that we saw in the previous video. We will now summarize the steps that have to take place in an RPC interaction between a client and a server. The first step, a server binding occurs. Here the client finds and discovers the server that supports the desired functionality. And that it will need to connect to. For connection oriented protocols, like TCP/IP that require that a connection be established between the client and the server process, that connection will actually be established in this step. Then, the client makes the actual Remote Procedure Call. This results in a call into the user stub, and at that point the rest of the client code will block. Next, the client stub will create a data buffer, and it will populate it with the values of the arguments that are passed to the procedure call. We call this process, marshalling the arguments. The arguments may be located at arbitrary non-contiguous locations in the client under space. But the RPC runtime will need to send a contiguous buffer to the sockets for transmission. So the marshal link process will take care of this and replace all the arguments into a buffer that will be passed to the sockets. Once the buffer is available, the RPC run time will send the message in the sending will involve whatever transmission protocol that both sides have agreed upon during the binding process. This may be TCP, UDP, or even shared memory based IPC if the client and the server are in the same machine. When the data is transferred onto the server machine, it's received by the RPC runtime and all of the necessary checks are performed to determine what is the correct server step that this message needs to be passed to. And in addition, it's possible to include certain access control checks at this particular step. The server stop will unmarshal the data. Umarshalling is clearly the reverse of marshalling. So this will take the byte stream that's coming from the receive buffers. It will extract the arguments and it will create whatever data structures are needed to hold the values of those arguments. One of the arguments are allocated and set to appropriate values the actual procedure call can be made. This calls the implementation of this procedure that's part of the server process. The server will computed the result of the operation, or potentially it will conclude that there is some kind of error message that needs to be returned. The result will be passed to the server side stub, and it will follow a similar reverse path in order to be returned back to the client. One more step is needed for all of this to work. Here we have as the zero initial step. That the client will need to find or discover the server, so that it can bind with it. But before that can happen somehow the server needs to do some things so that it can be found. The server will need to announce to the rest of the world what is the procedure that it knows how to perform, what are the argument types that are required for that procedure. What is its location? The IP address, the port number, any information that's necessary for that server to be discovered and so that somebody can bind with it. What that means is that the server also executes some registration step when this operation happens. Another thing about RPC is that the client and the server don't need to be developed together as part of the same application. They may be completely independent processes written by different developers, written even in completely different programming languages. But for this to work there must be some type of agreement so that the server can explicitly see what are the procedures that it knows how to execute and what are the arguments that are required for those procedures. The reason this information is needed is so that, on the client side, the client can perform decisions, which particular server it should bind with. Standardizing how this information is represented is also important so that the RPC run time can incorporate certain tools that will automate the process of generating the stub functionality. To address these needs RPC systems rely on use of interface definition languages, or IDLs. The IDLs serve as a protocol of how this agreement will be expressed. An interface definition language is used to describe the interface that a particular server exports. At the minimum, this will include the name of the procedure and also the type of the different arguments that are used for this procedure as well as the result type. So you see this is very similar to defining a function prototype. Another important piece of information is to include a version number. If there are multiple servers that perform the same operation, the same procedure, the version number helps a client identify which server is most current, which server has the most current implementation of that procedure. Also the use of version numbers is useful when we are trying to perform upgrades in the system. For instance, we don't have to upgrade all the clients and all the servers at the same time. Using this version number however, the clients will be able to identify the server that supports exactly the type of procedure implementation that is compatible with the rest of the client program. So this is basically useful for so-called incremental upgrades. The RPC system can use an interface definition language for the interface specification that's completely agnostic to the programming languages that are otherwise used to write the client and the server processes. SunRPC which is an example of an RPC system that we will look at later in this lesson, uses an IDL that's called XDR. External data representation. And XDR is a completely different specification from any other programming language that's out there. We will describe XDR in more detail, but here is an example of something that's described with XDR. And you can notice that the definitions of things like the string variable with these angular brackets, that's not really something that's used in other programming languages. It's very XDR specific. If you would like, by the way, to read ahead and examine a SunRPC example and look at XDR in more detail, there are links provided in the instructor notes. The opposite of a language-agnostic choice for an IDL is to choose a language- specific IDL to describe the interfaces. For instance, the Java RMI, which is a Javaequivalent of RPC uses the actual, the same, programming language JAVA. To specify the interfaces that the RMI server is exporting. Here is an example of an interface specified for Java RMIs. Those of you that know Java will immediately recognize that this looks just like Java. For programmers that know Java, use of a language specific IDL is great because they don't have to learn yet another set of rules to, how to define data structures or procedures in another language. For those that don't know Java that are not familiar with the specific programming language that's supported by the server for instance. Then this becomes irrelevant if you have to learn something they might as well learn something simple and that is one of the goals that XDR has. Now let me iterate one more time that whatever the choice for the IDL language, this is used only for specification of the interface that the server will export. The interface, whatever is written with this IDL language will be used by the RPC system for tasks like automating the stop generation process. Generating the marshalling procedures. And to generate information that's used in the service discovery process. The IDL is not actually used for the actual implementation of the service. To understand Marshalling, lets look at the ad example again. The variables i and j are somewhere in the memory of the client processing address space. They're two separate variables so there's absolutely no guarantee that they will be next to one another. The client makes a call to the RPC procedure rpc.add and passes i and j as arguments to it. At the lowest level of the RPC run time, this will somehow need to result in a message that's stored in some buffer that needs to be sent via socket API to some remote server. This buffer needs to somehow be some contiguous location of bytes that includes the argument as well as some information about the actual procedures, some identifier for the procedure, so that on the other end. The server can make sense of what needs to be done and how the rest of the bytes in this packet need to be interpreted. And this buffer gets generated by the marshalling code. The marshalling code will take these variables i and j, and then it will copy them into this buffer. It will serialize the arguments of the procedure into a contiguous memory location in this manner. In case the previous example is too trivial, here is what would happen if we need to perform a array add procedure, which takes as arguments and integer i. And some array, j and then adds this integer to all of the elements of the array. Then again, the marshalling code will need to serialize the arguments i and j. Serializng the array j can be done in different ways. For instance, the agreement can be that arrays are serialized in a way that we first place the size of the array. And then we add all of the elements of the array. So then the total buffer that's produced, as a result of the marshaling process, will include both the specification of the procedure, in this case, it's a different procedure. Array_add. The first element, i, the first argument and then the second argument of the procedure, j, that happens to be an array. And in this particular process, the agreement is that the array includes the array size and then the elements. Another type of agreement that can make sense for a marshal in arrays is that we would just list all of the elements of the array. And then we would include some special character to denote the end of array. That's, for instance, what's typically used for strings, and then the null character is used to denote the end of array. Either way, what this means is that the marshaling process needs to encode the data into some agreed upon format. So that it can be correctly interpreted on the receiving side. The encoding specifies the data layout when it's serialized to the byte stream so that anybody that looks at it can actually make sense of it. In the un-marshalling code in contrast, we take the buffer that's provided by the network protocol. And then based on the procedure descriptor and the data types that we know are required for that procedure descriptor, we parse the rest of the byte stream from that buffer. We extract correct number of bytes and we use those bytes to initialize data structures that correspond to the argument types. As a result of the un-marshal link process, these I and J will be allocated somewhere in the server address space, and they will be initialized to values that corresponds to whatever was placed in the message that was received by the server. Now again, the marshal link and un-marshal link routines aren't something that the developer will explicitly have to write, instead the RPC systems typically include a special compiler that takes an IDL specification, a specification that describes the procedure prototype and the data types for the arguments. And from that it generates the marshal link and the unmarshal link routines that are used in the steps to perform these translations. These routines are also responsible to generate the appropriate encoding related actions. So exactly how will an array be represented when its encoded in a byte stream. That's an example what will take place in these auto generated routines and there are other examples of what constitutes encoding. For instance, converting integers like this value i from one NDN format another NDN format, like from big NDN to little NDN depending on what's required by the server or by the client for the results. That's an example of a automated action that would be incorporated into the marshal encode. Once this IDL is compiled and all of the code is generated that provides the implementation for the marshal link and un-marshal link routines, all the developer needs to do is to take that code and just to make sure that it links it. With the program files for the server, or the client codes when generating executables. Let's talk a little bit about binding now. Binding is the mechanism that's used by the client to determine which is the server that it needs to connect to. Based on things like the name of the the service that it needs perform, the version number of that service. And also it's used to determine how to connect to that particular server to basically discover the IP address or the network protocol that need to be used for that connection to be established. To do this, to support binding, the system software needs to support some form of database of all the available services. And this is often called a registry. You can think of the registry as the Yellow Pages that you need to look up based on the service name that you require and then find the best match based on the protocol, the version number, the proximity. Some other information. That match will then provide you with the contact details for that particular service instance, so the address, the port number, the protocol that needs to be used. At one extreme this registry can be some distributed online service may be called something like rpcregistry.com, that any RPC server can register with. And the clients then have a well-known contact point, how they can find information regarding the services they need. At the other extreme, the registry can be a dedicated process that runs on every single server machine and knows only about those services that run on that particular machine. That means that the clients have to know the machine address, when they need to request some particular service. And the registry still provides useful information. It will tell the clients what is the port number that they need to use when they try to connect with the particular server. Regardless of how the registry's implemented, it will require some sort of naming protocol, some sort of naming conventions. For instance, the simplest approach could require that a client has to specify the exact name and version number of the service that it requires. Or a more sophisticated naming scheme could consider the fact that words like summation and sum and addition are likely equivalent to the use of the word add. And so any service that uses any one of these function names or service names is a fair candidate to be considered when trying to find the best match. Allowing this kind of reasoning for required supports for things like oncologists or other cognitive for learning methods, and we will not discuss this in this course. To illustrate the use of binding and registries by applications when they use RPCs, we will draw an analogy with how toy shops rely on directories of outsourcing services. For instance, in a toy shop when considering whether or not to use some kind of outsourcing service, the manager will want to know who out there can provide that particular service. What are the specifics service details that those outsourcing companies offer? And exactly, what are the shipping or packaging options they provide. For instance, the toy shop manager may consider looking at the directories service to find out what are the shops where outsourcing of assembly operations can be supported. He will look up what are the exact services that each of these shops provide. And for instance he's trying to find the service where the assembly of train cars can be provided. And then the manager may be interested in exactly what are the shipping options that they offer. For instance, whether they ship with UPS. To give an analogous example in the context of operating systems and the applications use of binding and registries in RPC. Now we can see that the same types of steps are required to be performed by applications when they rely on the RPC to execute some service. For instance, they have to look up the registry to find out who can provide a particular service. They can look up a registry with a service name that requires specify somehow some image processing. The registry provides some detail regarding the various services that are provided by each server, the version number. All of this relays on the use of some interface definition language so that the interface can be describe in some standard way. And then finally, also the registry will provide information regarding the protocols that a particular server or services support like TCP or UDP. The applications can take all of this information into consideration when determining which particular process to bind with, which particular server to bind with. And similarly in the toy shop, the toy shop manager can consider the answers to all of these questions when determining how to outsource a service. A tricky issue when it comes to RPC's is the use of pointers as arguments to procedures. In regular procedures it makes perfect sense to have procedures like this foo that takes two arguments, an integer, and the second argument is a pointer to an integer or even an integer array. When this procedure is called. The second argument, is a pointer to some address, in the address base of the calling process, where the particular, the area about this argument is stored. However, in RPC, passing a pointer to the remote server makes no sense. Since this pointer points to some location in the caller address space, the server cannot possibly get to the contents that are stored at this particular address. To solve this problem, RPC systems can make one of two decisions. The first decision is not to allow for pointers to be use this argument of any procedure that an RPC procedure that will be exported and can be called in remotely. The second solution is to allow pointers to be used but in the RPC run time to ensure that the marshalling code that gets generated understands the fact that the argument is a pointer. And that, instead of just taking that argument and copying it into the send buffer, that it actually serializes the pointer. What that means that it will copy the reference, the pointed to data structure,. Into the data buffer into one serial representation. On the server side, the RPC runtime will first have to unpack all the data to create the same data structure. Then it will record the address to this data structure and that is the value that's the pointer that it will use as an argument when it makes the call to the actual local implementation of this particular operation. Since we're talking about the trickiness of RPC calls, let's also talk about errors in fault handling and reporting. When the client hangs while waiting on a remote procedure call, it is often hard to take what exactly is the problem? The server can be overloaded, the client request may be lost, the response may be lost, the server machine may have crashed, or the server process may have crashed, or some element in the network, some switch or router may be down. Even if the RPC runtime incorporates some mechanisms that time out whenever a client RPC call hangs, and then retries them automatically. They're really no guarantees that the problem will be resolved or that the RPC runtime will be able to provide some better understanding of what's going on. And potentially, for some cases, it is possible to really understand what is the cause of the error, but in principle that is too complex. It would have involve a lot of overhead, and ultimately, it's still unlikely that it will provide a definitive answer. For this reason IPC systems typically try to introduce a new type of error notification or a new type of signal or exception that tries to capture what went wrong with an RPC request without claiming to provide the exact detail. This serves as a catch all for all types of errors, all types of failures that can potentially happen during an RPC call. And it also can potentially indicate a partial failure, so maybe the call really didn't quite fail, it's just that the client doesn't know what succeeded and what failed. Consider the following scenario. An RPC call fails and returns a timeout message. Given this timeout message, what is the reason for the RPC failure, that can be concluded by the RPC run time? Here are the options that are available. The client packet was lost. The server packet was lost. The network link was down. The server machine was down. The server process failed. The server process was overloaded. All of the above. Or any of the above. Check all that apply. The only answer that the RPC run time can definitely be confident that is the correct answer is any of the above. As we explained in the previous morsel, any one of these things can be a possible cause of failure. Also, hypothetically, though perhaps not very likely, it is possible that every one of these things happened at the same time, and that's why even all of the above is one of the possible answers. So, any of the above is the only correct answer. That is the only thing that the RPC runtime can know for sure when it sees that request time back. In the last few videos, we described some issues with remote communication and the RPC mechanisms that solve them. This included the binding mechanism that's used so that the clients can figure out how to find the server and what is the server that they need to talk to in the first place. We discuss the use of interface definitional languages, to determine how to package arguments and results that are being exchanged among the client and the server. And in that sense, the IDL is used to specify how the client and the server talk to one another. How they are able to unders, understand each other. Next, we observe the problem of dealing with pointers as arguments in remote procedure calls. And we said that, the use of pointers should either be completely disallowed, or that the RPC system should build in some kind of support to serialize the data that's being pointed. Finally, we'll also talk about partial failures, and explained how it is tricky to determine exactly what went wrong in an RPC system. And that instead, the RPC run-time provides some special errors and tries to, in as much as possible, determine what exactly was the cause of the error without making any kind of guarantees that it will be able to provide a precise answer. For all of these, we mention that there are multiple choices that can be made in the concrete implementation of an RPC system. For instance, for binding, we can choose to have a distributed or a per machine registry. Or we can choose to use a language agnostic or language specific interface definition language. In summary, these issues define the design space for an RPC system in different RPC or RPC like solutions, we'll make different choices in this space. And we will also very briefly contrast this with the RPC like support in Java called remote method invocations or Java RMI Sun RPC is an RPC package originally developed by Sun in the 80s for their network file system NFS for UNIX systems but it became popular and now it's widely available in other platforms. Sun RPC makes the following design choices. In Sun RPC it's assumed that the server machine is known up front and therefore the registry design choice is such that there is a registry daemon per-machine. When a client wants to talk to a particular service, it needs to first talk to the registry on that particular machine to find out how to contact the exact service that it requires. Sun RPC makes no assumption regarding the programming language that used by the client or by the server process. To maintain neutrality Center PC relays on a language agnostic interface definition language, XDR. And this is used both for the specification of the interface of the RPC service, as well as for the specification of the encoding. How data types will be encoded when they're being transmitted amongst machines? Some RPC does allow the use of pointers and data structures that are pointed by these pointers will be serialized. And finally, Sun RPC incorporates some mechanisms for dealing with errors. First, it has internally retry mechanism to retry contacting a server when a connection times out. This will be done for a specific number of times. Second, as much as possible, the RPC run time will try to return meaningful errors. So that a caller can at least distinguish between things like the server is not available, or there is a mismatch, or unsupported protocol or version. Or there is simply a time out related failure that just covers all of the other types of possible failures. Similarly to the generic description of RPC, like some other PC, the client and the server are allowed to interact via a procedure called interface. The server specifies the interface that it supports in a .x file written in XDR. Also Sun RPC includes a compiler called rpcgen that will compile the interface specified in the .x file to language specific stub. It will generate separate stubs for the client side and for the server side stuff. The server process when launched will register itself with their registry daemon that's available on the local machine. The per machine registry will keep track of information that includes the name of the service, the version, many of the protocols that are supported with the service, and also the port number that needs to be contacted when the client side RPC sends a request through the server. The client must explicitly contact the registry on the target machine in order to obtain information about the server process. When the binding happens, the client creates an RPC handle, and this handle is used whenever the client makes any RPC calls. And in this way, the runtime is able to track all of the per-client RPC-related state. I should note that with Sun RPC, or any other RPC, for that matter, the client and the server process that are communicating amongst each other may be on different machines. Or they may be on the same machine, just two processes running on the same physical node. So, the RPC in that case works like other forms of IPC, except it has a much higher level semantics. It has procedure called semantics, which is more complex than the IPC mechanisms that we saw before. Before we look at the key components of Sun RPC, if you would like to view a more complete reference. Then, take a look at these Sun RPC tutorial and examples that are now maintained by Oracle. Oracle purchased Sun in 2010. The link to this is provided in the instructor notes. At that link, you will find references to TI-RPC as opposed to Sun RPC. TI stands for transfer independent RPC. And that means that the protocol that will be used for the client and server communication doesn't have to be specified at compile time. It can be specified dynamically at run time. Other than that and a few smaller issues the documentation and the examples closely follow the original Sun RPC specification as well as the XDR interface definition language. Also, a number of older online references are still valid reference points. And you can, finally, look at the Linux man pages by looking for man rpc. This will give you all of the Linux supported APIs. We'll now take a look at the various components of Sun RPC using an example. The client again will be contacting a server that can perform calculations except this time the client will pass a single argument x for which it will warn the server to compute the squared value, x squared. Here's the .x file for this example with which the server specifies its interface. In the .x file, the server specifies all the data types that are needed for the arguments, or the results of the procedures that it supports. In this case, the server supports one procedure, square underscore proc. That has one argument of the type square underscore in. And the returns are resolved of the type square underscore out. The data type square in, and square out, are both defined in the .x file. If we take a look at them, it turns out that both of them have a single element and that's an int. And in XDR an int, is an integer just like the integers in C. So it's a 32-bit integer. Also note that this notation under square_in, square_out is not any part of the required syntax for specifying the input and the output data types in XDR. Other than the data types, the .x file describes the actual RPC service and all of the procedure it supports. First there is the name of the RPC service. In our case that's square_prog. And this is the name that will be used by clients when they're trying to find an appropriate service to bind with. A single RPC server can support one or more procedures. For instance, a calculator server can support all sorts of arithmetic operations. In our case, the square proc service supports exactly one procedure and that's square underscore proc, procedure. There is an ID number that's associated with it. This is one in this case. This number is not used by the programmer. This will be used internally by the RPC run time. When it's trying to identify which particular procedure is being called. So it's not going to pass between the client and the server in the packets. The name SQUARE_PROC, instead it will use this value 1 as a reference. In addition to this ID number and the input and output data types, each procedure is also identified by a version. And in fact the version may apply to an entire collection of procedures. We see that in this case, the version number for a service is 1. Over time, however, we may choose to refine that SQUARE_PROC procedure or add additional procedures. And as we're doing that, we don't want to be forced to immediately go ahead and update all of the clients with this perhaps semantically different or syntactically different square_proc procedure. In that case, what makes sense is that whenever clients and servers interact, they reference the version number of the procedure that they're requesting. When a client contacts a server that does not support a procedure with the appropriate version number, then the communication can be rejected. What this also illustrates is that it's possible for a single server to support multiple versions of the same procedure, and this helps with, in general, the evolution of the system. We don't have to coordinate an upgrade of all the servers and all the clients at the exact same time. Finally, the .x file also specifies service ID. This id is a number that's used by the RPC runtime to differentiate among the different services. So the client will use things like service name, and procedure name, and the version number, whereas the RPC runtime will refer to the service id, the procedure id, and again, it has to know the version id. For the service ID, you're allowed to specify a value in this range. The remaining values for service ID's either have some predefined values like for instance, for the network file system, or they're reserved for future use. Let's show how you actually compile a .x file. Assume that we're using the same squared example as in the previous videos. In the file, the .x file for that example is square.x. You'll see that by using this .x file, we will automatically generate a bunch of the code that's used for the client and the server-side processing. To do this, Sun RPC relies on a compiler, rpcgen. And to generate C code, rpcgen is used with the option -c, so that full command is rpcgen -c, and then square.x. That's the .x XDR file. The outcome of this operation will be that a number of files will be generated. First, they will generate a header file, square.h, that will have all of the language-specific definitions of data types and function prototypes. Next, they will generate the code for the client and the server-side stubs. For the client, this is a proper stub, for the server side code, this actually also includes the skeleton of the actual servers. It has the main retaining. The only thing that's not available will be the actual implementation of the service, of the procedure, and this makes perfect sense since the compiler has no way of knowing what exactly what do we want a particular procedure to do. In this case, clearing a number. Finally, the compilation stub will also generate a separate files, square_xdr.c. And this will include some common code that's related to the marshalling and unmarshalling routines for all of all of the data types, the arguments and the results, that are used both at the client and on the server-side. If you take a look at the file square_svc, which stands for service, you will see that it has two parts. The first part is the main function for the server and that will include the code that, that does the registration step and also some additional housekeeping co-operations. In addition to main, the stubble contain all of the code that's related to the particular RPC service. So in our squared case, this is the square_prog service. And, it is the first version of that particular service so for all of the procedures in that particular service, the file will include automatic regenerate code in order to parse the request. So as to determine which particular procedure to be called to generate the arguments, all of the argument marshalling corporations will be invoked here, and other steps. In addition, in the step file, the auto-generated code will include the prototype for the actual procedure that's invoked in the server process. For the square_proc procedure that we describe, this is the procedure name. And that will include also the _1, that refers to the version number. And this piece if code has to be implemented by the developer, this is not automatically generated. The client stub will include a procedure that's automatically generated, squareproc_1. And this will represent a wrapper for the actual RPC call that the client makes to the server-side process where the implementation of the service, this squareproc_1.svc is actually called. Once we have all of this, the developer then writes the client application and makes call to this wrapper function that looks something like this, y equals squareproc of x. This very much looks like a regular procedure call. There is no need to create sockets, create buffers, copy data into the buffers, and this is what makes RPC appealing. We will now summarize one more time the steps involved in developing RPC applications. And this figure here will serve as an illustration. We have to write the .x file in XDR and pass it through the rpcgen compiler. That will generate a number of files. The header file, the stubs. It will generate even the skeleton for the server. And it will also generate an underscore XDR file that has a number of helpful marshalling routines. For the server-side application, the developer has to provide the implementation of the actual service procedure. The square.proc_1, for the first version, _svc. This is the naming convention. On the client side, the developer has to develop the client application and whenever necessary, call the wrapper procedure squareproc_1. This is what will actually invoke all of the communication with the server process and the execution of this particular service implementation. The developer has to make sure that he includes all of .h file, particularly the auto-generated ones from the rpcgen compiler. And also that it links the client and the server code with the stub object. The RPC runtime that is called from the stub things, provides all other functionality, including interactions with the operating systems, creating sockets, managing connections, and everything else. I should point out that, that rpcgen, when used only with the flag -C generates code that's not thread safe. The output of the compilation results in a function that will need to be called with something like this. And the problem with this function is that internally, the implementation of this operation, as well as at the runtime level, there are a number of statically allocated data structures included for the result. And this leads to race conditions when multiple threads are trying to make RPC calls to this routine, concurrently. To generate thread safe code, the code must be compiled with the -M option, and M stands here for multithreading safe. This will also create a wrapper function squareproc_1, however, it has a different signature and its implementation differs, for instance it will dynamically allocate memory for the results of this operation. So some of the issues that are coming up with the previous implementation will not come up in this case. Using the -M flag doesn't actually create a multithreaded server, the implementation that's provided, that generated in the _svc file. That won't be multithreaded. On Solaris platforms there's another option, -a, using this option, that actually generates multithreaded server code. But in Linux, this option is not supported and any multithreaded server has to be created manually. Of course, with using the multithreaded safe routines as a starting point. Soon enough you will be writing your own XDR files and implementing RPC. But right now let's take a look at what would happen if we were compiling this square.x file that's used in the examples that we talked about in the previous videos. So here is a short quiz. For this square.x file, that's also provided in the instructor's notes, what is the return type of the square proc_1 procedure, when the square.x file is compiled with rpcgen -C or rpcgen -C -N? Write your answers in the text boxes. After compiling the file you should get the following answers. Our compiled files have been included in the instructor notes. Note that the thread safe and the non-thread safe versions of this function have a different prototype and they resolve in a different return values Let's talk briefly about the Sun RPC registry. Remember we said already that the actual code that the server needs to register with the registry's auto generated in the RPC general process, and it's part of the main function. In Sun RPC the registry process or the registry daemon is a process that runs on every single machine and it's called portmapper. To start this process in Linux you have to have administrative permissions or sudo access privileges and then you can launch it with the following command, sbin portmap. This is the process that has to be contacted both by the servers when they need to register a service, and also by the clients when they need to find what is the specific contact information for a particular service they are looking for. Now given that the client already got to talk to this RPC daemon, it clearly knows what is the IP address of the machine that it will need to interact with. So the information that the client can extract from the port mapper includes things like what is the port number that the client needs to use to talk to a server, or whether the particular version and protocol are supported for the server that the client requires. Once the RPC daemon is running we can explicitly check what are the services that are registered with it using our rpcinfo -p. You may need to explicitly type in the full path for this command but once you run it you will see that it returns information like what is the program ID, the service name, the version of every single service that's registered on that particular machine. Also for every service it will incorporate the contact information. So what is the protocol that that service speaks so to say. And what is the socket port number that needs to be contacted by the client side RPC runtime when it wants to initiate communications with a service. When you run this service, you will also probably notice that the port mapper service is registered with tcp and udp on the same port number, 111. This means that there are two different sockets that this server is listening to. One is a tcp socket, and the other one is a udp socket, and they both happen to use the exact same port number, 111. This means that this service, the port mapper, will be able to talk to both the tcp, as well as udp clients. And the last part of Sun RPC that I wanted us to talk about is the binding process. The binding process is initiated by the client using the following operation. So, clnt_create with a number of parameters. For the specific squaring example that we talked about, this operation will look like this. We will specify the host name of the server, as well as the protocol that we want to use when communicating with the server. And we will specify the name of the RPC service as well as the version number. These two arguments of the clnt_create operations are auto-generated in the RPC generation process from the .x file. And will be included in the header file in the .h file as hash defined values. What this means is that if the client needs to now support a different version number, it will need to be recompiled, given that this is essentially a static piece of information. However, none of the other portions of the client code have to be modified. Also note that the return from this operation is a variable clnt_handle that's of data type CLIENT. This is the clnt_handle that the client will include in every single RPC operation that it requests. And this handle will be used to track certain information, such as what is the status of the current RPC operation, any error messages, or it can even be used to capture certain authentication-related conformation. In the basic square RPC example, we said that all of the data types for the input and output arguments must be described in the .x file. All of these types and data structures must be XDR supported data types. Some of the default XDR data types are those that are commonly available in programming languages like C for things like character and byte and integer and float. But XDR supports many other data types. For instance, if you specify that something is a const, it will be translated after compilation into a constant, which is C, will be a #define value. Data types like hyper or quadruple are used to refer to a 64-bit integer or a 120-bit float, respectively. And XDR also supports a so-called opaque type, which really corresponds to data type that's uninterpreted binary data. So, similar to the C byte type. So for instance, if you want to transfer an image, that image will be represented as an array of opaque elements. Let's talk more specifically about arrays because in XDR, you can specify two types of arrays. The first is a fixed-length array that's described as follows. And here, the exact number of elements in the array is specified. The RPC runtime will allocate the corresponding amount of memory whenever arguments of this data type are sent or received. And it will also know exactly how many bytes from the incoming packet stream it should read out in order to populate a variable that's of this data type, this type of array. There are also variable-length arrays, where the length is specified in angular brackets. And this doesn't denote the actual length, rather the maximum expected length. When compiled, this will translate into a data structure that has two fields. An integer, len, that corresponds to the actual size of this array. And a pointer, val, that is the address of where the data in this array is actually stored. When the data is sent, the sender has to specify len, the size of the array, and then set val to point to the memory location where the data is stored. On the receiving end, the server will know that it's expecting data structure that's a variable-length. So it will know to read the first 4 bytes to determine what is the length, what is the size of the array. And then to allocate the appropriate amount of memory, and then to read the remaining portions of the incoming byte stream and to populate that memory with those values. The only exception to this are the strings. A variable-length string is defined as follows, and this line is really just the C pointer to character. In memory, the string will be stored just like a normal null-terminated string, so it will be an array of characters with the null character at the end. Operations like string copy and string length need that particular representation in order to be able to determine where is the end of the string. However, when that variable-length string is encoded for transmission, it will be encoded as a pair of length and data. So from that perspective, that will be similar, actually identical to what we see for other variable-length data structures. Let's look at the use of XDR in a quiz. Let's assume that an RPC routine uses a variable length integer array of a maximum size 5. Now if the array is full, how many bytes are needed in order to represent this data structure in a client in C on a 32-bit machine? You should provide your answer in bytes. Now since this is a variable length array it will be compiled in C, the length of the array len, and the pointer, the address where the actual data structure is stored, val. Len is an integer. So that is four bytes. And val is an address. And given that this is a 32 bit machine val will also be four bytes. To add to that, the memory that's required for five integers, that's four bytes each. The total amount of memory is 28 bytes. XDR provides the RPC runtime with some helpful routines. For instance, after we compile a .h XDR file, the compiler will generate a number of routines that are used for marshalling or unmarshalling, the various data types in the RPC operations. In the example that we talked about, the square rpc example, these will all be found in the square_xdr.c file. In addition, the compiler will generate certain clean up operations like xdr free, that are used to deallocate to free up memory regions that are used for the data structures, the arguments in the RPC operations. These routines will typically be called within a procedure that the name of the procedure typically ends with _freeresult. For instance, in our square program it will be square_prog_1_freeresult. And this is yet another user defined procedure, where the user can specify what are all of the different data structures, or pieces of state, that need to be freed up. And the allocated, after the runtime is done servicing RPC request and returning results. So the RPC runtime will automatically call this procedure after it's done computing the result. One thing that we didn't explain is what actually ends up in the buffers that are being passed for transmission among the client and the server. For instance, the server can support multiple procedures. It is important to not just pass the arguments but actually to include an RPC header that will uniquely identify what is the procedure that's being called, the version number, something about the requests so that we can detect repeated requests on retries. Similar type of information will be sent from the server back to the client, again, as part of the RPC header. So this is one component of what actually goes on the wire in the packets that are being transmitted. Then, clearly we have to put the actual data, the actual arguments or results on the wire as well. However, as opposed to just directly copying from memory into the packets, we have to first encode all of the different data types for the arguments in the results into a byte stream to serialize them in a way that depends on the actual data type. It is important to have a specific agreement on how this encoding is done so that the server has the ability to interpret the byte stream and recreate the appropriate data structure in the server address space. In order for the server to actually call the procedure that implements the service. it needs to have the arguments present in the server memory. That's why this step is necessary. Similar kind of requirement, we have also on the return when the result is passed to the client. The client needs to be able to look at this byte stream and figure out how it needs to take that information in populated data structure in the client memory. In some cases there may be a one to one mapping between the in memory representation and how the data is encoded in the packet. But in other cases that may not be the case. And finally when all this information is placed in a packet that needs to be preceded with the transport header with the networking header that will specify the protocol, the destination address and will make sure that on the client and on the server, all of the protocol specific operations take place. As we hinted already with the discussion of the syntax data type, XDR specifies both the syntax, the interface definition language. So how our data type's described, and also it specifies the encoding. So what is the binary representation of data when it's on the wire? As we handled already in the discussion of the string data type, XDR that corresponds both to the interface definition language, essentially that's the syntax. How we are describing data types. And also it specifies the encoding. That's the binary representation of how is data represented when it's being transmitted between the client and the server on the wire. Here's some encoding rules. All data types are encoded in multiples of 4 bytes so transmitting a single byte argument would include a single byte for the data and 3 bytes of padding in order to make that up to 4 bytes. This is to help with alignment when moving data to and from memory and the network packets and the network card. Big endian is used as the transmission standard. What this means is that regardless of the endian in this type, of the client or the server machine, every type of communication will require the data is first translated into big endian representation and then if necessary, translated into the appropriate endian in this for the target machine. In some cases, this may be pure overhead just because the client and the server machine are both, let's say little endian machines. But, in principle, it's easier to have this type of standard agreement so that there's never any kind of ambiguity, what is the encoding that's being used on-the-wire. And how to interpret the bytes that are coming into the network packets. Other rules include things like, two's compliment is used to represent integers. And the IEEE is used for floating point numbers, so other roles. Let's explain this a little bit better using an example. So, let's say that in the .x file, we have a definition of a data type that is a variable length array of a size up to 10. And let's say we have an argument, hello, that needs to be passed from the client to the server. In the client or the server address space, if these are C processes, this variable will take 6 bytes. 5 bytes for each of the characters and then the last 6 bytes for the null terminating character. However, for transmission, this variable will be encoded to take 12 bytes. The first 4 bytes will be used for the length. In this particular case, the length is 5, it's 5 characters. The next 5 bytes will be used for those characters, H-E-L-L-O. Notice we're not going to be transmitting the null terminated character. And then, at the very end, we will have 3 characters use the padding, because XDR specifies that everything needs to be on 4 by boundaries. Here's a quiz that's very similar to the one that you recently took on XDR data types except this time, we're concerned with the data being transmitted over the network and the encoding cost of such transmissions. Let's again assume that an RPC routine uses a variable-length array of integers and that the array is full. For that situation, answer the following. How many bytes are needed to encode this five element array so that it can be sent from a client to a server where both the client and the server are 32-bit machines? In your answer, please do not include any bytes for the headers or the protocol related information. And provide your final answer concerning this encoding of this data structure in bytes. Variable length arrays will be encoded so that the first four bytes will correspond to the integer value of the array size, len, and then the rest of the bytes will correspond to the actual array elements. In this case the array has five elements where each is a four byte integer. No additional padding needs to be performed. So the total length of the encoded representation of this data structure is 24 bytes. In reality, you would need to account also for any of the RPC specific headers as well as the protocols, but we're not concerned with that in this particular answer. Another popular type of RPC-like system is Java RMI, Java Remote Method Invocations. It's also pioneered by Sun as a form of client-server communication method, among address spaces in the Java virtual machine. Java is an object-oriented language where objects interact via method invocations and not via procedure calls. For this reason, this inter-process communication mechanism matches the Java object-oriented semantics as in the form of remote method invocations. Its architecture is similar to what we saw with the remote procedure calls. Client and server processes have client-side stubs and server-side stubs. The server-side stub is referred to as a skeleton. In the Java virtual machines, all of the processes, all clients and all servers, are written in the Java programming language. For that reason, the interface definition language for the Java RMIs is also Java. It doesn't make sense to adopt a different interface definition language, like in the case of XDR for RPC, where in this case, everything will be written in Java in the first place. So RMI uses a language-specific interface definition language choice. And in this case, that's Java. The runtime layer is separated into two components, the remote reference layer and the transport layer. This bottom layer implements all of the transport protocol related functionality. This can be TCP, UDP, shared memory based communications if the two processes are running on the same machine. Above that is the remote reference layer. This component captures all of the common code that's needed to provide different reference semantics. For instance, it can support unicast, where a client interacts with a single server, like what we had in the previous examples. But RMI can be used for other types of server reference semantics. For instance, with broadcast, the client will contact multiple servers. And then the reference semantics can be such that it will return only once the first response arrives, or only when all of the responses arrive and the responses match. It also makes sense to design other types of reference semantics. These are not the exclusive list. Regardless of the underlying transport protocol, this type of functionality will be implemented in a similar way. So RMI separates it and captures it in a separate component, this remote reference layer. As a developer, you can either just specify the reference semantics you want from the RMI interactions and the system will take care of the rest. Or if you want something exotic, you can implement just this component and the rest of the system can remain the same. We're mentioning in this lesson Java RMIs just for completeness. We're not going to discuss it in any detail. If you would like to know more, visit the resources that are linked in from the instructor's notes. In this lesson, we looked at remote procedure calls to RPCs. This is a popular interprocess communication mechanism that's used to support client-server types of interactions. We said that an RPC system requires the user of an interface definition language, or IDL, in order for us to describe the remote service, and the mechanisms such as registries and binding and marshalling in order to enable the remote data exchanges. We described in more detail Sun RPC, and with the examples that we looked at, you should have enough information to start using and implementing Sun RPC. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In this lesson we will talk about distributed file systems and about some of the design decisions that are available in this space. We will describe what are the specific decisions made for NFS, which is a popular example of a distributed file system. And we will also look at a research paper, Caching in the Sprite Network File System. This paper is useful because it has a lot of information why certain decisions were made in the Sprite file system. One thing you should have in mind is that all of the discussion in this lesson will focus on distributed file systems. The methods that we will talk about generalize to other types of distributed services. First, let's start with a visual metaphor. Distributive file systems are like distributed storage facilities, specifically storage facilities containing toy parts for a toy shop. First, each is accessed via some well-defined interface. Also, both focus on maintaining consistent state as one of the major functional requirements. And finally, both of these support different distribution models. For the distributed storage facilities, the toy shop manager needs to be able to access, check, and make decisions regarding the available resources without having to leave the toy shop and directly visit some of the storage facilities. This should be done via some well-defined interfaces that the storage facilities export. Next, the distributed storage facilities must constantly update their inventories to represent consistent information about the parts that they can deliver. This helps managers and other workers accurately determine what are the inventories and what are the times that these products can be delivered, for instance. Finally, these distributive facilities can be configured in different ways. Some of them can be for storage-only. Some can provide both storage and processing services. Some can be specialized for different types of toys or different types of parts. So there will be different types of distribution models. Now let's look at how distributed file systems relate to these properties of distributed storage facilities. First, distributed file systems are also accessed via via some higher level well-defined interfaces. In particular, this would be the virtual file system interface that we already talked about. This allows the operating system to take advantage of multiple types of storage devices or storage machines, regardless of where they are. Next, distributed file systems need to maintain consistent state of the files shared among clients. If a file is updated by one client, the distributed file system needs to track information regarding all of the file updates, some state that's necessary, for instance, for execution of cache coherence, algorithms, and other information. And also, the distributed file systems can be implemented via different distribution models. They can be replicated or the files can be partitioned across the front machines in the file system. Or all of the notes in the file system can act more like peers amongst each other when they're providing the distributed file system service. >From our previous discussion about file systems, we said that modern operating systems hide the fact that there may be different types of storage devices with different types of internal file system organizations. And this is done by exporting a higher level virtual file system interface. Underneath this virtual file system interface, the operating system can also hide the fact that there isn't even local physical storage on a particular machine where the files are stored. But that instead everything is maintained on a remote machine, or on a remote file system that is being accessed over the network. These kinds of environments, where there are multiple machines that are involved in the delivery of the file system service together form a distributed file system. In this case, simply the client, where the data may be cached, and the server, where the data is stored, sit on two different machines, and that's enough for us to have a distributed file system. More generally, a distributed file system is a file system that can be organized in any of the following ways. The first model is when the client or the clients and the file server are on different machines. The second model is when the file server is not just single machine but instead, it's distributed on multiple machines. This may mean that all the files are replicated, and available, on every single machine. This helps in the event that there are failures, because there's always a replica of the file that's available on one of the other servers. And this also helps with availability since all the requests that are coming in can be split across these different servers. Another way that the file server can be distributed among multiple machines is by splitting the files, dividing them, or partitioning them. So that different physical machines store different files. This makes the system more scalable than the replicated model because if you need to store more files, you simply add additional machines. In the replicated model, we still need every single server to store all files so if we need to scale the file system to be able to store more files, we'll have to buy machines with larger discs. At some point, unfortunately, will likely reach a limit, whereas with the partitioned model, we can just keep adding additional machines. Or we may use some combination of both approaches, replication and partitioning. For instance, we can partition all the files and then each of the partitions can independently be replicated across multiple machines. A lot of the big data file systems that are used by companies like Google and Facebook use this kind of approach. Finally, the files may be stored on and served from all of the machines in the system. This solution blurs the distinction between client machines and server machines. Because all the nodes in the system are peers in the sense that they're all responsible for maintaining the shared files and also providing the file systems service. Each peer in the system will take some portion of the load by servicing some requests, likely those that are for files that are locally stored on the peer machine. In this lesson, we will primarily focus on discussion of the first model, clients and separate server machines. And, we will focus on issues related to caching and consistency management in such environments. Note that, in the next lesson, we will discuss, more generally, issues related to distributed state management, when there are multiple peer nodes in a distributed environment, that are involved jointly in the management of some distributed state. Then we will talk about memory ascending sample but there will be a lot of similarities between the mechanisms that will describe for distributed memory management and the ones that one could apply for distributed file management in this kind of model. Let's look at the different ways that a remote file service can be provided. We will assume that there is one client and one server. Whereas that one extreme we have what it is called the Upload/Download model. This means that when the client wants to access the file, it downloads the entire file, performs locally the operations and then when done, it uploads the file back to the server. This is less similar to proper file system but it's more similar to the way FTP servers behave or even the way version management systems like SVN servers behave. The benefit of this model is that once the client has the file, it can perform all of the operations locally so that can be done fast. One downside of this model is that the client needs to download the entire file even for a small file access for a small file modification. A second downside with this model is that it takes away file access control from the server. Once the server gives the file to the client, it has no idea what the client is doing with it, or when will it get it back. This makes some operations related to sharing files a little bit difficult to execute. At the other extreme, we have a model that's called the true remote file access. In this model, the file remains on the server and every single operation, every read or every write, has to go to the server. In this model, the client makes no attempt whatsoever to leverage any kind of local buffer cache, local disks, or anything like that. Everything has to go to the server. The benefits of this extreme is that the server has full control and knowledge of how the clients are accessing and modifying the shared state and the shared files. This makes it easier to ensure that the state of the file system is consistent, and that there are no situations where multiple clients are overwriting the same portions of the single file at the same time. The down side of this model is that every single file operation has to incur the cost of remote network latency. In this model, the latency costs are suffered even when clients are only reading repeatedly from a read-only file. This model will bypass any resources that the client has locally, to cache that file. Also, since every single file operation goes to the server, the server will get overloaded more quickly, meaning that the single server will not be able to support as many clients. This limits the scalability of the file system solution. The two models are two extremes, but in reality, what makes sense is to have something in between. First, we should allow clients to benefit from using their local memories and their local disk, and to store at least some parts of the file they're working on. For instance, it makes sense for clients to download some blocks of the file that they're accessing. Or even to apply certain prefetching techniques just like what regular file systems do when they're trying to prefetch blocks from disks into their memory cache. This will lead both to lower latencies for those file operations that acts as these locally stored or cached portions of the file. And by doing this, also some load is removed from the server since some operations can be fully performed on the client machines. And thus the server becomes more scalable. Now, once we allow the clients to store portions of this file locally and to perform operations on the file, including modifications, it becomes necessary for the clients to start interacting with the server for two reasons. First, the clients need to notify the server of any modifications to the file that they have made. And also, the clients need to find out from the server if any of the files that they have locally cached has been modified by someone else. These interactions have to happen with some reasonable frequency so that it's not too late by the time that we discover that certain changes have been made. This is beneficial because unlike in the upload-download model, with this, the server is still in the loop. It has insights into what exactly are the clients doing, and it has some control over which accesses should be permitted versus not. So it becomes easier to maintain consistency. However, the problem with this is that it makes the file server more complex. It means that the server would have to perform additional tasks and maintain additional state so as to make sure that it can provide consistency guarantees. And achieving a practical solution of this model means that the clients would have to understand somewhat different file sharing semantics compared to what they are used to in a typical local file system. Before we can discuss some of the design and implementation challenges with the models of remote services that we described in the previous videos, we need to distinguish among stateless and stateful file servers. First, the stateless server is one which doesn't maintain any information regarding which clients access which files, how many different clients are serviced, nothing. Every request has to be self-described, self-contained, so that it has all of the parameters regarding the filings being accessed, the absolute offset within that file, along with any data that needs to be written. This model is suitable for the upload-download model, or the other extreme, true remote file access service. But, it cannot be used for the more practical model, which relies on caching, because without state, we cannot achieve consistency management in the distributed file system. This is one of the biggest negatives of this approach. It prevents the use of caching, which is an important performance optimization technique. The other downside is, because all of the requests have to be self-contained, there will be more bits that will need to be transferred in order to describe each request. There are benefits to the approach as well. For instance, since there is no state that's maintained on the file server, no resources will be consumed on the server side to maintain that state. No memory will be used, no CPU will be spent, in order to make sure that the state is consistent, so that's one benefit. The most important benefit of the approach is that, because it is stateless, and because the requests are self-contained, this leads to a very resilient design in terms of failures. If the server crashes, it just needs to be restarted, and it can continue to service all of the future client requests without any potential errors. Sure, the clients will need to reissue any of the requests that have timed out, but they will ultimately receive the exact same type of answers, regardless of the fact that the server has failed. Neither the clients, nor the server, will need to do anything else special in order to recover from this failure, just restart. In contrast to this, a stateful server is one that maintains information about the clients in the system, what are the files they're accessing, what types of accesses they're performing, reads versus writes. For instance, for each file, the file system may track information, who has portions of the file cached, who has read or written to it recently, or similar information. Because of this state, it becomes possible for the file server to allow data to be cached and at the same time to guarantee consistency. Something that wasn't possible with the stateless server. And also to provide other benefits, like locking. We need state to make sure that we can keep track of who has locked the file, and whether or not the file is locked. Incremental operations is another thing that can be supported by having some state. A client can issue a request, I want to read the next kilobyte of data from this file. With a stateless design, there is no concept of what the next kilobyte would be. You have to describe every single request fully, with the offset of the file, with the specific file handle, and everything. The problem with this is that on failure, all that state that the server maintains needs to be recovered, so that the file system overall is still in a consistent state. That requires some more complex mechanisms, for instance, the state needs to be checkpointed. We have to have to some way to rebuild it in order to have a good representation of what it is that the clients were doing before the failure occurred. And of course, the runtime overheads with the stateful server because it needs to maintain this state, and also execute all of the necessary consistency protocols. Exactly what those overheads will be, will depend on the details of how caching is enabled, when and how it can occur, and then what are the consistency requirements that the distributed file system needs to support. We will look at what are some of the options in this space in the next videos. The first distributed file system mechanism we will look at is caching. Caching is a general optimization techniques in distributed systems, where the clients are permitted to locally maintain a portion of the state. In this case, portions of the files or file blocks. And also the clients are permitted to perform some operations on that cached state locally. For instance, some of the open or read or write operations can be issued against the locally cached files or file portions. This can be done potentially without contacting, and overloading the actual file servers. Keeping the cached portions of the file consistent with the on server representation of that file, requires that we have some coherence mechanisms. For instance, in this illustration, clients one and two both cache a portion of the file F. However, Client 2 has subsequently modified that file, F prime, and has also updated the file server to reflect those changes. The question then is, how and when will Client 1 find out that Client 2 has performed these changes? The problem here is similar to maintaining cache coherence in shared memory multi processors. There we said that we use mechanisms like write-update and write-invalidate, and these mechanisms get triggered whenever a particular variable, or a particular memory location gets written to. What this would mean in the context of this example that whenever Client 2 performs any kind of update to some portion of file F in its cache, that, that would be propagated to Client 1. Either as a write-invalidation message or a write-update, the actual change will be visible here. But given the very different communication costs, and also latencies that exist in distributed systems, achieving this may not be realistic. And also, it may not even be necessary, given the ways that files are being shared. Instead for distributed file systems, things that make sense would be to trigger some of these coherence mechanisms on demand when the client needs to access a file or periodically whenever the client is open. And when exactly get executed will also depend on whether the coherence mechanism is something that is client driven so the client initiates. I need to find out if the file is updated. I need to see the new version of this file. Or, server-driven, where the server notifies the clients who have cached the file, in this case, Client 1, that something has changed about their cached state. The exact details of how and when the coherence mechanisms are executed have to be such so that the file system can maintain consistence guarantees. However, what those details will be will depend on the file sharing semantics that this distributed file system needs to support. Before we move on, I would like to ask you a question to collect your initial thoughts about file caching in a Distributed File System. Where do you think files, or file blocks can be cached in a Distributed File System that has a single file server, and many clients? Type your answer in the text box, and then compare it to my solution in the following video. First, the files or the file blocks can be cached in the client memories in their buffer cache. The first place where files or file blocks can be cached will be the client's memory. As the files or the file blocks are brought in from the server to the clients, they can be present in the client's memory as part of their buffer cache. This is what regular file systems do when they're bringing in files they're reading from their local discs. Second, the clients may store cache components on their local storage devices, hard discs, SSDs. It may be faster to retrieve portions of the file from the local storage than to go via the network to the remote file system. Finally, the file blocks can also be cached on the server-side in the buffer cache in memory on the file server machine. However, how useful this will be will be the hit rate on that buffer cache will depend a lot on how many clients are accessing the server at the same time, and how are their requests interleave. If there is high request interleaving, the buffer cache may prove not to be very useful. Because there may not be much locality among the accesses which are originating, in this case, from many, many clients. To explain the file sharing semantics in a distributed file system, let's see what these look like first in a single node environment and then compare how they differ from the distributed multi-node environment. On the single node in a Unix environment, we have the following semantics. Whenever a file is modified by any process, in this case process A write c, that change will immediately be visible by any other process on that machine. So Process B will immediately see that c when it performs a read on this file, will get abc. This will be the case, even if the change isn't actually pushed out to disk, because both of these processes have access to the same buffer cache. In distributed file systems, that's not the case. Even if the fact that Process A performed an update, wrote c to this file, gets pushed to the file server immediately, that message, it may take some time before that message actually arrives here. So it is possible for Process B to not be able to see that message for a while, and whenever it performs a read operation on that file, even in that period after this file was written to, it will continue seeing that the file consists of only the elements a and b. Given that message latencies may vary, we have no way of determining how much should we delay every possible read operation in order to make sure that any write that may have happened anywhere in the system arrives to the file server, so that we can guarantee that this Process B does read the most recent version of the file. So in order to maintain acceptable performance, distributed systems would typically sacrifice some of the consistency, and they will accept some more relaxed file sharing semantics. Let's see what are some possible, meaningful file sharing semantics for distributed file systems. As a baseline we will contrast this to the case when your file system is on the single machine where every write is immediately visible, and this is what's referred to as UNIX semantics. Something that makes sense to do is to enforce what we call session semantics. Whenever a file is closed, the client writes back to the server all of the changes that it has applied to that file in its cache. Whenever a client needs to open a file, it doesn't use the cache contents, instead, goes and checks with the file server whether or not there is a more recent version of that file. We call the session semantics with the period between the file open and the file close being referred to as one session. With session semantics it is very possible for one client to be reading a stale version of a file while another client is either updating that file or even reading a more recent version of the file. But at least by knowing that when we open or when we close the file, we will be consistent with the rest of the file system at that particular moment. We can reason something about what kind of sharing guarantees do we have when we run in this type of distributed file system? Although session semantics are intuitive to reason about, they're really not good for situations when clients want to concurrently share a file, write to it and see each other's updates, then have to open it and close it repeatedly. And also when files stay open or are being updated for long periods of time, these can result in long periods of inconsistency in the file system. For that reason, it makes sense to introduce some time intervals when these updates happen. With introducing time intervals, the client updates, writes, will be propagated to the server periodically. One way to think about this is that the client has some sort of lease on how long they can use the cached data. However in this case know that we don't really mean that it's an exclusive lease, like locking. This is a completely separate issue. In the same way, the server notifications, the invalidations, are also periodically sent out to the clients. This can establish some sort of time bounds during which the system can potentially be inconsistent. So at least if there are any conflicts, it will be easier to correct for them. There will be likely fewer of them that have accumulated during the period of these updates. Since the client doesn't really have any idea about what are the start and the end times of these synchronization periods, the file system can also provide some explicit operations to let the client flush its updates to the remote server. Just like what we do with flushing the updates to disk when it comes to local storage, and also to explicitly sync its state with that of the remote server. Again these types of operations are not necessarily distributed file system specific, they're used in regular, local file systems as well. Other file sharing policies also exist and make sense for certain situations. For instance, files may be simply immutable, you never really modify a file, you simply delete it, or you create a new file with a new name. When you're sharing photos via Instagram or Facebook, you don't really upload a photo and then go and edit it, if you need to change the photo you change the photo and you upload the modified photo. These types of distributed storage have these kind of semantics immutable files. Another useful file sharing semantics would be for the file system to provide transactional guarantees. What this would mean is that the file system will need to export some interfaces, some API so that the clients can specify what is the collection of files or the collection of operations that need to be treated like a certain single transaction? And then, the file system can make some guarantees that all those changes are atomically committed, atomically made visible into the file system. As we work through the rest of the lesson, we will look at what are some of the file sharing semantics that are supported in some distributed file systems, and also what are the mechanisms that are required to achieve these types of semantics. Let's take a quiz. For this quiz, I want you to imagine a distributed file system where the file sharing is implemented via a server-driven mechanism and with session-semantics. Given this design, which of the following items should be part of the per file data structures that are maintained by the server? The options are: readers, current writer, current writers, version number. You should check all that apply. Before we answer the question, let me remind you, that a server driven mechanism means that it is the server that will push any invalidations to the clients. And also, that session-semantics means that any changes made to a file will become visible when the file is closed, when the session is closed, and when a subsequent client opens that file, starts a new session. So that means that it is possible for overlapping seasons to see different versions of the file, so it is possible to have concurrent writers. Session semantics doesn't specify what will happen to the file when all of these writers close that file. Whether one of the versions will become the valid one, or whether they will be merged in some ways, or whether some error will be raised back to the client so that they can resolve the conflicts. So given this information, the items that make sense to be part of a per file data structure in this kind of distributed file system will include information about what are the current readers of the system. Also, what are all of the current concurrent writers, potentially multiple, not just one. And also, it makes sense to keep track of something like a version number so that the clients know which version were they given, and the server also understands which clients have been modifying an old version of the file versus the newest one. As we mentioned multiple times in this course, understanding the workload, in this case, that will be the access pattern, how the files are being accessed. This is an important piece of information, that's useful when we're trying to design and optimize a particular solution in a certain way. When we're thinking about how to design the system so as to support a particular file sharing semantics, we need to understand things like what is the sharing frequency, what is the write frequency, so what is the access pattern. And also how important is it to maintain a consistent view for that particular type of files. Once we understand these workload properties, the design of the file system must be done so that it's optimized for the common case. One issues, however, that file systems have two different types of files, regular files and directories. In these two types of files, we often have very different access patterns in terms of what is the locality, what is the lifetime of the files, the size distribution, how frequently are they accessed. For these reasons, it is not uncommon for these two types of files to be treated differently. For instance, we can adopt one type of semantics for the regular files and another type of semantics for the directories. Or, if we use periodic updates as a mechanism for both, then we may choose to use less frequent write-backs for the regular files versus for the directories. This can be based on observations that, for instance, directories are more frequently shared, than individual files in them. Later in this lesson, we will look at the choices, how to treat these two different types of files, for the network file system manifest, and the sprite file system. Before moving onto concrete examples, I want to explicitly mention one more design dimension when it comes to distributed file systems. We said that the clients and the server can be distributed to different machines, but the file server itself can also be distributed. This can be achieved via replication or partitioning. With replication, the file system can be replicated onto multiple machines, such that every single machine holds an exact replica of all of the files in the system. The benefit of this can be that the client request can be load balanced across all replicas, across all machines. And this can lead to better performance. The system overall can be more available, it will return responses more quickly. And also it is more fault tolerant. When one replica fails, the remaining replicas can continue serving the data for all the files. The downside is that now the write operations that update to the file system state, may become more complex. Not only do we have to worry about the consistency among the clients that may cache the file, and the servers, but also about the consistency among all of the replicas. A simple solution is to force every single write to each replica. And only after that is done to actually return to the client and to consider that the write operation has completed. This will slow down all writes. An alternative would be to allow the writes to be applied to one server, to a single replica copy. And then to have some background process that asynchronously propagates these writes to the other replicas. If it turns out that there are any differences among the state of a file on different replicas, then these differences need to be resolved. For instance, we can use a simple technique like voting, where the votes are taken from all servers and majority wins. There are many other techniques how these sorts of issues can be resolved, but these are beyond the scope of this course. The other technique is to distribute the file system state using partitioning. As the name suggests, in partitioning every single machine has only a portion of the state, only a subset of all the files in the system. This may be done based on file names. For instance, all the files from a to m sit on one machine, and all the files from n to z sit on another machine. Or we may choose a policy where different directories are stored on different machines, where we'd somehow partition the hierarchical name space of the directory tree structure. There can be various criterias that can be used to decide how to partition all the state in the file system. Using this technique, we can definitely achieve greater availability compared to a single server design. Each server here will hold fewer files and therefore, will be able to respond to a request for those files more quickly, so will appear to be much more available. The most important benefit of this design is that it provides for greater scalability when we consider the size of the file system, the overall size of all the files stored in that file system. With replication, given that every server has to hold all the files, the size of the file system will be limited by the capacity of a single machine. In partitioning, if we need the bigger file system, we just add more machines and problem solved. And finally, unlike in the replication case, in the partitioning case when we need to perform a write to a single file, that will remain localized to a single machine. So that's much simpler than what we have here. One of the main problems with this approach is that when there's a failure, a portion of the data in this file system will be lost. So, all of the files that are stored on that particular machine, the machine that's failed, will not be accessible anymore. In addition, balancing the system is more difficult because we have to take into consideration how the specific files are accessed. If there is a particular file that's more frequently accessed by most clients in the systems, then that will create hotspots. Finally, these two techniques can be combined to have a solution where the files are partitioned across different groups or in different volumes. And each of these groups is then replicated, potentially with different degree of replication. For instance, you can have partitions of read-only files versus files that are also written to, and you can replicate the read-only files to a greater degree. Or you can consider having smaller partitions where there are files that are more frequently accessed, versus larger partitions that consist of more files but less frequently accessed files. And then you can consider using different degrees of replication for the partition that has more frequently accessed files, versus less frequently accessed files. So that overall each machine has approximately the same number of expected client requests. For a quiz, let's compare replication with partitioning. I want you to consider server machines that can hold 100 files each. Using three such machines, the Distributed File System can be configured using replication or partitioning. I want you to answer the following. First, how many total files can be stored in the replicated versus the partitioned Distributed File System? And second, what is the percentage of the total files that will be lost if one machine fails in the replicated versus the partition DFS. You should round your response to the nearest percentile. The answers are pretty straightforward. For the first question, we know each machine can hold 100 files in the replicated case, all files are present on every single machine, so regardless of the fact that we have three machines, we still can only hold 100 files. In the partitioned case however, every single machine can hold 100 different files, and therefore the total size of the file system is 300. This illustrates one benefit of the partitioned design, because it can support larger number of files in the file system. For the second question, if we lose one machine in the replicated case, we will still have two other machines that have all the files, so we will lose zero percent of the total files. In the partitioned file system however, 33% of the files will be lost. This is because when one machine fails, all the files that were stored on that particular machine will be lost. This data point illustrates one case in which the replicated design is better. It provides greater fault tolerance versus in the previous example, the partitioned distributed file system providing greater scalability in terms of the number of files. Also this is why a mixed approach of using both replication and partitioning types of techniques will provide greater flexibility in terms of achieving both size and resiliency. We will now look at NFS, a very popular distributive file system. In NFS, clients acts as the remote server over a network. And hence, the name Network File System. It's another contribution to computer science made by the great systems researchers at Sun. In fact, one of the reasons why protocols like RPC were developed was to help with the use of NFS. Its architecture is shown in this figure. Client requests an access files using the virtual file system interface and using the same types of file descriptors that they use to access files in their local storage. The VSF layer will determine whether the file belongs to the local file system or whether it needs to be pushed to the NFS client, so that it can pass it to the remote file system. The NFS client interacts via RPC with they NFS server that resides on a remote machine. This is the machine that actually stores the files. The NFS server accepts the request, forms them into a proper file system operation that's then issued to the local virtual file system and from there, it gets passed to the local file system on top of the local storage. What that means is that on the server machine, the requests that are coming from the NFS server module are serviced as any other file system operation that comes from any application running on this machine. When an open request comes from the client, the NFS server will create a file handle. This will be a bite sequence that encodes both the server machine as well as the server local file information. This will be returned back to the client machine and it will be maintained by the NFS client, so whenever the client application tries to access files that are stored on the remote server on the NFS. Internally, this file handle will be passed with every single request. If the files get deleted or the server machine dies, using this handle will result in an error, because we're trying to use stale data. Data that's no longer valid. On client's right operations, the data that needs to be written to the file will be carried as part of the RPC request from the client to the server machine. And in file read, the data blocks that will be read from the file will be the results from that RPC request that was sent from the client to the server. And as such, they will be passed back to the NFS client. And then ultimately, back to the application that issued the read operation. Let's recap the design of NFS by asking a question about file handles. In the previous morsel, we mentioned that a file handle can become stale. What does this mean? The file is outdated? The remote server is not responding? The file on the remote server has been removed? Or the file has been opened for too long? >From these options, the only correct option is the file on the remote server has been removed. The file is outdated really implies that the file has been written by somebody else potentially, since a particular client acquired the file handle. So that may be a consistency related problem, but it really doesn't return a stale handle. The fact that the remote server is not responding, that's really more of an RPC layer error. If the server is not responding that has nothing to do with our ability to access a file at some point or another on that machine. It may simply be a network error. And the final option is also wrong, the file handle will not become stale just because a file was open for too long. There may be some distributed file systems which provide only time places where a client is only allowed to keep a file open for a period of time, but NFS is not one of them. So for that reason this answer is not correct. NFS has been around since the 80s and has gone through several revisions. The popular NFS versions that are in use today and that come standard with Linux distributions are NFS version 3 and version 4. There is one fundamental difference between these two versions, and that is that, according to the protocol specifications, NFS version 3 is stateless whereas NFS version 4 is stateful. The fact that NFS version 4 is stateful, allows it by design, to support operations like client caching and file logging. And although NFS version 3 is stateless, actual implementation of this protocol typically incorporate additional modules so that file caching and logging can be supported. The caching semantics in NFS are as follows. For files that are not accessed concurrently, NFS behaves with session semantics. On close, the changes are flushed to disk. For files that are not accessed concurrently, NFS behaves with session semantics. On close, all of the changes made to a file are flushed to the remote server. And then on open, a check is performed, and if necessary, the cached parts of the file are actually updated, so the new versions of those files are brought in. However, as an additional optimization, NFS also supports periodic updates. These are used to make sure that there aren't any changes on the file that the client is working with. And using these periodic updates, we'll break the session semantics, when there are multiple clients that are concurrently updating a file. The periods on when these checks happen can be configured but, by default, NFS uses these values. It uses 3 second checks for the regular files and, at 30 second intervals, it checks whether there are any changes with the directories. The rationale behind these numbers is that, the directories as files are modified less frequently, and that when modified it is easier to merge those changes. So we don't have to check for changes in the directories as frequently as we have to check for changes in the regular files, and still have a consistent system. NFS version 4 further incorporates a delegation mechanism where the server delegates to the client, all rights to manage a file for a period of time, and this will avoid any of the update checks that we described here. With server side state, NFS can support locking. The way NFS supports locking is using a lease-based mechanism. When a client acquires a lock, the server assigns it a particular time period during which the lock is valid. It is then the client's responsibility to make sure that it either releases the lock within the leased amount of time or that it explicitly extends the lock duration. This helps deal with situations of client failure, so in this case, when a client fails, on the server side, we'll just realize that okay, the lease for this client expired, so we can assign the lock to somebody else. And then when the client comes back again, or when network connectivity is re-established, the client will know that the lease expired, it no longer has a valid lock, so whatever changes it was trying to make, it simply has to redo. They weren't applied in a exclusive manner. NFS version 4 also supports more sophisticated mechanisms than just a basic log. For instance, it supports a reader writer lock called share reservation, along with mechanisms serve on how one can upgrade from being a reader to being a writer for a file, and vice versa. To recap again, the file sharing semantics supported by NFS, here is a quiz asking you how NFS maintains the cache consistency. Which of the following file sharing semantics are supported by NFS in its cache consistency mechanisms? Check all that apply from these options. UNIX, session, periodic, immutable, or neither. Let's see first which options we can eliminate quickly. NFS definitely allows for files to be modified, so immutable is not one of the correct answers. And also being a distributed system and if this doesn't meet guarantees that an update for a file will immediately be visible, so it's not Unix. Now we said that in principle, NFS tries to make session semantics in that the updates made to a file will be flushed back to the server when the file is closed. And also when a client performs an open operation, the client can check with the server to see whether the file has been updated. And in that case, the client will also update the cached value. The problem is however, that NFS can be configured to periodically have client and server interactions that check for any intermediate updates to our file during a session. Now how frequently this is done can be configured and in fact, it can be completely disabled. So, in that case, NFS will always behave like with session semantics. However, given that this option for periodic updates exists. It is not quite a session semantics. At the same time, it's not purely periodic file sharing semantics, because we will still have changes in the file propagate and file close. Or on file open, just as what happens with a session-based file sharing semantics. So for both session and periodic, yes, perhaps there are elements of the sharing semantics that NFS supports that are session like or periodic like. And whether it will behave like with session or periodic semantics, it will really depend on how NFS is configured. That leaves that by default NFS is really neither. It is not purely session-based file sharing semantics distributed file system. And also, it doesn't purely support just periodic file sharing semantics. Let's now look at another distributed file system example, the Sprite Distributed File System. And also the decision it makes regarding its consistency protocols. We will base our discussion on the Sprite Distributed File system as described in the research paper Caching in the Sprite Network File System by Nelson and others. This is an older paper, and it doesn't describe a production file system like when we talked about NFS, instead, Sprite was a research-distributed file system. But at the same time, it was also actually built and deployed at UC Berkeley, and people were using it. What's really nice about this paper is that it has a lot of detailed explanations of what was the usage pattern, the file access pattern, that motivated the particular design that Sprite has. The authors used trace data that was gathered from users using a real distributed filesystem to understand something about the usage and the file access patterns that appeared in the real world. And then based on that, they understood what are the actual design requirements for a distributed filesystem? So they were able to really justify the decisions that they made in Sprite. In the paper on caching in the sprite system, the authors performed a study of how are files accessed in the file system used at their department. That was a production system used for all types of tasks. This is what they found. 33% of all of the file accesses are writes. What this means is that caching can be an important mechanism to improve performance. Two-thirds of all of the operations can be improved, but what about updating this remaining one third of the accesses? If we choose a policy that's write-through where every single write goes back to the server, that means that these one-third of the file accesses will not be able to use the fact that there is a local cache on the client side. What this means is that caching is okay, it's a useful policy to use in sprite. However, using write-through is not going to be sufficient. They need to figure out what else they can do. So one thing they considered was how about session semantics. We don't have to write-through whenever the file is updated, but when it's closed. However, then they looked at these two numbers and it turns out that 75% of the files are opened for only half a second, just very briefly. And that if you look at what is the number of files that's opened less than ten seconds, we go up to 90% of all of the files. This means that with session semantics, they will need to perform updates to the remote server within less than half a second for many of the files and then within less than ten seconds for really most of the files. So for that reason, session semantics is really not going to solve their problems. They're still going to have too many interactions with the remote file server and this is what they're trying to avoid. Now next observe something interesting. They realized that a lot of the data is deleted within a relatively short time after it is created. 20 to 30% of the new data is deleted within 30 seconds in their trace. And then for 50% of the new data, they observed that it was deleted within five minutes of being created. And they also observed that file sharing in their system was rare. That the situations in which multiple clients are at the same time working on a file, that really doesn't occur very, very often. So because of these observations, they made first the following decision. A write back on close, which is what appears in session semantics. Well, that's really not necessary. We don't really have two sharing situations and most of the data will get deleted anyways. So forcing the data to be written back to the server when the file is closed, doesn't seem like it will be useful. If the file is deleted, who cares. Now, all of these things are not very friendly to situations where a file needs to be accessed concurrently by multiple clients. However, the fact that they observe that file sharing is very rare, that meant that, that's okay. There's no need to optimize for this kind of situation of concurrent access. However, they did observe some file sharing. It's not like their statement is that there is no file sharing in the system. So because of that, they have to make sure that this distributed file system is useful for the situation when the files are truly shared and it somehow must be supported. Based on this workload analysis, the author's made the following design decisions in Sprite. First, the Sprite will support caching and it will use a write back policy. First, every 30 seconds the client will write back all the blocks. That have not been modified in the past 30 seconds. The intuition behind this is that the blocks that are more recently modified will continue being modified, that the client is still working on that part of the data. So it does not make sense to force to write-back those particular blocks. Instead, wait a little bit until the client is done working on that piece of the file. And then right them back to the server. This will avoid repeatedly sending the same blocks over and over back to the server. Note that this 30 second threshold is directly related to this value 30 seconds which is their observation that a lot of the data will get deleted within 30 seconds. When a client comes along and wants to open a file that's currently being written by another client, the server will contact this writer client and will collect all of the outstanding dirty blocks. In the system, there is no explicitly write-back on close, so it's possible that a client has finished working on a file completely, closed the file, and there's still modified blocks in that particular client cache. Note that with this policy, Sprite allows for a file to be open, modified, closed, open, modified, closed multiple times, before any data actually gets written back to the server, and this is one way in which Sprite is able to optimize this 33% of write accesses. Now, note for this to work, every open operation has to go to the server. And what that really means is that the directories cannot be cached on the client. So the client can not perform a directories related operation, that looks up a file, and opens a file, and creates a file directly, using its cache only. It has to go to the server. And finally, in the cases where these rare concurrent writes do occur, then Sprite will completely disable the caching for that file. And all of the writes will be serialized at the server side. In summary, Sprite distinguishes between two situations. When the files are accessed in a way where the writes don't really happen concurrently, instead over time the clients take turns who's writing to the file, Sprite allows caching and provides sequential semantics for the file sharing. In contrast when Sprite determines that a file is shared concurrently for write when multiple clients want to write to that file, then Sprite completely forbids caching. It will disable caching. Because this situation doesn't happen frequently the penalty on performance will not be significant. Let's now illustrate the policies that are used in Sprite by walking through what happens in different types of file axises. In the process, we will also look at what are the different pieces of stale that the Sprite distributed file system would have to maintain and the server, and at the client's side in order to be able to maintain the required file sharing semantics. Let's say, initially we have n clients that are a, accessing the files for reading and one writer client. All open operations will go through the server and the server will allow all accesses. All of the clients, readers and writers will be allowed to cache blocks of this file and the writer client will have to keep track of when was each block of the file modified in order to be able to enforce the write back policy in Sprite every 30 seconds, the blogs that have not been modified in the past 30 seconds. So we'll have to keep track of some timestamps. The Sprite writer can keep closing the file and then deciding to reopen it to modify it some more. When it decides to do this, the contents of the file are cached locally in the writer's cache, but the open still has to go to the server. At this point, the writer will need to check quickly with the server whether its cached value is the same as what the server sees. And because of that, they'll need to keep some sort of version number. To support these operations, the client would need to keep track of some information for each file. This includes some status whether or not the file is in the cache or not, then what are all the cached blocks from that file. For all of the dirty blocks, when were they modified last? So that we can run the write back mechanism and then also version number. The server will also keep track of some information for each file, like what are the current readers? What is the current writer of the file? And also what is the current version of this file? Now let's say, at some point after the writer W1 has closed the file, another writer W2 shows up. And this is what you refer to as a sequential writer. They're not trying to write the file at the same time. When a situation like this happens, this is what we refer to as sequential sharing. What needs to happen in that case is the server needs to contact the last writer that it is aware of. In this case, that's going to be W1 and to gather all of the dirty blocks and it's very convenient that W1 keeps track of the dirty blocks. If W1 has closed the file, the server will update. The new version will update the new writer. The W2 is the current writer of the file and at that point, W2 can proceed and it can actually cache the file. And now while W2 is still modifying the file, it still has the file open and is accessing it and it's writing to it. We have that unfortunate situation that rare situation where W3 appears and it wants to concurrently at the same time write to the file. So this is what Sprite refers to as concurrent sharing. There are two writers, W3 and W2 trying to write to the file at the same time. When the write request comes, the server will just like before, contact the last writer to gather the dirty blocks and that's going to W2 in this case. Now, once the server contacts W2, it will also realize that W2 hasn't actually closed the file. What will happen in that case is that W2 will write back all of the dirty blocks and then the server will mark this file as it is not cachable, it will disable the caching of this file for everybody. Both W3 and W2 will continue to have access to the file, except they will not be able to use their client's side caches and all of the file accesses will have to go to the server. For this reason, it makes sense on the server side to maintain some information for each file whether or not the file is cachable. When one of the two writers, W2 or W3, closed the file, the server will now see that close operation, because every single operation in this case will go to the server. The file is not cached, otherwise. When the server sees that one of the clients closes the file, at that point, it will change this cacheable flag to point to yes, the file is cacheable. And so the remaining clients can start caching the file and can enjoy the performance optimization of caching. So, one unique feature of Sprite is that it dynamically enables and disables caching depending on whether or not there is a sequential write sharing versus concurrent write sharing among the clients in the system. In this lesson we looked at distributed file systems and described the use of caching and accompanying mechanisms to provide optimized file access performance on one side. And also to maintain meaningful file-sharing semantics on the other. We looked at the specific decisions made in NFS and the sprite distributed file system. And for sprite we looked how its design was motivated by certain observations regarding file usage and file access patterns. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In this lesson, we will talk about distributed shared memory systems for DSMs. These are an an example of applications that perform certain distributed state management operations. Now, in this lesson, a lot of what we will talk about will be in the context of memory management. Still the methods that we will discuss will apply to other types of distributed applications. Specifically, we will present certain design alternatives, with respect to DSM systems. And we will discuss the trade offs that are associated with those decisions. In addition, we will discuss several important consistency models that are commonly used in systems that manage distributed and shared state In this lesson we will be talking about another distributed service, specifically distributed shared memory. Now recall that in a previous lesson on memory management, we made a visual analogy between the memory contents of a process and the parts used by workers sharing a single work space. Holding off of that analogy, managing distributed shared memory is like managing the toy parts or even the tools across all the work spaces in a toy shop. As you can see from this image, it is very similar to the one that we used for the distributed file systems because we want to show that there are multiple distributive nodes that are working together towards providing a service. The service that's provided is the sharing of the tools and the toy parts among workers that are working on multiple such work benches. So you imagine even in a single location like this, there will be multiple such workbenches with multiple workers using certain toys and toy parts and tools and this is what we want to facilitate, the sharing of those tools and parts. For the management of tools and toy parts in a toy shop there are three major decisions that must be made. The first one is how are we going to place all these resources in the distributed environment. Next we must decide how and when to move tools around the different shops or the different work benches in those shops, and finally we must establish some rules how those toy parts and tools will be shared. For instance, regarding placement we would probably decide to place those resources close to the workers that are currently needing them to perform their task. When a worker on one work bench is done with a particular resource and another worker on another workbench or potentially in another location needs that resource we should move it as soon as possible. And finally we must have policies on how long can a worker keep a tool on their desk or when and how workers should let others know that a tool or a toy part is ready and is available for use by others. Or for instance when should the tools or the toy parts be placed into a storage facility that others can access as well. For managing distributed shared memory, we must make similar types of decisions however clearly the details are going to be very different. For instance, when deciding placement, we should typically place the contents of memory or the contents of a page in a physical memory that's close to a particular process. This would be the process that either created that content and or will be using that content. When a process needs access to memory that's stored in a page param on a remote node, we should come up with some policy how to migrate that content to the local node. For instance, we can copy that content that memory page from the remote to the local physical memory. Since the memory page may be concurrently accessed by multiple processes, and in addition since we may have multiple copies of that page in different locations in the system, it's important to establish some rules how the different operations are ordered. How they're propagated across the different copies, so that there is a consistent view of the entire memory. In the previous lesson, we talked about Distributive File Systems. We said that Distributive File Systems were an example of a distributive service where the state, the files, were stored on some set of nodes, the file servers. And then the file servers were accessed by many distributive nodes clients. All of the file access operations open, read, write on these files. Where then service that was provided by these servers, and that these client nodes were requesting from them. Our primary focus of the Distributed File System lesson was the caching mechanisms. These are useful so that server data is cached on the clients. And this is a mechanism that improves the performance that's seen by the clients. The clients can now access these files more quickly. Caching is also a mechanism that enhances the scalability of the distributed file system overall of the servers. Because some of the operations are now performed by the clients, the servers don't have to perform every single access and therefore they can scale to support a larger number of clients and serve larger number of files potentially. What we didn't talk about is, what happens in the situation when there are multiple file servers? And how do they control and coordinate the access of shared state, of shared files, among them? We also didn't talk about situations in which all of the nodes in the systems were both servers, storing files and allowing access to those files, as well as clients sending request to other nodes in the system. In such a scenario there really isn't a clear distinction between client nodes and server nodes. Every single one of the nodes would be performing all of the services. In this is lesson distributed shared memory, we will talk about these kinds of scenarios, the distributed applications, where all the nodes in the distributed environment are both the servers that provide the state and provide the access to that state and also issue requests to other nodes in the system to their peers. So, perform some of the client functionality that we saw before. Let's talk now about these cases where there isn't a clear distinction between servers and clients. For instance if you look at all of the big data applications that we hear about today, the state of the application is distributed and stored across all nodes. The processors on these nodes run certain applications such as the analytics codes, the mining codes, searching for patterns or searching in order to answer your Google web search request or some other applications. And they access the state that's distributed across all the other nodes in the system. The key about these applications is that every node in the systems owns some portion of the state. By own here we mean that there is some state that's locally stored in a particular node or that's generated by computation that's running on that node. So the overall application state is the union of all of these pieces that are present on every one node. It's as if all the nodes in the system are peers. They all require accesses to the state anywhere in the system and they all provide access to their local storage state to any of their other peers in the system. For instance, think about the large web search applications, like Google. The state in that application, which is the information about every single webpage in the world, is stored on many machines. And at the same time, it's also processed on all of them. Whenever you type in a web search, it may get processed on a different processor in this distributed system, but the statements required for that web search, regardless in which processor it hits, may be somewhere on some of the other nodes. Applications like Facebook and YouTube, also have state that includes billions of images and videos. In these images and videos are a process to detect certain objects to transform them into different formats and sizes. So there's some computation that's performed on that distributed content. The code that performs these processing operations, these transformations, trans-coding of the videos or the images, may run on different nodes in the system, but may potentially need to access state that's distributed on other nodes. These applications are examples of applications where every node is responsible for managing the locally stored state and providing access to the locally stored state. But also at the same times accessing the state that's stored by the remaining nodes in the system. The reason I'm putting the term peer here in quotation marks, and not using a popular term that you may have heard of, peer to peer systems, is that in these kinds of distributed applications it is still possible that there will be some number of designated nodes that will perform some overall management and configuration tasks for the entire system. So, there's some portion of control plan tasks or management plan tasks that are performed by some more centralized designated set of nodes that's not evenly distributed among all nodes. In a peer-to-peer system, even the overall control and management of this system would be done by all. A concrete example of this type of applications is distributed shared memory. Distributed shared memory is a service that manages the memory across multiple nodes so that applications that are running on top will have an illusion that they're running on a shared memory machine, like a SMP processor. Each note in the system owns some portion of the physical memory, so owns some state, and it also provides the operations in that state, the reads and writes. They may be originating from any of the other notes in the system. Also, each note has to be involved in some consistency protocol, so as to make sure that the shared accesses to the state have meaningful semantics. For instance, when nodes read and write shared memory locations, these reads and writes have to be ordered and observed by the remaining nodes in the system in some meaningful way. Ideally, in the exact same way that they would have been perceived if this was indeed, a shared memory machine. In this lesson, using distributed shared memory as an example, we will illustrate some of the issues that come up with distributed state management, beyond just caching, that we already saw in the distributed file system lecture. We will also discuss some meaningful consistency protocols that are useful in these kinds of scenarios. Distributed shared memory mechanisms are important to study because they permit scaling beyond the limitations of how much memory we can include in a single machine. If you have a multi-thread application or in general an application that was developed with the expectation of shared memory underneath and all of the sudden you need to support work loads that require more state, you have to add more memory to that system. Now, if you look at how the cost of computer systems is affected by the amount of memory they're configured with, you will see that beyond a certain limit, the cost starts increasing rapidly, and machines with very large amounts of memory can be in the order of half million dollar range. Instead with distributed shared memory we can simply add additional nodes and achieve shared memory at a much lower cost. Yes, access in remote memory will be slower than access in the local memory. However, if you're smart about how data is placed in the first place in the system, how it's migrated across the different nodes, and what kind of sharing semantics are enforced whenever something gets updated, we may hide those access difference from the applications so we may not even perceive there is any kind of slowdown because they're executing and distributed in that environment. One goal of this lecture is to teach you what are some of the opportunities to hide these access differences. Distributed shared memory is becoming more relevant today, because commodity interconnect technologies offer really low latency among nodes in a system. For instance, these are interconnect technologies that connect the servers in a data center and they offer these RDMA based interfaces, where RDMA stands for Remote Direct Memory Access. That provide a really low latency when accessing the remote memories and that really helps address this particular challenge, the fact that accessing remote memory is slower. Using these advanced interconnection technologies makes these remote accesses significantly better than what they were before such interconnection opportunities existed. Because of that, distributed shared memory based solutions are becoming more sustainable. Distributed shared memory can be supported either in hardware, or by software. The key component of the hardware supported distributed shared memories, is that they rely on some high end interconnect. The operating system running on each individual node, is under the impression that it has much larger physical memory, constituting memory that spans multiple memory nodes, multiple physical nodes in the system. So, the OS is allowed to establish virtual to physical memory mappings that correspond to memory locations that are in other nodes. The memory accesses that reference memory locations that don't correspond to the local physical memory are then passed to the network interconnect card and this is a NIC that corresponds to this advanced high end interconnect technology. So, these NICs know how to translate that memory operation, into an actual message that gets sent to the correct remote node, the correct NIC. The NICs in these nodes will participate in al aspects that are necessary to deal with memory accesses, and management of memory consistency and we'll also support certain atomic operations just like the atomics that we saw in shared memory systems. Now while it's very convenient to rely on the hardware to do everything, this type of hardware is typically very expensive and it's used only for the ultra high end machines or for the super computing platforms. Instead, distributed shared memory systems can be realized in software. Here the software would have to detect which memory accesses are local versus remote to create and send those messages to the appropriate note, whenever that's what, what's necessary. The software would also have to accept messages from other notes and perform the specific memory operations for them and also be involved in all of the aspects of memory sharing and consistency support. This can be done at the level of the operating system or it can be done with support of a programming language and the runtime product programming language. For a quiz, I would like you to take a look at a paper, Distributed Shared Memory: Concepts and Systems. This was an older survey paper that describes several implementations of distributed shared memory systems, and compares them along multiple dimensions that we will discuss in this lesson. The specific question you need to answer is the following. According to the paper Distributed Shared Memory: Concepts and Systems, what is a common task that's implemented in software, in hybrid, hardware plus software, DSH implementations? The choices are prefetch pages, address translation, or triggering invalidations. As a hint go straight to page 76 of the original paper PDF that's linked in the instructor's notes and start reading at the hybrid DSM implementations heading. So you don't have to read the entire paper, but I do encourage you to do so if you have a chance or at least to look at it in a little bit more detail than just jumping to this hybrid DSM implementation section. If you read through the hybrid DSM implementation section you will see that it mentions several examples of DSM systems and specifically describes what are the software tasks that those hybrid implementations support. And for every one of those examples prefetching is listed as one of the software tasks. It makes sense that prefetching is a good task to be implemented in software. Whether or not prefetching is useful is going to depend on a particular application, on the kind of access pattern that it exhibits. At the same time, address translation or triggering invalidations are more concretely defined. And it's easier to implement them with hardware support. For these reasons, prefetching pages is the only correct answer to this question. Several design points are important to consider when designing a distributed shared memory system. First is the granularity of sharing. In S and P systems, the granularity of sharing is a cache line. The hardware tracks concurrent memory accesses at the granularity of a single cache line. And triggers all the necessary coherence actions like invalidations, if it detects that a cache line has been modified, if that cache line has been shared with other caches. For distributed systems, adopting a solution where every single cache line sized write message is being sent to nodes over a network will potentially be too expensive. And it will be hard to justify the use of such system, the performance slowdown will be significant, and likely, it won't be very useful. Instead, distributed shared memory designs look at larger granularities of sharing. Some options include variables, or pages of virtual memory, or entire objects as defined by some higher level programming language. Variables are meaningful from the programmer's perspective so potentially DSM solutions can benefit. Because the programmer can provide some explicit support to tell the distributed shared memory system how and when individual variables should be shared. However, this is still potentially too fine granularity. We have a lot of variables that just few bytes long, like integers. And in those settings, the DSM system would still have very high overheads. Using something larger, like an entire page of content or a larger object, that begins to make more sense. If the distributed shared memory system is to be integrated at the operating system level, the operating system doesn't understand objects. The only thing that it understands is pages of virtual memory. And then, the OS tries to map those pages of virtual memory to the underlying physical page frames. So at the operating system level, it makes sense to integrate some page-based DSM solutions. The OS would track when pages are modified, and then, it would trigger all of the necessary messages that need to be exchanged with remote nodes on page modification. Pages are larger. We set a common page sizes for kilobytes in many environments. And so it is possible to then amortize the cost of the remote access for these larger granularities. With some help from the compiler, application level objects can be laid out on different pages, and then we can fully just rely on the page base operating system level mechanism. Or, we can have a distributed shared memory solution that's actually supported by the programming language and the runtime, where the runtime understands which objects are local versus remote objects. And for those objects that are remote ones, the run-time will generate all of the necessary communications with remote nodes and all the necessary operations to provide distributed shared memory. In that case, the operating system doesn't really need to know anything about the DSM solution. So, the benefit of that is that the operating system doesn't have to be modified in this case, but this is clearly going to be a less general solution. It will be applicable only for those languages for which there is such a DSM support. Once we start increasing the granularity of sharing, one important problem that everyone has to be aware of is what's called false sharing. Consider a page or even a higher level object that internally has two variables, x and y. A process in one node is exclusively accessing and modifying x. It doesn't completely care about anything that's stored in the variable y. Similarly, a process on another node is exclusively concerned with y. And it has no reference whatsoever, to the other variable, x. Now when x and y are shared on the same page, as in this example here, the distributed shared memory system, when it's using these larger granularities, that's the only thing that it understands. So it understands a shared page. So it will interpret these two write accesses to that shared page as some indication of concurrent accesses to the same location. And it will trigger any necessary coherence operations, invalidations, updates, or any of the other overheads that are associated with maintaining consistency among these two copies. Such coherence overheads won't benefit anyone. P1 doesn't care what happened to y, and also, P2 doesn't care what happened to x. In order to avoid these kinds of situations, the programmer must be careful how data is allocated and laid out on pages, or how it's grouped in higher level objects. Or the other alternative is to rely on some smart compiler that will be able to understand what is really shared state. And then allocate it within a page or within an object, versus what is something that will trigger these false sharing situations. Another important design point in distributed shared memory systems is to understand what are the types of access algorithms that need to be supported on top of that DSM solution? In other words, to understand what are the kinds of applications that will be running on top of the DSM layer. The simplest example is single reader/single writer. For these kinds of applications, the main role of the DSM layer is to provide additional memory, to provide the application with the ability to access remote memory. In this case, there really aren't any consistency or sharing related challenges that need to be supported at the DSM layer. The more complex examples are of an application support either multiple readers and single writer, or both multiple readers and multiple writers. In those cases, it's not just about how to read or write to the correct physical memory location in the distributed system, but it's also about how to make sure that the reads return the correctly written, the most recently written value of a particular memory location, and also that all of the writes that are performed are correctly ordered. This is necessary so as to present the consistent view of the distributed state of the distributed shared memory to all of the nodes in the system. Multiple reader, single writer, is a special, simpler case of the multiple reader, multiple writer problem. And so, in our discussion in this lesson, we will focus on DSM support for multiple readers, multiple writers. For a distributive shared memory solution to be useful, it must provide good performance to applications. If we think about the core service that's provided by distributive shared memory systems, accessing memory locations, then it's obvious that the performance metric that's relevant for DSM systems is what is the latency with which processes running on any one of these nodes can perform such remote memory accesses. Clearly, accessing local memory is faster than remote memory. So, what can we do in order to maximize the number of cases where local memory's accessed versus remote? One way to maximize the number of local accesses and achieve low latency is to use a technique called migration. Whenever a process on another node needs to access remote state, we literally copy that state over to the other node. This makes sense for situations where we have a single reader, single writer. Since only one node at a time will be accessing this state, so it does make sense to move the state over to where that single entity is. However, this requires moving the data, copying the data over to the remote node, and that incurs some overheads. So even for these single reader, single writer cases, we should be careful when we trigger these types of mechanisms because if it's only going to be a single access that will be performed in this other location, then migrating, copying over the entire state over to node four, it won't be amortized. We won't get much in terms of low latency improvements if we have to copy all this data just for a single read or write access. For the more general case, however, when there are multiple readers and multiple writers, migrating the state all over the place doesn't make any sense since it needs to be accessed by multiple nodes at the same time. So, a mechanism such as replication where the state is copied on multiple nodes, potentially on all nodes, is a more general mechanism. Use of caching techniques, which create a copy of the state on each node where the state was accessed, can lead to some similar behavior as what's seen with replication. One problem with this is that it requires consistency management. Now this state will be accessed concurrently on multiple nodes. And we have to make sure we coordinate those operations, as we said, to order all of the writes, propagate the most recent write operation to wherever or whomever is performing the next read operation. This is some overhead. One way to control the overhead is to perhaps limit the number of replicas, the number of copies that can exist in this system at any given point of time since the consistency management has overhead that's proportional with the number of copies that need to be maintained consistently. Let's take a quiz about DSM performance. The question is: if access latency as a performance metric is a primary concern, which of the following techniques would be best suited in your DSM design? The choices are: migration, caching, or replication. And you should check all that apply. Before answering we should consider the access algorithm, like single reader, single writer, multiple reader, multiple writer. If we only have single reader, single writer then migration is okay, but it's not good in all of the other cases. In a more general problem that has multiple readers and multiple writers, with migration, pages would have to be flipped back and forth between nodes, and so migration is really not a good technique. Migration can in fact lead to an increase in latencies for the more general problem. If we look at the other two options, caching brings the data on the node, where it's accessed, and therefore it will definitely improve the latency of the subsequent operations to that data. And similarly replication in general will create copies of the data that are potentially closer to where the data is accessed and therefore can leads to improvements in latency. Now whenever there are multiple concurrent writes in the system, caching and replication can also lead to high overheads. If you remember in the distributed file system lecture, we mentioned that in the sprite file system, whenever it was detected that there are multiple concurrent writers, caching or in general the presence of multiple copies of the particular state, a file in that case, was disabled so as not to have to deal with multiple invalidations or loss of consistency. Once we've permitted multiple copies of the same data page or object to be stored in multiple, locations, the question of maintaining consistency comes up. Since distributed shared memory is intended to behave in a similar manner to shared memory in shared memory multi processors, let's remind ourselves what we did in shared memory multi processors, for consistency management. In the lesson one synchronization, we explained that in shared memory multiprocessors consistency is managed using two mechanisms, a write-invalidate or write-update. With write-invalidate, whenever the content particular memory location that's cached to multiple caches is changes on one CPU in one cache. Back via the coherence mechanisms will be propagated to other caches and in the case of write and validate the other caches will invalidate their cache content. Or in the event that we have a write update coherence mechanism, then the other caches will receive the newly updated copy of that particular memory location. These necessary coherence operations are triggered by the shared memory support in the hardware on every single write operation. And the overhead of supporting that in the distributed shared memory system where the latencies and the costs of performing distributed communication are too high is not going to be justifiable. For these reasons for distributed shared memory, we'll look at coherence operations that are more similar to what we discussed in the distributed file systems lecture. One option is to push invalidation messages when a data item is written to. This is similar to the server-based approach that we talked about in the distributed files systems lesson. But remember that the state management in DSM systems is done by all peers. We don't have clients and servers, per se in this case. The other option is for the nodes to pull information about any modified state from one or more of the other nodes in the system. This can be done either periodically or purely on demand whenever some process needs to access that state locally, it will check with others to see whether it's been modified or not. I intentionally chose the terms push and pull since these are commonly used to distinguish between this more proactive versus this more reactive approach to accomplishing some tasks. In this case, maintaining the consistency among two notes in the distributed system. Another set of terms associated with these types of actions is eager versus lazy. The push based method is eager since it forces propagation of information immediately, eagerly. in contrast, the pull method is lazy, since it lazily gets the information when it's convenient or when it becomes absolutely necessary. And yet, another set of terms to distinguish between these two types of approaches is pessimistic versus optimistic. This push based eager method is pessimistic in that it expects the worst. They expect that the modified state will be needed at other places at other nodes immediately. And so with these methods, nodes are in a rush to notify others that a modification happened. In contrast, these optimistic methods hope for the best. Here the hope is that the modified state wouldn't be needed elsewhere anytime soon. And that there is plenty of opportunity to accumulate information regarding modifications before anyone has to pay for the cost of sending an invalidation or moving data across the distributed system. Regardless of whether we talk about the push versus pull based methods. When exactly they get triggered, whether its after every single data has been modified or whether its with a period of five seconds or ten seconds or one millisecond. Or in some other manner that really is going to depend on the consistency model for the shared state and we will discuss what are the options for consistency models a little bit later in this lecture. Based on what we described so far, let's take a look at how a distributed shared memory system can be designed. This type of system consists of a number of nodes, each with their own physical memory. Everyone of the nodes may contribute towards the distributed shared memory system, only a portion of their physical memory, or it can contribute all of it. Let's assume here, that only a portion of the physical memory, is contributed towards the DSM service and can explicitly be addressed. Whereas the rest of the memory is used either for caches or for replication or for some metadata that's needed for the DSM layer. The pool of memory regions, the memory pages that every single one of these nodes contributes, forms the global shared memory that's available for applications running in the system. Every address in this memory pool will be uniquely identified based on the identifier for the node where it's, residing, as well of the page frame number of that particular physical memory location. The node where our page is located is typically referred to as the home node of that page. Now let's say, we're trying to solve the most general case where the system is supposed to support applications that have multiple reader, multiple writer requirements. For that reason, in order for the system to deliver acceptable performance, and achieve low latency with which the memory accesses are performed, the DSM layer will incorporate use of caching. Pages will be cached on the nodes where they are accessed, and for a state that's cached, for memory pages that are stored on these remote nodes. The home node, or the manager node, will be responsible for driving all of the coherence related operations, so, it will maintain state, that's necessary to track the number of readers, the writers, whatever cache has been modified, et cetera. In this way, all of the notes in the system are responsible for some portion of the management operations for that distributed memory. The information that's maintained at the home, or the manager node is similar to the kind of information that we saw that the servers have to maintain in the distributed file system example. Except that in this situation every single one of the nodes is responsible for both providing the service, the acting as a server for that particular portion of the shared memory. And also being the client with respect to the other nodes. So every one of the nodes will participate in interactions with, a home node in case they are locally caching some of the memory that's stored at a remote site. The home node will have to keep track of states, such as, what are the pages accessed, who is it accessed by, whether it's been modified. We may also choose to incorporate mechanisms to dynamical enable/disable caching for. For a similar motivation, like what we had in the sprite file system. What are the pages locked, that's another useful piece of information. All this information is used in enforcing the, shearing semantics that this particular DSM system will implement. One particular page is repeatedly and even exclusively accessed on a node that's not its home node. It would be too expensive to repeatedly contact the home node to perform any necessary state updates, so one mechanism that's useful in DSN systems is to separate the notion of home node, from the so-called owner. The owner is the node that currently owns the page that's, like the exclusive writer for instance. And that's the node that can control all of these state updates and can drive any consistency related operations. So this owner may be different from the home node, and in fact the owner may change, as, whoever is accessing this page migrates throughout the system or new processes, new threads require access to this particular page. They may become owners as well. The role of the home node for that page, in this case, that was node two, is to keep track of who is the current owner of that page. In addition to creating page copies via caching, in an on demand manner, page replicas can be explicitly created, for reasons such as load balancing, hot spot avoidance, or reliability reasons so that the page contents do not disappear if some note in the system fails. For instance, in data center environment that have lots of machines, where a certain distributed shared state is managed, it makes sense to triplicate such shared state on the original machine, on a nearby machine, for instance in the same rack, and then, on another remote machine, whether it's in another rack or even potentially in another data center. The consistency of these replicas is controlled either by the home node or by some designated management node. In summary, here's some of the key architectural features of distributed shared memory systems. Specifically we're talking about page based distributed shared memory systems. Every node contributes part of its main memory pages to the DSM layer. Local caches are maintained to improve performance by minimizing the latency of the access to story node modes. All nodes are responsible for some portion of the distributed memory and of its management. Specifically, the home node for a page manages the accesses to that page and also tracks who currently owns that page. Who has ownership rights. And finally, some explicit replication could be considered in order to assist with load balancing. In order to further improve performance and to address certain reliability concerns. One important aspect of distributed shared memory systems is, how do we determine where a particular page is? In order to do this the DSM component has to maintain some metadata. First, let's see how is a page identified in this system in the first place. Well, the page has its address. And that may be some notion of the node identifier plus the page frame number locally at that node. And we said that the node identifier also identifies the home node of the system. Considering this address format then, we see that the address itself directly identifies what is the manager node, the home node. That knows everything about that particular page. Every single node in the system just by looking at the address of a particular page. Can uniquely determine what is the manager node for that page. So it's as if the manager information is available on every single node in the system. This could be captured via a global map that has to be replicated that will tell us how for a particular object we find the manager. What that means is that the information about the manager is available on every single node. So it's as if the information about the manager is available on every single node. Since whenever any of the nodes in the system wants to access a particular address, they just need to look at the node identifier. And we'll be able to get to the specific manager, the specific home node for that address. This information, that captures the translation from an identifier of an object, a page in this case. To the management node of that object. Represents a global map. And this needs to be replicated. It needs to exist. This information needs to be available on every single one of the nodes. Once we get to a particular manager, that's the entity that will have the per-page, or per-object, metadata. That's necessary in order to perform this specific access to that page or to enforce its consistency. What this means is that in the DSM system, the metadata for individual objects or individual pages. It's partitioned across all of the home nodes. All of the management nodes in the system. But in order to quickly find where the manager for a particular data item is. It is necessary to maintain an additional data structure, a global map, that will be replicated on every single one of the nodes. That will in some way take an object identifier and map it to that manager. One final note, in the example that we discussed so far. We somehow implied that certain bits from the address are used to identify the node identifier. And that means that for every single page, there will be a fixed manager uniquely identified from that page identifier. If we want some additional flexibility, we can take that object identifier and then use it as an index into a mapping table. This mapping table is the one that will be used at every single node for the global map. And every entry in that mapping table will actually encode a manager node. So for a particular object or a particular page identifier, we will first index into that mapping table. Using some bytes from the object id or some hash function. That's applied on top of this object id. And then, the entry at the particular location at the mapping table will tell us what the manager node for that page is. What's nice about this approach is. If for any reason, we want to change who is the manager node for a particular object or of a particular page. Whether the original manager field or whether we need to rebalance the system or any other reason. We just need to update the mapping table. There is no need to consider making any kind of changes for the object identifier. The object can remain identified in the exact same way as it was before we chose to make a change into the manager nodes. Now that we described the possible DSM architecture, I'd like to comment on its possible implementation. One thing to consider when implementing a distributed shared memory system is that the distributed shared memory layer has to intercept every single access to that shared state. This is needed in order to detect whether the access is local or remote and then trigger remote messages requesting access, if that's necessary. Or to detect that somebody is performing an update operation to a locally controlled portion of the distributed shared memory and then to trigger any necessary coherence messages. These overheads should be avoided whenever we're trying to just access local, non-shared pages, or non-shared state. So what we would like to achieve is an implementation where it is possible to dynamically engage whether the distributed shared memory layer will be triggered, and will be intercepting any accesses to memory in order to determine what to do about them. Or disengage the distributed shared memory layer if we are performing access to pages which are really not shared and are just local pages accessed on a particular node. To achieve this, a DSM implementation can leverage the hardware support that's available at the memory management unit level. As we explained earlier, if the hardware MMU doesn't find the valid mapping for a particular virtual address in the page table, it will trap into the operating system. And similarly, the hardware will also cause a trap if it's detected that there is an attempt to modify a page that has been protected for a write operation, so a write protected page. We can leverage this mechanism to implement the DSM system. Whenever we need to perform an access to a remote memory, there will not be a valid mapping from the local virtual address to the remote physical address. The hardware will generate a trap in that case. And at that point, the operating system will detect what is the reason for that trap will pass that page information to the DSM layer, and the DSM layer will send the message. Similarly, whenever content is cached in a particular node, the DSM layer will ensure that that content is write protected. And that will cause a trap if anybody tries to modify that content that will turn control over to the operating system. The operating system can pass relevant information to the DSM module. And then that one will trigger all of the necessary coherence operations. When implementing a DSM system, it is also useful perhaps to leverage additional information that's also maintained by the existing memory management that the operating system and the underlying hardware provide. For instance, we can track information, like whether our page is dirty. We can track information whether our page has been accessed in the first place. And this can let us implement different coherence mechanisms and consistency policies. For an object-based distributed shared memory system that's implemented at the level of the programming language run time, the implementation can consider similar types of mechanisms that leverage the underlying operating systems services or as an alternative, everything can be done completely in software with support from the compilers, so tracking whatever particular object reference is remote or local. Or whether an object is going to be modified or not, we can make sure that we generate code that will perform those checks on every single object reference. We said multiple times throughout this lecture that the exact details of how a distributed shared memory system should be designed or how the coherence mechanisms will be triggered depends on the exact Consistency Model. But before describing several Consistency Models, let's first explain what is a Consistency Model. Consistency models exist in the context of the implementations of applications or services that manage distributed state. The consistency model is a guarantee that the state changes will behave in a certain way, as long as the upper software layers follow a certain set of rules. For the memory to behave correctly what that means is that we're making some guarantees how our memory access is going to be ordered. For the memory to behave correctly, what that means is that the way that the operations will access memory will somehow be representative to how those operations were issued in the first place. And that, we will be able to make some guarantees that whenever somebody is trying to read the memory location that the value that they will see will be a value that's representative of what was written to that location recently. Now, what it means is that a consistency model guarantees that the memory will behave correctly, that the accesses will be correctly interleaved and the updates will be correctly propagated only if the software follows certain rules. That implies that the software needs to either use specific APIs when it performs an update or when it requests to perform an access, or that the software needs to set certain expectations, based on what this memory guarantee is. For instance that I'm just not going to enforce that updates the particular memory location are written in the exact same order as they were issued. If the software knows this particular information, then it is up to the programmer to use some additional operations, such as locking constructs or similar in order to get the desired behavior. This is not something that's specific to a distributed system. Even in a single CPU, when we have multiple threads that are trying to access a particular memory location, we know that there's no guarantee how those thread accesses will be ordered. And which particular update will be returned when, let's say, thread end tries to read a particular shared variable. If we want to achieve some guaranties that are stronger than that, the software will have to use locks, would have to use atomic operations, would have to use some counters. So, a consistency model, it specifies how the memory will behave and the upper layers of the software must understand that and set their expectations accordingly. At the same time, the memory layer may export certain API's, certain operations like the locks that we mentioned. And then, if the software uses those API's correctly, then perhaps the memory system can make even stronger guarantees. In the discussion of consistency models, we will use timeline diagrams which will look like this. Which will show when certain operations occur according to real time, based on some neutral external observer that sees everything instantaneously the minute those operations occur. Our notation for this is as follows: R_m1(x) means that this value x was read from a particular memory location m1. What this means here, is that at this particular point of time, the value x was read from memory location m1. And then at this later point in time, the value y was read from memory location m2. Similarly, W_m1(y) means that the value y was written to in this case, memory location m1. So, at this particular point in time, the value z was written to a memory location m3. We will also assume that initially at the start of this timeline diagrams all of the memory set to zero Theoretically for a perfect consistency model, we would like to achieve absolute ordering and immediate visibility of any state update and axis, and we also want this to be in the exact same manner as those updates were performed in some real time. With this model, changes are instantaneous and immediately visible everywhere, so even if we had some read operations that were immediately performed over here to the locations m1 and m2, they would have still returned these values x and y. So in P3, regardless how far away from P1 it is, P3 would always be able to instantaneously see that P1 performed this write operation to the memory location m1 and that it wrote x there. Furthermore, what's also really important about the strict consistency model is that it guarantees that every single node in the system will see all of the writes that have happened in the system in the exact same order. So, if we have the situation where P3 is maybe closer to P2 than P1, and these time intervals are really, really small, in reality it's possible that it took longer for this message from P1 to get to P3 so that P3 can see this x value. That's not allowed with strict consistency. Strict consistency, every single update has to be immediately visible and everywhere visible, and the ordering of these updates needs to be preserved. In practice, even in a single shared memory processor, there are no guarantees regarding the ordering of memory access operations from different cores, unless we use some additional locking and synchronization primatives. In distributed systems, the additional latency, any possibility for the messages to be lost or reordered make this not just harder, but also even impossible to guarantee. For that reason strict consistency remains a nice theoretical model, but in practice it is not something that's sustainable and other consistency models are used instead. Given the strict consistency is next to impossible to achieve, the next best option with reasonable cost is sequential consistency. If you look at the example from the previous idiom, if x and y are completely unrelated,. And P2 computed the value of y independently without any reference to the memory location m1. Then these two operations don't really have to strictly appear in this particular order. For instance, it is possible for P3 to observe, just like what we did in the previous case, that the value of m1 has become x. And then to see that the value of m2 has become y. But it is also perfectly reasonable for P3 at a particular point in time to access the value of m1 and to see that it's still zero. So it doesn't know that this right operation happened. However, at a subsequent moment when it tries to read the memory location m2, it sees that it has already been updated to y. This is the equivalent of what would happen in a regular shared memory system if we have multiple threads. And if we don't use any additional controls and your walking sequentializations and anything like that. When these two operations are issued, they will be applied in some way some order. But we don't necessarily have control which is the order that these operations will be applied in. So that's why we say that as long as the ordering that's observed is equivalent to some possible ordering of these operations. If they executed on a single shared memory system, then that would be considered a legal operation. According to this sequential consistency model. So, in summary, according to sequential consistency, the memory updates from different processors may be arbitrarily interleaved. However, if, we let one process see one ordering of these updates. We have to make sure that all other processes see the exact same ordering of those updates. For instance, it is not okay to let process P3 see that the update to m1 with the value x. Appear before the update of m2 with the value y. And instead, at the same time to allow process 4 to think that the value of m1 was still zero. When the value of m2 was already updated to y. In this case, process P4 thinks that m2 has been updated before the memory location m1. It would be too hard to reason about what exactly will happen in a program execution. If every processor can see completely different ordering of some updates,. And if we, the software or the programmer doesn't have an understand how and when are those updates reorder. What this means is that the sequential consistency, every single process will see the exact same interleaving. This interleaving may not correspond to the real way in which these operations were ordered. However, sequential consistency at least guarantees that every single process in the system. Will see the exact same sequential ordering of all of this. And one constraint of the interleaving is that the updates that are made by the same process will not be arbitrarily interleaved. For instance, if P1 makes another update in the future. Then on any of the other processes, it will not be possible to first observe that the value z was written to the memory location, m3. And only then, find out that the value x was written to memory location m1. So for instance, what we see on P4. This will be correct under sequential consistency because the updates to m1 and m3 are observed here in the correct way. When it sees that the value of m3 was z, it already knew that the value of m1 had become x. Forcing all processes to see the exact same order on all updates may be an overkill. For instance, P3 and P4 here may perform completely different in independent computations with the values m1 and m2. And furthermore, the update to m2 had nothing to do with the value of m1, they were also completely independent. However, if we take a look at the second example, before the value m2 was written, the value of the memory location m1 was read. This means that there is potentially some relationship between the memory location m2, this particular update to the memory location m2 and this update to the memory location m1. Given that this read operation that was performed at P2, just before this write. Return the value of this write operation that happened on processor one. There is clearly some flow of information, some potential dependence between these two updates that happened on two different processors. Based on this observation, clearly it is not okay for these two operations, this m2 update to y and the update of memory location m1 to x to be perceived in this different order. It would be very incorrect for somebody like P4 in this case to observe that the value of m2 was updated to y before the value of m1 was updated to x. P4 here thinks that m1 is still zero and then it finds out that m2 became y. But there is dependence between the fact that y was written to m2 and the fact that x was already written to m1. So clearly, this is not a reasonable execution. It will be very difficult for software to understand what is exactly happening with the memory if it cannot reason about dependencies like this. However, in this case, with such dependence did not exist. It will probably be okay for us to tolerate this kind of reordering. The consistency model that provides exactly this kind of behavior is called causal consistency. Causal consistency models guarantee that they will detect the possible causal relationship between updates. And if updates are causally related, then the memory will guarantee that those rights, those update operations will be correctly ordered. What that would mean is that in this situation where the two updates are in fact correctly ordered, we have to make sure that every processor observes that the update of x to the memory location m1 happened before the update of y to the memory location m2. So we have to make sure that on P4, this first read of the value of the memory location m1 returned an x. In this case, this execution will be causally consistent given this particular relationship between the updates. For writes that are not causally related or they're referred to in causal consistency as concurrent writes, there are no guarantees. Causal consistency doesn't guarantee anything about such updates and they may perfectly legally appear in arbitrary orders on different processors. Just like before, causal consistency also makes sure that the writes that are performed on the same processor will be visible in the exact same order on other processors. Again, it would be too much of a challenge to understand how to really reason about a system if the updates from a single processor can be arbitrarily reordered. In the consistency models we discussed so far, the memory was accessed via read and write operations only. In the weak consistency models, it's possible to have something beyond just read and write operations when memory's accessed. In the models we've described so far, the underlying memory managers, or state managers in general, had to determine or infer what are important updates and then how to order and propagate the information regarding those updates. For instance, there's no way for the distributed memory managers to understand whether or not indeed the value of y was somehow computed based on this value x that was read from m1. So it's perfectly possible that p2 really just completely randomly came up with this value y that was written in m2, and that had nothing to do with this fact that it previously read the value of x from location m1. So it saw this particular update. However, with the causal consistency model, it will be forced that every process, every processor in the system has to observe that the value of the memory location m1 has been updated to x before the value of the memory location was updated to y. However, for instance, in the same process, p2, maybe at a later time there is another update that's happening to memory location m3, and the value z is written there. And perhaps this particular value, z, was directly computed, based on the value that was read from memory location 1. So there maybe is a dependence between this particular write, and the value that was read here, but not between these two. How can a memory system distinguish these? To do this, the memory system introduces synchronization points. The synchronization points are operations that the underlying memory system makes available to the upper layers of the software. So, in addition to being able to read and write a memory location, you'll also be able to tell the distributed shared memory sync. What a synchronization point does, it makes sure that all of the updates that have happened prior to sync, the synchronization point will become visible at other processors. And also, synchronization point makes sure that all of the updates that have happened on other processors will become visible subsequently at this particular processor in the future. If p1 performs a synchronization operation at this particular point after this write was performed, that doesn't guarantee that p2 will see that particular update at this moment. Although this read operation is happening according to some real time, some external observer, at a later point in time than the synchronization point in the preceding write, p2 has not made an attempt to synchronize with the distributed shared memory. So p2 has no guarantees that it will see the updates performed by others. So this synchronization point, it has to be called both by the process that's performing the updates, and also by the processes that want to see, and want to see the guarantee that they will see updates. However, once the synchronization operation is performed in p2, afterwards, p2 will be guaranteed that it will see all of the previous updates that have happened to any memory location in the system. So if we perform a read operation of the memory location m1 at this point in time, after the synchronization, we're guaranteed that p2 will see this update. In this case, in this model, we use a single synchronization operation for all of the variables in the system, for the entire shared memory. It is possible to have solutions where different synchronization operations are associated with some subset of the state. For instance, with a granularity of individual pages. It is also possible to separate the steps when a particular process requires that all of the updates performed by other processors are visible to it. We call that the entry point, or the point when we are acquiring the updates made by others. For instance, at this particular synchronization point we can perform an entry operation and synchronize with all the updates that were performed on others, in this case on p1. And then we can have a separate synchronization primitive, that's the exit point or this is the point where we release to all other processes or processors the fact that we have performed certain updates. So for instance, at this synchronization point, only the updates made by p1 will be forced to other processes. The idea with these finer grained operations or these mechanisms to really directly control what kinds of messages will be sent by the underlying state management system, by the DSM layer at a particular point of time, is to make sure that the system controls the amount of overheads that are imposed by the DSM layer. The idea is to limit the required data movement, the required messages and coherence separations, that will be exchanged among the nodes in the system. But the downside of this is that the distributed shared memory layer will have to maintain some additional state to keep track of exactly what are all the different operations that it needs to support and how it needs to behave when it sees a particular type of request. In summary, all of these models that, in addition to just pure read and write operations, introduce some types of synchronization primitives are referred to as weak consistency models and they allow us to control the overheads of the system from one aspect. However, they introduce some additional overheads that are necessary in order to support that the added operations. For the next few quizzes, I'll ask questions that are all related to the consistency models we just looked at. For each quiz I will show you a timeline, and I will ask you whether if that particular execution is consistent according to some consistency model or not. Here is the first quiz. Consider the following sequence of operations. I'm showing two processes P1 and P2 and according to some real time they're performing certain operations. The question to you is is this particular execution of these two processes sequentially consistent? The possible answers are yes or no. This is a somewhat trivial example so the answer yes. This is a sequentially consistent execution. Any of the updates in the system are all performed from processor one. We know that sequential consistency requires that all of the updates from a single processor, that they be visible in the same order. And if we take a look at P2, yes indeed. The fact that the memory location m3 was updated to value y only becomes visible after we have seen that on P2 that the value of m1 has become x. If we had a situation where these reads here to m1 were still returning zero. So, this write was not visible however, this read from m3 already returned y so this write became visible that would've been the problem, but that's not the case in this scenario. So this is the correct answer. Here is a second quiz. Again, there are a number of processes, P1 through P4, and their execution over time. And the question is, considering the following sequence of operations, is this execution sequentially consistent? And then what if the question was, is it causally consistent? For each of the models, sequentially consistent or causally consistent, answer the question with yes or no. If we look at the execution here, we see that P3 and P4 observe the updates to the memory allocations M1 and M2 in reverse way. We take a look at what's happening here, we see that M1 and M2 are not causally related. So regarding causal consistency, this is okay. This execution is correct with respect to causal consistency. However, regarding sequential consistency, we said that that all the processors in the system must observe the same order of the events that are occurring on some other processors. So, in this case, this would not be legal as far as sequential consistency is concerned. So, the correct answers for these questions is this is not a valid execution for sequential consistency, but it is with respect to causal consistency. Meaning that, if we're running an application on top of a memory model, that we know [INAUDIBLE] causal consistency, if we observe this kind of behavior, this is perfectly legal. We cannot complain to anyone. However, if we observe this kind of behavior when running an application on top of a supposedly sequentially consistent system, we have the rights to complain, this is not correct. Somebody has made a mistake with their implementation of this system. Now let's look at the quiz number three in this sequence. We have a very similar set of executions and processors P1 and P2, and slightly different focus on what's happening on P3 and P4. The question is the same. Considering the following sequence of operations, is this execution sequentially consistent? You need to provide your answer yes or no. And also, is this execution causally consistent? Again, you need to provide your answer yes or no. If you take a look at what's happening on P3 and P4 in this case, we see that now P3 and P4 are observing reverse order of how they saw the updates to m3 and m2. As far as P3 is concerned, P3 observed that this update to m2 happened, and it also observed that this update to m3 happened. As far as P4 is concerned it observed this update of z to the memory location m3. But it still has not seen that P2 updated the memory location m2 with the value of y. It thinks that still the memory location m2 holds value zero. So P4 thinks that this write occured before this write from P2, and then P3 thinks the other way around. This is clearly illegal as far as sequential consistency is concerned. Now if we take a look at these two updates we see that on P1 before the value of the memory location m3 was updated to z, P1 read the value of the memory location m2 and it saw that that value was y. So it saw the effects of this particular update. What that means is that these operations are potentially causally related, and therefore every processor in the system has to observe that this particular update happened after the update to m2 given the causal relationship between the two. Therefore the answer is no for both of these questions. Here's another quiz on the consistency model. Now we are looking at a weakly consistency model, so we see that in addition to writes and reads, the processor performs synchronization operations. And the question that I'm asking you is, given this sequence of operations, is this execution weakly consistent? Again, pick either yes or no. The answer to this question is yes. Although P2 and P3 observe these operations in arbitrary way, neither one of them synchronized and forced the memory, the underlying distributed shared memory, to make any kind of guarantees regarding the updates that it observes. Weak consistency will not make any guarantees regarding the ordering unless explicit synchronization operations are used. And for the final quiz in this sequence, if you ignore this sync operations that we had in the fourth quiz, is the rest of the execution causally consistent? Yes or no? The answer to this question is no. It is not causally consistent because causal consistency does not permit that the rights from a single processor are arbitrarily reordered. In this lesson we talked about distributed shared memory systems, and about the mechanisms they use in order to manage that distributed state. We talked about coherence mechanisms, we talked about consistency models that are meaningful in those environments,. As a final reminder, note again that all of what we talked about was in the context of memory management. These types of methods apply to other applications that are responsible for managing distributed and shared state. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future. In this last lesson of the course, I want to provide a brief and high-level overview of some of the technical challenges and technologies that are relevant to today's internet scale applications that are popular. And the underlying data center systems that support their execution. Please note my emphasis on brief and high-level. The space of data center technologies is so vast and so dynamic, that there is no way we can cover it in sufficient technical depth in just one lesson. In fact, there are a number of courses at Georgia Tech that devote a substantial percentage of their content to these topics. Instead what I want to achieve with this lesson, is to make sure I provide you with some context so that you can understand how some of the mechanisms that were discussed earlier in this course, apply to those problems, to those technologies that are in popular use today. Specifically we will describe the typical multi-tier architectures for supporting internet skill services and applications, and their related management challenges. Next we will talk about cloud computing. We will explain what it is and what is the role that it plays in addressing the limitations of some of these traditional approaches. And finally we will briefly comment on some of the key technologies that shape the cloud computing landscape, and that enable us, with scale and flexibility, in how we process large amounts of information. To get us thinking about the importance of the upcoming topics, here are two quick trivia questions. First, how many datacenters are there worldwide in 2015? Second, how much space in square feet is required to house all of the world's datacenters in 2015? These questions are just for fun, so take a couple of guesses. Now let's see how you did. Well, to change this number to 2011, for which we have definitive numbers, there are about 510,000 data centers in 2011 that occupied about 285.5 million square feet of space. These numbers were taken from datacenterknowledge.com and were aggregated for 2011, and if you look at the current numbers, you will see estimates that go anywhere from 1 to 1-1/2 billion data centers alone. And of course, that has implications on the million square feet, the estimate that's present currently in the community. Of course, these numbers continue changing all the time. In the context of data centers, let's first talk about internet services. An internet service is any type of service that's accessible via web interface. >From checking current weather conditions at weather.com to searching or checking email via Google search and mail services, to making online bank transactions or flight reservations. The common way in which these services are used is end users send web requests via web browsers and then receive a response. Most commonly, these types of services are then composed into three components. A presentation component that interfaces with the end users, it's typically responsible for static content related to the web page layout. A business logic component, this is a component that integrates all of the business-specific processing. And this would be all of the dynamic, user-specific content. And then a component that deals with all of the data storage and management, typically in a database. These components are commonly referred to as the presentation, the business and the database tiers. Although these are separate components, they are not always implemented as separate processes running on separate machines. What makes sense will depend on the service complexity, the resource requirements, expected loads. And also in the underlying technologies that are used. For example, the Apache http web server can be used to implement both the presentation logic, to serve static content, and also, as part of the same process, php modules for php processing can be used to generate the dynamic content. There are in fact many open source as well as proprietary solutions that are available that offer technologies for either one of these tiers. To put these applications using these kinds of tiers together, there are various integrated software, or rather middleware, components that are used, that provide commonly used functionality. Messaging services, service configuration and management, security, accounting, some persistence guarantees, recovery management, and many others. One important point to make is that for services that are organized as multiple processes, the inter-process communication between those processes is carried out via some form of IPC, such as RPC or RMI, or from Java based architectures. As well as, some use of optimizations that are based on shared memory, in case that different processes are running from the same machine. So these are some of the examples of the interprocess communication mechanisms that we already dis, discussed in this course. And these are relevant in the context of real world deployments of Internet services. For services that need to deal with high or variable request rates, choosing the configuration that involves multiple processes configured on potentially multiple nodes, becomes necessary. This is because the multi-process configurations are the easiest way to deal with scale, to deal with increases in the incoming request rates. What will do in such cases is we'll scale out the service deployment, by launching the same service on multiple machines. And so these are referred to as scale out architectures. A typical architecture could then be organized as follows. A front-end dispatching or load balancing component would a route to the incoming request to the appropriate or available machine that implements the internet service. This is in some sense similar to the boss-worker pattern that we discussed when we talked about structuring multi-threaded parallel programs. One possibility afterwards is that every single one of the nodes is capable for executing any possible step that's required for the processing of the request. And that they can handle any possible request that may come in from the external users. The other possibility is that the nodes can be specialized to execute only some specific step, or some specific steps in the request processing, or they can be specialized only for certain types of requests. This is again analogous to what we talked about when discussing the boss-worker pattern, that the workers may be general purpose, so they may perform any processing, or they may be specialized for certain tasks. We can further classify these architectures as those having a property of being functionally homogenous. Like when all of the workers performed, are capable of performing, any processing step. Or, as functionally heterogeneous, like in the case where certain notes are specialized in certain functions. Functionally homogeneous architectures can be characterized as follows. Any node can process any type of reques,t and can do any of the processing actions that are required in the end to end request processing. Again, this resembles the basic boss worker model. The front end component is the boss, responsible for pushing requests onto the workers. And, all the workers can perform all of the required steps for a request. The benefit of this is that the front-end can be made kept fairly simple. It doesn't have to keep track which one of the nodes can perform which particular action, or can service which types of request. Instead, the front end can simply in a round robin manner, send requests onto the next available node. Or, the front end could be enhanced in some simple way in which it keeps some information regarding the CPU loads, the CPU utilization, on the other nodes in the system. And then, it can use that type of information to direct requests onto the worker nodes. This design doesn't mean that every single one of the nodes has to store all of the data, all of the state that's needed for the internet service. Instead, data may be somehow replicated, or distributed across these nodes. But, with the homogeneous design, every single one of the nodes knows how to get, how to access any type of information that may be necessary for the execution of the service. One downside of this approach is that there is little opportunity to benefit from caching. If the front end is simple, and it just passes requests round robin onto the other nodes. It may not be able, and in fact, it will not be able to explore opportunities like, this particular node, already serviced some state, and therefore it's likely present in its cache. And, it will not need to perform a remote access in order to access that particular information. The second common architecture for Internet services is referred to as functionally heterogeneous. In these kinds of architectures, different nodes are designated to perform certain functions, or even to see in the particular types of requests. For instance, the request may be divided among the servers based on the request content, such as when the requests are divided based on the alphabetical order of the requested file name. Or for a service such as eBay, servers may be designated to perform browsing requests versus buying or bidding requests. With this design, it is still possible to have all of the data in some distributed, shared file system and accessible to all of the nodes. But if the functions are somehow designated to be executed only on some subset of the node, then it is possible that not all of the data, not all of the state, will be necessary to be made available across all nodes in the system. So there's some optimization opportunities there. Clearly, one benefit of this approach is that every single node in this system is more specialized for some of the tasks that it performs, or for some of the state that it needs to serve, and therefore, it is more likely that it will take advantage of some benefits like caching and locality. There are also some obvious trade-offs. The first one is that the front end will now need to be more complex because it will need to parse sufficient portion of the incoming request so as to be able to figure out which one of the underlying nodes this particular request should be passed to. Second, the overall management of these systems becomes more complex when nodes fail or when request rates increase, and we need to add more machines, it is important to know what kinds of machines to add. How to configure those machines, what types of requests or tasks are those machines suppose to be designated for. Also even if something changes in the workload pattern and all of a sudden there are more requests of a certain kind, then we have to reconfigure this. We can't just redirect those access of requests to some of the other nodes if they are not capable of processing those kind of actions or serving that kind of data. So that may even be the case if the other nodes are otherwise idling. The result of that is that it is much more easy in these kinds of environments to end up in situations in which some of the nodes are hotspots, and then there are long backlogs of pending requests for those kinds of nodes. Diving into the visual metaphors that we used in this course, I want to ask some questions. How the internet service architecture designs that we just described translate in the context of the toy shop. Consider a toy shop where every worker knows how to build any type of toy, so this is like the homogeneous architecture example. If the rate at which new orders are arriving starts increasing, how will you keep this homogeneous architecture balanced? Note that this quiz is open-ended, and also as a hint, you may want to think about answering the quiz by saying add more of what. In this scenario, in order to deal with an increase in the order rates, it would mean that we would need to add more workers, which is analogous to the processes, or add more work benches, analogous to the servers, where those processes need to execute. Or add more tools and parts, so analogous to the storage, the state that's needed for the execution of the service. But we don't have to perform any additional decisions to decide what exactly are those workers intended to do or what specific tools or parts do we need to add. Just a little bit more of everything. The bottom line is that, in this scenario, the management is fairly simple. Still takes some time to do this but we don't have to put a lot of thought into exactly how we need to configure these increased resources. Consider a toy shop where every worker knows how to build some specific toy, so this is analogous to the heterogeneous architecture. In this scenario, if a higher rate of incoming requests for toy orders starts arriving, how would you keep this heterogeneous architecture balanced? Again, this is an open-ended quiz that as a hint, the way you maybe should think about structuring your answers would be add more of x, but what else do you need to do? Now in this scenario again, we'll need to ultimately end up more workers, more work benches, more parts so in a heterogeneous architecture we would need to add more processes, more server note, more storage. However, before we can do that, we first need to profile what kinds of toys are in the end. What kinds of requests are the external users requesting? And then also profile what kinds of resources is required for those particular types of requests for those toys. Only after we perform these profiling tasks will we be able to figure out exactly what kinds of resources need to be added in order to continue maintaining some balance in this heterogeneous design. The bottom line is that the management of these systems is much more complex compared to what's required in a homogeneous architecture. A very popular reference that discusses the design space of these large scale Internet services is Eric Brewer's paper Lessons from Giant Skill Services. This paper also describes some of the trade-offs associated with choices for data replication versus partitioning that we already discussed in the previous two lessons. I have linked the paper in the instructor note. Finally, I want you to consider, again, a toy shop where every worker knows how to build any toy. And in this case, we said that as the rates at which toy orders start coming in increases, the manager keeps scaling up, keeps adding workers and workbenches and parts, etc. My question to you is, until when does this work? As a hint, think about whether endless scaling is possible. The answer to this is that at some point the manager will simply not be able to control any more resources. There is a limit to the attention that a single person has, and therefore one single manager will not be able to continue controlling all of the different types of resources in the shop. In addition, another limit to the scale maybe that there is a physical limit as to how many different things, how much staff, or how many workers, how much staff can simply fit in the shop. Outsourcing could sometimes be an option, however if the manager only trusts his own workers, then, he will not be able to outsource those tasks, and that's going to compose some limits to the scale. These kinds of limitations exist in the context of internet services as well. Given the size of the data center, there may be a limit on the amount of resources that can be placed there. Given the complexity of the management processes there will be limits as to how many different things can be managed. And if we want to run our own software stacks, our own operating systems, our own software services then we'll clearly be limited by these factors. In order to address these challenges, cloud computing has emerged as a solution that allows us to address some of these limitations regarding the scale that can be supported with existing Internet service solutions. Before I talk more about cloud computing, I'd like to illustrate the opportunities that it delivers with an example based on Animoto. This is an example service. And this was a poster child of cloud computing in early years, around 2008, 2009. This example that I'll describe next is repeatedly referenced in cloud computing talks, again, especially in those early introductory years. In the mid 2000's, Amazon was already a dominant online retailer servicing large volumes of online sales transactions. Vast majority of these transactions were taking place during the US holiday shopping season between Thanksgiving and Christmas. And to deal with this peak load, Amazon provisioned the hardware resources, so made sure that they've acquired sufficient number of servers for this particular load. What that means is that the rest of the year a lot of these resources were idle. Or they were serviced for other company tasks, for instance, forecasting or other analytics. But regardless, there were still a lot of idle cycles. Now, since Amazon had already developed some software infrastructure to allow these machines to be reprovisioned and used by other services within the company. What they ended up doing in 2006, they opened up those exact same type of API's to the rest of the world. What this did, it allowed third party work load. So not just Amazon work loads, but completely random customers to run their workloads on Amazon's hardware, obviously, for a fee. This was the birth of Amazon's Web Services, or AWS, and Amazon's Elastic Compute Cloud, or EC2. One of the companies that appeared around the same time as Amazon's EC2 cloud was Animoto. Some of you may have used the service. It turns an album of pictures into a video. And although this sounds simple, it's a fairly compute intensive job that involves a lot of image processing steps so that the video appears as smooth as possible. So they decided to focus their resources on the development of the mechanisms that make better videos. And instead of buying and running their own equipment, they chose to rent some of the web provided computer infrastructure that was part of Amazon's compute cloud. The company was doing okay, and it had a relatively steady work load which required about 50 of these Amazon's compute instances. Now these are not physical servers, instead they were virtual machines. They had no control over exactly how Amazon runs these virtual machines, whether they are on the same physical server or many other physical servers. Then, in April 2008, Animoto became available on the Facebook platform. What it means is that it became an option available to Facebook users with a click of a button to turn their timeline photos or albums into cool video. What happened afterwards was the definition of going viral. Within 3 days, Animoto signed up 750,000 new users. And from the 50 compute instances, so the 50 machines that it needed on Monday. That number became 400 by Tuesday. So an order of magnitude larger in a day. And then by the end of that week, by Friday, that number of machines was 3,400. Two orders of magnitude increase in the resources, just within that week, just within those four or five days. There's no way they could have been able to respond to this dramatic increase in demand if they had gone with a traditional route of owning and managing their own infrastructure. You just would not be able to bring in, install, wire, configure, et cetera, as many machines in such a short time. Even if by some miracle they actually did have the physical space and could fit all of these machines in the machine room, and then that there was sufficient power to power all that. The only reason that Animoto was able to do this is because they used a cloud-based deployment and then they leveraged the capabilities that cloud computing offers. Let's look more formally at what Cloud computing provides. Traditionally businesses would buy and configure the resources that are needed for their services. How many resources should be purchased and configured? So what is the capacity of those resources? Well that typically would be based on some expectations regarding the demand for that particular business, for that particular service, typically considering the peak demand. So if we have a situation in which the Demand follows this pink or this red line, then the business would configure their resources, their capacity, based on this blue line, so that it can tolerate the expected peak in the service demand. Now the expectations turn out to be not quite accurate and the demand ends up exceeding the provision capacity. The business will end up with a situation in which requests have to be dropped and there will be a lost opportunity. In the case of given the very rapid spike in the workload, this lost opportunity would have been tremendous. Instead, what we would like would be the ideal case is if the following were to happen. The capacity or the available resources should scale elastically with the demand and the scaling should be instantaneous. As soon as the demand increases, the capacity should increase too. And then in the other direction, too, as soon as the demand decreases the capacity should decrease as well. Meaning that the cost to operate these resources, the cost to support this service should be proportional to the demand to the revenue opportunity. All of this should happen automatically without the need for some hacking wizardry. And all these resources can be accessed anytime from anywhere. One potential [INAUDIBLE] here is that you wouldn't necessarily own these resources that magically appear on demand. But that may be something that you're willing to compromise on provided that you really do achieve these kinds of benefits. And that there is some kind of proper assurance as to what exactly can possibly go wrong with these resources that you're accessing. So the goal of Cloud Computing is to provide these capabilities that match the idea scenario as much as possible. The resulting requirements can be distilled as follows. Cloud computing should provide on demand, elastic resources and services. There should be fine-grained pricing based on usage. Not for actual or potentially idle physical servers like in the hosting data centers that were the alternative at the time. All the resources should be professionally managed and hosted. And all of this should be available via APIs that can be used for remote access. Given these requirements, cloud computing provides the following. First, a pool of shared resources. These can come in as infrastructure, compute storage networking. That doesn't mean that the cloud provider like Amazon, is renting out physical machines, or physical disks. But instead that it's renting out virtual machines that are even potentially interconnected to form some virtual clusters of such [INAUDIBLE] along with some ability to store some amount of state in the underlying storage. Cloud computing can also come in as shared resources that are used by higher level software services, so these are soft resources. For instance, this can correspond to certain popular services like email or database or some processing run times. So it may be easier to just rent all the infrastructure along with the software stack that's appropriately configured, as opposed to, as a customer, to come in and rent actual infrastructure and then deal with the deployment of the service, the configuration and the management of the service. So both of these are options in terms of what it is that cloud computing provides as a resource. These infrastructure software resources are all made available via some APIs for access and configuration. The point is that they need to be accessed and manipulated as necessary remotely, over the internet. This typically includes web-based APIs, also APIs that are wrapped in libraries that can be integrated in popular language runtimes like Java, or command line interfaces, or other types of tools. Providers offer many different types of billing and accounting services. There is an entire marketplace surrounding these cloud services and cloud resources that includes pricing models with spot prices or reservations for future prices or other types of models. One common characteristic is that billing is typically not done based on actual usage. Just because the overheads that are associated with ultra fine grain monitoring and management tend to be pretty high, and instead billing is done based on some discreet step function. For instance, computer resources may come in some number of preconfigured alternatives, like tiny and medium and large and extra large and each of these will have a different cost. So, then the user picks which one of these matches their needs and then pays the corresponding grade when using the VM. And all of this, finally, is managed by the cloud provider via some sophisticated software stack. Common software stacks that are used in this space include the Open Source OpenStack and also the VMWare's vSphere software stack. Two basic principles provide the fundamental theory behind the cloud computing approach. The first is the so-called Law of Large Numbers. This says that once you start considering many customers who's resources may vary over time, the average across all of them will tend to be fairly constant. That's why a cloud computing provider can pick any amount of resources in capacity for the cloud. And with that capacity, can service lots of customers, whose peaks in the expected demand are shifted in time. Second, there is the Principle of Economies of Scale. Essentially, things are cheaper by the dozen. When you start considering how many different customers a cloud provider is able to leverage on a single physical piece of hardware resource. The cost of that hardware resource ends up being amortized. And ends up being an offset by all of the fees that the cloud provider is able to gather from its customers. Interestingly, all the cloud computing is a relatively new trend. As a vision, it existed for a very long time. The oldest description, by John McCarthy, predates the technology by almost half a century, and it appeared in 61. He said the following. If the computers of the kind I have advocated become the computers of the future, then computing may some day be organized as a public utility, just as the telephone system is a public utility. The computer utility could become the basis of a new and important future. If you look at this statement, it describes a vision where computing is a utility just like other utilities. If we need more electricity, we just turn the switch on. We don't worry about where the electricity actually comes from. Based on this vision, cloud computing should turn the IP resource into a fungible utility, where we don't care about what type of CPUs or disk I'll ultimately use or where they are. Virtualization technology is one enabling the process, certainly. Unfortunately there's still limitations before this vision of just seamless computing utility can be realized. For instance, even with virtualization there's still some hardware dependencies that simply cannot be masked. So if software expects a certain type of hardware plot form to execute then we cannot just have it run on another type of resource. Then there is the API lock-in where if we're using a particular cloud provider then we're expected to continue using those API's. There isn't a uniform standard across cloud providers. And then, unlike with some of these other utilities, where we're simply bringing this resource, the electricity in, in the case of cloud computing we're putting data, potentially core business services out in the cloud. So some of the privacy and security concerns become much more important. Clearly latency's is an issue given that there is an actual geographic distance related to our axis to our. Here is a short excerpt from the National Institute of Standards & Technology's document that defines cloud computing. This is a document that was read or published, rather, on October 25, 2011. Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources, including network servers, services. It can be rapidly provisioned and released with minimal management effort or service provider interactions. I want you to place the following underline phrases, which there are five of them, into these text boxes, where each text box corresponds to one cloud computing requirement. If you need to list multiple phrases in a single text box, please comma separate them, and if you need to leave a text box empty, or blank rather, then don't write anything in it. Looking at this definition and the cloud computing requirements that we discussed already, here are how these phrases map to these requirements. When we think about elastic resources, that means that these are on demand network resources, whenever we need them they will get created. They come out of a shared pool of resources, so that's how we can support this requirement, and this provisioning happens rapidly. When we think about API-based, that really means that the resources, the cloud computing resources are ubiquitously available. They need to be professionally managed, so that's the definition, the part of the definition that talks about minimum management effort. There really isn't anything in this statement that talks about the pricing aspect of cloud computing and in principle, there really isn't a solid well-established definition of what cloud computing is, that the entire community subscribes to. The National Institute of Standards and Technology, which adapted a definition for cloud computing in 2011 also defined different types of clouds. The first dimension how clouds are classified is based on the deployment models. Clouds can be public like Amazon's EC2 cloud is a public cloud. And this is a cloud where the infrastructure belongs to the cloud provider. However, third party customers or tenants, anyone with a credit card really, can come in and run their services, their infrastructure, on top of Amazon's own hardware. Clouds can be private. I said earlier that Amazon used the same type of technology that they opened up as Amazon Web Services. They used it originally internally to enable elastic and dynamic provisioning of the physical infrastructure across the different types of applications and services that Amazon internally executes. In the case of a private cloud, it is the infrastructure as well as the tenants, the services, the applications, that are run on top of that infrastructure. Everything is owned by the same entity and it's just cloud computing technology is used to enable some of the provisioning and the elasticity that cloud computing offers to applications. Then there is a hybrid cloud as a possible deployment model. This is a scenario where a private cloud is interfaced with some public cloud where these public resources are used either for failover for some additional redundancy, for dealing with spikes. I know of certain companies who choose these public resources to run simulated workloads that would then generate supposed request patterns on top of their, privately run core services. Finally the definition also references that there is a community cloud as a type of cloud. This is really just a public cloud where the third party customers or the third party users of the services or the information that's provided by this community cloud, isn't really just used by arbitrary customers but by a certain type of users. The second way how clouds are commonly differentiated is based on the service model they provide. Here's a great image that depicted the differences between the different service models. Without the use of cloud computing, you run your application on premise, and that means you have to take care of all of these different layers yourself. You manage them personally. The applications, the data, the runtime, middleware, O/S, so different components of the software stack including the virtualization technology. And then the lower level, all of the components of the hardware stack. In the other extreme, you can use cloud computing services and then directly leverage the fact that cloud computing can provide you with an application. So this is a model called software as a service. And this resembles what Gmail offers you. You don't have to run your mail server, install quotas, manage security patches, etc., for the employees in your company. You can just force them to use Gmail and, in fact, can just contract Gmail to run the email services for your company. In the case of Gmail, everything is run by Google in this case. It's their hardware, it's their software stack, system software. They own and manage the data, they run the application, configure it. Update it, upgrade it and everything else that's necessary. So this is the software as a service, or SaaS model. A second model is where the cloud service can provide you certain APIs that you can access to develop particular types of applications. The cloud service provides a development or execution environment. And that includes the operating system, various libraries, various tools that are necessary for the applications that need to run in that environment. One example is the Google App Engine and this is called platform as a service offering, or PaaS. In the case of the Google App Engine, the platform as a service offering is that it offers everything that's necessary to develop applications for Android platforms, for instance. Windows Azure was originally purely intended as a pass offering for developing and running .NET applications, and more recently another type of extension was added to the Windows Azure portfolio services. Finally at the lowest level, clouds can provide infrastructure instances, like compute instances that consist of the CPUs, or rather virtual CPUs with accompanying memory, or storage, or the necessary network resource in order to form a cluster of such compute instances. Amazon's compute cloud is an example of such an infrastructure as service cloud computing model. This can also include other types of hardware resources, like GPUs, so you can rent from Amazon instances with GPUs for instance. Again know that these types of clouds don't really provide you with the physical resources directly. Instead, the resources are virtualized. And so it may turn out that you are using independently some subset of the physical resources available in that cloud system. But it's quite possible and likely that you're sharing the resources with other tenants. An exception to this is that, in particular of the case of Amazon, they do provide some high performance instances. So if you're renting through Amazon, a cluster of VMs or virtual cluster or virtual high performance instance, you will in fact end up at a virtual machine that runs independently on physical hardware. The same thing happens when it comes to the GPUs. Your GPUs won't be shared in this case. But that's just because of some technical challenges in terms of how to get sufficient performance level in a virtualized environment for these high performance instances or GPU enabled instances that Amazon provides. >From everything that we have said so far, the technologies that are used to enable computer cloud offerings must address a number of requirements. Clouds must provide fungible resources which means that the resources can easily be repurposed to support different customers. Potentially even with different types of requirements. Otherwise if cloud providers have to allocate and maintain physical resources for every one of their customers, and the exact same type of physical resources as what the customers at least think they require, then the economic opportunity for cloud computing just won't be there. This fungibility is also necessary to deal with some of the V that exists within the cloud resources. And we have different generation of servers, maybe even different types of servers. And you don't really want the users to have to deal with that. You don't want them to be exposed to the level of heteroingenuity. Clouds must integrate resource management methods that support the premise of cloud computing so they must be able to dynamically adjust the resources that are allocated to cloud users pending underneath. So this needs to be done very flexibly, very elastically. Such management methods must be able to operate at very large scales of thousands and tens of thousands of nodes. The scale is important, both from the perspective of the cloud provider. So how many resources it needs to manage. But also from the perspective of the customers. Customers often look at clouds for their resource needs because otherwise they don't have access to sufficiently large resource pools. So then clouds must be able to provide to potential customers the ability to allocate really large collections of resources for your individual needs to the scalabilities required with respect to that dimension as well. Once scale is introduced, failure has become inevitable, really. If we have some probability that one component in the system will fail, the more such components we put together, the greater the probability that something in that collection will fail. So we have to incorporate mechanisms to deal with failures. Clouds, by definition,, are shared resources across multiple tenants, so cloud management has to deal with this multi-tenancy, has to provide mechanisms that guarantee performance and provide isolation across multiple workloads, multiple tenants. We cannot have one misbehaving tenant that will somehow take control over all of the resources and punish and hurt the performance of the remaining tenants. And finally an important concern that must be addressed is isolation of the state that's being accessed in cloud systems. With this respect, clouds need to make guarantees regarding the privacy of their tenants' data and the security of the execution environment that the cloud guarantees. Another aspect of security's also to make sure that their privacy and security guarantees, not just across tenants, but also that the cloud provider isn't going to somehow access or take advantage of the state that is managed by individual tenants. And then also the other way around, that the cloud computing player is protected from certain vulnerabilities that exist within a single tenant. And therefore, that one tenant isn't going to affect the entire cloud platform. I mentioned failures in the previous Morris cell, so let me try to illustrate why our failure is more of a problem in a large scale system like a cloud platform, than if we're not really concerned with scale. Think about the following question. A hypothetical cloud has N=10 components, or 10 CPUs, where each CPU has a failure probability of 0.03. My question is, what is the probability that there will be a failure somewhere in this system, in case there are ten components. Then think about also what will happen if there are more components. So they're 100 components, what in that case is the probability of a failure? You should provide the answer in terms of what is the percentage chance. To insert a question, think about this set of formulas. p is our probability that a single component will fail. Then 1-p is the probability that a single component will not fail. For there not to be a failure anywhere in this system, so we're trying to answer the inverse of this question. That means that not a single one of the components in the system should fail. If the probability of one component not failing is 1-p, then the probability of no components in the system failing is (1-p) to the nth. Their total of end components in the system. So this will be the probability that nothing will go wrong in the system. And the question here is what is the probability that something will go wrong? So there will be a failure somewhere in the system, that's clearly 1 minus this result. The formula for the total answer is, on parentheses, 1 ,minus p to the n. If you do the math, you will see that with this probability, so there's a 3% of something failing and you have ten components. If you do this math, you have a 26% chance that something will go wrong. If you increase the scale and your n becomes 100, then if you do this math you'll realize that with this high probability of failure, 95% of the time you will have a failure at some point. Our cloud systems don't have 10 or 100 components. They have thousands and hundreds of thousands of components. Yes, they may be more stable than this 0.03 probability of failure, but the point is that things will fail. The more components you put together, the more you'll have situations in which something, somewhere, is failing. So you have to have mechanisms that are prepared to deal with those kinds of failures. For instance, your software may have to incorporate mechanisms that include timeouts, so that you can detect that a failure has occurred or to integrate mechanisms for restart and retry in order to be able to recover from failure. You have to incorporate mechanisms for backup, for application, for checkpointing. So you have to accept, you have to embrace the fact that failures will happen, and so you have to build robust software that will be able to detect and recover or avoid failures from occurring. Given the requirements that we listed in terms of what a cloud system has to incorporate, several different technologies come into play. First, we already talked about the need for virtualization. This is required in order to address this ability to provide fungible resources that can be dynamically repurposed. Which application they serve and then exactly how are they used, what are the expectations? You don't have to have the exact same CPU or the exact same type of device based on the customer's needs. Then you need some technologies to enable the dynamic provisioning and scheduling of the infrastructure resources. Certain platforms like Meso, or Yarn, Hadoop's Yarn, that serve this role. In order to address the customers' needs for scale, cloud technologies need to provide abilities to process and store large amounts of data. There are a number of big data processing frameworks that are out there. Hadoop MapReduce is one. Spark's another one that is popular. Regarding the data storage layer, cloud computing platforms include things like distributed file systems. These are file systems that typically operate in some append only mode where you're not arbitrarily rewriting data and deleting data and modifying data. And then there are other important technologies that enable the storage, access and manipulation of data at scale. These include a number of NoSQL technologies, which are a departure of traditional relational SQL databases. The ability to store data across distributed memories, as opposed to repeatedly perform disk accesses, and a number of other enhancements. Cloud computing customers, they not only need to get the right amount of resources. However, they need to somehow be isolated from each other so that every one of them thinks that they own their resource slots. For that reason, cloud computing technologies need to include some kind of mechanism that enables these software-defined slices of resources so that costumers are provided with their own software-defined data center or subset of the storage that's controlled and defined by the software or the network resource, etc. And then also, especially given the complexity of these environments, it's important for cloud computing platforms to also use a certain efficient monitoring technologies. In real time to process logs, to detect certain anomalies or failures, but also to provide that as a service to the users that may not just want some batch access to potentially large data but more of a real time, more of a maybe interactive access to information that's produced from their applications. Some of the technologies that provide this monitoring type functionality or real-time log processing include Flume and CloudWatch, this is the Amazon service, or LogInsight, this is a product that's part of the VMware portfolio. Flume is Open Source, and many, many others. One benefit of cloud computing is that it empowers anyone to porpetually have infinite resources. As long as you can pay, you can have the resources that are required to work on maybe hard problems that involve lots of data and that require lots of processing. Here are some illustrations that I've found pretty informative. To illustrate how much of data are we dealing with in terms of our everyday life in our world. So if we just think about one year, 2011, there were 1.8 zettabytes of data that were created in that one year. This amount of data is equivalent to everyone in the US tweeting 4,320 times a day. Pretty much not doing much else. Or another way to think about it's this amount of data is 200 billion high definition movies that are each about two hours long. If you lived for 47 million years, you'd be just about done watching these 200 billion movies. Or to store this much data, it would require so many of the Apple's 32 gigabit iPads, that if you stack them up together, it would be 25 times higher than Mt Fiji. Pretty powerful these illustrations. And this is all the data from 2011 only. Clearly not every application needs access to all of the data that's generated that's available in the world. But a lot of the applications for scientific discovery, for improving social services, for a number of other types of applications, they do require a lot data. Looking at cloud computing as a big data engine, there are a number of layers that are necessary in order for the cloud to provide that kind of infrastructure and adopt those kinds of services that are needed by these big data applications. Cloud platforms that offer as a platform as a service stack for big data processing. At the minimum have to have some layer that enables the storage and access of the data from cross many, many nodes. And also the ability to describe applications in a certain parallel fashion, and ideally to do that easily, so that the data can be processed across potentially many nodes. We talked earlier in this course that access to memory and access to local memory is faster than going to disk. And particularly if you have to go to disk that need these nodes that's attached to another node. So most of the cloud stacks also incorporate some data caching layer where the data can be brought in memory, or maybe even a cross multiple memories, in the distributed system, big data system. I said a cloud platform for big data must incorporate some data processing framework. But commonly, when we talk about small or smaller data, the way we are analyzing it is using things like SQL queries or some other type of querying mechanism. And so often cloud stack, big data stacks, would incorporate some language front end that would make this data processing a little bit more easier. One common thing that people do with big data, that they analyze it. They search for certain patterns, so they perform some data mining operations that often involved common functions, such as some machine learning algorithms. So just as it's common for cloud big data platforms to support popular language front-ends, it's also common for them to support popular analytics libraries. And also, a lot of the types of data that we're interested in isn't just created once, and then that's it, that's a complete data set. Instead, there's continuously generated knowledge. And then potentially, the answers from the analysis on that knowledge are derived continuously and updated over time. So therefore, a common service that needs to be incorporated in big data clouds would be something that provides as streaming abilities, both as inputs and as outputs. So we mentioned, a number of complete technologies that are used in cloud computing and also we mentioned a number of types of functionality that cloud computing technology needs to provide. Again, let me offer a birds eye view of two popular stack for big data that are used in cloud computing platforms. The two concrete stacks I want to show you is the Hadoop Big Data Stack, it's an open source stack and then also the Berkeley Data Analytics Stack, BDAS. This is another open source stack. These are by the way, not the only options. There are number of both propriety stacks and also a number of other open source stacks. For instance, one example is the so-called HPCC stack by LexisNexis. This actually, even predates Hadoop. However, it wasn't necessarily popular or widely used outside of the LexisNexis environment. When it comes to the overall popularity and adoption rate, both in commercial and academic settings or research settings, these two are probably the dominant stacks out there today. The Hadoop architecture looks like this. At the bottom layer, there is the data storage layer, the Hadoop file system and this takes care of taking large amounts of data and somehow, splitting them and replicating and making sure the data doesn't get lost and making sure that there are no hot spots in the system, etc. Then there is a processing framework that can allow computation to operate on such data and deals with the scheduling of what are the specific notes that particular data manipulation operations need to be scheduled. Ideally, the scheduling should be done in such a way, so that you don't have to constantly move data from one note to another, depending on what is the task that needs to process it. We're really not going to talk in more detail about the Map Reduce framework. The intent of these is just as an illustration to provide you with an overview of the breadth of technologies in this space. There's a component Hbase and this provides a table like view of the stored data that is more similar to with what we're familiar with in the context of databases, but it is not the same as the relation of database representation of the data as were used in the SQL environment. In order to support data processing and data query operations like what SQL databases support, there's a language front end hive that will take an SQL query and then translate it into a number of these Map Reduce operations. That will then operate on top of the data that's stored in this Hbase tables on top of the distributed file system. Order number of higher levels services that would also provide the same kind of things. So end users can use the R environment in order to describe certain rules about how data should be processed and analyzed or we can use machine learning algorithms, one of a collection of many that are supported in the Mahout component that's part of the Hadoop stack. And a number of other technologies that provide more specialized type soft functionality or different kinds of interfaces to the end users. At the lowest level, however, all of these end up being a number of these more primitive operations that can execute on top of the distributed file system where the data is stored. And there are a number of supporting services that are needed in order to deal with locker or coordination or to provide the streaming of data into this distributed environment and this is what the Berkeley BDAS looks like. There's similarly a component that's the data storage layer and then there is a in memory component that is the in memory file system, so that you can avoid the caches. You can run the regular Hadoop Map Reduce engines on top of this layer or you can run another type of programming model, another processing framework called Spark. There are a number of front-ends for the Spark in terms of the kinds of execution models that are supported, so whether it's in SQLite queries or stream processing or graph processing or machine learning types of operations that should be executed on top of this processing framework. Another component that's important to mention is this Mesos component that's the lowest level scheduling framework, the lowest level resource manager that allows the resources to be partitioned and then used by different types of frameworks. You may have a partition that's a Hadoop partition. A partition that's a Spark partition. A partition that's an MPI partition. And using this stack, the two will be able to basically coordinate and negotiate among them and then you'll have elasticity across these very different types of framework. In this lesson, we talked about several technologies that are relevant to the data center space. We described the commonly used multi-tier software architecture models, and discussed the management tradeoffs that exists among them. We talked about cloud computing and the role that it plays in addressing some of the scale and cost related challenges to supporting really large-scale applications and services. And we also briefly mention some of the popular technologies that form the cloud computing landscape today. As the final quiz, please tell us what you learned in this lesson. Also, we would love to hear your feedback on how we might improve this lesson in the future.