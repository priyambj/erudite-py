Hi, and welcome to Introduction to Data Analysis. I'm Caroline and I'll be your instructor for the course. Now a data analyst is someone who uses data to answer questions, and that's exactly what you'll be doing in this course. You'll tackle questions like what makes students more likely to submit their projects, or what countries around the world have the highest and lowest employment rates, and how does the country you live in compare? Or how the subway ridership varied by location, time of day and weather conditions, and many more. For each data set you'll have the chance to pose your own questions and perform your own investigations. Along the way you'll learn to use some libraries that make data analysis a lot easier. Specifically, you'll work with NumPy, Pandas and Matplotlib. But you'll start off analyzing data using pure Python. Now you won't need any experience with data analysis in order to take this course. But you will need to be comfortable programming in Python. You should be familiar with things like loops, functions, lists, dictionaries, objects, and things like that. If you need to learn more about any of those topics, check out the links in the instructor notes. Now let's get started. For the rest of this lesson, you'll be analyzing Udacity student data. But, before you get started on that, let's talk about some of the other kinds of problems you might solve as a data analyst. When many people hear the term data analyst, they picture someone who works at a tech company, and probably in Silicon Valley. Many data analysts do in fact work at tech companies. For example, Netflix uses data analysis to provide personalized movie recommendations for its users. Facebook uses it in their newsfeed ranking algorithm. And OkCupid uses it to predict which people are likely to be good romantic matches. In addition to building these kinds of systems, many companies also use data analysis to publish papers or write blog posts about their findings. For example, Facebook has studied the ideological diversity of people's political posts. They found that when posts appear on people's newsfeeds written from a different perspective, users are less likely click on those posts. OkCupid wrote a blog post about, what are the best questions to ask someone on a first date? Did you know that long term couples are very likely to agree about whether they like horror movies? Data analysts don't just work at tech companies, though. Walmart looks through their purchase records and people's posts on social media in order to figure out what to stock in their stores. Apparently, strawberry Pop-Tarts are more likely to sell out right before a hurricane. Bill James is famous for applying data analysis to baseball. He used it to understand who are the top performers, and how can you best predict future performance? And pharmaceutical companies have started using machine learning to predict which chemical compounds are most likely to make effective drugs. Using this, they can make better decisions about which compounds are likely to be a good use of the resources involved in running a clinical trial. If you'd like to learn more about any of these applications, check out the links in the instructor notes. So what are the steps involved in these types of projects? Before you jump in and write a fancy algorithm, you'll want to spend some time getting familiar with your data. The process begins with a question you want to answer or a problem you want to solve. This might be something like, what are the characteristics of students who pass their projects? Or how can I better stock my store with the products people most want to buy? The next step of the process is data wrangling and this really has two parts, data acquisition and data cleaning. First, you need to acquire the data that you need to answer your question or solve your problem. Then it's time to begin investigating the data and cleaning up any problems that you find. The third phase is data exploration. During this phase, you spend some time getting familiar with your data, building your intuition about it and finding patterns. Once you're familiar with your data, you'll usually want to draw some conclusions about it or maybe make some predictions. >From one of our examples previously, Netflix's movie recommendation systems needs to predict which movies its users will like. Also the Facebook paper we saw concluded that users are less likely to click certain articles, specifically one's written from a different ideological perspective than the user has. This phase usually involves statistics or machine learning that are beyond the scope of this course, but there are additional Udacity courses you can take on these topics, if you're interested learning more. And finally, you'll need to communicate your findings to other people. Your findings are only as useful as your ability to communicate them. Even if your end goal is to build some sort of system, like a movie recommender or a news feed ranking algorithm, you'll usually need to share what you've built and how it works with your team. There are a variety of formats this communication can take. You might write a blog post, a paper, an email, a PowerPoint presentation, or just have an in-person conversation. Data visualization is a common technique that's almost always useful when communicating findings about data. Now, this process doesn't actually follow a straight line. Especially the data wrangling phase and to the data exploration phase are very intertwined because you can't really clean any problems with the data before you take a look to see what problems there are. And even when you think you're done wrangling and you're ready to just explore, you'll keep finding more problems and have to go back. Throughout the process, you may need to return to your question and refine it as you become more familiar with the data set. And sometimes data acquisition actually comes before you pose a question. If a new, exciting data set is released, you might acquire the data first, take a look and see what's there, and then think of some questions you could answer with the data. That's what we'll be doing in this lesson. However, this should give you an idea of the high level steps that are involved when you're doing data analysis. Let's get started with the Udacity student data. We'll start with the data wrangling phase. That is, acquiring and cleaning the data and you'll pose a question once you know what data you have to work with. Data acquisition can happen in a variety of ways. In this case, you acquired the data by downloading some files that were available. In other cases, you might need to get the data from an API or you might need to scrape it from a web page. You'll also often need to combine data from multiple different formats. Now don't worry if some of these terms are unfamiliar to you. They'll be covered in more depth in our data wrangling course which is linked in the instructor notes. It's a great course if you'd like to learn more about how to get data from a variety of different sources. Your files are in a format called CSV which stands for comma separated values. A CSV file is similar to a spreadsheet with no formulas. The CSV format is also very easy to process using code. Unlike, for example, and XLSX file, which is the format used by Microsoft Excel. For example, let's take a look at one of your files, which contains student enrollments. Here's what the file looks like in Google spreadsheets. There is a row for each time a student enrolls and columns for different pieces of information such as the account key, the enrollment date, and the cancellation date, if any. Now here is what the file looks like if you open it in a plain text editor. A plain text editor is a program like Notepad or Subline that shows exactly what is present in the file. As you can see, the actual contents of the file are very simple. The header row from the spreadsheet is present as the first line of the CSV file. The second row of the spreadsheet is the second line of the CSV and so on. Within each row, you'll see the first cell, followed by a comma, followed by the second cell, followed by a comma, and so on. This makes CSV's very easy to process in programming languages such as Python. In Python, the contents of a CSV file are commonly represented as a list of rows. There are two common choices for how to represent each row. In the first option, each row is a list. So then the overall data structure is a list of lists. In the second option, each row is a dictionary. This option works well if you have a CSV header because then the keys of each dictionary can be column names and the fields can be values. So then your overall data structure would be a list of dictionaries. Now you could write the code to read in the data from the CSV yourself and it wouldn't be too hard, but there are libraries already written to do it for you. Here I've written some code using Python's unicodecsv library to read in the student enrollments and print out the first record. I'll go ahead and run the cell. This code is taken pretty directly from the example code on the CSV module's documentation page. There's a link to this page in the instructor notes. So in this code, first I create the list of enrollment. Then I open the file. The mode, rb, here, means that the file will be opened for reading. And the b flag changes the format of how the file is read. The CSV documentation page mentions that I need to use this when I'm using this library. Next, you'll notice I'm using the DictReader, which means each row will be a dictionary. I chose this since our data does have a header row, and this will allow me to refer to each column by its name, rather than its number. Now the reader won't actually be a list of rows. Instead it will be something called an interator. If you're curious, you can learn more about iterators by following the link in the instructor notes. What you need to know here though, is that the iterator let's you write a for loop to access each element, but only once. So if I were to add this second loop to print out all the row in the reader, then actually nothing would be printed, because you can only loop over an iterator once. Since I want to access the data multiple times, I used my one loop to append each row to a list. And finally I need to close the file. Then since this row is the last row in the cell, it gets the output in the output area. And actually this code is a bit like lengthier than it needs to be in a couple of ways. The first is that I can avoid having to close the file by instead using a with statement. When I open the file using a with statement, I need to indent everything that accesses that file. Then, once the indented block ends, the file will automatically be closed. This is closer to how the example code in the CSV documentation page looked. And second, there's actually an easier way to convert an iterator to a list. Here, by calling list{reader}, I've created a list of the enrollment data without using a loop. So now I can delete this line and this line. Now I'd like you to write code to read in the other two CSV files. And also take a look at the first row of each file to make sure it worked properly. In case you're still getting everything set up, you'll also be able to complete this exercise in the Udacity code editor at the end of this video. If you do, you can use test run to see the output of your code, and submit to see if your solution is correct. If you do have everything set up locally, then the starter notebook will contain the codes that I've just written, as well as some other code you'll see later in the course. Find the point marked number 1, and add your code there. Make sure you name your variables, daily_engagement, and project_submissions, or some of the later code will not work. I'm going to show you the process I went through to solve this problem. You can also find the final version of my code in the instructor notes in case you'd like to take a closer look. The code should look pretty similar for each file. So first I'm going to copy the codes that I used before. Then I'll paste the code twice, and I'll need to change the file names into the variable names. So this should be doing engagement.csv, this should be projectsubmissions.csv, I'll name this variable daily engagement and this one one project submissions. Then I'll go ahead and print out one row of each and since I want to print just one row in the cell, I've used print statements. This worked, but anytime you find yourself copying and pasting code or just writing two pieces of code that are very similar, it might be a good idea to write a function instead. That way, if you find a bug later, you'll only have to fix it in one place. In this case, the only thing that changed between these three blocks of code was the file name and the variable name. So I'm going to write a function named read_csv that takes the file name as an input and then returns the list of rows. I'll use this code that I've already written as my starting point. Then instead of the file name here, I'll use the input that I took and instead of creating a variable, I'll return the list. Now I can delete these two blocks. And call the function three times. Now I'll run the code again just to check that it still works. Since this function is so short, it's debatable whether writing it was helpful. But I'm happy I wrote it. For example, if I hadn't realized I needed to open the file with the argument rb, and then later I noticed that I needed it. I could have updated this function, rather than updating the code in three separate places. Now one thing that looks a little funny about this data is that every value is a string. But days to cancel looks like logically it could be an integer. Cancel date is clearly a date and is cancelled looks like a boolean, but they are representative strings. That's because the CSV library doesn't try to detect what type each column has. It's up to me to convert these data types if I want. I could wait and convert them later when I really need to. For example, every time I need to check weather is cancelled is true, I could use the code, if is cancelled is equal to the string true, rather than the simpler if is canceled that I could use if I had a boolean. However, I prefer to update all my data types upfront because otherwise I tend to forget that they're not the type I expect, and then I waste time debugging later. I've written some code for you which you can find in your starter notebook which updates a bunch of the types. Here you can see the main loop that's updating the types of the enrollments data. I didn't update the type of account key, because even though the account key looked like an integer, I'm never going to do arithmetic with it. I'm not going to add up two account keys, for example, so, the types might as well remain a string. I did convert the cancel date to a python date time object, and I wrote this function to do that for me. First I check whether the date is actually the empty string, in which case I return none. For example, the cancel date could be an empty string if the student hadn't cancelled yet. Otherwise, I use the strptime function to parse the string as a date time. Don't worry if you've never seen the strptime function. I'm going to take care of the date time handling in this lesson. Similarly, for the days to cancel function, I've written a function called parse maybe int. This function takes something that might be an integer but might not be. For days to cancel, it might not be an integer if the student hasn't cancelled yet. If the argument is the empty string, this function will return none and otherwise it will cast the argument to an integer. Then, for the is canceled column, I check if the value is equal to the string true. So this comparison will return the boolean true if the field is equal to the string true and false otherwise. The rest of the conversions are similar. The only other thing that's a bit strange is in the daily engagement table where you can see that a few times I've cast the number to a float first and then an end. To see why, I'm going to scroll up and see what the lesson's completed value looked like. The first value for lessons completed was zero point zero. Because of the decimal point, directly converting the string to an integer would fail. But, logically, I know lessons completed should be an integer since if you only complete half a lesson, you'll be marked as completing zero lessons, not 0.5. So, once I've converted the value to a floating point number, I then convert it to an integer. I've done the same thing for number of courses visited and projects completed. This code is included in the starter notebook. Once you have everything set up locally, you should make sure you run each of these cells. That way you'll have the correct types throughout the rest of the lesson. In general, make sure you run all the provided starter code, or some later code may not work correctly. Now that you've loaded the student data into Python, I want you to think about what questions you could answer with this data. For example, you could figure out how long it usually takes students before the submit their projects. Or you could look at some different characteristic of students and try to find differences between students who pass their projects and students who don't. This is a great time to let your curiosity run wild. There's a lot of really interesting stuff you can figure out with this data and you're about to do it. So, start brain storming. Take another look at what data is available and try to think of at least 5 questions you could answer using it. Share your thoughts with your classmates on the forums. There's a lot of interesting questions we could answer. Besides the questions I already mentioned, we could look at how much time the average student spends taking classes on Udacity. We could look how time spent relates to the number of lessons or projects students complete. We could look at how student engagement changes over time. Or how many times students submit projects before they pass and whether this differs for different projects. I'm sure you thought of a lot of interesting questions as well. In this lesson, I'm going to focus on how the numbers in the daily engagement table differ for students who pass their first project versus students who don't. But feel free to explore any of the other questions you've thought of yourself and share what you find on the forums. To find the number of enrollments I'll use the len function. And running this cell I can see that there were 1,640 enrollments. To find the number of unique students who enrolled, I'll add the account keys to a set and then look at the length of the set. So, first I'll create an empty set called unique_enrolled_students. Then I'll loop through the enrollments. And I'll add the account key from each enrollment to this set. Finally, I'll look at the length of the set. And it turns out that there were 1302 unique students who enrolled. The reason these two numbers are different is that students can enroll then cancel. Then re-enroll later. Next I'll check how many entries are in the daily engagement table, and there are a lot more, which makes sense since there should be multiple entries for each student. To find the unique number of students in the daily engagement table I'll need similar code to before, so I'll copy and paste it. First, I'll need to change the table I'm lifting over. Now, I don't technically need to change the name of the set. I could name both sets unique students for example. But then the second set would overwrite the first set. So when using iPython Notebook it's often a good idea to give a new name to each new variable so that you'll still have access to the previous variable if you want. So I'll change the set name also. I'll also change the looping variable to make the code more clear. Finally I'll need to change the account key column name to ACCT. Again since I copied and pasted some code here, writing a function would have been a good idea. I'll get to that in a later video. The main difficulty in this case would be that this account key has become ACT in this loop but it's possible to handle that. And I saw that there where 1237 unique students in the engagement table. When I first ran this code, I was surprised that these two numbers weren't the same. We'll see in a later video why that happened. And finally, I followed a similar process for the projects submission table and found there were 3642 project submissions from 743 unique students. Now I noticed a couple problems when looking at the data. The first thing I'm wondering is why were there more unique students in the enrollment table than in the engagement table? The engagement table is supposed to include a row for each day that each student is enrolled, even if the student didn't visit the site at all that day. So there should have been the same number of unique students in both tables. The second problem was that the column containing student account keys was named account_key in two of the tables and acct in the third. This one isn't really a big problem but it is inconvenient. I kind of expect to keep forgetting which table has account_key and which table has acct. The first problem is confusing. When I first started analyzing this data, I didn't know how to fix it. I didn't even know why it had happened. We'll investigate this together in a minute. Fixing the second problem is easier. You can simply the column name from acct to acount_key by removing the value for acct from each row and adding it back under the name account_key. Go ahead and write code to fix the problem. Once you're done, check whether your code worked by running this command, and enter the output in this box. If you prefer to create a new variable rather than modifying the existing variable, then use your new variable name in place of daily_engagement in this command. If you're not sure how to remove an element from a dictionary, the post in the instructor notes might be helpful. I choose to modify the list rather than create a new list. I looped over the daily engagement table. And for each row I first created a new key, account key, and set it equal to the existing values stored under the key ACCT. Then once the value had been copied, I deleted the ACCT key from the dictionary. And I'll go ahead and run this code, it doesn't have any output. Now if you chose to create a new list that's fine. But if you gave it a new name, you'll need to update the rest of the starter code to use your name instead of daily_engagement. Now that I've changed this field it will be easier to write a function to get the unique students from each table. So I'll go ahead and do that. First, I'll move this change up above the previous investigation. Since I'd already run this cell, it actually wasn't necessary to move it up, but this will make it so that if I close the notebook and reopen in, the code will still work if I run all cells in order. Then I'll insert a new cell below this point and write my function there. I'll name the function, get unique students, and it will take one argument, the data. My code here looks very similar to the code I wrote before. I'm first creating an empty set, then looping over the data and adding each account key to the set. And finally, I'll return the set of unique students. I could also have gone one step further and returned the number of unique students. But I happen to know that later I'll want to have access to these sets, so I'm going to return them and store them in a variable. If I hadn't known in advance that I would want the sets later, then that wouldn't be a problem, I could just change this function. Then I'll update the later cells to call this function. And I'll rerun the cells to make sure I get the same results. Finally, I'll run this command I told you to run to double check that renaming the column worked. And sure enough, the account_key field is now present in the daily_engagement table. So why are some students missing from the daily engagement table? When you see something strange like this, it can be tempting to just move on with the analysis. But it's actually really important to investigate things like this. If you don't understand why something's happening in your data, you could be missing something important in your analysis, and you can't really trust your results. The process I like to use to investigate this type of problem is, first, identify which specific data points are surprising. In this case, that would be any enrollment record with no corresponding engagement data. Next, I like to print out one or a few of the surprising data points. Sometimes I can tell just by looking what the problem is. So go ahead and do that. Find any one row in the enrollment table where that student is not present in the engagement table, and print that row out. If you're not sure how to print only one of the rows, you could try using Python's break statement. There's a link describing it in the instructor notes. And, here's a hint. The sets I made earlier containing the unique students in each table might come in handy here. Do you see anything strange about the record you printed? Describe what you see here. The data point I saw was this one, although depending on your code, you might have seen a different one. If you couldn't figure out what was wrong with the data point, that's okay. Sometimes a little more investigation will be necessary. But I happened to notice that the join_date and the_cancel date are the same and the days_to_cancel is equal to 0. That could definitely explain why there is no record in the engagement table for this student. The student probably needs to be enrolled for a full day before their engagement is recorded. In order to find this record, I looped over the enrollment table and found the account key for each enrollment. Then I checked whether that account key was in the set of unique students in the engagement table that I had found earlier. If the student was missing from the engagement table, I printed the record and then used break to exit the loop. Once you've looked at some of the surprising points, it's time to fix any problems that you find. Of course, you won't always see the problem at first glance so more investigation may be necessary. Often though, you will find a problem or two just by looking at some of the surprising points, or you might find that there actually wasn't even a problem in the first place. In this case, I saw that a student had cancelled within one day of joining, and that's not really a problem, and it does explain why that student was missing from the engagement table. This is a good thing to be aware of, though. Depending on the analysis I do later, I might want to exclude students like this, or I might just need to know these students exist in case they cause an edge case in my code. At this point, the process repeats. Once again, I need to identify any remaining surprising data points. Of course there might not be any more surprises at this point, since a student who cancels the same day as they joined no longer counts as surprising. But it's still important to check in case there are other problems I haven't found yet. Go ahead and figure out whether there are any surprising enrollment records left. That is, rows in the enrollment table where the student is missing from the engagement table but they stayed enrolled, at least a day. Enter the number of surprising records you find in this box. It could be zero. To figure this out, I first created a variable to store the number of problem students. Then I wrote a similar loop to earlier, where I look at each account key in the enrollment table and check whether that account key was present in the daily engagement table. I also added another check to see whether the join date was equal to the cancel date. I could have also looked at whether days to cancel was equal to zero. Then, if there was a missing student for whom the two dates were not equal, I added one to the total number of problem students. At the end of the loop, I looked at that number and I saw that there were still three surprising records left. At this point, since their were only three remaining problematic enrollments, I printed out all of them. However, this time when I looked at the records, I wasn't really sure what the problem was. I ended up asking a Udacity data scientist if he knew what was going wrong. He pointed out that these students were all Udacity test accounts. Which he could tell because they had is Udaucity equal to true. These test accounts aren't guaranteed to be present in the daily engagement table. This is really good to know. I should also probably be excluding any test accounts from my analysis anyway. At this point, there weren't any more surprising data points. I did go ahead and move all of the Udacity test accounts from the data right away since I didn't plan to include them in any future analysis. First, I created a set of all the Udacity test accounts by looping over the enrollment data, and I found that there were six of them in this data set. Then I wrote a function to remove any data associated with those accounts. Finally, I called my function on all three of my tables and checked how many records were left in each. Most of the data is still included, which is what I expected. I did create new variables for the updated data, just in case I ever want to look at the original data again. At this point I'm going to move on from the wrangling phase to the exploration phase. Although of course I might find more problems later and need to fix them. What I want to look at first is how the numbers in the daily engagement table differ for students who eventually pass the first project versus those who don't. However, there are a few things about this question as stated that are under specified or would be a bit weird. First, if I look at all records in the daily engagement table, that will include data from after the project was submitted. That data isn't really relevant, so I might want to only look at engagement data from before the submission of the first project. Second, if I do only look at engagement from before the first submission and students submit after different lengths of time then I'm comparing engagement data from different lengths of time, but student's engagement might have trends over time. For example, engagement might tend to drop off after the first few days. If that were the case, then comparing average engagement over one month for one student versus two months for another might be misleading. And third since the daily engagement table we've been using includes engagement numbers for the entire nano degree program. It's including engagement for courses that are not related to the first project, which is little weird. To handle the first two problems, I'm only going to look at engagement data from the students' first week of enrollment and I'm going to exclude students who cancel within a week. That way I'll be comparing equal amounts of time. This will also have the benefit of excluding people who cancelled during the free trial which was seven days at the time this data was collected. The third issue doesn't seem like a showstopper to me. I'm going to go ahead and look at engagement numbers for any course in the nano degree program and just keep this decision in mind. If you did want to look at engagement data in only the first course of the nano degree you could do that using the daily engagement full table available in the downloadable section which includes a breakdown by course as well. But since this file is pretty big, computations on it can take longer. So, to make things easier, or in case your computer doesn't have that much memory, I'm going use the non-granular version, for now. As a first step, create a dictionary of students who either haven't cancelled yet. Or who stayed enrolled for more than seven days before cancelling. If the student hasn't cancelled yet the days_to_cancel column will be None. And if they've stayed enrolled for more than 7 days, the days_to_cancel field will be > 7. The keys of your dictionary should be account keys and the values should be the date that the student enrolled, since this date will be useful later to find engagement records during the first week. You should name your dictionary paid_students. Enter the number of students who meet these conditions in this box. I'll start off by creating an empty dictionary. Then, I'll loop through the enrollments and notice here that I'm using the new enrollments variable I created, which doesn't include the audacity test accounts. I want to add the student to the dictionary if they haven't cancelled yet or if it was at least seven days before they did cancel. Then I'll pull out the account, cancel the enrollment date and save those in variables. And I'll add those to the dictionary with the account_key as the key and the enrollment_date as the value. Then I'll use the line function to see how many students this is and it comes out to 995. Now this works, but there's something a little strange about it. Earlier we saw that the same student can enroll multiple times and if that happens, then I'm just going to be saving an arbitrary one of those enrollment dates. Instead, I think it makes sense to save their most recent enrollment date in that case. To accomplish that, I'll add another check and I'll only add the enrollment date to the dictionary if either the account key was not already present, or if this enrollment date is more recent than the date that's already present. And I'll rerun the cell. Of course, that shouldn't change the number of paid students, it just changed what enrollment date was saved for certain students. Now I'd like you to find the point in the starter notebook marked 7, and create a list of rows that includes only some of the records from the engagement table. Specifically, you should include rows where the student is one of the paid students you just found, and the date is within one week of the student's join date. Give your new list the variable name paid_engagement_in_first_week. I've created this function you can use to determine if one date is within one week of another. Take a look at the number of engagement records that are present in your new list and enter your number in the box on the next screen. To solve this I first start a function to remove any data points corresponding to students who canceled during the free trial. Next I called this function on each of the three tables and saved the results in new variables. I also took a look at the length of these variables to see how much data had been removed. Next, I started off the variable paid_engagement_in_first_week as the empty list. Then I looped over each record in the engagement table and saved the student's account key in this variable. I also looked up the student's join date by using the dictionary I created earlier. And I saved the data of the engagement record into this variable. Next, I checked whether the two dates were within one week of each other. And if so, I appended this record to the variable paid_engagement_in_first_week. Finally, I printed out how many records there were, and you can see that about 21,000 of the engagement records come from the students' first week. Based on the question I want to answer. At this point the next task that would make sense for me to do is to split the data into two groups. Data for students who eventually pass the first project and for students who don't. But I have all this data about students' engagement in the first week, and I'm getting curious. I want to take a look at it. My rule of thumb is when I'm curious about my data, I investigate. Getting a better sense of your data is almost always useful. In this case, I'll probably be able to reuse some of the code I'm writing later. But even if that weren't true, exploring your data is never a waste of time for a data analyst. One thing I'd like to look at is the average number of minutes students spend in the classroom during their first week. Right now I have a bunch of engagement records for different students, and they're not in any particular order. First, I'd like to separate out the engagement records into groups where each group contains all the engagement records for a particular student. I'll represent these groups using a dictionary where the keys are student account keys and the values are lists of engagement records for that student. Next, I'll compute the total number of minutes spent in the classroom by each student by summing up the number of minutes in each engagement record for that student. Finally, I'll compute the average of the totals to get the answer I'm looking for. Now I'll show you the code I used to solve this problem. First I created the dictionary I mentioned to group engagement data by the account key. You can see that I used the defaultdict instead of a vanilla dictionary. If you haven't seen Python's defaultdict before, that's okay. It allows you to specify a default value. In this case I've specified list. That means if I ever try to look up a key in the dictionary and that key is not there, I'll get the empty list instead. You'll see how this can be helpful in a minute. Then I looped through each engagement record and saved the account key in this variable. Next I looked up the list of engagement records for that account key. Here's where the defaultdict that comes in handy if I had never added that account key yet, then I get the empty list. Then I append this engagement record to the list of engagement records for that account key. Next I wanted to add up the number of minutes visited by each account and I decided to save that in this dictionary. So this dictionary will store a number for each account key, instead of a list of engagement records. Next I looped over the dictionary I created previously. And by looping over the .items, I got both the key and the value for each entry in the dictionary. For each account, the number of minutes starts at 0. Next I loop over each engagement record in the list of engagement records for that student. And I add the number of minutes that student visited in that engagement record to the total number of minutes for the student. Finally, I store that number of minutes under that account key in the new dictionary. Now I'm ready to take the average of those totals, and for that the account keys are no longer relevant. I'm just interested in the numbers themselves. So I'll use the .values method to get out just the values from that dictionary. Now I could calculate the mean by hand, but instead I'm going to use a library called numpy to do it. You'll be seeing a lot of numpy later in this course. And I've imported it using the line import numpy as np. Adding as np allows me to later refer to the numpy library as np, instead of referring to it as numpy. It's just a little shorter and easier to type. You don't have to do this, but lots of code posted online will have this convention, so it's good to have seen it before. Then I can take the mean by calling np.mean and passing in the total minutes. I see that the average number of minutes spent was 647. I'm interesting in knowing more than just the mean here, though. I'd also like to know what's the standard deviation and what's the minimum and what's the maximum. It turns out numpy has functions to calculate those as well. They are np.std, np.min and np.max. Now, you might know that Python actually also has a min and max function that you can use without importing the numpy library. You'll learn later why you might want to use numpy's version instead. But for now I'm just going to go ahead and use it since I've already imported the library. And when I run this I see that the standard deviation is pretty large, larger than the mean. The minimum is 0 and the maximum is about 10,000 minutes. That's a huge maximum and in fact there's gotta be a problem here, because when you calculate it out, that's actually slightly more than the total number of minutes in one week. So something has gone wrong. So I just noticed that the maximum number of minutes spent by any student during the first week is actually greater than the number of minutes in a week. My process for debugging problems like this is similar to my process tracking down the missing engagement records before. I'd like you to follow the same steps here. First, you'll need to decide which data points count as surprising. Then, print some of those out and see if you can find anything wrong. Were you able to find a problem? Describe any problems you found here. The bug is actually in some of the starter code that I gave you. So if you can't figure out exactly what's causing any problems that you find, don't feel bad. The surprising data point I wanted to investigate was the student who had the maximum number of minutes. First, I needed to find that student. First, I initialized the student with the max minutes to be none and the maximum minutes spent to be zero. Then I looped over each student and the number of minutes they spent in the total minutes by account dictionary. If the total number of minutes was greater than the maximum I had found so far, then I reset both the maximum number of minutes and the maximum student to be this number of minutes and this student. Then I output the maximum number of minutes I'd found to make sure it was the same, and it looks like it worked. Next, I wanted to print each engagement record for that student. So I looped over the engagement records, and if the engagement record's account key was equal to this student's account key, then I printed the record, and these are the records I found. The first thing I noticed is that there are way more than seven entries here, which shouldn't be the case since this is within the paid engagement only in the first week dictionary. This should only include one week's worth of data. The data points aren't falling within a one week time span either. You can see here the first data is January 7th and then scrolling to the bottom, the last date is April 26th. This makes me suspect that something might be wrong with my within one week function. In this function, I'm checking that the engagement date came at most seven days after the join date. I'm not checking that the engagement date actually came after the join date. So for students who enroll once, then cancel, then enroll again later, everything from the first enrollment will count as their first week. To fix this, I checked that at least zero days had passed from the join date to the engagement date. That way, I'm only considering data from the most recent enrollment which was my intention. Then I reran this cell, as well as all the cells after it. Now the maximum time spent in the first week comes out to about 3,500 minutes, which is roughly 60 hours. That's a lot of time to spend studying in one week but, at least, it's possible. Now, even though this number sounds reasonable, there could still be a mistake, so it's a good idea to rerun the same check that I did before. I'll go ahead and do that. This time, I'm seeing exactly seven entries within the first week, which sounds perfect. Now adapt the code I just showed you to analyze the number of lessons the students complete in their first week instead of the number of minutes they spend. Like I did, calculate the mean, standard deviation, minimum, and maximum, and enter your numbers in these boxes to two decimal places. See if you can write one or more functions to reuse the code I wrote rather than copying and pasting it. I decided to write three functions. One to group the records by account_key. One to sum up all the entries for each account_key. And one to print out summary statistics like the mean and standard deviation. I named my function group_data. And I decided to make it take the data as an argument, and return the dictionary mapping account keys to lists of records. Now I could have left the function like this, and made it always group by account key, but I wanted to leave some flexibility there. So I added an extra argument key name which would indicate the key to group by. Then I indented the code I had previously, so it would be part of the function. And I changed the variable I was looping over to be the input data. I also changed the field name I was grouping by, to be the input key_name rather than account_key. Then I changed each of the variable names to be suitably generic. And finally return to the grouped data. Finally, I called this function to create the same engagement by account dictionary that I had previously. Now the calculation I asked you to do for a number of lessons, will use this same engagement by account dictionary, so it actually wasn't necessary to write this function just yet. But I liked the idea of writing it, because this felt like a clean interface for our function. It takes in some data and it groups it by some key name. Later, you'll see that the library pandas has some function that works pretty similarly to this one. Next, I moved on to the function to sum up the entries for each account key. I named my function, sum_grouped_items and I had it take as inputs the grouped data and the field name to sum up. In this case, the field name definitely needed to be an argument. Because I wanted to sum up both the total minutes visited and the lessons completed. Again I indented the code I had previously to be a part of the function. Next I changed the dictionary I was looping over to be the input grouped data, and I changed the field name I was summing up to be the input field name. Again, I updated each of the variable names to be suitably generic. And finally I returned the summed data. Again, I called my function to recreate the total minutes by account dictionary that I had created recently. Next I was ready to write a function to print summary statistics about the data, such as the mean and standard deviation. I named my function, describe data and I had it take one argument, the data to describe. Then I moved all these print statements within the function, and I changed the data I was summarizing to the argument data. I also moved the import statement to the top of the cell. And I called my new function to describe the total minutes data. When I re-ran this cell, I got the same results as before which is good. Next I was ready to analyze the lessons completed by account using the functions I'd just written. I started with the engagement data grouped by account. And then I summed up the number of lessons completed for each account key. Then I called the describe data function and again, I needed to take the values of my lessons completed by account dictionary before passing into the describe data function. When I ran this cell, I saw that the mean number of lessons completed in the first week is about one and a half. With a standard deviation of three. It makes sense that there would be a lot of variations, since some students might complete a lot more lessons in the first week than others. The minimum number of lessons competed is zero, which is not surprising. I certainly expected there would be some students who didnâ€™t complete any lessons in their first week. And the maximum is 36, which is quite some. Now you probably didn't write exactly the same functions that I did, and that's fine. There's a lot of valid ways to do this. As long as you got the correct answers, then you're good. Now I could do the same thing for the num_courses_visited field that I did for the other two fields. That is, group the records by account key. Then sum up the num_courses_visited field for each account key. However, if I did this, the results would be a little bit strange. For example, suppose this student visited three courses their first day, zero courses their second day, and one course their third day. Then the total for this student would be four. But that's not equal to the number of days the student visited the classroom at all, which is two. And it's not equal to the number of unique courses the student visited, which is either three or four, depending this was the same course as one of these three. Instead, I would prefer to analyze the total number of days each student visited the classroom at all. Regardless of how many courses they visited that day. There are a couple of ways I could accomplish this. First, I could change the code in the sum_grouped_items function or create a new similar function that instead of adding each number up, adds up the total number of records where the number is greater than zero. Alternatively, I could create a new field in the data called has_visited, that is either 1 or 0. It's 1 if the student visits the classroom at all on that day, and 0 otherwise. If I do this, I could call the same sum_grouped_items function that I've already written to add up these numbers. And what I would get is the total number of days that the student visited the classroom. The second way is the way I would prefer to use, but either one should work. Using whichever these two methods you prefer, calculate the mean, standard deviation, minimum and maximum for this number, the number of days student visit any manner of degree course during their first week. Then enter your results in these boxes. I chose to create a new field in the data called has_visited. Now there's a couple of choices for where to add this field. The latest I could possibly add it is to this engagement_by_account data since that's where I'll need it. On the other hand, then that field won't be available in all the other lists of data that I have, which could annoy me later. Another option would be to add it at the very beginning of the notebook when I first load in the data and then reevaluate all the cells. But some of these cells take a couple seconds to run and I'm not really excited about doing that. As a compromise I decided to add it to the paid_engagement table after I had removed the free trial cancels but before I had separated out just the engagement data for the first week. To do this I looped over each engagement in the paid_engagement table. Then if that engagement record showed that more than zero courses were visited, I set the has_visited field to one. Otherwise, I set the has_visited field to zero. Remember the reason that I'm making this field numeric, zeros and ones, is so that I can add this field up to get the total number of days that the student visited the classroom. Now I'll need to run this cell, as well as rerun all the cells below it, to make sure that the has_visited field gets added anywhere it needs to be added. Then I can analyze the days_visited_by_account using code very similar to what I had previously. Again, I'll start with the engagement data grouped by account and then I'll sum up the has_visited field for each account. And again, I'll describe my data. And when I run this cell I see that the mean number of days visited in the first week is a little less than three. The standard deviation is a bit more than two. The minimum is 0 and the maximum is 7. The minimum and maximum in particular are exactly what I expected. In fact, I would be pretty surprised if there weren't at least one student who visited 0 days in their first week and at least one student who visited all 7. Now find the point marked number 11 in your starter notebook and split the daily engagement data into two lists. The first list should contain the engagement data for students who eventually passed the first project, and you should name this list passing engagement. The second should contain records for students who don't eventually pass the first project, and you should name this list non-passing engagement. Throughout most of the time that this data was being collected, the first nano degree project was analyzing New York subway data, so that's the project that you're going to assume is the first project for this purpose. Now this project was actually updated at some point and the key identifying it changed, so I've given you both keys here. You should consider a student to have passed the first project if they ever pass a project with either of these two keys. As a reminder, here's what the project submission data looks like. You can use the lesson_key field to see which project was submitted, and you can use the assigned_rating field to see whether the student passed the project. The table descriptions file available in the downloadable section lists the possible values for the assigned rating field. Once you're finished, find how many engagement records correspond to students who passed the first project and how many correspond to students who didn't, and enter your number in the boxes on the next screen. To solve this problem, I decided to first create a set of all students who pass the subway project at some point. First I looped over each submission in the paid_submissions table, which contains all the project submissions from students that we're analyzing. Next I pull out the project key into the rating. Next I wanted to check if this project was the subway_project, and I could have done that by separately comparing is this project the first project in the list, or is it the second project in the list? This would work, but there's actually an easier way to do this if you're familiar with Python's in keyword. So instead I checked if this project's in the list subway_project_lesson_keys. Next I wanted to check if the student passed the project, so I added a condition that rating is equal to pass. However, this isn't actually the only way to pass the project. In the table description, it mentions that past means meets specifications, but distinction means exceeds specifications. So I also wanted to consider a rating of distinction to be passing the project. Finally, when using multiple ands and ors in the same check, it's always important to add parentheses appropriately. In this case I wanted to check that the project was the subway project and that the student had passed the project. If so, then I added this student's account key to the past subway project set. Finally, I wanted to check how many unique students had passed the subway project, and that turned out to be 647 in this data set. Next, I was ready to split the data in the passing_engagement and non_passing_engagement and I started each office the empty list. I looped over each engagement record in the paid engagement_in _first_week table, and I check if the account key for that engagement record was one of the account keys that had passed the subway project. If so, I appended this engagement record to the passing_engagement table. Otherwise, I appended it to the non_passing_engagement table. Then I printed out the number of entries in both tables. There are about 4,500 entries in the passing_engagement table and about 2,300 in the non_passing table Now I want you to calculate some metrics you're interested in for both groups of students. Students who pass the subway project and students who don't. For example, a good starting point would be the three that we looked at earlier. The number of minutes students spend in the classroom, the number of lessons they complete, and the number of days that they visit the classroom. Try to think of some other things to look at as well, though. You might look at whether these students are more likely to go on to complete other projects. Or you might try looking at some of the more granular data in daily_engagement_full.csv, and see if there's anything you can do with that. Be creative here. There's a lot available in this data that you could explore. Do the results surprise you? Do you notice anything interesting about them? Choose the metric where you think the difference is the most interesting, and share what you found on the forums. Also select which metric you shared here and enter the mean for students who pass in this box and the mean for students who don't pass in this box. If your most interesting finding was non-numerical, you can leave these boxes blank. I'm hoping you came up with your own idea for something to explore and that you'll check the other box, but you might also want to enter your value for say minutes spent to check that your code is working correctly. Also check this box once you've posted on the forums. I used the functions that I wrote earlier to calculate the same three metrics that I already looked at, but this time for both passing and non-passing students. First I used the group data function to group the data for passing students by account key. And then I did the same for non-passing students. Next, I summed up the total minutes visited for non-passing students, and described the results. I also did the same thing for passing students. I decided to print the data for both groups of students within the same cell so that it would be easier to compare the output. And because of this, I also added these print statements so that I could differentiate which output went to which group. The first thing that I noticed about this output is that the average time spent in the classroom is a lot higher for passing students than for non-passing students. These means correspond to about six and a half hours in the first week for students who pass, and two and a half hours for students who don't pass, which is quite a difference. The standard deviation is also a lot higher for students who pass. This isn't surprising to me since the mean is so much higher for passing students. And the more time students are spending, the easier it is to have a lot of variation among the times. The minimum is zero for both, which is as I expected. I wouldn't guess that not visiting the classroom during the first week makes it impossible to pass your project. And the maximum amount of time spent is also a lot higher for passing students, roughly 60 hours a week compared to 30 hours a week. Next I ran a very similar piece of code for the number of lessons completed in the first week. Again, I saw a much higher mean for students who passed the first project. I find it interesting that students who pass are completing on average two lessons per week, since I would have guessed that a reasonable pace might be one lesson per week. However, it is true that a problem set counts as a lesson within this data, so we might be seeing students who are completing one lesson and one problem set. The maximum is also higher for students who pass, although the difference here isn't as dramatic as it was for total minutes spent in the classroom. And finally, I ran very similar coding again to examine the has_visited field. In this case, I saw that the minimum and maximum days visited weren't different between students who passed and student who didn't pass. They were zero and seven in both cases. That's not really surprising since there are hard limits on how many days you can spend in your first week. It has to be at least zero and at most seven. The mean again is noticeably higher for passing students, although possibly not as dramatic as in the other two cases, at about three and a third for passing students, and a little less than two days in the first week for non-passing students. If I had to pick which of these difference I found most interesting, I would probably pick total number of minutes visited, since the difference between roughly six and a half hours in the first week versus two and half hours, seems like the most striking difference. But this is pretty subjective. Here are my plots for the total number of minutes spent in the classroom in the first week. We saw earlier that the mean number of minutes spent was much higher for students who passed the project, and we can see that here since these labels are increasing faster than these. We can also see that for students who passed, the number of students who fall into one of the longer buckets is higher relative to the non-passing students. On the other hand, the rough shape of the two histograms is fairly similar. Most students spent the shortest amount of time. And then fewer and fewer students fall in each bucket as you increase the amount of time. This shape holds for both students who pass and students who don't pass the first project. These are the histograms for number of lessons completed. Again, the shapes of the two histograms look fairly similar to each other although you can see that the mean is higher for passing students. Which is in line with what we saw earlier. Finally, here are the histograms that I got for number days visited. You'll notice that in both histograms there are these weird gaps. That's because of the number of bins chosen. This data can only be integers from 0 to 7, so if too many bins are created, then some bins will be completely empty. And that's what happened here. I'm going to fix this later in the lesson, but if you'd like to fix it now, there's some advice in the instructor notes. Ignoring the gaps for a moment, I can see that the shapes of these two histograms are very different. For non-passing students, the shape is similar to earlier, where less and less students are in each bucket as the bucket corresponds to more and more days. However, for the passing students, there's a very different pattern. You can see that students who passed the first project are about equally likely to visit the classroom 7 days during their first week, as 0 days. To make these histograms, I added to the line plt.hist(data) to my described data function. That way I would be able to easily make the histogram for each metric by rerunning the later cells where I describe the data. And the histograms would show up next to the summary statistics. Of course in order for this line to work I needed to import map plot lib. And I also added the line %pylab inline to the top of the cell. So that my plots would show up within my notebook and not in a separate window. Now that we've fixed a few problems in the Udacity student data, and started to uncover some relationships between the variables, it's time to move on to drawing conclusions, or making predictions. You've probably already started to draw some tentative conclusions. For example, that students who pass the subway project spend more minutes in the classroom during their first week than students who don't. However, even if there was no real difference between students who passed their project than students who didn't, you wouldn't expect the two means to be exactly equal. How can you tell the difference you saw is a true difference or if it's due to noise in the data? The differences we saw looked pretty large so it may be hard to imagine this could be due to noise but intuition does not always work very well in these situations. To rigorously check how likely it would be to see these results by random chance, you need to use statistics. To learn more about how to do this, check out Udacity statistics course linked in the instructor notes. For now we're not going to go further in this direction since statistics isn't required to take this course, but keep in mind that the results we have are only tentative. They haven't been validated. Another type of conclusion you might want to be able to draw, is to say that if you change one thing, another thing is likely to change also. For example, you saw that students who passed the first project are more likely to visit the classroom multiple times in their first week. This is a correlation. Passing the first project is correlated with visiting the classroom multiple times. Does this mean that visiting the classroom multiple times in the first week causes the students to pass their project? This would be a statement of causation. For example, suppose Udacity sent emails to students during their first week reminding them to come back. This caused more students to visit multiple times. Would you expect this to increase the number of students who complete their project? It certainly sounds plausible, but we can't be sure based on this data. To see why, let's take a look at this graph, which shows per capita cheese consumption in red. And the number of people who died by becoming tangled in their bedsheets in black. As you can see, these two variables are highly correlated. As one increases, the other tends to increase as well. However, do you think eating cheese causes people to become tangled in their bedsheets? Probably not. This could be a coincidence or there could be some third factor causing both. In this case, it looks like both cheese consumption and bed sheet tangling are increasing over time. If this is the case, then if you banned cheese, bedsheet tangling would probably continue to increase overtime. So what are some third factors that could be causing the correlation between visiting the classroom and passing projects? One that I thought of is level of interest. Maybe people who don't find the data analysis courses very interesting, don't visit the site many times. And then they also don't pass the project because they're not interested in working on it. If you somehow convinced these students to visit the site more, their lack of interest might still prevent them from working on the project. A better approach might be for the student to try learning something else that they find more interesting. Another factor might be background knowledge. Maybe some people don't have the programming knowledge they need to take the data analysis courses and they get stuck and don't come back. If you convince these students to come back and keep trying, they might remain stuck and not be able to make forward progress. In this case, a better solution might be taking a programming course to fill in the missing background knowledge. On the other hand, this correlation could be because of causation. And I actually find that explanation pretty plausible. But the point is that we don't know based on the data we have. To find out whether one change causes another, you need to run an experiment, and online experiments are frequently costed A/B tests. To learn more about how to set these experiments up and some common gotchas that can arise check out Udacity's course on A/B testing linked in the instructor's notes. Another thing you might want to do at this point is try to predict which students are likely to pass their first project, and which students aren't. You could take a first pass of this using some heuristics based on your exploration. For example, you could predict that students who spend more time in the classroom during their first week are more likely to eventually pass the first project. However, getting a really good prediction this way could be difficult. For one thing, there are lots of different pieces of information that we probably should be looking at. You'd probably at least want to look at the things I looked at earlier, lessons completed, minutes spent, and days visited. But there could be a lot more pieces of information or features that you'd like to look at as well. This wouldn't be so bad except that all of your different features can have complex interactions with each other. For example, when you look at minutes spent in the classroom on its own, it might look like a really important feature. But then when you look at lessons completed and minutes spent together, you might see that after you already know how many lessons a student completed, knowing how many minutes they spent doesn't give you any additional information. In many situations like this one, you can use machine learning to make pretty good predictions automatically, usually a lot better than the predictions you could make by hand. Depending on the machine learning algorithm you use, it might also give you a ranked list of which features were most important to the prediction, which can be very interesting. If you'd like to learn more about this, you might want to take our introduction to machine learning course linked in the instructor notes. After drawing conclusions and thinking predictions, the final step of the process is to communicate your findings. Now, even though you don't have rigorous conclusions backed by statistics or machine learning, you can certainly think about how to communicate the results of your exploration. At this stage, you generally want to decide which of your findings you think are most interesting or most worth sharing, and how you want to present those findings. For example, I was interested in the difference in total minutes spent in the classroom between students who passed the first project and students who didn't. And I think the clearest way to present this is simply to present the means for both groups. I also thought the difference in number of days visited in the first week was interesting. And then this case, I thought the histograms did a better job of showing back numbers. If you do want to share any visualizations, in general it's a good idea to spend some time polishing them. You want to make them look nicer and explain what trends you'll noticing in them. You always know what you made a plot of, but your reader doesn't. Thanks for sharing your findings. I can't wait to see what discoveries you've made. I wanted to improve the histograms showing the number of days students visit the classroom during their first week, for both passing and non-passing students. So I started off with the same plotting code that I had used previously. Then I added the line, import seaborn as sns. To the top of the first cell, since importing seaborn will automatically make the plots look a little nicer. You can see the difference here. The colors are a little bit better and there's a grid in the background. Next, I wanted to use the bins argument to specify that there should be exactly eight bins for this histogram. This will fix those blank spots that we saw earlier. There should be eight bins since there are eight possibilities for the number of days you visit the classroom in the first week, 0 through 7 inclusive. I also used plt.xlabel to specify that the x access was measuring number of days. And I used plt.title to add a title that would explain what the graph was showing to someone who didn't already know. Re-running the cell, I can see that the strange gaps have disappeared and my axis label and title have been added. Then I made very similar changes to the second histogram adding an xlabel, a title and setting the number of bins to eight. I didn't re-import Seaborn since that's not necessary. Seaborn will apply to all plots that are made from now on. And again I can see that all the weird gaps are gone and my x label and my title have been added. One more thing you may be wondering is, how does the term Data Analysis relate to similar terms like Data Science, Data Engineering or Big Data? First, Data Science is very similar to Data Analysis. In fact, many people might describe some of the examples I gave near the beginning of the lesson, such as Netflix's movie recommendations as data science. I often hear people use the terms, data analysis and data science interchangeably. However, to the extent that there is a difference. In my experience data science is a little more likely to refer to building some sort of a system, like a recommendation system, or a ranking algorithm. While a blog post or a paper is more likely to be refered to as data analysis. It's definitely a fuzzy line though. If you hear some advice for a data scientists, it's probably relevant for a data analyst as well. In terms of job postings, a data scientist title sometimes requires more experience than a data analyst title. Although of course this varies by company. Data engineers are more focused on the data wrangling step of the data analysis process. They make data pipelines and insure that data collection is fault tolerant and scales well. They're more involved in storing and processing data, and less involved in drawing conclusions based on the data. Big data is a fuzzy term that refers to the fact that many companies today are dealing with a lot of data, terabytes, petabytes, and more. There's no rule for how much data you need before it is big, but certainly the amount of data we have today seems big compared to ten years ago. Data analysts, data scientists, and data engineers can all work with either big data or not so big data. Congratulations. You've reached the end of the lesson. You went through the data analysis process from start to finish, starting with posing a question, all the way through sharing what you found. I'm sure you made some fascinating discoveries. In the next lesson you'll learn some additional libraries that can make writing data analysis code a lot easier. I'll see you then. In the last lesson, you did a bunch of data analysis using Python. And you did use NumPy and matplotlib a bit, but for the most part, you completed the analysis without using any additional libraries. In this lesson, you're going to get a lot more familiar with NumPy. And I'll also introduce you to a library called pandas. Both of these libraries, especially Pandas take a little time to learn how to use. But once you do, they'll make writing data analysis code a lot easier. And your code will also run faster when you use them. When I first started using Pandas, I spent a lot of time reading online documentation, going through tutorials and I found it really difficult. This course contains the information I wish I'd had when I started out. I won't be able to cover every feature that you might want to use, but by the end, you'll be better equipped to read the documentation and learn more on your own. In this lesson, I'll be focusing on using NumPy and Pandas to analyze one dimensional data, so something like a single column of a spreadsheet. Let's get started Throughout this lesson, I'm going to use examples from a new dataset which contains information about employment levels, life expectancy, GDP, and school completion rates in various countries. This data was collected by the site Gapminder which is linked in the instructor notes. In this lesson I won't be going through the data analysis process in order, since I want to focus on how to use NumPy and Pandas effectively. Instead you'll be doing a bunch of analysis on this dataset, but not necessarily in order. I still want you to start off by thinking of some questions though. In the downloadable section there is a file which explains what data is available in more detail. Take a look at that file and try to think of at least five questions that you could answer using this data. Then share the questions you thought of with your classmates on the forums, and when you're done, check these boxes. The csv files containing the actual data are also available in the downloadable section, if you'd like to take a look. But that shouldn't be necessary since you'll be able to complete all the programming exercises in this lesson within the Udacity editor. Here are some questions I thought of that I'm interested in. First, how has employment in the US varied over time? Of course, you could ask the same question about another country. I'm particularly interested in the US, because that's where I live. But you may have another country you're interested in investigating. In the most recent year we have data on, what are the highest and lowest employment levels out of all the countries? Which countries have those employment levels? And where is the US on that spectrum? Of course, I could ask these same questions for any of the other variables in the data set, as well. I'm also interested in whether there are any consistent trends across countries. For example, have there been any global recessions and if so, can I find them in this data? Answering this last question would be a bit complicated and we're not going to get into it in this lesson, but we can definitely tackle the others. To show you why Numpy and pandas are useful. I'm going to revisit some code from last lesson, and see how it would look if I wrote it using pandas. This is the code to first take a CSV and read it into a list, and then get the unique accounts out of that list. I'm running this code on the daily engagement full file, since that file is rather large, and pandas deals with large files nicely. Now if I run the code to load the file into a CSV, it takes a little while and finally it finished. Now, we cut some of the loading time out of the video, but it took about one minute. Then after that, getting the unique students takes a little while as well. In fact, this one is going to take about six seconds, and now we're done. As you'll see in a minute, when I use pandas, the code will run much more quickly. In fact, in pandas this will also be only three lines of code. The first will be this import statement. In the second, I'll read it in the file using pandas built-in read CSV function. And, finally, I can specify that I want to look at the acct column. And then use pandas built-in unique function to get the unique accounts. As you can see, pandas has a lot of convenient functions, like read CSV and unique, that previously we had written ourselves. Now of these three lines of code, the one that actually takes the longest for me is importing pandas, which takes several seconds the first time I do it. That is certainly very long for just importing a library. But it more than makes up for it with the speed of the rest of the code. After that, reading the data into the CSV takes just a few seconds, rather than the one minute that I waited earlier. And actually getting the unique accounts finishes before I can even blink. Both Panda's and NumPy have special data structures, made to represent one-dimensional data. By the way, NumPy stands for Numerical Python. In Pandas, this data structure is called a series, and in NumPy it is called an array. Pandas and NumPy also both have structures for two-dimensional data, which you'll learn about in the next lesson. I typically prefer to use Pandas' series, since they have more features than NumPy arrays. But NumPy arrays are simpler, so I'm going to cover those first. Panda series are built on top of NumPy arrays, so it will be important to understand NumPy arrays when you move on to using series. In many ways, a NumPy array is similar to a Python list. It contains a sequence of elements and those can be anything. But suppose you had a Numpy array of US state codes. So here's Alabama, Alaska, Arizona, Arkansas, California, and so on. I included quotes around these states because they are strings. Like lists, the elements of a NumPy array are in order, and you can access elements by their position. In this NumPy array, Alaska is at position 0, Arkansas is at position 1, and so on. So if you had a NumPy array named a, then a[0] would return the string AL for Alaska. Also like lists, you can access a range of elements from a NumPy array using something called slicing. Again, the syntax is the same as for lists. So, a[1:3], would return an NumPy array containing Alaska and Arizona. Note that just like for lists, the upper bound is not inclusive, so I got the element at position 1 and position 2, but not the element at position 3. And finally you can use for loops with NumPy arrays using code like, for x in a:. So, what are the differences between a NumPy array and a Python list? First, NumPy arrays are designed for each element to have the same data type. So, either each element should be a string, each element should be an int, each element should be a boolean or something else. You can create a Python list that contains some strings, some ints, some booleans, and so on, all mixed together. And you can still do that in NumPy, but it was designed for each array to have a single data type. Second, NumPy includes a bunch of convenient functions. For example, the functions to take the mean into standard deviation. Now you saw last lesson that you can also use these functions on Python lists. But, if your data is in NumPy array then these functions will be faster. Now this is one place that the type of the elements matters, since it only makes sense to take the mean of the array if all of the elements are numerical. Another difference is that NumPy arrays can be multidimensional. You'll learn more about this in the next lesson, but it's similar to making a list of lists in Python. On the next screen you'll be given a chance to play around with some NumPy code, and see what it does. I recommend that you read each section of code, and predict what will be printed, then change false to true, and use Test Run to see if you were right. If you're not sure what will be printed just use Test Run to find out. You can also add your own code and try out other things if you're curious. At the bottom of the code, there's a function for you to write called max_employment to check your understanding. It take these arguments to NumPy arrays. The first one contains country names and the second one contains employment data for each country. The country names are in the same order as the employment data. The function should return the name of the country with the highest employment, and the corresponding maximum employment value based on the given data. When you click Test Run, in addition to seeing the output of any code you write, you'll also see the result of running your function on the full employment data set for a few different years. If you click Submit, the grader will check whether your function is correct. If you want to test your function on something smaller than the full data set, I recommend testing it out on the arrays provided at the top of this section of code. This one contains the first 20 countries in the data set, and this one contains employment data for those countries in the year, 2007. Since those arrays are named countries and employment, you can test your function on those arrays by using the line print, max_employment(countries, employment). I'll show you how to write the max_employment function first, and then I'll go over the output you should have seen from the provided test code. I'm going to show running all the code in an iPython notebook. To write the code I started off with the maximum employment value as 0 and the corresponding country as None. Next, I looped through each element of the two NumPy arrays. Since the two arrays should have the same length, I could use either line of countries or line of employment here, it wouldn't make a difference. Next I checked whether the employment data from that country was greater than the maximum employment I had seen so far. If so, I updated both the maximum country and the maximum employment value. Finally I returned the two values. Now this piece of code doesn't look any different from what I would have written if I were using lists instead of NumPy arrays. That's a great technique to fall back on. If you can't find a specific NumPy function to do what you want, you can always write code treating them just like lists. However, in this case, there's also an easier way to solve this using NumPy. I didn't expect you to know that, but I'm going to show you how it works. You may have realized you can use NumPy's max function to find the maximum employment value. But that wouldn't give you the corresponding country name. NumPy also has a function called argmax and instead of the maximum value, it returns the position where the maximum value occurs. I'll name that position i, then I can get both the country and the employment value by using that position. Now I'll go through the test code. The first line imported the NumPy library. The next line creates a NumPy array of the first 20 countries we have employment data for. One of the easiest ways to create a NumPy array is the way I did it here. First you create a python list and then you call np.array with that list as an argument which converts it to a NumPy array. The next line uses the same technique to create an NumPy array of employment rates in 2007. Next you should have seen that this line printed the country at position 0, changing the 0 to a 3 prints the country at position 3. The next few lines each print a slice of the array. This one prints everything from position 0 up to but not including position 3. In this line, I've left the first number out, so the slice starts at the beginning of the array, which gives the same results. Here I've left the second number out, so the slice goes from position 17 all the way to the end of the array. And finally I can actually leave both numbers off in which case I get the entire array from the beginning to the end. There's not really a good reason to do this since you could just use the original array, but it makes the notation consistent. Now I'll look at the element type of a few arrays. Which NumPy calls the dtype. I can see that the country array has type S22. S stands for string, and 22 means that the length of the longest string in the array is 22 characters. The employment array has type float64. That means the employment values are floating point values. That is, they have a decimal point. And they're being stored using 64 bits. The next four arrays contain integers, floating point values, booleans and strings of length two. When I loop over the countries array, I see that each country gets printed out one at a time. And I can use this type of loop to go over pairs of values from two different arrays. Here i is the position in each array and then I get the country at that position and the employment value at that position. Finally I just print both values out. If you haven't seen Python's string format function before, it finds each occurrence of these { } and then replaces them with the arguments of the format function. When I run this I see each country printed out with its corresponding employment value. And finally, these functions give the mean, standard deviation, maximum and sum of the employment data. Another benefit of nomparies is that they support something called vectorized operations. The name is taken from linear algebra. But if you aren't familiar with linear algebra, that's fine. For the purposes of this discussion, you can think of a vector as just being a list of numbers. So, for example, one, two three or four, five, six would both be vectors. Now in linear algebra you can add two vectors of the same length. This is called vector addition. Now if you were defining vector addition how would you define it? Specifically, if you were to add these two vectors what would you expect the result to be? I've added some choices here. There's no right or wrong answer to this question, just select the one you would choose. Now these are all reasonable answers. And in fact, they are all implemented by different programming languages. This first one is vector addition as defined by linear algebra. First, the first two numbers were added to get 5. Then the second two were added to get 7. And finally the last two were added to get 9. This is also the behavior you get when you add two. The second result is the result of something called list concatenation, and this is what adding two lists does in Python. A new list is created with all the elements from the first list followed by all the elements from the second list. And the last option is very common. Most languages do implement list concatenation. But in a lot of languages you call a function or use some different symbol other than the plus sign. So then using the plus sign with two arrays or less would give you an error. Since this first option is what Num Py does that's the option we'll be focusing on in this course. Here's another question, suppose you multiply a vector by a single number, also called a scalar. What would you expect the result to be? I've added some options here. Again, there's no right or wrong answer. Again, these are all reasonable answers. The first, this is what Python does. It creates a list with all the values from the original list repeated three times. One, two, three. The second is the behavior you get in both linear algebra and with NumPy arrays. The first element of the original vector was multiplied by 3 to get the first element of the result. The second was also multiplied by 3 and the third was multiplied by 3. And again, the third option is very common. Many programming languages would not allow you to multiply a list or an array by a number. You might be wondering if you can add two vectors, can you subtract, multiply or divide two vectors? In NumPy the answer is yes. You can vectorize almost any operation. You can add two NumPy arrays with a plus sign, subtract with a minus sign, multiply with an asterisk, divide with a slash and exponentiate with a double asterisk. These operations all work with two NumPy arrays. Or with a numpy array and a single number. You can also do logical operations. If you have two arrays of Booleans you can use ampersand to take the logical and. And you can use this vertical bar symbol to take the logical or. This is underneath the delete or backspace key on most keyboards. And finally you can use the tilde to take the logical not. Which is on the top left of most keyboards. Now make sure your arrays contain Booleans if you're going to perform these operations. If your arrays have integers instead then these symbols will do something a bit surprising called bit wise and, bit wise or, and bit wise not. You can read about those operations in the instructor notes if you're curious. But it's not important for this course. You can also use all the standard comparison operations. Now these vectorized operations make a lot of data analysis code super clean. And they're also usually faster than doing the operations manually, using a loop. By the way, there's a cheat sheet available in the downloadable section that will help you remember these operations. As well as a bunch of other common bits of NumPy in Panda's code. On the next screen, you'll have a chance to test out some vectorized operations and see how they work. You'll also write a function that uses vectorized operations to calculate the overall school completion rate in each country using the female school completion rate and the male school completion rate. To do this, assume that exactly half of the population in each country is male, and half is female. Before getting started on this function, I'm going to think about how I would calculate the overall completion rate for a single country. So let's say i have these two values for a female and male completion rate. To get the overall completion rate, I would add up these two values and divide by two. Sure enough, that looks like it worked. Now because of Numpy's vectorized operations, I can actually use this exact same line of code for arrays. To demonstrate that, I'll show the results on these shorter arrays with data for only 20 countries. First, if I add the two arrays together, I get an array with the sum of the female and male completion rate for each country. So each element in the resulting array is the sum of the corresponding elements from the two previous arrays. I'll name this array total_array. Then if I divide the total array by two, I get an array where each element is half of the corresponding element from total array. This is the result I wanted, so I'll scroll up and fill in my function. So the code for this function is return (female_completion + male_completion) / 2. And just like when doing arithmetic on single numbers, adding parentheses is important. Now I'm going to give you another chance to practice using vectorized operations. One question you'll often want to answer in data analysis is, how does a single data point compare to the rest of the data points? For example, I mentioned that I'm interested in how the employment rate in the U.S. compares the the employment rate in other countries. Is it higher or lower than the average, and by how much? The question isn't very well-defined, but one typical way that people use to answer it, is to convert each data point to a number of standard deviations away from the mean. This is called standardizing the data. For example, let's consider the employment data we have for 2007. The mean employment rate was 58.6%, the standard deviation was 10.5%, and the employment rate in the United States was 62.3%. Then the difference between the United States employment, and the mean employment rate was 3.7%. This is equal to about 0.35 standard deviations or about a third of a standard deviation. In Mexico on the other hand, the mean employment rate in 2007 was 57.9%. Thus for Mexico the difference between the employment rate and the mean employment rate was -0.7%. Note that I use a negative difference to indicate a value that's below the mean, and a positive difference to indicate a value that's above the mean. This translates to about 0.067 standard deviations below the mean, giving a standardized score of negative 0.067. On the next screen, I'd like you to write a function that takes a numpy array as an input. And the array could contain any type of numerical data, such as employment data for example. Then I want you to return a standardized array, where each data point has been replaced by the standardized data point. Again, I'll first think about how to standardize a single data point, say the value at position zero. First, I'll calculate the distance of that value from the mean, which I can do by subtracting. Then to convert to a number of standard deviations, I'll need to divide by the standard deviation. And again, parentheses are important here. So this will calculate a single standardized value. And I could use a loop to standardize all the values, but instead I'm going to use vectorized operations. Again, vectorized operations make the code to operate on an entire array very similar to the code that operates on a single value. I'll simply change this variable from a single value to the entire array of values. Now values is the entire array, and the mean is still a single number. So this subtraction subtracts the mean from each value in the array. Then the standard deviation is still a single number, so this division divides each element of the array by the standard deviation. And since this standardized all the values in the array, I will change this variable to standardized values, and I'll delete this line. And finally, I will return the standardized values. There's another useful operation you can do when you have two NumPy arrays of the same length. This time, the second NumPy array needs to contain booleans, and the first can contain any type, not just numbers. For example, suppose your first array, a, contains the numbers 1, 2, 3, 4, and 5. And your second array, b, contains the booleans false, false, true, true and true. Then the code a[b] would return a shorter array containing only the elements 3,4, and 5. The second array is called an index array, and it tells you which elements of the first array to keep. So here we didn't keep 1 because that element was false. We didn't keep 2 because that was false. But we did keep 3, 4, and 5 because those were true. When you combine this with the vectorized operations you learned about previously, this can be very powerful. In this example, I kept all the elements greater than 2. So I could have created the boolean array b with the code b = a > 2. I could also have created this array without using the variable b at all. To do this I would use the code a [a>2], and that would have the same result. In my opinion, this piece of code is a very concise and clear way of saying keep all the elements of a that are greater than two. Right now it may take more effort to understand this piece of code than it would have taken you to understand a loop. First, you have to remember that a is an array. Then you have to remember that comparing an array to a single number is a vectorized operation that returns an array of booleans, and then you have to remember about index arrays. However once you're used to it this can really speed up the process of writing your code. And as usual, this code will run faster than the loop since all the operations are implemented in C. To give you some practice using index arrays I'm going to take an example from the previous lesson. I want you to write a function that takes in two NumPy arrays. The first, time_spent, contains the total time each student spends in the classroom in the first week. The second, days_to_cancel, contains contains the number of days before that student cancelled. I want you to calculate the mean time spent in the first week among only students who do not cancel for at least 7 days. As a hint, you'll first want to convert days_to_cancel into an array of booleans that you can use as an index array. The first thing I want to do is create an array of booleans indicating whether each student cancelled in the first week. I'll call the array is_paid and it will contain true for students who remained enrolled for at least seven days before cancelling and false for others. I can create this array using vectorized operations by comparing the days_to_cancel array to the value 7. Then I'll create an array called paid_time. Which includes the time spent for each student, but only including paid students. To keep only the paid students, I'll us is paid as an index array into time spent. Finally I'll take the mean of this array to get the mean time spent by paid students. And this code could actually all be simplified down to a single line. And again, once you're used to index arrays and vectorized operations, I think this line of code is very clear and concise. It says look at the time spent for each student, but only for students who stayed enrolled at least seven days before cancelling. And then take the mean of that time. Next, I want to talk about the difference between + and += in NumPy. Before I tell you what the difference is, I want you to think about what you expect the difference to be. Look at these two code snippets, which are the same, except that one says a += this array and the other says a = a + this array. For each code snippet, select what you think its output will be. Your options are the array([1, 2, 3, 4]), the array([2, 3, 4, 5]), or the code will produce an error. If you're not sure you can either take your best guess or you can try it and see. The important thing is not getting this right on the first try but that you learn the answer for the future. The first code snippet will print 2, 3, 4, 5. I haven't shown you the plus equals operation before, but it will modify the existing array. So, before you run this line, both a and b will point to the same array which contains the elements 1, 2, 3, and 4. Running this line updates the values to 2, 3, 4 and 5. So now if you print a or if you print b, you'll see the same result, the array 2, 3, 4, 5. The second code snippet, on the other hand, will print 1, 2, 3, 4. Once again, a and b start off referring to the same array. 1, 2, 3, 4. However, the plus operator without an equal sign first creates a new array. The values in the new array are 2, 3, 4, and 5. Then setting A equal to this new array updates A to refer to the new array. B still refers to the previous array which has not been modified. So when you print B you get the output 1, 2, 3, 4. A common term for the behavior you just saw is that plus equals operates in place while plus does not. That is, plus equals is storing all the new values in the same places the original values were stored rather than creating a new array to store them in. Operations that are not in place can be a lot easier to think about. Like you just saw, when you do in-place operations, variables can be modified even when it doesn't look like they will change, such as the variable b in the previous example. In this course we'll mostly be using operations that are not in-place. I prefer to use just plus for vectorized addition, not plus equals. Here's another place where this concept comes up. Read through this code snippet, and decide what you think the output will be. Again, if you're not sure, either try it out, or take your best guess. The answer to this question is a bit surprising or at least, it surprised me when I first learned it. Because Numpy arrays have different behavior than Python lists. You start off with the variable a referring to the array 1, 2, 3, 4, 5. However, when you take a slice of the first three elements of the array a, Numpy doesn't actually create a new array. Instead, slice refers to what's called a view of the original array. It will look like a new array, but if you modify it, the original array is modified as well. This makes slicing a Numpy very fast, since you don't have to create a new array or copy any new values. But it means you should be very careful any time you want to modify a slice of an array. So the correct answer here is that the output is this second choice. An array containing 100, 2, 3, 4, and 5. Now that you're familiar with NumPy arrays, let's move on to pandas series. A series is similar to a NumPy array, but it has a little additional functionality. For example, if you have a series named s, then the function s.describe will print out the mean standard deviation, median, and some other statistics about your series as this function isn't available for NumPy arrays. There are also some other benefits of series over arrays which I'll go over in a minute. For now though, I want to focus on the similarities between series and arrays. All the things you just learned how to do on Numpy arrays will also work on Pandas series. You can access elements using the square bracket notation to get either a single element or a slice of elements. You can still loop over your series. You can call the same convenient functions you just saw such as mean and max, and you can still used vectorized operations. And just like NumPy, pandas series are implemented in C, and thus are much faster than using Python lists. On the following screen, you'll see some test code you can try out using Pandas series. I'd also like you to write a function that takes in two series as arguments, for example, life expectancy in 2007 and GDP in 2007. The countries will be in the same order for each. The function should perform the following simple heuristic to check whether these variables seem to be related. The function will look at the question, when a country has a life expectancy above the mean, is the GDP above the mean also? Or vice versa. If the life expectancy is below the mean, is the GDP below the mean? Your function should return two numbers. The first number is the number of countries where both values are above the mean or both values are below the mean. The second number should be the number of countries where one value is above the mean and the other is below the mean. So these two numbers should add up to the total number of countries. Like I mentioned, the inputs will be panned as a series, but you should be able to write the same code to solve this as you would for NumPy arrays. Here's a hint you can use to make your code a little shorter. In Python, you can actually add booleans up. True will be treated as one and false will be treated as zero. So in Python true + true = 2. So how I would approach this problem is, first create an array of booleans. The boolean is true if both numbers are both below the mean or both are above, and false otherwise. Then I would take the sum of this array to find out how many countries meet that condition. I'll start off by calculating how many of the pairs of data points are in the same direction relative to their respective means. That is, both above the mean or both below the mean. To do this, I'll first create an array called is_same_direction, which contains a boolean for each pair of data points. Now there are two cases for how the pairs could be in the same direction. They could either be both above their means or both below their means. So I'll create two separate arrays of booleans for those two cases. I can use this code to check whether each value of variable1 is greater than the mean of variable1, and each value of variable2 is greater than the mean of variable2. Again make sure you add the parentheses. It's important to do the comparisons first and then do the logical and. The code to check whether both variables are below the mean is identical except with less than signs instead of greater than signs. Then two variables are in the same direction, if they are both above or both below their means. Like I mentioned in the instruction video, you can add up an array of booleans to see how many of them were true. So I can find the number of pairs that are in the same direction by calculating is the same direction dot sum. Finally, I can calculate nom different direction by taking the len of variable one minus the number of pairs in the same direction. Last, I'll need to return the two numbers. Now if you ran this function for the life expectancy and GDP arrays given in the programming exercise, you should have seen that there were 17 pairs that went in the same direction, and three that went in different directions. The fact that most of the pairs went in the same direction indicates that these two variables are positively correlated. That is, when one is higher, the other tends to be higher as well. If the first number had been small, and the second number had been large, that would indicate that the variables are negatively correlated. When one tends to be large, the other tends to be small. And if the two numbers had been roughly equal, then that indicates that there might not be a strong relationship. It's about equally likely for one variable to be positively correlated with the other as negatively correlated. So, what are the benefits of using a panda series instead of a numpy array? I mentioned before that panda series have some extra functions not available for numpy arrays. For example, the describe function. However, the main difference between panda series and numpy arrays is that a panda series has something called an index. Let's take a look at an example. Throughout this lesson, we've been looking at data for a bunch of different countries, and I've been creating one array to hold the actual data points and one to hold the country names. In pandas, I would only create one series, and I would use the data points as the values of that series. Then I would use the country names as the index for the series. Now I'll delete the numpy array code and print out the series. You can see here that pandas is matching up each index value to the corresponding data point. I've said before that numpy arrays are like souped version so of Python lists. Similarly, a pandas series is like a cross between a list and a dictionary. In a list, elements are stored in order and you access them by their position. And you've just seen that that's true for a series also. These values are stored in order, and you can access them by their position. In the dictionary, on the other hand, you can have a key such as a country name, and a value such as a life expectancy, and you can look up values by keys. You saw earlier that the code life expectancy at zero will give the life expectancy at position zero in the series. Pandas also has an attribute called walk that lets you look up values by their index. If I run life_expectancy.loc Angola, then I'll see the life expectancy for Angola, without needing to know which position Angola is In the list. So, what happens if you create a series without specifying an index, like we did earlier? Well, then the numbers zero, one, two, and so on get used as the index, by default. So, before, when I accessed elements using square brackets, it wasn't clear whether I was accessing by index, or by position. At this point, it should be clear to you that the square brackets are accessing elements by position, since there is no value in our life expectancy series with index zero. I prefer to be a bit more explicit when using a series though. Pendas has an attribute corresponding to the lock attribute call iloc to access elements by position. These two lines of code do the same thing, but the version using iloc is just a little more clear in my opinion. By the way, here's a terminology note. In this numpy array, many people will say something like the country Albania is at index zero. And they'll call using square brackets to access an array element indexing into the array. I've been trying to avoid this terminology, and call this position zero rather than index zero, since in a panda series, the index and the position are not the same. But be aware that you might hear the term index used inconsistently elsewhere. On the next screen, I want you to rewrite the max_employment function that you wrote at the beginning of this lesson. Which finds the highest employment rate in some given data, and the country name with that employment rate. This time, the input will be a single series, rather than two numpy arrays, and the index of that series will be the country names. Try using the pandas argmax function. There's a link to the documentation in the instructor notes. The argmax function on the panda series returns the index where the maximum value occurred. In this case, the index will be the country name. Then to find the value corresponding to that country, I'll use the loc attribute. And it'll auto return the country and the value. Series indexes have an interesting effect on vectorized operations. When you add two NumPy arrays, you're adding up elements of the same position since there is no index. Now, for the series that you've added up so far, the positions and indexes have been the same. What do you think happens if that's not the case? And if you were to add based on index rather than position, what do you think would happen if you were to add up two series that had different indexes? Try it out and see. On the next screen you'll see code to add a few different Pandas series with with different indexes. Take a look at the results and see if you can figure out the pattern. Once you have a theory about how vectorized operations work for Pandas series, watch the solution video to see if you are correct. First I'll look at what happens if I add two series with the same index. So here are both series 1 and series 2 have the index a, b, c, d. When I add these series together, the results aren't surprising. The first element from each series is added together, the second element from each series is added together and so on. Now we'll take a look at a case where the two series have the same indexes, but the values are in a different order. So series 1 still has the index a, b, c, d, but series 2 now has the index b, d, a, c. Now when I add these two series together, I can see that the matching up has been done by index rather than by position. Instead of adding the first two values together, both values for index a have been added together, both values for index b have been added together, and so on. Now we'll take a look at two series where the indexes don't have exactly the same values. Here, both series 1 and series 2 have c and d, but only series 1 has a and b, and only series 2 has e and f. When I add these two series, I see that c and d, the index values in common, have been matched up. For index values that were only present in one series, a, b, e and f, NaN is the result, which stands for not a number. Given that, I have a hypothesis for what will happen when I add series with indexes that don't overlap at all. But I'll still try it and see. So in this case, series 1 has index values a, b, c and d. And series 2 has index e, f, g, and h. Sure enough, when I add them together, I get all the index values from both series and value in each case is NaN. By the way, other vectorized operations with series work the same way. Values are matched up based on index not position. And if an index value is present in one series but not another, the resulting value will be NaN, for not a number. So you just saw that if you add two series together and if the indexes don't match, NaN will be filled in. Now, in a lot of cases, I might not want NaN or not a number to show up in my resulting series. So if I didn't want the NaNs to be included, how would I deal with that? For questions like this there's almost a way to do what you want with Pandas, but it can be a bit tricky to figure out what that way is. One thing I might try is doing a Google search for pandas remove missing values. The second result, dropna looks promising. The first thing I noticed about this documentation is that it operates on DataFrames, not series. You'll learn about DataFrames next lesson. Now the documentation for pandas isn't always perfect. When I saw this result, I wondered if dropna would work on a series as well. I decided to try it and it runs out it did. This documentation also refers to an axis. Again, this is something you'll learn about in the next lesson. For now, the fact that this function is emitting labels where some or all of the data are missing sounds promising. So I'll try saving the result of this addition to the variable sum result and then I'll run sum_result.dropna. And sure enough, I get a new series where all missing values are removed. There's actually a few different ways you could solve this problem in pandas. For example, there's a way to treat missing values as zero before the addition. So then the result of adding these two series would have value 1 for index a, since there's an assumed zero in series 2. See if you can figure out how to do this. If you find a third solution to the problem of NaNs, that's great as well. Now figuring things out like this by Googling can be very challenging, but it's also very important. Pandas has a lot of functionality, and this course won't be able to cover all of it. If you don't find what you're looking for right away, try a few different queries. If you get stuck, you can always watch the solution video. You can use the programming quiz on the next screen to test out whether your solution is working. Here's a query I use to try to find the answer to this problem. Pandas add two series without nans. And it looks like the first result is about adding pandas series with different indices without getting nans. That sounds like what I'm wondering. The first answer on this page suggests using the fill value and some sample code is provided. In the example code, I can see that the person who posted, is advocating using the .add function, instead of the +. And then the .add function can take an argument, fill_value=0. So I'll try that out on these two series. And it looks like the result is exactly what I was hoping for. So far in this lesson, you've been doing computations on series using built-in functions. For example, the mean function and vectorized operations. For example, plus for vectorized addition. But what do you do when you want to perform some calculation that's not built into pandas and can't be performed using simple vector s operations? Like you saw before, one option would be to treat the series as a list using four loops or accessing individual elements of thee series. There is another option though, pandas has a function called apply(), which you can use to perform computations that aren't built into Pandas. Apply() takes a series and a function and it creates a new series by applying the function to every element of the original series. So for example, suppose the function was add three, which takes a number and adds three to it. Applying this function to the series on the left would give the series on the right. First, add three would be applied to the first element of the series giving four. Then add three would be applied to the second element of the series, giving five. Then to the third element of the series, giving six and so on. If you're familiar with the Python function map apply() is exactly the same, but it works on series instead of lists. If you haven't heard of map, that's okay too. Now in this example of adding three, it would have been better to use vectorized addition. If this series were named s, then you could create the new series with the code s+3. And this would do the same thing as s.apply(add 3). However, apply() can be very useful in cases where there is no built-in operation that does what you want. For example, suppose you have a series of states where some states are represented by the full name and others by the abbreviation. I want to clean up the data by replacing each full state by its abbreviation. Now let's say, I've already written a function that does this for a single state. The code might look something like this. First, I check if the value is already an abbreviation. In which case, I return it unchanged. Next, I check if the state is Alabama. In which case, I return the abbreviation for Alabama, and so on, you get the idea. Now I could create an empty list, then loop over the series, then call my clean_state function on each value in the series and append each new value to the list. But I probably really wanted by cleaned data to be a panda series rather than a list, so I could convert this list to a series at the end of the loop. Instead though, I could use the apply() function. First, I type the name of my original series, which is states, then I call the apply function. And as an argument to the apply() function, I give the clean_state function that I wrote to clean up each state. Note that I don't put parenthesis after the clean_state function, I'm not calling the function here. I'm giving the entire function as an input to apply(). The main advantage of using apply() over a loop is that it makes your code a lot more concise. It might also be slightly faster. But in this case, the bulk of the time will probably be spent within the clean_state function. So, it probably wonÃ‚Â´t matter much from a speed perspective whether I use a loop or apply(). Now, it's your turn. Write code that will take a series of names in the form of, firstname, lastname and then put them into the form lastname, firstname. You might find Python's split function helpful, there's a link to documentation in the instructor notes. In order to write the reverse_names function, I'm going to start off by writing a function that reverses only a single name. First I'll split the name on a space, which should give me a list containing the first name and the last name separately. Then I'll extract the first name and the last name using split_name at 0 and split_name at 1. And finally I'll return the last name, followed by a comma and a space, followed by the first name. Now before trying to apply my reverse_name function to the entire series, I'll test it out on a single value. I'll use the iloc attribute to get the name at position zero, and then reverse that name. And sure enough, the name Andre Agassi has been transformed to Agassi, Andre. Now I can reverse all the names using the apply function. Specifically, I'll return names that apply and all passes and argument the reverse name function. And when I call reverse names I get each of the names reversed and collected in a panned series. We provided code for you to load each variable into something called a data frame, which you'll learn about next lesson. Then this code extracted a series from each data frame corresponding to the US only. If you wanted to examine a country other than the United States, you could have changed this value. Now, to make my plots, I'll add the lines %pylab inline, to make my plots appear within the iPython notebook. But if youre not using iPython notebook you wouldn't have added this line. And then I can plot the employment in the US by calling employment_us.plot. Here, I can see that the years, which are the indexes of the series, have been filled in on the x-axis. And the employment values have been filled in on the y-axis. And the same code would work for the other variables. Congratulations, you finished the lesson. You learned how to use Numpy and Pandas to analyze one-dimensional data using code that's cleaner and runs faster, great job. In the next lesson, you'll build on what you've learned and start analyzing two-dimensional data. I'll see you then. In the last lesson you used NumPy and Pandas to analyze one dimensional data. In this lesson, you'll learn a lot more features of both libraries that you can use to analyze two dimensional data. By the end of the lesson, you'll be able to rewrite a lot of the codes that you wrote in lesson one but this time using Pandas and if you're wondering why I'm wearing this shirt, it's because you'll also be working with a new data set this lesson containing subway ridership and weather data for New York City. Let's get started. Just like last lesson, you'll be able to complete all the programming exercises for this lesson on the Udacity website. But the data is available in the downloadable section in case you'd like to explore it further. Again, I'll be focusing on NumPy and Panda's rather than going through the data analysis process in order. But I still want you to start by asking questions. So download the description of the data from the downloadable section, then think of at least five questions you could answer with this data and share them on the forums. When you're finished, check these boxes. Here are some questions I thought of. In general, I'm interested in what variables are related to higher or lower subway ridership. If I were working for the New York Subway, this information might be useful when scheduling how many trains to run. But since I don't I'm just curious. There are several variables that could be related to subway ridership. For example, which stations have the most riders and how dramatic is the difference? I'd also like to know what are the ridership patterns like over time? Will I be able to see rush hour effects? How will weekdays and weekends differ? This data includes the month of May, which includes Memorial Day, so I wonder whether Memorial Day ridership will be similar to weekend days? I could also look at how the weather affects subway ridership. Do more or fewer people ride the subway when it's raining, when it's hotter out, etc. I could also investigate weather patterns in New York independently of the subway. For example, since May comes before the summer, is the weather systematically getting hotter and hotter throughout the month? And how does the weather vary across the city? Hopefully your excited to answer some of these questions. Lets get started. Last lesson, you worked a lot with one-dimensional data. In this lesson, you're going to work with two-dimensional data, data with both rows and columns. In pure Python without any libraries, you might represent this data using a list of lists. So one list for the first row, one list for the second row and so on, and then put each of those lists on a list. In NumPy instead of creating an array of arrays, you can create a single two-dimensional array. In Pandas, there's a different data structure for two-dimensional data called a DataFrame. Just like last lesson, I prefer to use Pandas since it has additional functionality, but I'm going to start with NumPy because it's simpler. So what's the difference between making a single two-dimensional array in NumPy Instead of making a one-dimensional array whose elements are also arrays? One important difference is that because of how NumPy is implemented, making a single 2D array is more memory efficient. If you'd like to learn more about this, follow the link in the instructor notes. The syntax for accessing elements is also a bit different. Say you wanted to access the element at row one and column three. You would do this using the code a[1,3] rather than a[1][3] which is what you would do for an array of arrays. You can also make either the row position, the column position or both a slice, rather than a single number using the colon notation you learned in last lesson. Another difference is with the functions you learned last lesson, mean, standard deviation, and so on. With a two dimensional array, these functions will operate on the entire array. So in this case, mean would calculate the mean of all these values, ignoring which rows and columns they were in. You can't take the mean of an array of arrays, although you could take the mean of each individual row. On the next screen, you'll get a chance to play around with using an umpire array to represent subway ridership. Each row will be a different date, and each column will be a different station. Try out the example code given to see what it does. Then write a function that first finds the station with the maximum riders on the first day. Then find the mean riders per day for that station. Also return the mean riders per day across all stations for comparison. First, I'm going to write code that will work on this small example array, so that I can see whether it's working at every step. Then when I'm finished, I'll turn it into this function, mean riders for max station. The first step is to find the station with the maximum riders on the first day. The first day is row 0 of the array, which I can get by passing 0 as the row position, and colon, meaning all of the columns, as the column position. And I"ll check that that worked and sure enough, the first row was printed. Now to find the maximum value, I would find the .max function. What I actually need is the station that had the maximum value. To get that, I'll use the arg max function, which will return the position of the maximum value. Taking a a look, I can see it was position 3. And sure enough, column 3 in the array has the maximum value. I'll name this value max_station. Next I want to find the mean riders per day for that station. First I want to get the ridership data for that station, so I'll select all rows, and the max_station column. And I'll take a look, and sure enough, that gives the correct column of the ridership data. Next I'll take the mean of those values. And I'll call it mean_for_max, meaning the mean value for the max station. Finally, I need to find the overall mean for comparison. I can do that by simply taking ridership.mean. Now I'm ready to move this code inside of my function. Since the example array and the argument to the function have the same name, ridership, I don't need to rename anything. And I'll return both values. Next I'll call this function on the example array. It looks like the overall mean was about 2300 and the mean for the max station was about 3200. So the station with the higher ridership on the first day also had a higher ridership overall than the average. Of course, that doesn't mean it has the maximum overall ridership out of any station, but you'll calculate that in the next exercise. You just saw that NumPy functions like mean operate on the array as a whole but in many cases it might make sense to calculate the operation by row or by column. For example, consider the array of subway riders that you just looked at, where each column is a station and each row is a date. It might make sense to take the mean ridership per day across all subway stations like you just did but you might also want to take the mean ridership for each station or for each date. Most functions built into NumPy take an axis argument for this reason, which can be either zero or one. If the axis is zero, then the function will be calculated for each column and if the axis is one, it will be calculated for each row. I find it hard to remember which axis is zero and which is one so, I always just try it out and see before using the axis argument. To get some practice, I want you to write some code to find the mean ridership per day for each subway station. Then, out of all the subway stations, return the maximum and the minimum ridership per day. First, I want to find the mean ridership per day for each subway station. Since each station is a column, I can do this using the mean function with an axis of 0. Sure enough, that gave me five means. One for each of the subway stations in this array. I'll call this array station riders since it contains the ridership for each station. Next, I need to find both the maximum daily ridership and the minimum. I can find the maximum by taking station_riders.max and the minimum by taking station_riders.min. Next I'll move this code inside of my function, and again, I don't need to change any variable names. And I'll return the max_daily_ridership, and the min_daily_ridership. Finally I'll run this function on the example array. It looks like the station with the maximum riders per day had about 3200 and the station with the minimum had about 1,000. The maximum is the same value that we saw last time as the mean ridership for the station with the maximum on the first day. Which indicates that the station in this case that had the most riders on the first day also had the most riders overall. The minimum is lower than the overall mean we saw before, which makes sense. In the last lesson you saw that num pi arrays have a d type which is the type of each element in the array. For example the d type of this array is int64. Int64 means a 64 bit integer. Now this also applies to two dimensional arrays. Each element in the array is expected to have the same type. This can make it inconvenient to use unconventional num pi arrays to represent the contents of a CSV file. For example, suppose I tried to create a num pi array to represent the student enrollment data from lesson one? I've recreated a small piece of that data here. Even though when I created the array, I entered integers and volumes without quotes, each element has been converted to a string. The dtype is also s14, meaning strings of length at most 14. Obviously it doesn't make sense to take the mean of strings, such as cancelled, but it would make sense to take the mean of just the days_to_cancel column. The days_to_cancel column is column three, so I can access it like this. However, if I take the mean of this column, then it doesn't work. Instead, num pi says that it cannot perform the reduce with a flexible type, which means that it doesn't know how to take the mean when not all of the values are numbers. That's one benefit of Panda's dataframes over two dimensional num pi arrays. A dataframe is also a two dimensional data structure, but each column is assumed to be a different type. Another benefit is that dataframes have indexes similar to Panda's theories. There is an index value for each row, and a name for column. So instead, I'll create a Pandas DataFrame from this data. I can do that by passing in a dictionary mapping column names to lists of values for that column. So, one key in the dictionary would be account_key, and the list would be all of the values for the account_key column. And similarly for the other columns. Now when I take a look at the data frame, it's displayed in a nice table. The column names are at the top and the role indexes are integers starting at zero. There are some other ways to creat Pandas data frames as well, which you'll take a look at soon. Now if I take the mean of the data frame, it takes the mean of each numerical column and ignores the others. It's udacity is a numerical column here with trues treated as one and falses treated as zero. You'll notice that Pandas did not take the mean of the entire data frame here, but of each column. Since each column is assumed to be a different type, this makes more sense for Pandas. You can also use the axis argument to take the mean of each row instead, although that won't work in this case, since each row contains some non-numerical data. Once you've created a data frame, how do you access its elements? As an example, I'm using the same ridership data that you looked at earlier, with station names as the column names. I'll also use the index argument to add dates, as the row indexes. When I take a look at the data frame, iPython notebook displays it as a table, with the appropriate column names and row indexes. Accessing a single row of a data frame is similar to accessing a single element of a series. You use .loc to access rows by their indexes. So here this code will give me the second row of the data frame. Similarly, you can use iloc to access rows by position, so this code gives me the last row of the data frame. To access a single element, you can also use loc and iloc, but this time, indicating both a row and a column. For example iloc at 0, 3 gives the element for row 0, column 3. Similarly, using .loc I can specify a date and a unit and get the corresponding value for that date and unit. You can also access columns using square brackets. So this code gives me the column just for unit R006 and you can see the beginning of that column here. If this is starting to seem like a lot to keep track of, don't worry, it's all in the cheat sheet available in the downloadable section. You can also use .values to get a 2D NumPy array containing only the values from your data frame, not the column names or row indexes. Of course you'll have to be careful of your data types if you do this. One reason you might do this is if you wanted to take the mean of all the values, rather than the mean of each row or each column. That operation isn't supported for Panda's data frames. But you could call .mean on the NumPy array using .values. On the next screen you'll have a chance to play around with data frames. I also want you to get some practice using data frames by re-writing one of the functions you wrote earlier, but this time for data frames. You could just call .values and then reuse the code that you wrote for NumPy arrays. But I recommend that you figure out which of the operations you did before will still work on data frames, and which you'll have to use .values for. I'll start with the same function I wrote before for NumPy arrays, and then I'll try each line to see whether it works on data frames as well. First I'll try out the first line of code, which is meant to take the argmax of the first row in the data frame. This gives me a pretty scary looking error message, starting with type error and ending with unhashable type. Now, it can be really hard to figure out why you're getting error messages like this, and I find that in general Panda's error messages aren't always very helpful. Usually what I do is try to double check the codes that I just wrote. So for example, if I was modifying some code from a documentation page, I'll make sure that I used exactly the same code as the documentation page used. In this case, the problem is that to access a row of a dataframe, I need to use i loc. I also don't need this colon anymore to specify all columns. And this time when I run the code I get the value R006. It looks like R006 is the fourth station which is indeed the one with the maximum riders on the first day. Note that for a numpy array the arc max function returned to the position. But for a series, the arc max function is returning the index of the series. Or in this case the column name from the data frame. So I'll update this line within my function. Next I'll try out the second line of code from the function. I've used R006 here as the max station. Again, I get a scary looking error message and again this is because this is not how you access a column of a data frame. I don't need the colon here. So trying again it looks like it worked this time. I'll go ahead and make that update in the function as well. And finally I'll try out the last line of code from the function. And this line of code does work but it provides a series containing the mean for each column. Rather than the overall mean, which is what I wanted. Like I mentioned before, there's no way to directly calculate the overall mean of the dataframe, so instead I'll use .values to take the mean of the NumPy array. And trying this out, I can see that it gives a single number, like I expected. And once again, I'll update the corresponding part of the function. Now I'll try calling this function on my ridership data frame and I get the same answer that I got before with an NumPy array, which is good. A data frame is a particularly good data structure to represent the contents of a CSV file. It's a two-dimensional data structure with a different type for each column, which is often exactly what you want for a CSV. Pandas has a function called read_csv, which takes in the file name of a CSV file and loads it into a data frame. I'll use this to load in the New York subway and weather data. Since there is a lot of data I don't want to print out the entire data frame like I've done before. Instead, I'll use the head function to print out just the first five lines. The head function returns a smaller data frame so the results display just like a small data frame. I can also use the describe function to see some statistics about each column like the mean and the standard deviation. You got a preview of the read CSB function at the end of the last lesson. It was used by the starter code that loaded in the gap minder dataset for you. In the following quiz, I'd like you to write a function to calculate what's called the correlation of two variables. It's also known as Pearson's r. In the previous lesson, you used a rough heuristic to determine how correlated two variables were. For each pair of values, you looked at whether both values were above their respective means, both below, or one above and one below. The idea behind Pearson's r is similar. First, you standardize each variable. That is, convert it to a number of standard deviations above or below the mean. Then, you multiply each pair of values and take the average of the products. So, r is equal to the average of x in standard units, times y in standard units. Let's take a look at what this is doing. First, by standardizing, we're converting both variables to a similar scale. So for example, if the variables were much more spread out in the y dimension than in the x dimension, then standardizing would spread the variables out more in the x dimension. Next, since the mean is subtracted from each data point, the area is essentially divided into four quadrants. Where both values are above the mean, both values are below the mean, or one value is above the mean and the other is below. If both values are above the mean, then the standardized x value and y value will both be positive, so the product will be positive as well. If both values are below the mean, then the standardized x and y coordinates will both be negative. So again the product will be positive. Thus is Pearson's r is positive, then as one variable increases the other tends to increase as well. On the other hand, if one value is below the mean, and the other value is above the mean, then the product will be negative. So if Pearson's r is negative, that means as one variable increases, the other tends to decrease. Pearson's r is more accurate than the heuristic you used earlier, since it takes into account more information than just which quadrant each data point lies in. For example, this data point is very close to the mean of the y values, so the contribution to Pearson's r will be small. On the other hand, this data point is very far from each of the means. So the contribution to Pearson's r will be large. Pearson's r can range from -1 to +1. If it is near zero, that means the variables are not strongly correlated. There's a link in the instructor notes to a page with graphs for different values of Pearson's r. So that you can get a feel for what different values of Pearson's r look like in a scatter plot. In the next exercise you're going to write a function to compute Pearson's r. Then you can use it to check some different variables from the New York subway data set, and see how correlated they are. When standardizing both x and y, you'll need to pass the argument ddof=0 to Panda's standard deviation function. This argument controls whether the corrected or the uncorrected standard deviation is taken. If you don't know what the difference is, there's more information on the page linked in the instructor notes. Many times when you're standardizing variables, it doesnt matter too much whether you take the corrected or uncorrected standard deviation. But to calculate Pearson's r specifically, it's important to take the uncorrected standard deviation, which you can do using ddof=0. To write this function, I'll first need to standardize both x and y. You've written code to standardize a variable before, but this time it's a little different because you'll need to make sure you take the uncorrected standard deviation by passing ddof = 0 for both variables. As before, I also need to be careful about the parenthesis. Then to multiply each pair of values, I'll use times which does vectorized multiplication. Then I'll use mean to take the average of the products, adding parentheses so that I take the mean of the products rather than just taking the mean of y and then multiplying by x. And I'll return this value. NumPy actually has a function that will calculate Pearson's r for you, but it can be useful to write the code yourself the first time so that you'll understand what it's doing. In the future though, I'd recommend using NumPy's function rather than writing your own. Next, I'll use this function to calculate the correlation between some of the variables in the subway data. First, I'll calculate the correlation between entries and hourly, or the number of entries each hour and mean precipi or the mean precipitation for the the day, and I can see that the result is about 0.03. This value is positive, which means that when precipitation is higher, subway ridership is higher as well. But it's also very small, which means that the relationship isn't particularly strong. For comparison, I'm going to take the correlation between two variables that I expect to be more strongly correlated. ENTRIESn_hourly is the number of people who ride the subway each hour, and ENTRIESn is a cumulative total of how many people have ridden the subway that increases each hour. I definitely expect these two variables to be correlated, although I don't expect the relationship to be super strong, necessarily, since the total number of entries depends on the number of entries for all previous hours as well as all the entries in the current hour. But the result I get here is about 0.58, which is much higher than for entries in hourly and mean percipi but is also much less than the maximum value for Pearson's r, which is one. Just like NumPy, many Panda's functions also take an axis argument. And you can use axis = 0 or axis = 1, like for NumPy, but you can also use axis = index or axis = columns. Now this sounds like it should make it easier to remember which version of the axis argument is which, but it can actually be pretty confusing. Taking the mean with axis equals columns will actually not give the mean of each column. Instead it gives the mean of each row. The idea is that you are taking the mean along the columns. Similarly with axis = index, you are taking the mean along the index, or the rows. I still have trouble remembering which way the axis argument will work, so again I always try it out in iPython and see what happens before I use the argument. Data frames also support vectorized operations, and they work similarly to vectorized operations for 2D NumPy arrays. Like pandaSeries, they match up elements by index and column name, rather than by position. On the next screen you'll get a chance to try out some different vectorized operations and see what they do. There will also be an exercise for you to get some practice. The Subway data in its original format did not include the hourly entries and exits. Instead, it had cumulative or total entries and exits. So, for example if the total entries were 10, 40, 60, 65 and 85 for the first five hours the station was open. Then the hourly entries for the first hour would be 10. Again, assuming that this is the first hour the station was open. And the entries for the next hour would be 30, since 30 people joined to bring the total from 10 up to 40. And the rest of the hourly values would be 20, 5, and 20. We converted the total entries and exits into the hourly entries and exits that are available in the file in the downloadable section. In the following exercise you'll write a function that takes in a data frame with cumulative entries in one column and cumulative exits in a second column. And then return the data frame with hourly entries and exits. You might find the shift function helpful. Documentation for it is linked in the instructor's notes but I recommend that you just try it out and see what it does. I mentioned that the shift function might be helpful. So I'll start off by looking at the documentation for that. The documentation says that the function will shift the index by the desired number of periods with an optional time frequency. Now that seems pretty cryptic to me. So, I'm just going to try the function out and see what it does. One thing I do see here, is that the shift function needs to take an argument called periods, which is an integer indicating the number of periods to move. So, I'll try shifting the example entries and exits data frame and to start off with, I'll shift with a number of periods equal to one. When I try this out, it looks like the first row of the resulting DataFrame contains NaNs. The second row corresponds to the first row of the original DataFrame, the third row corresponds to the second row, and so on. Just to make sure I understand, I've also run this function with the argument 2, and this time the first two rows are NaNs. And the third row of the resulting data frame corresponds to the first row of the original data frame. To calculate the hourly entries and exits, I want to take each value in the data frame and subtract the value from the previous row, which I can do using vectorized operations and the shift function. Here I've taken each value in the data frame and subtracted, that dataframe shifted by one, which will give me the previous row. When I run this function on the example dataframe, I get the following output, which looks reasonable. One odd thing is that the first row contains NaNs, but when you think about it, this makes sense. Because for the first row in the data frame, you dont know how many hourly entries and exits there were, since there's no previous row to compare to. It might make sense though to drop this row or replace these values with zero. Just like for a pandas series, you sometimes might want to perform some computation on a DataFrame that can't be done using built-in functions or operations. Similarly to a series, you can write a function, let's say f, that takes in a single element from the DataFrame and returns a new element. Then you can apply that function f to each element of the original DataFrame in order to get a new DataFrame. For example, last lesson, I wrote a function called clean_state to take in a state that might be in one format or another, and return it in a standardized format. To call this function on each element of the DataFrame df, you would call df.applymap(clean_state). Now, why is this function called applymap instead of apply like it was for series? I think it's kind of unfortunate, but you'll see in the next video that apply does something slightly different for DataFrames. In the next exercise, I want you to write a function that converts numerical grades to letter grades. You'll be given a DataFrame where each column is an exam, each row is a student and each value is the score that student got on that exam. You should use the rule that a score of 90 or higher is an A, 80 or higher is a B, and so on. Use applymap to create a new DataFrame, where each numerical score has been replaced by a letter grade. To write the convert_grades function I'll first write a function that converts a single grade, and then I can apply that function to the entire data frame. I'll call the singular version of the function convert_grade. I wrote this function using a series of if statements. First, if the grade was at least 90 then I returned A, then if the grade was at least 80 I returned B and so on. Now I'll try this function out on a few single values. If I convert the grade 80, I get B. And if I convert 79, I get C, which is what I expected. Next I can use the applymap function to call the convert_grade function on each element of the data frame. Now I'll call this function on the example grid's data frame and it looks like it worked. I mentioned in the last video that data frames also have an apply function, so what does it do? Well, in some sense, apply on data frames actually is the direct analog of apply on a series. It depends on what you consider a single piece of a data frame. You could say that a single piece is a column, rather than a single element. The function you pass to apply, which I'll call f, takes in a panda series, which is an entire column of the data frame. And it returns a new panda series, which is a new column of the data frame. Then when you call df.apply(f), apply calls your function f on each column of the data frame. You might be wondering why you would do this, instead of using apply map. And the answer is that the operation you want to do to each element of the data frame might depend on the entire column. Consider the example of grading on a curve. Suppose I want to give an A to the 20% of students with the highest grades, B to the next 30%, C to 30%, D to 10%, and F to 10%. Let's not debate whether this is a reasonable grading policy. Instead, let's talk about the code I would write. I could store the numerical scores in a data frame just like I did before, with one exam in each column and one student in each row. But this time I can't use apply map. If I just look at one score, I don't know what letter grade it should get without looking at the other scores for that exam. For example, is a score of 70 an A? If the other scores on the exam are all high, then 70 would not be an A. But if the other scores on the exam are all low because this was a harder exam, than a 70 might be an A. So what I would do is write a function f that takes in the scores for a single exam, and returns converted scores for that exam. Then I would call df.apply(f) to apply that function to the entire data frame. Apply also takes an access argument so you can also write a function that operates on one row at a time if that's what you need. Last lesson you wrote a function to standardize a single panda series. That is, convert each value to the number of standard deviations that value is above or below the mean. Now, I'd like you to use apply to standardize each column of a data frame. In order to write the standardized function, I'm first going to write a function that standardizes one column of a DataFrame, which I can then call on each column of the DataFrame. I'll call the function that works on a single column, standardized_column. And hopefully, the code in this function looks familiar by now. I'll test this function by calling it on a single column of the grades DataFrame. And it looks good to me. Then to standardize each column of the data frame, I'll call df.apply and pass in the standardized column function. When I try this out, I can see that I got the same results as before for the first column, and the results for the second column have also been computed. There's also another possible way to use the apply function for data frames. Instead of taking in one column and returning a new column, the function f could take in one column and return just a single value. In this case, instead of creating a new data frame, apply will create a new series. Where each column of the data frame has been reduced to a single value. For example, if I wanted to find the maximum value in each column, I could call, df.apply(np.max) to call NumPy's max function on each column of the data frame. This would give me the same result, it's called the built-in function df.max. So in this case using the built-in function would be simpler. But as usual apply can be useful when there's not a built-in function that does what you want. For example what if you wanted to find the second largest value in each column? In that case it would make sense to use the apply function. In the next exercise, I want you to do just that. As before, I'll start off by writing a function that operates on a single column, which I'll call second_largest_in_column. Now there are a few different ways that I could find the second largest value in a column. One way that occurs to me is I could sort the column in descending order, then look at the second value in the column. We haven't sorted a panda series before, so I'll start by Googling panda's sort series. This first link is the pandas documentation page for a function called sort. This looks like what I want, but there's a note that this function has been deprecated. And instead, I should use the sort values function with inplace equals true for inplace sorting. I'll click this link at the bottom to see the documentation for the sort values function. I can see that the inplace argument affects whether the series will be sorted inplace, the alternative being to create a new series. Now, I don't want to modify the original series. So I'll use inplace equals false, which is the default value for inplace. I can also see that there's a flag called ascending, which controls whether the series is sorted in ascending or descending order. I'll want to pass false, so that the series will be sorted in descending order. So here, I'm creating a sorted column by calling the sort_values function with ascending=False. And like I said, I'm using the default value for in place. The sorted column will be a panda's series, so I can access the second value using iloc and the second value will be at position one. Now, I'll try calling this function on one of the columns in the data frame to see if it works. It returned 4, which is indeed, the second largest value in column A. Now I'm ready to take in the second_largest function, which will take in an entire data frame. The real work has been done already and now I just need to apply the second_largest_in_column function, to the data frame. Finally, I'll call this function on the test data frame and it looks like it worked. You saw before that you could use vectorized operations to add two DataFrames together, just like you can add two series together. It's also possible in pandas to add a DataFrame to a series. What do you think this will do? On the next screen, try it out and see if you can figure out what is happening. Once you think you know how it works, watch the solution video to see if you were right. I'm going to run each piece of example code and see what it does. The first piece of code involves adding a series to a square data frame. That is, a data frame with the same number of rows as columns. I've used 0, 1, 2, and 3 as both the index values and the column names here. I've printed out both the original data frame, and the result of adding the series here for comparison. It looks like the value 1 was added to each value in the first column, 2 was added to each value in the second column, and so on. So it looks like each value from the series was added to one of the columns of the dataframe. Given this, I have an idea what will happen if I add a series to a dataframe with a single row. But I'll try it anyway. Again, I've printed both data frames here for comparison, and again it looks like each value from the series was added to a single column from the data frame. 1 was added to the first column, 2 was added to the second column and so on. Something that might not be obvious given this is what will happen if I add a series to a data frame with only one column? Will it figure out somehow that it should add the values to each row of the data frame instead? I tried it out here, and you can see that's not what happened. Instead, the result of the addition actually has four columns, one for each value in the series, and every column except the first contains NaNs. The first column had 1 added to each value, as before. By the way, the code df.add s it's usually the same as df plus s, so I usually don't call this function. But the benefit of using this function is that it can also take an axis argument. If I use axix = columns, I get the same result as df plus s. However if I use axis='index' then each value of the series will be added to one row of the data frame, which might be what I want in some cases. Now in all the examples I've tried so far, the index of the series has been the same as the column names of the data frame, 0, 1, 2, and 3. Now I'm going to look at an example where that's not the case. Here the index of the series is B,C, D, and E and the column name of the data frame are A, B, C, and D. The result here is very similar to what happened when we added Pandas series and the indexes didn't match exactly. For each letter that wasn't present in both the data frame and the series, the result is NaN. For letters that were present in both, the addition has been matched up based on letter. So every value in the b column has had 1 added to it and 1 was the value in the series with index b. So in summary, adding a data frame to a series, adds each value of the series to one column of the data frame. And it matches up the series to the dataframe using the index of the series and the column names of the dataframe. Earlier this lesson you wrote code to standardize each column of the data frame using apply. Now using what you just learned, I want to try the same thing again, but this time without using apply. Instead, use rectorized operations. As an extra challenge, try standardizing each row of the data frame instead. Again, without using apply. This one is tricky. You might have to experiment for a while to get it to work. I'm going to show writing the code on this data frame of grades with two exams and five students. First, I know that I'm going to need the mean of each column, which I can get using grades_df.mean. This gives me the mean for exam1 and the mean for exam2 in a Pandas series. I'll also take a look at the original data frame here. Now, in order to standardize each column, I want to subtract the mean of each column from that column. So I want to subtract the first mean from every value in the first column, and I want to subtract the second mean from every value in the second column. This works the same way you saw addition just working. I'll take grades_dr minus grades_df.mean. And sure enough, looking at the first two scores for exam1, I see a negative score for Andre since his score was below the mean, and a positive score for Barry since his score was above the mean. Next I need to divide each column by its corresponding standard deviation, which I can do in the same way. I use grades_df.std to get the standard deviation of each column, and then I divide the data frame by the series to divide each exam score by its corresponding deviation. As usual, watch out for the parentheses. And running this again, I can see that each exam has been put in standard deviation units. Notice that this code actually looks exactly the same as the code to standardize a single series. That's pretty cool, but as you'll see in a minute, it's sort of deceptively simple since standardizing each row is a little more involved. So how would I go about standardizing each row? Well, first I'll take a look at the original data frame again for reference. I know that this time I'll need the mean of each row and I can get that by using axis = columns. Remember this means I'm taking the mean along each column which will give me the mean of each row. And sure enough this gave me the mean of each student across the two exams. Now I need to subtract the first mean from the first row of the data frame, the second mean from the second row and so on. However, if I try to use grades df minus grades_df.mean like I did a minute ago, it will try to subtract the amount of these values from one column of the data frame which actually won't work since the indexes of this series don't line up with the column names of the data frame. So I would just get a bunch of demands. Instead, I'll need to use the sub function which stands for subtract and does the same thing as the minus sign but it can also take an access argument. In this case, the access should be index since I want to match up the elements of the series with elements of the data frame using the row indexes. And now I'll try this out. Here I can see that all the values in column one and all the values of column two have the same absolute value. Which is what happens when you subtract the mean from only two values. I'll name this data frame mean_diffs to use later. Next, to divide by the standard deviation I need very similar code to before. I'll take the standard deviation of the grades column with axis equals columns so that I get the standard deviation of each row. Then I'll use mean_diffs.div to divide the mean_diffs data frame by the standard deviations series. And then I'll use axis='index' to match up elements based on row indexes. You can see here that the results are the same for each row, which again happens because there's only two columns. Now this code actually looks exactly the same as the code to standardize each column, except that each axis argument was the opposite of the default. This is something to watch out for when writing Pandas code. If you're doing a very common operation then you likely won't need to specify an axis since the default axis will be correct for you. On the other hand, if you're doing something slightly unexpected like standardizing each row instead of each column of a data frame then you're going to have to specify non-default axes all over the place. A lot of the questions I have about the subway data set have something in common. For example, I'd like to look at how ridership varies by hour of the day. But the file contains many entries for each hour of the day from different days and different stations. So first, I'll need to separate the data into one group for each hour of the day, then take the average ridership for each hour, then I can analyze these numbers. Similarly, I want to look at whether rain affects subway ridership. One possibility would be to break the data into two groups, data when it is raining and data when it is not raining, then I would compare ridership between the two groups. Again, there's a grouping step followed by an aggregation step where I sum up all the entries within each group or take the average of the entries within each group. I also said I was interested in how the ridership varies by day of the week where again, I would need to group by the day of the week then aggregate for each day. This might remind you of the code for lesson one where you grouped data by account key and then added up engagement records for each account key. In lesson one, I chose to accomplish the first step with a function name group_data, and the second step with a function named sum_grouped_items. Pandas has similar built in functions allowing this type of analysis to be done with much less code. Let's take a look at a concrete example, suppose that I had the engagement data for each student in a Pandas data frame. I've created a small data frame here to use as an example. Then the code I would write to get the average number of minutes spent in the classroom would be engagement_df.groupyby ('account_key').sum()['total_minutes_vi- sited'].mean( ). Now let's go through what each piece of this code does one bit at a time. First, calling engagement_df.groupby('account_key') creates a special type of object called a DataFrameGroupBy object. I think of it as being a dictionary mapping each account key to a smaller data frame containing only the rows that match that account key. Although, it's actually this custom object rather than a dictionary. If you want to, you can get a look at the mapping using the .groups attribute. This shows that the account key 1175 corresponds to rows 7 through 13 of the original data frame. And the account key 1200 corresponds to rows 0 through 6. It's not usually necessary to directly examine the underlying groups like this, but sometimes, it can be useful to get a sense of what's going on. Next, I'll use the .sum function on the group by object to add up each column within each group. You can see that this calculated that the total minutes visited for the account key 1175 was 2.7. And the total for the account key 1200 was 696.6. Note that the date column is not included in this output because it's not possible to take the sum of a bunch of dates. But if there were other numerical columns in the data frame, they would be included here. Now when I add at total_minutes_visited, I get the same data back but this time, it's in a Panda series. Notice that before, this was in a data frame with only one column, which you can tell because it renders as a table in the IPython Notebook. Again, if there were more numerical columns, they would show up in this data frame, but this series will contain only the total minutes visited column. And now, since I want to take the average across all students I add .mean. And that gives me a single number, about 350 minutes for these two students. Or perhaps even better, I could use the describe function. And then I'll get a bunch of metrics including the mean, standard deviation, minimum, and maximum. Now, the piece of this code that I found most confusing when I first learned about groupby was the sum function. You've seen the sum function before but that was for data frames. This is a groupby object. So, I wondered, what other functions do groupby objects have available. What if there is no function that does what you want? Well, similarly to series and data frames, groupby objects have a lot of convenient functions built in and if the one you want isn't built in, you can use apply and write your own. In the following exercise, I'd like you to group the subway data by a variable of your choice, maybe day of the week, hour of the day, or whether it was raining. Then find the mean subway ridership for each different value, and either produce a plot showing the results, or simply print out the numbers if that makes more sense. I decided to make a plot of subway ridership by day of the week. The first thing I wanted to do was take a look at the data and remind myself where that information was stored. And I can see here that it's in the day_week column. So the first thing I'm going to do is group the subway data by the day of the week. And as before, this gives me a data frame grouped by object so I can't really see what's going on yet. Next, I want to take the mean ridership for each day of the week, so I'll add .mean. This time, I can see some results. This is showing me the mean value of each column for each day of the week. I'm specifically interested in the ENTRIESn_hourly column, so I'll look at that one. And then running this again, I see a series containing only the values for the ENTRIESn_hourly column. And I'll name this series ridership_by_day. Next I'm ready to make a plot of these values. As usual I'll use %pylab inline to make sure the plots appear within the notebook and import seaborn as sns to make my plots look a little nicer. Next, I'll call ridership_by_day.plot. And this is the plot I get. The most striking I notice is that the ridership is significantly lower on days 5 and 6 than during the rest of the week. Days 5 and 6 are Saturday and Sunday, so this makes sense. I'll use the function I wrote earlier as a starting point since that will be the code to calculate the hourly entries and exits for a single group. This time I've named the functionhourly_for_group. Now the first thing I'll need to do is group the ridership_df by unit. As usual this gives the DataFrameGroupBy object. Now it might seem logical to apply the hourly_for_group function at this point. However, if I try this I get a message that says type error. Unsupported operated types for minus. String and string. The reason this is happening is pandas is trying to apply the hourly for group function to each column of the data frame, including the unit and time columns. Since these columns contain strings, the function doesn't work on them. You can't subtract a string from a string. To fix this, I need to select just the entries and exits columns, before applying this function. Once I've done that, the function works as expected. This time, I see two rows of NaNs, one for each station. That's because for each station there was one row, where there was no previous row to compare it to. So far in this lesson, you've worked with just one data frame at a time. However, it's often the case that you'll have multiple data frames you'd like to work with. For example, in Lesson 1, you had three CSVs. So if you had been using Pandas, you would have loaded them into three data frames. For example, suppose I wanted to analyze how likely students are to cancel once they've submitted a project. That would require information from both the project submissions table and the enrolments table. I've only included a few columns from each table to make the example easier. Pandas has an operation called merge that will allow me to combine the two tables into one table. This will create a new table with all the columns from both tables. Notice that there isn't enrollment without a corresponding project submission. In this example, I'm only interested in students who didn't submit a project, so I'm not going to include that row. You'll see how to do that in a minute. So in the new table, there will be one row for each row in the submissions table. And the columns corresponding to the enrollments table will be matched up by account key. So the join date for account key 1 was 5/11. The join date for account key 2 was 5/13, and so on. Note that if there had been a duplicate entry in the enrollment table, for example two enrollments for the account key 2, then any projects submitted by the account key 2 would also be duplicated. So for the same project submission, there would be one row for the first enrollment, and one row for the second enrollment. For this reason it might be a good idea to remove duplicates from the enrolments table before performing the merge, depending on your use case. The code I would write to perform this merge is submissions.merge(enrolments, on='account_key', how='left'). The on='account_key' specifies how to match up different rows from the different tables. So, in this case they were matched by account key. The how=left determines what happens if some account keys are present in one table but not in the other for example this one. The options for this argument are inner right left and outer. And in this case how=inner would also have worked. When you use inner, only rows with an account key present in both tables are kept, so this row was thrown out. If how had been equal to right, then rows from the right hand table enrollments would have been kept, even if there was no matching entry in the submissions table. Since there's no data for this account key in the project submissions table, NaN would have been filled for those columns. How equals left does the exact opposite. If there are rows present in the left table that are not present in the right table, it keeps those rows. In this case there aren't any rows like that, so left and inner do the same thing. If you use outer, then all rows from both tables are kept and nans are filled if needed. I've found that inner is the most common kind of merge that I like to do, but it really does depend on the example. By the way, if you've used SQL, you may recognize that a merge in Pandas is very similar to a join in SQL. If you haven't used sequel though, don't worry about it. Now, for the New York dataset, the subway data, and for the weather data were originally gathered from separate sources. In the following exercise I would like you to use the merge function to combine the two files into a single data frame. This will put the data in a similar format that you've been using in the lesson up until now. Iâ€™ll start out by taking a look at the first three rows of the two data frames I want to merge. The first thing I'll need to decide is what to pass to the on keyword, that is how to match up rows from the two different data frames. It looks to me like Iâ€™ll want to match up by date and hour, latitude and longitude. And so I'll call subway_df.merge, weather_df. And then the on keyword will be a list of those four fields. I'll also need to decide whether how should be inner, outer, left, or right, and I've chosen inner, since I only want rows that are present in both tables. The result is a data frame with all the columns from both tables. For example, the entries and exits columns are from the subway data, and the fog and pressure columns are from the weather data. They've been matched up by date, hour, latitude and longitude. Now one thing that was convenient here is that the four columns I wanted to merge on have the same name in both data frames. What would I do if that weren't the case? For example, what would I do if the date column were called DATEn in the subway data and lowercase date with no n in the weather data? Because of this, Pandas allows you to specify the left_on, meaning the columns for the left table and the right on meaning the columns for the right table separately. For the left on, the columns will be the same as before. And for the right on, the columns will be the same but with the other name for the date column. And this code should give the same results as before. I decided to make a scatterplot of subway stations with latitude and longitude as the x and y axes, and ridership as the bubble size. The first thing I needed to do was group the data by latitude and longitude, and I also wanted to find the mean ridership for each location. And I'll just take a look at the first few rows to make sure that it worked. Now, this looks good, but the next thing I'd like to do is get out the latitudes and the longitudes in order to make the scatter plot. However, if I just add at latitude to the end of this code, I'll get an error. The reason is because latitude and longitude are no longer columns of the data frame. Instead, they've been transformed into the row indexes for the data frame. I can prevent this by passing as_index=False to the groupby function. And now I'm able to get out just the latitude or similarly just the longitude. I'll add the usual %pylab inline and imports next. And next I'll create the scatter plot with latitude as the x coordinates and longitude as the y coordinates. Now I'll go ahead and take a look. This worked to create a scatter plot of all the subway stations, but I haven't yet made the bubble size dependent on the ridership. I wasn't quite sure how to do this so I decided to Google for matplotlib scatterplot bubble size. The first result here about the plot marker size looks good. Looking at the example code here in the first answer, it looks like I can give the argument S to the scatter function. And that argument can be in a array, so that I can control the size of each bubble separately. So I'm going to try adding the argument s to my code and giving it the ENTRIESn_hourly column to determine the size of each bubble. Running this, it looks like it more or less worked, but a lot of the bubbles are huge, which is making the plot really hard to understand. To fix this, I'm going to create a new variable called scaled_entries. And to scale the hourly entries I'll divide each value by the standard deviation. Next I'll change the size argument to the scatter plot function to be the scaled entries rather than the raw entries. And running this code the resulting plot looks much more reasonable. Now, the bubbles are maybe a little small now, so I could play around with the scaling factor a bit. But already I can see that there are some areas of New York that tend to have higher ridership. Say down here and maybe also up here a little bit. Congratulations, you've finished the course. Hopefully, you've made some interesting discoveries about the New York subway data. At this point, you know how to go through the data analysis process from start to finish. And you can use NumPy's and Pandas to write data analysis code that's clean, concise, and runs quickly. Great job. I hope you've enjoyed taking this course as much as I enjoyed creating it. I want to hear your thoughts. Please write a forum post sharing what were your favorite parts of the course and what suggestions do you have for how to improve it. And by the way for some extra fun check out the pandas links in the instructor notes.