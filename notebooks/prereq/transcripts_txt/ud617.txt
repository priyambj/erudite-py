Hi, welcome to the introduction to Hadoop and MapReduce. I'm Sarah Sproehnle. I'm the vice president of educational services at Cloudera, the company that helps develop, support, and manage Apache Hadoop. And I'm Ian Wrigley, I'm Cloudera's senior curriculum manager. Between us, Sarah and I have been responsible for bringing Hadoop training to well over 20,000 people. And we're excited to bring it to a much larger audience with Udacity. So during this course, we're going to talk about what Big Data is? What Hadoop is? Why Hadoop's useful? And we're going to teach you how to write map reduce code. In fact, by the end of the course, you'll write a MapReduce program that efficiently processes a very large web server log. And by the end of lesson two, you'll already be running a MapReduce job in Apache Hadoop. So let's start. In this lesson we'll talk about what is Big Data. We'll talk about the problems that it creates, and then how Apache Hadoop addresses those problems. Organisations have been generating data since way back But as time goes on, more & more data is being generated. IBM estimates that 90% of world's data was created in the last two years alone. This is a simple example. Think of your cellphone. Whenever your cellphone is turned on, it's connected to the cell towers. As you move around, it'll connect to different towers in a different signal streaks. All of that connection data is collected by the phone company & it's being logged. They can use that information to find dead spots in the coverage & know which towers are busiest & need increased capacity. They can even trace you if you make an emergency phone call but don't get your exact location. This is an enormous amount of data we have. Another example as when you visit a website like Amazon or Netflix, everything, you do there is logged: what pages you view, how long you spend there, where you coming from. They can even capture things like what browser you are using. Again this is a huge amount of data. Phone data & website logs are just examples. In addition, things like X-rays are creating huge amounts of data. & people doing research to detect similarities in tumors. The increase in amount of data we're generating opens up huge possibilities. But it comes with problems too. Where do we've to store all this data? & process it too? But not everything is a big data problem. There are lots of cases where you can use a traditional system to store, manage and process your data. So the first thing you need to decide, is do you really have big data? To make that call, we need some sort of definition of what big data means. Which of these would you consider to be big data? You won't be graded on this. Just give it your best guess. Would you consider the order details for purchases at a single store to be big data. Or all orders across hundreds of stores nationwide. Say, an individual's stock portfolio. Or all stock transactions for the New York Stock Exchange for a year. Most people would consider all orders across hundreds of stores, and all stock transactions for the New York Stock Exchange to be big data. You can imagine that the order details for a single store or one person's individual stock portfolio, could be stored in a traditional relational database system. Maybe even a spreadsheet. But those traditional systems start to struggle with all the orders across the hundreds of stores, and the entire data from the New York Stock Exchange. There's not one definition for big data, its a very subjective term. Most people would certainly consider a data set of several terabytes to be big data. But there are certainly plenty of people using Hadoop on significantly smaller data sets with really great results. A reasonable definition of big data might be, it's data that's too big to be processed on a single machine. The challenges with big data are not just about its size. What additional problems can you see with data? Could it be that most data is inherently worthless and it's hard to find the useful bits? Could the fact that data is created fast be a challenge? Could there be difficulty because data's coming from different sources in various formats? A potential challenge with big data is that it is created very fast. And does come from different sources, which could be in a variety of formats. In my experience, most data is not worthless. But actually does have a lot of value. When you read or talk about big data, you often hear people say the three Vs. Volume refers to the size of the data you're dealing with. Variety refers to the fact that the data is often coming from a lot of different sources and different formats. And velocity refers to the speed at which it's being generated, and the speed at which it needs to be made available for processing. So let's look in more detail at each of these. Let's start with volume. The price to store data has dropped incredibly over the last 60 years. In 1980, the cost per gigabyte was several hundred thousand dollars. In 2013, it's merely $0.10. Although it's worth saying, to actually store the data reliably. You're going to end up paying more than that. That's particularly the case with more traditional storage devices such as storage area networks, or SANS, which can be extremely expensive. The high cost of reliable storage puts a cap on the amount of data companies can practically store. At some point they say, okay, it's too expensive to store all that data. And I'm not doing anything with it. Let's just store the critical stuff, like my actual sales. But it turns out, as you'll see, that the data you're currently throwing away can be incredibly useful. What we need is a cheaper way to store it reliably. And of course storing the data is only part of the equation. You also need to be able read and process it efficiently. Storing a terabyte of data on a SAN, is not that hard. But streaming the data from the SAN across the network to say, some central processor, can take a long time and processing can be extremely slow. Which of the following data sets do you think is worth storing and analyzing? For example, transactions, like financial or government related, logs, business data such as product catalogs, prices, customers, user data, documents, videos, pictures. Sensor information about the weather, pollution, earthquakes. Medical like X-rays or brain scans. Social like email, Twitter, et cetera. And the answer of course is all of these provide useful information. But in order to store them, you'll need a way to scale your storage capacity up to massive volume. Hadoop, which stores data in a distributed way across multiple machines does that. You'll see how in the next lesson. The second v is data variety. For a long time, people have used databases such as SQL Server, or MySQL, or big data warehouses from companies like Oracle and IBM to store their data. The problem is that to store data in systems like that, the data needs to be able to fit in pre-defined tables. And a lot of the data that we deal with these days, tends to be what we call unstructured or semi-structured data. So Sarah can give us some examples of that. By unstructured we mean data arrives in lots of different formats. For example, a bank might have list of your credit card and account transactions. They may also have scans of checks, records with customer service interactions. Maybe even recordings of those phone calls. All that data in a variety of different formats can be hard to store and reconcile in a traditional system. And this brings us back to volume. You want to store that data in its original format so you're not throwing any information away. That way you can then process the data later in different ways. For instance, if we transcribe a call center conversation into text we have what people said to customer service representatives. But if we had the actual recording as well then later we might develop software which can interpret the tone of voice the customer uses. And that might lead to a very different interpretation of the data. And the nice things about Hadoop is that it doesn't care what format your data comes in. Unlike a traditional database, you can store the data in its raw format and manipulate it and reformat it later. Sometimes the most unlikely data can be the most useful and lead to savings, due to better planning. For example, a conventional system for coordinating logistics might send the closest truck to a factory to pick up a package. However, it might be that the closest truck is not the best solution. Perhaps there's a traffic jam, or the closest route is actually on small roads that take a long time. Maybe the truck doesn't have enough free space for the new load. So what kind of data would be helpful making a better plan that could save money and time for the company. Would current GPS data for all trucks be useful? Or how about the current plan for each truck for the day? Would it be helpful if we had real time information about the traffic in the area? How about the current load in each truck and its volume and weight. May be the fuel efficiency of our different vehicles. And again, all these answers are correct. You can save a lot of money and time by making better decisions driven by more data. The world we live in is extremely complex. And there's lot of variables to consider that we can tweak to get large benefits. Velocity, the third v, is about the speed at which the data arrives, ready to be processed. We need to be able to accept and store that data, even when it's coming in at a rate of terabytes per day. If we can't store it as it arrives, we'll end up discarding some of it, and that's what we want to avoid. Think about an ecommerce website. If we knew what products you've looked at in the past, we can recommend similar products the next time you visit our site. If you've had spent five minutes looking at a particular item, we could maybe send you an email informing you the product is on sale. If we know that you typically browse our site using a first generation iPad, we could suggest the latest model. This is a huge difference to what we could do before, when we only stored the records of the actual purchases. If we can store and process all of our web server logs, along with the purchase data that's in our traditional data warehouse. We could give the customer a much better shopping experience. Which would directly translate into bigger profits. Yet another example is a movie site like Netflix. Based on what they know about your viewing habits, they can recommend movies to you. As you can see here, because of what Ian's rated highly before, this movie is recommend to him. And they can even predict what rating he would give this movie. What kind of data interests you? This is a survey question. There's no right or wrong answer. We're just curious about what kind or problems you want to solve by using data. Are you interested in processing scientific data? E-commerce or medical? Maybe social networks. Financial? Sporting information? Utilities like oil and gas? Or something else? So there are plenty of things we can do with big data, but first we need to be able to store the data in a cost effective way, and then process it efficiently. And it turns out, that's not easy to do, when we're talking about massive amounts of data. Luckily, some smart people at Google were working on this in the late 90s. They published some research papers, in 2003 and 2004. To learn more about that, let's talk to Doug Cutting, one of the founders of Hadoop, and the Chief Architect at Cloudera. So, let me tell you how Hadoop came to be. About ten years ago in around 2003, I was working on an Open Source web search engine called Nutch, and we knew it needed to be something very scalable, because the Web was you know, billions of pages. terabytes, petabytes, of data, that we needed to be able to process, and we set about, you know, doing the best job we could and it was tough. We got things up and running on four or five machines not very well, and around that time Google published some papers. About how they were doing things internally. Published a paper about their distributed file system, TFS. and about their processing, framework, MapReduce. So my partner and I, at the time, in this project, Mike Cafarella. Said about trying to reimplement these in Open Source. So that more people could use them than just folks at Google. Took us a couple of years, and we had Nutch up and running on, instead of four or five machines, on, 20 to 40 machines. It wasn't perfect, it was it wasn't, wasn't totally reliable, but it worked. And we realize that to get it to the point where it was scaled to thousands of machines, and be as bullet proof as it needed to be, would take more than just the two of us, working part time. Around that time, Yahoo approached me and said they were interested in investing in this. So I went to work for Yahoo in January of 2006. First thing I did there, was, we took the parts of Nutch that were a distributed computing platform, and put em into a separate project. A new project christened Hadoop. Over the next couple years, with, Yahoo's help, and the help of others. we took Hadoop, and really got it to the point where it did scale to petabytes, and running on thousands of processors. And doing so quite reliably. it spread to lots of companies. and mostly in the Internet sector, and became quite a success. after that, we, we started to see a bunch of other projects grow up around it. And Hadoop's grown to be the kernel of a, which, pretty much an operating system for big data. we've got, we've got tools that, allow you to, more easily do, MapReduce programming. so, you can develop using SQL or a data flow language called Pig. and we've also got the beginnings of higher-level tools. We've got interactive SQL with Impala. We've got Search. and so we're really seeing this develop to being a general purpose platform for data processing. that scale's much better and that it is much more flexible than anything that's, that's, else is out there. That's the story of the genesis of Hadoop. It's based on work done by folks at Google. And, it's now grown to the point, where there are hundreds of people contributing to the project. And, it's being used by thousands and thousands of companies worldwide. The Hadoop logo is a little yellow elephant. But, do you know where that logo came from? It's actually a funny story attached to that, and here's Doug to explain. So the name Hadoop comes from my son's toy elephant. When he was about two, a friend gave him a little stuffed elephant which he played with incessantly. and we overheard him calling it something, this strange word that he invented. and said Hadoop. So I, I immediately wrote it down because I was in the, the software business. And we're always looking for good names. And this one came with a, a mascot, even. and a few years later when I needed a project name, pulled it out. Now I, I wrote it down as H A D O O P. And figured that everyone would say Hadoop. Now it turns out everyone says Hadoop instead, but I persist in saying Hadoop. Now my son, of course, is 13, and expects royalties for the name. He he wants more credit. He also accuses me of stealing the toy. At some point, he was using it in some kind of rocket ship experiment, and I had to rescue it. And now it, it lives in my sock drawer for, for safety. The core Hadoop project consists of a way to store data, known as the Hadoop distributed file system, or HDFS. And a way to process data with MapReduce. The key concept is that we split the data up and store it across the collection of machines known as a cluster. Then when we want to process the data, we process it where it's actually stored. Rather than retrieving the data from a central server, the data's already on the cluster, so we can process it in place. You can add more machines to the cluster as the amount of data you're storing grows. And, indeed, many people start with just a few machines, and add more as they're needed. The machines in your cluster don't need to be anything particularly high end. Although most clusters are built using rack-mounted servers, they are typically mid-range servers, rather than top of the range equipment. We've been talking about core Hadoop. Which consists of HDFS and map reduce but since the project was first started, an awful lot of other software has grown up around it. And that's what we call the Hadoop Ecosystem. Some of the software is intended to make it easier to load data into the Hadoop cluster. Well lots of it designed to make Hadoop easier to use. For example as you'll see in the next lesson. Writing Map reduced code isn't completely simple. You need to know a programming language such as Java, Python, Ruby or Perl. But there are lots of folks out there who aren't programmers, but can write SQL queries to access data in a traditional relational database system, like Sequel Server. And, of course, lots of business intelligence tools one way to hook into Hadoop. For that reason, Other open source projects have been created to make it easier for people to query their data without knowing how to code. Two key ones are Hive and Pig. Instead of having to write macros and reducers, in Hive you just write statements like this. Which looks very much like standard SQL. The Hive interpreter turns the SQL into map produced code, which then runs on the cluster. And an alternative is Pig, which allows you to write code to analyse your data in a fairly simple scripting language, rather than map reduce. Again the code is just turned into map reduce and run on a cluster. Hive and Pig are great, but they're still running map reduce jobs. Which as you'll see can take a reasonable around of time to run. Especially over large amounts of data. So another open source project, is called Impala. Impala was developed as a way to query your data with SQL, but which directly accesses the data in HDFS. Rather than needing map reduce. Impala is optimized for low latency queries. In other words Impala queries run very quickly, typically many times faster than Hive, while Hive is optimized for running long batch processing jobs. Another project used by many people is Sqoop. Sqoop takes data from a traditional relational database, such as Microsoft SQL Server. And, puts it in HDFS, as the limited files. So, it can be processed along with other data on the cluster. Then, there's Flume. Which injests data as it's generated by external systems. And, again, puts it into the cluster. HBase is a real time database, built on top of HDFS. And there's more. Hue is a graphical front end to the questor. Oozie is a workflow management tool. Mahout is a machine learning library. In fact there are so many ecosystem projects that making them all talk to one another, and work well, can be tricky. To make installing and maintaining a cluster like this easier, Cloudera, the company we work for, has put together a distribution of HADOOP called CDH. CDH or the Cloudera distribution including a patchy HADOOP, takes all the key ecosystem projects, along with HADOOP itself, and packages them together so that installation is a really easy process. And the components are all tested together, so you can be sure there's no incompatibilities between them. Of course, it's free and open source, just like Hadoop itself. While you could install everything from scratch, it's far easier to use CDH, and that's certainly what we'd recommend. In the next lesson, in fact, you'll be downloading and running a virtual machine, which has CDH installed. For more information on the Hadoop ecosystem and how each of these components works, see the instructor notes. Congratulations on completing the first lesson, where you learned what big data is, and how Hadoop solves big data problems. In the next lesson, you'll go deeper in the two main components of Hadoop: HDFS, the Hadoop distributed file system, and map reduce, which is how Hadoop processes data. Files are stored in something called the Hadoop Distributed File System. Which everyone just refers to as HDFS. As a developer, this looks very much like a regular file system. The kind you're used to working with on a standard machine. But it's helpful to understand what's going on behind the scenes. So that's what we're going to talk about here. Imagine we're going to store a file called mydata.txt. In HDFS. This file is 150 megabytes. When a file is loaded into HDFS, it's split into chunks which we call blocks. Each block is pretty big. The default is 64 megabytes. Each block is given a unique name, which is BLK, an underscore, and a large number. We'll call ours block one, two, and three. And in our case the first block is 64 megabytes. The second block is 64 megabytes. The third block is the remaining 22 megabytes, to make up our 150 megabyte file. As the file is uploaded to HDFS, each block will get stored on one node in the cluster. There's a Damon, or piece of software, running on each of the machines in the cluster, called the DataNode. Now clearly we need to know which blocks make up the original file. And that's handled by a separate machine, running the Damon called the NameNode. The information stored on the NameNode is known as the Metadata. That's fine as far as it goes, but there are some problems with this. Take a look at the diagram and see if you can spot where we might run into trouble. For example, let's say there was network failure between the nodes, or a disk failure on a DataNode. Do you think it's an issue that not all of the DataNodes are used to store data? Or could it be a problem that that the block sizes differ? Two of them are 64 megabytes, but one was only 22 megabytes. Finally, do you think it'd be a problem if we had a disk failure on the NameNode? It definitely is a problem if there's a network failure between the nodes and they can't talk to one another. It's also a problem if we had a disk failure, on an individual data node. It's not a problem that some nodes are unused, they'll be used by data later. It's also not a problem that some of the block sizes are different. It is a problem if we were to lose the disk on the name node and lose our metadata. The problem with things right now, is that if one of our nodes fails, we're left with missing data for the file. If the node goes away for example, we've got a 64 megabyte hole in the middle of my data.txt, and of course similar problems with any other files that happen to be stored on that node. To solve this problem, Hadoop replicates each block three times, as it's stored in HDFS. So block one doesn't just live here, it may also be here and here. Block two is here, here and here, and block three is here, here, and here. Hadoop just picks three nodes at random and puts one copy of the block on each of the three. Well actually, it's not totally random, but that's close enough for us right now. So now if a single node fails, it's not a problem because we have two other copies of the block on other nodes. And the NameNode is smart enough to see that these blocks are now under-replicated and it will arrange to have those block re-replicated on the cluster. So we're back to having three copies of them. So we've taken care of the problem, if one of our data node fails. But there's another obvious single point of failure here. What happens if the NameNode has a hardware problem? Might the data be inaccessible? Or is the data in HDFS lost forever? Or is everything fine and there's no problem? It depends on the kind of failure. If there's a network failure to the name node, then the data would be temporarily inaccessible. If the disk on the single name node were to fail, the data in HDFS could be lost forever. For a long time, the Name Node was a single point of failure in Hadoop. If it died, the entire cluster was inaccessible. If the meta data on the Name Node was lost completely, the entire cluster's data was lost. Sure, you've still got all the blocks on the data nodes but you've no way of knowing which block belongs to which file whithout the meta data. So, to avoid the problem, people would configure the Name Node to store meta data, not only on it's own hard drive but also somewhere on a network file system. NFS is a method of mounting a remote disk. That way, even if the Name Node bursts into flames, there would be a copy of the metadata elsewhere on the network. These days, there's an even better alternative. The Name Node is not a single point of failure in most production clusters, because they've configured two Name Nodes. The active Name Node works as before, but the standby can be configured to take over if the active one fails. That way the cluster will keep running if any of the nodes, even one of the Name Nodes, fails. Ian's now going to show us a demonstration of how to use HDFS. So, here I have a directory on my local machine, which contains a couple of files, and I want to put one of them into hdfs. All of the commands which interact with the Hadoop file system start with Hadoop FS. So first of all, let's see what we have in hdfs to start with. I do that by saying hadoop fs minus ls. That gives me a listing of what's in my home directory on the Hadoop cluster. Because I'm logged in to the local machine as a user called training, my home directory in hdfs is /user/training. And as you can see, there's nothing there. So now, let's upload our purchases.txt file. We do that with hadoop fs minus put purchases.txt. Hadoop fs minus put takes a local file and places it into hdfs. Since I'm not specifying a destination filename, it'll be uploaded with the same filename. So, it takes a few seconds to upload. And now I can do another hadoop fs minus ls, and we can see that that file is now in hdfs. I can take a look at the last few lines of the file by saying, hadoop fs minus tail, and then the filename, and that just displays the last few lines on the screen for me. There's also a hadoop fs minus cat, which will display the entire contents of the file and we'll use that later. There are plenty of other hadoop fs commands and as you'll probably have started to realize, they closely mirror standard UNIX file system commands. So, if I want to rename the file, for example, I can say hadoop fs minus mv, which moves purchases.txt, in this case, to newname.txt. If I want to delete a file, hadoop fs minus rm will remove that file for me. So, let's get rid of newname.txt from hdfs. I create a directory in hdfs by saying hadoop fs minus mkdir and then the directory name, and now let's upload purchases.txt and place it in the myinput directory so that it's ready for processing by hdfs. Once I've done that, hadoop fs minus ls myinput will show me the contents of that directory. And just as I expected, there's the file. Thanks, Ian. Okay, now that we've seen how data is stored in HDFS, let's discuss how that data is processed with MapReduce. Say I had a large file. Processing that serially from the top to the bottom could take a long time. Instead, MapReduce is designed to process data in parallel, so your file is broken into chunks, and then processed in parallel. To explain, let's look at a real world scenario. Let's imagine we run a retailer with thousands of stores around the world. And we have a big ledger that contains all of our sales, the big book of sales for the year 2012. We've been asked to calculate things like the total sales per store for the year. Let's say the lines of the ledger look like this. The date of the purchase, the location of the store, what they bought, and the amount. One way we could do this is just start at the beginning of the ledger and for every entry we see, write down the store location and the amount. Then keep going. For the fourth entry, we see that we've already written down Miami. So we'll update the amount to 62.15, and we would continue. Historically, we'd probably use an associative array or a hash table to solve this problem in a traditional computing environment. The location would be the key and the sales for that store, the value. Then we'd process the input file one line at a time, adding each store as a key. What problems do you see with this approach? Say if you were running it on one terabyte of data. Do you think that it flat out won't work? Or could you run out of memory having to store this hash table. Could it take an excessively long time? Or will we not get the right answer? Since we have millions and millions of sales to process, it's going to take a very long time for our computer to first have to read the data off-disk and then process it line by line. Also, the more stores we have, the longer it takes to check if we've seen that store, add it to the hash table, and add the running value to the total. We may even run out of memory holding the hash table. However, it probably would work. And you would get the right result eventually. Let's see how we could do this more efficiently with a MapReduce job Rather than one person reading the entire ledger, say we had more people to help, like the staff of your accounting department. We'll split them into two groups, called the mappers and the reducers. Then we'll take the ledger, break it into chunks, give each chunk to one of the mappers. That way all the mappers can work at the same time, each over a small fraction of the data. Let's see what a mapper would do. They will take the first record in their ledger, and on an index card write the name of the store, and the amount of a sale. Then they'll take the next record, and repeat. As they're writing the index cards they'll pile them up so that cards for the same store go in the same pile. By the end, each mapper will have a pile of card per store. Once the mappers have finished, the reducers can collect their sets of cards. We'll tell each reducer which store they're responsible for. Such as this one for New York City, and this one for Miami and Los Angeles. The reducers go to the mappers and retrieve the pile of cards for their stores. It's fast for them to do because the mappers have already separated the cards into a pile per store. Once the reducers have retrieved all their data, they collect all the small piles and create a larger pile. Then they start going through the piles one at a time. All they have to do at this point is add up all the amounts from all the cards in the pile. That gives them a total sales for that store. They can then write that amount on their final sheet. To keep things organized each reducer goes through his or her set of piles in alphabetical order. In other words, Los Angeles before Miami. And that's how Map Reduce works. Lets summarize. The Mappers are just little programs that each deal with a relatively small amount of data, and work in parallel. We call that output the Intermediate Records. That's what we were writing on our index cards. Hadoop deals with all data in the form of key and value. So these records are actually keys and values. In our example, the key was the store name and the value was the sale total for each particular piece of input. Once the Mappers have finished, a phase of Map Reduce called the Shuffle and Sort takes place. The Shuffle is the movement of the intermediate records from the Mappers to the Reducers. The Sort is the fact that the Reducers will organize these sets of records, in our case these piles of index cards, into sorted order. Finally, each Reducer works on one set of records at a time, or one pile of cards. It gets the key and then a list of all the values. In our case, a store like New York City. And in all the sales for that store, it processes those values in some way. In our case, we were adding up the sales. Then it writes out the final results. But let's say you wanted your final result to be sorted. How could we get our final results in sorted order? Do you think it's impossible? Or what if we had only one reducer? Or if after the job, we did an extra step to merge the result files? Well, it is possible to get your results in a final sorted order. Either by having one reducer. Though that doesn't scale very well. Or by an extra step that merges the results. So when we have multiple reducers, say these two, and the mapper might output keys apple, banana, carrot and grape. Which keys, will go to the first reducer? Here are some options. Maybe apple and banana go to the first reducer, or apple and carrot, carrot and grape. Or apple and grape, or, we don't know specifically, but they'll be split evenly with each reducer getting two of the keys. Or we don't know, and it's possible that one reducer could get none of the keys. Since there is no guarantee that each reducer will get the same number of keys. It might be that one will get none. We just don't know. For more on how this works, please see the instructor notes, and learn more about partitioners. So now we've seen conceptually how MapReduce works. In the next lesson, we'll talk about how to write the code to perform these MapReduce jobs on your cluster. But before that, let's go back to the cluster. Just as we was with HDFS, there are a set of daemons, which is just a piece of code, running all the time, running on each of these machines. There were the data nodes and the name node. When you run a MapReduce job, you submit the job to what's called the Job Tracker. That splits the work into mappers and reducers. Those mappers and reducers will run on the other cluster nodes. Running the actual map and reduce tasks is handled by a daemon called the TaskTracker. The TaskTracker software will run on each of these nodes. Note that since the TaskTracker runs on the same machine as the data nodes, the Hadoop framework will be able to have the map tasks work directly on the pieces of data that are stored on that machine. This will save a lot of network traffic. As we saw, each Mapper processes a portion of the input data. That's known as the input split. And by default, Hadoop use an HDFS block as the input split for each Mapper. It will try to make sure that a Mapper works on data on the same machine. If this green black, for example, needs processing, then the TaskTracker on this machine will likely be the one chosen to process that block. That won't always be possible, because the TaskTrackers on these three machines that have the green block could already be busy. In which case, a different node will be chosen to process the green block, and it will be streamed over the network. This actually happens rather rarely. So the Mappers will read their input data. They'll produce intermediate data, which the Hadoop framework will pass to the reducers, remember that's the shuffle and sort. Then the reducers process that data and write their final output back to HDFS. So let's have Ian run a job on our cluster. It's often the case that MapReduce code is written in Java. However, to make things a little easier for us, we've actually written our mapper and reducer in Python instead. And we can do that thanks to a feature called Hadoop streaming, which allows you to write your code in pretty much any language you'd like. So first of all, let's double-check that we have our input data in HDFS. So, if I Hadoop fs minus ls, then there's my input directory. And if I look at that directory, then yes, there's purchases.txt in there. And in my local directory, I have mapper.py and reducer.py, that's the code for the mapper and reducer, written in Python. We'll look at the actual code in the next lesson. Okay, to submit the job we have to give this rather cumbersome command. We say hadoop jar, a path to a jar, then I specify the mapper, I specify the reducer, I need to say -file, for both the mapper and the reducer code. I specify the input directory in HDFS and I specify the output directory to which the reducers will write their output data. And we're calling that joboutput. I hit Enter and off we go. Hadoop's pretty verbose, as you can see. As the job runs, you'll see a bunch of output which shows us how far along the job is. It turns out that for this job Hadoop will be running four mappers. And our virtual machine here can only run two at a time. So the job is going to take longer than it would on a larger cluster. Actually, that's worth mentioning here. With the size of the data we have for this example which is only 200 megs, realistically, we could probably have solved this problem faster by just importing the data into a relational database and querying it from there. And that's often the case when we're developing and testing code. Because the test data sets are pretty small, Hadoop isn't necessarily the optimal tool for the job. But when we're done testing and we need to process our full production data, that's when Hadoop really comes into its own. So, as you can see the job is now nearly complete, and when the job has finished we'll get a [UNKNOWN] output. And we'll see that the last line tells me that the output directory is called joboutput. Let's take a look at what we've got in there. Hadoop fs minus ls, shows me that yes I do have a job output directory. And if we look at the job output directory, you'll see that it contains three things. It contains a file called _SUCCESS, which just tells me that the job has successfully completed. It contains a directory called _logs, which contains some log information about what happened during the job's run. And then, it contains a file called part-00000. That file is the output from the one reducer that we had for this job. Let's take a look at that by saying hadoop fs minus cat part 00000, and we'll pipe that to less on our local machine. That's the contents of that file, which is the output from our reducer. It's the sum total sales broken down by store exactly as we want it. Incidentally, if you want to retrieve data from HFDS and put it onto your local disk, you can do that with Hadoop fs minus get. Hadoop fs minus get is the opposite of Hadoop fs minus put. It just pulls data from HDFS and puts it on the local disk. So as you can see, now I have my local file.txt on my local disk And I can manipulate that however I'd like. That Hadoop job command we typed was pretty painful to have to remember. So to save you time, we've created an alias in the demo virtual machine that you'll be downloading. You can just type hs followed by four arguments, the mapper script, the reducer script, the input directory, and the output directory. Here's one important thing to note, though. When you're running a Hadoop job, the output directory must not already exist. And as you can see, if we try and run the command with an existing directory. In this case, job output. Hadoop refuses to run the job. This is actually a feature of Hadoop. It's designed to stop you inadvertently deleting or overwriting data that's already in the cluster. But as you can see, if we specify a different directory, which doesn't already exist, then the job will begin just fine. The example we just talked about was calculating the total sales per store. And there are lots of other things we can do with MapReduce that actually quite similar to that, for example, log processing. Say we had a set of logs from a web server which looked like this. You may want to know how many times each page has been hit. Well really, it's similar to calculating the sales per store. Your mapper will read a line of the log at a time. And it will extract the name of the page. Such as, index.php. It's intermedia data. We'll have the name of the page as the key. The intermediate data we use index.php as the key and 1 as the value. This is saying that that page was accessed one time. Then we'll continue into the other lines of the file just like that. When all the mappers are done the reducers will get the keys, which are just all the pages on our website, and a list of values which will be a list of ones, meaning how many times that page was accessed. The reducers can add up the ones to know how many total hits that page had on our web site. Simple, but far more efficient, than writing a standalone program to go through all the logs start to finish. Especially, if we could have hundreds of gigabytes or more. And that's just the beginning of what you can do with MapReduce. Recommendation systems, fraud detection, item classification are all great problems for MapReduce. These all have some basic characteristics in common. They have a lot of data, and the work can be parallelized rather than slogging through in serial. Perhaps one of the more difficult things to learn when you're new to Hadoop. Is how to think about solving problems in terms of MapReduce. It can be very different than how you're used to working. And frankly, it takes a lot of practice. So in the next lesson, we're going to write the code for the sales by store problem, and we'll also start talking about other MapReduce problems. We've provided a virtual machine, with CDH, that's Clouderas Distribution for Hadoop, pre-installed. We say that Hadoop is running in pseudo-distributed mode, that means it's a full cluster just running on one machine. It also includes sample data sets and sample solutions the problems we ask you to solve. Now would be a great time to see the instructor notes on how to download the virtual machine. You'll open it up, start the VM, and then follow the step by step instructions, which show you how to upload a file to HDFS and run a MapReduce job. So that's the end of the lesson. You learned how Hadoop uses HDFS to store data and MapReduce to process it. In the next lesson we'll walk through the code for a MapReduce program, and you'll be ready to write your own MapReduce programs to analyze data. In this lesson we'll walk through the code for calculating the total sales per store. Our data may have come from a database, and we could have used Scoop to import it into HDFS. So, let's take a closer look at our input data. Remember that each mapper processes a portion of the input data. And, each one will be given a line at a time. The lines look like this. The mapper needs to take that line, and extract the information it needs. Often, when we're dealing with text, it's pretty free-form. So we'd use something like a regular expression. But in this case, it's regular, it's tab limited. So we can split the line on tab and extract the values. In this example, they're date, time, store name, product type, cost, and method of payment. Let's say you've been asked to find the total sales per store. How would you choose an intermediate key and value? Could it be the time and the store name? Or the cost of the item and the store name? Or rather the store name as a key and the cost? Or the store name and the product type. Remember that our line looks like this. The correct answer is store name and cost. We want all of the data grouped by store name. So, that we can add up the total sales, by store. Here's our mapper code. Let's look at it line by line. We're going to loop around standard input, which gives us a line at a time. Of course, the line will have a new line character so let's strip that off, plus any other white space around the line. Since our line is tab delimited, let's split untab at the same time. This gives us an array, called data. So now we'll take the array, data, and assign it into individual variables for date, time, store, item, cost, and payment. Lastly, we'll print the items we care about, which is store and cost. As you use Hadoop more and more, you'll discover that the more data you have, the more likely you are to encounter weirdness in the data. Lines could be malformed. There could be strange log messages in the data, and many other cases that you might not even imagine. So we should make sure that no matter what kind of data we get, the mapper can continue working. You don't want to be halfway through a two terabyte job when it dies. So let's go back and add some defensive programming, to make sure things don't break, even if you get a strange line in the middle of your data. In this case, we're checking to see that the line has six fields. If it doesn't, we'll ignore that line. Another good thing to do would be to check that the cost is actually a number. Assuming that line is okay, we'll simply write out our intermediate data in the form of a key, then a tab, then a value. And we'll loop back around and do the next line. Now that our mapper is done, what happens between the mapper phase and the reducer phase? Is it called a bubble sort? Or shuffle and sort? Is it find and sort? Or is it a quicksort? The next phase is the reducer. In our case we only have a single reducer because that's the HADOOP default. It will get all the keys. The reducer is going to get all the data coming in something like this. If we had specified more than one reducer, each would receive some of the keys along with the values for those particular keys. We're using HADOOP streaming here because we want to write our code in PYTHON. Hadoop streaming allows you to write your mappers and reducers in any language, rather than forcing you to use Java. But the way reducers get their data is a little bit tricky. As you can see, the data comes in streamed as lines of text. Each line contains a store name and the cost. The store names are sorted, which we're guaranteed because of the shuffle and sort. So we know that all lines for, say, Miami, are together. So if we receive our data like this, what variables might we need to keep track of to calculate the total sales per store? Do we need to know the cost of the previous line? Do we need to store the current cost? Or how about the total sales per store? Do we need to save the previous store, or the store that we're currently working with? We need to know current cost, so we'll read that in. We're also keeping track of the total sales. That will require a variable. We need to store both the current store and the previous store we've seen, so that we can tell when we're changing keys. In other words, when we go from the Miami Line to the New York City Line. However, the cost of the previous line is not useful. So here's the reducer code. Let's step through it. We'll start by setting up a couple of variables. Sales total, which we'll keep the running total, is initialized to 0. Because we haven't read any keys yet, old key starts at none. Then we start reading from standard input. As with the mappers we receive a line at a time that's tabbed eliminated. In this case we expect a store name, a tab, and one of the sales. So we'll strip out the new line, separate the value and the key by tab. That should give us two items which we'll store in an array called data. If we don't have two items, something strange has happened so we'll skip that line, but that should never be the case. Because we know that mappers send us the data in this format. For the sake of clarity, we'll store the two items in the array into variables. ThisKey will get the store name, thisSale, the value. So here is the tricky part. We want to know if the key has changed since the last row we read. So we'll check if oldKey is even set, because if it's not, then this is the first line we're reading. And if it is, we'll see if it's different to the key we just read in. If that's true, then the key has just changed. In our example data, we had just switched from the Miami row to the New York City row. So now we need to write out the result. Which'll be key, which is the store name, a tab and the sales total for that store. Then we reset sales total back to 0. Now for every row that we process, we'll set oldKey to the key we are working on, then add the current sale to the running total. And then we'll loop back to the next row. Eventually, we'll run out of data, which will take us out of the loop. Do you think we're done? At the end of this code, we are not done, because we have not processed the last key. Be careful when you exit the loop. We haven't yet output the data for the last key. That's what these two last lines are for. We write out the last key and value. If we didn't have these two, we would not have the result for the final store. Now let's have Ian talk about testing the code and then actually running it on a cluster. So, Sarah's just shown you the code. And one of the nice things about using hadoop streaming is that it's really easy to test your code outside of hadoop. So, let's take a look at how to do that. Our mapper takes input from standard input. So, in order to test it, we can just run it from the command line and type data in to test it. Here, I'm just typing, as standard input, a couple of lines of sample data. Six fields, separated by tabs. And then when I hit Ctrl+D, to simulate the end of input, you can see that the mapper outputs the results, just as I'd expect. Even better, we can just build a small sample data file, and pipe that into the mapper. So let's do that. I'm going to just take the first 50 lines of purchases.txt and save those into a test file, which I'll call testfile. Now, I can just pipe that to the mapper by saying cat testfile, pipe that to mapper.py. And that gives mapper that data a standard input, and as you can see, the mapper produces its output again to the command line. If we have problems, we could just go back and edit the mapper until it works. It's really nice and fast to be able to do this without having to run a complete hadoop job every time during the development phase. We can do a similar thing with the reducer. It's expecting a set of lines, each of which looks like the store name then a tab then the value. So, again, we can create a sample file which looks like that and pass it in to the reducer. But even nicer, we can test the entire pipeline. Remember that the mapper's output is sorted by the hadoop framework, and then passed to the reducer. So, we can simulate the entire thing on the Unix command line, like this. I cat testfile. I pipe it to the mapper. I then pass that to the Unix sort command, and pass that output to the reducer. When I run that, that simulates the entire map followed by shuffle and sort, followed by reduced phase, and as you can see, I've got the output from the reducer. So, now that we've tested this on the command line we can test it on the cluster. The best practice when you're developing map reduce jobs is first to test locally with a small data set before you run your code on the entire, huge, set of data. So, now that we've tested on the command line, we can test our code on the cluster. Best practice, when you're developing that reduced jobs, is to first test with a small data set. Before you run your code on your entire huge set of data. But, we're already, pretty, confident here. So, let's just run the thing on the wholepurchases.txt file. We'll use the HS alias to save some typing. I specified my mapper, my reducer, my input directory, and a new output directory which we'll call output two. And off hadoop goes. It starts running the job. It turns out that we can see what's going on on the cluster by taking a look at the hadoop job tracker web user interface. So, you point your web browser at the job tracker, which on our machine is just local host on port 50030. And here you can see that this just one running job. If we click on the job name, then that takes us to a page, which shows us the progress of the job, and a lot of other interesting information as well. We can drill down and look at the individual map, or reduce tasks just by clicking on the words map or reduce. And here we can see that two tasks have completed, two tasks are still running. If I click on one of the completed tasks, I can even go as far to look at the logs from that task. So, here's our job. We're now 25% of the way through our reducers. As you can see, we get graphs at the bottom of this page to show us exactly what's happening with the job. So, we nearly finish with the job, when the job's finished, hadoop fs minus ls of output 2 shows me the just I'd expect is a part dash 00000 file in that and just as we did before we can hadoop fs minus cat that file to see our actual results. Congratulations. You finished the last lesson of this course. Now you'll practice by answering questions on the same data set we've been using. Then for the capstone of the course, you're going to use a different data set and apply all the concepts you've learned. Hi, my name is Andy and I'm a instructor at Udacity. I worked with Sara and Ian and Ngundega to help prepare this lesson and today I'm going to deliver it for you. So, let's get started. Up to this point you've familiarized yourself with MapReduce, you've written some mapper code, some reducer code. And you probably have a decent understanding of what's going on behind the scenes. But you might not yet know when to use MapReduce to solve a problem. And that's what we're going to try and get at in this lesson. We're going to give you a bunch of problems to solve and actually, each of these problems has been deliberately selected, because each of these problems actually represents some larger class of problems. In reality, the problems you're likely to run into on the job are going to fall into one of these certain classes of problems, which is very convenient because it means you can apply a sort of boiler plate solution, with some modifications. These boilerplate solutions we call design patterns, and they're really helpful. In this lesson we'll learn about some of these design patterns, and we'll practice using them on various problems. We're not going to get into the details of where the patterns came from, or why they look the way they do, but, at the very least, at the end of this lesson, you will have, well for one, you'll practice using these patterns and you also have a resource. That you can always come back to whenever you run into a problem that can be solved using one of these patterns. If this kind of stuff is really interesting to you, you should definitely check out a book called MapReduce Design Patterns. It's written by Donald Miner and Adam Shook and it's really interesting. Anyways, let's get started. So, in this lesson, what patterns are we going to learn? Well, we'll learn about filtering patterns, we'll learn about summarization patterns, and something we'll call structural patterns. And these are all just frameworks for using MapReduce for solve problems that you've probably encountered as a programmer before. Filtering patterns solve filtering problems. Like, sampling data, or, maybe even generating some top-n list. Maybe, top ten something. Summarization patterns solve summarization problems. Getting a high-level view of your data. Things like counting records. Finding min and max, then calculating basic statistical values, like mean, median, and mode. We can also use these patterns to create an index, finally, these structural patterns will solves structural problems. And, the one more focus on, in this lesson, is combining two data sets. And, just to warn you, this lesson will go fast, and, that means you probably won't remember everything about all of these patterns. Totally okay, not expected. The point of this lesson, is for you to know that these patterns exist. And that if you ever find yourself in a situation with big data where you say this problem is really just a filtering or a summarization problem. You'll know where to look, to find a technique to solve. So, filtering patterns all have one thing in common. They don't change the actual records of our data. So by that I mean, if we have some long list of data with lots and lots of records. One, two, three, so on. A filtering pattern will, go through the entire data set and just. Decide which data to keep, and which to throw away. And so if this giant list is the input this smaller list, this subset of the original data would be the output. And there's a lot of ways filtering can happen. We can have a simple filter, which is basically just a function which returns keep or throw away for. Each of these records, and for the records that get kept. Well, they wind up in the output. There's a bloom filter, which we're actually not going to talk much about, but it's worth knowing about, because this is a much more efficient probabilistic filter. That comes with some trade-offs and you should check out the instructor notes if you want to read more. We also have things like sampling. Which is just a way to make a smaller data set from a large data set. By pulling out some sample, maybe the highest values of a particular field. If we want to pull out a representative sample, one that we think matches the characteristics of the original data. We can use random sampling. And finally, we can use this idea of filtering to find the, I don't know, top ten of something, and that's an exercise we're actually going to do. Click. So, let's get some practice filtering. And the data set we're going to work, with [INAUDIBLE] comes from your fellow classmates. It's our forum data. Now, forum posts, in our forums can be long. Or they can be very short. Or maybe, they're somewhere in the middle. And for this problem, we don't care about the long posts. We're in the medium ones. We want to filter just the very short posts, the posts that are one sentence or less. An interesting application of MapReduce and these design patterns is making top N record lists. These are especially useful if you work at a company like BuzzFeed. But even if you don't, you still might want to use these. And in this question we're going to, in particular, find the top ten longest forum posts. Now, with a relational data base management system you just first sort your data. In this case forum posts. And then pick the top N records. But with MapReduce, this just won't work. The data isn't sorted and it's processed in several machines. So, instead, what we're going to have to do is have each mapper generate a top N list and then send these local lists to the reducers who can then find the global top N. It's sort of like what happens in the Olympics. If you want to find the top three I don't know, swimmers, every country needs to send their top three swimmers to the Olympics. And then, through a competition, figure who globally the top three swimmers are. Anyways, let's go ahead and find the top ten longest forum posts in the Udacity forum data. The next design patterns we're going to discuss, are summarization patterns. And, these are patterns that give you, some quick easy high level understanding of your data. And we're going to make a distinction here. We're going to talk about making what's called an inverted index. And this is very similar to the index you'd find in a back of a book, or the indexing that Google does when it crawls the web. And, we're also going to talk about numerical summarizations. And, these are things like finding counts, how many of a certain record type, or min and max. And min and max of course, can be used to calculate first or last. Or calculate statistics like, mean and median, and basically any other high level numerical value that you can use to summarize your data set. In this section of summarization patterns, we'll also talk about some interesting additional [INAUDIBLE] functionality, something that can exist between [INAUDIBLE] reducers, known as combiners. So, let's keep going. There are a lot of times when we want to build a reverse index from a data set, to allow for some faster searching. If you've ever read some sort of reference book, you've probably used an index. So here's a book I like, A Brief History of Time. And the subtitle is From the Big Bang to Black Holes. So let's say I just want to read about the big bang. Well, only thing I can do is crawl through all of this text, and look for the words big bang. But of course, I'm not going to do that. Somebody's already gone through put in the work in advance to create an index for me. So at the end of the book. I can just go to the index, and on the first page we have B, and I see big bang. And these are all the pages where I can find it. And of course you can do the same thing with the web, except instead of numbers, we'd have links to web pages. This is a common problem, and so there's a design pattern to solve. Why don't you take a shot at building your own inverted index? Now let's move on to numerical summarizations. These are some of my favorite. These are the summarizations you're going to want to use when your boss says, "show me the numbers". These are the numbers you're going to want to show your boss. And, what kind of numerical summarizations? Well, we've talked about it a bit already. We might want to do a word or record count. And for a word or record count, which is something you've already done with the web server log files, the [UNKNOWN] just outputs the thing you're interested as the key, and outputs one for the value. The reducer then chugs along, sums up the values, bada bing, bada boom, problem solved. This is like the Hello World of map produce. Slightly more difficult are things like calculating the mean or the median, or if we really want to get really fancy, the standard deviation of a set of data. So let's do it. Let's calculate mean and standard deviation. And to do that, let's think back to our example with stores and sales. And let's say the question we want to answer is, is there any correlation between the day of the week and how much money people spend on various items? And what's interesting about this design pattern is that all the mapper has to do is, I'll put the day of the week as a key, so maybe Monday, and the value of a sale, maybe $5.20 as a value. That's it. What does that leave for the reducer? Well, it leaves all the math for the reducer. And the general reason for this rule of thumb, for what the mapper and reducer are doing, comes from the fact that oftentimes with these, with these summary statistics, you sort of need to know all of the statistics or all of the parent data before you can make any calculations. So we don't want to jump the gun and have the mapper do calculations before it's ready. So why don't you go ahead and calculate the mean and standard deviation for sales for each day of the week, to help us try to answer this question. If there's any correlation between the day of the week and how much people spend. Now we're going to talk about a pretty cool new tool. Something that goes between the map and the reduce phase, called the combiner. Before we do that, let's review how you may have calculated the mean in the last problem. And I don't know exactly what you did, but one thing you may have done is, something like this. First, you probably had your mapper go through the data. I should say your mappers, go through the data, and output, key value pairs that looked something like, day of week, amount spent. Then, for each day of the week, your reducer probably kept a total of the sum and a count. So actually maybe your value here, was the amount of money spent. And then also a one, to help increment the count. And then you probably would divide the sum by the count, and it looks pretty good. But remember, net produce is happening on this whole network of data nodes. Each of these little boxes, we'll pretend is a data node. And your data's spread out across all of these different machines. And now let's think about where each of these three steps took place. So, we're not going to focus on what happened but where it happened. Well, for step one, the mappers were all over the place. There were mappers, anywhere there is relevant data. But, for any given day, all of the reduction had to happen on a single machine, where that reducer was living. And actually, that's enough, we've already exposed a problem. Because if there's a lot of data, even if each individual output was simply a day and a number, depending on the number of records we have, transferring all of this, all this data to this machine, with reducer lift, that, that's a lot of bandwidth. That's going to be a lot of traffic on our network that we don't necessarily want, and it doesn't actually seem like we need to have. So what if we could do some of this reduction locally? What if on each machine, before we send our key-value pairs off, off to be reduced, what if we could do some pre-reduction? And it turns out we can. And that pre-reduction happens with these things called combiners. And before you practice using combiners let me try to sell you a bit more on their value. So as you know, when you use a map reduce job you get some output like this. This is where it's displaying how much mapping has happened and how much reducing. But you get this tracking URL. And if you open that URL in a browser you get a job page, and that job page looks something like this. And actually if you scroll down below here you'll see some more interesting statistics. And in fact we ran two jobs. They were identical jobs, except one implemented combiners, so they were reducing locally as much as possible, before shuttling that data off to that machine that held the end reducer. Let's compare the statistics from those two jobs. The one with combiners, the one without. So let's compare what happened on these job pages. The one without the combiner versus the one with combiner. So, a lot of stuff is the same. The mappers to the same amount, the input matches the output for without and with. That's as expected. In fact, a lot of things look pretty similar. But where things get really interesting is down here at reduce input records. So, the reducers, had to handle the whole 4 million records that the macro had put when there is no combiner. But when there was a combiner, these 4 million mapped output records are combined into 412 reduced input records, or reducer input records. That's pretty amazing. That's a huge change. In fact, the reducers had to shuffle far fewer bytes. And for time spent, with the combiner this job took 31 seconds to run versus without where it took 43. That's a significant difference. So you can really use combiners to improve the efficiency of your map reduced jobs. And we're going to try that next. Finally, we're going to talk about structural patterns. And this is a very pragmatic pattern to discuss honestly because it's a pattern you use if you're ever migrating data, from relational data base management systems, to a Hadoop system. And what you do with this pattern, is taking advantage of the fact that Hadoop, doesn't care what format your data is in, which means you can take advantage of hierarchical data, which will allow you to avoid doing live joins and several data sets at the time of analysis. So if you know a kind of information you're going to want to get later you can save a lot of time by reformatting your data. Now in order to use this pattern, your data sources must be linked by some foreign keys, and the data must be structured and row based, and in fact this pattern in the book of [INAUDIBLE] design pattern is actually they refer to as structured hierarchical pattern. So we going to use this structural to higher our pattern, to combine two data sets and these data sets are going to come from forms and what we're going to want to do, is understand what the reputation or the karma of the author of a post is and to do this we're going to have to combine two tables. A post table and a user table. Sorry. And a post contains data about each post. Some ID, the title of the post, any tags this author ID, which is going to be interesting and then the body of the post, and some other things. The user table. This is the user pointer ID. And that is the common key we're going to use to combine these two data sets. It also has some other information like reputation, which is going to help us answer this question and gold, silver and bronze badges. So, go ahead and see if you can answer this question. So we've talked about a few design patterns with MapReduce. We've talked about filtering patterns, summarizing patterns, and structural patterns, but what good are they? Well, these patterns, can be thought of as sort of lenses. To try and view your problems through. If you have a big data problem and you're trying to do some analysis, that at its heart is really about filtering, about selecting from a large data set and getting some smaller sample of that data set. There's a good chance that you can use this filtering pattern. Likewise, if the fundamental nature of your, of your problem is summarizing or going from a structured to hierarchical sort of database. In fact, these aren't all the patterns you might want to use. There's also organizational patterns. There's patterns that deal with input and output and there's others. But getting yourself to ask this question, can, you just have to say. Is this problem I'm dealing with filtering, or summarizing, or structural, or something else? That isn't simple. In fact, just identifying when map reduce is the appropriate tool for a problem is a large part of what it means to be a MapReduce expert. And getting this expertise takes practice. This practice can come naturally in the job environment. Or you could choose to make it happen deliberately by seeking out problems and trying to solve them. But, if you enjoy learning about and thinking about these patterns, you should check that book, I mentioned earlier, Mapreduce Design Patterns and you can learn more there. But now, it's time to try implementing some of these patterns in, in sort of novel fitting.